{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualisation\nimport tensorflow as tf\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":160,"outputs":[{"output_type":"stream","text":"['year_prediction.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/year_prediction.csv\")\ndata = data.rename(index=str, columns={\"label\":\"year\"})","execution_count":161,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot a histogram to understand how evenly spread the data is by viewing number of songs we have for a given year."},{"metadata":{"trusted":true},"cell_type":"code","source":"nsongs = {}\nfor y in range(1922,2012):\n    nsongs[y] = len(data[data.year==y])\nyrs = range(1922,2011)\nvalues = [nsongs[y] for y in yrs]\nplt.bar(yrs, values, align='center')\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of songs\")","execution_count":162,"outputs":[{"output_type":"execute_result","execution_count":162,"data":{"text/plain":"Text(0, 0.5, 'Number of songs')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate input attributes and output into different dataframes\nX = data.iloc[:,1:]\nY = data.iloc[:,0]\n\n# Train set\nX_train = X.iloc[0:463715,:]\ny_train = Y.iloc[0:463715]\n\n# Validation set\nX_test = X.iloc[463715:,:]\ny_test = Y.iloc[463715:]\n","execution_count":163,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each of the features takes a wide range of different values and distributions.\n\nWe apply MinMax scaling to our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit on training set only.\nscaler.fit(X_train)\n# Apply transform to both the train set and the test set.\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train_scaled,columns=X_train.columns)\nX_test = pd.DataFrame(X_test_scaled,columns=X_train.columns)","execution_count":164,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":165,"outputs":[{"output_type":"execute_result","execution_count":165,"data":{"text/plain":"          TimbreAvg1         ...          TimbreCovariance78\ncount  463715.000000         ...               463715.000000\nmean        0.691393         ...                    0.347715\nstd         0.100947         ...                    0.023989\nmin         0.000000         ...                    0.000000\n25%         0.634471         ...                    0.336685\n50%         0.705958         ...                    0.346333\n75%         0.765257         ...                    0.356798\nmax         1.000000         ...                    1.000000\n\n[8 rows x 90 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TimbreAvg1</th>\n      <th>TimbreAvg2</th>\n      <th>TimbreAvg3</th>\n      <th>TimbreAvg4</th>\n      <th>TimbreAvg5</th>\n      <th>TimbreAvg6</th>\n      <th>TimbreAvg7</th>\n      <th>TimbreAvg8</th>\n      <th>TimbreAvg9</th>\n      <th>TimbreAvg10</th>\n      <th>TimbreAvg11</th>\n      <th>TimbreAvg12</th>\n      <th>TimbreCovariance1</th>\n      <th>TimbreCovariance2</th>\n      <th>TimbreCovariance3</th>\n      <th>TimbreCovariance4</th>\n      <th>TimbreCovariance5</th>\n      <th>TimbreCovariance6</th>\n      <th>TimbreCovariance7</th>\n      <th>TimbreCovariance8</th>\n      <th>TimbreCovariance9</th>\n      <th>TimbreCovariance10</th>\n      <th>TimbreCovariance11</th>\n      <th>TimbreCovariance12</th>\n      <th>TimbreCovariance13</th>\n      <th>TimbreCovariance14</th>\n      <th>TimbreCovariance15</th>\n      <th>TimbreCovariance16</th>\n      <th>TimbreCovariance17</th>\n      <th>TimbreCovariance18</th>\n      <th>TimbreCovariance19</th>\n      <th>TimbreCovariance20</th>\n      <th>TimbreCovariance21</th>\n      <th>TimbreCovariance22</th>\n      <th>TimbreCovariance23</th>\n      <th>TimbreCovariance24</th>\n      <th>TimbreCovariance25</th>\n      <th>TimbreCovariance26</th>\n      <th>TimbreCovariance27</th>\n      <th>TimbreCovariance28</th>\n      <th>...</th>\n      <th>TimbreCovariance39</th>\n      <th>TimbreCovariance40</th>\n      <th>TimbreCovariance41</th>\n      <th>TimbreCovariance42</th>\n      <th>TimbreCovariance43</th>\n      <th>TimbreCovariance44</th>\n      <th>TimbreCovariance45</th>\n      <th>TimbreCovariance46</th>\n      <th>TimbreCovariance47</th>\n      <th>TimbreCovariance48</th>\n      <th>TimbreCovariance49</th>\n      <th>TimbreCovariance50</th>\n      <th>TimbreCovariance51</th>\n      <th>TimbreCovariance52</th>\n      <th>TimbreCovariance53</th>\n      <th>TimbreCovariance54</th>\n      <th>TimbreCovariance55</th>\n      <th>TimbreCovariance56</th>\n      <th>TimbreCovariance57</th>\n      <th>TimbreCovariance58</th>\n      <th>TimbreCovariance59</th>\n      <th>TimbreCovariance60</th>\n      <th>TimbreCovariance61</th>\n      <th>TimbreCovariance62</th>\n      <th>TimbreCovariance63</th>\n      <th>TimbreCovariance64</th>\n      <th>TimbreCovariance65</th>\n      <th>TimbreCovariance66</th>\n      <th>TimbreCovariance67</th>\n      <th>TimbreCovariance68</th>\n      <th>TimbreCovariance69</th>\n      <th>TimbreCovariance70</th>\n      <th>TimbreCovariance71</th>\n      <th>TimbreCovariance72</th>\n      <th>TimbreCovariance73</th>\n      <th>TimbreCovariance74</th>\n      <th>TimbreCovariance75</th>\n      <th>TimbreCovariance76</th>\n      <th>TimbreCovariance77</th>\n      <th>TimbreCovariance78</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>...</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n      <td>463715.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.691393</td>\n      <td>0.469181</td>\n      <td>0.496357</td>\n      <td>0.350035</td>\n      <td>0.395117</td>\n      <td>0.358260</td>\n      <td>0.515310</td>\n      <td>0.397887</td>\n      <td>0.477290</td>\n      <td>0.426607</td>\n      <td>0.691819</td>\n      <td>0.530809</td>\n      <td>0.061015</td>\n      <td>0.036932</td>\n      <td>0.052829</td>\n      <td>0.046941</td>\n      <td>0.045245</td>\n      <td>0.051828</td>\n      <td>0.054789</td>\n      <td>0.053319</td>\n      <td>0.040334</td>\n      <td>0.083562</td>\n      <td>0.041914</td>\n      <td>0.029258</td>\n      <td>0.588071</td>\n      <td>0.354463</td>\n      <td>0.451344</td>\n      <td>0.555480</td>\n      <td>0.508205</td>\n      <td>0.473871</td>\n      <td>0.676466</td>\n      <td>0.443812</td>\n      <td>0.369822</td>\n      <td>0.404615</td>\n      <td>0.353309</td>\n      <td>0.424043</td>\n      <td>0.371572</td>\n      <td>0.562552</td>\n      <td>0.605844</td>\n      <td>0.486739</td>\n      <td>...</td>\n      <td>0.615812</td>\n      <td>0.421879</td>\n      <td>0.573337</td>\n      <td>0.431647</td>\n      <td>0.744069</td>\n      <td>0.400336</td>\n      <td>0.372550</td>\n      <td>0.266772</td>\n      <td>0.362311</td>\n      <td>0.301800</td>\n      <td>0.512831</td>\n      <td>0.537379</td>\n      <td>0.417522</td>\n      <td>0.477130</td>\n      <td>0.719306</td>\n      <td>0.530612</td>\n      <td>0.369416</td>\n      <td>0.353011</td>\n      <td>0.269319</td>\n      <td>0.554657</td>\n      <td>0.639893</td>\n      <td>0.595036</td>\n      <td>0.795590</td>\n      <td>0.521606</td>\n      <td>0.512374</td>\n      <td>0.488463</td>\n      <td>0.494068</td>\n      <td>0.376795</td>\n      <td>0.631798</td>\n      <td>0.563245</td>\n      <td>0.354631</td>\n      <td>0.487984</td>\n      <td>0.368845</td>\n      <td>0.657332</td>\n      <td>0.568014</td>\n      <td>0.466481</td>\n      <td>0.600205</td>\n      <td>0.343815</td>\n      <td>0.503517</td>\n      <td>0.347715</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.100947</td>\n      <td>0.071570</td>\n      <td>0.056527</td>\n      <td>0.036814</td>\n      <td>0.051474</td>\n      <td>0.063671</td>\n      <td>0.040431</td>\n      <td>0.044802</td>\n      <td>0.038783</td>\n      <td>0.063964</td>\n      <td>0.043463</td>\n      <td>0.045744</td>\n      <td>0.040440</td>\n      <td>0.026595</td>\n      <td>0.034290</td>\n      <td>0.034337</td>\n      <td>0.023954</td>\n      <td>0.034274</td>\n      <td>0.029750</td>\n      <td>0.032271</td>\n      <td>0.022275</td>\n      <td>0.044558</td>\n      <td>0.027650</td>\n      <td>0.015673</td>\n      <td>0.024969</td>\n      <td>0.018895</td>\n      <td>0.020548</td>\n      <td>0.028204</td>\n      <td>0.024609</td>\n      <td>0.020223</td>\n      <td>0.017601</td>\n      <td>0.020666</td>\n      <td>0.016301</td>\n      <td>0.013634</td>\n      <td>0.015131</td>\n      <td>0.025654</td>\n      <td>0.018390</td>\n      <td>0.024946</td>\n      <td>0.016436</td>\n      <td>0.012650</td>\n      <td>...</td>\n      <td>0.025309</td>\n      <td>0.017857</td>\n      <td>0.012536</td>\n      <td>0.016366</td>\n      <td>0.025507</td>\n      <td>0.023793</td>\n      <td>0.022147</td>\n      <td>0.015102</td>\n      <td>0.020012</td>\n      <td>0.017587</td>\n      <td>0.015973</td>\n      <td>0.023356</td>\n      <td>0.026971</td>\n      <td>0.014372</td>\n      <td>0.021669</td>\n      <td>0.017463</td>\n      <td>0.030666</td>\n      <td>0.015855</td>\n      <td>0.014724</td>\n      <td>0.021925</td>\n      <td>0.022821</td>\n      <td>0.019606</td>\n      <td>0.015579</td>\n      <td>0.018304</td>\n      <td>0.024269</td>\n      <td>0.018045</td>\n      <td>0.013502</td>\n      <td>0.026496</td>\n      <td>0.015304</td>\n      <td>0.014604</td>\n      <td>0.025093</td>\n      <td>0.019768</td>\n      <td>0.024255</td>\n      <td>0.019886</td>\n      <td>0.026819</td>\n      <td>0.016738</td>\n      <td>0.024350</td>\n      <td>0.019045</td>\n      <td>0.012446</td>\n      <td>0.023989</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.634471</td>\n      <td>0.431166</td>\n      <td>0.464150</td>\n      <td>0.328296</td>\n      <td>0.363307</td>\n      <td>0.314100</td>\n      <td>0.492040</td>\n      <td>0.371622</td>\n      <td>0.455227</td>\n      <td>0.384236</td>\n      <td>0.666902</td>\n      <td>0.502762</td>\n      <td>0.032903</td>\n      <td>0.019860</td>\n      <td>0.029653</td>\n      <td>0.024999</td>\n      <td>0.029936</td>\n      <td>0.028599</td>\n      <td>0.034962</td>\n      <td>0.032599</td>\n      <td>0.026155</td>\n      <td>0.053510</td>\n      <td>0.023966</td>\n      <td>0.019620</td>\n      <td>0.576851</td>\n      <td>0.345704</td>\n      <td>0.443649</td>\n      <td>0.543756</td>\n      <td>0.496901</td>\n      <td>0.463525</td>\n      <td>0.668716</td>\n      <td>0.435032</td>\n      <td>0.362304</td>\n      <td>0.398372</td>\n      <td>0.347068</td>\n      <td>0.410214</td>\n      <td>0.363964</td>\n      <td>0.550613</td>\n      <td>0.599561</td>\n      <td>0.481041</td>\n      <td>...</td>\n      <td>0.603614</td>\n      <td>0.412993</td>\n      <td>0.567825</td>\n      <td>0.424660</td>\n      <td>0.735812</td>\n      <td>0.388816</td>\n      <td>0.362327</td>\n      <td>0.260170</td>\n      <td>0.352446</td>\n      <td>0.293710</td>\n      <td>0.505172</td>\n      <td>0.526958</td>\n      <td>0.404470</td>\n      <td>0.471317</td>\n      <td>0.710898</td>\n      <td>0.523496</td>\n      <td>0.354213</td>\n      <td>0.346601</td>\n      <td>0.262169</td>\n      <td>0.545554</td>\n      <td>0.631992</td>\n      <td>0.586874</td>\n      <td>0.789989</td>\n      <td>0.512644</td>\n      <td>0.501426</td>\n      <td>0.481987</td>\n      <td>0.487710</td>\n      <td>0.365284</td>\n      <td>0.627335</td>\n      <td>0.556783</td>\n      <td>0.340913</td>\n      <td>0.480545</td>\n      <td>0.356429</td>\n      <td>0.648426</td>\n      <td>0.556221</td>\n      <td>0.459283</td>\n      <td>0.589716</td>\n      <td>0.333782</td>\n      <td>0.498175</td>\n      <td>0.336685</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.705958</td>\n      <td>0.479041</td>\n      <td>0.499274</td>\n      <td>0.345928</td>\n      <td>0.396288</td>\n      <td>0.350109</td>\n      <td>0.516244</td>\n      <td>0.398215</td>\n      <td>0.477664</td>\n      <td>0.425654</td>\n      <td>0.692281</td>\n      <td>0.529513</td>\n      <td>0.052644</td>\n      <td>0.030402</td>\n      <td>0.045217</td>\n      <td>0.038357</td>\n      <td>0.040531</td>\n      <td>0.043547</td>\n      <td>0.048959</td>\n      <td>0.045778</td>\n      <td>0.035742</td>\n      <td>0.074719</td>\n      <td>0.034995</td>\n      <td>0.026409</td>\n      <td>0.584910</td>\n      <td>0.352183</td>\n      <td>0.451626</td>\n      <td>0.556034</td>\n      <td>0.507122</td>\n      <td>0.471686</td>\n      <td>0.676283</td>\n      <td>0.443550</td>\n      <td>0.369430</td>\n      <td>0.403198</td>\n      <td>0.354251</td>\n      <td>0.420130</td>\n      <td>0.370622</td>\n      <td>0.560477</td>\n      <td>0.607698</td>\n      <td>0.486623</td>\n      <td>...</td>\n      <td>0.615471</td>\n      <td>0.421269</td>\n      <td>0.572661</td>\n      <td>0.431431</td>\n      <td>0.747703</td>\n      <td>0.397364</td>\n      <td>0.371202</td>\n      <td>0.265863</td>\n      <td>0.361004</td>\n      <td>0.301399</td>\n      <td>0.511876</td>\n      <td>0.538022</td>\n      <td>0.415502</td>\n      <td>0.478171</td>\n      <td>0.720191</td>\n      <td>0.531227</td>\n      <td>0.369155</td>\n      <td>0.351679</td>\n      <td>0.267990</td>\n      <td>0.554698</td>\n      <td>0.642040</td>\n      <td>0.595653</td>\n      <td>0.796303</td>\n      <td>0.521472</td>\n      <td>0.514188</td>\n      <td>0.490602</td>\n      <td>0.492406</td>\n      <td>0.378191</td>\n      <td>0.634349</td>\n      <td>0.563230</td>\n      <td>0.349484</td>\n      <td>0.490259</td>\n      <td>0.366313</td>\n      <td>0.656434</td>\n      <td>0.568816</td>\n      <td>0.466181</td>\n      <td>0.600938</td>\n      <td>0.341908</td>\n      <td>0.502698</td>\n      <td>0.346333</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.765257</td>\n      <td>0.517551</td>\n      <td>0.530164</td>\n      <td>0.367222</td>\n      <td>0.427238</td>\n      <td>0.393690</td>\n      <td>0.539991</td>\n      <td>0.424327</td>\n      <td>0.500152</td>\n      <td>0.468346</td>\n      <td>0.717497</td>\n      <td>0.557309</td>\n      <td>0.078641</td>\n      <td>0.046080</td>\n      <td>0.066969</td>\n      <td>0.058735</td>\n      <td>0.054802</td>\n      <td>0.065766</td>\n      <td>0.067635</td>\n      <td>0.064731</td>\n      <td>0.048880</td>\n      <td>0.102909</td>\n      <td>0.051911</td>\n      <td>0.035122</td>\n      <td>0.597456</td>\n      <td>0.360806</td>\n      <td>0.459407</td>\n      <td>0.567574</td>\n      <td>0.518682</td>\n      <td>0.481778</td>\n      <td>0.684181</td>\n      <td>0.452904</td>\n      <td>0.376703</td>\n      <td>0.408807</td>\n      <td>0.360523</td>\n      <td>0.434532</td>\n      <td>0.377622</td>\n      <td>0.572979</td>\n      <td>0.614005</td>\n      <td>0.492403</td>\n      <td>...</td>\n      <td>0.627763</td>\n      <td>0.429982</td>\n      <td>0.578105</td>\n      <td>0.438619</td>\n      <td>0.756769</td>\n      <td>0.408971</td>\n      <td>0.381147</td>\n      <td>0.271646</td>\n      <td>0.370888</td>\n      <td>0.308952</td>\n      <td>0.519270</td>\n      <td>0.548485</td>\n      <td>0.428473</td>\n      <td>0.483641</td>\n      <td>0.729664</td>\n      <td>0.537933</td>\n      <td>0.384313</td>\n      <td>0.357536</td>\n      <td>0.274986</td>\n      <td>0.563966</td>\n      <td>0.650647</td>\n      <td>0.603455</td>\n      <td>0.802214</td>\n      <td>0.530068</td>\n      <td>0.524763</td>\n      <td>0.497455</td>\n      <td>0.498366</td>\n      <td>0.390441</td>\n      <td>0.639235</td>\n      <td>0.570163</td>\n      <td>0.362847</td>\n      <td>0.497755</td>\n      <td>0.378403</td>\n      <td>0.665694</td>\n      <td>0.581546</td>\n      <td>0.473821</td>\n      <td>0.611202</td>\n      <td>0.351684</td>\n      <td>0.507992</td>\n      <td>0.356798</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"90 features is lot of features and so we attempt dimensionality reduction by implementing PCA."},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{},"cell_type":"markdown","source":"First we normalise our data using scikits StandardScalar.\nThis is a necessary step for pca:\n> Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\n(https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Fit on training set only.\nscaler.fit(X_train)\n# Apply transform to both the train set and the test set.\nX_train_std = scaler.transform(X_train)\nX_test_std = scaler.transform(X_test)","execution_count":166,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_std = pd.DataFrame(X_train_std,columns=X_train.columns)\nX_train_std.describe()","execution_count":167,"outputs":[{"output_type":"execute_result","execution_count":167,"data":{"text/plain":"         TimbreAvg1         ...          TimbreCovariance78\ncount  4.637150e+05         ...                4.637150e+05\nmean   7.860182e-16         ...                4.832588e-15\nstd    1.000001e+00         ...                1.000001e+00\nmin   -6.849083e+00         ...               -1.449495e+01\n25%   -5.638876e-01         ...               -4.598088e-01\n50%    1.442775e-01         ...               -5.763047e-02\n75%    7.317100e-01         ...                3.786469e-01\nmax    3.057122e+00         ...                2.719132e+01\n\n[8 rows x 90 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TimbreAvg1</th>\n      <th>TimbreAvg2</th>\n      <th>TimbreAvg3</th>\n      <th>TimbreAvg4</th>\n      <th>TimbreAvg5</th>\n      <th>TimbreAvg6</th>\n      <th>TimbreAvg7</th>\n      <th>TimbreAvg8</th>\n      <th>TimbreAvg9</th>\n      <th>TimbreAvg10</th>\n      <th>TimbreAvg11</th>\n      <th>TimbreAvg12</th>\n      <th>TimbreCovariance1</th>\n      <th>TimbreCovariance2</th>\n      <th>TimbreCovariance3</th>\n      <th>TimbreCovariance4</th>\n      <th>TimbreCovariance5</th>\n      <th>TimbreCovariance6</th>\n      <th>TimbreCovariance7</th>\n      <th>TimbreCovariance8</th>\n      <th>TimbreCovariance9</th>\n      <th>TimbreCovariance10</th>\n      <th>TimbreCovariance11</th>\n      <th>TimbreCovariance12</th>\n      <th>TimbreCovariance13</th>\n      <th>TimbreCovariance14</th>\n      <th>TimbreCovariance15</th>\n      <th>TimbreCovariance16</th>\n      <th>TimbreCovariance17</th>\n      <th>TimbreCovariance18</th>\n      <th>TimbreCovariance19</th>\n      <th>TimbreCovariance20</th>\n      <th>TimbreCovariance21</th>\n      <th>TimbreCovariance22</th>\n      <th>TimbreCovariance23</th>\n      <th>TimbreCovariance24</th>\n      <th>TimbreCovariance25</th>\n      <th>TimbreCovariance26</th>\n      <th>TimbreCovariance27</th>\n      <th>TimbreCovariance28</th>\n      <th>...</th>\n      <th>TimbreCovariance39</th>\n      <th>TimbreCovariance40</th>\n      <th>TimbreCovariance41</th>\n      <th>TimbreCovariance42</th>\n      <th>TimbreCovariance43</th>\n      <th>TimbreCovariance44</th>\n      <th>TimbreCovariance45</th>\n      <th>TimbreCovariance46</th>\n      <th>TimbreCovariance47</th>\n      <th>TimbreCovariance48</th>\n      <th>TimbreCovariance49</th>\n      <th>TimbreCovariance50</th>\n      <th>TimbreCovariance51</th>\n      <th>TimbreCovariance52</th>\n      <th>TimbreCovariance53</th>\n      <th>TimbreCovariance54</th>\n      <th>TimbreCovariance55</th>\n      <th>TimbreCovariance56</th>\n      <th>TimbreCovariance57</th>\n      <th>TimbreCovariance58</th>\n      <th>TimbreCovariance59</th>\n      <th>TimbreCovariance60</th>\n      <th>TimbreCovariance61</th>\n      <th>TimbreCovariance62</th>\n      <th>TimbreCovariance63</th>\n      <th>TimbreCovariance64</th>\n      <th>TimbreCovariance65</th>\n      <th>TimbreCovariance66</th>\n      <th>TimbreCovariance67</th>\n      <th>TimbreCovariance68</th>\n      <th>TimbreCovariance69</th>\n      <th>TimbreCovariance70</th>\n      <th>TimbreCovariance71</th>\n      <th>TimbreCovariance72</th>\n      <th>TimbreCovariance73</th>\n      <th>TimbreCovariance74</th>\n      <th>TimbreCovariance75</th>\n      <th>TimbreCovariance76</th>\n      <th>TimbreCovariance77</th>\n      <th>TimbreCovariance78</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>...</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n      <td>4.637150e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.860182e-16</td>\n      <td>-2.091763e-15</td>\n      <td>1.355798e-16</td>\n      <td>-1.357427e-15</td>\n      <td>-8.698030e-16</td>\n      <td>1.346587e-15</td>\n      <td>-5.625596e-16</td>\n      <td>4.602650e-16</td>\n      <td>-3.256487e-16</td>\n      <td>1.911129e-15</td>\n      <td>4.018197e-15</td>\n      <td>-2.502608e-15</td>\n      <td>-6.190568e-17</td>\n      <td>-1.069821e-17</td>\n      <td>-3.529729e-16</td>\n      <td>-9.763661e-17</td>\n      <td>-2.724692e-16</td>\n      <td>2.504086e-16</td>\n      <td>8.627746e-16</td>\n      <td>4.121119e-16</td>\n      <td>-6.631095e-16</td>\n      <td>3.394529e-16</td>\n      <td>-7.586569e-16</td>\n      <td>-8.156104e-17</td>\n      <td>-8.968334e-15</td>\n      <td>-2.391046e-15</td>\n      <td>6.454967e-15</td>\n      <td>-2.494976e-15</td>\n      <td>4.325894e-15</td>\n      <td>2.140151e-17</td>\n      <td>5.876518e-15</td>\n      <td>-1.396214e-15</td>\n      <td>3.546603e-15</td>\n      <td>1.237439e-14</td>\n      <td>1.326574e-14</td>\n      <td>-1.499921e-15</td>\n      <td>-2.790937e-15</td>\n      <td>-4.764826e-16</td>\n      <td>8.122309e-15</td>\n      <td>5.200963e-15</td>\n      <td>...</td>\n      <td>-7.736401e-16</td>\n      <td>8.136067e-15</td>\n      <td>2.407077e-14</td>\n      <td>5.731586e-15</td>\n      <td>1.305549e-15</td>\n      <td>5.385272e-16</td>\n      <td>1.614794e-15</td>\n      <td>-3.452024e-15</td>\n      <td>-1.443823e-15</td>\n      <td>1.322732e-17</td>\n      <td>2.803837e-15</td>\n      <td>8.785695e-15</td>\n      <td>-9.397954e-16</td>\n      <td>-6.418378e-15</td>\n      <td>2.609099e-15</td>\n      <td>-8.714935e-16</td>\n      <td>2.895461e-15</td>\n      <td>-1.027189e-14</td>\n      <td>3.537959e-16</td>\n      <td>3.607981e-15</td>\n      <td>-5.811512e-15</td>\n      <td>3.722570e-15</td>\n      <td>-4.277726e-15</td>\n      <td>5.148444e-15</td>\n      <td>7.730112e-16</td>\n      <td>-2.349867e-15</td>\n      <td>-7.733918e-16</td>\n      <td>2.570563e-15</td>\n      <td>7.668392e-15</td>\n      <td>1.063528e-14</td>\n      <td>1.832125e-16</td>\n      <td>8.840391e-16</td>\n      <td>-4.720778e-15</td>\n      <td>-1.128972e-15</td>\n      <td>-4.590983e-15</td>\n      <td>-8.157310e-15</td>\n      <td>-2.556776e-15</td>\n      <td>-1.843250e-15</td>\n      <td>-7.529481e-15</td>\n      <td>4.832588e-15</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>...</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-6.849083e+00</td>\n      <td>-6.555536e+00</td>\n      <td>-8.780881e+00</td>\n      <td>-9.508272e+00</td>\n      <td>-7.675981e+00</td>\n      <td>-5.626720e+00</td>\n      <td>-1.274530e+01</td>\n      <td>-8.881098e+00</td>\n      <td>-1.230653e+01</td>\n      <td>-6.669490e+00</td>\n      <td>-1.591731e+01</td>\n      <td>-1.160387e+01</td>\n      <td>-1.508788e+00</td>\n      <td>-1.388673e+00</td>\n      <td>-1.540662e+00</td>\n      <td>-1.367069e+00</td>\n      <td>-1.888816e+00</td>\n      <td>-1.512189e+00</td>\n      <td>-1.841663e+00</td>\n      <td>-1.652227e+00</td>\n      <td>-1.810713e+00</td>\n      <td>-1.875352e+00</td>\n      <td>-1.515879e+00</td>\n      <td>-1.866748e+00</td>\n      <td>-2.355239e+01</td>\n      <td>-1.875925e+01</td>\n      <td>-2.196547e+01</td>\n      <td>-1.969503e+01</td>\n      <td>-2.065124e+01</td>\n      <td>-2.343220e+01</td>\n      <td>-3.843378e+01</td>\n      <td>-2.147554e+01</td>\n      <td>-2.268682e+01</td>\n      <td>-2.967671e+01</td>\n      <td>-2.335012e+01</td>\n      <td>-1.652946e+01</td>\n      <td>-2.020461e+01</td>\n      <td>-2.255056e+01</td>\n      <td>-3.686078e+01</td>\n      <td>-3.847736e+01</td>\n      <td>...</td>\n      <td>-2.433150e+01</td>\n      <td>-2.362536e+01</td>\n      <td>-4.573577e+01</td>\n      <td>-2.637486e+01</td>\n      <td>-2.917145e+01</td>\n      <td>-1.682569e+01</td>\n      <td>-1.682179e+01</td>\n      <td>-1.766429e+01</td>\n      <td>-1.810442e+01</td>\n      <td>-1.715994e+01</td>\n      <td>-3.210613e+01</td>\n      <td>-2.300836e+01</td>\n      <td>-1.548023e+01</td>\n      <td>-3.319885e+01</td>\n      <td>-3.319514e+01</td>\n      <td>-3.038489e+01</td>\n      <td>-1.204649e+01</td>\n      <td>-2.226450e+01</td>\n      <td>-1.829168e+01</td>\n      <td>-2.529849e+01</td>\n      <td>-2.803944e+01</td>\n      <td>-3.034925e+01</td>\n      <td>-5.106798e+01</td>\n      <td>-2.849609e+01</td>\n      <td>-2.111252e+01</td>\n      <td>-2.706919e+01</td>\n      <td>-3.659346e+01</td>\n      <td>-1.422095e+01</td>\n      <td>-4.128419e+01</td>\n      <td>-3.856717e+01</td>\n      <td>-1.413267e+01</td>\n      <td>-2.468577e+01</td>\n      <td>-1.520687e+01</td>\n      <td>-3.305562e+01</td>\n      <td>-2.117983e+01</td>\n      <td>-2.786972e+01</td>\n      <td>-2.464943e+01</td>\n      <td>-1.805304e+01</td>\n      <td>-4.045652e+01</td>\n      <td>-1.449495e+01</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-5.638876e-01</td>\n      <td>-5.311584e-01</td>\n      <td>-5.697513e-01</td>\n      <td>-5.905186e-01</td>\n      <td>-6.179717e-01</td>\n      <td>-6.935698e-01</td>\n      <td>-5.755554e-01</td>\n      <td>-5.862542e-01</td>\n      <td>-5.688679e-01</td>\n      <td>-6.624309e-01</td>\n      <td>-5.732855e-01</td>\n      <td>-6.131185e-01</td>\n      <td>-6.951746e-01</td>\n      <td>-6.419074e-01</td>\n      <td>-6.759057e-01</td>\n      <td>-6.390212e-01</td>\n      <td>-6.390820e-01</td>\n      <td>-6.777498e-01</td>\n      <td>-6.664736e-01</td>\n      <td>-6.420709e-01</td>\n      <td>-6.365427e-01</td>\n      <td>-6.744530e-01</td>\n      <td>-6.491041e-01</td>\n      <td>-6.149120e-01</td>\n      <td>-4.493756e-01</td>\n      <td>-4.635728e-01</td>\n      <td>-3.744862e-01</td>\n      <td>-4.156852e-01</td>\n      <td>-4.593146e-01</td>\n      <td>-5.116203e-01</td>\n      <td>-4.403281e-01</td>\n      <td>-4.248182e-01</td>\n      <td>-4.611867e-01</td>\n      <td>-4.578923e-01</td>\n      <td>-4.124200e-01</td>\n      <td>-5.390717e-01</td>\n      <td>-4.136821e-01</td>\n      <td>-4.785796e-01</td>\n      <td>-3.822540e-01</td>\n      <td>-4.504104e-01</td>\n      <td>...</td>\n      <td>-4.819230e-01</td>\n      <td>-4.976248e-01</td>\n      <td>-4.397595e-01</td>\n      <td>-4.269083e-01</td>\n      <td>-3.237309e-01</td>\n      <td>-4.841597e-01</td>\n      <td>-4.615884e-01</td>\n      <td>-4.371545e-01</td>\n      <td>-4.929491e-01</td>\n      <td>-4.600017e-01</td>\n      <td>-4.795339e-01</td>\n      <td>-4.462117e-01</td>\n      <td>-4.839344e-01</td>\n      <td>-4.044806e-01</td>\n      <td>-3.880277e-01</td>\n      <td>-4.075018e-01</td>\n      <td>-4.957911e-01</td>\n      <td>-4.042848e-01</td>\n      <td>-4.855927e-01</td>\n      <td>-4.151934e-01</td>\n      <td>-3.462071e-01</td>\n      <td>-4.163205e-01</td>\n      <td>-3.595521e-01</td>\n      <td>-4.896345e-01</td>\n      <td>-4.511315e-01</td>\n      <td>-3.588562e-01</td>\n      <td>-4.708746e-01</td>\n      <td>-4.344471e-01</td>\n      <td>-2.916292e-01</td>\n      <td>-4.425110e-01</td>\n      <td>-5.466954e-01</td>\n      <td>-3.763002e-01</td>\n      <td>-5.118673e-01</td>\n      <td>-4.478800e-01</td>\n      <td>-4.397373e-01</td>\n      <td>-4.299846e-01</td>\n      <td>-4.307504e-01</td>\n      <td>-5.268242e-01</td>\n      <td>-4.292140e-01</td>\n      <td>-4.598088e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.442775e-01</td>\n      <td>1.377638e-01</td>\n      <td>5.161888e-02</td>\n      <td>-1.115648e-01</td>\n      <td>2.275066e-02</td>\n      <td>-1.280171e-01</td>\n      <td>2.309700e-02</td>\n      <td>7.320083e-03</td>\n      <td>9.665689e-03</td>\n      <td>-1.489907e-02</td>\n      <td>1.064278e-02</td>\n      <td>-2.832901e-02</td>\n      <td>-2.070092e-01</td>\n      <td>-2.455417e-01</td>\n      <td>-2.220075e-01</td>\n      <td>-2.499821e-01</td>\n      <td>-1.967996e-01</td>\n      <td>-2.416020e-01</td>\n      <td>-1.959821e-01</td>\n      <td>-2.336870e-01</td>\n      <td>-2.061242e-01</td>\n      <td>-1.984461e-01</td>\n      <td>-2.502268e-01</td>\n      <td>-1.817949e-01</td>\n      <td>-1.266212e-01</td>\n      <td>-1.206790e-01</td>\n      <td>1.372887e-02</td>\n      <td>1.965026e-02</td>\n      <td>-4.398765e-02</td>\n      <td>-1.080744e-01</td>\n      <td>-1.039974e-02</td>\n      <td>-1.267623e-02</td>\n      <td>-2.402420e-02</td>\n      <td>-1.039665e-01</td>\n      <td>6.226814e-02</td>\n      <td>-1.525126e-01</td>\n      <td>-5.164891e-02</td>\n      <td>-8.316110e-02</td>\n      <td>1.128178e-01</td>\n      <td>-9.200374e-03</td>\n      <td>...</td>\n      <td>-1.347085e-02</td>\n      <td>-3.418526e-02</td>\n      <td>-5.398361e-02</td>\n      <td>-1.315678e-02</td>\n      <td>1.424748e-01</td>\n      <td>-1.248806e-01</td>\n      <td>-6.082867e-02</td>\n      <td>-6.022220e-02</td>\n      <td>-6.531348e-02</td>\n      <td>-2.276454e-02</td>\n      <td>-5.982990e-02</td>\n      <td>2.751061e-02</td>\n      <td>-7.488806e-02</td>\n      <td>7.243625e-02</td>\n      <td>4.084039e-02</td>\n      <td>3.523497e-02</td>\n      <td>-8.535268e-03</td>\n      <td>-8.404490e-02</td>\n      <td>-9.020841e-02</td>\n      <td>1.888310e-03</td>\n      <td>9.409576e-02</td>\n      <td>3.146517e-02</td>\n      <td>4.577475e-02</td>\n      <td>-7.307116e-03</td>\n      <td>7.474668e-02</td>\n      <td>1.185304e-01</td>\n      <td>-1.230538e-01</td>\n      <td>5.269330e-02</td>\n      <td>1.667031e-01</td>\n      <td>-1.049565e-03</td>\n      <td>-2.051342e-01</td>\n      <td>1.151222e-01</td>\n      <td>-1.043898e-01</td>\n      <td>-4.517888e-02</td>\n      <td>2.990574e-02</td>\n      <td>-1.788398e-02</td>\n      <td>3.008970e-02</td>\n      <td>-1.001326e-01</td>\n      <td>-6.574410e-02</td>\n      <td>-5.763047e-02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.317100e-01</td>\n      <td>6.758454e-01</td>\n      <td>5.980758e-01</td>\n      <td>4.668744e-01</td>\n      <td>6.240130e-01</td>\n      <td>5.564446e-01</td>\n      <td>6.104375e-01</td>\n      <td>5.901695e-01</td>\n      <td>5.894924e-01</td>\n      <td>6.525343e-01</td>\n      <td>5.907976e-01</td>\n      <td>5.793166e-01</td>\n      <td>4.358331e-01</td>\n      <td>3.439870e-01</td>\n      <td>4.123615e-01</td>\n      <td>3.435088e-01</td>\n      <td>3.989686e-01</td>\n      <td>4.066661e-01</td>\n      <td>4.317881e-01</td>\n      <td>3.536420e-01</td>\n      <td>3.836953e-01</td>\n      <td>4.342104e-01</td>\n      <td>3.615573e-01</td>\n      <td>3.741295e-01</td>\n      <td>3.758779e-01</td>\n      <td>3.356775e-01</td>\n      <td>3.924052e-01</td>\n      <td>4.288088e-01</td>\n      <td>4.257723e-01</td>\n      <td>3.909540e-01</td>\n      <td>4.383029e-01</td>\n      <td>4.399820e-01</td>\n      <td>4.221423e-01</td>\n      <td>3.074795e-01</td>\n      <td>4.768112e-01</td>\n      <td>4.088835e-01</td>\n      <td>3.289584e-01</td>\n      <td>4.179632e-01</td>\n      <td>4.965377e-01</td>\n      <td>4.477446e-01</td>\n      <td>...</td>\n      <td>4.722113e-01</td>\n      <td>4.537573e-01</td>\n      <td>3.802789e-01</td>\n      <td>4.260445e-01</td>\n      <td>4.979216e-01</td>\n      <td>3.629212e-01</td>\n      <td>3.882181e-01</td>\n      <td>3.226990e-01</td>\n      <td>4.285659e-01</td>\n      <td>4.066440e-01</td>\n      <td>4.031240e-01</td>\n      <td>4.755073e-01</td>\n      <td>4.060183e-01</td>\n      <td>4.530141e-01</td>\n      <td>4.780044e-01</td>\n      <td>4.192291e-01</td>\n      <td>4.857568e-01</td>\n      <td>2.854014e-01</td>\n      <td>3.849043e-01</td>\n      <td>4.246213e-01</td>\n      <td>4.712277e-01</td>\n      <td>4.293974e-01</td>\n      <td>4.251828e-01</td>\n      <td>4.622619e-01</td>\n      <td>5.105105e-01</td>\n      <td>4.982904e-01</td>\n      <td>3.183511e-01</td>\n      <td>5.150252e-01</td>\n      <td>4.859609e-01</td>\n      <td>4.736714e-01</td>\n      <td>3.273959e-01</td>\n      <td>4.942984e-01</td>\n      <td>3.940738e-01</td>\n      <td>4.204911e-01</td>\n      <td>5.045541e-01</td>\n      <td>4.385694e-01</td>\n      <td>4.516087e-01</td>\n      <td>4.131805e-01</td>\n      <td>3.595684e-01</td>\n      <td>3.786469e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.057122e+00</td>\n      <td>7.416766e+00</td>\n      <td>8.909792e+00</td>\n      <td>1.765551e+01</td>\n      <td>1.175114e+01</td>\n      <td>1.007897e+01</td>\n      <td>1.198797e+01</td>\n      <td>1.343956e+01</td>\n      <td>1.347767e+01</td>\n      <td>8.964296e+00</td>\n      <td>7.090614e+00</td>\n      <td>1.025687e+01</td>\n      <td>2.321918e+01</td>\n      <td>3.621242e+01</td>\n      <td>2.762232e+01</td>\n      <td>2.775638e+01</td>\n      <td>3.985768e+01</td>\n      <td>2.766484e+01</td>\n      <td>3.177211e+01</td>\n      <td>2.933540e+01</td>\n      <td>4.308266e+01</td>\n      <td>2.056735e+01</td>\n      <td>3.465084e+01</td>\n      <td>6.193636e+01</td>\n      <td>1.649785e+01</td>\n      <td>3.416372e+01</td>\n      <td>2.670132e+01</td>\n      <td>1.576088e+01</td>\n      <td>1.998443e+01</td>\n      <td>2.601625e+01</td>\n      <td>1.838174e+01</td>\n      <td>2.691332e+01</td>\n      <td>3.865847e+01</td>\n      <td>4.366883e+01</td>\n      <td>4.273974e+01</td>\n      <td>2.245118e+01</td>\n      <td>3.417137e+01</td>\n      <td>1.753563e+01</td>\n      <td>2.398130e+01</td>\n      <td>4.057395e+01</td>\n      <td>...</td>\n      <td>1.517978e+01</td>\n      <td>3.237494e+01</td>\n      <td>3.403535e+01</td>\n      <td>3.472804e+01</td>\n      <td>1.003386e+01</td>\n      <td>2.520328e+01</td>\n      <td>2.833136e+01</td>\n      <td>4.855057e+01</td>\n      <td>3.186487e+01</td>\n      <td>3.969876e+01</td>\n      <td>3.049951e+01</td>\n      <td>1.980749e+01</td>\n      <td>2.159619e+01</td>\n      <td>3.638141e+01</td>\n      <td>1.295372e+01</td>\n      <td>2.687900e+01</td>\n      <td>2.056303e+01</td>\n      <td>4.080572e+01</td>\n      <td>4.962670e+01</td>\n      <td>2.031260e+01</td>\n      <td>1.577953e+01</td>\n      <td>2.065479e+01</td>\n      <td>1.312081e+01</td>\n      <td>2.613535e+01</td>\n      <td>2.009278e+01</td>\n      <td>2.834790e+01</td>\n      <td>3.747219e+01</td>\n      <td>2.352090e+01</td>\n      <td>2.405977e+01</td>\n      <td>2.990595e+01</td>\n      <td>2.571906e+01</td>\n      <td>2.590152e+01</td>\n      <td>2.602152e+01</td>\n      <td>1.723191e+01</td>\n      <td>1.610766e+01</td>\n      <td>3.187494e+01</td>\n      <td>1.641892e+01</td>\n      <td>3.445501e+01</td>\n      <td>3.989141e+01</td>\n      <td>2.719132e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Our values are normalised and so we now actually apply PCA\n(https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n# Make an instance of the Model\npca = PCA(.90)\n\n# We fit to only our training set\npca.fit(X_train_std)\n# Print number of components generated\npca.n_components_","execution_count":168,"outputs":[{"output_type":"execute_result","execution_count":168,"data":{"text/plain":"55"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"PCA(.90) means that scikit-learn choose the minimum number of principal components such that 90% of the variance is retained.\n\nIn this case, 90% of the variance amounts to 55 principal components."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_proc = pca.transform(X_train_std)\nX_test_proc = pca.transform(X_test_std)","execution_count":169,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_proc = y_train - min(y_train)\ny_test_proc = y_test - min(y_test)\n# y_train_proc","execution_count":170,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras import Sequential\nfrom tensorflow.python.keras.layers import Dense, Lambda, Dropout\n# from tensorflow.python.keras.initializers import Initializer\nfrom tensorflow.python.keras.utils import to_categorical\nfrom tensorflow.python.keras.callbacks import ReduceLROnPlateau, EarlyStopping","execution_count":171,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_hot = to_categorical(y_train_proc, 90)\ny_test_hot = to_categorical(y_test_proc, 90)","execution_count":172,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_proc.shape)\nprint(y_test_hot.shape)","execution_count":173,"outputs":[{"output_type":"stream","text":"(463715, 55)\n(51630, 90)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(history):\n    epochs = range(1, len(history.history['loss']) + 1)\n    plt.plot(epochs, history.history['mean_absolute_error'], label='train');\n    plt.plot(epochs, history.history['val_mean_absolute_error'], label='val');\n    plt.xlabel('epoch');\n    plt.ylabel('mae');\n    plt.legend();\n    plt.show();","execution_count":174,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Dense(55, input_shape=(55,)))\nmodel1.add(Dense(110))\nmodel1.add(Dense(90, activation='softmax'))","execution_count":175,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=4, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)","execution_count":176,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(optimizer='adam'\n             , loss='categorical_crossentropy'\n             , metrics=['accuracy'])","execution_count":177,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit1 = model1.fit(x=X_train_proc, y=y_train_hot\n          , epochs=5\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_hot)\n          , callbacks=[learning_rate_reduction])","execution_count":178,"outputs":[{"output_type":"stream","text":"Train on 463715 samples, validate on 51630 samples\nEpoch 1/5\n463715/463715 [==============================] - 29s 63us/sample - loss: 3.3505 - acc: 0.0754 - val_loss: 3.8399 - val_acc: 0.0064\nEpoch 2/5\n463715/463715 [==============================] - 27s 59us/sample - loss: 3.3124 - acc: 0.0770 - val_loss: 3.8251 - val_acc: 0.0096\nEpoch 3/5\n463715/463715 [==============================] - 27s 59us/sample - loss: 3.3097 - acc: 0.0769 - val_loss: 3.8261 - val_acc: 0.0065\nEpoch 4/5\n463715/463715 [==============================] - 28s 60us/sample - loss: 3.3078 - acc: 0.0770 - val_loss: 3.8290 - val_acc: 0.0110\nEpoch 5/5\n463715/463715 [==============================] - 27s 58us/sample - loss: 3.3065 - acc: 0.0781 - val_loss: 3.8458 - val_acc: 0.0073\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot(fit1)","execution_count":179,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.summary()","execution_count":180,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_37 (Dense)             (None, 55)                3080      \n_________________________________________________________________\ndense_38 (Dense)             (None, 110)               6160      \n_________________________________________________________________\ndense_39 (Dense)             (None, 90)                9990      \n=================================================================\nTotal params: 19,230\nTrainable params: 19,230\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model1.predict_classes(X_test_proc)","execution_count":181,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.array(y_test_proc))\nprint(preds)\nnp.mean(np.absolute((preds-np.array(y_test_proc))))","execution_count":182,"outputs":[{"output_type":"stream","text":"[80 76 78 ... 79 79 78]\n[84 84 84 ... 82 83 84]\n","name":"stdout"},{"output_type":"execute_result","execution_count":182,"data":{"text/plain":"11.950125895797017"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Dense(55, input_shape=(55,), activation='relu'))\nmodel2.add(Dense(1))","execution_count":187,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction1 = ReduceLROnPlateau(monitor='mean_absolute_error', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)","execution_count":188,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile(optimizer='adam'\n             , loss='mse'\n             , metrics=['mae'])","execution_count":189,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"fit2 = model2.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1])","execution_count":null,"outputs":[{"output_type":"stream","text":"Train on 463715 samples, validate on 51630 samples\nEpoch 1/10\n463715/463715 [==============================] - 23s 49us/sample - loss: 425.4128 - mean_absolute_error: 13.5303 - val_loss: 128.0690 - val_mean_absolute_error: 7.8810\nEpoch 2/10\n463715/463715 [==============================] - 22s 47us/sample - loss: 96.4457 - mean_absolute_error: 7.1979 - val_loss: 119.5640 - val_mean_absolute_error: 7.5935\nEpoch 3/10\n463715/463715 [==============================] - 22s 47us/sample - loss: 93.0017 - mean_absolute_error: 7.0175 - val_loss: 121.5767 - val_mean_absolute_error: 7.6230\nEpoch 4/10\n463715/463715 [==============================] - 23s 49us/sample - loss: 91.6697 - mean_absolute_error: 6.9513 - val_loss: 118.2401 - val_mean_absolute_error: 7.6818\nEpoch 5/10\n463715/463715 [==============================] - 22s 46us/sample - loss: 90.8626 - mean_absolute_error: 6.9179 - val_loss: 120.5156 - val_mean_absolute_error: 7.6266\nEpoch 6/10\n463715/463715 [==============================] - 21s 46us/sample - loss: 90.4073 - mean_absolute_error: 6.8948 - val_loss: 118.0931 - val_mean_absolute_error: 7.6143\nEpoch 7/10\n463715/463715 [==============================] - 21s 46us/sample - loss: 90.0283 - mean_absolute_error: 6.8768 - val_loss: 117.8655 - val_mean_absolute_error: 7.5700\nEpoch 8/10\n463715/463715 [==============================] - 23s 49us/sample - loss: 89.7296 - mean_absolute_error: 6.8631 - val_loss: 119.2925 - val_mean_absolute_error: 7.6388\nEpoch 9/10\n445760/463715 [===========================>..] - ETA: 0s - loss: 89.5090 - mean_absolute_error: 6.8528","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_model_rms = model2.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(fit2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import mean_squared_error, r2_score\n# mean_squared_error(predictions_linearRegr, np.array(y_test_proc))\nes = EarlyStopping(monitor='val_mean_absolute_error', patience=2, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = Sequential()\nmodel3.add(Dense(55, input_shape=(55,), activation='relu'))\nmodel3.add(Dense(110, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(1))\n\nmodel3.compile(optimizer='adam'\n             , loss='mse'\n             , metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit3 = model3.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1, es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_model_rms = model3.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(fit3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"batch size = 64"},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.compile(optimizer='adam'\n             , loss='mse'\n             , metrics=['mae'])\nfit3 = model3.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=128\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1, es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_model_rms = model3.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(fit3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"batch size = 128"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import RMSprop\n# adam = optimizers.Adam()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.compile(optimizer='RMSprop'\n             , loss='mse'\n             , metrics=['mae'])\nfit4 = model3.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1, es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_model_rms = model3.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(np.round_(np.array(preds_model_rms), decimals=-1), np.round_(np.array(y_test_proc), decimals=-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nind = list(range(1920,2030,10))\ndf_heat = pd.DataFrame(cm, index=ind, columns=ind)\nlen(ind)\n# lab = pd.unique(df_heat[0])\nsns.heatmap(df_heat)\ndf_heat\n# df_plot.transpose().corr()\n# cm.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}