{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/housing-prices-in-metropolitan-areas-of-india/Hyderabad.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe() #Descriptive analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.info() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>The dataset has target variable <b>Price</b> and all other are independent variables<br>\nAll independent variables are boolean in nature except Area, No.of.Bedrooms and location also the null values have been represented as 9 in boolean columns, there are no blank or missing data from count</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=data['Area'],y=data['Price'],alpha=0.5)\nplt.ylabel('Rupees of Order 10 Crores')\nplt.xlabel('Area')\nplt.title('outlier points at area > 6000 & price at  16Cr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.boxplot(x=data['Price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['Area'],bins=12)\nplt.title('Most of House areas in dataset are under 2000')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data['Location'].unique()) \n#There are 243 Unique Locations ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Location')['Price'].count().sort_values(ascending=False)[0:10]\n#Top 30 Locations of houses from the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Location_text = \" \".join(data.Location)\nwordcloud = WordCloud(width=720, height=360,collocations=False).generate(text=Location_text)\nplt.figure(figsize=(30,18))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n#wc=WordCloud().generate(text=Location_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Location_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Data Cleaning</h2>\nOutliers and Null Values (9 in this data set)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.impute import SimpleImputer\n#imp=SimpleImputer(missing_values=9,strategy=median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Replacing 9 with Null values\ndata[5:]=data[5:].replace(to_replace=9,value=np.nan)\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data) #Before removing null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data=data[5:].dropna(axis=0)\nlen(data)\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data) #After removing null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#To impute data rather than removing nulls\n\"\"\"\nfrom sklearn.impute import SimpleImputer\nimp=SimpleImputer(missing_values=9,strategy='most_frequent')\ncols=data.columns[5:]\ndata[cols]=imp.fit_transform(data[cols])\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.describe() #the data is clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.to_csv('Hyderabad-vcr.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"q1=data['Price'].quantile(0.25)\nq3=data['Price'].quantile(0.75)\nirp=q3-q1\nlow=q1-1.5*irp\nupr=q3+1.5*irp\n#low=data['Price'].min() #since low was neg\nlow,upr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imp(val):\n    if val>upr:\n        return upr\n    if val<low:\n        return low\n    else:\n        return val\ndata['Price']=data['Price'].apply(imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=data['Area'],y=data['Price'],alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.boxplot(x=data['Price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (25, 20))\ncolormap= sb.diverging_palette(220, 10, as_cmap = True)\nsb.heatmap(data.corr(), annot=True, cmap = colormap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transforming location variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"#For location it has many unique values and has alternate hypothesis so we need to bin the values to fewer groups\n#For that we are going to find the mean Price for each location and sorting them in ascending order\nLocation_table=data.groupby('Location').agg({'Price':'mean'}).sort_values('Price',ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"Location_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Location_table['Loc']=pd.cut(Location_table['Price'],bins=10,labels=['G0',\n                                                          'G1',\n                                                          'G2',\n                                                          'G3',\n                                                          'G4',\n                                                          'G5',\n                                                          'G6',\n                                                          'G7',\n                                                          'G8',\n                                                          'G9'],\n                           include_lowest=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"Location_table['Loc'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Location_table=Location_table.drop(columns=\"Price\")\n#TO merge two tables we use \"merge\" function from pandas using zipcode as identifier\ndata=pd.merge(data,Location_table,\n                left_on='Location',\n                how='left',\n                right_index=True)\ndata.drop(columns='Location',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#now create dummies for Location\ndata=pd.get_dummies(data,columns=['Loc'],drop_first=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.to_csv('Hyderabad-vtcr.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler as ss\n#Scaling the data set\nscalar=ss()\nY=data['Price']\n# Scaling\nX=scalar.fit_transform(data.drop(columns=['Price']))\n# Converting to pandas dataframe for easy manipulation\nX=pd.DataFrame(data=X,columns=data.drop(columns=['Price']).columns)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Checking multicollinearity\nk=X.corr()\nz=[[str(i),str(j)] for i in k.columns for j in k.columns if (k.loc[i,j]>abs(0.5)) & (i!=j)]\nz,len(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Caluclating VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nVIF=pd.Series([vif(X.values,i) for i in range (X.shape[1])],index=X.columns)\nVIF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to remove vif > 5 and each time we remove one column the vif data changes so we remove,check vif,remove\ndef MC_rem(data):\n    VIF=pd.Series([vif(data.values,i) for i in range (data.shape[1])],index=data.columns)\n    if(VIF.max()>5):\n        data.drop(columns=[VIF[VIF==VIF.max()].index[0]],inplace=True)\n        print(VIF[VIF==VIF.max()].index[0],'has been removed from \"X_copy\"')\n        return data\n    else:\n        print('no multicollinearity')\n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"X_copy=X.copy()\nfor i in range(5):\n    X_copy=MC_rem(X_copy)\nX=X_copy\n###After Removing collinearity\nVIF=pd.Series([vif(X_copy.values,i) for i in range (X_copy.shape[1])],index=X_copy.columns)\nVIF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=101)\n\nX_train.shape,X_test.shape,Y_train.shape,Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \nlr=LinearRegression(normalize=True)\n# If norm=true intercept=0\nlr.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=lr.predict(X_test)\nlr.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals=predictions-Y_test\nresidual_table=pd.DataFrame({'residual':residuals,\n                            'prediction':predictions})\nresidual_table=residual_table.sort_values(by='prediction')\nz=[i for i in range(int(residual_table['prediction'].max()))]\nk=[0 for i in range(int(residual_table['prediction'].max()))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi=130,figsize=(17,7))\nplt.scatter(residual_table['prediction'],residual_table['residual'],color='red',s=25)\nplt.plot(z,k,color='green',linewidth=3,label='Regression line')\nplt.ylim(-800000,800000)\nplt.xlabel('Fitted points(ordered by predictions)')\nplt.ylabel('Residuals')\nplt.title('residual plot')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the distribution of errors\nplt.figure(dpi=100,figsize=(10,7))\nplt.hist(residual_table['residual'],color='red',bins=200)\nplt.xlabel('Residuals')\nplt.ylabel('frequency')\nplt.title('Distribution of residuals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefftab=pd.DataFrame({\n    'column':X_train.columns,\n    'coeff':lr.coef_\n})\ncoefftab=coefftab.sort_values(by='coeff')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6),dpi=120)\nx=coefftab['column']\ny=coefftab['coeff']\nplt.barh(x,y)\nplt.xlabel('Coefficients')\nplt.ylabel('variables')\nplt.title('Normalised Coefficient Plot')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}