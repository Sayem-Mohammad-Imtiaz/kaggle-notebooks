{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-28T21:04:41.933884Z","iopub.execute_input":"2021-05-28T21:04:41.934349Z","iopub.status.idle":"2021-05-28T21:04:41.94986Z","shell.execute_reply.started":"2021-05-28T21:04:41.934309Z","shell.execute_reply":"2021-05-28T21:04:41.948553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset was published by:\nP. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/student-performance-data-set/student-por.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:44.257385Z","iopub.execute_input":"2021-05-28T21:04:44.257774Z","iopub.status.idle":"2021-05-28T21:04:44.29472Z","shell.execute_reply.started":"2021-05-28T21:04:44.257741Z","shell.execute_reply":"2021-05-28T21:04:44.293934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:46.101882Z","iopub.execute_input":"2021-05-28T21:04:46.102411Z","iopub.status.idle":"2021-05-28T21:04:46.10895Z","shell.execute_reply.started":"2021-05-28T21:04:46.102377Z","shell.execute_reply":"2021-05-28T21:04:46.108138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:46.437767Z","iopub.execute_input":"2021-05-28T21:04:46.438285Z","iopub.status.idle":"2021-05-28T21:04:46.443598Z","shell.execute_reply.started":"2021-05-28T21:04:46.438251Z","shell.execute_reply":"2021-05-28T21:04:46.442794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attribute Information:\n\n\n* 1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n* 2 sex - student's sex (binary: 'F' - female or 'M' - male)\n* 3 age - student's age (numeric: from 15 to 22)\n* 4 address - student's home address type (binary: 'U' - urban or 'R' - rural)\n* 5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n* 6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n* 7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n* 8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n* 9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n* 10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n* 11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n* 12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n* 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n* 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n* 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n* 16 schoolsup - extra educational support (binary: yes or no)\n* 17 famsup - family educational support (binary: yes or no)\n* 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n* 19 activities - extra-curricular activities (binary: yes or no)\n* 20 nursery - attended nursery school (binary: yes or no)\n* 21 higher - wants to take higher education (binary: yes or no)\n* 22 internet - Internet access at home (binary: yes or no)\n* 23 romantic - with a romantic relationship (binary: yes or no)\n* 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n* 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n* 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n* 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n* 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n* 29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n* 30 absences - number of school absences (numeric: from 0 to 93)\n\n \n* 31 G1 - first period grade (numeric: from 0 to 20)\n* 31 G2 - second period grade (numeric: from 0 to 20)\n* 32 G3 - final grade (numeric: from 0 to 20, output target)\n\n","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:48.806043Z","iopub.execute_input":"2021-05-28T21:04:48.806562Z","iopub.status.idle":"2021-05-28T21:04:48.824111Z","shell.execute_reply.started":"2021-05-28T21:04:48.806529Z","shell.execute_reply":"2021-05-28T21:04:48.823173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values, that makes the preparation of this dataset much easier. But still there are a lot of features that have to be properly encoded. Before I start to learn more about this dataset I'm going to put the test set aside.","metadata":{}},{"cell_type":"code","source":"data[\"school\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:50.850752Z","iopub.execute_input":"2021-05-28T21:04:50.851145Z","iopub.status.idle":"2021-05-28T21:04:50.859832Z","shell.execute_reply.started":"2021-05-28T21:04:50.851109Z","shell.execute_reply":"2021-05-28T21:04:50.858329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a small dataset, so I think that random sampling is not that good idea. I will use 'school' feature to separate the dataset into two strata and perform stratified sampling.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\nfor train_index, test_index in split.split(data, data[\"school\"]):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:52.86485Z","iopub.execute_input":"2021-05-28T21:04:52.865299Z","iopub.status.idle":"2021-05-28T21:04:52.87505Z","shell.execute_reply.started":"2021-05-28T21:04:52.865259Z","shell.execute_reply":"2021-05-28T21:04:52.874127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see below the proportions of students from both schools are almost identical in the full set and in the test set. That means that our sample is representative.","metadata":{}},{"cell_type":"code","source":"strat_test_set[\"school\"].value_counts()/len(strat_test_set)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:54.205226Z","iopub.execute_input":"2021-05-28T21:04:54.20579Z","iopub.status.idle":"2021-05-28T21:04:54.216612Z","shell.execute_reply.started":"2021-05-28T21:04:54.20574Z","shell.execute_reply":"2021-05-28T21:04:54.215598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"school\"].value_counts()/len(data)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:55.562843Z","iopub.execute_input":"2021-05-28T21:04:55.563218Z","iopub.status.idle":"2021-05-28T21:04:55.571566Z","shell.execute_reply.started":"2021-05-28T21:04:55.563188Z","shell.execute_reply":"2021-05-28T21:04:55.570613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I am going to work on the training set.","metadata":{}},{"cell_type":"code","source":"train = strat_train_set.copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:57.507717Z","iopub.execute_input":"2021-05-28T21:04:57.508143Z","iopub.status.idle":"2021-05-28T21:04:57.513534Z","shell.execute_reply.started":"2021-05-28T21:04:57.508107Z","shell.execute_reply":"2021-05-28T21:04:57.512248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:57.90359Z","iopub.execute_input":"2021-05-28T21:04:57.904049Z","iopub.status.idle":"2021-05-28T21:04:57.932409Z","shell.execute_reply.started":"2021-05-28T21:04:57.904Z","shell.execute_reply":"2021-05-28T21:04:57.93098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:59.173815Z","iopub.execute_input":"2021-05-28T21:04:59.174228Z","iopub.status.idle":"2021-05-28T21:04:59.242417Z","shell.execute_reply.started":"2021-05-28T21:04:59.174191Z","shell.execute_reply":"2021-05-28T21:04:59.241423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime','failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n\ntrain[numeric].hist(figsize=(20,17))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:04:59.611942Z","iopub.execute_input":"2021-05-28T21:04:59.612371Z","iopub.status.idle":"2021-05-28T21:05:02.182032Z","shell.execute_reply.started":"2021-05-28T21:04:59.612332Z","shell.execute_reply":"2021-05-28T21:05:02.180801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most so called numerical variables in this dataset are actually ordinal. Realy numerical are only 'age', 'absences', 'G1', 'G2' and 'G3'. If I'm going to use Gradient Descent to find parameters, than I have to standardize numerical values to make it work faster.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,15))  \nsns.heatmap(train.corr(), annot=True, vmin=-1, ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:06.602738Z","iopub.execute_input":"2021-05-28T21:05:06.603111Z","iopub.status.idle":"2021-05-28T21:05:08.048303Z","shell.execute_reply.started":"2021-05-28T21:05:06.603075Z","shell.execute_reply":"2021-05-28T21:05:08.046996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"G1 and G2 are highly correlated with G3. Ideally we should be able to predict G3 without G1 and G2, so I guess I have to test combinations of features to find more information, so that I don't need G1 and G2 that much to predict G3.\n\nAt first I want to encode all non-numerical features and look if there is more correlation. ","metadata":{}},{"cell_type":"code","source":"categorical = []\n\nfor column in train.columns:\n    if column not in numeric:\n        categorical.append(column)\n        \ncategorical","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:22.504026Z","iopub.execute_input":"2021-05-28T21:05:22.504529Z","iopub.status.idle":"2021-05-28T21:05:22.511113Z","shell.execute_reply.started":"2021-05-28T21:05:22.504495Z","shell.execute_reply":"2021-05-28T21:05:22.510343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncat = ColumnTransformer([('check', OrdinalEncoder(), categorical)],remainder='passthrough')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:24.727452Z","iopub.execute_input":"2021-05-28T21:05:24.728126Z","iopub.status.idle":"2021-05-28T21:05:24.733376Z","shell.execute_reply.started":"2021-05-28T21:05:24.728073Z","shell.execute_reply":"2021-05-28T21:05:24.732494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tr = cat.fit_transform(train)\ntrain_tr = pd.DataFrame(train_tr, columns=(categorical + numeric))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:26.311352Z","iopub.execute_input":"2021-05-28T21:05:26.311935Z","iopub.status.idle":"2021-05-28T21:05:26.332417Z","shell.execute_reply.started":"2021-05-28T21:05:26.311884Z","shell.execute_reply":"2021-05-28T21:05:26.331728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(25, 25))  \nsns.heatmap(train_tr.corr(), annot=True, vmin=-1, ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:26.801964Z","iopub.execute_input":"2021-05-28T21:05:26.802587Z","iopub.status.idle":"2021-05-28T21:05:33.273139Z","shell.execute_reply.started":"2021-05-28T21:05:26.802519Z","shell.execute_reply":"2021-05-28T21:05:33.271568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can get additional information from our categorical variables. Most correlation G3 has with interest in higher education, school, sex, address, internet access and mother's job. I will prepare a total pipeline to encode nominal varables and binary variables separately with OneHotEncoder and OrdinalEncoder. Otherwise we introduce some order to nominal categories, which can lead to mistakes of our model.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:44.045047Z","iopub.execute_input":"2021-05-28T21:05:44.045406Z","iopub.status.idle":"2021-05-28T21:05:44.050553Z","shell.execute_reply.started":"2021-05-28T21:05:44.045375Z","shell.execute_reply":"2021-05-28T21:05:44.049373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nominal = ['Mjob', 'Fjob', 'reason', 'guardian']\nbinary = []\n\nfor cat in categorical:\n    if cat not in nominal:\n        binary.append(cat)\nbinary","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:45.608801Z","iopub.execute_input":"2021-05-28T21:05:45.609164Z","iopub.status.idle":"2021-05-28T21:05:45.618393Z","shell.execute_reply.started":"2021-05-28T21:05:45.609132Z","shell.execute_reply":"2021-05-28T21:05:45.616939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_scale = ['age', 'absences', 'G1', 'G2']\n\ntrain_x = train.drop(['G3'], axis=1)\ntrain_labels = train['G3'].copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:45.995015Z","iopub.execute_input":"2021-05-28T21:05:45.995425Z","iopub.status.idle":"2021-05-28T21:05:46.002675Z","shell.execute_reply.started":"2021-05-28T21:05:45.995383Z","shell.execute_reply":"2021-05-28T21:05:46.001427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The full pipeline includes:\n* OrdinalEncoder to encode binary categorical variables\n* OneHotEncoder to encode nominal categorical variables\n* StandardScaler to standardize numerical variables, not encoded ordinal variables though","metadata":{}},{"cell_type":"code","source":"full_pipeline = ColumnTransformer([(\"bin\", OrdinalEncoder(), binary),\n                                   (\"cat\", OneHotEncoder(), nominal),\n                                  (\"num\", StandardScaler(), to_scale)], remainder='passthrough')\n\ntrain_encoded = full_pipeline.fit_transform(train_x)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:47.837411Z","iopub.execute_input":"2021-05-28T21:05:47.837908Z","iopub.status.idle":"2021-05-28T21:05:47.861708Z","shell.execute_reply.started":"2021-05-28T21:05:47.837875Z","shell.execute_reply":"2021-05-28T21:05:47.860966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_enc = pd.DataFrame(train_encoded)\ntrain_enc.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:49.115143Z","iopub.execute_input":"2021-05-28T21:05:49.115646Z","iopub.status.idle":"2021-05-28T21:05:49.245378Z","shell.execute_reply.started":"2021-05-28T21:05:49.115612Z","shell.execute_reply":"2021-05-28T21:05:49.244612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to compare different approaches to find parameters that minimize the cost function. The first approach is normal equation. We calculate the best possible vector of parameters directly for the global minimum (only minimum) of the cost function.\n","metadata":{}},{"cell_type":"code","source":"X = train_encoded\nm = X.shape[0]\n\nX_b = np.c_[np.ones((m, 1)), X]\nX_b","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:05:55.794554Z","iopub.execute_input":"2021-05-28T21:05:55.795281Z","iopub.status.idle":"2021-05-28T21:05:55.80492Z","shell.execute_reply.started":"2021-05-28T21:05:55.795214Z","shell.execute_reply":"2021-05-28T21:05:55.803604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_labels\n#best_theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\nbest_theta = np.linalg.pinv(X_b).dot(y)  #used pseudoinverse instead of inverse\nbest_theta","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:00.282801Z","iopub.execute_input":"2021-05-28T21:06:00.283143Z","iopub.status.idle":"2021-05-28T21:06:00.303056Z","shell.execute_reply.started":"2021-05-28T21:06:00.283114Z","shell.execute_reply":"2021-05-28T21:06:00.301969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make some predictions and look how good such a simple model can be.","metadata":{}},{"cell_type":"code","source":"test = strat_test_set\n\ntest_x = train.drop(['G3'], axis=1)\ntest_labels = train['G3'].copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:04.501242Z","iopub.execute_input":"2021-05-28T21:06:04.501574Z","iopub.status.idle":"2021-05-28T21:06:04.5085Z","shell.execute_reply.started":"2021-05-28T21:06:04.501544Z","shell.execute_reply":"2021-05-28T21:06:04.507281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_encoded = full_pipeline.transform(test_x)\ntest_encoded","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:05.615527Z","iopub.execute_input":"2021-05-28T21:06:05.61624Z","iopub.status.idle":"2021-05-28T21:06:05.640883Z","shell.execute_reply.started":"2021-05-28T21:06:05.616199Z","shell.execute_reply":"2021-05-28T21:06:05.640071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bT = np.c_[np.ones((m, 1)), test_encoded]\nX_bT","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:07.76023Z","iopub.execute_input":"2021-05-28T21:06:07.760842Z","iopub.status.idle":"2021-05-28T21:06:07.768941Z","shell.execute_reply.started":"2021-05-28T21:06:07.760788Z","shell.execute_reply":"2021-05-28T21:06:07.767545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = X_bT.dot(best_theta)\ny_predict","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:08.962235Z","iopub.execute_input":"2021-05-28T21:06:08.962885Z","iopub.status.idle":"2021-05-28T21:06:08.993344Z","shell.execute_reply.started":"2021-05-28T21:06:08.962834Z","shell.execute_reply":"2021-05-28T21:06:08.99206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(test_labels, y_predict)\nrmse = np.sqrt(mse)\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:12.6946Z","iopub.execute_input":"2021-05-28T21:06:12.695002Z","iopub.status.idle":"2021-05-28T21:06:12.703436Z","shell.execute_reply.started":"2021-05-28T21:06:12.694964Z","shell.execute_reply":"2021-05-28T21:06:12.702281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next I've used LinearRegression model from sklearn, it implements SVD","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:16.209338Z","iopub.execute_input":"2021-05-28T21:06:16.209783Z","iopub.status.idle":"2021-05-28T21:06:16.214356Z","shell.execute_reply.started":"2021-05-28T21:06:16.209743Z","shell.execute_reply":"2021-05-28T21:06:16.213301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin = LinearRegression()\nlin.fit(train_encoded, train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:17.203648Z","iopub.execute_input":"2021-05-28T21:06:17.20434Z","iopub.status.idle":"2021-05-28T21:06:17.232793Z","shell.execute_reply.started":"2021-05-28T21:06:17.204279Z","shell.execute_reply":"2021-05-28T21:06:17.23122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin.intercept_, lin.coef_","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:18.433531Z","iopub.execute_input":"2021-05-28T21:06:18.433969Z","iopub.status.idle":"2021-05-28T21:06:18.441894Z","shell.execute_reply.started":"2021-05-28T21:06:18.433927Z","shell.execute_reply":"2021-05-28T21:06:18.440979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = lin.predict(test_encoded)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:21.069997Z","iopub.execute_input":"2021-05-28T21:06:21.07058Z","iopub.status.idle":"2021-05-28T21:06:21.082264Z","shell.execute_reply.started":"2021-05-28T21:06:21.070544Z","shell.execute_reply":"2021-05-28T21:06:21.081237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_lin = mean_squared_error(test_labels, predictions)\nrmse_lin = np.sqrt(mse_lin)\nrmse_lin","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:22.321025Z","iopub.execute_input":"2021-05-28T21:06:22.32137Z","iopub.status.idle":"2021-05-28T21:06:22.328369Z","shell.execute_reply.started":"2021-05-28T21:06:22.321339Z","shell.execute_reply":"2021-05-28T21:06:22.327221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RMSE of both models is almost identical. Now I want to try out the Batch Gradient Descent. It's result should be the same as the previous two.","metadata":{}},{"cell_type":"code","source":"alpha = 0.01 # learning rate\nn_iterations = 20000\nn = X_b.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:46.390478Z","iopub.execute_input":"2021-05-28T21:06:46.390921Z","iopub.status.idle":"2021-05-28T21:06:46.395504Z","shell.execute_reply.started":"2021-05-28T21:06:46.390886Z","shell.execute_reply":"2021-05-28T21:06:46.39417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y.to_numpy().reshape(519,1)\n\ndef gradient_descent(alpha, x, y, numIterations):\n    m = x.shape[0] # number of samples\n    theta = np.random.randn(n,1)\n    x_transpose = x.transpose()\n    for iter in range(0, numIterations):\n        hypothesis = np.dot(x, theta)\n        loss = hypothesis - y\n        \n        gradient = np.dot(x_transpose, loss) / m         \n        theta = theta - alpha * gradient  # update\n    return theta","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:47.099903Z","iopub.execute_input":"2021-05-28T21:06:47.100375Z","iopub.status.idle":"2021-05-28T21:06:47.108397Z","shell.execute_reply.started":"2021-05-28T21:06:47.100338Z","shell.execute_reply":"2021-05-28T21:06:47.107424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theta = gradient_descent(alpha, X_b, y, n_iterations)\ntheta","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:48.040318Z","iopub.execute_input":"2021-05-28T21:06:48.040999Z","iopub.status.idle":"2021-05-28T21:06:49.240891Z","shell.execute_reply.started":"2021-05-28T21:06:48.040946Z","shell.execute_reply":"2021-05-28T21:06:49.239855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = X_bT.dot(theta)\n\nmse_gd = mean_squared_error(test_labels, y_predict)\nrmse_gd = np.sqrt(mse_gd)\nrmse_gd","metadata":{"execution":{"iopub.status.busy":"2021-05-28T21:06:51.234562Z","iopub.execute_input":"2021-05-28T21:06:51.234937Z","iopub.status.idle":"2021-05-28T21:06:51.244287Z","shell.execute_reply.started":"2021-05-28T21:06:51.234902Z","shell.execute_reply":"2021-05-28T21:06:51.243224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In case of Linear Regression all three approaches give us the same result. Even though the LinearRegression model from sklearn is a lot easier to use.","metadata":{}},{"cell_type":"markdown","source":"I hope this notebook can be interesting to anyone, I'm happy to hear how I could improve this project and get more interesting insights:)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}