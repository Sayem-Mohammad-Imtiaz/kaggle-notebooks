{"cells":[{"metadata":{},"cell_type":"markdown","source":"AI@Penn Venture Fellows Airbnb Exploratory Data Analysis\nBy - Michael O'Farrell\n\nWe'll begin this analysis of the Airbnb by first importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Library imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport geopandas as gpd\nimport geopy\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport tqdm\nfrom tqdm._tqdm_notebook import tqdm_notebook\nfrom numpy import median\nimport seaborn as sns \nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nimport keras\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport os\nimport collections\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import random\nfrom numpy import median\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll then read in and then examine the head of the airbnb data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/us-airbnb-open-data/AB_US_2020.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Examining the missing info, a large part of the missing data seems to stem from neighborhood groups, and features related to reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick Look at the missing values indicate a vast majority of the missing data stems\n# from neighborhood groups, last_reviews, and reviews_per_month\nprint(data.isna().sum())\nprint(data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at our more quantitative features, we'll drop the id feature, and get a good sense of the standard statistical measurements for these features\n\nThe incredibly large maximum values compared to the relatively small upper quartile values and large standard devitations suggest that this data contains many outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = data.select_dtypes(include = ['int64', 'float64'])\nnonnumeric_features = data.select_dtypes(include = ['object'])\nnumeric_features = numeric_features.drop(['id'], axis = 1);\ndata = data.drop(['id'], axis = 1)\nnumeric_features.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll first deal with some of the missing data. Looking at how the number of reviews, latest reviews, and reviews per month relate to each other when one of them is missing. \n\nHere we see that if the number of reviews being zero leads to the other two being NaN, so we'll fix this by fixing the reviews per month feature to be zero when this occurs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# If the number of reviews were 0, the last review would be NaN\nprint(data[data['number_of_reviews'] == 0 & data['last_review'].isna()].iloc[:,11:13].shape)\nprint(data[data['last_review'].isna()].iloc[:,12:13].shape)\nprint(data[data['number_of_reviews'] == 0].iloc[:,11:12].shape)\n\n# Investigating if number_of_reviews = 0 leads to reviews_per_month also being NaN\nprint(data[data['reviews_per_month'].isna()].iloc[:,11:13].shape)\ndata[data['reviews_per_month'].isna()].iloc[:,13] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['number_of_reviews']==0 & data['last_review'].isna(), 'reviews_per_month'] = 0\ndata[data['number_of_reviews']==0 & data['last_review'].isna()].iloc[:,11:14]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before filtering out the many outliers, I first wanted to get a good hold on where our airbnb listings are in this dataset. Found the neighborhood feature to be of mixed importance when trying to figure that out. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us know what locations we are dealing with\ndata['neighbourhood'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, using the latitude and longitude coordinates, I outputted on a map of the US where the Airbnbs were located, where the size of the dot was proportional to its availability throughout the year. I did this using python's BaseMap library."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (12,9))\nm = Basemap(width=12000000, height = 9000000, projection = 'lcc', lat_1 = 45., lat_2 = 55, lat_0 = 50, lon_0 = -107.)\n\nm.shadedrelief()\nm.drawstates(color = 'black')\nm.drawcountries(color = 'black')\nm.scatter(data['longitude'].tolist(), data['latitude'].tolist(), latlon=True, c = ['red'], s = data['availability_365']/4,\n         marker = 'o', alpha = .4, edgecolor = 'k', zorder = 2)\n\nplt.title('Airbnb Listing Density in US', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wanted to get a good idea of what states most of the airbnbs are in, because oftentimes the cities would just also be county names or in the case of a rhode island, a state name. \nTo get a uniform sense of where the airbnb listings are, I decided to reverse geocode the latitude and longitude coordinates into state information using Nominatim. \nAfter getting the state information, I then feature engineered the state feature into the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reverse Geolocator for the N cities, and then autofill for the rest\n# Allows for us to feature engineer a states column\nlocator = Nominatim(user_agent=\"myGeocoder\")\nstates = {}\nfor x in data['city'].unique():\n    house = data[data['city'] == x].iloc[0]\n    geom = str(house['latitude']) + \",\" + str(house['longitude'])\n    location = locator.reverse(geom)\n    state = location.raw\n    states[x] = state['address']['state']\ndata['state'] = data['city'].map(states)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here I was able to visualize how many Airbnb listings were in each 'city' and each state. "},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x = 'city', data = data, kind = 'count', palette = 'crest', label = 'big', aspect = 3, order = data['city'].value_counts().index)\nplt.title(\"Number of Listings in Each City\")\nplt.xticks(rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x = 'state', data = data, palette = \"crest\", kind = 'count', aspect= 2, label = 'big', orient = 'h', order = data['state'].value_counts().index)\nplt.title(\"Number of Listings in Each State\")\nplt.xticks(rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I then turned to focus on the outlier data.\nI did this by first visualizing the density curves for the numeric features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(9, 9))\ngs = f.add_gridspec(3, 3)\nindex = 0\nfor i in range(3):\n    for j in range(3):\n        ax = f.add_subplot(gs[i, j])\n        sns.kdeplot(data = data, x  = numeric_features.columns[index], ax= ax).set_title(numeric_features.columns[index] + \" density\")\n        index += 1\nplt.subplots_adjust(wspace=1, hspace=1)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For filtering out price, I only considered prices that were 1.5 the IQR from the lower and upper quartile prices. \nWhen attempting to do this for the other features, I was only left with 67 listings in the end. Thus, for calculated host listings, reviews per month, and number of reviews, I looked at the density curves above and filtered below the lower quartile on the x axis. \nI also filtered minimum nights to be less than 2 months. "},{"metadata":{"trusted":true},"cell_type":"code","source":"iqr = data['price'].quantile(.75)-data['price'].quantile(.25)\nlower = data['price'].quantile(.25)-1.5*(iqr)\nupper = data['price'].quantile(.75)+1.5*(iqr)\nwithout_outliers = data[data['price'].between(lower,upper, inclusive = True)]\nwithout_outliers = without_outliers[without_outliers['calculated_host_listings_count'] < 125]\nwithout_outliers = without_outliers[without_outliers['reviews_per_month'] < 10]\nwithout_outliers = without_outliers[without_outliers['number_of_reviews'] < 250]\nwithout_outliers = without_outliers[without_outliers['minimum_nights'] < 60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Revisualizing this dataset without outliers, we see multimodal distributions for minimum nights and availability 365. This makes sense, because the peaks of the minimum nights peaks close to a single digit value and peaks again around 30, suggesting daily/weekly rental airbnbs and monthly ones. \n\nnumber of reviews, reviews per month, and calculated host listings all follow power-law distibutions by the looks of it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(9, 9))\ngs = f.add_gridspec(3, 3)\nindex = 0\nfor i in range(3):\n    for j in range(3):\n        ax = f.add_subplot(gs[i, j])\n        sns.kdeplot(data = without_outliers, x  = numeric_features.columns[index], ax= ax).set_title(numeric_features.columns[index] + \" density\")\n        index += 1\nplt.subplots_adjust(wspace=1, hspace=1)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this filtered dataset, I now begin to look at median numerical feature data in each city and state, to get a better sense of how the airbnb listings vary by state. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\norder = without_outliers.groupby(['city'])['price'].aggregate(np.median).reset_index().sort_values('price', ascending = False)\nsns.catplot(x = 'price', y = 'city', kind = 'bar', order = order['city'], palette = \"crest\", orient = 'h', data =without_outliers, estimator = median)\nplt.title(\"Median Listing Price in each city\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\norder = without_outliers.groupby(['state'])['price'].aggregate(np.median).reset_index().sort_values('price', ascending = False)\nsns.catplot(x = 'state', y = 'price', kind = 'bar', aspect = 2, order = order['state'], palette = \"crest\", orient = 'v', data =without_outliers, estimator = median)\nplt.title(\"Median Listing Price in each State\")\nplt.xticks(rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I prepare a correlation matrix for the numeric features of our filtered dataset. Unfortunately, there are not strong predictors in the numeric features for price. "},{"metadata":{"trusted":true},"cell_type":"code","source":"order = without_outliers.groupby(['state'])['availability_365'].aggregate(np.mean).reset_index().sort_values('availability_365', ascending = False)\nsns.catplot(x = 'availability_365', y = 'state', kind = 'bar', order = order['state'], palette = \"crest\", orient = 'h', data =without_outliers)\nplt.title(\"Mean Availability in each state\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order = without_outliers.groupby(['state'])['number_of_reviews'].aggregate(np.median).reset_index().sort_values('number_of_reviews', ascending = False)\nsns.catplot(x = 'number_of_reviews', y = 'state', kind = 'bar', order = order['state'], palette = \"crest\", orient = 'h', data =without_outliers, estimator = median)\nplt.title(\"Median Review Count in each state\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order = without_outliers.groupby(['state'])['availability_365'].aggregate(np.median).reset_index().sort_values('availability_365', ascending = False)\nsns.catplot(x = 'availability_365', y = 'state', kind = 'bar', order = order['state'], palette = \"crest\", orient = 'h', data =without_outliers, estimator = median)\nplt.title(\"Median Availability in each state\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(without_outliers, col = 'room_type')\ng.map(sns.scatterplot, 'number_of_reviews', 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(without_outliers.corr(), annot=True, fmt = \".2f\", cmap = \"crest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I then wondered if the name of the airbnb listing (which also usually entails a descriuption of the place) could help us predict price. Using the WordCloud library and an image mask of a house, I was able to generate a house-shaped word cloud "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure the house image mask\nhouse_mask = np.array(Image.open(\"../input/image-of-house/45180.png\"))\nprint(house_mask.shape)\ndef transform_format(val):\n    if val == 0:\n        return 255\n    elif val == 247:\n        return 255\n    else:\n        return val\ntransformed_house_mask = np.ndarray((house_mask.shape[0],house_mask.shape[1]), np.int32)\nfor i in range(len(house_mask)):\n    transformed_house_mask[i] = list(map(transform_format, house_mask[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the word cloud\ncontents = \" \".join(name for name in without_outliers[without_outliers['name'].notna()]['name'].values.astype(str))\nstopwords = set(STOPWORDS)\nwc = WordCloud(background_color = \"white\", stopwords=stopwords, min_font_size = 8, max_words=100, mask = transformed_house_mask,contour_width=3, contour_color='black').generate(contents)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"Words Associated with Airbnb Names\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Descriptions like 'beautiful', 'downtown', and 'beach' could be associated with higher-priced Airbnbs. \nWe will now begin to build the model for predicting airbnb housing prices. First, I do want to quantify the images n the word cloud though. \nWe'll begin by dropping all listings with no description"},{"metadata":{"trusted":true},"cell_type":"code","source":"without_outliers = without_outliers[without_outliers['name'].notna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll then split out data into training and test sets, converting room types and state features into one-hot, numeric representations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Data\nfeatures = ['name', 'calculated_host_listings_count', 'room_type', 'state', 'minimum_nights','availability_365']\nX = without_outliers[features]\nX = pd.get_dummies(X, columns = ['room_type', 'state'])\nX_train, X_test, y_train, y_test = train_test_split(X, without_outliers['price'], test_size = .2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now convert the names of the airbnbs into NN embeddinds, and then pad them so that they are all the same size."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X['name'].values)\nname_train_embed = tokenizer.texts_to_sequences(texts = X_train['name'])\nname_test_embed = tokenizer.texts_to_sequences(texts = X_test['name'])\ntrain_embed = keras.preprocessing.sequence.pad_sequences(name_train_embed, maxlen=120)\ntest_embed = keras.preprocessing.sequence.pad_sequences(name_test_embed, maxlen=120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now build a sequence model to produce a numeric output based on the description of the airbnb. The embeddings will be put through an embedding layer, flattened, then run through a dense layer. "},{"metadata":{"trusted":true},"cell_type":"code","source":"layers = keras.layers\ndeep_inputs = layers.Input(shape=(120,))\nembedding = layers.Embedding(40000, 10, input_length = 120)(deep_inputs)\nembedding = layers.Flatten()(embedding)\nembedding = layers.Dense(units = 15, activation = 'relu')(embedding)\nembed_out = layers.Dense(units = 1,activation = 'relu')(embedding)\ndeep_model = keras.Model(inputs=deep_inputs, outputs=embed_out)\ndeep_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\ndeep_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_model.fit(train_embed, y_train, epochs = 10, verbose = 1)\n_, accuracy = deep_model.evaluate(test_embed, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll now evaluate the MSE of this sequence model."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_test = deep_model.predict(test_embed)\nrms = mean_squared_error(y_test, predictions_test, squared = False)\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll now feature engineer an 'output' feature, which is a numeric representation to how the name of an airbnb could relate to its price. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['output'] = deep_model.predict(train_embed)\nX_test['output'] = predictions_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll now feature scale minimum_nights, availability_365, the host_listings count, and the output feature, and then we'll drop the name, keeping only numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train.drop(columns = ['name'])\ncol_names = ['availability_365','minimum_nights',\n       'calculated_host_listings_count', 'output']\nscaled_X_train = X_train.copy()\nscaled_X_test = X_test.copy()\n\nfeatures = scaled_X_train[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\n\nfeatures2 = scaled_X_test[col_names]\nscaler2 = StandardScaler().fit(features2.values)\nfeatures2 = scaler2.transform(features2.values)\n\nscaled_X_train[col_names] = features\nscaled_X_test[col_names] = features2\n\nscaled_X_train = scaled_X_train.drop(columns = 'name')\nscaled_X_test = scaled_X_test.drop(columns = 'name')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll now evaluate which model performs the best\nHere we'll compared an SGD regressor with a random forest regressor. Was going to do an SVR, but it did not scale well with the data. \nComparing the RMSE, the  RF regressor is far betterthan the SGD regressor. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr = RandomForestRegressor(max_depth = 5)\nrfr.fit(scaled_X_train,y_train)\nprint(\"Random Forest regressor RMSE: \" + str(mean_squared_error(y_test, rfr.predict(scaled_X_test), squared = False)))\n\nsgd = SGDRegressor(max_iter=2000, tol=1e-4)\nsgd.fit(scaled_X_train, y_train)\nprint(\"SGD Regressor RMSE: \" +  str(mean_squared_error(y_test, sgd.predict(scaled_X_test), squared = False)))\n\nlasso = Lasso()\nlasso.fit(scaled_X_train, y_train)\nprint(\"Lasso RMSE: \" + str(mean_squared_error(y_test, lasso.predict(scaled_X_test), squared = False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}