{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf=pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.iloc[:,[0,1]]\ndf.columns=['labels','text']\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(df.labels)\nplt.title('Frequency of Both labels ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['labels']=='ham'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['labels']=='spam'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df['text']\nY=df.labels\nX,Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the labels\n#It is not a case of MultiClass classification\n# we have to perform Binary classification\n# So , LabelEncoder is a better option\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nY=le.fit_transform(Y)\nY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I have fixed the number_of_columns=1\nY=Y.reshape(-1,1)\nY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords   #remove stopwords\nfrom nltk.stem.porter import PorterStemmer   #stemming\nimport re\n#Different models for converting text to vector\nfrom sklearn.feature_extraction.text import CountVectorizer #BOW\nfrom sklearn.feature_extraction.text import TfidfVectorizer    # Tf-idf\nfrom gensim.models import Word2Vec   # word2vec method\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"port_stem=PorterStemmer()\ncorpus=[]\nfor i in range(len(df['text'])):\n    text_1=re.sub('[^a-zA-Z]',\" \",df['text'][i])\n    text_1=text_1.lower()\n    text_1=text_1.split()\n    text_1=[port_stem.stem(word) for word in text_1 if word not in stopwords.words('english')]\n    text_1=' '.join(text_1)\n    corpus.append(text_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(corpus),len(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxtrain,xval,ytrain,yval=train_test_split(corpus,Y,test_size=0.2,random_state=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(xtrain),len(ytrain),len(xval),len(yval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndocuments=[text.split() for text in xtrain]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(documents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nw2v_model = gensim.models.Word2Vec(size=300, \n                                            window=3, \n                                            min_count=5, \n                                            workers=8)\n# min_count (int, optional) – Ignores all words with total frequency lower than this.\n# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n# window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n# vector_size (int, optional) – Dimensionality of the word vectors.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.build_vocab(documents)\n# Build vocabulary from a dictionary of word frequencies.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nw2v_model.train(documents,total_examples=len(documents),epochs=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.most_similar(\"answer\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer=Tokenizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(xtrain)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words\", vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom keras.preprocessing.sequence import pad_sequences\nx_train = pad_sequences(tokenizer.texts_to_sequences(xtrain), maxlen=300)\nx_test = pad_sequences(tokenizer.texts_to_sequences(xval), maxlen=300)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x_train),len(ytrain),len(x_test),len(yval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build Embedding Layer\nembedding_matrix = np.zeros((vocab_size, 300))\nprint(embedding_matrix)\nfor word, i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Activation, Dense, Dropout, Embedding\nembedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n# Build The model\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ReduceLROnPlateau, EarlyStopping\ncallbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory = model.fit(x_train, ytrain,\n                    batch_size=32,\n                    epochs=8,\n                    validation_split=0.1,\n                    verbose=1,\n                    callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nscore = model.evaluate(x_test, yval, batch_size=32)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}