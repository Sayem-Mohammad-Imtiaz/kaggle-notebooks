{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Download, explore, and prepare UCI credit card default data¶","metadata":{}},{"cell_type":"markdown","source":"https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/xgboost_pdp_ice.ipynb","metadata":{}},{"cell_type":"markdown","source":"#### Python Libraries/Packages Import","metadata":{}},{"cell_type":"code","source":"import numpy as np                   # array, vector, matrix calculations\nimport pandas as pd                  # DataFrame handling\nimport shap                          # for consistent, signed variable importance measurements\nimport xgboost as xgb                # gradient boosting machines (GBMs)\n\nimport matplotlib.pyplot as plt      # plotting\npd.options.display.max_columns = 999 # enable display of all columns in notebook\n\n# enables display of plots in notebook\n%matplotlib inline\n\nnp.random.seed(12345)                # set random seed for reproducibility","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-19T12:59:42.750155Z","iopub.execute_input":"2021-09-19T12:59:42.751668Z","iopub.status.idle":"2021-09-19T12:59:51.834941Z","shell.execute_reply.started":"2021-09-19T12:59:42.751543Z","shell.execute_reply":"2021-09-19T12:59:51.833764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Import Data and Clean","metadata":{}},{"cell_type":"code","source":"# Read in Data\ndata = pd.read_csv(\"../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\")\n\n# Rename target column so that it's not using dots in names \n# (replace them with underscore instead; make naming convention consistent)\ndata = data.rename(columns={'default.payment.next.month': 'DEFAULT_NEXT_MONTH'}) ","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:00:03.921902Z","iopub.execute_input":"2021-09-19T13:00:03.922191Z","iopub.status.idle":"2021-09-19T13:00:04.138929Z","shell.execute_reply.started":"2021-09-19T13:00:03.922154Z","shell.execute_reply":"2021-09-19T13:00:04.137873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **LIMIT_BAL**: Amount of given credit (NT dollar)\n- **SEX**: 1 = male; 2 = female\n- **EDUCATION**: 1 = graduate school; 2 = university; 3 = high school; 4 = others\n- **MARRIAGE**: 1 = married; 2 = single; 3 = others\n- **AGE**: Age in years\n- **PAY_** : History of past payment; PAY_0 = the repayment status in September, 2005; PAY_2 = the repayment status in August, 2005; ...; PAY_6 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- **BILL_AMT** : Amount of bill statement (NT dollar). BILL_AMNT1 = amount of bill statement in September, 2005; BILL_AMT2 = amount of bill statement in August, 2005; ...; BILL_AMT6 = amount of bill statement in April, 2005.\n- **PAY_AMT** : Amount of previous payment (NT dollar). PAY_AMT1 = amount paid in September, 2005; PAY_AMT2 = amount paid in August, 2005; ...; PAY_AMT6 = amount paid in April, 2005.","metadata":{}},{"cell_type":"markdown","source":"#### Assign Model Roles","metadata":{"execution":{"iopub.status.busy":"2021-09-14T02:44:46.353739Z","iopub.execute_input":"2021-09-14T02:44:46.354386Z","iopub.status.idle":"2021-09-14T02:44:46.361638Z","shell.execute_reply.started":"2021-09-14T02:44:46.354348Z","shell.execute_reply":"2021-09-14T02:44:46.360403Z"}}},{"cell_type":"code","source":"# assign target and inputs for GBM\ny = 'DEFAULT_NEXT_MONTH'\nX = [name for name in data.columns if name not in [y, 'ID']]\n\nprint('y =', y)\nprint('X =', X)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:01:55.105115Z","iopub.execute_input":"2021-09-19T13:01:55.105463Z","iopub.status.idle":"2021-09-19T13:01:55.113473Z","shell.execute_reply.started":"2021-09-19T13:01:55.105431Z","shell.execute_reply":"2021-09-19T13:01:55.112135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Display descriptive statistics","metadata":{}},{"cell_type":"code","source":"data[X + [y]].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:02:02.018497Z","iopub.execute_input":"2021-09-19T13:02:02.018915Z","iopub.status.idle":"2021-09-19T13:02:02.142014Z","shell.execute_reply.started":"2021-09-19T13:02:02.018844Z","shell.execute_reply":"2021-09-19T13:02:02.141411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigate pair-wise Pearson correlations for DEFAULT_NEXT_MONTH","metadata":{}},{"cell_type":"markdown","source":"Monotonic 이란! X 와 Y가 같은 방향성을 가지고 움직일 때 (움직임의 정도는 같을 필요가 없다; 즉 linear과 monotonic은 다른 개념)\n\n- Monotonic relationships are much easier to explain to colleagues, bosses, customers, and regulators than more complex, non-monotonic relationships\n- monotonic relationships may also prevent overfitting and excess error due to variance for new data.","metadata":{}},{"cell_type":"markdown","source":"Constrainsts are supplied to XGBoost in the form of a Python tuple with length equal to the number of inputs. Each item in the tuple is associated with an input variable based on its index in the tuple. The first constraint in the tuple is associated with the first variable in the training data, the second constraint in the tuple is associated with the second variable in the training data, and so on. The constraints themselves take the form of a 1 for a positive relationship and a -1 for a negative relationship.","metadata":{}},{"cell_type":"markdown","source":"#### Calculate Pearson correlation","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(data[X + [y]].corr()[y]).iloc[:-1]","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:03:55.861265Z","iopub.execute_input":"2021-09-19T13:03:55.861582Z","iopub.status.idle":"2021-09-19T13:03:55.936745Z","shell.execute_reply.started":"2021-09-19T13:03:55.861553Z","shell.execute_reply":"2021-09-19T13:03:55.935626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create tuple of monotonicity constraints from Pearson correlation values","metadata":{}},{"cell_type":"code","source":"# creates a tuple in which positive correlation values are assigned a 1\n# and negative correlation values are assigned a -1\n\nmono_constraints = tuple([int(i) for i in np.sign(data[X + [y]].corr()[y].values[:-1])])","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:04:49.519755Z","iopub.execute_input":"2021-09-19T13:04:49.520069Z","iopub.status.idle":"2021-09-19T13:04:49.583109Z","shell.execute_reply.started":"2021-09-19T13:04:49.520041Z","shell.execute_reply":"2021-09-19T13:04:49.582308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train XGBoost with monotonicity constraints","metadata":{}},{"cell_type":"markdown","source":"- monotone_constraints tuning parameter is used to enforce monotonicity between inputs and the prediction for DEFAULT_NEXT_MONTH. \n- XGBoost's early stopping functionality is also used to limit overfitting to the training data","metadata":{}},{"cell_type":"markdown","source":"#### Split data into training and test sets for early stopping","metadata":{}},{"cell_type":"code","source":"np.random.seed(12345) # set random seed for reproducibility\nsplit_ratio = 0.7 #70/30% train & test split\n\n# execute the split\nsplit = np.random.rand(len(data)) < split_ratio\ntrain = data[split]\ntest = data[~split]\n\n# summarize split\nprint('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\nprint('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:05:47.098554Z","iopub.execute_input":"2021-09-19T13:05:47.098883Z","iopub.status.idle":"2021-09-19T13:05:47.117558Z","shell.execute_reply.started":"2021-09-19T13:05:47.098852Z","shell.execute_reply":"2021-09-19T13:05:47.116145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train XGBoost GBM classifier","metadata":{}},{"cell_type":"markdown","source":"- training and test data must be converted from Pandas DataFrames into SVMLight format. (DMatrix function 사용)\n- grid search 로 다양한 parameter들의 이상적인 값들을 찾겠지만 간략성을 위해서 그 값들을 미리 찾았다고 가정\n- Because gradient boosting methods typically resample training data, an additional random seed is also specified for XGBoost using the seed parameter\n- To avoid overfitting, the early_stopping_rounds parameter is used to stop the training process after the test area under the curve (AUC) statistic fails to increase for 50 iterations.","metadata":{}},{"cell_type":"code","source":"# pip install xgboost --user --upgrade pip","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:06:42.479982Z","iopub.execute_input":"2021-09-19T13:06:42.48079Z","iopub.status.idle":"2021-09-19T13:06:42.485747Z","shell.execute_reply.started":"2021-09-19T13:06:42.480744Z","shell.execute_reply":"2021-09-19T13:06:42.484466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost uses SVMLight data structure, not Numpy arrays or Pandas DataFrames\ndtrain = xgb.DMatrix(train[X], train[y])\ndtest = xgb.DMatrix(test[X], test[y])\n\n# used to calibrate predictions to mean of y\nbase_y = train[y].mean()\n\n# tuning parameters\nparams = {\n    'objective': 'binary:logistic',             # produces 0-1 probabilities for binary classification\n    'booster': 'gbtree',                        # base learner will be decision tree\n    'eval_metric': 'auc',                       # stop training based on maximum AUC, AUC always between 0-1\n    'eta': 0.08,                                # learning rate\n    'subsample': 0.9,                           # use 90% of rows in each decision tree\n    'colsample_bytree': 0.9,                    # use 90% of columns in each decision tree\n    'max_depth': 15,                            # allow decision trees to grow to depth of 15\n    'monotone_constraints':mono_constraints,    # 1 = increasing relationship, -1 = decreasing relationship\n    'base_score': base_y,                       # calibrate predictions to mean of y \n    'seed': 12345                               # set random seed for reproducibility\n}\n\n# watchlist is used for early stopping\nwatchlist = [(dtrain, 'train'), (dtest, 'eval')]\n\n# train model\nxgb_model = xgb.train(params,                   # set tuning parameters from above                   \n                      dtrain,                   # training data\n                      1000,                     # maximum of 1000 iterations (trees)\n                      evals=watchlist,          # use watchlist for early stopping \n                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in AUC\n                      verbose_eval=True)        # display iteration progress","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:06:42.765129Z","iopub.execute_input":"2021-09-19T13:06:42.765418Z","iopub.status.idle":"2021-09-19T13:06:53.016003Z","shell.execute_reply.started":"2021-09-19T13:06:42.765391Z","shell.execute_reply":"2021-09-19T13:06:53.015348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Global Shapley variable importance","metadata":{}},{"cell_type":"code","source":"# dtest is DMatrix\n# shap_valeus is np.array\n\n# shap_values = xgb_model.predict(dtest, pred_contribs = True,ntree_limit = xgb_model.best_ntree_limit)\n# ====> ntree_limit parameter deprecated\n\n# pred_contribus = True ==> display Shapley values\nshap_values = xgb_model.predict(dtest, pred_contribs = True, ntree_limit=xgb_model.best_ntree_limit)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:09:03.255568Z","iopub.execute_input":"2021-09-19T13:09:03.25586Z","iopub.status.idle":"2021-09-19T13:09:34.675236Z","shell.execute_reply.started":"2021-09-19T13:09:03.255832Z","shell.execute_reply":"2021-09-19T13:09:34.674474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot Shapley variable importance summary \n\nshap.summary_plot(shap_values[:, :-1], test[xgb_model.feature_names])","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:09:34.676466Z","iopub.execute_input":"2021-09-19T13:09:34.681108Z","iopub.status.idle":"2021-09-19T13:09:36.9347Z","shell.execute_reply.started":"2021-09-19T13:09:34.681059Z","shell.execute_reply":"2021-09-19T13:09:36.933839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculating partial dependence and ICE to validate and explain monotonic behavior","metadata":{}},{"cell_type":"markdown","source":"- Partial Dependence Plot (PDP): Global Average Explanations\n    - https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf (10.13 section 참고)\n- Individual Conditional Expectational (ICE) Plot: more localized explanations for a single observation of data using the same basic ideas as partial dependence plots\n    - 논문 참고: https://arxiv.org/abs/1309.6392\n- Partial dependence can be misleading in the presence of strong interactions or correlation. ICE curves diverging from the partial dependence curve can be indicative of such problems.\n- Histograms are also presented with the partial dependence and ICE curves, to enable a rough measure of epistemic uncertainty for model predictions: predictions based on small amounts of training data are likely less dependable.","metadata":{}},{"cell_type":"markdown","source":"#### Function for calculating partial dependence","metadata":{}},{"cell_type":"code","source":"def par_dep(xs, frame, model, resolution=20, bins=None):\n    \n    \"\"\" Creates Pandas DataFrame containing partial dependence for a \n        single variable.\n    \n    Args:\n        - xs: Variable for which to calculate partial dependence.\n        - frame: Pandas DataFrame for which to calculate partial dependence.\n        - model: XGBoost model for which to calculate partial dependence.\n        - resolution: The number of points across the domain of xs for which \n                    to calculate partial dependence, default 20.\n        - bins: List of values at which to set xs, default 20 equally-spaced \n              points between column minimum and maximum.\n    \n    Returns:\n        Pandas DataFrame containing partial dependence values.\n        \n    \"\"\"\n    \n    # turn off pesky Pandas copy warning\n    pd.options.mode.chained_assignment = None\n    \n    # initialize empty Pandas DataFrame with correct column names\n    par_dep_frame = pd.DataFrame(columns=[xs, 'partial_dependence'])\n    \n    # cache original column values \n    col_cache = frame.loc[:, xs].copy(deep=True)\n  \n    # determine values at which to calculate partial dependence\n    if bins == None:\n        min_ = frame[xs].min()\n        max_ = frame[xs].max()\n        by = (max_ - min_)/resolution\n        bins = np.arange(min_, max_, by)\n        \n    # calculate partial dependence  \n    # by setting column of interest to constant \n    # and scoring the altered data and taking the mean of the predictions\n    for j in bins:\n        frame.loc[:, xs] = j\n        dframe = xgb.DMatrix(frame)\n        par_dep_i = pd.DataFrame(model.predict(dframe, ntree_limit=xgb_model.best_ntree_limit))\n        par_dep_j = par_dep_i.mean()[0]\n        par_dep_frame = par_dep_frame.append({xs:j,\n                                              'partial_dependence': par_dep_j}, \n                                              ignore_index=True)\n        \n    # return input frame to original cached state    \n    frame.loc[:, xs] = col_cache\n\n    return par_dep_frame","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:14:01.725741Z","iopub.execute_input":"2021-09-19T13:14:01.726086Z","iopub.status.idle":"2021-09-19T13:14:01.738028Z","shell.execute_reply.started":"2021-09-19T13:14:01.726053Z","shell.execute_reply":"2021-09-19T13:14:01.737186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculate partial dependence for the most important input variables in the GBM","metadata":{}},{"cell_type":"code","source":"# Looking at just three variables\npar_dep_PAY_0 = par_dep('PAY_0', test[X], xgb_model)         # calculate partial dependence for PAY_0\npar_dep_LIMIT_BAL = par_dep('LIMIT_BAL', test[X], xgb_model) # calculate partial dependence for LIMIT_BAL\npar_dep_BILL_AMT1 = par_dep('BILL_AMT1', test[X], xgb_model) # calculate partial dependence for BILL_AMT1","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:14:04.085831Z","iopub.execute_input":"2021-09-19T13:14:04.086998Z","iopub.status.idle":"2021-09-19T13:14:05.84911Z","shell.execute_reply.started":"2021-09-19T13:14:04.086934Z","shell.execute_reply":"2021-09-19T13:14:05.848253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display partial dependence for PAY_0\npar_dep_PAY_0","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:14:06.962841Z","iopub.execute_input":"2021-09-19T13:14:06.963139Z","iopub.status.idle":"2021-09-19T13:14:06.976718Z","shell.execute_reply.started":"2021-09-19T13:14:06.963108Z","shell.execute_reply":"2021-09-19T13:14:06.975724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display partial dependence for LIMIT_BAL\npar_dep_LIMIT_BAL","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:14:13.118546Z","iopub.execute_input":"2021-09-19T13:14:13.118872Z","iopub.status.idle":"2021-09-19T13:14:13.133955Z","shell.execute_reply.started":"2021-09-19T13:14:13.11884Z","shell.execute_reply":"2021-09-19T13:14:13.132797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display partial dependence for BILL_AMT1\npar_dep_BILL_AMT1","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:14:13.55081Z","iopub.execute_input":"2021-09-19T13:14:13.551188Z","iopub.status.idle":"2021-09-19T13:14:13.565099Z","shell.execute_reply.started":"2021-09-19T13:14:13.551152Z","shell.execute_reply":"2021-09-19T13:14:13.564065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Helper function for finding percentiles of predictions","metadata":{}},{"cell_type":"markdown","source":"- Calculating and analyzing ICE curves for every row of training and test data set can be overwhelming\n- so... start with calculating ICE curves at every decile of predicted probabilities in a dataset, giving an indication of local prediction behavior across the dataset\n- The function below finds and returns the row indices for the maximum, minimum, and deciles of one column in terms of another -- in this case, the model predictions (p_DEFAULT_NEXT_MONTH) and the row identifier (ID), respectively.","metadata":{}},{"cell_type":"code","source":"def get_percentile_dict(yhat, id_, frame):\n\n    \"\"\" Returns the percentiles of a column, yhat, as the indices based on \n        another column id_.\n    \n    Args:\n        yhat: Column in which to find percentiles.\n        id_: Id column that stores indices for percentiles of yhat.\n        frame: Pandas DataFrame containing yhat and id_. \n    \n    Returns:\n        Dictionary of percentile values and index column values.\n    \n    \"\"\"\n    \n    # create a copy of frame and sort it by yhat\n    sort_df = frame.copy(deep=True)\n    sort_df.sort_values(yhat, inplace=True)\n    sort_df.reset_index(inplace=True)\n    \n    # find top and bottom percentiles\n    percentiles_dict = {}\n    percentiles_dict[0] = sort_df.loc[0, id_]\n    percentiles_dict[99] = sort_df.loc[sort_df.shape[0]-1, id_]\n\n    # find 10th-90th percentiles\n    inc = sort_df.shape[0]//10\n    for i in range(1, 10):\n        percentiles_dict[i * 10] = sort_df.loc[i * inc,  id_]\n\n    return percentiles_dict","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:14:48.696361Z","iopub.execute_input":"2021-09-19T13:14:48.696692Z","iopub.status.idle":"2021-09-19T13:14:48.704926Z","shell.execute_reply.started":"2021-09-19T13:14:48.696661Z","shell.execute_reply":"2021-09-19T13:14:48.70405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Find some percentiles of yhat in the test data\n\nThe values for ID that correspond to the maximum, minimum, and deciles of p_DEFAULT_NEXT_MONTH are displayed below. ICE will be calculated for the rows of the test dataset associated with these ID values.","metadata":{}},{"cell_type":"code","source":"# merge GBM predictions onto test data\nyhat_test = pd.concat([test.reset_index(drop=True), pd.DataFrame(xgb_model.predict(dtest, \n                                                                                   ntree_limit=xgb_model.best_ntree_limit))], axis=1)\nyhat_test = yhat_test.rename(columns={0:'p_DEFAULT_NEXT_MONTH'})\n\n# find percentiles of predictions\npercentile_dict = get_percentile_dict('p_DEFAULT_NEXT_MONTH', 'ID', yhat_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:15:09.10157Z","iopub.execute_input":"2021-09-19T13:15:09.102154Z","iopub.status.idle":"2021-09-19T13:15:09.144321Z","shell.execute_reply.started":"2021-09-19T13:15:09.102118Z","shell.execute_reply":"2021-09-19T13:15:09.143579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:15:20.318064Z","iopub.execute_input":"2021-09-19T13:15:20.318839Z","iopub.status.idle":"2021-09-19T13:15:20.325537Z","shell.execute_reply.started":"2021-09-19T13:15:20.318804Z","shell.execute_reply":"2021-09-19T13:15:20.324145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display percentiles dictionary\n# ID values for rows from lowest prediction to highest prediction\npercentile_dict","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:15:20.938622Z","iopub.execute_input":"2021-09-19T13:15:20.938906Z","iopub.status.idle":"2021-09-19T13:15:20.945756Z","shell.execute_reply.started":"2021-09-19T13:15:20.938879Z","shell.execute_reply":"2021-09-19T13:15:20.944467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculate ICE curve values","metadata":{}},{"cell_type":"markdown","source":"- ICE values represent a model's prediction for a row of data while an input variable of interest is varied across its domain. \n- The values of the input variable are chosen to match the values at which partial dependence was calculated earlier, and ICE is calculated for the top three most important variables and for rows at each percentile of the test dataset.","metadata":{}},{"cell_type":"code","source":"# retreive bins from original partial dependence calculation\n\nbins_PAY_0 = list(par_dep_PAY_0['PAY_0'])\nbins_LIMIT_BAL = list(par_dep_LIMIT_BAL['LIMIT_BAL'])\nbins_BILL_AMT1 = list(par_dep_BILL_AMT1['BILL_AMT1'])\n\n# for each percentile in percentile_dict\n# create a new column in the par_dep frame \n# representing the ICE curve for that percentile\n# and the variables of interest\nfor i in sorted(percentile_dict.keys()):\n    \n    col_name = 'Percentile_' + str(i)\n    \n    # ICE curves for PAY_0 across percentiles at bins_PAY_0 intervals\n    par_dep_PAY_0[col_name] = par_dep('PAY_0', \n                                    test[test['ID'] == int(percentile_dict[i])][X],  \n                                    xgb_model, \n                                    bins=bins_PAY_0)['partial_dependence']\n    \n    # ICE curves for LIMIT_BAL across percentiles at bins_LIMIT_BAL intervals\n    par_dep_LIMIT_BAL[col_name] = par_dep('LIMIT_BAL', \n                                          test[test['ID'] == int(percentile_dict[i])][X], \n                                          xgb_model, \n                                          bins=bins_LIMIT_BAL)['partial_dependence']\n    \n\n\n    # ICE curves for BILL_AMT1 across percentiles at bins_BILL_AMT1 intervals\n    par_dep_BILL_AMT1[col_name] = par_dep('BILL_AMT1', \n                                          test[test['ID'] == int(percentile_dict[i])][X],  \n                                          xgb_model, \n                                          bins=bins_BILL_AMT1)['partial_dependence']","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:16:57.741074Z","iopub.execute_input":"2021-09-19T13:16:57.741453Z","iopub.status.idle":"2021-09-19T13:17:00.78884Z","shell.execute_reply.started":"2021-09-19T13:16:57.741417Z","shell.execute_reply":"2021-09-19T13:17:00.788178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Display partial dependence and ICE for LIMIT_BAL","metadata":{}},{"cell_type":"code","source":"par_dep_PAY_0","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:17:10.515567Z","iopub.execute_input":"2021-09-19T13:17:10.516493Z","iopub.status.idle":"2021-09-19T13:17:10.544422Z","shell.execute_reply.started":"2021-09-19T13:17:10.51644Z","shell.execute_reply":"2021-09-19T13:17:10.543267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"par_dep_LIMIT_BAL","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:17:11.017113Z","iopub.execute_input":"2021-09-19T13:17:11.017985Z","iopub.status.idle":"2021-09-19T13:17:11.045132Z","shell.execute_reply.started":"2021-09-19T13:17:11.017913Z","shell.execute_reply":"2021-09-19T13:17:11.044178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"par_dep_BILL_AMT1","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:17:29.986175Z","iopub.execute_input":"2021-09-19T13:17:29.986517Z","iopub.status.idle":"2021-09-19T13:17:30.019428Z","shell.execute_reply.started":"2021-09-19T13:17:29.986487Z","shell.execute_reply":"2021-09-19T13:17:30.018663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Plotting partial dependence and ICE to validate and explain monotonic behavior","metadata":{}},{"cell_type":"markdown","source":"Overlaying partial dependence onto ICE in a plot is a convenient way to validate and understand both global and local monotonic behavior. Plots of partial dependence curves overlayed onto ICE curves for several percentiles of predictions for DEFAULT_NEXT_MONTH are used to validate monotonic behavior, describe the GBM model mechanisms, and to compare the most extreme GBM behavior with the average GBM behavior in the test data. Partial dependence and ICE plots are displayed for the three most important variables in the GBM: PAY_0, LIMIT_BAL, and BILL_AMT1.","metadata":{}},{"cell_type":"code","source":"#### Function to plot partial dependence and ICE\n\ndef plot_par_dep_ICE(xs, par_dep_frame):\n\n    \n    \"\"\" Plots ICE overlayed onto partial dependence for a single variable.\n    \n    Args: \n        xs: Name of variable for which to plot ICE and partial dependence.\n        par_dep_frame: Name of Pandas DataFrame containing ICE and partial\n                       dependence values.\n    \n    \"\"\"\n    \n    # initialize figure and axis\n    fig, ax = plt.subplots()\n    \n    # plot ICE curves\n    par_dep_frame.drop('partial_dependence', axis=1).plot(x=xs, \n                                                          colormap='gnuplot',\n                                                          ax=ax)\n\n    # overlay partial dependence, annotate plot\n    par_dep_frame.plot(title='Partial Dependence and ICE for ' + str(xs),\n                       x=xs, \n                       y='partial_dependence',\n                       style='r-', \n                       linewidth=3, \n                       ax=ax)\n\n    # add legend\n    _ = plt.legend(bbox_to_anchor=(1.05, 0),\n                   loc=3, \n                   borderaxespad=0.)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:18:08.879652Z","iopub.execute_input":"2021-09-19T13:18:08.879958Z","iopub.status.idle":"2021-09-19T13:18:08.887367Z","shell.execute_reply.started":"2021-09-19T13:18:08.879919Z","shell.execute_reply":"2021-09-19T13:18:08.886428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Partial dependence and ICE plot for LIMIT_BAL","metadata":{}},{"cell_type":"markdown","source":"- Monotonic decreasing behavior is evident at every percentile of predictions for DEFAULT_NEXT_MONTH. \n- Most percentiles of predictions show that sharper decreases in probability of default occur when LIMIT_BAL increases just slightly from its lowest values in the test set. \n- However, for the custumers that are most likely to default according to the GBM model, no increase in LIMIT_BAL has a strong impact on probabilitiy of default.","metadata":{}},{"cell_type":"code","source":"plot_par_dep_ICE('LIMIT_BAL', par_dep_LIMIT_BAL) # plot partial dependence and ICE for LIMIT_BAL","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:18:15.068736Z","iopub.execute_input":"2021-09-19T13:18:15.069032Z","iopub.status.idle":"2021-09-19T13:18:15.446575Z","shell.execute_reply.started":"2021-09-19T13:18:15.069002Z","shell.execute_reply":"2021-09-19T13:18:15.445767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train['LIMIT_BAL'].plot(kind='hist', bins=20, title='Histogram: LIMIT_BAL')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:20:41.987074Z","iopub.execute_input":"2021-09-19T13:20:41.987504Z","iopub.status.idle":"2021-09-19T13:20:42.265499Z","shell.execute_reply.started":"2021-09-19T13:20:41.987467Z","shell.execute_reply":"2021-09-19T13:20:42.264396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" As can be seen from the displayed histogram, above ~$NT 500,000 prediction behavior may have been learned from extremely small samples of data.","metadata":{}},{"cell_type":"markdown","source":"#### Partial dependence and ICE plot for PAY_0","metadata":{}},{"cell_type":"markdown","source":"- Monotonic increasing prediction behavior for PAY_0 is displayed for all percentiles of model predictions. \n- Predition behavior is different at different deciles, but not abnormal or vastly different from the average prediction behavior represented by the red partial dependence curve. \n- The largest jump in predicted probability appears to occur at PAY_0 = 2, or when a customer becomes two months late on their most recent payment. ","metadata":{}},{"cell_type":"code","source":"plot_par_dep_ICE('PAY_0', par_dep_PAY_0) # plot partial dependence and ICE for PAY_0","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:21:10.139506Z","iopub.execute_input":"2021-09-19T13:21:10.139832Z","iopub.status.idle":"2021-09-19T13:21:10.502919Z","shell.execute_reply.started":"2021-09-19T13:21:10.139796Z","shell.execute_reply":"2021-09-19T13:21:10.502014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train['PAY_0'].plot(kind='hist', bins=20, title='Histogram: PAY_0')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:21:13.375124Z","iopub.execute_input":"2021-09-19T13:21:13.37585Z","iopub.status.idle":"2021-09-19T13:21:13.636605Z","shell.execute_reply.started":"2021-09-19T13:21:13.375785Z","shell.execute_reply":"2021-09-19T13:21:13.635527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Partial dependence and ICE plot for BILL_AMT1","metadata":{}},{"cell_type":"markdown","source":"- Monotonic decreasing prediction behavior for BILL_AMT1 is also displayed for all percentiles. \n- Mild decrease in probability of default as most recent bill amount increases could be related to wealthier, big-spending customers taking on more debt but also being able to pay it off reliably. \n- customers with negative bills are more likely to default, potentially indicating charge-offs(과금: 채권자가 부채 금액을 징수하지 않을 것이라는 선언) are being recorded as negative bills.","metadata":{}},{"cell_type":"code","source":"plot_par_dep_ICE('BILL_AMT1', par_dep_BILL_AMT1) # plot partial dependence and ICE for BILL_AMT1","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:21:45.263415Z","iopub.execute_input":"2021-09-19T13:21:45.264682Z","iopub.status.idle":"2021-09-19T13:21:46.024381Z","shell.execute_reply.started":"2021-09-19T13:21:45.264631Z","shell.execute_reply":"2021-09-19T13:21:46.023398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train['BILL_AMT1'].plot(kind='hist', bins=20, title='Histogram: BILL_AMT1')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:21:46.025907Z","iopub.execute_input":"2021-09-19T13:21:46.026169Z","iopub.status.idle":"2021-09-19T13:21:46.270069Z","shell.execute_reply.started":"2021-09-19T13:21:46.026138Z","shell.execute_reply":"2021-09-19T13:21:46.26917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- predictions below 0 and above 400,000 are based on very little training data.","metadata":{"execution":{"iopub.status.busy":"2021-09-16T01:16:15.287776Z","iopub.execute_input":"2021-09-16T01:16:15.288085Z","iopub.status.idle":"2021-09-16T01:16:15.295537Z","shell.execute_reply.started":"2021-09-16T01:16:15.288055Z","shell.execute_reply":"2021-09-16T01:16:15.294302Z"}}},{"cell_type":"markdown","source":"### 6. Generate reason codes using the Shapley method","metadata":{}},{"cell_type":"markdown","source":"#### Select most risky customer in test data\n\nOne person who might be of immediate interest is the most likely to default customer in the test data. This customer's row will be selected and local variable importance for the corresponding prediction will be analyzed.","metadata":{}},{"cell_type":"code","source":"test.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:23:31.596797Z","iopub.execute_input":"2021-09-19T13:23:31.597139Z","iopub.status.idle":"2021-09-19T13:23:31.602917Z","shell.execute_reply.started":"2021-09-19T13:23:31.597105Z","shell.execute_reply":"2021-09-19T13:23:31.601448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decile = 99\nrow = test[test['ID'] == percentile_dict[decile]]","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:23:31.887987Z","iopub.execute_input":"2021-09-19T13:23:31.888324Z","iopub.status.idle":"2021-09-19T13:23:31.896493Z","shell.execute_reply.started":"2021-09-19T13:23:31.888277Z","shell.execute_reply":"2021-09-19T13:23:31.895455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create a Pandas DataFrame of Shapley values for riskiest customer","metadata":{}},{"cell_type":"markdown","source":"The most interesting Shapley values are probably those that push this customer's probability of default higher, i.e. the highest positive Shapley values. Those values are plotted below.","metadata":{}},{"cell_type":"code","source":"# reset test data index to find riskiest customer in shap_values \n# sort to find largest positive contributions\ns_df = pd.DataFrame(shap_values[row.index[0], :][:-1].reshape(23, 1), columns=['Reason Codes'], index=X)\ns_df.sort_values(by='Reason Codes', inplace=True, ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:23:43.527606Z","iopub.execute_input":"2021-09-19T13:23:43.528363Z","iopub.status.idle":"2021-09-19T13:23:43.536624Z","shell.execute_reply.started":"2021-09-19T13:23:43.528323Z","shell.execute_reply":"2021-09-19T13:23:43.535688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_df","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:23:45.190547Z","iopub.execute_input":"2021-09-19T13:23:45.190859Z","iopub.status.idle":"2021-09-19T13:23:45.208403Z","shell.execute_reply.started":"2021-09-19T13:23:45.190829Z","shell.execute_reply":"2021-09-19T13:23:45.20738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot top local contributions as reason codes","metadata":{}},{"cell_type":"code","source":"_ = s_df[:5].plot(kind='bar', \n                  title='Top Five Reason Codes for a Risky Customer\\n', \n                  legend=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:23:53.276128Z","iopub.execute_input":"2021-09-19T13:23:53.277321Z","iopub.status.idle":"2021-09-19T13:23:53.455421Z","shell.execute_reply.started":"2021-09-19T13:23:53.27725Z","shell.execute_reply":"2021-09-19T13:23:53.454415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the customer in the test dataset that the GBM predicts as most likely to default, the most important input variables in the prediction are, in descending order, PAY_0, PAY_5, PAY_6, PAY_2, and LIMIT_BAL.","metadata":{}},{"cell_type":"markdown","source":"#### Display customer in question","metadata":{"execution":{"iopub.status.busy":"2021-09-16T01:51:06.834008Z","iopub.execute_input":"2021-09-16T01:51:06.83439Z","iopub.status.idle":"2021-09-16T01:51:06.839116Z","shell.execute_reply.started":"2021-09-16T01:51:06.834356Z","shell.execute_reply":"2021-09-16T01:51:06.838346Z"}}},{"cell_type":"code","source":"row # helps understand reason codes","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:24:30.657822Z","iopub.execute_input":"2021-09-19T13:24:30.658128Z","iopub.status.idle":"2021-09-19T13:24:30.68189Z","shell.execute_reply.started":"2021-09-19T13:24:30.658099Z","shell.execute_reply":"2021-09-19T13:24:30.68055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The local contributions for this customer appear reasonable, especially when considering her payment information. Her most recent payment was 3 months late and her payment for 6 months and 5 months previous were 7 months late. Also her credit limit was extremely low, so it's logical that these factors would weigh heavily into the model's prediction for default for this customer.\n\n","metadata":{}},{"cell_type":"markdown","source":"To generate reason codes for the model's decision, the locally important variable and its value are used together. If this customer was denied future credit based on this model and data, the top five Shapley-based reason codes for the automated decision would be:\n\n- Most recent payment is 3 months delayed.\n- 5th most recent payment is 7 months delayed.\n- 6th most recent payment is 7 months delayed.\n- 2nd most recent payment is 2 months delayed.\n- Credit limit is too low: 10,000 $NT.","metadata":{}},{"cell_type":"markdown","source":"(Of course, credit limits are set by the lender and are used to price-in risk to credit decisions, so using credit limits as reason codes or even in a probability of default model is likely questionable. However, in this small, example data set all input columns were used to generate a better model fit. For a slightly more careful treatment of gradient boosting in the context of credit scoring, please see: https://github.com/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}