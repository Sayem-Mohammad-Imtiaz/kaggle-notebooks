{"cells":[{"metadata":{"trusted":true,"_uuid":"594ea6f63397a308e3f484f264714b574a15982e"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"231ebf2c08cbb74a4775d7aad943c0ef1a83363d"},"cell_type":"markdown","source":"# Getting Data and doing some sanity checks"},{"metadata":{"trusted":true,"_uuid":"06b5b4ff3bd472dcb6dc3f136f2d004161d1b5e5"},"cell_type":"code","source":"wine = pd.read_csv(\"../input/winequality-red.csv\",sep=',')","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1820685220d9c1dc6774634ff49d3792dc2e49c2"},"cell_type":"code","source":"wine.head()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a94d6a1959b9bbe4be77334dcbe1b859aae96224"},"cell_type":"code","source":"wine.isnull().sum()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29edb8820da2e2925f1c6614dee8d2c012996cfb"},"cell_type":"code","source":"wine.info()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4992bddcdc436168e3c5bbac99283ba513a4fc8e"},"cell_type":"code","source":"wine.describe()","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"72b0bd52cace041ba6097eb9c85e7e104828a82b"},"cell_type":"markdown","source":"## There are no null values in dataset. It's pretty clean dataset. No data cleaning has done. And all the data is having numerical values. But we should standardize the data before using into models. Because most of the algorithms assume that all the features are centered around zero and have same variance. We will do that in later stages."},{"metadata":{"_uuid":"088a3c104ddffdd2d5d8ab31e8b3d7b615a8745f"},"cell_type":"markdown","source":"## Going to check how the features are correlated with each other. Here am just doing for White wine data. In case if we have lots of features in the dataset. It is best practise to check which features are more correlated with the target variable. The model will give more reliable output when we pass significant features into the model."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0e048ec81e0356a3921a5a2cb553456f5a8eaef1"},"cell_type":"code","source":"import seaborn as sns","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c287d92275ed430e852ea457349016f5e4c5614"},"cell_type":"code","source":"    corr = wine.corr()\n    fig, ax = plt.subplots(figsize = (10,10))\n    g= sns.heatmap(corr,ax=ax, annot= True)\n    ax.set_title('Correlation between variables')","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecc022e5ed5c754e5a02f69d26433ed3a59adfda"},"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"9726ef505b44dd812ab77adc94e5d9e78f5c1216"},"cell_type":"markdown","source":"## Here am taking all the features as predictors but we can eliminate pH, free sulfur dioxide, residual sugar features from predictor list. Becase they are not much correlated with target variable \"Quality\"."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"86a343d6f1fe964c5837847643daa0d688806f80"},"cell_type":"code","source":"y = wine.quality\nX = wine.drop('quality',axis = 1)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fb9c946e03869854b68d0ad9e46875ce4ddf8721"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"28076d17f8f4ad7a2738a4ece525ca05af2fb0b5"},"cell_type":"code","source":"train_x,test_x,train_y,test_y = train_test_split(X,y,random_state = 0, stratify = y)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3e8e5b78d4479bab5174ba36d1f576dee8d4caee"},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"7318c9eab23ede5f87409b5166d235002c15b254"},"cell_type":"markdown","source":"## Here am using standarscaler function to standardize the data. We can use other methods also to do this. But this function will make sure the test data also standardized based on training data mean. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d697d7321286340d692c16246ed7b60254e6007"},"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(train_x)\ntrain_x_scaled = scaler.transform(train_x)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2808cef539a5a99090ce6cf78a433a307632656f"},"cell_type":"code","source":"test_x_scaled = scaler.transform(test_x)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"cb71d24e472154a43bc90fc0e061e1405a21b3a3"},"cell_type":"markdown","source":"## Cross validating the most common regression models to find which algorithm works better"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8cef0cf58e105c09d93d7d59b6bd5401e3e9d95a"},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0fb56ffd051ea2bad45553978d5bf4d990a85319"},"cell_type":"code","source":"models = []\nmodels.append(('DecisionTree', DecisionTreeRegressor()))\nmodels.append(('RandomForest', RandomForestRegressor()))\nmodels.append(('GradienBoost', GradientBoostingRegressor()))\nmodels.append(('SVR', SVR()))\nnames = []","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ee4d030d3caaaa8da116bd6d7380c3fffcf6c66f"},"cell_type":"code","source":"for name,model in models:\n    kfold = model_selection.KFold(n_splits=5,random_state=2)\n    cv_results = model_selection.cross_val_score(model,train_x_scaled,train_y, cv= kfold, scoring = 'neg_mean_absolute_error')\n    names.append(name)\n    msg  = \"%s: %f\" % (name, -1*(cv_results).mean())\n    print(msg)","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"a64c3dda8b8a504739a2b792a9f34e5da9c10747"},"cell_type":"markdown","source":"## Regression models can be validated using Mean Absolute Error(MAE). The less the mae value the better the model works. From above results we can decide that Random Forest model works better than other models for our data.\n## For classification models , we will use accuracy score and confusion matrix to validate across the models. Accuracy score should be high for best results."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"70777808d2739f3be2e1f9a1da2a7e7ba7c05296"},"cell_type":"code","source":"model = RandomForestRegressor()\nmodel.fit(train_x_scaled,train_y)\npred_y = model.predict(test_x_scaled)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1d83b49892bd0fa7cc712abbede26d91bf20f53"},"cell_type":"code","source":"mean_absolute_error(pred_y,test_y)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a15a53ef5728a1cfe36272d2a6bdf995d42a62b5"},"cell_type":"code","source":"test_y.head()","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebe97b28263887db7ffcf29fd8ccfd3b40108d4a"},"cell_type":"code","source":"pred_y","execution_count":33,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e95e2fb3cb9e5fd1689e8149bed2956aaf7574b3"},"cell_type":"code","source":"RandomForestRegressor?","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"c4d7ea0485cfc7289bcf74f521f525b3f0769375"},"cell_type":"markdown","source":"## Below am going to find the parameter n_estimator, to tune the model for better result. The same way we can find other paramters also. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bedd61b2db223b3fe606340846dedfdabce25995"},"cell_type":"code","source":"def get_mae_rf(num_est, predictors_train, predictors_val, targ_train, targ_val):\n\n    # fitting model with input max_leaf_nodes\n    model = RandomForestRegressor(n_estimators=num_est, random_state=0)\n\n    # fitting the model with training dataset\n    model.fit(predictors_train, targ_train)\n\n    # making prediction with the test dataset\n    preds_val = model.predict(predictors_val)\n\n    # calculate and return the MAE\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4eb56f907f9c16fd9e46c39ef76dcf8452ecb96a"},"cell_type":"code","source":"plot_mae = {}\nfor num_est  in range(2,50):\n    my_mae = get_mae_rf(num_est,train_x_scaled,test_x_scaled,train_y,test_y)\n    plot_mae[num_est] = my_mae","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b881b3b6c03589837863929452826c1df63195c"},"cell_type":"code","source":"plt.plot(list(plot_mae.keys()),plot_mae.values())\nplt.show()","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"fe735ec6c770fff168be6908d41f0ebb048a14ac"},"cell_type":"markdown","source":"* ## The mae is less after 36. Lets check how much performance is improved by passing this parameter into the model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a2079d5e10b6b2ac7403220d677b5fd2e32768fe"},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=36)\nmodel.fit(train_x_scaled,train_y)\npred_y = model.predict(test_x_scaled)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4382a1fc3a7d5a4969e6857b0d028819ee109b99"},"cell_type":"code","source":"mean_absolute_error(pred_y,test_y)","execution_count":40,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}