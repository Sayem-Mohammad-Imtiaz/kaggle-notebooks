{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Definition\nUnsupervised Learning is a type of Machine Learning learning algorithm used to draw inferences from data without any specified targets. Hence the goal for the algorithm in this case is to find underlying structure/pattern from unlabeled data.  \n\nThere are 3 main usages of Unsupervised Learning:-\n1. **Clustering** (Forming Groups of Data Points)\n2. **Association** (Example:- People who buy Bat also tend to buy ball)\n3. **Dimensionality Reduction** (Reducing dimensionality of data by projection)\n\n# About this Notebook\n* Here we are going to learn the most widley used aspect of Unsupervised Learning:- \"**Clustering**\".\n* Being **beginner friendly**, this notebook will focus solely on basics, getting to know the data and build a primitive yet effective model.\n* We will explore the following clutering algorithms:-\n    1. k-Means Clustering\n    2. Heirarchical Clustering\n    3. Affinity Propagation\n    4. Mean Shift\n    5. Spectral Clustering\n    6. DBSCAN\n    7. Gaussian Mixture Model\n* We will also learn how to determine the **optimal number of clusters** for some of the algorithms.\n* Lastly we will learn how draw some interesting insights using the results of models.\n\n# What is Clustering\nClustering is a task of segregating the whole population into smaller groups in which the members of each group have more similar traits to each other than to members of other groups.  \nFor example:-  \nThe complete set of news in a day can be clustered into groups like Political News, Sports News, Entertainment News and Weather report. In such case each member of Sports news (for example Cricket and Formula 1) will have more similarities to one another than with any member of the Weather Report cluster.\n\n# Types of Clustering Algorithms\n1. **Centroid Algorithm**:- This is an iterative approach of finding cluster centroid and deciding cluster based on the point's distance to each cluster center. This process is repeated until the centroid movement converges. (Ex:- k-Means)\n2. **Density Algorithm**:- Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups/clusters in the data, based on the idea that a cluster in a data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density. This algorithm requires only one-pass over the data space. (Ex:- DBSCAN)\n3. **Distribution Algorithm**:- This algorithm is based on the idea that clusters can easily be defined as objects belonging most likely to the same distribution. (Ex:- Gaussian Mixture Models)\n4. **Connectivity Algorithm**:- The core idea behind this algorithm is that datapoints closer to one another in N-dimensional space tend to have similar properties than data points much farther away. (Ex:- Heirarchical Clustering)\n\n# Applications of Clustering\nClustering is widely used in many different domains inluding:-  \n* Fraud Detection\n* Recommender Systems\n* News Segmentation\n* Spam Detection\n\nand many more...\n\n# Problem Statement\n\n## About the Data:-\nThe data contains the followig columns:-\n\n| Column Name | Description |\n|:------------|:------------|\n| country | Name of the Country |\n| child_mort | Child Mortality Rate |\n| exports | Per capita export of goods and services |\n| health | Per capita spending on health |\n| imports | Per capita import of goods and services |\n| Income | Per capita Income |\n| Inflation | Annual growth rate of GDP |\n| life_expec | Life Expectancy |\n| total_fer | Fertility rate |\n| gdpp | Per capita GDP |\n\n## Expected Outcome:-\nBased on the above socio-economic factors we need to determine which contries to invest that are in the direst need of aid.  \n\nSo now that this is clear, let's get into it starting with some basic imports:-"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# For Reproducable results\nfrom numpy.random import seed\nseed(1)\n\n# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\n# Visialisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# Models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation\nfrom sklearn.cluster import MeanShift, estimate_bandwidth, SpectralClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\nimport scipy.cluster.hierarchy as shc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '../input/unsupervised-learning-on-country-data'\n\ntrain_file_path = os.path.join(data_path, 'Country-data.csv')\n\nprint(f'Training file path: {train_file_path}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\n\n## 1. Descriptive Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(train_file_path)\ntrain_df.sample(10) # Random 10 rows from the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic Decriptive Analysis\ntrain_df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are no null values in any of the columns. So 1 step less for us. Now let's move on to understanding each column..."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all feature columns (except country) are numerical in nature. And the desciption makes it clear that none of them are any categorical numbers as well. So eveything is continuous. Good for us... Moving on to visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\n    'child_mort', 'exports', 'health','imports',\n    'income', 'inflation', 'life_expec', 'total_fer',\n    'gdpp'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features of smilar scales grouped together for better visibility\nfeatures_1 = [\n    'income', 'gdpp'\n]\n\nfeatures_2 = [\n    'child_mort', 'exports','imports',\n    'inflation', 'life_expec'\n]\n\nfeatures_3 = [\n    'health','total_fer'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train_df[features_1], orient=\"h\", palette=\"Set2\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train_df[features_2], orient=\"h\", palette=\"Set2\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train_df[features_3], orient=\"h\", palette=\"Set2\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Pair-Wise Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(train_df[features])\ng.fig.set_size_inches(12,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_cor_spear = train_df[features].corr(method='spearman')\nplt.figure(figsize=(10,8))\nsns.heatmap(train_df_cor_spear, square=True, cmap='coolwarm', annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_cor_spear = train_df[features].corr(method='spearman')\nmask = np.zeros_like(train_df_cor_spear)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(10,8))\nsns.heatmap(train_df_cor_spear, mask=mask, square=True, cmap='coolwarm', annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA Inferences:-\nInteresting dataset! Some inferences I can make-out right now are as follows:-  \n1. Child Mortality rate decreases with Increase in GDPP. Both of them have a very high correlation, which is expected as developed contries(having higer GDPP) will have better healthcare and hence better chance of survival.\n2. Child Mortality rate is directly proportional to total fertility rate. Which is sort of a dependent feature in my opinion. Women are giving birth to more children if the previous ones didn't survive unfortunately.\n3. Child Mortality rate is iversely proportional to Life expectance. Which is again a very dependent feature, because if more number children unfortunately die too early it pulls down the overall life expectancy of the country.\n4. Inflation is weakly inversely proportional to GDPP which in my opinion might be due to the economic saturation in highly developed nations.\n5. Per capita Income is heavily correlated to GDPP because one is roughly a function of another.\n6. Import and exports increade with one another which implied the trading power of the contry as a whole grows; i.e, countries who export more are also likely to import something else more.\n7. Spendings on health increase with GDPP and Income, which is self-explanatory.\n8. Income rises with exports which might be because people generate income by generating goods and services which are later exported.\n9. Child Mortality rate is iversely proportional to Healthcare expenditure and Income. Which shows that unfortunate circumstances with low income groups is often responsible for a low life expentancy among children.  \n\nKeeping thse in mind, we see that there is a very high level of correlation between most the features. So let's first drop one of the variable pairs which have very strong correlation."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_drop = ['gdpp', 'child_mort', 'total_fer']\ntrain_df.drop(features_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Trade_Deficiency'] = train_df['exports'] - train_df['imports']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [feat for feat in train_df.columns if feat not in ['country']]\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling Data\nSince many of our algorithm sare based on point-to-point distance, it is essential to scale the data as a higher variance in one dimension might lead to worse performance of the model. We are just going to use the StandardScaler algorithm within sklearn. This will essentially make the mean of the data ~ 0 and variance ~ 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaled_data = scaler.fit_transform(train_df[features])\nscaled_data = pd.DataFrame(scaled_data, columns=features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's move on to the modelling part... But before that let's create some functions which will help us through the whole process because we will be repeating similar steps for most of the models. Also this makes the code generic and reusable for anyone of you interested to follow same avenues.\n\n# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def project_to_2d(df, features=features, plot=False, cluster=None):\n    pca = PCA(n_components=2)\n    projected = pca.fit_transform(df[features])\n    if plot:\n        if cluster is None:\n            sns.scatterplot(\n                x=projected[:, 0],\n                y=projected[:, 1]\n            )\n        elif cluster is not None:\n            num_clusters = df[cluster].nunique()\n            sns.scatterplot(\n                x=projected[:, 0],\n                y=projected[:, 1],\n                hue=df[cluster].values,\n                palette=sns.color_palette(\"husl\", num_clusters)\n            )\n    return projected","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pair_plot_cluster(df, scaled_data, cluster, features=features):\n    df[cluster] = scaled_data[cluster]\n    num_clusters = df[cluster].nunique()\n    g = sns.pairplot(\n        df[features + [cluster]],\n        hue=cluster,\n        palette=sns.color_palette(\"husl\", num_clusters)\n    )\n    g.fig.set_size_inches(12,10)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we cluster the data, let's look at how it looks in a 2-D projection."},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation\nWe will be generating various models and discussed earlier. And we will use the utility functions defined above to judge and derive inferences from various models."},{"metadata":{},"cell_type":"markdown","source":"# 1. KMeans  \nThe KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KMeans(n_clusters=2, init='k-means++', random_state=RANDOM_SEED)\n# 2 is just an arbitrary number, we will find the exact number soon below\n\nmodel.fit(scaled_data[features])\nscaled_data['KMeans'] = model.predict(scaled_data[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='KMeans')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2 broad methods to find the optimal number of clusters, let's look at them, one by one...\n### A. Elbow Method  \nIn this method we iterate over various number of probable clusters and find the overall inertia of the clusters. The plot of the same forms a hard like structure with shoulders, elbow, forearm, etc. Based on this we **\"eyeball\"** and try to find the elbow of this plot and take this as our optimal nummber of cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"INERTIAS = []\nfor cluster in range(1,20):\n    model = KMeans(n_clusters = cluster, init='k-means++',\n                   n_jobs = -1, random_state=RANDOM_SEED)\n    model.fit(scaled_data[features])\n    INERTIAS.append(model.inertia_)\n\ninert_df = pd.DataFrame({'Num_Clusters':range(1,20), 'Inertia':INERTIAS})\nplt.figure(figsize=(12,6))\nsns.lineplot(data=inert_df, x=\"Num_Clusters\", y=\"Inertia\", marker='o')\nplt.ylim(0, 1200)\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on this image it looks like our elbow lies somewhere in the 3-7 region. Let's take 3 as our optimal number of cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KMeans(n_clusters=3, init='k-means++', random_state=RANDOM_SEED)\nmodel.fit(scaled_data[features])\nscaled_data['KMeans'] = model.predict(scaled_data[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='KMeans')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you might have guessed, this method is very subjective and involves a lot of eyeballing and assumptions. Which can be quite trivial and frankly we are going to encounter much more complex ploblems than this in real life. So, let's try the second method.\n### B. Silhouette Coefficient Method\nA higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n* The mean distance between a sample and all other points in the same class.\n* The mean distance between a sample and all other points in the next nearest cluster.\n\nFinally, the Total Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"SILHOUETTES = []\nfor cluster in range(2,20):\n    model = KMeans(\n        n_clusters = cluster, init='k-means++',\n        n_jobs = -1, random_state=RANDOM_SEED)\n    model.fit(scaled_data[features])\n    labels = model.labels_\n    SILHOUETTES.append(silhouette_score(\n        scaled_data[features],\n        labels, metric = 'euclidean'\n    ))\n\ninert_df = pd.DataFrame({'Num_Clusters':range(2,20), 'Silhoette':SILHOUETTES})\nplt.figure(figsize=(12,6))\nsns.lineplot(data=inert_df, x=\"Num_Clusters\", y=\"Silhoette\", marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhoette Score');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that we have a clear peak at 2, thus our optimal number of clusters as per kmeans is 2.  \nAs you can see Silhouette method is much more objective in nature and we do not have to guess the position.\n\n***NOTE:- You might be wondering why we do not set a very high number of clusters. That is because the inertia of the clusters will always keep decreasing as we increase the number of clusters until we have exactly the number of points in the dataset. But this will defy the purpose, we do not want 100% purity, but we want to group toghether SIMILAR data points.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KMeans(n_clusters=2, init='k-means++', random_state=RANDOM_SEED)\nmodel.fit(scaled_data[features])\nscaled_data['KMeans'] = model.predict(scaled_data[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='KMeans')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see the differenciating factors among various clusters are Income, Health and Life expectancy. Which happen to be key indicators of lifestyle.  \nSo based on these we can classify the nations into 'Healthy Lifestyle' and 'Unhealthy Lifestyle'.\n### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict = {\n    0 : 'Unhealthy Lifestyle',\n    1 : 'Healthy Lifestyle'\n}\n\ntrain_df['Kmeans_Prediction'] = train_df['KMeans'].map(label_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There can be nations where people earn a lot but do not spend on healthcare or vice-versa. This broad category takes care of that as well. Let's see some example of each class..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Kmeans_Prediction'] == 'Healthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Kmeans_Prediction'] == 'Unhealthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Hierarchical Clustering\nIn data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. This is an unsupervised clustering algorithm which involves creating clusters that have predominant ordering. Strategies for hierarchical clustering generally fall into two types:\n1. Agglomerative Hierarchical Clustering\n2. Divisive Hierarchical Clustering\n\nAnd unlike kMeans we have to use **dendogram method** here to determine the number of ideal clusters as per hierarchy. What is dendogram method? Let me explain:-\n\n### Dendogram\nWe can use a dendrogram to visualize the history of groupings and figure out the optimal number of clusters.\n1. Determine the largest vertical distance that doesn’t intersect any of the other clusters.\n2. Draw a horizontal line at both extremities.\n3. The optimal number of clusters is equal to the number of vertical lines going through the horizontal line.\n\n![](https://miro.medium.com/proxy/1*LBOReupihNEsI6Kot3Q6YQ.png)\nSource:- [Medium](https://towardsdatascience.com/machine-learning-algorithms-part-12-hierarchical-agglomerative-clustering-example-in-python-1e18e0075019)  \nIn this example, the ideal number of cluster will be 4. Now let's draw a similar diagram for our problem..."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(50, 20))\n_ = shc.dendrogram(shc.linkage(scaled_data[features], method='ward'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So in our case the optimal number of clusters will be **2** using the dendogram method.  \n### Agglomerative Hierarchical Clustering\nBottom up approach. Start with many small clusters and merge them together to create bigger clusters.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\nmodel.fit(scaled_data[features])\nscaled_data['Agglomerative_H'] = model.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='Agglomerative_H')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'Agglomerative_H')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict = {\n    0 : 'Healthy Lifestyle',\n    1 : 'Unhealthy Lifestyle'\n}\n\ntrain_df['Agglomerative_Prediction'] = train_df['Agglomerative_H'].map(label_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Agglomerative_Prediction'] == 'Healthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Agglomerative_Prediction'] == 'Unhealthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Affinity Propagation\n\nAffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given."},{"metadata":{"trusted":true},"cell_type":"code","source":"af = AffinityPropagation(preference=-200)\naf.fit(scaled_data[features]);\nscaled_data['Affinity'] = af.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='Affinity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'Affinity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict = {\n    1 : 'Healthy Lifestyle',\n    0 : 'Unhealthy Lifestyle'\n}\n\ntrain_df['Affinity_Prediction'] = train_df['Affinity'].map(label_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Affinity_Prediction'] == 'Healthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Affinity_Prediction'] == 'Unhealthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see Affinity propagation has made similar splits to earlier.\n\nThe best thing about Affinity propagation is that number of clusters is calculated automatically depending on the hyperparameters, so we do not have to guess the number of clusters."},{"metadata":{},"cell_type":"markdown","source":"# 4. Mean Shift\nMeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids."},{"metadata":{"trusted":true},"cell_type":"code","source":"bandwidth = estimate_bandwidth(scaled_data[features], quantile=0.2)\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(scaled_data[features])\nscaled_data['Mean Shift'] = ms.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='Mean Shift')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'Mean Shift')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Mean Shift is not working particularly well on this type of data. But nevertheless it is a very powerful algorithm and should be tried if someone is planning to approach any unsupervised problem.  \n\n# 5. Spectral Clustering\n\nSpectralClustering performs a low-dimension embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = SpectralClustering(\n    n_clusters=2, assign_labels=\"kmeans\",\n    affinity='nearest_neighbors',\n    random_state=RANDOM_SEED\n)\nsc.fit(scaled_data[features])\nscaled_data['Spectral'] = sc.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='Spectral')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'Spectral')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict = {\n    1 : 'Healthy Lifestyle',\n    0 : 'Unhealthy Lifestyle'\n}\n\ntrain_df['Spectral_Prediction'] = train_df['Spectral'].map(label_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Spectral_Prediction'] == 'Healthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['Spectral_Prediction'] == 'Unhealthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. DBSCAN\nThe DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dbs = DBSCAN(eps=1, min_samples=5)\ndbs.fit(scaled_data[features])\nscaled_data['DBSCAN'] = dbs.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='DBSCAN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'DBSCAN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DBSCAN is not performing particularly well on this dataset, but it has two major hyperparameters that can be tuned to achieve better performance. DBSCAN is a very powerful algorithm and is extensively used in unsupervised problems.  \n# 7. Gaussian Mixture Model\nThe GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"gmm = GaussianMixture(n_components=2, random_state=RANDOM_SEED)\ngmm.fit(scaled_data[features])\nscaled_data['GMM'] = gmm.predict(scaled_data[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"projected_df = project_to_2d(scaled_data, features, plot=True, cluster='GMM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_plot_cluster(train_df, scaled_data, 'GMM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict = {\n    1 : 'Healthy Lifestyle',\n    0 : 'Unhealthy Lifestyle'\n}\n\ntrain_df['GMM_Prediction'] = train_df['GMM'].map(label_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['GMM_Prediction'] == 'Healthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['GMM_Prediction'] == 'Unhealthy Lifestyle'].sample(10)['country'].to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recommendation:-  \nAs we have only 2 distinct classes in almost all of the algorithms it is very apparent from the problem statement that we need to recommend the 'Unhealthy Lifestyle' group countries for investment to the firm. As those people are in more need of the money since income and healthcare expensiture were one of the major distinguishing factors of the classification as we can see from the distribution on the pair-plots."},{"metadata":{},"cell_type":"markdown","source":"This was a quick overview/implementation example of almost all of the major unsupervised machine learning models for tabular data.  \nHope you learnt something from this notebook.  \nI will always keep updating and adding new things to this notebook as and when I come across more algorithms worth sharing. So come back for more if you liked this one...  \n\n**Also if you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}