{"cells":[{"metadata":{},"cell_type":"markdown","source":"# List of Dependencies"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"!pip install tqdm\n!pip install 'seaborn == 0.11.0'\n!pip install xgboost\n!pip install catboost\n# !pip install statsmodels\n# !pip install comet_ml\n# !pip install sklearn-genetic\n# !pip install alibi","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","lines_to_next_cell":2,"trusted":true},"cell_type":"code","source":"import time\nfrom itertools import product\nfrom math import ceil\n\nimport ast\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport re\nimport seaborn as sns\nimport tensorflow as tf\nimport warnings\nfrom catboost import CatBoostClassifier\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster import hierarchy\nfrom scipy.sparse import csr_matrix\nfrom scipy.stats import spearmanr\nfrom sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    BaggingClassifier,\n    ExtraTreesClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n)\nfrom sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    RocCurveDisplay,\n    auc,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.model_selection import (\n    RandomizedSearchCV,\n    StratifiedKFold,\n    StratifiedShuffleSplit,\n    cross_val_predict,\n    cross_validate,\n    validation_curve,\n)\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tqdm import tqdm\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42\n\n# Set random seed for Numpy and TensorFlow\ntf.random.set_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# Set the default font size of all matplotlib plots\nplt.rcParams.update({'font.size': 12})\n\n# Set the display option of pandas objects\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions below are needed to pickle objects later on to cut down total execution time."},{"metadata":{"trusted":true},"cell_type":"code","source":"PICKLE_PATH = '../input/pickles/health_insurance/'\n\n\ndef dump_objects(file_name, *objects):\n    with open(f'{file_name}.sav', 'wb') as file:\n        for obj in objects:\n            pickle.dump(obj, file)\n\n\ndef load_objects(file_name, num_objects=1):\n    objects = []\n    with open(f'{PICKLE_PATH}{file_name}.sav', 'rb') as file:\n        while num_objects > 0:\n            objects.append(pickle.load(file))\n            num_objects -= 1\n    return objects","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fetch Data"},{"metadata":{},"cell_type":"markdown","source":"The dataset is obtained from https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"PARENT_DIRECTORY = '../input/health-insurance-cross-sell-prediction'\nTRAIN_DATA_PATH = 'train.csv'\nTEST_DATA_PATH = 'test.csv'\nSAMPLE_SUBMISSION_PATH = 'sample_submission.csv'\n\n\ndef fetch_data(path, parent_dir=PARENT_DIRECTORY):\n    path = os.path.join(PARENT_DIRECTORY, path)\n    return pd.read_csv(path, low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train = fetch_data(TRAIN_DATA_PATH)\ninsurance_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_num_cat_attrs(df, y_feature, num_dtype='float64'):\n    num_attr = []\n    cat_attr = []\n\n    num_dtypes = ['float64', 'int64']\n\n    for col in list(df.columns):\n        dtype = df[col].dtype\n        if dtype in num_dtypes:\n            df[col] = df[col].astype(num_dtype)\n            num_attr.append(col)\n        else:\n            cat_attr.append(col)\n\n    if y_feature in num_attr:\n        num_attr.remove(y_feature)\n    else:\n        cat_attr.remove(y_feature)\n\n    print(f'Numerical attributes: {\", \".join(num_attr)}')\n    print(f'Categorical attributes: {\", \".join(cat_attr)}')\n\n    return df, num_attr, cat_attr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_FEATURE = 'Response'\ninsurance_train2, num_attr, cat_attr = get_num_cat_attrs(\n    insurance_train, Y_FEATURE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis & Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_subplots(cols, num_cols_per_row, fig_w, fig_h):\n    num_rows = ceil(len(cols) / num_cols_per_row)\n    indexes = list(product(range(num_rows), range(num_cols_per_row)))\n    fig, axs = plt.subplots(num_rows, num_cols_per_row)\n    fig.set_size_inches(fig_w, num_rows * fig_h)\n    return num_rows, indexes, axs, fig\n\n\ndef plot_countplot(df, cols, num_cols_per_row=4, fig_w=16,\n                   fig_h=7, rotation=0, color='steelblue'):\n    num_rows, indexes, axs, fig = create_subplots(\n        cols, num_cols_per_row, fig_w, fig_h)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/len(cols)\n\n        for idx, col in enumerate(cols):\n            ax = axs if num_cols_per_row == 1 else axs[\n                idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n            sns.countplot(y=col, data=df, color=color, ax=ax)\n            ax.set(title=col, ylabel=None)\n            ax.tick_params(axis='x', rotation=rotation)\n\n            pbar.update(progress_unit)\n    fig.tight_layout()\n    plt.show()\n    plt.close()\n\n\ndef plot_countplot_by_class(df, cols, num_cols_per_row=4, fig_w=16,\n                            fig_h=7, rotation=0, y_feature=Y_FEATURE):\n    num_rows, indexes, axs, fig = create_subplots(\n        cols, num_cols_per_row, fig_w, fig_h)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/len(cols)\n\n        for idx, col in enumerate(cols):\n            ax = axs if num_cols_per_row == 1 else axs[\n                idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n            sns.countplot(y=y_feature, hue=col, data=df, ax=ax)\n            ax.set(title=col, ylabel=None)\n            ax.tick_params(axis='x', rotation=rotation)\n\n            pbar.update(progress_unit)\n    fig.tight_layout()\n    plt.show()\n    plt.close()\n\n\ndef plot_boxplot(df, cols, num_cols_per_row=4, fig_w=16, fig_h=7):\n    num_rows, indexes, axs, fig = create_subplots(\n        cols, num_cols_per_row, fig_w, fig_h)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/len(cols)\n\n        for idx, col in enumerate(cols):\n            ax = axs if num_cols_per_row == 1 else axs[\n                idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n            sns.boxplot(y=df[col], ax=ax)\n            ax.set(title=col, ylabel=None)\n            pbar.update(progress_unit)\n    fig.tight_layout()\n    plt.show()\n    plt.close()\n\n\ndef plot_hist(df, cols, kde=True, num_cols_per_row=3, fig_w=18, fig_h=5):\n    num_rows, indexes, axs, fig = create_subplots(\n        cols, num_cols_per_row, fig_w, fig_h)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/len(cols)\n\n        for idx, col in enumerate(cols):\n            ax = axs if num_cols_per_row == 1 else axs[\n                idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n            data_range = (df[col].max() - df[col].min())\n            binwidth = data_range / 50 if data_range >= 50 else None\n            sns.histplot(data=df, x=col, kde=kde, ax=ax, binwidth=binwidth)\n            ax.set(title=col, xlabel=None)\n            pbar.update(progress_unit)\n    fig.tight_layout()\n    plt.show()\n    plt.close()\n\n\ndef plot_dendro_corr(X, feature_names, fig_w, fig_h,\n                     orientation='top', font_size=15,\n                     rotation=90):\n    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n    corr = spearmanr(X).correlation\n    corr_linkage = hierarchy.ward(corr)\n    dendro = hierarchy.dendrogram(\n        corr_linkage, labels=feature_names, ax=ax, leaf_rotation=rotation,\n        leaf_font_size=font_size, orientation=orientation\n    )\n    fig.tight_layout()\n    plt.show()\n    plt.close()\n\n\ndef plot_heatmap_corr_full(X, X_features, fig_w, fig_h, annot=False, enable_mask=True):\n\n    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n\n    corr = X[X_features].corr(method='spearman')\n    corr.index\n\n    # triu\n    if enable_mask:\n        mask = np.tril(np.ones_like(corr, dtype=bool))\n    else:\n        mask = False\n    sns.heatmap(corr, linewidths=0.1, linecolor='white',\n                square=True, annot=annot, mask=mask,\n                vmin=-1, vmax=1, center=0, ax=ax,\n                xticklabels=True,\n                yticklabels=True)\n\n    fig.tight_layout()\n    plt.tick_params(axis='both', which='minor', labelsize=15)\n    plt.show()\n    plt.close()\n\n\ndef plot_heatmap_corr(X, X_features, selected_features,\n                      fig_w, fig_h, annot=False):\n\n    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n\n    corr = X[X_features].corr(method='spearman')[\n        selected_features].drop(index=selected_features)\n    non_selected_features = corr.index\n    x_axis = selected_features\n    y_axis = non_selected_features\n    if len(y_axis) < len(x_axis):\n        corr = corr.T\n        xticklabels = non_selected_features\n        yticklabels = selected_features\n    else:\n        xticklabels = selected_features\n        yticklabels = non_selected_features\n\n    sns.heatmap(corr, linewidths=0.1, linecolor='white',\n                square=True, annot=annot,\n                vmin=-1, vmax=1, center=0, ax=ax,\n                xticklabels=True,\n                yticklabels=True)\n\n    ax.set_xticklabels(xticklabels, rotation='vertical')\n    ax.set_yticklabels(yticklabels, rotation='horizontal')\n    fig.tight_layout()\n    plt.tick_params(axis='both', which='minor', labelsize=15)\n    plt.show()\n    plt.close()\n\n\ndef plot_attr_dist_by_class_table(data, col, y_feature=Y_FEATURE):\n    classes = sorted(list(data[y_feature].value_counts().index))\n    x = list(data[col].value_counts().index)\n    result = []\n\n    for class_ in classes:\n        data_tmp = data.loc[data[y_feature] == class_, :]\n        value_count = data_tmp[col].value_counts(normalize=True)\n        result.append(value_count)\n\n    return pd.DataFrame(np.array(result), index=classes, columns=[str(i)+'_value' for i in x])\n\n\ndef compare_distribution_by_class(df, cols, num_cols_per_row=4, y_feature=Y_FEATURE,\n                                  fig_w=16, fig_h=7, rotation=0, color='steelblue'):\n    num_rows, indexes, axs, fig = create_subplots(\n        cols, num_cols_per_row, fig_w, fig_h)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/len(cols)\n\n        for idx, col in enumerate(cols):\n            ax = axs if num_cols_per_row == 1 else axs[\n                idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n            sns.boxplot(x=Y_FEATURE, y=col, data=df, ax=ax)\n            ax.set(title=col, ylabel=None)\n            ax.tick_params(axis='x', rotation=rotation)\n\n            pbar.update(progress_unit)\n    fig.tight_layout()\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_countplot(insurance_train2, [Y_FEATURE],\n               num_cols_per_row=1, fig_w=5, fig_h=4, rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_countplot_by_class(insurance_train2, cat_attr +\n                        [num_attr[2], num_attr[4]], num_cols_per_row=3, fig_w=15, fig_h=4, rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train2['Driving_License'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if the all id values are unique to ensure all records are unique."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(insurance_train2['id'].value_counts().index) == len(insurance_train2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop *id* columns since it is irrelevant."},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train2 = insurance_train2.drop(columns='id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attr = ['Age', 'Region_Code', 'Policy_Sales_Channel',\n        'Vintage', 'Annual_Premium']\nplot_hist(insurance_train2, attr, num_cols_per_row=3, fig_w=12, fig_h=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since *Annual_Premium* contains many outliers, we are going to use [robust scaler from scikit-learn API](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)\nto standardize the feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_distribution_by_class(insurance_train2, cols=['Vintage', 'Age', 'Annual_Premium'],\n                              num_cols_per_row=3, fig_w=12, fig_h=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Wrangling"},{"metadata":{},"cell_type":"markdown","source":"Convert *Gender* to *Gender_female*, *Vechicle_Damage* to binary values of 0 or 1.\nIt is because *Gender_female* is negatively correlated with *Gender_male*, therefore we\nonly need one of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"index = insurance_train2['Gender'] == 'Male'\ninsurance_train2['Gender_female'] = 1.0\ninsurance_train2.loc[index, 'Gender_female'] = 0.0\n\nprint(insurance_train2['Gender'].value_counts(dropna=False))\nprint(insurance_train2['Gender_female'].value_counts(dropna=False))\n\ninsurance_train2 = insurance_train2.drop(columns='Gender', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = insurance_train2['Vehicle_Damage'] == 'Yes'\ninsurance_train2.loc[~index, 'Vehicle_Damage'] = 0.0\ninsurance_train2.loc[index, 'Vehicle_Damage'] = 1.0\ninsurance_train2['Vehicle_Damage'] = insurance_train2['Vehicle_Damage'].astype(\n    'float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for null values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train2 = insurance_train2.copy()\nnp.any(insurance_train2.isna(), axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical Independence Test"},{"metadata":{},"cell_type":"markdown","source":"Reassign the types of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attr = ['Age', 'Vintage']\nnum_attr_with_outliers = ['Annual_Premium']\nbinary_cat_attr = ['Driving_License', 'Gender_female',\n                   'Vehicle_Damage', 'Previously_Insured']\ncat_attr = ['Policy_Sales_Channel', 'Vehicle_Age', 'Region_Code']\nY_FEATURE = 'Response'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chi-Squared Test"},{"metadata":{},"cell_type":"markdown","source":"<p style='line-indent:5.0%;line-height:2.0;text-align:justify;'>\nThis score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes.\n</p>\n<p style='line-indent:5.0%;line-height:2.0;text-align:justify;'>\nThe chi-square test measures dependence between stochastic variables, so using this function \"weeds out\" the features that are the most likely to be independent of class and therefore irrelevant for classification.\n</p>\n\n[Source](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)"},{"metadata":{},"cell_type":"markdown","source":"Get the one-hot-encoded categorical variables to perform chi-squared test."},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train2[cat_attr] = insurance_train2[cat_attr].astype('object')\ninsurance_train2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train3 = pd.get_dummies(insurance_train2.copy(), columns=cat_attr)\ninsurance_train3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_attr = insurance_train3.columns.tolist()\none_hot_encoded_attr = [list(filter(lambda x: re.match(\n    f'^{attr}', x), preprocessed_attr)) for attr in cat_attr]\none_hot_encoded_attr = sum(one_hot_encoded_attr, [])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate the chi-squared values between categorical features and y features."},{"metadata":{"trusted":true},"cell_type":"code","source":"DECIMAL_PLACE = 5\nattr = binary_cat_attr+one_hot_encoded_attr\nX = insurance_train3[attr]\ny = insurance_train3[Y_FEATURE]\n\nchi2_values, chi2_p_values = chi2(X, y)\n\nchi2_result = zip(chi2_values, chi2_p_values)\nchi2_result = pd.DataFrame(chi2_result, index=attr, columns=[\n                           'chi2_value', 'p_value'])\nchi2_result['chi2_value'] = chi2_result['chi2_value'].apply(\n    lambda x: round(x, DECIMAL_PLACE - 3))\nchi2_result['p_value'] = chi2_result['p_value'].apply(\n    lambda x: round(x, DECIMAL_PLACE))\nchi2_result = chi2_result.sort_values('chi2_value', ascending=False)\nchi2_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The categorical features with p-value of more than 1% is consider as independent of y feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"p_value_cutoff = 0.01\ninsignificant = chi2_result['p_value'] > p_value_cutoff\nchi2_result_low_p_val = chi2_result[insignificant].sort_values('p_value')\nchi2_result_low_p_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"less_significant_features = chi2_result_low_p_val.index.tolist()\nprint(\n    f'We have eliminated {len(less_significant_features)} features with low chi-squared value.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ANOVA f-test"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0;\">\nANOVA is used when one variable is numeric and one is categorical, such as numerical input variables and a classification target variable in a classification task.\n</p>\n<p style=\"text-align: justify; line-height: 2.0;\">\nThe results of this test can be used for feature selection where those features that are\nindependent of the target variable can be removed from the dataset.\n</p>\n<p style=\"text-align: justify; line-height: 2.0;\">\nOur p-value cutoff is 0.01.\n</p>\n<p style=\"text-align: justify; line-height: 2.0;\">\nAnalysis of variance (ANOVA) uses F-tests to statistically assess the equality of means for two or more groups.\nBased on the result, Age and Annual Premium has difference of means in the two groups which are statistically significant.\n</p>\n<p style=\"text-align: justify; line-height: 2.0;\">\nSince Vintage has very low f_value and high p_value, we are going to remove this feature.\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"attr = num_attr + num_attr_with_outliers\nX = insurance_train3[attr]\ny = insurance_train3[Y_FEATURE]\n\nanova_f_values, anova_p_values = f_classif(X, y)\n\nanova_result = zip(anova_f_values, anova_p_values)\nanova_result = pd.DataFrame(\n    anova_result, index=attr, columns=['f_value', 'p_value'])\nanova_result['f_value'] = anova_result['f_value'].apply(\n    lambda x: round(x, DECIMAL_PLACE - 3))\nanova_result['p_value'] = anova_result['p_value'].apply(\n    lambda x: round(x, DECIMAL_PLACE))\nanova_result = anova_result.sort_values('f_value', ascending=False)\nanova_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"less_significant_features.append('Vintage')\nnum_attr.remove('Vintage')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train4 = insurance_train3.drop(\n    columns=less_significant_features, axis=1)\ninsurance_train4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_with_output(data, y_feature=Y_FEATURE,\n                            method='pearson'):\n    cor = insurance_train2.corr(method=method)\n    cor_target = abs(cor[Y_FEATURE])\n    return cor_target.sort_values(ascending=False)[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every features are not quite correlated with each other since all of them have low Spearman's correlation coefficient (< 0.5)."},{"metadata":{"trusted":true},"cell_type":"code","source":"attr = num_attr + num_attr_with_outliers\ncorrelation_with_output(insurance_train4[attr], method='spearman')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass DataCleaner(BaseEstimator, TransformerMixin):\n\n    def __init__(self, features_to_be_dropped=None):\n        self.features_to_be_dropped = features_to_be_dropped\n\n    def fit(self, data, y=None):\n        return self\n\n    def transform(self, data, y=None):\n\n        data = data.copy()\n        data = data.reset_index(drop=True)\n        data = data.drop(columns='id', axis=1)\n\n        index = data['Gender'] == 'Male'\n        data['Gender_female'] = 1.0\n        data.loc[index, 'Gender_female'] = 0.0\n\n        data = data.drop(columns='Gender', axis=1)\n\n        index = data['Vehicle_Damage'] == 'Yes'\n        data.loc[~index, 'Vehicle_Damage'] = 0.0\n        data.loc[index, 'Vehicle_Damage'] = 1.0\n        data['Vehicle_Damage'] = data['Vehicle_Damage'].astype('float64')\n\n        num_attr = ['Age', 'Vintage']\n        binary_cat_attr = ['Driving_License', 'Gender_female',\n                           'Vehicle_Damage', 'Previously_Insured']\n        cat_attr = ['Policy_Sales_Channel', 'Vehicle_Age', 'Region_Code']\n        Y_FEATURE = 'Response'\n\n        data[cat_attr] = data[cat_attr].astype('object')\n\n        data = pd.get_dummies(data.copy(), columns=cat_attr)\n\n        preprocessed_attr = data.columns.tolist()\n        one_hot_encoded_attr = [list(filter(lambda x: re.match(\n            f'^{attr}', x), preprocessed_attr)) for attr in cat_attr]\n        one_hot_encoded_attr = sum(one_hot_encoded_attr, [])\n\n        DECIMAL_PLACE = 5\n        attr = binary_cat_attr+one_hot_encoded_attr\n        X = data[attr]\n        y = data[Y_FEATURE]\n\n        chi2_values, chi2_p_values = chi2(X, y)\n\n        chi2_result = zip(chi2_values, chi2_p_values)\n        chi2_result = pd.DataFrame(chi2_result, index=attr, columns=[\n                                   'chi_value', 'p_value'])\n        chi2_result['chi_value'] = chi2_result['chi_value'].apply(\n            lambda x: round(x, DECIMAL_PLACE - 3))\n        chi2_result['p_value'] = chi2_result['p_value'].apply(\n            lambda x: round(x, DECIMAL_PLACE))\n        chi2_result = chi2_result.sort_values('chi_value', ascending=False)\n\n        p_value_cutoff = 0.01\n        insignificant = chi2_result['p_value'] > p_value_cutoff\n        chi2_result_low_p_val = chi2_result[insignificant].sort_values(\n            'p_value')\n        chi2_result_low_p_val\n\n        less_significant_features = chi2_result_low_p_val.index.tolist()\n\n        less_significant_features.append('Vintage')\n        num_attr.remove('Vintage')\n\n        data = data.drop(columns=less_significant_features, axis=1)\n\n        return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataPreprocessor():\n\n    def __init__(self, data, num_attr, cat_attr, y_feature,\n                 binary_cat_attr=None, num_attr_with_outliers=None):\n        self.num_attr = num_attr\n        self.cat_attr = cat_attr\n        self.y_feature = y_feature\n        self.binary_cat_attr = binary_cat_attr\n        self.num_attr_with_outliers = num_attr_with_outliers\n        self.data = data\n        if cat_attr is not None:\n            self.generate_1_hot_enc(self.data)\n        else:\n            self.one_hot_enc = None\n\n    def get_1_hot_enc(self):\n        return self.one_hot_enc\n\n    def generate_1_hot_enc(self, data):\n        # Generate one-hot-encoded feature's names\n        index = np.any(pd.isnull(data[self.cat_attr]), axis=1)\n        X_cat = data.loc[~index, self.cat_attr].copy()\n        one_hot_enc = OneHotEncoder()\n        one_hot_enc.fit(X_cat)\n        self.one_hot_enc = one_hot_enc\n\n    def get_1_hot_attr(self):\n        return self.one_hot_enc.get_feature_names(self.cat_attr)\n\n    def generate_preprocessed_attr(self):\n        one_hot_attr = self.get_1_hot_attr()\n        return self.num_attr + self.num_attr_with_outliers + \\\n            self.binary_cat_attr + list(one_hot_attr)\n\n    def preprocessX(self, X):\n        X_train_preprocessed = self.make_preprocess_pipeline().fit_transform(X)\n        preprocessed_attr = self.generate_preprocessed_attr()\n        if type(X_train_preprocessed) == csr_matrix:\n            X_train_preprocessed = X_train_preprocessed.toarray()\n        X_train_preprocessed = pd.DataFrame(\n            X_train_preprocessed, columns=preprocessed_attr)\n        return X_train_preprocessed\n\n    def preprocessX_without_standardization(self, X):\n        X_train_preprocessed = self.make_preprocess_pipeline2().fit_transform(X)\n        preprocessed_attr = self.generate_preprocessed_attr()\n        if type(X_train_preprocessed) == csr_matrix:\n            X_train_preprocessed = X_train_preprocessed.toarray()\n        X_train_preprocessed = pd.DataFrame(\n            X_train_preprocessed, columns=preprocessed_attr)\n        return X_train_preprocessed\n\n    # Data Preprocessing Pipeline\n    def make_preprocess_pipeline(self):\n\n        numerical_pipeline = make_pipeline(\n            SimpleImputer(strategy='mean'),\n            StandardScaler()\n        )\n\n        robust_numerical_pipeline = make_pipeline(\n            SimpleImputer(strategy='most_frequent'),\n            RobustScaler()\n        )\n\n        binary_categorical_pipeline = make_pipeline(\n            SimpleImputer(strategy='most_frequent')\n        )\n\n        if self.one_hot_enc is None:\n            categories = None\n        else:\n            categories = self.one_hot_enc.categories_\n\n        categorical_pipleline = make_pipeline(\n            SimpleImputer(strategy='most_frequent'),\n            OneHotEncoder(categories=categories)\n        )\n\n        pipeline = []\n\n        if self.num_attr is not None:\n            pipeline.append(('num_pp', numerical_pipeline, self.num_attr))\n\n        if self.num_attr_with_outliers is not None:\n            pipeline.append(\n                ('robust_num_pp', robust_numerical_pipeline, self.num_attr_with_outliers))\n\n        if self.binary_cat_attr is not None:\n            pipeline.append(\n                ('bin_cat_pp', binary_categorical_pipeline, self.binary_cat_attr))\n\n        if self.cat_attr is not None:\n            pipeline.append(('cat_pp', categorical_pipleline, self.cat_attr))\n\n        preprocess_pipeline = ColumnTransformer(\n            pipeline, remainder='passthrough')\n\n        return preprocess_pipeline\n\n    def make_preprocess_pipeline2(self):\n\n        numerical_pipeline = make_pipeline(\n            SimpleImputer(strategy='mean')\n        )\n\n        robust_numerical_pipeline = make_pipeline(\n            SimpleImputer(strategy='most_frequent')\n        )\n\n        binary_categorical_pipeline = make_pipeline(\n            SimpleImputer(strategy='most_frequent')\n        )\n\n        if self.one_hot_enc is None:\n            categories = None\n        else:\n            categories = self.one_hot_enc.categories_\n\n        categorical_pipleline = make_pipeline(\n            SimpleImputer(strategy='most_frequent'),\n            OneHotEncoder(categories=categories)\n        )\n\n        preprocess_pipeline = make_column_transformer(\n            (numerical_pipeline, self.num_attr),\n            (robust_numerical_pipeline, self.num_attr_with_outliers),\n            (binary_categorical_pipeline, self.binary_cat_attr),\n            (categorical_pipleline, self.cat_attr),\n            remainder='passthrough')\n\n        return preprocess_pipeline\n\n    # Training pipeline\n    def make_training_pipeline(self, ml_model):\n\n        training_pipeline = make_pipeline(\n            self.make_preprocess_pipeline(),\n            ml_model\n        )\n\n        return training_pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataPreparator(BaseEstimator, TransformerMixin):\n\n    def __init__(self, y_feature, test_size=0.20, suppress_print=False):\n        self.num_attr = []\n        self.cat_attr = []\n        self.y_feature = y_feature\n        self.test_size = test_size\n        self.suppress_print = suppress_print\n\n    def get_num_attr(self):\n        return self.num_attr\n\n    def get_cat_attr(self):\n        return self.cat_attr\n\n    def get_num_cat_attrs(self, num_dtype='float64'):\n\n        num_dtypes = ['float64', 'int64']\n\n        for col in list(self.data.columns):\n            dtype = self.data[col].dtype\n            if dtype == 'float32':\n                self.num_attr.append(col)\n            elif dtype in num_dtypes:\n                self.data[col] = self.data[col].astype(num_dtype)\n                self.num_attr.append(col)\n            else:\n                self.cat_attr.append(col)\n\n        if self.y_feature in self.num_attr:\n            self.num_attr.remove(self.y_feature)\n        else:\n            self.cat_attr.remove(self.y_feature)\n\n        if not self.suppress_print:\n            print(f'Numerical attributes: {self.num_attr}')\n            print(f'Categorical attributes: {self.cat_attr}')\n\n    def shuffle_data(self):\n        self.data = self.data.iloc[np.random.permutation(\n            len(self.data))].reset_index(drop=True)\n\n    def split_data(self, test_size=None, combine_X_and_y=False):\n\n        if test_size is None:\n            test_size = self.test_size\n\n        X = self.data.drop(columns=self.y_feature, axis=1)\n        y = self.data[self.y_feature]\n\n        split = StratifiedShuffleSplit(test_size=test_size,\n                                       random_state=RANDOM_SEED)\n\n        train_index, test_index = next(split.split(X, y))\n        X_train, X_test, y_train, y_test = \\\n            X.iloc[train_index], X.iloc[test_index], y[train_index], y[test_index]\n\n        print(f'Train X   : {X_train.shape}')\n        print(f'Train y   : {y_train.shape}')\n        print(f'Test X   : {X_test.shape}')\n        print(f'Test y   : {y_test.shape}')\n\n        if combine_X_and_y:\n            train = pd.concat([X_train, y_train], axis=1)\n            test = pd.concat([X_test, y_test], axis=1)\n            return train, test\n\n        return X_train, X_test, y_train, y_test\n\n    def fit(self, data, y=None):\n        self.data = data.copy()\n        self.shuffle_data()\n        self.get_num_cat_attrs()\n        return self\n\n    def transform(self, data=None, y=None):\n\n        X_train, X_test, y_train, y_test = self.split_data()\n\n        return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection"},{"metadata":{},"cell_type":"markdown","source":"## Preparing Data"},{"metadata":{},"cell_type":"markdown","source":"\nWe are only going to train on 10% of the dataset to save time,\nsince overwhelmingly large samples will not improve models' performance in\nany significant manner."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning data\nfinal_insurance_train = DataCleaner().fit_transform(insurance_train.copy())\nfinal_insurance_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_train2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_attr = ['Region_Code', 'Vehicle_Age', 'Policy_Sales_Channel']\npreprocessed_attr = final_insurance_train.columns.tolist()\none_hot_encoded_attr = [list(filter(lambda x: re.match(\n    f'^{attr}', x), preprocessed_attr)) for attr in cat_attr]\none_hot_encoded_attr = sum(one_hot_encoded_attr, [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attr = ['Age']\nnum_attr_with_outliers = ['Annual_Premium']\nbinary_cat_attr = ['Gender_female', 'Vehicle_Damage',\n                   'Previously_Insured']+one_hot_encoded_attr\ncat_attr = None\nY_FEATURE = 'Response'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_insurance_train.drop(\n    columns=one_hot_encoded_attr, axis=1).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test split\nX_train, X_test, y_train, y_test = DataPreparator(\n    Y_FEATURE, test_size=0.90).fit_transform(final_insurance_train)\n\n# Preparing training pipeline\npreprocessor = DataPreprocessor(final_insurance_train, num_attr, cat_attr, Y_FEATURE,\n                                binary_cat_attr, num_attr_with_outliers)\ntraining_pipeline = preprocessor.make_training_pipeline\n\nX_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Classifiers"},{"metadata":{},"cell_type":"markdown","source":"We are going to omit machine learning algorithms like\nK Nearest Neighbors and Support Vector Machines since the\ntraining samples are just too big."},{"metadata":{"trusted":true},"cell_type":"code","source":"short_names = ['log_reg', 'neural_network', 'decision_tree', 'rand_forest',\n               'extra_tree', 'ada_boost_cf', 'gradient_b_cf', 'bagging_cf',\n               'catboost_cf', 'xg_boost']\n\nnames = ['Logistic Classifier', 'Multi-layer Perceptron classifier',\n         'Decision Tree', 'Random forest Classifier', 'Extra Tree Classifier',\n         'AdaBoost Classifier', 'Gradient Boosting Classifier',\n         'Bagging Classifier', 'CatBoost Classifier', 'XGBClassifier']\n\nfunctions = [\n    LogisticRegression(random_state=RANDOM_SEED, n_jobs=-1, max_iter=1000),\n    MLPClassifier(random_state=RANDOM_SEED, early_stopping=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n    ExtraTreesClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n    AdaBoostClassifier(random_state=RANDOM_SEED),\n    GradientBoostingClassifier(random_state=RANDOM_SEED),\n    BaggingClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n    CatBoostClassifier(random_seed=RANDOM_SEED, silent=True),\n    XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1)\n]\n\nclassifiers_idx = {}\nclassifiers = {}\n\n# Zip all classfiers together into a dictionary for convenient access\nfor idx, s_name, name, func in zip(range(len(names)), short_names, names, functions):\n    classifiers_idx[idx] = {'name': name, 'func': func}\n    classifiers[s_name] = {'name': name, 'func': func}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Phase 1: Performance Measure"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_models_performance(models, X, y, training_pipeline, n_splits,\n                           scoring_metrics, random_state=RANDOM_SEED):\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    cv = StratifiedKFold(n_splits=n_splits,\n                         shuffle=True, random_state=random_state)\n\n    mean_cols = []\n    std_cols = []\n    for name in ['train', 'test']:\n        mean_cols += [f'{name}_{metric}' for metric in scoring_metrics]\n        std_cols += [f'{name}_{metric}_std' for metric in scoring_metrics]\n    cols = mean_cols + std_cols\n\n    results = {'model_name': [], 'duration': []}\n\n    for col in cols:\n        results[col] = []\n\n    # Loop through all models\n    for idx in range(len(models)):\n        cf_name = models[idx]['name']\n\n        print(f'{cf_name} has started...')\n        # Count time to get the duration of the models\n        start = time.time()\n\n        ml_pipeline = training_pipeline(clone(models[idx]['func']))\n        # cross_validate returns both train_score and test_score by setting return_train_score to True\n        cv_scores = cross_validate(ml_pipeline, X, y,\n                                   scoring=scoring_metrics, cv=cv,\n                                   return_train_score=True)\n\n        end = time.time()\n        duration = end - start\n        print(f'{cf_name} ended in {duration} seconds.\\n')\n\n        updateRecord(results, cv_scores, mean_cols, std_cols,\n                     cf_name, duration, scoring_metrics)\n\n    # Return as DataFrame instead of dictionary\n    return pd.DataFrame(results)\n\n# Append values to the dictionary based on key_name passed into the function\n\n\ndef updateRecord(df, scores, mean_cols, std_cols, model_name, duration, scoring_metrics):\n    df['model_name'].append(model_name)\n    df['duration'].append(duration)\n    for mean_col, std_col in zip(mean_cols, std_cols):\n        df[mean_col].append(np.mean(scores[mean_col]))\n        df[std_col].append(np.std(scores[mean_col]))\n\n\ndef sortValues(df, cols, sort_idx, ascending=False):\n    df = df.copy()\n    try:\n        cols.remove('model_name')\n        cols.remove('duration')\n    except ValueError:\n        pass\n    regex = '(?:^.+)(_after|_before)$'\n    for col in cols:\n        match = re.search(regex, col)\n        if not match:\n            col_std = f'{col}_std'\n        elif match.group(1) == '_before':\n            col_std = f'{col[:-7]}_std_before'\n        else:\n            col_std = f'{col[:-6]}_std_after'\n        df[col] = df[col].astype('float64')\n        df[col_std] = df[col_std].astype('float64')\n        def display(row): return f'{row[0]:.4f} +/-{row[1]:.4f}'\n        df[col] = df[[col, col_std]].apply(display, axis=1)\n    cols = np.array(cols)\n    sort_cols = list(cols[sort_idx]) if isinstance(sort_idx, list) \\\n        else [cols[sort_idx]]\n    df = df[['model_name'] +\n            list(cols)].sort_values(sort_cols, ascending=ascending)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results are loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nwarnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n\nscoring_metrics = ['f1', 'roc_auc', 'precision', 'recall']\nperformance_results = get_models_performance(classifiers_idx, X_train, y_train,\n                                             training_pipeline, 5, scoring_metrics)\n\ndump_objects('performance', performance_results)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n\nscoring_metrics = ['f1', 'roc_auc', 'precision', 'recall']\n\n[performance_results] = load_objects('performance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Models like Gradient Boosting Classifier, AdaBoost Classifier, Multi-layer Perceptron Classifier** perform and generalise well.\n\nHowever, **Random Forest Classifier, Extra Tree Classifier, Bagging Classifier and Decision Tree** severely\noverfit the train dataset.\n\nSo we try if increasing train size from 10% to 50% will improve these classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"roc = ['train_roc_auc', 'test_roc_auc']\nsortValues(performance_results, roc, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2_train, X2_test, y2_train, y2_test = DataPreparator(\n    Y_FEATURE, test_size=0.50).fit_transform(final_insurance_train)\n\nclassifiers_idx2 = {}\nfor idx, key in enumerate([2, 3, 4, 7]):\n    classifiers_idx2[idx] = classifiers_idx[key]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results are loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nwarnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n\nperformance_results2 = get_models_performance(classifiers_idx2, X2_train, y2_train,\n                                              training_pipeline, 5, scoring_metrics)\n\ndump_objects('performance2', performance_results2)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"[performance_results2] = load_objects('performance2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are quite depressing haha. Well, we stick with train size of 10%."},{"metadata":{"trusted":true},"cell_type":"code","source":"roc = ['train_roc_auc', 'test_roc_auc']\nsortValues(performance_results2, roc, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0;\">\nI will choose Logistic Classifier, XGB Classifier, Random Forest Classifier.\nBesides, I also going to tune XGB Classfier and Random Forest Classifier since they have higher tendencies to\noutperform most models after some hyperparameter tweaking.\n</p>"},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0;\">\nI would like to explore some of the feature selection method available in Scikit-Learn API.\nI want to know if these methods are useful for dimensionality reduction without sacrificing too much on\noverall models' performance.\n</p>\n\nTwo popular feature selection techniques that can be used for numerical input data and a categorical (class) target variable:\n- ANOVA F-test\n- Mutual Information Statistics\n\nSince we have already try ANOVA F-test just now, we will try Mutual Information Statistics now."},{"metadata":{},"cell_type":"markdown","source":"Estimate mutual information for a discrete target variable.\n\n<p style=\"text-align: justify; line-height: 2.0;\">\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n</p>\n\n[Source](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html)"},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results are loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nfrom sklearn.feature_selection import mutual_info_classif\n\nresults = mutual_info_classif(X_train, y_train, discrete_features='auto', n_neighbors=3, random_state=RANDOM_SEED)\nresult_arr = np.column_stack((list(X_train.columns), results))\n\nMI = 'Mutual Information'\nMI_results = pd.DataFrame(result_arr, columns=['col_name', MI])\ndump_objects('MI_results', MI_results)\n```"},{"metadata":{},"cell_type":"markdown","source":"We try and see if removing features with value 0 in mutual information will help in\nreducing model complexity without introducing too much bias to our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"MI = 'Mutual Information'\n[MI_results] = load_objects('MI_results')\n\nMI_results[MI] = MI_results[MI].astype('float64')\nMI_useful = MI_results.loc[MI_results[MI] != 0.0]\nMI_useful = MI_useful.sort_values(MI, ascending=False).reset_index(drop=True)\nprint(f'Total of useful features: {len(MI_useful)}.')\nMI_useful.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_features = list(MI_useful['col_name'])\nfinal_insurance_train2 = final_insurance_train[useful_features+[\n    Y_FEATURE]].copy()\n\n# Train test split\ndata_prep2 = DataPreparator(Y_FEATURE, test_size=0.90)\nX2_train, X2_test, y2_train, y2_test = data_prep2.fit_transform(\n    final_insurance_train2)\nnum_attr2, cat_attr2 = data_prep2.get_num_attr(), data_prep2.get_cat_attr()\n\nnum_attr2.remove('Annual_Premium')\nnum_attr_with_outliers2 = ['Annual_Premium']\n\npreprocessor2 = DataPreprocessor(final_insurance_train2, num_attr2, None, Y_FEATURE,\n                                 cat_attr2, num_attr_with_outliers2)\ntraining_pipeline2 = preprocessor2.make_training_pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results are loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nperformance_results3 = get_models_performance(classifiers_idx, X2_train, y2_train,\n                                             training_pipeline2, 5, scoring_metrics)\n\ndump_objects('performance3', performance_results3)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"[performance_results3] = load_objects('performance3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm, by comparing the *performance_result3* with the *performance_result*, we notice a little bit of performance boost.\n\n<p style=\"text-align: justify; line-height: 2.0;\">\nSo, I am going to choose Gradient Boosting Classifier, Multi-layer Perceptron classifier, Logistic Classifier,\n    XGB Classifier, and Random Forest Classifier.\nBesides, I also going to tune XGB Classfier and Random Forest Classifier since they have higher tendencies to\noutperform most models after some hyperparameter tweaking.\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sortValues(performance_results3, roc, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortValues(performance_results, roc, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reset train and test set since we found a better combination of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_features = list(MI_useful['col_name'])\nfinal_insurance_train = final_insurance_train[useful_features+[\n    Y_FEATURE]].copy()\n\n# Train test split\ndata_prep = DataPreparator(Y_FEATURE, test_size=0.90)\nX_train, X_test, y_train, y_test = data_prep.fit_transform(\n    final_insurance_train)\nnum_attr, cat_attr = data_prep.get_num_attr(), data_prep.get_cat_attr()\n\npreprocessor = DataPreprocessor(final_insurance_train, num_attr, None, Y_FEATURE,\n                                cat_attr, num_attr_with_outliers)\ntraining_pipeline = preprocessor.make_training_pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyparameter Tunning"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-indent: 5.0%; text-align: justify; line-height: 2.0;\">\nWe are going to use randomized search instead of grid search since it takes too much time to go through all possibilities.\n</p>\n\n<p style=\"text-indent: 5.0%; text-align: justify; line-height: 2.0;\">\nWe are going to choose the best combination of hyperparameters sorted by the num_trials, mean_cv_score and mean_test_score. Assuming X_train and y_train is the intial training set we feed into the algorithm, the algorithm will perform nested cross validated randomized search by splitting X_train and y_train into k1-outer-fold X_outer_train and y_outer_train. Then, the algorithm will split X_outer_train and y_outer_train into k2-inner-fold X_inner_train and y_inner_train. num_trials is the number of times a combination of hyparameters explored by the randomized search during the k2-inner-fold sets. mean_test_score is the average test score for each k2-inner-fold sets for each hyperparameters combination. mean_cv_score is the average cross validated score across all k1-outer-fold sets for each hyperparameters combination.\n</p>\n\n<p style=\"text-indent: 5.0%; text-align: justify; line-height: 2.0;\">\nTo put it simply, k2-inner-fold sets are used to find the hyperparameter's combination of the best estimator, while the k1-outer fold sets are used to evaluate that best estimator with that hyperparameter's combination. The purpose is to prevent the randomized search from producing overly optimistic results, which cause the model to overfit the original training set and does not generalize well to real-world data with different variations and distributions.\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def nested_cv_param_search(training_pipeline, param_grid, X, y,\n                           n_iter, scoring, n_outer_splits, n_inner_spits,\n                           random_state=RANDOM_SEED):\n\n    cv_outer = StratifiedKFold(n_splits=n_outer_splits, shuffle=False)\n    cv_inner = StratifiedKFold(n_splits=n_inner_spits, shuffle=True,\n                               random_state=random_state)\n\n    outer_roc_score = list()\n    inner_roc_score = list()\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 75/(n_outer_splits)\n\n        for train_ix, test_ix in cv_outer.split(X, y):\n            X_train, X_test = X.loc[train_ix, :], X.loc[test_ix, :]\n            y_train, y_test = y[train_ix], y[test_ix]\n\n            search = RandomizedSearchCV(training_pipeline, param_grid, n_iter=n_iter,\n                                        scoring=scoring, cv=cv_inner, refit=True)\n            result = search.fit(X_train, y_train)\n\n            inner_roc_score.append(result.cv_results_)\n\n            best_model = result.best_estimator_\n            y_test_pred = best_model.predict(X_test)\n            roc_score = roc_auc_score(y_test, y_test_pred)\n            outer_roc_score.append(roc_score)\n\n            pbar.update(progress_unit)\n\n        features = ['params', 'mean_test_score', 'std_test_score']\n        base = pd.DataFrame()\n\n        for roc_score in inner_roc_score:\n            roc_score = pd.DataFrame(roc_score)[features]\n            base = base.append(roc_score, ignore_index=True)\n\n        base['params'] = base['params'].astype('str')\n        agg_mean = base.groupby('params')['mean_test_score']\n        new_df = {'mean_test_score': agg_mean.mean(),\n                  'std_test_score': agg_mean.std(), 'num_trials': agg_mean.count()}\n        param_result = pd.DataFrame(new_df).reset_index()\n\n        param_result['mean_cv_score'] = 0\n        param_result['std_cv_score'] = 0\n\n        params = list(param_result['params'].value_counts().index)\n        progress_unit = 25/(n_outer_splits * len(params))\n\n        for param in params:\n            outer_roc_score = []\n            for train_ix, test_ix in cv_outer.split(X, y):\n                X_train, X_test = X.loc[train_ix, :], X.loc[test_ix, :]\n                y_train, y_test = y[train_ix], y[test_ix]\n\n                training_pipeline.set_params(**ast.literal_eval(param))\n                result = training_pipeline.fit(X_train, y_train)\n\n                y_test_probas = training_pipeline.predict_proba(X_test)\n                roc_score = roc_auc_score(y_test, y_test_probas[:, -1])\n                outer_roc_score.append(roc_score)\n\n                pbar.update(progress_unit)\n\n            mean = np.mean(outer_roc_score)\n            std = np.std(outer_roc_score)\n            index = list(param_result['params'] == param).index(True)\n            param_result.loc[index, 'mean_cv_score'] = mean\n            param_result.loc[index, 'std_cv_score'] = std\n\n    outer_roc_score = pd.DataFrame(\n        outer_roc_score, columns=[f'{scoring}_score'])\n    return param_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style='text-align:justify;line-height:2.0;'>\nWe are going to manually search a reasonable range of hyperparameters by plotting the validation curve for each hyperparameters.\nBy doing so, we are able to reduce the burden of randomized search and produce more optimal results.\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_validation_curve(model, X, y, param_name, param_range,\n                          cv=3, scoring='roc_auc', figsize=(6, 6),\n                          random_state=RANDOM_SEED):\n\n    cv = StratifiedKFold(n_splits=cv, shuffle=True,\n                         random_state=random_state)\n\n    # Compute train and test scores for an estimator with different values of the specified parameter\n    train_score, test_score = validation_curve(model, X, y, param_name=param_name,\n                                               param_range=param_range, cv=cv, scoring=scoring,\n                                               verbose=1, n_jobs=-1)\n    avg_train_score = train_score.mean(axis=1)\n    avg_test_score = test_score.mean(axis=1)\n\n    fig, ax = plt.subplots(figsize=figsize)\n    # Adjust settings for the plot (eg. set title of the plot)\n    ax.plot(param_range, avg_train_score, label=\"Training Score\")\n    ax.plot(param_range, avg_test_score, label=\"Cross-Validation Score\")\n    ax.set(xlabel=param_name, ylabel=scoring,\n           title=f'Cross Validation Curve {scoring} against {param_name}')\n    ax.legend(loc=\"best\")\n    ax.grid()\n    plt.show()\n\n    # Get the index position of the value of the specified parameter,\n    # when it produces the highest test_score\n    max_val_index = np.argsort(avg_test_score)[-1]\n\n    # Print the value of the specified parameter with highest test_score\n    print(f'{param_name} with value {param_range[max_val_index]}' +\n          f' generates the highest CV score: {avg_test_score[max_val_index]:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 1: Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_forest = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'randomforestclassifier__n_estimators'\nrand_forest_param = [10, 50, 100, 150, 200]\n\nplot_validation_curve(training_pipeline(rand_forest),\n                      X_train, y_train, key, rand_forest_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'randomforestclassifier__max_depth'\nrand_forest_param = [10, 30, 50, 70, 90]\n\nplot_validation_curve(training_pipeline(rand_forest),\n                      X_train, y_train, key, rand_forest_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'randomforestclassifier__min_samples_split'\nrand_forest_param = [10, 100, 300, 500, 700, 900]\n\nplot_validation_curve(training_pipeline(rand_forest),\n                      X_train, y_train, key, rand_forest_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'randomforestclassifier__min_samples_leaf'\nrand_forest_param = [1, 2, 5, 10, 20, 30, 40]\n\nplot_validation_curve(training_pipeline(rand_forest),\n                      X_train, y_train, key, rand_forest_param)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to tune *max_depth, min_samples_split, and min_samples_leaf* for Random Forest Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_forest = RandomForestClassifier(\n    n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n\nrand_forest_param_grid = {\n    'randomforestclassifier__max_depth': [10, 20, 30],\n    'randomforestclassifier__min_samples_split': [200, 300, 400],\n    'randomforestclassifier__min_samples_leaf': [5, 10, 15],\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here's the nested cross validated scores from randomized search for each explored hyperparameters' combinations for Random Forest Classifier."},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results are loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nrand_forest_param_result = nested_cv_param_search(training_pipeline(rand_forest), rand_forest_param_grid,\n                                                  X_train, y_train, 9, 'roc_auc', 5, 5)\n\ndump_objects('rand_forest_cf_cv_rand_search', rand_forest_param_result)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"[rand_forest_param_result] = load_objects('rand_forest_cf_cv_rand_search')\nrand_forest_param_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_forst_param_ranked = rand_forest_param_result.sort_values(\n    ['mean_test_score', 'mean_cv_score'], ascending=False).reset_index(drop=True)\nrand_forst_param_ranked","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best combination of hyperparameters for Random Forest Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_forest_best_param = rand_forst_param_ranked.loc[1, 'params']\nrand_forest_best_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2: XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_cf = XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'xgbclassifier__n_estimators'\nxgb_boost_param = [5, 10, 15, 30, 60]\n\nplot_validation_curve(training_pipeline(xgb_cf), X_train,\n                      y_train, key, xgb_boost_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'xgbclassifier__max_depth'\nxgb_boost_param = [5, 10, 15, 20, 25, 30]\n\nplot_validation_curve(training_pipeline(xgb_cf), X_train,\n                      y_train, key, xgb_boost_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'xgbclassifier__learning_rate'\nxgb_boost_param = [0.01, 0.05, 0.08, 0.12, 0.15]\n\nplot_validation_curve(training_pipeline(xgb_cf), X_train,\n                      y_train, key, xgb_boost_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'xgbclassifier__min_child_weight'\nxgb_boost_param = [5, 15, 20, 25, 30, 50, 80, 100]\n\nplot_validation_curve(training_pipeline(xgb_cf), X_train,\n                      y_train, key, xgb_boost_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'xgbclassifier__subsample'\nxgb_boost_param = [0.9, 0.99, 0.999, 0.9999]\n\nplot_validation_curve(training_pipeline(xgb_cf), X_train,\n                      y_train, key, xgb_boost_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'xgbclassifier__colsample_bytree'\nxgb_boost_param = [0.1, 0.3, 0.5, 0.7, 0.9]\n\nplot_validation_curve(training_pipeline(xgb_cf), X_train,\n                      y_train, key, xgb_boost_param)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to tune *min_child_weight, learning_rate, and max_depth* for XGBoost Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_cf = XGBClassifier(n_estimators=50, colsample_bytree=0.3,\n                       random_state=RANDOM_SEED, n_jobs=-1)\n\nxgb_cf_param_grid = {\n    'xgbclassifier__min_child_weight': [10, 25, 40],\n    'xgbclassifier__learning_rate': [0.01, 0.05, 0.08],\n    'xgbclassifier__max_depth': [5, 10]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here's the nested cross validated scores from randomized search for each explored hyperparameters' combinations for XGBoost Classifier."},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results are loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nxgb_cf_param_result = nested_cv_param_search(training_pipeline(xgb_cf), xgb_cf_param_grid,\n                                             X_train, y_train, 8, 'roc_auc', 5, 5)\n\ndump_objects('xgb_cf_cv_rand_search', xgb_cf_param_result)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"[xgb_cf_param_result] = load_objects('xgb_cf_cv_rand_search')\nxgb_cf_param_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_cf_param_ranked = xgb_cf_param_result.sort_values(\n    ['mean_test_score', 'mean_cv_score'], ascending=False).reset_index(drop=True)\nxgb_cf_param_ranked","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best combination of hyperparameters for XGBoost Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_cf_best_param = xgb_cf_param_ranked.loc[0, 'params']\nxgb_cf_best_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the Final Models"},{"metadata":{},"cell_type":"markdown","source":"Now, we will set the optimal hyperparameter based on the results from randomized search just now."},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_optimal_param(model, param):\n    new_model = clone(model)\n    param = re.sub('(?<=\\')[^_,]+__(?=.+\\')', '', param)\n    new_model.set_params(**ast.literal_eval(param))\n    return new_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are going to combine Gradient Boosting Classifier, Multi-layer Perceptron classifier, Logistic Classifier,\ntuned XGB Classifier, and tuned Random Forest Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_xg_boost = set_optimal_param(\n    classifiers['xg_boost']['func'], xgb_cf_best_param)\n\noptimal_rand_forest = set_optimal_param(\n    classifiers['rand_forest']['func'], rand_forest_best_param)\n\noptimized_models = [\n    classifiers['gradient_b_cf']['func'],\n    classifiers['neural_network']['func'],\n    classifiers['log_reg']['func'],\n    optimal_rand_forest,\n    optimal_xg_boost\n]\n\no_short_names = ['gradient_b_cf', 'neural_network',\n                 'log_reg', 'rand_forest', 'xg_boost']\n\no_names = ['Gradient Boosting Classifier', 'Multi-layer Perceptron classifier', 'Logistic Classifier',\n           'Random forest Classifier', 'XGBClassifier']\n\noptimized_cf_idx = {}\noptimized_cf = {}\n\n# Zip all classfiers together into a dictionary for convenient access\nfor idx, s_name, name, model in zip(range(len(names)), o_short_names, o_names, optimized_models):\n    optimized_cf_idx[idx] = {'name': name, 'func': model}\n    optimized_cf[s_name] = {'name': name, 'func': model}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if the final models have any performance changes..."},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results are loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\noptimized_cf_idx2 = {key: optimized_cf_idx[key] for key in range(5)}\n\nperformance_results4 = get_models_performance(optimized_cf_idx2, X_train, y_train,\n                                              training_pipeline=training_pipeline,\n                                              scoring_metrics=scoring_metrics, n_splits=5)\n\ndump_objects('performance4', performance_results4)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"[performance_results4] = load_objects(file_name='performance4')\nperformance_results4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortValues(performance_results4, roc, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortValues(performance_results3, roc, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = ['train_f1', 'test_f1']\nsortValues(performance_results4, f1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortValues(performance_results3, f1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_recall = ['train_precision',\n                    'test_precision', 'train_recall', 'test_recall']\nsortValues(performance_results4, precision_recall, [1, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sortValues(performance_results3, precision_recall, [1, 3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Final Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_vs_recall(classifier, cf_name, X_train, y_train, ax, method, label=False):\n\n    # get accurate y_scores using cross_val_predict, not from overfitted models\n    # y_scores are generate using 'predict_proba' method of each models,\n    # therefore probabilities of each class (total of 2) are returned\n    y_scores_cv = cross_val_predict(classifier, X_train, y_train,\n                                    cv=3, method=method, n_jobs=-1)\n\n    # Get the last columns of the y_scores only if more than one columns are detected\n    if y_scores_cv.ndim > 1:\n        y_scores_cv = y_scores_cv[:, -1]\n\n    precisions, recalls, thresholds = precision_recall_curve(\n        y_train, y_scores_cv)\n\n    # Adjust settings for the plot (eg. set title of the plot)\n    if label:\n        label_name = cf_name\n    else:\n        label_name = None\n    ax.plot(recalls, precisions, label=label_name)\n    ax.set(xlabel='recall', ylabel='precision',\n           title=f'PR Curve for {cf_name}')\n    ax.title.set_fontsize(16)\n    ax.grid()\n\n    return precisions, recalls, thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_vs_recall2(model, model_name, X, y, threshold, ax, method='predict_proba'):\n\n    # Call custom function plot_precision_vs_recall,\n    # to get precisions, recalls, thresholds and\n    # plot precision-recall curve\n    precisions, recalls, thresholds = plot_precision_vs_recall(model, model_name,\n                                                               X, y, ax, method)\n\n    # Get the index position of first recall value >= threshold\n    best_idx = np.argmin(recalls >= threshold)\n    selected_threshold = thresholds[best_idx]\n    selected_precision = precisions[best_idx]\n    selected_recall = recalls[best_idx]\n\n    # Adjust settings for the plot (eg. set title of the plot)\n    ax.set(title='Precision-Recall Graph')\n    ax.title.set_fontsize(20)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1.2)\n\n    # Annotate the chosen recall and preicsion in the form of (x, y)\n    x_coord = recalls[best_idx]\n    y_coord = precisions[best_idx]\n    ax.annotate(f'({x_coord:.2f}, {y_coord:.2f})',\n                (x_coord + 0.010, y_coord + 0.010))\n\n    # Plot the line to show the location of chosen recall and preicsion\n    ax.plot([selected_recall, selected_recall], [0, selected_precision], \"r:\")\n    ax.plot([0, selected_recall], [\n            selected_precision, selected_precision], \"r:\")\n    ax.plot([selected_recall], [selected_precision], \"ro\")\n\n    print(\"Selected Threshold = {:f}\".format(selected_threshold))\n    print(\"Selected Precision = {:.2f}\".format(selected_precision))\n    print(\"Selected Recall    = {:.2f}\".format(selected_recall))\n\n    return precisions, recalls, thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create 3 X 2 subplots to plot 6 graphs in one figure\nnum_row = 2\nnum_col = 3\nfig, axs = plt.subplots(num_row, num_col)\nfig.set_size_inches(15, 10)\n\nindexes = list(product(range(num_row), range(num_col)))[:len(optimized_cf)]\n\nwith tqdm(total=100) as pbar:\n    progress_unit = 100/len(optimized_cf)\n    # Iterate all 9 classifiers to produce the graphs\n    for index, classifier in zip(indexes, optimized_cf.values()):\n        plot_precision_vs_recall(training_pipeline(classifier['func']), classifier['name'],\n                                 X_train, y_train, axs[index[0]][index[1]],\n                                 method='predict_proba')\n        pbar.update(progress_unit)\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(15, 8)\nroc_scores = []\n\nwith tqdm(total=100) as pbar:\n    progress_unit = 100/len(optimized_cf)\n    # Iterate all classifiers to plot on the same axis\n    for classifier in optimized_cf.values():\n\n        new_cf = clone(classifier['func'])\n        method = 'predict_proba'\n\n        # get cross_validated y_score of training set from cross_val_predict,\n        # without having to fit the whole training set or use test set\n        y_score_cv = cross_val_predict(training_pipeline(\n            new_cf), X_train, y_train, cv=3, method=method)\n\n        # Get the last columns of the y_scores only if more than one columns are detected\n        if y_score_cv.ndim > 1:\n            y_score_cv = y_score_cv[:, -1]\n\n        # Plot the ROC curve\n        fpr, tpr, threshold = roc_curve(y_train, y_score_cv)\n        roc_auc = auc(fpr, tpr)\n\n        graph = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n                                estimator_name=classifier['name'])\n        graph.plot(ax=ax)\n\n        roc_scores.append({'name': classifier['name'], 'auc_score': roc_auc})\n        pbar.update(progress_unit)\n\nax.set(title=\"Receiver operating characteristic with cross validation\")\nax.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combining the Models"},{"metadata":{},"cell_type":"markdown","source":"Now, we are going to ensemble all the models into a single unified hard voting classifier. Let's see if having a ensemble of models is better."},{"metadata":{"trusted":true},"cell_type":"code","source":"class RobustVotingClassifier(RegressorMixin, BaseEstimator):\n\n    def __init__(self, optimized_models=[], keras_models=[], fitted_models=[]):\n        self.optimized_models = optimized_models\n        self.keras_models = keras_models\n        self.fitted_models = fitted_models\n        self.init()\n\n    def init(self):\n        self.trained_models = []\n\n    def fit(self, X, y):\n        for model in self.optimized_models:\n            new_model = clone(model)\n            new_model.fit(X, y)\n            self.trained_models.append(new_model)\n        self.models = self.trained_models + self.keras_models + self.fitted_models\n        return self\n\n    def predict(self, X=None):\n        y_preds = []\n        if type(X) == csr_matrix:\n            X = X.toarray()\n        num_models = len(self.models)\n        for model in self.models:\n            y_pred = model.predict(X)\n            y_preds.append(y_pred)\n\n        positive = np.sum(y_preds, axis=0) >= ceil(num_models / 2)\n        results = np.zeros(len(X))\n        results[positive] = 1.0\n\n        return results\n\n    def predict_proba(self, X=None):\n        y_probas = []\n        if type(X) == csr_matrix:\n            X = X.toarray()\n        for model in self.models:\n            y_proba = model.predict_proba(X)\n            if y_proba.shape[1] > 1:\n                y_proba = y_proba[:, 1]\n            y_probas.append(y_proba)\n\n        results = np.mean(y_probas, axis=0)\n\n        return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vote_ensemble_cf = RobustVotingClassifier(optimized_models)\nensemble_pp = training_pipeline(vote_ensemble_cf)\nensemble_pp = ensemble_pp.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow beautiful, look at that ROC score, 0.85! Phew, no overfitting or underfitting issues :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_proba = ensemble_pp.predict_proba(X_test)\n\nfinal_auc = roc_auc_score(y_test, y_proba)\nprint(f'The ROC_AUC score after combining the final model is {final_auc}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare for Submission"},{"metadata":{},"cell_type":"markdown","source":"Submit the Kaggle Task! Woohoo!"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_test = fetch_data(TEST_DATA_PATH)\ninsurance_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attrs = ['Region_Code', 'Policy_Sales_Channel']\nfor attr in attrs:\n    set1 = set(insurance_train[attr].value_counts().index.tolist())\n    set2 = set(insurance_test[attr].value_counts().index.tolist())\n    dff = set2.difference(set1)\n    if len(dff) == 0:\n        print(f'{attr} in train set contains all the values in test set.')\n    else:\n        print(f'{attr} in train set doesn\\'t contain values which are {list(dff)}, which these values are found in test set.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass DataCleaner2(BaseEstimator, TransformerMixin):\n\n    def __init__(self, features_to_be_remained=None):\n        self.features_to_be_remained = features_to_be_remained\n\n    def fit(self, data, y=None):\n        return self\n\n    def transform(self, data, y=None):\n\n        data = data.copy()\n        data = data.reset_index(drop=True)\n        data = data.drop(columns='id', axis=1)\n\n        index = data['Gender'] == 'Male'\n        data['Gender_female'] = 1.0\n        data.loc[index, 'Gender_female'] = 0.0\n\n        data = data.drop(columns='Gender', axis=1)\n\n        index = data['Vehicle_Damage'] == 'Yes'\n        data.loc[~index, 'Vehicle_Damage'] = 0.0\n        data.loc[index, 'Vehicle_Damage'] = 1.0\n        data['Vehicle_Damage'] = data['Vehicle_Damage'].astype('float64')\n\n        binary_cat_attr = ['Driving_License', 'Gender_female',\n                           'Vehicle_Damage', 'Previously_Insured']\n        cat_attr = ['Policy_Sales_Channel', 'Vehicle_Age', 'Region_Code']\n\n        data[cat_attr] = data[cat_attr].astype('object')\n\n        data = pd.get_dummies(data.copy(), columns=cat_attr)\n\n        preprocessed_attr = data.columns.tolist()\n        one_hot_encoded_attr = [list(filter(lambda x: re.match(\n            f'^{attr}', x), preprocessed_attr)) for attr in cat_attr]\n        one_hot_encoded_attr = sum(one_hot_encoded_attr, [])\n\n        data = data[self.features_to_be_remained]\n\n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_test_cleaned = DataCleaner2(\n    X_train.columns.tolist()).fit_transform(insurance_test.copy())\ninsurance_test_cleaned.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = fetch_data(SAMPLE_SUBMISSION_PATH)\nsample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_proba = ensemble_pp.predict_proba(insurance_test_cleaned)\nmy_submission = pd.DataFrame(\n    zip(insurance_test['id'].tolist(), y_proba), columns=sample_submission.columns)\nmy_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.to_csv('my_submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}