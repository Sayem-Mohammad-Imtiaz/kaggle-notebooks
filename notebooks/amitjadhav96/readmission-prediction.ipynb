{"cells":[{"metadata":{},"cell_type":"markdown","source":"## *Will a patient be readmitted to a hospital within 30 days?*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://dsuqs7rf8y6gn.cloudfront.net/wp-content/uploads/2019/03/patient-based.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Patient Readmittance Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## In this notebook, I have tried to analyze if a patient will be readmitted to a hostipal given certain medical and demographic data. To understand this data, certain degree of healthcare domain knowledge is needed, like ICD9 diagnosis codes. I have used pandas for EDA and data preprocessing and pyspark for modelling. Models used: Logistic Regression, Random Forest.I have also extracted the coefficients and important features from both models to find out which parameters have most weight on determining if a patient will be readmitted. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### *If you have any questions or feedback, please comment! And if you find this notebook helpful, do leave an  UPVOTE*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n### Importing required libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark\nimport pandas as pd\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nimport numpy as np\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport numpy as np\n\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext\n\n\nfrom pyspark.ml import feature, regression,classification, Pipeline, evaluation \nfrom pyspark.sql import functions as fn, Row\nfrom pyspark import sql\n\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration and Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/diabetes/diabetic_data.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"readmitted\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns 24 to 47 are medications which are categories. Most of the columns are nominal and categorical. We will deal with them by creating dummies. Notice that we are dropping one dummy. So during our interpretation we will have to interpret coefficients of other dummies with reference to the dropped dummy!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df,columns = [df.columns.values[i] for i in range(24,47) ], prefix=[df.columns.values[i] for i in range(24,47)], prefix_sep='_',drop_first=True) \n##Dummy reference Medication Down","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our target variable. We convert it to 0 and 1 for Binary Classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['readmitted'] = df['readmitted'].map({'NO': 0, '<30': 1, \">30\":2})\ndf['readmittedbinary'] = df['readmitted'].map({0: 0, 1: 1, 2:1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns=[\"change\",'max_glu_serum','A1Cresult','diabetesMed'], prefix = [\"change\",'max_glu_serum','A1Cresult','diabetesMed'],prefix_sep='_',drop_first=True)\n## Dummy Reference A1Cresult_>7,max_glu_serum>200,diabetesMed=No, change=ch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'] = df['age'].map({'[0-10)':5,'[10-20)':15, '[20-30)':25,'[30-40)':35,'[40-50)':45,'[50-60)':55,'[60-70)':65,'[70-80)':75,'[80-90)':85,'[90-100)':95})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['encounter_id','patient_nbr','weight','admission_type_id','discharge_disposition_id','admission_source_id','medical_specialty','payer_code'],axis=1,inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.loc[df['gender'].isin(['Male','Female'])]#df.loc[df['B'].isin(['one','three'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.replace('?', np.nan, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.dropna()##Clean pandas df without dummy variables ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = sns.countplot(x = df['age'], hue= df['readmitted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = sns.countplot(x = df['readmitted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = sns.countplot(x = df['gender'], hue= df['readmitted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = sns.countplot(x = df['race'], hue= df['readmitted'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"count_of_y = df[\"age\"].groupby(df[\"readmitted\"]).value_counts().rename(\"counts\").reset_index()\ncount_of_y\nfig = sns.lineplot(x=\"age\", y=\"counts\", hue=\"readmitted\", data=count_of_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25, 8))\na = df.corr()\nb = a['readmitted']\nc= b.to_frame()\ntype(c)\nc.sort_values(by = ['readmitted'], ascending = False , inplace = True)\npos = c.head(8)\nc.sort_values(by = ['readmitted'], ascending = True , inplace = True)\nneg = c.head(8)\nneg\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos.index.name = 'feature'\npos.reset_index(inplace=True)\npos\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg.index.name = 'feature'\nneg.reset_index(inplace=True)\nneg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos=pos.drop(pos.index[0:2])\npos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"posplot = sns.barplot(x='feature', y=\"readmitted\", data=pos)\nposplot.set_xticklabels(posplot.get_xticklabels(),rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negplot = sns.barplot(x='feature', y=\"readmitted\", data=neg)\nnegplot\nnegplot.set_xticklabels(negplot.get_xticklabels(),rotation=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"clean.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Spark DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spark_df = spark.read.csv('clean.csv', header=True, inferSchema=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spark_df=spark_df.withColumnRenamed(\"glimepiride-poglitazone\",\"glimepiridepoglitazone\").withColumnRenamed(\"glyburide-metformin\",\"glyburidemetformin\").withColumnRenamed(\"glipizide-metformin\",\"glipizidemetformin\").withColumnRenamed(\"glimepiride-pioglitazone\",\"glimepiridepioglitazone\").withColumnRenamed(\"metformin-rosiglitazone\",\"metforminrosiglitazone\").withColumnRenamed(\"metformin-pioglitazone\",\"metforminpioglitazone\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here, I created a function to deal with ICD-9 codes. Please read the data description for more information about these codes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def amit(row):    \n    ma=0\n    mb=0\n    md=0\n    me=0\n    sa=0\n    dr=dd=ddt=di=dm=dg=dn=dr2=dd2=ddt2=di2=dm2=dg2=dn2=dr3=dd3=ddt3=di3=dm3=dg3=dn3=0\n    \n    if \"V\" in row.diag_1 or \"E\" in row.diag_1:\n        dr=dd=ddt=di=dm=dg=dn=0\n    #elif 390 <= float(row.diag_1) <= 459 or float(row.diag_1) == 785: #DUMMY DROPPED REFERENCE\n    #    dc =1 ##Circulatory\n    elif 460 <= float(row.diag_1) <= 519 or float(row.diag_1) == 786:\n        dr =1 #Respiratory\n    elif 520 <= float(row.diag_1) <= 579 or float(row.diag_1) == 787:\n        dd =1 #Digestive\n    elif 250 <= float(row.diag_1) <= 250.999:\n        ddt =1 #Diabetes\n    elif 800 <= float(row.diag_1) <= 999:\n        di =1 #Injury\n    elif 710 <= float(row.diag_1) <= 739:\n        dm =1 #musculoskeletal\n    elif 580 <= float(row.diag_1) <= 629 or float(row.diag_1) == 788:\n        dg =1 #Genitourinary\n    elif 140 <= float(row.diag_1) <= 239:\n        dn =1 #Neoplasms\n    else:\n        dr=dd=ddt=di=dm=dg=dn=0\n        #do=1#others\n        \n    if \"V\" in row.diag_2 or \"E\" in row.diag_2:\n        #do2=1\n        dr2=dd2=ddt2=di2=dm2=dg2=dn2=0\n    #elif 390 <= float(row.diag_2) <= 459 or float(row.diag_2) == 785: #DUMMY DROPPED REFERENCE\n    #    dc2 =1 ##Circulatory\n    elif 460 <= float(row.diag_2) <= 519 or float(row.diag_2) == 786:\n        dr2 =1 #Respiratory\n    elif 520 <= float(row.diag_2) <= 579 or float(row.diag_2) == 787:\n        dd2 =1 #Digestive\n    elif 250 <= float(row.diag_2) <= 250.999:\n        ddt2 =1 #Diabetes\n    elif 800 <= float(row.diag_2) <= 999:\n        di2 =1 #Injury\n    elif 710 <= float(row.diag_2) <= 739:\n        dm2 =2 #musculoskeletal\n    elif 580 <= float(row.diag_2) <= 629 or float(row.diag_2) == 788:\n        dg2 =1 #Genitourinary\n    #elif 140 <= float(row.diag_2) <= 239:\n    #   dn2 =1 #Neoplasms\n    else:\n        #do2=1#others\n        dr2=dd2=ddt2=di2=dm2=dg2=dn2=0\n        \n    if \"V\" in row.diag_3 or \"E\" in row.diag_3:\n        #do3=1\n        dr3=dd3=ddt3=di3=dm3=dg3=dn3=0\n    #elif 390 <= float(row.diag_3) <= 459 or float(row.diag_3) == 785:#DUMMY DROPPED REFERENCE\n    #    dc3 =1 ##Circulatory\n    elif 460 <= float(row.diag_3) <= 519 or float(row.diag_3) == 786:\n        dr3 =1 #Respiratory\n    elif 520 <= float(row.diag_3) <= 579 or float(row.diag_3) == 787:\n        dd3 =1 #Digestive\n    elif 250 <= float(row.diag_3) <= 250.999:\n        ddt3 =1 #Diabetes\n    elif 800 <= float(row.diag_3) <= 999:\n        di3 =1 #Injury\n    elif 710 <= float(row.diag_3) <= 739:\n        dm3 =1 #musculoskeletal\n    elif 580 <= float(row.diag_3) <= 629 or float(row.diag_3) == 788:\n        dg3 =1 #Genitourinary\n    elif 140 <= float(row.diag_3) <= 239:\n        dn3 =1 #Neoplasms\n    else:\n        dr3=dd3=ddt3=di3=dm3=dg3=dn3=0\n        #do3=1#others  \n\n    \n    if row.race == \"Caucasian\":\n        ma = 1\n        #prfloat(\"A\")\n    elif row.race == \"Asian\":\n        mb = 1\n    #elif row.race == \"AfricanAmerican\": #DUMMY DROPPED REFERENCE\n    #    mc = 1\n    elif row.race ==\"Hispanic\":\n        me = 1\n    else:# :\n        ma=0\n        mb=0\n        me = 0\n\n\n    if row.gender == \"Male\":\n        sa = 1\n    #elif row.gender == \"Female\": #DROPPED DUMMY REFERENCE\n    #    sb = 1\n    \n    \n    r = Row(Caucasian=int(ma) ,Asian=int(mb) ,Hispanic=int(me),male=float(sa),\n            Respiratory=dr,\n            Digestive= dd,\n            Diabetes = ddt,\n            Injury= di,\n            Muscuskeletal= dm,\n            Neoplasms=dn,\n            Genitourinary = dg,\n            \n            Respiratory2=dr2,\n            Digestive2= dd2,\n            Diabetes2 = ddt2,\n            Injury2= di2,\n            Muscuskeletal2= dm2,\n            Neoplasms2=dn2,\n            Genitourinary2 = dg2,\n            \n            Respiratory3=dr3,\n            Digestive3= dd3,\n            Diabetes3 = ddt3,\n            Injury3= di3,\n            Muscuskeletal3= dm3,\n            Neoplasms3=dn3,\n            Genitourinary3 = dg3,\n            \n      )\n    return(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df = spark.createDataFrame(spark_df.rdd.map(amit))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am joining the two spark dataframes here on row number. This will be our final data with all dummy variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import monotonically_increasing_id, row_number\nfrom pyspark.sql.window import Window\n# since there is no common column between these two dataframes add row_index so that it can be joined\nspark_df=spark_df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\ndummy_df=dummy_df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n\ndummy_df = dummy_df.join(spark_df, on=[\"row_index\"]).drop(\"row_index\")\ndummy_df.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping these columns as we already have their dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df = dummy_df.drop(\"diag_1\",\"diag_2\",\"diag_3\",\"gender\",\"race\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df = dummy_df.drop(\"_c0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df.select('male',\"Caucasian\",\"Hispanic\",\"Asian\").show()#,'Male','Female', 'Circulatory','Circulatory2','Circulatory3').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting into training, testing and validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_df, validation_df, testing_df = dummy_df.randomSplit([0.6, 0.3, 0.1], seed=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking at our final columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featlist = ['Asian',\n 'Caucasian',\n 'Diabetes',\n 'Diabetes2',\n 'Diabetes3',\n 'Digestive',\n 'Digestive2',\n 'Digestive3',\n 'Genitourinary',\n 'Genitourinary2',\n 'Genitourinary3',\n 'Hispanic',\n 'Injury',\n 'Injury2',\n 'Injury3',\n 'Muscuskeletal',\n 'Muscuskeletal2',\n 'Muscuskeletal3',\n 'Neoplasms',\n 'Neoplasms2',\n 'Neoplasms3',\n 'Respiratory',\n 'Respiratory2',\n 'Respiratory3',\n 'male',\n 'age',\n 'time_in_hospital',\n 'num_lab_procedures',\n 'num_procedures',\n 'num_medications',\n 'number_outpatient',\n 'number_emergency',\n 'number_inpatient',\n 'number_diagnoses',\n 'metformin_No',\n 'metformin_Steady',\n 'metformin_Up',\n 'repaglinide_No',\n 'repaglinide_Steady',\n 'repaglinide_Up',\n 'nateglinide_No',\n 'nateglinide_Steady',\n 'nateglinide_Up',\n 'chlorpropamide_No',\n 'chlorpropamide_Steady',\n 'chlorpropamide_Up',\n 'glimepiride_No',\n 'glimepiride_Steady',\n 'glimepiride_Up',\n 'acetohexamide_Steady',\n 'glipizide_No',\n 'glipizide_Steady',\n 'glipizide_Up',\n 'glyburide_No',\n 'glyburide_Steady',\n 'glyburide_Up',\n 'tolbutamide_Steady',\n 'pioglitazone_No',\n 'pioglitazone_Steady',\n 'pioglitazone_Up',\n 'rosiglitazone_No',\n 'rosiglitazone_Steady',\n 'rosiglitazone_Up',\n 'acarbose_No',\n 'acarbose_Steady',\n 'acarbose_Up',\n 'miglitol_No',\n 'miglitol_Steady',\n 'miglitol_Up',\n 'troglitazone_Steady',\n 'tolazamide_Steady',\n 'tolazamide_Up',\n 'insulin_No',\n 'insulin_Steady',\n 'insulin_Up',\n 'glyburide-metformin_No',\n 'glyburide-metformin_Steady',\n 'glyburide-metformin_Up',\n 'glipizide-metformin_Steady',\n 'glimepiride-pioglitazone_Steady',\n 'metformin-rosiglitazone_Steady',\n 'metformin-pioglitazone_Steady',\n 'change_No',\n 'max_glu_serum_>300',\n 'max_glu_serum_None',\n 'max_glu_serum_Norm',\n 'A1Cresult_>8',\n 'A1Cresult_None',\n 'A1Cresult_Norm',\n 'diabetesMed_Yes']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building and Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Logistic model 1 with all features no parameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Pipeline(stages=[feature.VectorAssembler(inputCols=featlist,\n                                        outputCol='features'),feature.StandardScaler(inputCol='features',outputCol = 'sdfeatures'),\n                 classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_model = model1.fit(training_df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pipe_modeldf = pipe_model.transform(validation_df).select(\"readmittedbinary\",\"prediction\")\npipe_modeldf.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp = pipe_modeldf[(pipe_modeldf.readmittedbinary == 1) & (pipe_modeldf.prediction == 1)].count()\ntn = pipe_modeldf[(pipe_modeldf.readmittedbinary == 0) & (pipe_modeldf.prediction == 0)].count()\nfp = pipe_modeldf[(pipe_modeldf.readmittedbinary == 0) & (pipe_modeldf.prediction == 1)].count()\nfn = pipe_modeldf[(pipe_modeldf.readmittedbinary == 1) & (pipe_modeldf.prediction == 0)].count()\nprint (\"True Positives:\", tp)\nprint (\"True Negatives:\", tn)\nprint (\"False Positives:\", fp)\nprint (\"False Negatives:\", fn)\nprint (\"Total\", dummy_df.count())\n\nr = (tp)/(tp + fn)\nprint (\"recall\", r)\n\np = float(tp) / (tp + fp)\nprint (\"precision\", p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us closely look at our results here. The confusion matrix here shows that there are 5742 true positives. This means that the model _correctly_ predicted 5742 patients who were readmitted to the hospital as readmitted. There are 12317 true negatives. This means that the model _correctly_ predicted 12317 patients who were NOT readmitted to hospital as NOT readmitted. Similarly, the model _incorrectly_ predicted 3351 people who were NOT readmitted to hospital as readmitted. Also, the model _incorrectly_ predicted that 7962 who were readmitted as NOT readmitted.\nLooking at the recall value we can say that our our model correctly predicts 42% of readmitted cases (positives). Lets calculate specificity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"specificity = tn/(tn+fp)\nprint(\"specificity\",specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting! Our model correctly predicts 79% of people who will not require readmittance correctly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = evaluation.BinaryClassificationEvaluator(labelCol='readmittedbinary')\nAUC1 = evaluator.evaluate(pipe_model.transform(validation_df))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"AUC1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(list(zip(featlist, pipe_model.stages[-1].coefficients.toArray())),\n            columns = ['column', 'Coefficients']).sort_values('Coefficients',ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Intercept: \" + str(pipe_model.stages[-1].intercept))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model interpretation (Intercept and Coefficients)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have all the coefficients and intercept, lets try to interpret their meaning. Remember that in logistic regression we do not predict the actual value like linear regression. Instead, we try to predict the probabilty of getting 1 or 0; readmitted or not readmitted. To do this we fit the data to sigmoid function. To explain y as a linear combination of x variables, we take logit, which  is log(p/(1-p)). Hence, the coefficients in logistic regression are in terms of log odds. \nLet us look at the intercept first. The intercept has value of -0.1306. Remember we have chosen dummies as reference. So we interpret the intercept as : The log odds or logit of an African American female with diag1,diag2,diag3 as Circulatory and and all medications as Down, and the following characteristics: A1Cresult_>7,max_glu_serum>200,diabetesMed=No, change=ch, is -0.1306. We can find the probability by plugging the value in sigmoid function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = 1/(1+np.exp(-pipe_model.stages[-1].intercept))\nprob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can say that with everything else being zero, an African American female with diag1,diag2,diag3 as Circulatory and and all medications as Down, and the following characteristics: A1Cresult_>7,max_glu_serum>200,diabetesMed=No, change=ch, has 46.73% probability of being readmitted to the hospital. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us talk about coefficient of number_inpatient. It has a value of 0.4582. We interpret as: For one unit increase in number_inpatient, the logit of being readmitted increases by 0.4582, everything else being constant. We can compute odd ratio by taking the exponent of the coefficient.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.exp(0.4582)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Odd ratio can be thought of as odds of being readmitted when number_inpatients is n+1 by odds of being readmitted when number_inpatients is n. Hence we can say that, holding all the other variables fixed, by increasing number_inpatient by one, we expect to see the odds of getting readmitted increase by about 58%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(list(zip(featlist, pipe_model.stages[-1].coefficients.toArray())),\n            columns = ['column', 'Coefficients']).sort_values('Coefficients').head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look at coefficient of num_procedures. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.exp(-0.068755)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every one unit increase in num_procedures decreases the odds of being readmitted by about 7 percent.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"beta = pipe_model.stages[-1].coefficients\nplt.plot(beta)\nplt.ylabel('Coefficients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingSummary = pipe_model.stages[-1].summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\nroc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fMeasure = trainingSummary.fMeasureByThreshold\nmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\nbestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n    .select('threshold').head()['threshold']\nmaxFMeasure","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()\n#pr['recall']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"va =feature.VectorAssembler(inputCols=featlist ,  outputCol='features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sd = feature.StandardScaler(inputCol='features',outputCol = 'sdfeatures')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic model 2 with all features and reg param","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_model2 = Pipeline(stages=[va,sd, lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paramGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.1, 0.3, 0.5]).addGrid(lr.elasticNetParam, [0.2, 0.8, 0.5]).addGrid(lr.maxIter, [15, 30, 50]).build())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CrossValidator(estimator=pipe_model2, \\\n                    estimatorParamMaps=paramGrid, \\\n                    evaluator=evaluator, \\\n                    numFolds=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvModel = cv.fit(training_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUC2 = evaluator.evaluate(cvModel.transform(validation_df))\nAUC2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_dict = cvModel.bestModel.stages[-1].extractParamMap()\n\nsane_dict = {}\nfor k, v in param_dict.items():\n    #print(k)\n    sane_dict[k.name] = v\n\nbest_reg = sane_dict[\"regParam\"]\nbest_elastic_net = sane_dict[\"elasticNetParam\"]\nbest_max_iter = sane_dict[\"maxIter\"]\nprint(best_reg)\nprint(best_elastic_net)\nprint(best_max_iter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest model 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrf = Pipeline(stages= [va, classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\")])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrffit= modelrf.fit(training_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrfdf = modelrffit.transform(validation_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrfdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUCrf = evaluator.evaluate(modelrffit.transform(validation_df))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"AUCrf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(list(zip(featlist, modelrffit.stages[1].featureImportances.toArray())),\n            columns = ['column', 'weight']).sort_values('weight', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp = modelrfdf[(modelrfdf.readmittedbinary == 1) & (modelrfdf.prediction == 1)].count()\ntn = modelrfdf[(modelrfdf.readmittedbinary == 1) & (modelrfdf.prediction == 0)].count()\nfp = modelrfdf[(modelrfdf.readmittedbinary == 0) & (modelrfdf.prediction == 1)].count()\nfn = modelrfdf[(modelrfdf.readmittedbinary == 0) & (modelrfdf.prediction == 0)].count()\nprint (\"True Positives:\", tp)\nprint (\"True Negatives:\", tn)\nprint (\"False Positives:\", fp)\nprint (\"False Negatives:\", fn)\n#print (\"Total\", df.count())\n\nr = (tp)/(tp + fn)\nprint (\"recall\", r)\n\np = float(tp) / (tp + fp)\nprint (\"precision\", p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity = tn/(tn+fp)\nprint(\"Sensitivity\",sensitivity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"beta = modelrffit.stages[-1].featureImportances\nplt.plot(beta)\nplt.ylabel('Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross-Validation Random Forest Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf=classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mrf = Pipeline(stages=[va,rf])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paramGrid = (ParamGridBuilder()\n             .addGrid(rf.numTrees, [40]).addGrid(rf.maxDepth,[5,10,15,30]).build())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvrf = CrossValidator(estimator=mrf, \\\n                    estimatorParamMaps=paramGrid, \\\n                    evaluator=evaluator, \\\n                    numFolds=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvrf1 = cvrf.fit(training_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUCn = evaluator.evaluate(cvrf1.transform(validation_df))\nAUCn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_dict1 = cvrf1.bestModel.stages[-1].extractParamMap()\n\nsane_dict1 = {}\nfor k, v in param_dict1.items():\n    #print(k)\n    sane_dict1[k.name] = v\n\n\nbest_max_depth = sane_dict1[\"maxDepth\"]\nprint(best_max_depth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multiclass classification for Readmitted","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrfmc = Pipeline(stages= [va, classification.RandomForestClassifier(labelCol='readmitted', featuresCol=\"features\",maxDepth=15, numTrees=30)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrffitmc= modelrfmc.fit(training_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluatormc = evaluation.MulticlassClassificationEvaluator(labelCol='readmitted',predictionCol=\"prediction\",metricName=\"accuracy\")\nAUCrfmc = evaluatormc.evaluate(modelrffitmc.transform(validation_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUCrfmc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(list(zip(featlist, modelrffitmc.stages[1].featureImportances.toArray())),\n            columns = ['column', 'weight']).sort_values('weight', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression on top features of original Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new_lr2 = Pipeline(stages=[feature.VectorAssembler(inputCols=['number_inpatient', 'number_emergency', 'number_diagnoses', 'Diabetes', 'number_outpatient','time_in_hospital','diabetesMed_Yes','age','rosiglitazone_Steady','Caucasian'],\n                                        outputCol='features'),sd,\n                 classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_model3 = model_new_lr2.fit(training_df)\nAUC5 = evaluator.evaluate(pipe_model3.transform(validation_df))\nAUC5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest on top features of original Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrfselected = Pipeline(stages= [feature.VectorAssembler(inputCols=['number_inpatient', 'number_emergency', 'number_diagnoses', 'Diabetes', 'number_outpatient','time_in_hospital','diabetesMed_Yes','age','rosiglitazone_Steady','Caucasian'],\n                                        outputCol='features'), classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\",maxDepth=15, numTrees=30)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrffitselected= modelrfselected.fit(training_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUCrfselected = evaluator.evaluate(modelrffitselected.transform(validation_df))\nAUCrfselected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression on top of Random Forest features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new_lr3 = Pipeline(stages=[feature.VectorAssembler(inputCols=['number_inpatient','num_medications','num_lab_procedures','number_diagnoses','time_in_hospital','age','number_emergency', 'number_outpatient','num_procedures','male'],\n                                        outputCol='features'),sd,\n                 classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_model4 = model_new_lr3.fit(training_df)\nAUC6 = evaluator.evaluate(pipe_model4.transform(validation_df))\nAUC6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = Pipeline(stages= [va, classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\",maxDepth=10, numTrees=40)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestmodel_fit= best_model.fit(training_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing the best model so far on Testing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"AUCfinal = evaluator.evaluate(bestmodel_fit.transform(testing_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUCfinal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(list(zip(featlist, bestmodel_fit.stages[1].featureImportances.toArray())),\n            columns = ['column', 'weight']).sort_values('weight', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the model to entire data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bestmodel_final_fit= best_model.fit(dummy_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}