{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Forecasting BTC using N-BEATS \nIn this kernel, I implement the N-BEATS[0] architecture that is the current SOTA for time series forecasting as far as I know. For more details on what the architecture is about, please read my medium blog on it.\n\nI modified [fastai](https://www.fast.ai/)'s tabular model/learner and use their one cycle learning to speed up training among other things!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n\nfrom fastai.tabular import *\nfrom fastai.callbacks import *\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\nimport matplotlib.pyplot as plt\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = Path(os.getcwd())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/bitcoin-historical-data/coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv')\ndf['date'] = pd.to_datetime(df['Timestamp'],unit='s').dt.date\ndf = df.groupby('date')\ndf = df['Weighted_Price'].mean(); df = df.to_frame() #convert series to df\n\n#testing to see if logging makes the model better: https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va\ndf['Weighted_Price'] = df['Weighted_Price'].apply(lambda x: math.log(x))\nprint(df.shape); df.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#What we will train the model on\ndf.iloc[:int(len(df)*.9)].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#What we will want the model to predict\ndf.iloc[int(len(df)*.9):].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_df(df,lag=1):\n  \"Extend df sideways to allow a longer more timesteps while taking advantage of fastai's dataset/loader\"\n  df_org = df.copy(deep=True)\n\n  for i in range(lag):\n    df_lag = df_org.shift(i+1)\n    df_lag = df_lag.add_suffix('_M' + str(i+1))\n    #not taking last column of lagged values because cannot use for competition\n    df = df.merge(df_lag, left_index=True, right_index=True ,suffixes=(False, False))\n    \n    df.dropna(inplace=True)\n  \n  return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_com = prep_df(df,5); df_com.reset_index(drop=True,inplace=True); df_com.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var = 'Weighted_Price'; cat_names =[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"procs = [FillMissing, Categorify, Normalize]\nvalid_idx = range(int(len(df_com)*.9), len(df_com))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = TabularDataBunch.from_df(path, df_com, dep_var, valid_idx=valid_idx, procs=procs, \n                                cat_names=cat_names,bs=64)\n\n#making shuffling false so that there is no data leakage\ndata.train_dl = data.train_dl.new(shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preview of data for sanity checks\nx,y = next(iter(data.train_dl))\n(cat_x,cont_x),y = next(iter(data.train_dl))\nfor o in (cat_x, cont_x, y): print(to_np(o[:5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class gen_block(nn.Module):\n  def __init__(self,n_in, n_hidden, theta_dim, n_out, bn:bool=True, ps:float=0., actn:Optional[nn.Module]=None):\n    super().__init__()\n    self.FC1 = nn.Sequential(*bn_drop_lin(n_in,n_hidden,bn,ps,actn))\n    self.FC2 = nn.Sequential(*bn_drop_lin(n_hidden,n_hidden,bn,ps,actn))\n    self.FC3 = nn.Sequential(*bn_drop_lin(n_hidden,n_hidden,bn,ps,actn))\n    self.FC4 = nn.Sequential(*bn_drop_lin(n_hidden,n_hidden,bn,ps,actn))\n    self.Fcst = nn.Sequential(*(bn_drop_lin(n_hidden,theta_dim,bn,ps,actn)+bn_drop_lin(theta_dim,n_out,bn,ps))) #forecast output shouldnt have relu\n    self.Bcst = nn.Sequential(*(bn_drop_lin(n_hidden,theta_dim,bn,ps,actn)+bn_drop_lin(theta_dim,n_in,bn,ps))) #same for backcast\n\n  def forward(self, x):\n    x1 = self.FC1(x)\n    x1 = self.FC2(x1)\n    x1 = self.FC3(x1)\n    x1 = self.FC4(x1)\n    x2 = self.Fcst(x1)\n    x3 = self.Bcst(x1)\n\n    return (x-x3, x2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FFModel(Module):\n    \"Modified tabular model from fastai for embedding projections, if needed\"\n    def __init__(self, n_hidden, theta_dim, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n        super().__init__()\n        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n        self.n_in  = self.n_emb + self.n_cont\n        self.n_out = out_sz\n        self.n_hidden = n_hidden\n        self.theta_dim = theta_dim\n\n        self.blk1 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk2 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk3 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk4 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk5 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n        self.blk6 = gen_block(n_in=self.n_in,n_hidden=self.n_hidden,theta_dim=self.theta_dim,n_out=self.n_out,ps=ps,actn=nn.ReLU(inplace=True))\n\n\n    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n\n        x, f1 = self.blk1(x)\n        x, f2 = self.blk2(x)\n        x, f3 = self.blk3(x)\n        x, f4 = self.blk4(x)\n        x, f5 = self.blk5(x)\n        x, f6 = self.blk6(x)        \n\n        x = f1+f2+f3+f4+f5+f6\n\n        if self.y_range is not None:\n            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def FFLearner(data:DataBunch, n_hidden, theta_dim, layers:Collection[int], emb_szs:Dict[str,int]=None, metrics=None,\n        ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, **learn_kwargs):\n    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n    model = FFModel(n_hidden, theta_dim, emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n                    y_range=y_range, use_bn=use_bn)\n    return Learner(data, model, metrics=metrics, **learn_kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_range=None\n#y_range = (df_com['Weighted_Price'].min(), df_com['Weighted_Price'].max()); print(y_range)\n#del df_com; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = FFLearner(data, n_hidden=512,theta_dim=8,layers=[0], metrics=mean_absolute_error,\n            emb_drop=0, y_range=y_range, \n                  callback_fns=[ShowGraph,partial(CSVLogger, append=True)], ps=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"learn.lr_find(); learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(4, 1e-2)\n#, callbacks=[SaveModelCallback(learn, every='epoch', monitor='mean_absolute_error')])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty muted predictions at first"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, y = learn.get_preds()\nplt.plot(preds, label = 'Predictions'); plt.plot(y, label = 'Actuals')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find(); learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(4, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the model getting more expressive as we train more."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, y = learn.get_preds()\nplt.plot(preds, label = 'Predictions'); plt.plot(y, label = 'Actuals')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('NBEATS_LAG5_LOG')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References\n[0] Oreshkin et al. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}