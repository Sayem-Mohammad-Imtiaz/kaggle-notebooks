{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qq datasets","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:08:39.349551Z","iopub.execute_input":"2021-09-08T15:08:39.350168Z","iopub.status.idle":"2021-09-08T15:09:05.545645Z","shell.execute_reply.started":"2021-09-08T15:08:39.350072Z","shell.execute_reply":"2021-09-08T15:09:05.544529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10,shuffle=True,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:41:31.245134Z","iopub.execute_input":"2021-09-07T14:41:31.245822Z","iopub.status.idle":"2021-09-07T14:41:31.788637Z","shell.execute_reply.started":"2021-09-07T14:41:31.245708Z","shell.execute_reply":"2021-09-07T14:41:31.7873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -qq install transformers\nfrom transformers import pipeline\n\nimport torch\nif torch.cuda.is_available() : device=0\nelse: device=-1","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:41:31.791065Z","iopub.execute_input":"2021-09-07T14:41:31.791373Z","iopub.status.idle":"2021-09-07T14:41:49.777985Z","shell.execute_reply.started":"2021-09-07T14:41:31.791345Z","shell.execute_reply":"2021-09-07T14:41:49.776649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"twitter-roberta-base-sentiment-mlm\"\n\nmodel_checkpoint_lm_class =      \"neoyipeng/\"+model_name+\"-class\"\nmodel_checkpoint_lm_skep_class = \"neoyipeng/\"+model_name+\"-skep-class\"","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:44:57.385913Z","iopub.execute_input":"2021-09-07T14:44:57.386542Z","iopub.status.idle":"2021-09-07T14:44:57.392921Z","shell.execute_reply.started":"2021-09-07T14:44:57.38646Z","shell.execute_reply":"2021-09-07T14:44:57.391728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint_lm_class, use_fast=True)\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments,EarlyStoppingCallback\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_lm_class,num_labels=3)\n\nbatch_size=64\ntraining_args = TrainingArguments(\n    \"Finbert-EXT-mlm-class\",\n    report_to='none',\n    warmup_ratio=0.1,\n    overwrite_output_dir=True,       #overwrite the content of the output directory to save space\n    save_total_limit=1,               #prevents saving many models\n    learning_rate=2e-5,\n    lr_scheduler_type='cosine_with_restarts', #similar to fastai's one cycle\n    fp16=True, #used in NEZHA (mixed precision, positional encoding, WWM)\n    per_device_train_batch_size=batch_size,  # batch size per device during training\n    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n    logging_strategy  = 'epoch',                    # we will log every epoch\n    evaluation_strategy = \"epoch\",\n    save_strategy='epoch',\n    metric_for_best_model='accuracy',#needed for early stopping callback to determine when to stop\n    load_best_model_at_end=True,\n    num_train_epochs=4,          # following simple way to fine tune bert - 20 epochs, 2e-5 and early stopping.\n    remove_unused_columns=True,  ## If we have ref files, need to avoid it removed by trainer. if not doing MLM REMOVE!\n    eval_accumulation_steps=5,        # evaluation every X steps. default takes entire dataset and puts it into\n    label_smoothing_factor =0.1 #add label smoothing,\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:47:33.694078Z","iopub.execute_input":"2021-09-07T14:47:33.694659Z","iopub.status.idle":"2021-09-07T14:47:44.910823Z","shell.execute_reply.started":"2021-09-07T14:47:33.694602Z","shell.execute_reply":"2021-09-07T14:47:44.909943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset,concatenate_datasets\n\nvals_ds   = load_dataset('financial_phrasebank','sentences_allagree',split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])\ntrains_ds = load_dataset('financial_phrasebank','sentences_allagree',split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:15:00.278187Z","iopub.execute_input":"2021-09-07T15:15:00.278582Z","iopub.status.idle":"2021-09-07T15:15:04.160044Z","shell.execute_reply.started":"2021-09-07T15:15:00.278544Z","shell.execute_reply":"2021-09-07T15:15:04.158901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples): return tokenizer(examples[\"sentence\"],padding='max_length',max_length=128,truncation=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:51:53.795767Z","iopub.execute_input":"2021-09-07T14:51:53.799601Z","iopub.status.idle":"2021-09-07T14:51:53.810221Z","shell.execute_reply.started":"2021-09-07T14:51:53.799524Z","shell.execute_reply":"2021-09-07T14:51:53.808749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall}","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:47:44.91223Z","iopub.execute_input":"2021-09-07T14:47:44.912921Z","iopub.status.idle":"2021-09-07T14:47:44.920839Z","shell.execute_reply.started":"2021-09-07T14:47:44.912864Z","shell.execute_reply":"2021-09-07T14:47:44.919734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc,f1=[],[]\nfor trn,val in zip(trains_ds,vals_ds):\n    \n    tokenized_val = val.map(tokenize_function, batched=True, remove_columns=['sentence'])\n    tokenized_trn = trn.map(tokenize_function, batched=True, remove_columns=['sentence'])\n    \n    trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_trn,\n    eval_dataset=tokenized_val,\n    callbacks=[EarlyStoppingCallback(1)],\n    compute_metrics=compute_metrics)\n    \n    trainer.train()\n    preds=trainer.predict(tokenized_val)\n    preds=torch.argmax(preds[0],dim=1).tolist()\n    \n    acc.append(accuracy_score(val['label'],preds))\n    f1.append (f1_score(val['label'],preds,average='macro'))\n\nprint('*'*99)\nf'10-fold accuracy is {sum(acc)/len(acc):0.0%} and f1 is {sum(f1)/len(f1):0.0%}!'","metadata":{},"execution_count":null,"outputs":[]}]}