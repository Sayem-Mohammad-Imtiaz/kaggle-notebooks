{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning Project by Teodor Chakarov","metadata":{}},{"cell_type":"markdown","source":"# Fake and real news dataset","metadata":{}},{"cell_type":"markdown","source":"Fake news are problem nowadays for this missinforamtion that we have all around the Internet and social media. Canwe really distinguish the difference between real and fake news. Can we make a good model for NLP so we can easily say if we can trust one news' source or not?\n\nSo I will explore and clean 2 csv files that i downloaded from \"https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\". ","metadata":{}},{"cell_type":"code","source":"import os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nimport seaborn as sns\nimport datetime ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re, string\nimport nltk \nimport nltk as nlp\n\nimport unicodedata\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, plot_confusion_matrix, plot_roc_curve, make_scorer, confusion_matrix\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.base import BaseEstimator, is_classifier, is_regressor, is_outlier_detector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\nreal_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_news","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will merge those 2 datasets and I will add another column for category (Fake: 0, Real: 1) ","metadata":{}},{"cell_type":"code","source":"fake_news['category'] = 0\nreal_news['category'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news = pd.concat([fake_news, real_news])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news = news.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news = news.drop('index', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\nplt.title('By category')\n\nsns.countplot(news.category)\n\nplt.ylabel(\"Count\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Dataset is well banalced in terms of news category also we don't have NaN values which i good ","metadata":{}},{"cell_type":"markdown","source":"#### Analysing the date\n\nI noticed that I can't covert to pd.to_datetime because i have different date column records.","metadata":{}},{"cell_type":"code","source":"news[news['date'].str.len() > 19]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Basically I will create new variable with those 10 records droped for date analysis.","metadata":{}},{"cell_type":"code","source":"http_filter = news['date'].str.contains('http')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_date = news[~http_filter]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_date = news_date[~news_date.date.str.contains('MSNB')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_date","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_date['date'] = pd.to_datetime(news_date.date)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_date","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Distribution of the news in the time period","metadata":{}},{"cell_type":"code","source":"sns.displot(news_date, x=news_date.date, hue=\"category\", element = 'step', height=8.27, aspect=11.7/8.27)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the destribution for the news through time and we can see that real news we have much move form period 2019/09 to 2018/01 but for fake we have more frequent int the past ","metadata":{}},{"cell_type":"markdown","source":"#### Count of news per year per news category","metadata":{}},{"cell_type":"code","source":"plt.close()\nplt.figure(figsize=(8,6))\nsns.countplot(news_date.date.dt.year, data=news_date, hue='category')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.set(style = \"whitegrid\",font_scale = 1.2)\n\nplt.title('Number of news per subject')\n\nsns.countplot(x= 'subject', hue = 'category', data = news)\n\nplt.ylabel(\"Count\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see we have different kind of subjects for both true and fake news. We are going to delete this column in the generel text we are going to inspect.","metadata":{}},{"cell_type":"markdown","source":"I'm going to combine text and title columns so we can inspect the whole text in general","metadata":{}},{"cell_type":"code","source":"raw_txt = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_txt['category'] = news.category\nraw_txt['text'] = news['text'] + \" \" + news['title']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_txt['text'] = raw_txt.apply(lambda x: x['text'].lower(),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_txt.text = raw_txt.text.str.replace('[^\\w\\s]','')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## General text analisys","metadata":{}},{"cell_type":"markdown","source":"### Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"#### Perform data cleaning process so we can proceed with ML part","metadata":{}},{"cell_type":"code","source":"stop = set(stopwords.words('english'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemming(text):\n    wnl = nlp.stem.WordNetLemmatizer()\n    stopwords = nlp.corpus.stopwords.words('english')\n    text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n    words = re.sub(r'[^\\w\\s]', '', text).split()\n    return [wnl.lemmatize(word) for word in words if word not in stopwords]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = remove_stopwords(text).lower()\n    re.sub(r'http\\S+', '', text)\n    re.sub(r'[^\\w\\s]', '', text)\n    wnl = nlp.stem.WordNetLemmatizer()\n    \n    return wnl.lemmatize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GetModelScores (estimator, X_train, y_train):\n    from sklearn.model_selection import cross_validate\n    \n    estimator.fit(X_train, y_train)\n    \n    scoring = {'acc': 'accuracy','f1_score': 'f1_weighted','roc_auc': 'roc_auc'}\n    \n    scores = cross_validate(estimator, X_train, y_train, cv=5, scoring = scoring,  return_train_score=True)\n    \n            \n    #print(scores.keys())\n    print(f\"Accuracity on Training: {scores['train_acc'].max()} and Validating: {scores['test_acc'].max()}\")\n    print(f\"F1_Score on Training: {scores['train_f1_score'].max()} and Validating: {scores['test_f1_score'].max()}\")\n    print(f\"ROC_AUC Score on Training: {scores['train_roc_auc'].max()} and Validating: {scores['test_roc_auc'].max()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClfSwitcher(BaseEstimator):\n\n    def __init__(self, estimator = LogisticRegression()):\n\n        self.estimator = estimator\n\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_words = stemming(''.join(str(raw_txt[raw_txt.category == 0]['text'].tolist())))\nreal_words = stemming(''.join(str(raw_txt[raw_txt.category == 1]['text'].tolist())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I joined all the rows for each fake and real news to list.","metadata":{}},{"cell_type":"code","source":"raw_txt['text']=raw_txt['text'].apply(denoise_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performed general denoisation in the text","metadata":{}},{"cell_type":"markdown","source":"#### N_Grams for Real News and Fake","metadata":{}},{"cell_type":"markdown","source":"Biagram Fake news","metadata":{}},{"cell_type":"code","source":"pd.Series(nlp.ngrams(fake_words,2)).value_counts()[:30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Biagram Real news","metadata":{}},{"cell_type":"code","source":"pd.Series(nlp.ngrams(real_words,2)).value_counts()[:30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Triagram Fake news","metadata":{}},{"cell_type":"code","source":"pd.Series(nlp.ngrams(fake_words,3)).value_counts()[:30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Triagram Real news","metadata":{}},{"cell_type":"code","source":"pd.Series(nlp.ngrams(real_words,3)).value_counts()[:30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we saw how frequent 2 or 3 words we can see in the text. Most of them are in USA and aiming presidents and overseas relations. ","metadata":{}},{"cell_type":"markdown","source":"#### Cloud of Words","metadata":{}},{"cell_type":"code","source":"# Start with one review:\ngeneric_words = \" \".join(raw_txt[raw_txt.category == 1].text)\n\nplt.figure(figsize  = (15,10))\n# Create and generate a word cloud image:\nwordcloud_ALL = WordCloud(max_font_size=500, max_words=500, background_color=\"black\", width = 1600 , height = 800).generate(\n    generic_words)\nplt.imshow(wordcloud_ALL, interpolation='bilinear')\nplt.title('Popular REAL news words', fontsize=30)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start with one review:\ngeneric_words = \" \".join(raw_txt[raw_txt.category == 0].text)\n\nplt.figure(figsize  = (15,10))\n# Create and generate a word cloud image:\nwordcloud_ALL = WordCloud(max_font_size=500, max_words=500, background_color=\"black\", width = 1600 , height = 800).generate(\n    generic_words)\nplt.imshow(wordcloud_ALL, interpolation='bilinear')\nplt.title('Popular FAKE news words', fontsize=30)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=raw_txt[raw_txt['category']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Real text')\ntext_len=raw_txt[raw_txt['category']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Fake text')\nfig.suptitle('Words in texts')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See words frequency in the text. Fake news are greather than Real news.","metadata":{}},{"cell_type":"markdown","source":"## Machine Learning","metadata":{}},{"cell_type":"markdown","source":"In this part we are going to see which algorithm is the best and is there a significant difference between TFIDF and Count Vectorisers.","metadata":{}},{"cell_type":"code","source":"X = raw_txt['text']\ny = raw_txt['category']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, shuffle = True,\n                                                   stratify = y, test_size = 0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_vectorised = TfidfVectorizer().fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_vectorised)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tuple represents: (document_id, token_id)\n\nThe value following the tuple represents the tf-idf score of a given token in a given document\n\nThe tuples that are not there have a tf-idf score of 0","metadata":{}},{"cell_type":"code","source":"GetModelScores(LogisticRegression(), X_train_vectorised, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetModelScores(DecisionTreeClassifier(), X_train_vectorised, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetModelScores(MultinomialNB(), X_train_vectorised, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetModelScores(SGDClassifier(), X_train_vectorised, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have really good scores without any tuning?! Let's see with a little bit am I going to overfitt a lot or ... ?","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline([\n    \n    ('vect', TfidfVectorizer()),\n    ('clf', ClfSwitcher())\n    \n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1) Logistic Rregression","metadata":{}},{"cell_type":"code","source":"parameters_log = [\n\n    {\n        'clf__estimator': [LogisticRegression()],\n        'vect__max_df': [0.75],\n        'clf__estimator__penalty': ['l2'],\n        'clf__estimator__C': [1,10,100]\n        \n    }\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_log = GridSearchCV(pipeline, parameters_log, scoring = 'f1')\ngscv_log.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_log.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_log.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2) Decision Trees","metadata":{}},{"cell_type":"code","source":"parameters_DT = [\n    {\n        'clf__estimator': [DecisionTreeClassifier()], \n        'vect__max_df': [0.75],\n        'clf__estimator__criterion': ['entropy'],\n        'clf__estimator__max_depth': [2, 20, 50, 100],\n        'clf__estimator__min_samples_leaf': [1, 2, 4]  \n    }\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_DT = GridSearchCV(pipeline, parameters_DT,  scoring = 'f1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_DT.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_DT.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_DT.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MODEL EVALUATION ","metadata":{}},{"cell_type":"code","source":"def ModelElavuation(estimator, X_train, y_train, X_test, y_test):\n    from sklearn.metrics import plot_confusion_matrix\n    \n    \n    scores_test = pd.DataFrame(columns= ['Accuracy','F1 Score','Precision','Recall', 'ROC_AUC'])\n    \n    try:\n        score_train = estimator.predict_proba(X_train)[:,1]\n        roc_train= roc_auc_score(y_train, score_train, average = \"weighted\")\n    except:\n        roc_train = 0\n        \n    try:\n        score_test = model.predict_proba(X_test)[:,1]\n        roc_test= roc_auc_score(y_test, score_test, average = \"weighted\")\n    except:\n        roc_test = 0\n\n    scores_test['Accuracy'] = accuracy_score(y_test, estimator.predict(X_test))*100,\n    scores_test['F1 Score'] = f1_score(y_test, estimator.predict(X_test), average = \"weighted\")*100,\n    scores_test['Precision'] = precision_score(y_test, estimator.predict(X_test), average = \"weighted\")*100,\n    scores_test['Recall'] = recall_score(y_test, estimator.predict(X_test), average = \"weighted\")*100,\n    scores_test['ROC_AUC'] = roc_train*100\n    \n\n    print(scores_test)\n    print(\"\")\n    plt.title('Train set')\n    sns.heatmap(confusion_matrix(y_train, estimator.predict(X_train)), annot = True)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    print(\"\")\n    plt.title('Test set')\n    sns.heatmap(confusion_matrix(y_test, estimator.predict(X_test)), annot = True)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ModelElavuation(gscv_log, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ModelElavuation(gscv_DT, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion (1)","metadata":{}},{"cell_type":"markdown","source":"What I heave GREAT results with those 2 ML algorithms???? But are they really great once or mabye Fake news and Real news have key distinguishable differences? How is it possible to predict that great those 2 categories?","metadata":{}},{"cell_type":"markdown","source":"## Further Inspection","metadata":{}},{"cell_type":"markdown","source":"I trained only the text column since the date if I input True news before 2016 I can be missclassified or the news' subject as well. Also the subject column is not accurate at all as we saw we have only 2 for Real News","metadata":{}},{"cell_type":"markdown","source":"Let me just go back to Real news dataset and Fake one :","metadata":{}},{"cell_type":"code","source":"real_news","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see with naked eye the (Reuters) in the real news dataset. Reuters is internation information and news provider organization. Maybe it's some kind of CODE word for real news. Let's see the % of this wor in both sets:","metadata":{}},{"cell_type":"code","source":"print('Real news have: ', real_news.text.str.contains('(Reuters)').sum()/len(real_news))\nprint('Fake news have: ', fake_news.text.str.contains('(Reuters)').sum()/len(fake_news))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes so basically it really show the test set what to category should be and maybe other observations are those who are misclassified.","metadata":{}},{"cell_type":"markdown","source":"In the fake news set I see 21st Century Wire... Let's see the % again.","metadata":{}},{"cell_type":"code","source":"print('Real news have: ', real_news.text.str.contains('21st Century').sum()/len(real_news))\nprint('Fake news have: ', fake_news.text.str.contains('21st Century').sum()/len(fake_news))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's inspect for dublicated text and remove those rows.","metadata":{}},{"cell_type":"code","source":"news[news.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have to get rid of these duplicated rows because mabye they can go to training and testing sets. We will have leaked label to the validation/training set. The number of rows are 209, it should be risky but let's see the results.","metadata":{}},{"cell_type":"code","source":"plt.title('Dublicated observations for category')\nsns.countplot(news[news.duplicated()]['category'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The majority is in the real news. Before droping the duplicated rows we have advantage for Fake news and after dropping them we will have 206 less and it's going to unbalance more. ","metadata":{}},{"cell_type":"code","source":"news_adjusted = news.drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_adjusted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I will remove (Reuters) and (21st Century Wire) in each oservation so I can make it more fair and less high biased data.","metadata":{}},{"cell_type":"code","source":"news_adjusted['text'] = news_adjusted['title'] + ' ' + news_adjusted['text'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_adjusted['text'] = news_adjusted['text'].replace('(Reuters)', '')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_adjusted['text'] = news_adjusted['text'].replace('21st Century Wire', '')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_adjusted.drop('title', axis  = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine Learning (2)","metadata":{}},{"cell_type":"markdown","source":"Let's try again to see if we can gat less high biased models","metadata":{}},{"cell_type":"code","source":"news_adjusted['text']=news_adjusted['text'].apply(denoise_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_adjusted = news_adjusted['text']\ny_adjusted = news_adjusted['category']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_adjusted_train, X_adjusted_test, y_adjusted_train, y_adjusted_test = train_test_split(X_adjusted, y_adjusted,\n                                random_state = 42, shuffle = True, stratify = y_adjusted, test_size = 0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_adjusted_train.shape, y_adjusted_train.shape, X_adjusted_test.shape, y_adjusted_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_adjusted_train_vectorised = TfidfVectorizer().fit_transform(X_adjusted_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetModelScores(LogisticRegression(), X_adjusted_train_vectorised, y_adjusted_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetModelScores(DecisionTreeClassifier(), X_adjusted_train_vectorised, y_adjusted_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters_DT = [\n    {\n        'clf__estimator': [DecisionTreeClassifier()], \n        'vect__max_df': [0.75],\n        'clf__estimator__criterion': ['entropy'],\n        'clf__estimator__max_depth': [2, 20, 50, 100],\n        'clf__estimator__min_samples_leaf': [1, 2, 4]  \n    }\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_DT_adjusted = GridSearchCV(pipeline, parameters_DT,  scoring = 'f1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_DT_adjusted.fit(X_adjusted_train, y_adjusted_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gscv_DT_adjusted.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ModelElavuation(gscv_DT_adjusted, X_adjusted_train, y_adjusted_train, X_adjusted_test, y_adjusted_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion|","metadata":{}},{"cell_type":"markdown","source":"So basically in general we have perfect models for only this dataset. I inspected some of the data leakage. The method for gathering data is not good because as we saw, we have \"KEY\" words (like Reuters or 21st Century) which indicates the model. We don't want this high bias or \"cheat tuned\" data. We saw with naked eye but obviusly the problem is still there, even though we did some more cleaning. I would suggest for more adequate data gathering and not putting \"CHEATING\" words in there. \n\nIn news filter we have to really be careful for misclassification because for False Positive will lead to backlash from the author/publisher and their readers who trust them, resulting in distrust of the filter and their administrator.\nOr for False Negatives Real news will not get to the people and they will miss maybe important information.  ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}