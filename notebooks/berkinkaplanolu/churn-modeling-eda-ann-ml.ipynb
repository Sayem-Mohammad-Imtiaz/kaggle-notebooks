{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"part0\"></a>\n# 0.Abstract:\n\nIn this analysis after numerically understand data.I visualized the dataset and tried to understand relations between columns.And i dropped tenure column after EDA. I transformed categorical values into numerical.Gender column label encoded and Geography column OneHotEncoded.To perform neural network better. I applied min max scaling to data.And I split dataset into 3 parts,Training ,Test and validation.NN model has 1 input layer and 5 hidden layer.I experimented a little bit at neural network architecture.Model i used gave best result in my experiences. I also tried Logistic Regression ,SVC and Random Forest Classifier. Because of our dataset is imbalanced. Result of Logistic regression and SVC are bad. But Random Forest Classifier gave almost same results with neural network.\n\n**Used Libraries:**\n* 1. Pandas\n* 2. Numpy\n* 3. Seaborn\n* 4. Matplotlib\n* 5. Sklearn\n* 6. Tensorflow\n\n![](https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.f8AX~1c8f3/w:1200/h:675/q:auto/https://www.unite.ai/wp-content/uploads/2019/11/artificial-neural-network-3501528_1920.png)\n\n**Table of contents:**\n\n* [0.Abstract:](#part0)\n* [1.Understanding Dataset:](#part1)\n* [2.Exploratory Data Analysis:](#part2)\n* [3.Encoding:](#part3)\n* [4.Normalization(minmax scaling) and Splitting the dataset:](#part4)\n* [5.Building the Neural Network Model:](#part5)\n* [6.Training the Neural Network:](#part6)\n* [7.Traditional Methods:](#part7)\n* [8.Conclusion:](#part8)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part1\"></a>\n# 1.Understand the Dataset:\n\nLets understand data with numbers.\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns # For visualization\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/churn-modelling/Churn_Modelling.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset has 10.000 rows and 14 columns.For training we don't need \"RowNumber\",\"CustomerId\" and \"Surname\" columns. Lets start with dropping these columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns = [\"RowNumber\",\"CustomerId\",\"Surname\"], inplace = True) #3 Columns dropped.\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second, Lets check balance of data and NaN values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Exited.value_counts(),data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like dataset has no Nan values but dataset is unbalanced. %80 of target value is 0.We can balance it but we are going to use neural network for training.We don't need to balance dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part2\"></a>\n# 2.Exploratory Data Analysis:\nLets discover relations between Exited and rest of the data.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,ax = plt.subplots(nrows=2, ncols=2 , figsize=(25,10))\nsns.barplot(x= pd.cut(data.CreditScore,bins = 5).unique().sort_values(),y = data.groupby(pd.cut(data.CreditScore,bins = 5)).mean()[\"Exited\"], ax = ax[0,0]),ax[0,0].set(xlabel='Credit Score')\nsns.barplot(x=np.sort(data.Geography.unique()),y = data.groupby(data.Geography).mean()[\"Exited\"],ax = ax[0,1])\nsns.barplot(x=np.sort(data.Gender.unique()),y = data.groupby(data.Gender).mean()[\"Exited\"],ax = ax[1,0])\nsns.barplot(x= pd.cut(data.Age,bins = 20).unique().sort_values(),y = data.groupby(pd.cut(data.Age,bins = 20)).mean()[\"Exited\"],ax = ax[1,1]),ax[1,1].set(xlabel='Customer Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like lower credit score means company is more likely to lose customer. Also being at age range between 45-65 means also company is going to lose customer.At figure [1,0] male customers seems like more loyal than female customers.Let's dig a little bit more.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,ax1 = plt.subplots(nrows=2, ncols=2 , figsize=(25,10))\nsns.barplot(x = data.Tenure.unique(),y = data.groupby(data.Tenure).mean()[\"Exited\"] , ax = ax1[0,0]),ax1[0,0].set(xlabel='Tenure')\nsns.barplot(x = data.NumOfProducts.unique(),y = data.groupby(data.NumOfProducts).mean()[\"Exited\"]*100 , ax = ax1[0,1]),ax1[0,1].set(xlabel=\"Number Of Products\")\nsns.boxplot(data.Balance, ax = ax1[1,0], color = \"lime\"),ax1[1,0].set(xlabel=\"Checking for outliers at Balance column\")\nsns.barplot(x = pd.cut(data.Balance , bins = 10).unique().sort_values(),y = data.groupby(pd.cut(data.Balance, bins = 10)).mean()[\"Exited\"]*100, ax=ax1[1,1]),ax1[1,1].set(xlabel=\"Balance column's customer loss percent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first graph, Company customer losses has no relation with customer membership duration. We can drop this column for better training.Second graph sayin, customers who has 2 and 4 products is more likely to exit.Last 2 figures about balance column. First one is for outliers at balance column and second one is customer loss rate.Customers with balance of +200k, have higher chance to exit.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.drop(columns = [\"Tenure\"],inplace = True)# I dropped \"Tenure\" column here.\nfig,ax2 = plt.subplots(nrows=2, ncols=2 , figsize=(25,10))\nsns.barplot(x = data.HasCrCard.unique(),y = data.groupby(data.HasCrCard).mean()[\"Exited\"] , ax = ax2[0,0]),ax2[0,0].set(xlabel='Has Credit Card')\nsns.barplot(x = data.IsActiveMember.unique(),y = data.groupby(data.IsActiveMember).mean()[\"Exited\"], ax = ax2[0,1]),ax2[0,1].set(xlabel=\"Is Active Member Column\")\nsns.boxplot(data.EstimatedSalary, ax = ax2[1,0], color = \"purple\"),ax2[1,0].set(xlabel=\"Checking for outliers at Estimated Salary column\")\nsns.barplot(x = pd.cut(data.EstimatedSalary , bins = 10).unique().sort_values(),y = data.groupby(pd.cut(data.EstimatedSalary, bins = 10)).mean()[\"Exited\"]*100, ax=ax2[1,1]),ax2[1,1].set(xlabel=\"Estimated Salary column's customer loss percent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"HasCrCard\" and \"EstimatedSalary\" columns have no pattern at \"Exited\" column.We might drop these columns too. Active members chance of leaving company is double times.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.drop(columns = [\"HasCrCard\",\"EstimatedSalary\"],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When i dropped these columns its effectted NN negatively but randomforest worked same.So i decided to not drop.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part3\"></a>\n# 3.Encoding:\n\nIn dataset we have 2 categorical columns. For geography column we're going to apply OneHotEncoding and use Label encoder for the gender column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n\ndata.Gender = LabelEncoder().fit_transform(data.Gender) # Gender label encoding\ndata = pd.concat((pd.DataFrame(OneHotEncoder(sparse = False).fit_transform(pd.DataFrame(data.Geography)),columns = [\"FromFrance\",\"FromSpain\",\"FromGermany\"]),data),axis = 1) #Geography onehotencoding\ndata.drop(columns= [\"Geography\"],inplace = True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good dataset looks better now.Before training one last thing left to do. Lets scale Balance, CreditScore and EstimatedSalary columns.Because we're going to perform Neural Network for training and NN works better with scaled data.I'am going to apply MinMaxScaler to our data for better neural network training.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part4\"></a>\n# 4.Normalization(minmax scaling) and Split the dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndata[[\"CreditScore\",\"Age\",\"Balance\",\"EstimatedSalary\"]] = MinMaxScaler(copy = False).fit_transform(data[[\"CreditScore\",\"Age\",\"Balance\",\"EstimatedSalary\"]])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data.iloc[:,:-1]\ny = data.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25,random_state = 10)\nvalitrain, X_test, valitest, y_test = train_test_split(X_test,y_test,test_size = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We finished preprocessing stage.Now We're going to create a neural network and use it for training.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part5\"></a>\n# 5.Building the Neural Network Model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(32 , activation = \"relu\",input_shape=(X_train.shape[1],)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(1,activation = \"sigmoid\")\n])\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),loss = tf.losses.BinaryCrossentropy(),metrics= [\"accuracy\",'binary_crossentropy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part6\"></a>\n# 6.Training the Neural Network:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"history = model.fit(X_train, y_train , validation_data = (valitrain,valitest),epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = np.round_(y_pred)\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test,y_pred)\ncm1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test,batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"85 percent accuracy not too bad. But recall percentage = 0.55 means we are predicting lost customers with %55 accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = pd.DataFrame(history.history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets visualize results.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1,figsize = (15,8))\nsns.lineplot(x = range(0,100),y = hist.binary_crossentropy , color = \"green\",label='Training binary crossentropy')\nsns.lineplot(x = range(0,100),y = hist.val_binary_crossentropy , color = \"blue\",label='Validation binary crossentropy')\nsns.lineplot(x = range(0,100),y = hist.accuracy , color = \"red\",label='Training Accuracy')\nsns.lineplot(x = range(0,100),y = hist.val_accuracy , color = \"purple\",label='Validation Accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like 30 epochs is better for training.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part7\"></a>\n# 7.Traditional Methods:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I also want to try some traditional ML methods like SVC ,Random Forest and Logistic Regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LogisticRegression()\nsvc = SVC()\nrf = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.fit(X_train,y_train)\nsvc.fit(X_train,y_train)\nrf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predle = le.predict(X_test)\ny_predsvc = svc.predict(X_test)\ny_predrf = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predle))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predsvc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predrf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmrf = confusion_matrix(y_test, y_predrf)\ncmsvc = confusion_matrix(y_test, y_predsvc)\ncmrf , cm1 , cmsvc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part8\"></a>\n# 8.Conclusion:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier and ANN gave us best results.Svc and Logistic regression seems failed a little bit.That because we are dealing with imbalanced dataset.To improve SVC and LogReg results.We can try smote for upsampling and train again.But i'm not going to do that. If you want to learn how to implement upsampling you can check my other notebooks.I tried couple things at ANN but i can't improve the results and ı don't know what to expect accuracy at loss customers. So do you have any suggestions to improve ANN result. Thanks for your time.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}