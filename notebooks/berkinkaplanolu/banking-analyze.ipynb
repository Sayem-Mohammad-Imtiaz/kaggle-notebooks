{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install imblearn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.over_sampling import SMOTE\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset:\nInput variables:\n\n   **# bank client data:**\n   \n   1 - ID : ID of client\n   \n   2 - age (numeric)\n   \n   3 - job : type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n   \n   4 - marital : marital status (categorical: \"divorced\",\"married\",\"single\"; note: \"divorced\" means divorced or widowed)\n   \n   5 - education (categorical: \"primary\",\"secondary\",\"tertiary,\"unknown\")\n   \n   6 - default: has credit in default? (categorical: \"no\",\"yes\")\n   \n   7 - balance : has money in account? (numeric)\n   \n   8 - housing: has housing loan? (categorical: \"no\",\"yes\")\n   \n   9 - loan: has personal loan? (categorical: \"no\",\"yes\")\n   \n   10 - contact: contact communication type (categorical: \"cellular\",\"telephone\") \n   \n   11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n   \n   12 - day: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n   \n   13 - duration: last contact duration, in seconds (numeric). Important note:  this attribute highly affects the output target (e.g., if duration=0 then y=\"no\"). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n   \n   14 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n   \n   15 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\n   \n   16 - previous: number of contacts performed before this campaign and for this client (numeric)\n   \n   17 - poutcome: outcome of the previous marketing campaign (categorical: \"failure\",\"unknown\",\"other\",\"success\")\n   \n  \n   **Output variable (desired target):**\n   \n   18 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n   \n   We can't use test.csv because of we don't have y_test values.In this notebook I'am going to use only train.csv.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/banking-dataset-marketing-targets/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/banking-dataset-marketing-targets/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape , test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1.Filling NaN(unknown) Values:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum() , test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **FILLING UNKNOWN VALUES :**\nIt's look like dataset has no NaN values but dataset has \"unknown\" values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"job\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can fill unknown job values with mode value but also we can fill unknown job values with mode of education degree per job.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eduRatio = pd.DataFrame({'Job' : []})\nfor i in train[\"job\"].unique():\n    eduRatio = eduRatio.append(train[(train[\"job\"] == i)][\"education\"].value_counts().to_frame().iloc[0] * 100 / train[(train[\"job\"] == i)][\"education\"].value_counts().sum())\neduRatio[\"Job\"] = train[\"job\"].unique()\neduRatio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.job == \"unknown\") & (train.education == \"secondary\"),\"job\"] = \"services\"\ntrain.loc[(train.job == \"unknown\") & (train.education == \"primary\"),\"job\"] = \"housemaid\"\ntrain.loc[(train.job == \"unknown\") & (train.education == \"tertiary\"),\"job\"] = \"management\"\ntrain.loc[(train.job == \"unknown\"),\"job\"] = \"blue-collar\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"job\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"marital\"].value_counts() , test[\"marital\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"education\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can fill unknown values at education column with eduRatio again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.education == \"unknown\") & (train.job == \"admin.\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"management\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"services\"),\"education\"] = \"tertiary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"technician.\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"retired\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"blue-collar\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"housemaid.\"),\"education\"] = \"primary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"self-employed\"),\"education\"] = \"tertiary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"student\"),\"education\"] = \"secondary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"entrepreneur\"),\"education\"] = \"tertiary\"\ntrain.loc[(train.education == \"unknown\") & (train.job == \"unemployed\"),\"education\"] = \"secondary\"\n#REST CAN BE SECONDARY\ntrain.loc[(train.education == \"unknown\"),\"education\"] = \"secondary\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"education\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"education\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"default\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"housing\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"contact\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"contact\"].replace([\"unknown\"],train[\"contact\"].mode(),inplace = True) # I replace unknown contact values with mode value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"contact\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"poutcome\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No need for ID column for training. Also dataset has pday column(number of days that passed by after last call).No need for \"day\" , \"month\" column for training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(columns = [\"ID\",\"day\",\"month\"],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"subscribed\"].value_counts() #We can see here dataset imbalanced.Later we are going to augment the data with smote.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2.Encoding:**\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to transform all categorical columns to numeric columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#OneHotEncoding of job column\nohe = OneHotEncoder(sparse = False)\ntrain = pd.concat((train , pd.DataFrame(ohe.fit_transform(train[\"job\"].to_frame()),columns = \"job_\" + np.sort(train[\"job\"].unique()))),axis = 1)\ntrain.drop(columns = [\"job\"],inplace = True)\n#Marital column has 3 values lets apply OneHotEncoding again.\ntrain = pd.concat((train , pd.DataFrame(ohe.fit_transform(train[\"marital\"].to_frame()),columns = \"marital_\" + np.sort(train[\"marital\"].unique()))),axis = 1)\ntrain.drop(columns = [\"marital\"],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good! We can label encode education column.Because its ordinal data.Also we can transform default column yes/no values to 0 and 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.education == \"tertiary\"),\"education\"] = 2\ntrain.loc[(train.education == \"secondary\") ,\"education\"] = 1\ntrain.loc[(train.education == \"primary\"),\"education\"] = 0\n#Default Column\ntrain.loc[(train.default == \"yes\"),\"default\"] = 1\ntrain.loc[(train.default == \"no\") ,\"default\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.balance.sort_values() # We have 2 outliner data.We can change their value to new maximum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.balance > 66721),\"balance\"] = 66721\n#Lets scale balance column.\ntrain[\"balance\"]= train[\"balance\"] / 66721","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Housing , loan and contact columns label encoding and dropping duration column(read intro)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.housing == \"yes\"),\"housing\"] = 1 # housing column label encoding\ntrain.loc[(train.housing == \"no\") ,\"housing\"] = 0\n\n#Loan column label encoding\ntrain.loc[(train.loan == \"yes\"),\"loan\"] = 1\ntrain.loc[(train.loan == \"no\") ,\"loan\"] = 0\n\n#contact column label encoding\ntrain.loc[(train.contact == \"telephone\"),\"contact\"] = 1 # 0 means cellular 1 means telephone\ntrain.loc[(train.contact == \"cellular\") ,\"contact\"] = 0\n\ntrain.drop(columns = [\"duration\"],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pdays means how many days past after last contact.So we should change -1(non-called) values to something big like 999","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.pdays == -1),\"pdays\"] = 999","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 column left to encode before training.I'm going to use label encoding for subscribed , OneHotEncoding for poutcome column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat((train , pd.DataFrame(ohe.fit_transform(train[\"poutcome\"].to_frame()),columns = \"poutcome_\" + np.sort(train[\"poutcome\"].unique()))),axis = 1)\ntrain.drop(columns = [\"poutcome\"],inplace = True)\n\ntrain.loc[(train.subscribed == \"yes\"),\"subscribed\"] = 1 # 0 means subscribed no 1 means yes\ntrain.loc[(train.subscribed == \"no\") ,\"subscribed\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before training we should transform object dtypes to int because some classifiers won't work with object dtype.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.education = train.education.astype(int)\ntrain.default = train.default.astype(int)\ntrain.housing = train.housing.astype(int)\ntrain.loan = train.loan.astype(int)\ntrain.contact = train.contact.astype(int)\ntrain.subscribed = train.subscribed.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(train.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like nothing highly correlated with subscribed column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.Training :","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Lets split the data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.subscribed.to_frame()\nX = train.drop(columns = [\"subscribed\"])\nX_train , X_test , y_train , y_test = train_test_split(X,y, test_size = 0.25, random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LogisticRegression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_predlr = lr.predict(X_test)\ncmlr = confusion_matrix(y_test, y_predlr)\nacclr = accuracy_score(y_test, y_predlr)\ncmlr , acclr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's look like we have 90 percent accuracy.But this predictions quite wrong because our accuracy at positive predicted values is : 6927 / (6927 + 63) = 0.99 but our accuracy will be low at negative predicted values: 134 / (788 + 134) = 0.14. This is happening because we did prediction at imbalanced dataset.Let's balance dataset with SMOTE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.Data Augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE()\nX_sm , y_sm = sm.fit_resample(X, y)\ny_sm.subscribed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.Traning after SMOTE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data balanced. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm , X_test_sm , y_train_sm , y_test_sm = train_test_split(X_sm,y_sm, test_size = 0.25, random_state = 10)\nlr2 = LogisticRegression()\nlr2.fit(X_train_sm,y_train_sm)\ny_predlr2 = lr2.predict(X_test_sm)\ncmlr2 = confusion_matrix(y_test_sm, y_predlr2)\nacclr2 = accuracy_score(y_test_sm, y_predlr2)\ncmlr2 , acclr2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This time our accuracy around 72 percent. But as you can see our prediction improved at Specificty in confusion matrix.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train_sm, y_train_sm)\ny_predsvc = svc.predict(X_test_sm)\ncmsvc = confusion_matrix(y_test_sm, y_predsvc)\naccsvc = accuracy_score(y_test_sm, y_predsvc)\ncmsvc , accsvc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(X_train_sm, y_train_sm)\ny_predknn = knn.predict(X_test_sm)\ncmknn = confusion_matrix(y_test_sm, y_predknn)\naccknn = accuracy_score(y_test_sm, y_predknn)\ncmknn , accknn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train_sm, y_train_sm)\ny_predrf = rf.predict(X_test_sm)\ncmrf = confusion_matrix(y_test_sm, y_predrf)\naccrf = accuracy_score(y_test_sm, y_predrf)\ncmrf , accrf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.Evaluation\nSensitivity : True Positive / (True Positive + False Negative) , Specificity : True Negative / (True Negative + False Negative)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Logistic Regression accuracy without SMOTE :{acclr * 100} Sensivity :{cmlr[0,0] * 100 / (cmlr[0,0] + cmlr[0,1])} Specificity : {cmlr[1,1] * 100 / (cmlr[1,1] + cmlr[1,0])}\")\nprint(f\"Logistic Regression accuracy with SMOTE :{acclr2 * 100} Sensivity :{cmlr2[0,0] * 100 / (cmlr2[0,0] + cmlr2[0,1])} Specificity : {cmlr2[1,1] * 100 / (cmlr2[1,1] + cmlr2[1,0])}\")\nprint(f\"Support Vector Classifier accuracy with SMOTE :{accsvc * 100} Sensivity :{cmsvc[0,0] * 100 / (cmsvc[0,0] + cmsvc[0,1])} Specificity : {cmsvc[1,1] * 100 / (cmsvc[1,1] + cmsvc[1,0])}\")\nprint(f\"K Nearest Neighbors Classfier accuracy with SMOTE :{accknn * 100} Sensivity :{cmknn[0,0] * 100 / (cmknn[0,0] + cmknn[0,1])} Specifictiy : {cmknn[1,1] * 100 / (cmknn[1,1] + cmknn[1,0])}\")\nprint(f\"Random Forest Classifier accuracy with SMOTE :{accrf * 100} Sensivity :{cmrf[0,0] * 100 / (cmrf[0,0] + cmrf[0,1])} Specificity : {cmrf[1,1] * 100 / (cmrf[1,1] + cmrf[1,0])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without any hyperparameter tuning Random Forest Classifier gave us best result. Before smote our specificity values was low around %13. Dataset was imbalance and we balanced dataset with smote. After SMOTE, maybe accuracy of classifications seems a little bit low. But evaluate results with just \"accuracy\" won't proof anything.Sensitivity decreased with smote but specificity increased 10 times. If you have any question or suggestion please comment.Thanks for your time.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}