{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. [Import Libraries and Data](#1)\n    * 1.1 [Import Libraries](#1.1)\n    * 1.2 [Load and Check Data](#1.2)\n2. [Variable Description](#2)\n    * 2.1 [Univariate Variable Analysis](#2.1)\n    * 2.2 [Selected Numerical Variable](#2.2)\n3. [Basic Data Analysis](#3)\n4. [Outlier Detection](#4)\n5. [Missing Value](#5)\n    * 5.1 [Find Missing Value](#5.1)\n6. [Visuzalization](#6)\n    * 6.1 [Correlation](#6.1)\n    * 6.2 [Radius Mean -- Diagnosis](#6.2)\n    * 6.3 [Perimeter Mean -- Diagnosis](#6.3)\n    * 6.4 [Area Mean -- Diagnosis](#6.4)\n    * 6.5 [Concavity Points Mean -- Diagnosis](#6.5)  \n7. [Modeling](#7)\n    * 7.1 [Train Test Split](#7.1)\n    * 7.2 [Scaling](#7.2)\n    * 7.3 [Training](#7.3)\n    * 7.4 [Hyperparameter Tuning -- Grid Search -- Cross Validation](#7.4)\n    * 7.5 [Lets try with NN](#7.5)"},{"metadata":{},"cell_type":"markdown","source":"<a id='1' a></r>\n# 1. Import Libraries, Load and Check Data"},{"metadata":{},"cell_type":"markdown","source":"<a id='1.1' a></r>\n## 1.1 Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\nfrom collections import Counter\n\n\nimport warnings \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1.2' a></r>\n## 1.2 Load and Check Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2' a></r>\n# 2. Variable Description\n  \n\n  \nAttribute Information:\n\n1) ID number  \n2) Diagnosis (M = malignant, B = benign)  \n3-32)  \n  \nTen real-valued features are computed for each cell nucleus:  \n  \na) radius (mean of distances from center to points on the perimeter)  \nb) texture (standard deviation of gray-scale values)  \nc) perimeter  \nd) area  \ne) smoothness (local variation in radius lengths)  \nf) compactness (perimeter^2 / area - 1.0)  \ng) concavity (severity of concave portions of the contour)  \nh) concave points (number of concave portions of the contour)  \ni) symmetry  \nj) fractal dimension (\"coastline approximation\" - 1)  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2.1' a></r>\n## 2.1 Univariate Variable Analysis\n**Categorical Variable** : diagnosis  \n**Numerical Variable** : radius_mean, texture_mean, perimeter_mean, area_mean, smoothness_mean, compactness_mean, concavity_mean,    \n                      concave points_mean, symmetry_mean, fractal_dimension_mean, radius_se, texture_se, perimeter_se, area_se,  \n                      smoothness_se,compactness_se, concavity_se, concave points_se, symmetry_se, fractal_dimension_se,   radius_worst,\n                      texture_worst, perimeter_worst, area_worst, smoothness_worst, compactness_worst, concavity_worst, concave                             points_worst, symmetry_worst, fractal_dimension_worst, id  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the categorical data values 'diagnosis'\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\ndf.iloc[:,1] = labelencoder_Y.fit_transform(df.iloc[:,1].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a pair plot\nsns.pairplot(df.iloc[:, 1:6], hue = 'diagnosis');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2.2' a></r>\n## 2.2 Selected Numerical Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(df[variable],bins = 10)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with histogram\".format(variable))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_numericalVar = ['diagnosis','radius_mean', 'perimeter_mean', 'area_mean','concavity_mean', \"concave points_mean\"]\nfor n in selected_numericalVar:\n    plot_hist(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3' a></r>\n# 3. Basic Data Analysis\n* Radius Mean - Diagnosis\n* Perimeter Mean - Diagnosis\n* Area Mean - Diagnosis\n* Concavity Mean - Diagnosis\n* Concave Points Mean - Diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Radius Mean - Diagnosis\ndf[['radius_mean', 'diagnosis']].groupby(['radius_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perimeter  Mean - Diagnosis\ndf[['perimeter_mean', 'diagnosis']].groupby(['perimeter_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Area Mean - Diagnosis\ndf[['area_mean', 'diagnosis']].groupby(['area_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concavity Mean - Diagnosis\ndf[['concavity_mean', 'diagnosis']].groupby(['concavity_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concave Points Mean - Diagnosis\ndf[[\"concave points_mean\", 'diagnosis']].groupby([\"concave points_mean\"], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4' a></r>\n# 4.Outlier Detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_outlier(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3-Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # Detect Outlier and Their Indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # Store Indices\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[detect_outlier(df, ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n                           'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n                           'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n                           'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n                           'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n                           'fractal_dimension_se', 'radius_worst', 'texture_worst',\n                           'perimeter_worst', 'area_worst', 'smoothness_worst',\n                           'compactness_worst', 'concavity_worst', 'concave points_worst',\n                           'symmetry_worst', 'fractal_dimension_worst'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So we have 83 rows outlier variables, lets drop them"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop outliers \ndf= df.drop(detect_outlier(df, ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n                           'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n                           'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n                           'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n                           'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n                           'fractal_dimension_se', 'radius_worst', 'texture_worst',\n                           'perimeter_worst', 'area_worst', 'smoothness_worst',\n                           'compactness_worst', 'concavity_worst', 'concave points_worst',\n                           'symmetry_worst', 'fractal_dimension_worst']), axis = 0).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5' a></r>\n# 5. Missing Value"},{"metadata":{},"cell_type":"markdown","source":"<a id='5.1' a></r>\n# 5.1 Find Missing Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns[df.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Unnamed: 32']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Just because we have an unnecessary column, I directly delete it"},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['Unnamed: 32']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6' a></r>\n# 6. Visuzalization"},{"metadata":{},"cell_type":"markdown","source":"<a id='6.1' a></r>\n# 6.1 Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a colorful correlation matrix \nsns.heatmap(df.iloc[:,1:12].corr(), annot = True);\n# as you can see, it is much better than the table\n# but if you want you can use\n# df.iloc[:,1:12].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.2' a></r>\n# 6.2 Radius Mean -- Diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'radius_mean', bins = 2)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.3' a></r>\n# 6.3 Perimeter Mean -- Diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'perimeter_mean', bins = 5)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.4' a></r>\n# 6.4 Area Mean -- Diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'area_mean', bins = 50)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.5' a></r>\n# 6.5 Concavity Mean -- Diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'concavity_mean')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.6' a></r>\n# 6.5 Concavity Points Mean -- Diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, \"concave points_mean\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7' a></r>\n\n# 7. Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7.1' a></r>\n\n# 7.1 Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Split the data set into independent (X) and dependent(Y) data sets\nX = df.iloc[:,2:31].values\ny = df.iloc[:,1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data set into 67% training and 33% testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state = 123)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7.2' a></r>\n\n# 7.2 Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale tge data (Feature scaling)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc  = sc.fit_transform(X_test)\n\n#X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7.3' a></r>\n\n# 7.3 Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train)*100,2) \nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7.4' a></r>\n\n## 7.4 Hyperparameter Tuning -- Grid Search -- Cross Validation\nCompare 5 ML classifier and evaluate mean accuracy of each of them by stratified cross validation\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n              SVC(random_state = random_state),\n              RandomForestClassifier(random_state = random_state),\n              LogisticRegression(random_state = random_state),\n              KNeighborsClassifier()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_param_grid = {'min_samples_split': range(10,500,20),\n                'max_depth': range(1,20,2)}\nsvc_param_grid = {'kernel': ['rbf'],\n                  'gamma' : [0.001, 0.01, 0.1, 1],\n                  'C'     : [1,10,50,100,200,300,1000]}\nrf_param_grid = {\"max_features\": [1,3,10],\n                 \"min_samples_split\":[2,3,10],\n                 \"min_samples_leaf\":[1,3,10],\n                 \"bootstrap\":[False],\n                 \"n_estimators\":[100,300],\n                 \"criterion\":[\"gini\"]}\nlogreg_param_grid = {'C'      : np.logspace(-3,3,7),\n                     'penalty':['l1', 'l2']}\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                  \"weights\"    : [\"uniform\",\"distance\"],\n                  \"metric\"     :[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                    svc_param_grid,\n                    rf_param_grid,\n                    logreg_param_grid,\n                    knn_param_grid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result,\n                           \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\"LogisticRegression\",\"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7.5' a></r>\n\n## 7.5 Lets try with NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head() # diagnosis part already encoded\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,2:31].values\ny = df.iloc[:,1].values\n\nmin_max_scaler = MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split our data 80% training / 10% testing / 10% validation\nX_train, X_val_and_test, y_train, y_val_and_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 123)\n# so let split val_and_test datas\nX_val, X_test, y_val, y_test = train_test_split(X_val_and_test, y_val_and_test, test_size = 0.5, random_state = 123)\nprint(\"X_train shape :\" ,X_train.shape, \"X_val shape :\", X_val.shape, \"X_test shape :\",X_test.shape)\nprint(\"y_train shape :\" ,y_train.shape, \"y_val shape :\", y_val.shape, \"y_test shape :\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the model and architecture of the deep neural network\nmodel = Sequential() # innitializes the NN\nmodel.add(Dense(units = 32, activation= 'relu',input_dim = 29))\nmodel.add(Dense(units = 32, activation= 'relu'))\nmodel.add(Dense(units = 32, activation= 'relu'))\nmodel.add(Dense(units = 1, activation= 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss function measures how well the model did on training and then tries to improve on it using optimizer\nmodel.compile(optimizer='sgd',\n              loss = 'binary_crossentropy',\n              metrics = ['accuracy']\n              )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nhist = model.fit(\n    X_train, y_train,\n    batch_size = 32,\n    epochs = 100,\n    validation_data = (X_val, y_val)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)[1] # I want to see accuracy, thats the why I wrote [1]\n# It says 1 accurate, It's perfect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a prediction\nprediction = model.predict(X_test)\nprediction = [1 if y>=0.5 else 0 for y in prediction]\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the training loss and validation loss to see if the model is over fitting\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc = 'upper right');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It seems not over fitted\nhist.history['val_accuracy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the training accuracy and validation accuracy to see if the model is over fitting\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc = 'lower right');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, prediction))\nprint(accuracy_score(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}