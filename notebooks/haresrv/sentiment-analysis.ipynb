{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\nimport gc\nimport json\nfrom keras_preprocessing.text import tokenizer_from_json\nfrom keras.models import model_from_json\n\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"alexa = pd.read_csv('/kaggle/input/amazon-alexa-reviews/amazon_alexa.tsv' , delimiter = '\\t' \n                    ,usecols = ['verified_reviews' , 'feedback'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alexa = alexa.rename(columns={'verified_reviews':'reviews', 'feedback':'sentiment'})\ndisplay(alexa['sentiment'].value_counts()/alexa.shape[0]*100)\nprint('Shape of Dataset -> ' , alexa.shape)\ndisplay(alexa.sample(6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter = pd.read_csv('../input/twitter-sentiment/Sentiment Analysis Dataset 2.csv', skiprows=[8835,535881] , usecols = ['Sentiment' , 'SentimentText'])\ntwitter = twitter.rename(columns = {'Sentiment': 'sentiment' , 'SentimentText':'reviews'})\ndisplay(twitter['sentiment'].value_counts()/twitter.shape[0]*100)\nprint('Shape of Dataset -> ' , twitter.shape)\ndisplay(twitter.sample(6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb = pd.read_csv('/kaggle/input/imdb-review-dataset/imdb_master.csv', encoding = \"ISO-8859-1\")\nimdb=imdb[imdb['label']!='unsup']\n#Preprocessing\nimdb=imdb.drop(['Unnamed: 0','type','file'],axis=1)\nimdb.label[imdb.label == 'neg'] = 0\nimdb.label[imdb.label == 'pos'] = 1\nimdb=imdb.rename(columns = {'label': 'sentiment' , 'review':'reviews'})\ndisplay(imdb['sentiment'].value_counts()/imdb.shape[0]*100)\nprint('Shape of Dataset -> ' , imdb.shape)\ndisplay(imdb.sample(6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([alexa, twitter , imdb], axis= 0)\ndel alexa , twitter , imdb\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\ndisplay(data.sample(5))\ndata['sentiment'].value_counts()/data.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.sample(frac= 0.05 , random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en.stop_words import STOP_WORDS\n# stop_words = stopwords.words(\"english\")\nstop_words = STOP_WORDS\nstemmer = SnowballStemmer(\"english\")\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|<.*?>|[^A-Za-z0-9]+\"\nEmoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n\ndef preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    text = re.sub(Emoji_pattern, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata.reviews = data.reviews.apply(lambda x: preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndocuments = [_text.split() for _text in data.reviews] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MODEL-1\n\n*WORD-2-VEC MODEL FOR EMBEDDING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"W2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\nw2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)\n\n\nw2v_model.build_vocab(documents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nw2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.most_similar(positive=['awesome'],topn=100)\n# [x[0] for x in w2v_model.most_similar(\"awesome\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str([x[0] for x in w2v_model.most_similar(\"fantastic\",topn=100)]))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.title(\"SIMILAR WORDS FOR FANTASTIC\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str([x[0] for x in w2v_model.most_similar(\"poor\",topn=100)]))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.title(\"SIMILAR WORDS FOR POOR\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(data.reviews)\nvocab_size = len(tokenizer.word_index)+1\nprint('Vocab Size is ',vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nx_data = pad_sequences(tokenizer.texts_to_sequences(data.reviews) , maxlen = SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_data = data.sentiment\nprint(x_data.shape)\nprint(y_data.shape)\ny_data = y_data.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv['sample'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size , W2V_SIZE))\nfor word , i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding( vocab_size , W2V_SIZE , weights = [embedding_matrix] , input_length = SEQUENCE_LENGTH, trainable = False)\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(128 , dropout = 0.2 , recurrent_dropout = 0.2 ,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, dropout = 0.2 , recurrent_dropout = 0.2 ))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\ncallbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n# ReduceLRonPlateau is to reduce Learning rate when model stopeed improving\n# Early Stopping to stop learning when staturation is reached.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nhistory = model.fit(x_data , y_data , batch_size = BATCH_SIZE , epochs = EPOCHS , validation_split = 0.1  , verbose = 1 , callbacks = callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(text):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    # Predict\n    score = model.predict([x_test])[0]\n\n    return {\"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predict('i am Happy'))\nprint(predict('i not feeling so great .Little Rest can help but you decide what should i do next '))\nprint(predict('i am sitting in library for 6 hours . i learned alot but i am tired'))\nprint(predict('i am tired'))\nprint(predict('good is not good'))\nprint(predict('bad is not good'))\nprint(predict('good is not bad'))\nprint(predict('how i can end up here'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('model_weights.h5')\nwith open('model_architecture.json', 'w') as f:\n    f.write(model.to_json())\n    \nmodel.save('entire_model.h5')\ntokenizer_json = tokenizer.to_json()\nwith open('tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}