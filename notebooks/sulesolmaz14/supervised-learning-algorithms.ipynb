{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n# 1. **INTRODUCTON**\n\nContent: \n\n1. [Introduction](#1)\n1. [K-Nearest Neighbors (KNN) Classification](#2)\n1. [Support Vector Machine(SVM) Classification](#3)\n1. [Naive Bayes Classification](#4) \n1. [Decision Tree Classification](#5)\n1. [Random Forest Classification](#6)\n1. [Evaluation Classification Models :Confusion Matrix](#7)\n1. [Regression](#8)\n1. [Cross Validation (CV)](#9)\n1. [ROC Curve](#10)\n1. [Hyperparameter Tuning](#11)\n1. [Pre-processing Data](#12)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id = \"2\"></a><br>\n # 2. **K-Nearest Neighbors (KNN) Classification**\n* KNN: K nearest labeled data points.\n* Classification method.\n* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict() : predicts the data\n* x: features y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. It means that look at the 3 closest labeled data points.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot\ncolor_list = ['red' if i =='Abnormal' else 'green'  for i in data.loc[:, 'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'], c= color_list, figsize = [15,15], diagonal = 'hist', alpha=0.5, s =200, marker = '*', edgecolor = \"black\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#N = data[data.class == \"Normal\"]\n#A = data[data.class == \"Abnormal\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx, y = data.loc[:, data.columns != 'class'], data.loc[:, 'class']\nknn.fit(x, y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We fit the data and predict it with KNN.\n* We predict correct or what is our accuracy or the accuracy is best metric to evaluate or result?\n\nWe need to split our data train and test sets.\n* train: use train set by fitting\n* test: make prediction on test set.\n* With train and test sets, fitted data the tested data are completely different.\n* train_test_split(x,y,test_size = 0.3, random_state = 1)\n  * x:features\n  * y: target variables(normal, abnormal)\n  * test_size: percentage of test size.0.3 ==> test size = 30% and train size = 70%\n  * random_state:sets a seed. If this seed is same number, train_test_split() produce exact same split at each time.\n* fit(x_train, y_train): fit on train sets\n* score(x_test, y_test):predict and give accuracy on test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split \nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size = 0.3, random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx, y = data.loc[:, data.columns != 'class'], data.loc[:, 'class']\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)\nprint('With KNN (K=3) accuracy is: {}', knn.score(x_test, y_test))# accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* K is called a hyperparameter. We need to choose is that gives best performance.\n* If k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Complexity\nneig = np.arange(1,25)\ntrain_accuracy=[]\ntest_accuracy=[]\n\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    #Fit with knn\n    knn.fit(x_train, y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    #test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n# Plot\nplt.figure(figsize=[13, 8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n# 3. **Supports Vector Machine(SVM) Classification**\n* We can use this algorithm to divide our data into two different clusters.\n* **SVM ==>** finds best decision boundary. It allows me to draw the best line.\n* SVM will find the maximum margin for me.\n* **Maximum margin:** the sum of the distances of my data closest to the line.\n* **Support Vector:** Two closest points affecting maximum margin.\n* My goal is to maximize the margin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\nsvm.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of SVM Algorithm:\", svm.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n# 4. **Naive Bayes Classification**\n* Naive Bayes algorithm is used to find which label belongs to any point.\n* **Formula:** P (Math | X) = P (X | Math) * P (Math) / P (X)\n\n  * **P (Math | X)** : probability of Math according to X\n  * **P (X | Math):** likelihood\n  * **P (Math) :** prior probability \n  * **P (X) :** marginal likehood\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of Naive Bayes Algorithm:\", nb.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n# 5. **Decision Tree Classification**\n* **CART :** Classification And Regression Tree\n* On the basis of the Decision Trees are **splits**.\n* It draws the boundary in the best separation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of Decision Tree Algorithm:\", dt.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n# 6. **Random Forest Classification**\n* ***ensamble learning model :*** We combine several decision tree algorithms and get 1 Random Forest model.\n* We have a data and seperate according to train and test.\n* We select n sample in train part and this is call **subsample.**\n* We train the subsamples with the decision tree.\n* I train my subsamples with n decision trees. that is, I have one random forest model consisting of n decision trees.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of Random Forest Algorithm:\", rf.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n# 7. **Evaluation Classification Models :Confusion Matrix**\n* In this algorithm, I correctly estimated how much of the normal and abnormal data or abnormal data estimated how much of the normal data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(x_test)\ny_true = y_test\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred)\n#Visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***59 :*** TN ==> True negative\n\n***7  :*** FP ==> False positive(type1 error)\n\n***7  :*** FN ==> False negative(type2 error)\n\n***20 :*** TP ==> True positive","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}