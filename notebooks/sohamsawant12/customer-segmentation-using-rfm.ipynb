{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\nimport datetime as dt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp=pd.read_csv('/kaggle/input/ecommerce-data/data.csv')\ndf= pd.read_csv('/kaggle/input/ecommerce-data/data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df.isnull().sum())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Since CustomerID is missing ,there is no known way to find it .It is best to drop all those rows* .**","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df= df[df['Quantity']>=0]\ndf = df[df['UnitPrice']>=0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['totalcost']=df['Quantity']*df['UnitPrice']\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pi=df.groupby('Country').count().sort_values(\"InvoiceNo\",ascending = False)\npi.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nf=df['Country'].unique()\n\nfig = plt.figure(figsize =(10, 7)) \nplt.pie(pi['InvoiceNo'].head(6), labels= f[:6],autopct='%1.2f%%',explode = [0.2, 0, 0, 0,0,0])\nplt.title(\"Country wise Distribution\")\nplt.show() \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p=df.groupby('Description').count().sort_values(\"InvoiceNo\",ascending = False)\np=p.head(15)\n\nsns.barplot(p.index,p['Quantity'],palette = \"rocket\")\nplt.xlabel('Top Selling Items')\nplt.xticks(rotation=90)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\ndf['Date'] = df['InvoiceDate'].apply(lambda x: x.date())\ndf.head()\n\n\n#df['Date'] = df['InvoiceDate'].dt.date\n#df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recency_df = df.groupby(by='CustomerID', as_index=False)['Date'].max()\nrecency_df.columns = ['CustomerID','LastPurchaseDate']\nrecency_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"now = dt.date(2021,12,1)\nprint(now)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recency_df['Recency'] = recency_df['LastPurchaseDate'].apply(lambda x: (now - x).days)\n\n\nrecency_df.drop('LastPurchaseDate',axis = 1,inplace=True)\nrecency_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = df.copy()\ntemp.drop_duplicates(['InvoiceNo','CustomerID'],inplace=True)\nfrequency_df = temp.groupby(by=['CustomerID'], as_index=False)['InvoiceNo'].count()\nfrequency_df.columns = ['CustomerID','Frequency']\nfrequency_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monetary_df = df.groupby(by = 'CustomerID',as_index=False).agg({'totalcost':'sum'})\nmonetary_df.columns = ['CustomerID','TotalCost']\nmonetary_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfm = recency_df.merge(frequency_df,on='CustomerID').merge(monetary_df,on='CustomerID')\nrfm.set_index('CustomerID',inplace=True)\na=rfm.columns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rfm.corr())\nsns.heatmap(rfm.corr(),cmap=\"RdYlGn\",annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(rfm, diag_kind=\"hist\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nrfm = pd.DataFrame(pt.fit_transform(rfm))\nrfm.columns = a\nrfm.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nrfm_scaled = sc.fit_transform(rfm)\nrfm_scaled[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\npca_tranformed_data = pca.fit_transform(rfm_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = rfm.copy()\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(X)\n\ndf_pca = pd.DataFrame(df_pca)\ndf_pca.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df_pca.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans \n\ncluster_range = range(1, 8)\ncluster_errors = []\ncluster_sil_scores = []\n\nfor num in cluster_range: \n    clusters = KMeans(num, n_init = 100,init='k-means++',random_state=0)\n    clusters.fit(X)\n    labels = clusters.labels_                     # capture the cluster lables\n    centroids = clusters.cluster_centers_         # capture the centroids\n    cluster_errors.append( clusters.inertia_ )    # capture the intertia\nclusters_df = pd.DataFrame({ \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors} )\nclusters_df[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.plot(clusters_df[\"num_clusters\"],clusters_df[\"cluster_errors\"],marker = 'o')\nplt.xlabel('count of clusters')\nplt.ylabel('error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from  sklearn.metrics import silhouette_score\n\nfor num in range(2,16):\n    clusters = KMeans(n_clusters=num,random_state=0)\n    labels = clusters.fit_predict(df_pca)\n    \n    sil_avg = silhouette_score(df_pca, labels)\n    print('For',num,'The Silhouette Score is =',sil_avg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 4,random_state=0)\nkmeans = kmeans.fit(df_pca)\nlabels = kmeans.predict(df_pca)\ncentroids = kmeans.cluster_centers_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca['Clusters'] = labels\ndf_pca.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca['Clusters'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df_pca,diag_kind='hist',hue='Clusters',palette='rocket')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X = df_pca[[0,1]]\nY = df_pca['Clusters']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\nlr = LogisticRegression(max_iter=1000,random_state=0)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint('Test accuracy = ', accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}