{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.metrics import r2_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T12:31:23.369229Z","iopub.execute_input":"2021-05-31T12:31:23.369866Z","iopub.status.idle":"2021-05-31T12:31:24.438393Z","shell.execute_reply.started":"2021-05-31T12:31:23.369762Z","shell.execute_reply":"2021-05-31T12:31:24.437401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/bmw.csv')\nprint(\"Data shape:\")\nprint(df.shape)\nprint(\"\\n\\n\")\n\nprint(\"First 5 rows in df:\")\nprint(df.head())\nprint(\"\\n\\n\")\n\nprint(\"Checking if there is Null:\")\nprint(df.isnull().sum())\nprint(\"\\n\\n\")\n\nprint(\"df describing:\")\nprint(df.describe())\nprint(\"\\n\\n\")\n\nprint(\"Checking duplicates:\")\nprint(df.duplicated().sum())\n\n# Removing duplicates\ndf=df.drop_duplicates(keep='first')\nprint(\"Duplicates are removed.\\n\\n\")\n\n\nprint(\"Checking duplicates:\")\nprint(df.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:31:24.439894Z","iopub.execute_input":"2021-05-31T12:31:24.4402Z","iopub.status.idle":"2021-05-31T12:31:24.633526Z","shell.execute_reply.started":"2021-05-31T12:31:24.440168Z","shell.execute_reply":"2021-05-31T12:31:24.632645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA**","metadata":{}},{"cell_type":"code","source":"# Scatter plots of each pair of features\nsns.pairplot(data = df, hue = 'transmission')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:31:24.635262Z","iopub.execute_input":"2021-05-31T12:31:24.635559Z","iopub.status.idle":"2021-05-31T12:31:52.019612Z","shell.execute_reply.started":"2021-05-31T12:31:24.635528Z","shell.execute_reply":"2021-05-31T12:31:52.017737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plots of each pair of features\nsns.pairplot(data = df, hue = 'fuelType')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:31:52.021531Z","iopub.execute_input":"2021-05-31T12:31:52.022169Z","iopub.status.idle":"2021-05-31T12:32:20.963238Z","shell.execute_reply.started":"2021-05-31T12:31:52.022123Z","shell.execute_reply":"2021-05-31T12:32:20.962147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking for outliers\nplt.figure(figsize=(40,20))\nsns.boxplot(data=df[['price', 'mileage']])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:32:20.964637Z","iopub.execute_input":"2021-05-31T12:32:20.964957Z","iopub.status.idle":"2021-05-31T12:32:21.255272Z","shell.execute_reply.started":"2021-05-31T12:32:20.964925Z","shell.execute_reply":"2021-05-31T12:32:21.254282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking for outliers\nplt.figure(figsize=(40,20))\nsns.boxplot(data=df[['tax', 'mpg']])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:32:21.256705Z","iopub.execute_input":"2021-05-31T12:32:21.256986Z","iopub.status.idle":"2021-05-31T12:32:21.542838Z","shell.execute_reply.started":"2021-05-31T12:32:21.25696Z","shell.execute_reply":"2021-05-31T12:32:21.541739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding outliers by Tukeyâ€™s box plot method\nq1=df.quantile(0.25)\nq2=df.quantile(0.75)\nIQR=q2-q1\n\nprint(\"Number of outliers is:\")\nprint(df[((df<(q1-1.5*IQR))|(df>(q2+1.5*IQR))).any(axis=1)].shape[0])    \n\n# Tukey's method show that there are almost 50% of data is outliers, so let's try to remove outliers by our hands\ndf = df[(df['tax'] <= 400) & (df['mpg'] <= 300) & (df['price'] <= 100000) & (df['mileage'] <= 200000)]\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:32:21.544213Z","iopub.execute_input":"2021-05-31T12:32:21.544546Z","iopub.status.idle":"2021-05-31T12:32:21.803421Z","shell.execute_reply.started":"2021-05-31T12:32:21.544495Z","shell.execute_reply":"2021-05-31T12:32:21.802604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building a model REGRESSIONS**","metadata":{}},{"cell_type":"code","source":"# Creating Dummy variables (for more information use link below)\n# https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40\ndummies = pd.get_dummies(df[['transmission', 'model', 'fuelType']])\ndf = pd.concat([df, dummies], axis = 1)\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:32:21.805028Z","iopub.execute_input":"2021-05-31T12:32:21.805309Z","iopub.status.idle":"2021-05-31T12:32:21.835397Z","shell.execute_reply.started":"2021-05-31T12:32:21.805281Z","shell.execute_reply":"2021-05-31T12:32:21.834164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting data into X, y - features and predictible variable\n# X = df.loc[:,['volatile acidity','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'pH', 'sulphates', 'alcohol']]\nX = df.drop(['transmission', 'model', 'fuelType', 'price'], axis = 1)\nscaler = preprocessing.StandardScaler().fit(X)\nX = scaler.transform(X)\ny = df['price']\n\n\n# Splitting X, y into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=12345)\n\n\n# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nimport sklearn.metrics as sm\nreg = LinearRegression().fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n\n# Evaluating model\nprint(\"Linear Regression:\")\nprint('Train success rate : %',reg.score(X_train, y_train)*100)\nprint('Validation success rate : %',r2_score(y_test, y_pred))\nprint(\"\\n\\n\")\n### !!!!! R2 score is negative. It is caused by some strange values in test set and means that my model is bad, can't predict value for some test rows. \n### There are two ways:\n### 1) try not to remove outliers\n### 2) don't use this model at all\n\n\n\n# Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_jobs = -1, random_state = 42).fit(X_train, y_train)\ny_pred = rf_reg.predict(X_test)\n# Evaluating model\nprint(\"Random Forest Regression:\")\nprint('Train success rate : %',rf_reg.score(X_train, y_train)*100)\nprint('Validation success rate : %',r2_score(y_test, y_pred))\nprint(\"\\n\\n\")\n# It looks like this model is an overfitted on train data\n# Let's try to use Grid Search to improve model\n# Let's look at deths of trees in random forest\nprint(\"Tree's depths:\")\nprint(plt.hist([est.get_depth() for est in rf_reg.estimators_]))\n\n\n# Grid Search for Random Forest Regression. Let's try to use Grid Search\nfrom sklearn.model_selection import GridSearchCV\nreg = RandomForestRegressor()\ngrid_values = {'n_estimators': [50, 100, 150],'max_depth': [25, 29, 30, 31], 'bootstrap': [True, False], 'n_jobs': [-1], 'random_state': [4]}\ngrid_reg_acc = GridSearchCV(reg, param_grid = grid_values, scoring = 'r2')\ngrid_reg_acc.fit(X_train, y_train)\ny_pred_acc = grid_reg_acc.predict(X_test)\n# Evaluating model\nprint(\"Random Forest Regression with Grid Search:\")\nprint('Train success rate : %',grid_reg_acc.score(X_train, y_train)*100)\nprint('Validation success rate : %',r2_score(y_test, y_pred_acc))\nprint(\"\\n\\n\")\nprint(\"Best parameters wich was choosen:\")\nprint(grid_reg_acc.best_params_)\nprint(\"\\n\\n\")\n\n\n\n# Also let's look how R2 score depends on max_depth of trees\nrf_scores = []\nfor md in range(1,35):\n    rf_reg = RandomForestRegressor(n_jobs = -1, random_state = 42, max_depth = md).fit(X_train, y_train)\n    y_pred = rf_reg.predict(X_test)\n    rf_scores = rf_scores + [rf_reg.score(X_train, y_train)*100]\nplt.figure(figsize=(40,20))\nplt.plot(list(range(1,35)), rf_scores)\n# Let's choose max_depth = 12 to optimize calculation time and save good R2 score.\nrf_reg = RandomForestRegressor(n_jobs = -1, random_state = 42, max_depth = 12).fit(X_train, y_train)\ny_pred = rf_reg.predict(X_test)\nprint(\"Random Forest Regression with max_depth = 12:\")\nprint('Train success rate : %',rf_reg.score(X_train, y_train)*100)\nprint('Validation success rate : %',r2_score(y_test, y_pred))\nprint(\"\\n\\n\")\n\n\n\n\n\n# XGBoost Regression\nfrom xgboost import XGBRegressor\nxgb_reg = XGBRegressor(max_depth = 3, learning_rate = 0.1, n_estimators = 100, verbosity = 0, random_state = 42).fit(X_train, y_train)\ny_pred = xgb_reg.predict(X_test)\n# Evaluating model\nprint(\"XGBoost Regression:\")\nprint('Train success rate : %',rf_reg.score(X_train, y_train)*100)\nprint('Validation success rate : %',r2_score(y_test, y_pred))\nprint(\"\\n\\n\")\n# It looks like this model is an overfitted on train data\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:32:21.836814Z","iopub.execute_input":"2021-05-31T12:32:21.83709Z","iopub.status.idle":"2021-05-31T12:35:30.654192Z","shell.execute_reply.started":"2021-05-31T12:32:21.837061Z","shell.execute_reply":"2021-05-31T12:35:30.652848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\nThe best model is Random Forest Regression.\nBy the Grid Search the best set of parameters is {'bootstrap': True, 'max_depth': 25, 'n_estimators': 150, 'n_jobs': -1, 'random_state': 4}, but even at {'bootstrap': True, 'max_depth': 12, 'n_estimators': 100, 'n_jobs': -1, 'random_state': 4} R2 score is almost the same. So to save calculation time I think should be better to use second parameters set.","metadata":{}}]}