{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Importing libraries and data**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, plot_confusion_matrix, f1_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T12:50:16.61007Z","iopub.execute_input":"2021-05-31T12:50:16.610466Z","iopub.status.idle":"2021-05-31T12:50:16.66048Z","shell.execute_reply.started":"2021-05-31T12:50:16.61043Z","shell.execute_reply":"2021-05-31T12:50:16.659577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Exploration**","metadata":{}},{"cell_type":"code","source":"print(\"First 5 rows in df:\")\nprint(df.head())\nprint(\"\\n\\n\")\n\nprint(\"Checking if there is Null:\")\nprint(df.isnull().sum())\nprint(\"\\n\\n\")\n\nprint(\"df describing:\")\nprint(df.describe())\nprint(\"\\n\\n\")\n\nprint(\"Checking duplicates:\")\nprint(df.duplicated().sum())\n\n# Removing duplicates\ndf=df.drop_duplicates(keep='first')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:16.664914Z","iopub.execute_input":"2021-05-31T12:50:16.665197Z","iopub.status.idle":"2021-05-31T12:50:16.722911Z","shell.execute_reply.started":"2021-05-31T12:50:16.665169Z","shell.execute_reply":"2021-05-31T12:50:16.721974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA**","metadata":{}},{"cell_type":"code","source":"# Scatter plots of each pair of features\nsns.pairplot(data = df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:16.72435Z","iopub.execute_input":"2021-05-31T12:50:16.724618Z","iopub.status.idle":"2021-05-31T12:50:42.401143Z","shell.execute_reply.started":"2021-05-31T12:50:16.724583Z","shell.execute_reply":"2021-05-31T12:50:42.400198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking for outliers\nplt.figure(figsize=(40,20))\nsns.boxplot(data=df)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:42.403787Z","iopub.execute_input":"2021-05-31T12:50:42.404072Z","iopub.status.idle":"2021-05-31T12:50:42.822479Z","shell.execute_reply.started":"2021-05-31T12:50:42.404043Z","shell.execute_reply":"2021-05-31T12:50:42.821413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing outliers by Tukeyâ€™s box plot method\n# For more information go to https://towardsdatascience.com/detecting-and-treating-outliers-in-python-part-1-4ece5098b755\nq1=df.quantile(0.25)\nq2=df.quantile(0.75)\nIQR=q2-q1\n\nprint(\"Number of outliers is:\")\nprint(df[((df<(q1-1.5*IQR))|(df>(q2+1.5*IQR))).any(axis=1)].shape[0])      \n\n\n\ndf=df[~((df<(q1-1.5*IQR))|(df>(q2+1.5*IQR))).any(axis=1)]\n\n# Visualizing data without outliers\nplt.figure(figsize=(40,20))\nsns.boxplot(data=df)\n\n\nprint(\"New df shape:\")\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:42.824043Z","iopub.execute_input":"2021-05-31T12:50:42.824342Z","iopub.status.idle":"2021-05-31T12:50:43.255606Z","shell.execute_reply.started":"2021-05-31T12:50:42.824313Z","shell.execute_reply":"2021-05-31T12:50:43.254518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let`s see the correlation between all variables\nplt.figure(figsize=(12,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:43.257052Z","iopub.execute_input":"2021-05-31T12:50:43.257451Z","iopub.status.idle":"2021-05-31T12:50:44.168817Z","shell.execute_reply.started":"2021-05-31T12:50:43.257409Z","shell.execute_reply":"2021-05-31T12:50:44.167587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building a model REGRESSIONS**","metadata":{}},{"cell_type":"code","source":"# Splitting data into X, y - features and predictible variable\nX = df.loc[:,df.columns != 'quality']\ny = df['quality']\n\n\n# Replacing quality column by new column with only two classes: 1 - good wine, 0 - bad wine\ndf['easy_quality'] = 0\ndf.loc[df['quality']>=7,'easy_quality'] = 1\ny = df['easy_quality']\n\n\n\n# Splitting X, y into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:44.169935Z","iopub.execute_input":"2021-05-31T12:50:44.170199Z","iopub.status.idle":"2021-05-31T12:50:44.179162Z","shell.execute_reply.started":"2021-05-31T12:50:44.170172Z","shell.execute_reply":"2021-05-31T12:50:44.178168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*In this competition classes are imbalanced, so we should look at F1 score (for class 1 in classification report) - not at accuracy*\nMore about scores:\nhttps://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c","metadata":{}},{"cell_type":"code","source":"# Naive Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Classification report for Naive Bayes Classifier:\")\nprint(classification_report(y_test,y_pred))\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(clf, X_test,y_test)\nprint(\"F1 Score is: \" + str(f1_score(y_pred, y_test)))\nprint(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:44.180307Z","iopub.execute_input":"2021-05-31T12:50:44.18074Z","iopub.status.idle":"2021-05-31T12:50:44.366607Z","shell.execute_reply.started":"2021-05-31T12:50:44.18071Z","shell.execute_reply":"2021-05-31T12:50:44.365592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choosing independent features by wrapper method (about it you can read by url down)\n# https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n\nimport statsmodels.api as sm\n\n# First iteration, choose where p_value is less 0.05\nX_1 = sm.add_constant(X)\nmodel = sm.OLS(y,X_1).fit()\nprint(model.pvalues)\n\n\n# Second iteration, choose where p_value is less 0.05\nX_1 = sm.add_constant(df.loc[:,['fixed acidity', 'volatile acidity','free sulfur dioxide', 'density', 'sulphates', 'alcohol']])\nmodel = sm.OLS(y,X_1).fit()\nprint(model.pvalues)\n# X = df.loc[:,['fixed acidity', 'volatile acidity','free sulfur dioxide', 'density', 'sulphates', 'alcohol']]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.25, random_state=12345)\n\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Classification report for Naive Bayes Classifier:\")\nprint(classification_report(y_test,y_pred))\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(clf, X_test,y_test)\nprint(\"F1 Score is: \" + str(f1_score(y_pred, y_test)))\nprint(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:44.369609Z","iopub.execute_input":"2021-05-31T12:50:44.370025Z","iopub.status.idle":"2021-05-31T12:50:44.615239Z","shell.execute_reply.started":"2021-05-31T12:50:44.369978Z","shell.execute_reply":"2021-05-31T12:50:44.614527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's try Logistic regression with weighted classes. More information by the link below:\n# https://towardsdatascience.com/weighted-logistic-regression-for-imbalanced-dataset-9a5cd88e68b\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)\n\nX_1 = sm.add_constant(df.loc[:,['fixed acidity', 'volatile acidity','free sulfur dioxide', 'density', 'sulphates', 'alcohol']])\nX_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.25, random_state=12345)\n\nw = [{0:1000,1:100},{0:1000,1:10}, {0:1000,1:1.0}, \n     {0:500,1:1.0}, {0:400,1:1.0}, {0:300,1:1.0}, {0:200,1:1.0}, \n     {0:150,1:1.0}, {0:100,1:1.0}, {0:99,1:1.0}, {0:10,1:1.0}, \n     {0:0.01,1:1.0}, {0:0.01,1:10}, {0:0.01,1:100}, \n     {0:0.001,1:1.0}, {0:0.005,1:1.0}, {0:1.0,1:1.0}, \n     {0:1.0,1:0.1}, {0:10,1:0.1}, {0:100,1:0.1}, \n     {0:10,1:0.01}, {0:1.0,1:0.01}, {0:1.0,1:0.001}, {0:1.0,1:0.005}, \n     {0:1.0,1:10}, {0:1.0,1:99}, {0:1.0,1:100}, {0:1.0,1:150}, \n     {0:1.0,1:200}, {0:1.0,1:300},{0:1.0,1:400},{0:1.0,1:500}, \n     {0:1.0,1:1000}, {0:10,1:1000},{0:100,1:1000} ]\nhyperparam_grid = {\"class_weight\": w }\n                         \n                    \nlg3 = LogisticRegression(random_state = 13)\ngrid = GridSearchCV(lg3, hyperparam_grid, scoring = \"f1\", cv = 50, n_jobs = -1, refit = True)\ngrid.fit(X_train,y_train)\nprint(f'Best score: {grid.best_score_} with param: {grid.best_params_}')\nprint(\"F1 Score is: \" + str(f1_score(grid.predict(X_test), y_test)))\nprint(\"\\n\\n\")\n\nlg4 = LogisticRegression(random_state = 13, class_weight = {0: 100, 1: 1000}, n_jobs = -1)\nlg4.fit(X_train,y_train)\nprint(\"F1 Score is: \" + str(f1_score(lg4.predict(X_test), y_test)))\nprint(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:50:44.616432Z","iopub.execute_input":"2021-05-31T12:50:44.616768Z","iopub.status.idle":"2021-05-31T12:51:08.253546Z","shell.execute_reply.started":"2021-05-31T12:50:44.616741Z","shell.execute_reply":"2021-05-31T12:51:08.252309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OverSampling**","metadata":{}},{"cell_type":"code","source":"# As classification without sampling data works bad, let's try to make an RandomOverSampler or RandomUnderSampler. More information by the link below:\n# https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nprint(\"Number of elements in classes before OverSampling:\")\nprint(Counter(y))\nprint(\"\\n\\n\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)\nros = RandomOverSampler(random_state=0)\nros.fit(X, y)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\nX_train = X_resampled\ny_train = y_resampled\n\nprint(\"Number of elements in classes after OverSampling:\")\nprint(Counter(y_resampled))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:51:08.255112Z","iopub.execute_input":"2021-05-31T12:51:08.255422Z","iopub.status.idle":"2021-05-31T12:51:08.274639Z","shell.execute_reply.started":"2021-05-31T12:51:08.255391Z","shell.execute_reply":"2021-05-31T12:51:08.273615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Classification report for Naive Bayes Classifier:\")\nprint(classification_report(y_test,y_pred))\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(clf, X_test,y_test)\nprint(\"F1 Score is: \" + str(f1_score(y_pred, y_test)))\nprint(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:51:08.2758Z","iopub.execute_input":"2021-05-31T12:51:08.276109Z","iopub.status.idle":"2021-05-31T12:51:08.45867Z","shell.execute_reply.started":"2021-05-31T12:51:08.276051Z","shell.execute_reply":"2021-05-31T12:51:08.457421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nimport sklearn.metrics as sm\nreg = LinearRegression().fit(X_train, y_train)\ny_pred = np.round(reg.predict(X_test))\nprint(\"Classification report for Linear Regression:\")\nprint(classification_report(y_test,y_pred))\nprint(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:51:08.460262Z","iopub.execute_input":"2021-05-31T12:51:08.460657Z","iopub.status.idle":"2021-05-31T12:51:08.477784Z","shell.execute_reply.started":"2021-05-31T12:51:08.460615Z","shell.execute_reply":"2021-05-31T12:51:08.476574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\nrf.fit(X_train, y_train)\ny_pred = np.round(rf.predict(X_test))\nprint(\"Classification report for Random Forest Classifier:\")\nprint(classification_report(y_test,y_pred))\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(rf, X_test,y_test)\nprint(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:56:57.791611Z","iopub.execute_input":"2021-05-31T12:56:57.791925Z","iopub.status.idle":"2021-05-31T12:57:00.422177Z","shell.execute_reply.started":"2021-05-31T12:56:57.791897Z","shell.execute_reply":"2021-05-31T12:57:00.421065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Classification report for Naive Bayes Classifier:\")\nprint(classification_report(y_test,y_pred))\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(clf, X_test,y_test)\nprint(\"\\n\\n\")\n\n\nfrom sklearn.metrics import f1_score\nprint(f1_score(y_pred, y_test))\n# Let`s look at depending F1 score on number of neighbours\nf1_scores = []\nfor numb in range(1,30):\n    clf = KNeighborsClassifier(n_jobs = -1, n_neighbors = numb)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    f1_scores = f1_scores + [f1_score(y_pred, y_test)]\nplt.figure(figsize=(12,10))\nplt.plot(list(range(1,30)), f1_scores)\n\n\n# It looks like this classifier is bad for this task","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:51:11.178402Z","iopub.execute_input":"2021-05-31T12:51:11.178775Z","iopub.status.idle":"2021-05-31T12:51:15.423372Z","shell.execute_reply.started":"2021-05-31T12:51:11.178731Z","shell.execute_reply":"2021-05-31T12:51:15.422424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVC \nfrom sklearn.svm import SVC\nclf = SVC()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Classification report for SV Classifier:\")\nprint(classification_report(y_test,y_pred))\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(clf, X_test,y_test)\nprint(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:51:15.424506Z","iopub.execute_input":"2021-05-31T12:51:15.424785Z","iopub.status.idle":"2021-05-31T12:51:15.696822Z","shell.execute_reply.started":"2021-05-31T12:51:15.424755Z","shell.execute_reply":"2021-05-31T12:51:15.69578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**","metadata":{}},{"cell_type":"markdown","source":"*In that case, the best model is NaiveBayes before oversampling with F1 score 0.56.*","metadata":{}}]}