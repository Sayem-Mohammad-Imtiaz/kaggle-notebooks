{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello Friends !\n\nThis kernel is an introduction to an end to end Machine learning Process for beginners. Here are the main steps you will\ngo through:\n\n1. Data ecxploration and visualization with seaborn and [plotly](https://plotly.com/)\n1. Data Processing for Machine Learning algorithms with customized scikit-learn tensformers\n1. Model Selection \n1. Model training\n1. Model fine tuning with GridSearch technique\n\nI referred to \n* The chapter 2 [git](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb) of the great best seller [book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) of Aurelien Géron\n* This excellent [post](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65) for how to construct Custom Transformers with Scikit-learn\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are aavailable in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport requests\nimport os\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nRANDOM_STATE = 75","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# get Data\ndf = pd.read_csv('../input/california-housing-prices/housing.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\n### 1. Quick look"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get statistical description of numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# \ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are obvious differences between numerical features scales that we have to re-adjust afterwards"},{"metadata":{},"cell_type":"markdown","source":"Missing values:\nwe will use [pandas.DataFrame.count](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.count.html) that Counts non-NA cells for each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df) - df.count()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"**total_bedrooms**\" contains missing values, we ll see how to handle it"},{"metadata":{},"cell_type":"markdown","source":"## 2. Split train test:\n\nWe have to put aside a test subset and never look at it cause otherwise the estimate would be too optimistic and once launched on a prod environment it would not perform as well as expected. This is called **data snooping bias**.\n\n### Stratified sampling: \nIf we want to devide train-test subset in such a way that the the test subset is representative of the overall population, we proceed by stratified sampling. if a feature is important to predict our target variable, we can stratify train, test subsets on that feature. a good way to find a suitable feature to strat on it, is to calculate **Pearson correlation** between each of the varaibles and taking the one having the highest absolute value of the correlation coefficient \nCheck out this excellent. [post](http://spss.espaceweb.usherbrooke.ca/pages/stat-inferentielles/correlation.php) for deep understanding of person's correlation "},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n\ncorr_matrix = df.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable <span style=\"color: #D35400\">median_income</span> seems to be very correlated to our target. the stratification can be applied to this feature so that the test set would be representative of the various categories of incomes in the whole dataset.\n\nA scatter plot can show better the linear correlation between the two variables as shown below"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.relplot(x=\"median_income\", y=\"median_house_value\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that it is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. to find how we ll cut \"median_income\" on significant intervals, let s take a look at our feature : whether a **histogram** or a **box plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\n\nsns.distplot(df[\"median_income\"],label=\"median income\")\n \nplt.title(\"Histogram of Median Income\") # for histogram title\nplt.legend() # for label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.box(df, y=\"median_income\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous charts show that most of median income values are clustered around **1.5** to **6**, so we'll split \"median_income\" values into 5 stratas:\n* values between 0 and 1.5 => strata 1\n* values between 1.5 and 3 => strata 2\n* values between 3 and 4.5 => strata 3\n* values between 4.5 and 6 => strata 4\n* values higher than 6 => strata 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb\ndf[\"income_cat\"] = pd.cut(df[\"median_income\"],\n                          bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                          labels=[1, 2, 3, 4, 5])\n# bar plot i=of income cat\nplt.figure(figsize=(12,7))\nsns.countplot(x='income_cat', data=df)\n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's apply stratified sampling based on the income categories (\"**income_cat**\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n\nfor train_index, test_index in split.split(df, df[\"income_cat\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. EDA : Exploratory Data Analysis:\nExploratory data analysis (EDA) is the task of analyzing data sets to summarize their main characteristics, often with **DataViz** methods, before applying any data transformation pipeline or launching a statistical/machine learning models. it aims basically to discover what can the disposed data tell us and possibly formulate hypotheses that could lead to new data collection and experiments. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = strat_train_set.copy().drop('income_cat', axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**float64** is quite more heavy in terms of memory, so if it is possible to change float variables to **int**, it is recommended to do it (even though in our case it does not really matter as the set is quite small)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['housing_median_age'] = df['housing_median_age'].astype('uint8')\ng = sns.catplot(x=\"housing_median_age\",\n                y=\"median_house_value\", \n                data=df, \n                kind=\"box\")\ng.fig.set_figwidth(16)\ng.fig.set_figheight(9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==> 'housing median age ' does not seem an interesting predictive feature\n\nHowever it seems like the the most old houses can be quite expensive comparing to less old other houses"},{"metadata":{},"cell_type":"markdown","source":"#### longitude X latitude "},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\nfig = px.scatter(df, x=\"longitude\", y=\"latitude\",color=\"median_house_value\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the housing prices are very much related to the location : regions close to the ocean have the highest house prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['total_rooms'] = df['total_rooms'].astype('uint16')\nax = sns.relplot(x='total_rooms', y=\"median_house_value\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"ocean_proximity\",\n                y=\"median_house_value\", \n                data=df, \n                kind=\"box\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\n#corr_matrix[\"median_house_value\"].sort_values(ascending=False)\nsns.heatmap(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's display scatter matrix of the top correalated vars to the \"median_house_value\""},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfig = px.scatter_matrix(df,\n    dimensions=[\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"])#,\n#    color=\"ocean_proximity\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like the most promising attribute to predict the median house value is the **median_income**, so let’s **zoom** in on their correlation scatterplot "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df, x=\"median_income\",\n                 y=\"median_house_value\", \n                 size='total_rooms')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation is actually very strong; the upward trend is quite clear"},{"metadata":{},"cell_type":"markdown","source":"## 4. Feature Engineering\n\n### Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a) Missing values \nAs we saw \"**total_bedrooms**\" has null values "},{"metadata":{},"cell_type":"markdown","source":"Most of Machine Learning algorithms does not support missing values, to address this issue we have three options:\n1. Drop entries with null values\n2. Drop feature with many missing values\n3. Impute nan values with a significant value (zero, the mean, the median, etc.).\n\n(1&2) it is always better to keep data than to delete them. \n(2) The only case that it may worth deleting a variable is when its missing values are**more than 60%** of the observations but only if that variable is insignificant. Taking this into consideration, _imputation is always a preferred choice over deleting variables_.\n\n(3) Mean/Median imputation decreases any correlations affecting the imputed the variables. This is because we assume that there is no relationship between the imputed variable and the other features. An attractive way to reduce that is to use k-nearest neighbors algorithm to impute missing values. The assumption behind this methode is that a data point can be approximated by the values of the its closest points, based on other variables.\n\nIf you choose the imputation option, please keep in mind that the imputaion value is  **<span style=\"color: #FF5733\">training</span>** set, and save it to use it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data."},{"metadata":{},"cell_type":"markdown","source":"We'll use [sklearn knnImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) to overcome the missing values issue\n\n#### how does it work?\nReferring to [sklearn doc](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) \n> Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer\n\n\nimputer = KNNImputer(n_neighbors=5)\ndf_filled = imputer.fit_transform(df.drop('ocean_proximity', axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check the result the 'total_bedrooms' values\nnp.isnan(df_filled[:,4]).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['total_bedrooms'] = df_filled[:,4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) Categorical variables:\n\n\nMost of Machine Learning algorithms deal with scalars rather than textual or categorical variables, so it is recommended to convert categorical varibles to scalars. One of the most common solution is the **one hot encoding** consisting of creating a new binary feature for each modality of the categorical variable: takes 1 when the category is the corresponding modality 0 otherwise The new_attributes are also called dummy attributes.\n\nThe [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) method is quite straightforward,  it converts categorical variable into dummy variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom transformer:\nto construct a transfromer customized to our dataset. All we have to do is to create a class inherited from `BaseEstimator` and override `fit` and `transform` methods (add the `TransformerMixin` class inheritance to have the `fit_transform` as a bonus method)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass AttributeAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, num_vars, add_bedrooms_per_room = True): \n        self.add_bedrooms_per_room = add_bedrooms_per_room\n        #self._num_vars = num_vars\n        self._cols = num_vars\n        \n    def fit(self, X, y=None):\n        return self # nothing else to do\n    \n    def transform(self, X, y=None):     \n        X = pd.DataFrame(X, columns=self._cols)\n        X['rooms_per_household'] = X['total_rooms']/X['households']\n        X['population_per_household'] = X['population']/X['households']\n        #self.new_cols_ = ['rooms_per_household', 'population_per_household']\n        if self.add_bedrooms_per_room:\n            X['bedrooms_per_room'] = X['total_bedrooms']/X['total_rooms']\n            #self.new_cols_.append('bedrooms_per_room')\n            \n        self._cols = X.columns.tolist()\n        return X.values\n    \n    def get_feature_names(self):\n        return self._cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Feature Scaling\nWith few exceptions, Machine Learning algorithms don’t appreciate diffrences between numerical features scales.\nSome examples of algorithms where feature scaling matters:\n* **k-NN** : based on $| |_2$ (Euclidean distance) to measure distances between data points, is sensitive to magnitudes and hence should be scaled for all features to **weigh in equally**.\n* **PCA** : Scaling is critical, because PCA algo looks for the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n* **Gradient Descent** We can speed up gradient descent by scaling. This is because $θ_{hat}$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n\nAlogrithms tolerant to different scales \n* **Tree based models** are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees.\n* **LDA**  and **Naive Bayes** are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect.\n\nThere are two common ways for feature scaling: min-max scaling and standardization.\n1. MinMax Scaling: (also called normalization) consists of shifting and rescaling values so that they end up ranging from 0 to 1 as follows:\n\n$$ \\frac{x-min}{max-min} $$ \n\n2. Standardization: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. \n$$\\frac{x-\\mu}{\\sigma}$$\nwhere μ is the mean value of the feature and σ is the standard deviation of the feature\n\nUnlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers.\n\n\n**PS IMPORTANT** : Scalers must be fitted only on the training data, (not to the whole dataset with the test ssubset). Then thes fitted trasformers are used to transform the training set, the test set and new dat"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().iloc[:, 2:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**total_rooms** variable takes values between 6 and 39320 whereas **housing_median_age** varies between 1 and 52 which reflect very different scales"},{"metadata":{},"cell_type":"markdown","source":"\n\n### Custom Pipeline:\nWith many data transformation steps it is recommanded to use Pipeline class provided by Scikit-learn that helps to make sequenced transformations in the right order.\nWe can do that using the [FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) estimator offered by scikit-learn. This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = strat_train_set.copy().drop('income_cat', axis=1)\nlabels = strat_train_set[\"median_house_value\"].copy()\ncat_vars, num_vars = [], []\nfor var, var_type in df.dtypes.items():\n    if var_type =='object':\n        cat_vars.append(var)\n    else:\n        num_vars.append(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import KNNImputer\n#from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import FeatureUnion, Pipeline \nfrom sklearn.compose import ColumnTransformer\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass FeatureSelector(BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, feature_names):\n        self._feature_names = feature_names \n        \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n        return X[self._feature_names].values \n    \n#Defining the steps in the categorical pipeline \ncat_pipeline = Pipeline( [ ( 'cat_selector', FeatureSelector(cat_vars) ),\n                          ( 'one_hot_encoder', OneHotEncoder( sparse = False ) ) ] )\n    \n#Defining the steps in the numerical pipeline     \nnum_pipeline = Pipeline([\n        ( 'num_selector', FeatureSelector(num_vars) ),\n        ('imputer', KNNImputer(n_neighbors=5)),\n        ('attribs_adder', AttributeAdder(num_vars=num_vars, add_bedrooms_per_room = True)),\n        ('std_scaler', StandardScaler()),\n    ])\n\n\n#housing_num_tr = num_pipeline.fit_transform(housing_num)\n\n#Combining numerical and categorical piepline into one full big pipeline horizontally \n#using FeatureUnion\nfull_pipeline = FeatureUnion( transformer_list = [ ( 'num_pipeline', num_pipeline ),\n                                                  ( 'cat_pipeline', cat_pipeline )] \n                            )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#num_pipeline.\n#num_pipeline.named_steps['attribs_adder'].get_feature_names()\n#cat_pipeline.named_steps['one_hot_encoder'].get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prepared = full_pipeline.fit_transform(df)\ndf_prepared","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_vars = (list(full_pipeline.transformer_list[0][1].named_steps['attribs_adder'].get_feature_names()) + \n            list(full_pipeline.transformer_list[1][1].named_steps['one_hot_encoder'].get_feature_names()))\nall_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Select Model\nWe ll train severel models and evaluate them using **CrossValidation** technique: we'll randomly split the training set into k=7 folds, then then we ll be training and evaluate our model 7 times picking a different evaluation fold at each iteration training on the remaining 6 folds. \n\nwe ll use `cross_val_score` of scikit learn setting `scoring` parameter to **neg_mean_squared_error** because it expects a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value)\nThe evaluation scores will be saved in an array.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nreg_tree = DecisionTreeRegressor()\n\n\nscores = cross_val_score(reg_tree, df_prepared, labels,\n                         scoring=\"neg_mean_squared_error\",\n                         cv=7)\n\ntree_scores = np.sqrt(-scores)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('CV scores', tree_scores)\nprint('CV best score', tree_scores.min())\nprint('CV mean score', tree_scores.mean())\nprint('CV std score', tree_scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(df_prepared, labels)\n\npreds = lin_reg.predict(df_prepared)\nlr_rmse = np.sqrt(mean_squared_error(labels, preds))\nlr_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscores = cross_val_score(lin_reg, df_prepared, labels,\n                         scoring=\"neg_mean_squared_error\",\n                         cv=7)\n\nlr_scores = np.sqrt(-scores)\nprint('CV scores', lr_scores)\nprint('CV best score', lr_scores.min())\nprint('CV mean score', lr_scores.mean())\nprint('CV std score', lr_scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators=10, max_depth=2)\nxgb.fit(df_prepared, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Now that the model is trained, let’s evaluate it on the training set:\n\n\npreds = xgb.predict(df_prepared)\nxgb_rmse = np.sqrt(mean_squared_error(labels, preds))\nxgb_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(n_estimators=10, max_depth=2)\nscores = cross_val_score(xgb, df_prepared, labels,\n                         scoring=\"neg_mean_squared_error\",\n                         cv=7)\n\nxgb_scores = np.sqrt(-scores)\nprint('CV scores', xgb_scores)\nprint('CV best score', xgb_scores.min())\nprint('CV mean score', xgb_scores.mean())\nprint('CV std score', xgb_scores.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Fine-Tuning  \nonce some models are selected we need to fine tune them. the most commun way is GridSearchCV evaluate all the possible combinations of setteled hyperparameter values using cross-validation.\n\n\nFor example, the following code searches for the best combi‐\nnation of hyperparameter values for the RandomForestRegressor:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(df_prepared, labels)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All in all, the grid search will explore **3x4 + 1x2x3 = 18** combinations of RandomForestRegressor hyperparameter values, and it will train each model five times (since we are\nusing five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90 rounds of training"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(-grid_search.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nsorted(zip(feature_importances, all_vars), reverse=True)\n#feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}