{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textblob import TextBlob\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets=pd.read_csv(\"/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv\")\ntrain_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets = train_tweets[['label','tweet']]\ntrain_tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets['length'] = train_tweets['tweet'].apply(len)\ntrain_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets['length'] = train_tweets['tweet'].apply(len)\nfig1 = sns.barplot('label','length',data = train_tweets,palette='PRGn')\nplt.title('Average Word Length vs label')\nplot = fig1.get_figure()\nplot.savefig('Barplot.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = sns.countplot(x= 'label',data = train_tweets)\nplt.title('Label Counts')\nplot = fig2.get_figure()\nplot.savefig('Count Plot.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_processing(tweet):\n    \n    #Generating the list of words in the tweet (hastags and other punctuations removed)\n#     def form_sentence(tweet):\n#         tweet_blob = TextBlob(tweet)\n#         return ' '.join(tweet_blob.words)\n#     new_tweet = form_sentence(tweet)\n    \n    #Removing stopwords and words with unusual symbols\n    def no_user_alpha(tweet):\n        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n        clean_s = ' '.join(clean_tokens)\n        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n        return clean_mess\n    no_punc_tweet = no_user_alpha(tweet)\n    \n    #Normalizing the words in tweets \n    def normalization(tweet_list):\n        lem = WordNetLemmatizer()\n        normalized_tweet = []\n        for word in tweet_list:\n            normalized_text = lem.lemmatize(word,'v')\n            normalized_tweet.append(normalized_text)\n        return normalized_tweet\n    \n    \n    return normalization(no_punc_tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets[train_tweets['label']==1].drop('tweet',axis=1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwordsX = train_tweets['tweet']\ny = train_tweets['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nmsg_train, msg_test, label_train, label_test = train_test_split(train_tweets['tweet'], train_tweets['label'], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('bow',CountVectorizer(analyzer=text_processing)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n])\npipeline.fit(msg_train,label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pipeline.predict(msg_test)\n\nprint(classification_report(predictions,label_test))\nprint ('\\n')\nprint(confusion_matrix(predictions,label_test))\nprint(accuracy_score(predictions,label_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}