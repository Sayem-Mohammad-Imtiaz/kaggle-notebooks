{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End ALS Challenge Solution for Task 1\n## by Randy Williams","metadata":{}},{"cell_type":"markdown","source":"![](https://alsnewstoday.com/wp-content/uploads/2017/04/shutterstock_576832348-1000x480@2x.jpeg)","metadata":{}},{"cell_type":"markdown","source":"# Task 1: One mechanism of action or multiple independent mechanisms of action?\n\n### Does ALS have one mechanism of action (one pathway) or is it caused by multiple independent or different mechanisms of action (multiple pathways)? For example, what is the genetic difference between people with ALS with Bulbar onset (they start the symptoms in bulbar functions) versus Limb (they start the symptoms in the limbs)?","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"For the project, I will be attempting to solve the first task for the End ALS challenge, where I will investigate whether there is one pathway or multiple pathways for Amyotrophic Lateral Sclerosis (ALS). In this notebook, I will look at various machine learning approaches to answer this question. I will create a model that accurately predicts the classifiers for each transcriptomics dataset in the DESeq folder. After creating and validating our predictive model, I will perform an over-representation analysis to investigate the ontologies of the most important features in the model.  ","metadata":{}},{"cell_type":"code","source":"# import the basics\n!pip install nb_black > /dev/null\n%load_ext lab_black\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import graphing packages\nimport matplotlib.pylab as plt\n\n%matplotlib inline\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports for run_bagged_validation\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.metrics import f1_score, log_loss, precision_score, recall_score\nimport lightgbm as lgb\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import for laplacian score combine with distance entropy for feature selection\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport scipy\nimport scipy.sparse\nfrom sklearn import preprocessing\nfrom scipy.sparse.linalg import expm\nfrom sklearn.metrics.pairwise import euclidean_distances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import for over-representation analysis\n!pip install gseapy\nimport gseapy as gp\nfrom gseapy.plot import barplot, dotplot","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading DESeq2 Datasets","metadata":{}},{"cell_type":"code","source":"example_clinical_data_path_1 = \"/kaggle/input/end-als/end-als/clinical-data/filtered-metadata/metadata/clinical/Demographics.csv\"\nexample_clinical_data_path_2 = \"/kaggle/input/end-als/end-als/clinical-data/filtered-metadata/metadata/clinical/ALSFRS_R.csv\"\nexample_transcriptomics_DESEQ2_data_path_1 = (\n    \"/kaggle/input/end-als/end-als/transcriptomics-data/DESeq2/bulbar_vs_limb.csv\"\n)\nexample_transcriptomics_DESEQ2_data_path_2 = (\n    \"/kaggle/input/end-als/end-als/transcriptomics-data/DESeq2/ctrl_vs_case.csv\"\n)\nexample_transcriptomics_DESEQ2_data_path_3 = (\n    \"/kaggle/input/end-als/end-als/transcriptomics-data/DESeq2/median_low_vs_high.csv\"\n)\nexample_transcriptomics_3counts_data_path = \"/kaggle/input/end-als/end-als/transcriptomics-data/L3_counts/CASE-NEUZX521TKK/CASE-NEUZX521TKK-5793-T/CASE-NEUZX521TKK-5793-T_P85.exon.txt\"\n\ndemographics = pd.read_csv(example_clinical_data_path_1)\ndemographics.to_csv(\"/kaggle/working/demographics.csv\")\nalsfrs_scores = pd.read_csv(example_clinical_data_path_2)\nalsfrs_scores.to_csv(\"/kaggle/working/alsfrs_scores.csv\")\nbulbar_vs_limb = pd.read_csv(example_transcriptomics_DESEQ2_data_path_1)\nbulbar_vs_limb.to_csv(\"/kaggle/working/bulbar_vs_limb.csv\")\nctrl_vs_case = pd.read_csv(example_transcriptomics_DESEQ2_data_path_2)\nctrl_vs_case.to_csv(\"/kaggle/working/ctrl_vs_case.csv\")\nmedian_low_vs_high = pd.read_csv(example_transcriptomics_DESEQ2_data_path_3)\nmedian_low_vs_high.to_csv(\"/kaggle/working/median_low_vs_high.csv\")\nexample_transcriptomics_3counts_data = pd.read_csv(\n    example_transcriptomics_3counts_data_path,\n    delim_whitespace=True,\n    skiprows=1,\n    low_memory=False,\n)\nexample_transcriptomics_3counts_data.to_csv(\"/kaggle/working/L3_counts.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Description","metadata":{}},{"cell_type":"markdown","source":"## Ctrl_vs_Cases","metadata":{}},{"cell_type":"markdown","source":"The traget variable is \"CtrlVsCase_Classifier\", which assesses whether a patient is a case with ALS (CtrlVsCase_Classifier=1) or a control (CtrlVsCase_Classifier=0).\n\n* Data Dimensions: 169 Patients and approx. 53000 Genes\n* 32 are list as Controls\n* 137 are list as cases with ALS\n","metadata":{}},{"cell_type":"markdown","source":"## Bulbar_vs_Limb","metadata":{}},{"cell_type":"markdown","source":"The target variable explains if the onset of the disease started in the bulbar region \n(SiteOnset_Class=0) or in the patient's limbs (SiteOnset_Class=1).\n\n* Data Dimensions: 116 Patients and approx. 53000 Genes\n* 31 patients with onset on the bulbar region \n* 85 patients with onset on limb regions\n","metadata":{}},{"cell_type":"markdown","source":"## Median_Low_vs_High","metadata":{}},{"cell_type":"markdown","source":"The Median Low vs High dataset has the classfier \"ALSFRS_Class_Median\" , which measures whether the patients ALSFRS score was above or equal to the median ALSFRS score (ALSFRS_Class_Median=1) or below (ALSFRS_Class_Median=0) the median ALSFRS score. The Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS) is a metric for evaluatings ALS patients' function status and monitor how it changes over time. \n\n* Data Dimensions: 92 Patients and approx. 53000 Genes\n* 45 Patients have ALSFRRSS scores less than the median score\n* 46 Patients have ALSFRRSS scores greater than or to equal the median score ","metadata":{}},{"cell_type":"markdown","source":"# Methods","metadata":{}},{"cell_type":"markdown","source":"For my approach, I will design a cross validation pipeline that will first split the numbers patients into folds. In the next step, the pipeline trains a seperate gradient boosting model on each fold. Within the model training step, the pipeline will perform feature selection based on fisher's score or laplacian score. For the final step I will evaluate our predictions for each fold.    ","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning ","metadata":{}},{"cell_type":"markdown","source":"Each RNA-seq dataset in the DEseq2 folder contains over 53,000 genes. It is necessary to perform some sort of gene filtering and feature selection in our model in order to properly predict the disease status of a patient. Before applying any of these methods there are some genes that we might be able to rule out initially as uninformative. One example of these genes are \"pseudogenes\", which are nonfunctional segments of DNA that resemble functional genes. It may be the case that some pseudogenes in the datasets could be seem correlated to our target variable by random chance but have no true biological signficance in predicting ALS. Therefore, it might be in our interest remove these genes before applying any feature selection methods in our model. We can use gene annotation database systems like Ensemble BioMart to generate a list of gene ids that are identified as pseudogenes.","metadata":{}},{"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/7/7c/Pseudogene_defects.png)","metadata":{}},{"cell_type":"code","source":"biomart_path = \"/kaggle/input/biomart-annotation/mart_export.txt\"\nmart_export = pd.read_csv(biomart_path)\nmart_export.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I made an annotation dataset from the BioMart database publicly available on Kaggle that you can look at called \"BioMart_Annotation\" (https://www.kaggle.com/rwilliams7653/biomart-annotation). The columns on the dataframe are: \"Gene stable ID\", \"Gene stable ID version\", \"Transcript stable ID\", \"Transcript stable ID version\", \"Gene type\", \"Gene name\". The only columns we are interested in is \"Gene stable ID\",\"Gene type\", and \"Gene name\". \"Gene type\" denotes the type of the gene. \"Gene stable ID\" is a list of ensmbl gene ids which start with the prefix \"ENSG\" (for example ENSG00000281806 ), while \"Gene name\" are gene symbols determined by HUGO Gene Nomenclature Committee (HGNC). There are usually some overlapped between gene symbols and gene ids. For example, the ensembl gene id ENSG00000277620 and the gene symbol KIR3DL3 refer to the same gene. However, there are plenty of cases where there is a gene that is identified with an ensembl id but has no corresponding gene symbol label. This might explain why features in the trancriptomics datasets in the DESeq2 folder are denoted with both gene symbols and ensembl ids. ","metadata":{}},{"cell_type":"markdown","source":" In the next line of code, there is a list of the unique genes labels","metadata":{}},{"cell_type":"code","source":"mart_export[\"Gene type\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I created a list of all of the unique gene types that referred to pseudogenes so that I can filter the annotation data to only the pseudogenes later.","metadata":{}},{"cell_type":"code","source":"pseudo_genes = [\n    \"transcribed_processed_pseudogene\",\n    \"polymorphic_pseudogene\",\n    \"processed_pseudogene\",\n    \"unprocessed_pseudogene\",\n    \"pseudogene\",\n    \"rRNA_pseudogene\",\n    \"IG_V_pseudogene\",\n    \"TR_V_pseudogene\",\n    \"unitary_pseudogene\",\n    \"IG_C_pseudogene\",\n    \"IG_J_pseudogene\",\n    \"transcribed_unitary_pseudogene\",\n    \"TR_J_pseudogene\",\n    \"translated_processed_pseudogene\",\n    \"TR_J_pseudogene\",\n    \"translated_unprocessed_pseudogene\",\n    \"IG_pseudogene\",\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before subsetting the annotation to the pseudogenes, I created a variable called \"Gene_name2\" that mimics the labeling scheme of the transcriptomics datasets provided by the authors. If you look in the next line of code, I display the first 26 gene features of the ctrl_vs_case dataset. The features names are a mixture of HGNC symbols (gene labels with naming convention like \"WASH7P\") and Ensembl Ids (gene labels with the prefix \"ENSG\").   ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(ctrl_vs_case.columns[2:28], columns=[\"gene id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As I mention earlier, even though there is some overlap in the labeling with HGNC symbols and Ensembl ids, there are plenty of times where the genes are only identifable with Ensembl ids. It looks like the authors of the datasets chose the convention of labeling the genes with their HGNC symbols when provided and labeling the genes with Ensembl ids when the HGNC symbols are not provided. In next line of code, I make a variable in the annotation set called Gene_name2 that follows this convention so that it is easier to search through the column names of the DESeq2 datasets for pseudogenes.","metadata":{}},{"cell_type":"code","source":"s = mart_export[\"Transcript stable ID\"].map(\n    mart_export.set_index(\"Transcript stable ID\")[\"Gene stable ID\"]\n)\nmart_export[\"Gene_name2\"] = mart_export[\"Gene name\"].mask(\n    mart_export[\"Gene name\"].isnull(), s\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I subset the annotation data to the pseudogenes annotations and use the variable Gene_name2 to find the list of psuedogenes in the ctrl_vs_cases dataset. When I apply the CV pipeline to these datasets, I will drop these columns beforehand.","metadata":{}},{"cell_type":"code","source":"pseudogenes_annotate = mart_export[mart_export[\"Gene type\"].isin(pseudo_genes)]\npseudogenes = ctrl_vs_case.columns[\n    ctrl_vs_case.columns.isin(pseudogenes_annotate[\"Gene_name2\"])\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection Methods","metadata":{}},{"cell_type":"markdown","source":"In this section, I list the functions used for feature selection. Each function is applied only on the training split of data for each fold in the CV pipeline.","metadata":{}},{"cell_type":"markdown","source":"The follow code demonstrates my implementation of fisher's score. Fisher score for feature selection is a supervised method. The fisher function calculates the mean and the standard deviation of the case and control observations the training set seperately for a given gene and apply it to the formula of the score. The fishers score is calculated with the formula:\n$ \\frac{|m_1-m_2|}{{\\sigma}_1 +{\\sigma}_2} $\n\nwhere $ m_1 $ is the mean of the cases, $ m_2 $ is the mean of controls , ${\\sigma}_1$ is the standard deviation of cases, and ${\\sigma}_2$ is the standard deviation of control.  The function applies fisher score on all genes and returns the K best features. For fisher score, the higher the score the better the performance of the feature.","metadata":{}},{"cell_type":"code","source":"def fisher(data_df, target, num_features):\n    \"\"\"\n    Makes feature Fisher selection according to the following formula:\n    D(f) = m1(f) - m2(f) / sigma1(f) + sigma2(f)\n    :return: the list of most significant features.\n\n    Arguments:\n    1. data_df: the dataframe which is pandas object\n    2. target: the name the target variable denoted with a string\n    3. num_features: specifes K for finding the top K features\n    \"\"\"\n    fisher_df = pd.DataFrame()\n    fisher_df[\"mean_case\"] = data_df[data_df[target] == 1].mean()\n    fisher_df[\"mean_control\"] = data_df[data_df[target] == 0].mean()\n    fisher_df[\"std_case\"] = data_df[data_df[target] == 1].std()\n    fisher_df[\"std_control\"] = data_df[data_df[target] == 0].std()\n\n    fisher_df[\"diff_mean\"] = (fisher_df[\"mean_case\"] - fisher_df[\"mean_control\"]).abs()\n    fisher_df[\"sum_std\"] = (fisher_df[\"std_case\"] + fisher_df[\"std_control\"]).abs()\n    fisher_df[\"fisher_coeff\"] = fisher_df[\"diff_mean\"] / fisher_df[\"sum_std\"]\n    fisher_df = fisher_df.sort_values([\"fisher_coeff\"], ascending=False)\n\n    # data_df['fisher_coeff'] = fisher_df['fisher_coeff']\n    most_significant_features = fisher_df[\"fisher_coeff\"].index[:num_features]\n    return list(most_significant_features.drop(target))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next line of code shows how I implement Laplacian score for feature selection. Laplacian score is an unsupervised method. First I create a function for making the weights matrix symmetrix that is used in the method and then we make a function for implementing Laplacian score. The concrete steps for utilizing the method are:\n\n1.  Make a k-nearest neighbor graph. (A k-nearest neighbor defines an edge in the graph for each observation if another observation one of its k-nearest neighbor's)\n\n2. We define a matrix S measuring the similarity between to nodes (observations) that are connected using a pre-defined distance measure.\n\n3. Next define a Laplacian graph for each feature.\n\n4. Afterwards, we compute the Laplacian score.\n\n5. Finally, we select the n best (n lowest Laplacian scores), where n is the number features we want.\n\n\nBecause calculating Laplacian scores is highly computational, in the cross validation (CV) Pipeline, I will only apply Laplacian score to 10,000 highly variable features when the method is pre-specified.","metadata":{}},{"cell_type":"code","source":"def construct_W(X, neighbour_size=4, t=1):\n    n_samples, n_features = np.shape(X)\n    S = kneighbors_graph(\n        X, neighbour_size + 1, metric=\"euclidean\"\n    )  # sqecludian distance works only with mode=connectivity  results were absurd\n    S = (-1 * (S * S)) / (2 * t * t)\n    S = S.tocsc()\n    S = expm(S)  # exponential\n    S = S.tocsr()\n    # [1]  M. Belkin and P. Niyogi, “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering,” Advances in Neural Information Processing Systems,\n    # Vol. 14, 2001. Following the paper to make the weights matrix symmetrix we use this method\n    bigger = np.transpose(S) > S\n    S = S - S.multiply(bigger) + np.transpose(S).multiply(bigger)\n    return S\n\n\ndef LaplacianScoreTopK(X, topK, columnlist, neighbour_size=4, t=1):\n    \"\"\"\n    Arguments:\n    1. X: is the features matrix which has to be prefined as a numpy array\n    2. topK: specifes K for finding the top K features\n    3. columnlist: specifies the variable names of X\n    4. neighbor_size: specifies the neighbor size when making the k-nearest neighbor graph.\n    5. t: a parameter used for the calculating the weights matrix in constuct_W\n    \"\"\"\n\n    W = construct_W(X, t=t, neighbour_size=neighbour_size)\n    n_samples, n_features = np.shape(X)\n\n    # construct the diagonal matrix\n    D = np.array(W.sum(axis=1))\n    D = scipy.sparse.diags(np.transpose(D), [0])\n    # construct graph Laplacian L\n    L = D - W.toarray()\n\n    # construct 1= [1,···,1]'\n    I = np.ones((n_samples, n_features))\n\n    # construct fr' => fr= [fr1,...,frn]'\n    Xt = np.transpose(X)\n\n    # construct fr^=fr-(frt D I/It D I)I\n    t1 = np.matmul(np.matmul(Xt, D.toarray()), I) / np.matmul(\n        np.matmul(np.transpose(I), D.toarray()), I\n    )\n    t1 = t1[:, 0]\n    t1 = np.tile(t1, (n_samples, 1))\n    fr = X - t1\n\n    # Compute Laplacian Score\n    fr_t = np.transpose(fr)\n    Lr = np.matmul(np.matmul(fr_t, L), fr) / np.matmul(np.dot(fr_t, D.toarray()), fr)\n\n    Lap_vals = np.diag(Lr)\n    top_index = pd.Series(Lap_vals).sort_values().head(topK).index\n    return list(columnlist[top_index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Pipeline","metadata":{}},{"cell_type":"markdown","source":"In the next line of code, we define a function for making a dataframe of the feature importances for each split.","metadata":{}},{"cell_type":"code","source":"def get_split_feature_importance(clf, split_n):\n    fi_df = pd.DataFrame(\n        clf.feature_importances_,\n        index=clf.feature_name_,\n        columns=[f\"importance_{split_n}\"],\n    )\n    return fi_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also define a function for computing the validation scores using the metrics F1 score, Precision, Recall, LogLoss .","metadata":{}},{"cell_type":"code","source":"def compute_metrics(y_val, pred_val, pred_probs_val):\n    f1 = f1_score(y_val, pred_val)\n    prec = precision_score(y_val, pred_val)\n    rec = recall_score(y_val, pred_val)\n    loglosscore = log_loss(y_val, pred_probs_val)\n    print(\n        f\"F1 Score: {f1:0.4f} - Precision {prec:0.4f} - Recall {rec:0.4f} - LogLoss {loglosscore:0.4f}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is the code of function for our CV pipeline.","metadata":{}},{"cell_type":"code","source":"def run_bagged_validation_new(\n    data,\n    target_col,\n    nsplits,\n    lgb_params,\n    gene_subset,\n    test_size=0.1,\n    random_state=529,\n    method=None,\n    topK=None,\n    print_metrics=True,\n):\n    \"\"\"\n    cross-validation pipeline\n    Arguments:\n     1. data: the a whole dataset with the target and feature vaiables included\n     2. target_col: specify the name target variable in the dataset.\n                    The value is a string.\n     3. nsplits: specify the number of splits to make for cross validation.\n     4. lgb_param: specifies the parameters to be applied the gradient boosting model.\n     5. gene_subset: pre-specify a subset of genes to be included in the model before\n                     applying feature selection.\n     6. test_size: represents the proportion of the dataset to include in the test split.\n     7. random_state: setting the random seed of splitting\n     8. method: specifies the feature selection method to used.\n                The options are \"Fisher\" and \"Laplacian\".\n                The default is \"None\".\n     9. topK: specifes K for finding the top K features.\n              This parameter is ignored if the method=\"None\".\n              * Note: if the method=\"Laplacian\", then the K cannot be higher than 10,000.\n                      The score is calculated on the top 10,000 highly variable features.\n    10. print_metrics: specifies if the score report should be printed. The default is true.\n\n    \"\"\"\n    # Create X, y\n    X = data[gene_subset].copy()\n    y = data[target_col].copy()\n\n    # This will do a random stratified shuffle split 100x\n    sss = StratifiedShuffleSplit(\n        n_splits=nsplits, test_size=test_size, random_state=random_state\n    )\n\n    pred_val_probs_all = []\n    pred_val_all = []\n    y_val_all = []\n    fis = []  # To Store feature importances\n    split_n = 0\n    for tr_idx, val_idx in tqdm(sss.split(X, y), total=nsplits):\n        y_tr = y.iloc[tr_idx]\n        y_val = y.iloc[val_idx]\n        if method == \"Fisher\" and (topK is not None):\n            fisher_features = fisher(data.iloc[tr_idx], target_col, topK)\n            X_tr = data.iloc[tr_idx][gene_subset + fisher_features]\n            X_val = data.iloc[val_idx][gene_subset + fisher_features]\n        elif method == \"Laplacian\" and (topK is not None):\n            N = 10000\n            TOP_N_HIGH_VARIANCE = (\n                data.iloc[tr_idx]\n                .drop([\"Participant_ID\", target_col], axis=1)\n                .var()\n                .sort_values(ascending=False)\n                .head(N)\n                .index.tolist()\n            )\n            data2 = data[TOP_N_HIGH_VARIANCE]\n            laplacian_features = LaplacianScoreTopK(\n                np.array(data2.iloc[tr_idx]),\n                topK=topK,\n                columnlist=data2.columns,\n            )\n            X_tr = data.iloc[tr_idx][gene_subset + laplacian_features]\n            X_val = data.iloc[val_idx][gene_subset + laplacian_features]\n        else:\n            X_tr = X.iloc[tr_idx]\n            X_val = X.iloc[val_idx]\n        clf = lgb.LGBMClassifier(**lgb_params)\n        clf.fit(X_tr, y_tr)\n        pred_val_probs = clf.predict_proba(X_val)[:, 0]\n        pred_val = clf.predict(X_val)\n        # Store predictions for each split\n        pred_val_probs_all.append(pred_val_probs)\n        pred_val_all.append(pred_val)\n        y_val_all.append(y_val)\n\n        fis.append(get_split_feature_importance(clf, split_n))\n\n        split_n += 1\n\n    # Flatten Predictions for scoring\n    pred_val_all = np.concatenate(pred_val_all)\n    pred_val_probs_all = np.concatenate(pred_val_probs_all)\n    y_val_all = np.concatenate(y_val_all)\n\n    results = pd.DataFrame(\n        [y_val_all, pred_val_all, pred_val_probs_all],\n        index=[target_col, \"pred\", \"pred_probs\"],\n    ).T\n\n    fis_all = pd.concat(fis, axis=1)\n    if print_metrics:\n        compute_metrics(y_val_all, pred_val_all, pred_val_probs_all)\n    return results, fis_all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"markdown","source":"I applied the cross validation pipeline over 10 splits for a model with only the known genes ( \"C9orf72\", \"SOD1\", \"TARDBP\", \"FUS\"), a model where the feature selection method was the K best features with fisher score,and a model where the feature selection method was the K best features with Laplacian score. For validation, I used the F1 score for the criterion of accuracy.","metadata":{}},{"cell_type":"markdown","source":"# Model of Ctrl vs Case with the known genes ","metadata":{}},{"cell_type":"code","source":"KNOWN_GENES = [\"C9orf72\", \"SOD1\", \"TARDBP\", \"FUS\"]\nlgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults, fis_all = run_bagged_validation_new(\n    ctrl_vs_case,\n    gene_subset=KNOWN_GENES,\n    target_col=\"CtrlVsCase_Classifier\",\n    nsplits=10,\n    lgb_params=lgb_params,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot below shows the feature importance for the known genes.","metadata":{}},{"cell_type":"code","source":"feature_order = fis_all.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(data=fis_all.loc[feature_order].T, palette=\"Blues_d\", orient=\"h\", ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model of Ctrl vs Case with  the top K highest fisher scores added","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults_fisher, fis_all_fisher = run_bagged_validation_new(\n    ctrl_vs_case.drop(columns=pseudogenes),\n    gene_subset=[],\n    target_col=\"CtrlVsCase_Classifier\",\n    method=\"Fisher\",\n    nsplits=10,\n    lgb_params=lgb_params,\n    topK=10,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There was an improvement in the F1 score from using fisher score feature selection.","metadata":{}},{"cell_type":"markdown","source":"The plot below shows the feature importance for the fisher score model.","metadata":{}},{"cell_type":"code","source":"feature_order_fisher = fis_all_fisher.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(\n    data=fis_all_fisher.loc[feature_order_fisher].T,\n    palette=\"Blues_d\",\n    orient=\"h\",\n    ax=ax,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model of Ctrl vs Case with  the top K best Laplacian scores added","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults_lap, fis_all_lap = run_bagged_validation_new(\n    ctrl_vs_case.drop(columns=pseudogenes),\n    gene_subset=[],\n    method=\"Laplacian\",\n    target_col=\"CtrlVsCase_Classifier\",\n    nsplits=10,\n    lgb_params=lgb_params,\n    topK=10,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown with the F1 score, the Laplacian score selection method was more accurate than the known genes model. ","metadata":{}},{"cell_type":"markdown","source":"The follow graph is a bar plot shows feature importance of the variables selected using Laplacian score model.","metadata":{}},{"cell_type":"code","source":"feature_order_lap = fis_all_lap.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(\n    data=fis_all_lap.loc[feature_order_lap].T, palette=\"Blues_d\", orient=\"h\", ax=ax\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictably,the selection of genes from Laplacian score selection differ from Fisher score feature selection.","metadata":{}},{"cell_type":"markdown","source":"## Over-Representation Analysis for Fisher Score","metadata":{}},{"cell_type":"markdown","source":"Unfortunately, for the over-repesentation analysis, there weren't any known terms to ALS that were significant for fisher score selection.","metadata":{}},{"cell_type":"code","source":"fis_all_fisher_T = fis_all_fisher.loc[feature_order_fisher].T\nctrlvscase_genelist_fisher = list(\n    fis_all_fisher_T.loc[:, fis_all_fisher_T.sum() > 600].columns\n)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gp.get_library_name()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Over-Representation Analysis for Laplacian Score","metadata":{}},{"cell_type":"code","source":"fis_all_lap_T = fis_all_lap.loc[feature_order_lap].T\nctrlvscase_genelist_lap = list(fis_all_lap_T.loc[:, fis_all_lap_T.sum() > 600].columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disease_enr3 = gp.enrichr(\n    ctrlvscase_genelist_lap,\n    gene_sets=[\"RNA-Seq_Disease_Gene_and_Drug_Signatures_from_GEO\"],\n)\n#disease_enr3.res2d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disease_enr3.res2d.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"barplot(\n    disease_enr3.res2d,\n    title=\"RNA-Seq_Disease_Gene_and_Drug_Signatures_from_GEO\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the over-representation analysis, there is a term \"ALS IPSCs-derived neurons\" which is associated with gene overlap of MACF1,SNN,DST,PNMA1,DNAJC5,AKAP6 and DCAF7 that was significant after multiple testing correction.  ","metadata":{}},{"cell_type":"code","source":"disease_enr3 = gp.enrichr(\n    ctrlvscase_genelist_lap,\n    gene_sets=[\"SysMyo_Muscle_Gene_Sets\"],\n)\ndisease_enr3.res2d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The term \"Human prim myotube caveolinopathy v normal\" from the SysMyo Muscle Gene Set was significant after multiple testing correction for 7 overlapping genes. ","metadata":{}},{"cell_type":"code","source":"disease_enr3 = gp.enrichr(\n    ctrlvscase_genelist_lap,\n    gene_sets=[\"ARCHS4_Tissues\"],\n)\ndisease_enr3.res2d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"barplot(\n    disease_enr3.res2d,\n    title=\"ARCHS4_Tissues\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The term fetal cortex was significant in the over-representaion using the ARCHS Tissues database after multiple testing correction for an overlap of 16 genes like ANKRD36C,DFFA,DST, SRD5A1 anad NEXMIF. The terms like prefrontal cortex and cerebral cotrex  had the original p-values significant but had adjusted p-values there unsignficant for similar genes. These results suggest that there are a cluster of genes that were selected through laplacian score that were likely associated with brain tissue. ","metadata":{}},{"cell_type":"code","source":"disease_enr3 = gp.enrichr(\n    ctrlvscase_genelist_lap,\n    gene_sets=[\"HumanCyc_2016\"],\n)\ndisease_enr3.res2d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bulbar vs Limb","metadata":{}},{"cell_type":"markdown","source":"# Model of Bulbar vs Limb with the known genes ","metadata":{}},{"cell_type":"code","source":"KNOWN_GENES = [\"C9orf72\", \"SOD1\", \"TARDBP\", \"FUS\"]\nlgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults2, fis_all2 = run_bagged_validation_new(\n    bulbar_vs_limb,\n    gene_subset=KNOWN_GENES,\n    target_col=\"SiteOnset_Class\",\n    nsplits=10,\n    lgb_params=lgb_params,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot below shows the feature importance for the known genes.","metadata":{}},{"cell_type":"code","source":"feature_order2 = fis_all2.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(data=fis_all2.loc[feature_order2].T, palette=\"Blues_d\", orient=\"h\", ax=ax)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model of Bulbar vs Limb with the top K highest fisher scores added","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults2, fis_all2_fisher = run_bagged_validation_new(\n    bulbar_vs_limb.drop(columns=pseudogenes),\n    gene_subset=[],\n    method=\"Fisher\",\n    target_col=\"SiteOnset_Class\",\n    nsplits=10,\n    lgb_params=lgb_params,\n    topK=10,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a noticeable improvement in the F1 score from applying fisher score feature selection.","metadata":{}},{"cell_type":"markdown","source":"The plot below shows the feature importance for the Fisher score selection model.","metadata":{}},{"cell_type":"code","source":"feature_order2_fisher = fis_all2_fisher.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(\n    data=fis_all2_fisher.loc[feature_order2_fisher].T,\n    palette=\"Blues_d\",\n    orient=\"h\",\n    ax=ax,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model of Bulbar vs Limb with the top K highest Laplacian scores added","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults2_lap, fis_all2_lap = run_bagged_validation_new(\n    bulbar_vs_limb.drop(columns=pseudogenes),\n    gene_subset=[],\n    method=\"Laplacian\",\n    target_col=\"SiteOnset_Class\",\n    nsplits=10,\n    lgb_params=lgb_params,\n    topK=10,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a noticeable improvement in the F1 score from applying Laplacian score feature selection.","metadata":{}},{"cell_type":"markdown","source":"The plot below shows the feature importance for the Laplacian score selection model.","metadata":{}},{"cell_type":"code","source":"feature_order2_lap = fis_all2_lap.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(\n    data=fis_all2_lap.loc[feature_order2_lap].T, palette=\"Blues_d\", orient=\"h\", ax=ax\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Over-Representation Analysis for Fisher Score","metadata":{}},{"cell_type":"code","source":"fis_all2_fisher_T = fis_all2_fisher.loc[feature_order2_fisher].T\nbulbvslimb_genelist_fisher = list(\n    fis_all2_fisher_T.loc[:, fis_all2_fisher_T.sum() > 600].columns\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disease_enr3 = gp.enrichr(\n    bulbvslimb_genelist_fisher,\n    gene_sets=[\"ARCHS4_IDG_Coexp\"],\n)\ndisease_enr3.res2d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"barplot(\n    disease_enr3.res2d,\n    title=\"ARCHS4_IDG_Coexp\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There were a couple of terms from the ARCHS4 IDG Coexpression gene set that were significant after multiple testing correction.","metadata":{}},{"cell_type":"markdown","source":"## Over-Representation Analysis for Laplacian Score","metadata":{}},{"cell_type":"markdown","source":"There no significant terms found that are associated with ALS from the Laplacian Score ","metadata":{}},{"cell_type":"code","source":"fis_all2_lap_T = fis_all2_lap.loc[feature_order2_lap].T\nbulbvslimb_genelist_lap = list(\n    fis_all2_lap_T.loc[:, fis_all2_lap_T.sum() > 600].columns\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disease_enr3 = gp.enrichr(\n    bulbvslimb_genelist_lap,\n    gene_sets=[\"ClinVar_2019\"],\n)\ndisease_enr3.res2d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Median Low vs High","metadata":{}},{"cell_type":"markdown","source":"# Model of Median Low vs High with the known genes","metadata":{}},{"cell_type":"code","source":"KNOWN_GENES = [\"C9orf72\", \"SOD1\", \"TARDBP\", \"FUS\"]\nlgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\n\nresults3, fis_all3 = run_bagged_validation_new(\n    median_low_vs_high.drop(columns=pseudogenes),\n    gene_subset=KNOWN_GENES,\n    target_col=\"ALSFRS_Class_Median\",\n    nsplits=10,\n    lgb_params=lgb_params,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The F1 score is fairly low for the known genes model.","metadata":{}},{"cell_type":"code","source":"feature_order3 = fis_all3.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(data=fis_all3.loc[feature_order3].T, palette=\"Blues_d\", orient=\"h\", ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model of Median Low vs High the top K highest fisher scores added","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults3, fis_all3_fisher = run_bagged_validation_new(\n    median_low_vs_high.drop(columns=pseudogenes),\n    gene_subset=[],\n    target_col=\"ALSFRS_Class_Median\",\n    nsplits=10,\n    method=\"Fisher\",\n    lgb_params=lgb_params,\n    topK=10,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Fisher score for feature selection made a big improvement on the F1 score, but the F1 score is still well below .7 .","metadata":{}},{"cell_type":"markdown","source":"The graph below shows the feature importance for the fisher score selection model.","metadata":{}},{"cell_type":"code","source":"feature_order3_fisher = fis_all3_fisher.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(\n    data=fis_all3_fisher.loc[feature_order3_fisher].T,\n    palette=\"Blues_d\",\n    orient=\"h\",\n    ax=ax,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model of Median Low vs High the top K highest Laplacian scores added","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 10_000,\n}\nresults3_lap, fis_all3_lap = run_bagged_validation_new(\n    median_low_vs_high.drop(columns=pseudogenes),\n    gene_subset=[],\n    target_col=\"ALSFRS_Class_Median\",\n    nsplits=10,\n    method=\"Laplacian\",\n    lgb_params=lgb_params,\n    topK=10,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After using Laplacian score for feature selection,the F1 score slightly decreased compared the baseline F1 score from the known genes. ","metadata":{}},{"cell_type":"markdown","source":"The graph below shows the feature importance of the laplacian score selection model","metadata":{}},{"cell_type":"code","source":"feature_order3_lap = fis_all3_lap.mean(axis=1).sort_values(ascending=False).index\nfig, ax = plt.subplots(figsize=(10, 20))\nsns.barplot(\n    data=fis_all3_lap.loc[feature_order3_lap].T, palette=\"Blues_d\", orient=\"h\", ax=ax\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Over-Representation Analysis for Fisher Score and Laplacian Score","metadata":{}},{"cell_type":"markdown","source":"Unfortunately, there wasn't a large enough of an improvement (if an improvement at all) on the poor baseline predictions from a model with the known ALS genes from using feature selection with Fisher score and Laplacian score. The F1 scores were too low to suggest that the selected features from both methods explain the target variable. Therefore, I refrained from performing an Overrepresentation Analysis with Fisher Score and Laplacian Score for the Median Low vs High ALSFRS score data.","metadata":{}},{"cell_type":"markdown","source":"## Over-Representation Analysis for Fisher Score","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"fis_all3_fisher_T = fis_all3_fisher.loc[feature_order3_fisher].T\nhighvslow_genelist_fisher = list(\n    fis_all3_fisher_T.loc[:, fis_all3_fisher_T.sum() > 600].columns\n)\ndisease_enr = gp.enrichr(highvslow_genelist_fisher, gene_sets=[\"DisGeNET\"])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"barplot(\n    disease_enr.res2d,\n    title=\"DisGeNET\",\n)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Over-Representation Analysis for Laplacian Score","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"fis_all3_lap_T = fis_all3_lap.loc[feature_order3_lap].T\nhighvslow_genelist_lap = list(\n    fis_all3_lap_T.loc[:, fis_all3_lap_T.sum() > 3000].columns\n)\ndisease_enr = gp.enrichr(highvslow_genelist_lap, gene_sets=[\"DisGeNET\"])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disease_enr.res2d","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discussion/Conclusion","metadata":{}},{"cell_type":"markdown","source":"Utilizing Fisher score and Laplacian score for feature selection modestly improved the F1 score and the selected features may have significance to ALS as suggested by the over-representation analysis.The results also suggest that there may be multiple mechanisms of action for the disease.\n\nIt is important to note that all three models perform poorly on the classifier \"ALSFRS_Class_Median\" based on the F1 score. Since predictive accuracy was lackluster for even the model on the known genes associated with ALS it may be that \"ALSFRS_Class_Median\" is poor target variable for the transcriptomic data.\n\nThere are some limitations to the feature selection methods that I utilized. For example, since calculating the Laplacian score is has a high computational cost, I had to implement the method on only the 10,000 most variable genes in the datasets. By using this approach, there is a hidden assumption that genes with more variance are usually going to be stronger predictors. However, this may not always be the case. It is possible that there could be features that are strong predictors of the target that are outside the top 10,000 variable genes,which are neglected from this approach. One disadvantage Fisher score is that it is calculated using information from cases and controls in the training set. If it happens that the training set has one of two the groups underrepresented in the split, then it hurts the accuracy of the results.\n\n\nNevertheless, the results seem promising from our CV pipeline. Some next steps that we could take is construct gradient boost models with selected features from Laplacian Score and Fisher Score on a new set of data to evaulate the generalizablity the models.  ","metadata":{}}]}