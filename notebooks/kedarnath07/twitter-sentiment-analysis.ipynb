{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Import Libarries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-11T14:08:02.35099Z","iopub.execute_input":"2021-09-11T14:08:02.3512Z","iopub.status.idle":"2021-09-11T14:08:02.361675Z","shell.execute_reply.started":"2021-09-11T14:08:02.351177Z","shell.execute_reply":"2021-09-11T14:08:02.360718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Import Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:08:02.362826Z","iopub.execute_input":"2021-09-11T14:08:02.363579Z","iopub.status.idle":"2021-09-11T14:08:02.445884Z","shell.execute_reply.started":"2021-09-11T14:08:02.363543Z","shell.execute_reply":"2021-09-11T14:08:02.445029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.label.value_counts(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:08:02.447115Z","iopub.execute_input":"2021-09-11T14:08:02.447371Z","iopub.status.idle":"2021-09-11T14:08:02.455129Z","shell.execute_reply.started":"2021-09-11T14:08:02.447342Z","shell.execute_reply":"2021-09-11T14:08:02.45431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:08:02.456322Z","iopub.execute_input":"2021-09-11T14:08:02.456565Z","iopub.status.idle":"2021-09-11T14:08:02.467377Z","shell.execute_reply.started":"2021-09-11T14:08:02.456538Z","shell.execute_reply":"2021-09-11T14:08:02.466596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Cleaning and preprocessing","metadata":{}},{"cell_type":"code","source":"#Data have some inpredictiable and useless character and also there are puncutations are present so will remove using Natural language preprocessing library and also we will remove stopwords.\nlem=WordNetLemmatizer()\n\ncorpus = []\nfor i in range(0, len(df)):\n    review = re.sub('[^a-zA-Z]', ' ', df['tweet'][i]) #Instead of Punctuations it just replacing with \" \".\n    review = review.lower() #Lowering each sentence\n    review = review.split() #After lowering and replacing puntucation will split the words\n    review = [lem.lemmatize(word) for word in review if not word in stopwords.words('english')] # Here we will drop words which are present in stopwords like ['can't,they,we,here,there].\n    review = ' '.join(review)\n    corpus.append(review) #All sentences are collect in corpus.","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:08:02.46859Z","iopub.execute_input":"2021-09-11T14:08:02.468859Z","iopub.status.idle":"2021-09-11T14:08:58.919464Z","shell.execute_reply.started":"2021-09-11T14:08:02.468823Z","shell.execute_reply":"2021-09-11T14:08:58.917956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[7]","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:23:05.45796Z","iopub.execute_input":"2021-09-11T14:23:05.458698Z","iopub.status.idle":"2021-09-11T14:23:05.464079Z","shell.execute_reply.started":"2021-09-11T14:23:05.458655Z","shell.execute_reply":"2021-09-11T14:23:05.46345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the TF-IDF model\nfrom sklearn.feature_extraction.text import TfidfVectorizer #Using TFIDF vectorizer we will convert document in vector form.\ncv = TfidfVectorizer(max_features = 7777,ngram_range = (1,3))\nx= cv.fit_transform(corpus).toarray()\nx","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:21:01.240306Z","iopub.execute_input":"2021-09-11T14:21:01.240828Z","iopub.status.idle":"2021-09-11T14:21:04.681569Z","shell.execute_reply.started":"2021-09-11T14:21:01.24079Z","shell.execute_reply":"2021-09-11T14:21:04.680656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:21:06.885725Z","iopub.execute_input":"2021-09-11T14:21:06.886029Z","iopub.status.idle":"2021-09-11T14:21:06.893014Z","shell.execute_reply.started":"2021-09-11T14:21:06.885998Z","shell.execute_reply":"2021-09-11T14:21:06.891726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df.loc[:,['label']]\ny","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:17:22.788882Z","iopub.execute_input":"2021-09-11T14:17:22.789578Z","iopub.status.idle":"2021-09-11T14:17:22.80372Z","shell.execute_reply.started":"2021-09-11T14:17:22.78952Z","shell.execute_reply":"2021-09-11T14:17:22.802833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Test Split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)\nprint('Training data:{}'.format(X_train.shape))\nprint('Test data:{}'.format(X_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:53:15.155817Z","iopub.execute_input":"2021-09-11T14:53:15.156124Z","iopub.status.idle":"2021-09-11T14:53:15.184963Z","shell.execute_reply.started":"2021-09-11T14:53:15.156095Z","shell.execute_reply":"2021-09-11T14:53:15.183854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv.get_feature_names()[:20]","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:21:20.028844Z","iopub.execute_input":"2021-09-11T14:21:20.029132Z","iopub.status.idle":"2021-09-11T14:21:20.046292Z","shell.execute_reply.started":"2021-09-11T14:21:20.029104Z","shell.execute_reply":"2021-09-11T14:21:20.045001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv.get_params()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:21:24.299779Z","iopub.execute_input":"2021-09-11T14:21:24.300077Z","iopub.status.idle":"2021-09-11T14:21:24.307361Z","shell.execute_reply.started":"2021-09-11T14:21:24.300045Z","shell.execute_reply":"2021-09-11T14:21:24.306534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_df = pd.DataFrame(X_train, columns=cv.get_feature_names())\n\ncount_df","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:22:01.295515Z","iopub.execute_input":"2021-09-11T14:22:01.295787Z","iopub.status.idle":"2021-09-11T14:22:01.360886Z","shell.execute_reply.started":"2021-09-11T14:22:01.295759Z","shell.execute_reply":"2021-09-11T14:22:01.360048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.Built model","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:33:04.867168Z","iopub.execute_input":"2021-09-11T14:33:04.868616Z","iopub.status.idle":"2021-09-11T14:33:05.59915Z","shell.execute_reply.started":"2021-09-11T14:33:04.868548Z","shell.execute_reply":"2021-09-11T14:33:05.598263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n#Confusion Matrix\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:33:42.19169Z","iopub.execute_input":"2021-09-11T14:33:42.191999Z","iopub.status.idle":"2021-09-11T14:33:42.208464Z","shell.execute_reply.started":"2021-09-11T14:33:42.19196Z","shell.execute_reply":"2021-09-11T14:33:42.20779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:34:18.522513Z","iopub.execute_input":"2021-09-11T14:34:18.522965Z","iopub.status.idle":"2021-09-11T14:34:18.604918Z","shell.execute_reply.started":"2021-09-11T14:34:18.52293Z","shell.execute_reply":"2021-09-11T14:34:18.604018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_corpus = []\nfor i in range(0, len(test)):\n    review = re.sub('[^a-zA-Z]', ' ', test['tweet'][i])\n    review = review.lower()\n    review = review.split()\n    review = [lem.lemmatize(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    text_corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:37:39.761124Z","iopub.execute_input":"2021-09-11T14:37:39.761855Z","iopub.status.idle":"2021-09-11T14:38:09.84526Z","shell.execute_reply.started":"2021-09-11T14:37:39.761809Z","shell.execute_reply":"2021-09-11T14:38:09.844346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the TF-IDF model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntest_cv = TfidfVectorizer(max_features = 7777,ngram_range = (1,3))\nx= test_cv.fit_transform(text_corpus).toarray()\nx","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:42:10.094013Z","iopub.execute_input":"2021-09-11T14:42:10.094314Z","iopub.status.idle":"2021-09-11T14:42:12.155543Z","shell.execute_reply.started":"2021-09-11T14:42:10.094286Z","shell.execute_reply":"2021-09-11T14:42:12.154952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.DataFrame(x, columns=test_cv.get_feature_names())\n\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:42:13.907508Z","iopub.execute_input":"2021-09-11T14:42:13.908359Z","iopub.status.idle":"2021-09-11T14:42:13.991025Z","shell.execute_reply.started":"2021-09-11T14:42:13.90832Z","shell.execute_reply":"2021-09-11T14:42:13.990341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_y = clf.predict(test_df)\ntest_y","metadata":{"execution":{"iopub.status.busy":"2021-09-11T14:42:35.249276Z","iopub.execute_input":"2021-09-11T14:42:35.249601Z","iopub.status.idle":"2021-09-11T14:42:35.691655Z","shell.execute_reply.started":"2021-09-11T14:42:35.249561Z","shell.execute_reply":"2021-09-11T14:42:35.690839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}