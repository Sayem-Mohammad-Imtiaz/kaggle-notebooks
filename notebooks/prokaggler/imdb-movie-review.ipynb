{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import packages\nimport pandas as pd, numpy as np\nimport tensorflow as tf\nassert tf.__version__ >= '2.0'\n\nfrom itertools import islice\n\n# Keras\nfrom keras.layers import Dense, Embedding, LSTM, Dropout, MaxPooling1D, Conv1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, Sequential\nfrom keras.preprocessing import sequence\nfrom keras.datasets import imdb\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Suppress warnings\nimport warnings; warnings.filterwarnings('ignore')\n\nrandom_state = 42\nnp.random.seed(random_state)\ntf.random.set_seed(random_state)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-15T06:44:56.912031Z","iopub.execute_input":"2021-07-15T06:44:56.912468Z","iopub.status.idle":"2021-07-15T06:45:04.008716Z","shell.execute_reply.started":"2021-07-15T06:44:56.912378Z","shell.execute_reply":"2021-07-15T06:45:04.007623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"vocab_size = 10000\nmaxlen = 300\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = vocab_size)\n\nx_train = pad_sequences(x_train, maxlen = maxlen, padding = 'pre')\nx_test =  pad_sequences(x_test, maxlen = maxlen, padding = 'pre')\n\nX = np.concatenate((x_train, x_test), axis = 0)\ny = np.concatenate((y_train, y_test), axis = 0)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state, shuffle = True)\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state = random_state, shuffle = True)\n\nprint('---'*20, f'\\nNumber of rows in training dataset: {x_train.shape[0]}')\nprint(f'Number of columns in training dataset: {x_train.shape[1]}')\nprint(f'Number of unique words in training dataset: {len(np.unique(np.hstack(x_train)))}')\n\n\nprint('---'*20, f'\\nNumber of rows in validation dataset: {x_valid.shape[0]}')\nprint(f'Number of columns in validation dataset: {x_valid.shape[1]}')\nprint(f'Number of unique words in validation dataset: {len(np.unique(np.hstack(x_valid)))}')\n\n\nprint('---'*20, f'\\nNumber of rows in test dataset: {x_test.shape[0]}')\nprint(f'Number of columns in test dataset: {x_test.shape[1]}')\nprint(f'Number of unique words in test dataset: {len(np.unique(np.hstack(x_test)))}')\n\n\nprint('---'*20, f'\\nUnique Categories: {np.unique(y_train), np.unique(y_valid), np.unique(y_test)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T06:45:07.906572Z","iopub.execute_input":"2021-07-15T06:45:07.9071Z","iopub.status.idle":"2021-07-15T06:45:20.695754Z","shell.execute_reply.started":"2021-07-15T06:45:07.907068Z","shell.execute_reply":"2021-07-15T06:45:20.694501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get word index and create a key-value pair for word and word id","metadata":{}},{"cell_type":"code","source":"def decode_review(x, y):\n  w2i = imdb.get_word_index()                                \n  w2i = {k:(v + 3) for k, v in w2i.items()}\n  w2i['<PAD>'] = 0\n  w2i['<START>'] = 1\n  w2i['<UNK>'] = 2\n  i2w = {i: w for w, i in w2i.items()}\n\n  ws = (' '.join(i2w[i] for i in x))\n  print(f'Review: {ws}')\n  print(f'Actual Sentiment: {y}')\n  return w2i, i2w\n\nw2i, i2w = decode_review(x_train[0], y_train[0])\n\n# get first 50 key, value pairs from id to word dictionary\nprint('---'*30, '\\n', list(islice(i2w.items(), 0, 50)))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T06:45:56.754133Z","iopub.execute_input":"2021-07-15T06:45:56.754528Z","iopub.status.idle":"2021-07-15T06:45:57.014431Z","shell.execute_reply.started":"2021-07-15T06:45:56.754496Z","shell.execute_reply":"2021-07-15T06:45:57.013505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## build LSTM using keras","metadata":{}},{"cell_type":"code","source":"# Model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 256, input_length = maxlen))\nmodel.add(Dropout(0.25))\nmodel.add(Conv1D(256, 5, padding = 'same', activation = 'relu', strides = 1))\nmodel.add(Conv1D(128, 5, padding = 'same', activation = 'relu', strides = 1))\nmodel.add(MaxPooling1D(pool_size = 2))\nmodel.add(Conv1D(64, 5, padding = 'same', activation = 'relu', strides = 1))\nmodel.add(MaxPooling1D(pool_size = 2))\nmodel.add(LSTM(75))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nprint(model.summary())\n\n# Adding callbacks\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 0)  \nmc = ModelCheckpoint('imdb_model.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T06:46:35.702416Z","iopub.execute_input":"2021-07-15T06:46:35.702925Z","iopub.status.idle":"2021-07-15T06:46:36.197804Z","shell.execute_reply.started":"2021-07-15T06:46:35.702893Z","shell.execute_reply":"2021-07-15T06:46:36.196757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model\nmodel.fit(x_train, y_train, validation_data = (x_valid, y_valid), epochs = 3, batch_size = 64, verbose = True, callbacks = [es, mc])\n\n# Evaluate the model\nscores = model.evaluate(x_test, y_test, batch_size = 64)\nprint('Test accuracy: %.2f%%' % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T06:46:46.695133Z","iopub.execute_input":"2021-07-15T06:46:46.695515Z","iopub.status.idle":"2021-07-15T06:59:59.534486Z","shell.execute_reply.started":"2021-07-15T06:46:46.695484Z","shell.execute_reply":"2021-07-15T06:59:59.533397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict_classes(x_test)\nprint(f'Classification Report:\\n{classification_report(y_pred, y_test)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T06:59:59.536674Z","iopub.execute_input":"2021-07-15T06:59:59.53699Z","iopub.status.idle":"2021-07-15T07:00:30.073996Z","shell.execute_reply.started":"2021-07-15T06:59:59.536959Z","shell.execute_reply":"2021-07-15T07:00:30.072851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_x_test = x_test[np.random.randint(10000)]\nfor layer in model.layers:\n\n    model_layer = Model(inputs = model.input, outputs = model.get_layer(layer.name).output)\n    output = model_layer.predict(sample_x_test.reshape(1,-1))\n    print('\\n','--'*20, layer.name, 'layer', '--'*20, '\\n')\n    print(output)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:00:30.07603Z","iopub.execute_input":"2021-07-15T07:00:30.076685Z","iopub.status.idle":"2021-07-15T07:00:31.582545Z","shell.execute_reply.started":"2021-07-15T07:00:30.076636Z","shell.execute_reply":"2021-07-15T07:00:31.581515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decode_review(x_test[10], y_test[10])\nprint(f'Predicted sentiment: {y_pred[10][0]}')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:00:51.261556Z","iopub.execute_input":"2021-07-15T07:00:51.261954Z","iopub.status.idle":"2021-07-15T07:00:51.370505Z","shell.execute_reply.started":"2021-07-15T07:00:51.261924Z","shell.execute_reply":"2021-07-15T07:00:51.368827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n- Sentiment classification task on the IMDB dataset, on test dataset,\n- Accuracy: ~ 90%\n- F1-score: ~ 90%\n- Loss of 0.25","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}