{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section is working of read wine quality dataset.","metadata":{}},{"cell_type":"code","source":"#read the data, check if there are missing values\nred_wine = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nred_wine.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Inspection of the data***\n\nThere is *no missing data* in this dataset. \n\nNo obvoius pair of features has really high correlation. As scale of the data does not affect the performance of random forest, therefore, no scaling will be performed. \n\nBelow graph is a pair plot for first 5 variables in the data. Fixed acidity and citric acid seem having a positive linaer relationship. Citric acid and volatile acidity seem to have a negative linear trend. ","metadata":{}},{"cell_type":"code","source":"sns.pairplot(red_wine.iloc[:,0:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no pair of features have really high correlation.","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(8,8))\nsns.heatmap(red_wine.corr(), cmap='BrBG', annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the training data and the test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(red_wine.loc[:,red_wine.columns != 'quality'], \n                                                    red_wine['quality'], test_size=0.15, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hyperparameter in RF Regressor: Try to avoid overfitting**\n\nThe ***max depth*** sets as 18, this is trying to **avoid overfitting** on each tree. This avoid too much split based on the training data, which might lead to the model doing well in the training set but not the test set. \n\nThe ***n_estimators*** sets as 80, smaller than the default number of estimator, too much number of number of estimators might lead to overfitting on the training set. Again, it is used to **avoid overfitting**.\n\n***n_job*** is -1 means using all processors.","metadata":{}},{"cell_type":"markdown","source":"\n***No feature scaling***\n\nNo feature scaling is taken as scale of the data would not affect the performance or the split of each of the decision trees in the forest. So, feature scaling would not be performed.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n#a function to build random forest with different dataset, with the prediction on training set and test set\ndef build_forest(X_train,y_train,X_test,y_test):\n    model = RandomForestRegressor(n_estimators= 80,max_depth=20, random_state=0,n_jobs = -1)\n    model.fit(X_train,y_train)\n    #test prediciton\n    rfr_test_pred = np.around(model.predict(X_test))\n    #train prediction \n    rfr_train_pred = np.around(model.predict(X_train))\n    #MAE calculation\n    mae_test = mean_absolute_error(y_test,rfr_test_pred)\n    mae_train = mean_absolute_error(y_train,rfr_train_pred)\n    \n    return model, rfr_test_pred,rfr_train_pred,mae_test,mae_train\n\nrfr, rfr_test_pred,rfr_train_pred,mae_test,mae_train = build_forest(X_train,y_train,X_test,y_test)\n\nprint('The MAE for the training set is ',round(mae_train,3))\nprint('The MAE for the test set is ',round(mae_test,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation of the graphs below: (how good the model is)**\n\nThe plots below show the actual values of compressive Strength against the predicted values. The closer the data points to the straight line, the more accuracy the prediction is. \n\nThe model is doing better in the training set than the test set.\n\nThe predictions of the regression tree are lying horizontally, which means they made the same predicted values.\n\nThe prediction at test set is not good as the training set since the data points are more far away from the middle line. And hence, there are only 3 unique predicted values in the test set.\n\nTo sum up, the model are doing pretty well, the predicted values are close to acutal values.","metadata":{}},{"cell_type":"code","source":"#Actual vs predicted values\ndef fitted_vs_actual(model,X_test,X_train,y_true_test,y_true_train,titles):\n    #Make prediction from the adaboost model\n    y_pred_test = np.around(model.predict(X_test))\n    y_pred_train = np.around(model.predict(X_train))\n    #plotting\n    fig, axes = plt.subplots(1, 2,figsize=(15,5))\n    colors = ['green','#01B6B7']\n    y_pred = [y_pred_test,y_pred_train]\n    y_true = [y_true_test,y_true_train]\n    for i in range(2):\n        sns.regplot(x = y_pred[i],y = y_true[i], color=colors[i],ax = axes[i])\n        axes[i].title.set_text(titles[i])\n        axes[i].set(ylabel='actual values',xlabel='predicted values')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fitted_vs_actual(rfr,X_test,X_train,y_test,y_train,['Test Data','Training Data'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the raw error are similar in both training set and the test set. ","metadata":{}},{"cell_type":"code","source":"def dis_error(model,X_test,X_train,y_test,y_train):\n    fig, axes = plt.subplots(1, 2,figsize=(18,5))\n\n    #The raw error for test\n    y_pred_test = np.around(model.predict(X_test))\n    y_pred_train = np.around(model.predict(X_train))\n    \n    sns.countplot(x = (y_pred_test-y_test).to_numpy(),ax=axes[0])\n    axes[0].title.set_text('Test set raw error')\n    axes[0].set_xlabel('Raw error')\n    axes[0].set_ylabel('Frequency')\n\n    sns.countplot(x=(y_pred_train-y_train).to_numpy(),ax=axes[1])\n    axes[1].title.set_text('Training set raw error')\n    axes[1].set_xlabel('Raw error')\n    axes[1].set_ylabel('Frequency')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dis_error(rfr,X_test,X_train,y_test,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importances","metadata":{}},{"cell_type":"markdown","source":"Retain only those features which has importance values above 5%. \n\nThe random forest model has an attribute that store the feature importance. It is impurity-based feature importances. \n\nThe higher, the more important the feature. \n\nThe importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.","metadata":{}},{"cell_type":"markdown","source":"Using SelectFromModel in sklearn can do the task. Putting the estimator and threshold value for feature selection intp the parameters, we can see whether the individual features higher than the threshold.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n#prefit is True since we have fitted the model\nselector = SelectFromModel(estimator=rfr,prefit=True,threshold=0.05)\n#Get the columns name with feature importance above threshold\nfeature_selected = np.array(X_train.columns)[selector.get_support()]\n#Get the columns name with feature importance below threshold\nremoved_feature= np.array(X_train.columns)[~selector.get_support()]\n\nprint('These are the features with importance values higher than 0.05 are:')\nprint('\\n')\nprint(', '.join([str(feature ) for feature in feature_selected]))\nprint('\\n')\nprint('The total number of features have been removed is', len(X_train.columns)-len(feature_selected),'.')\nprint('\\n')\nprint('The feature removed is',', '.join([str(feature ) for feature in removed_feature]),'.')\nprint('\\n')\nprint('The total feature importance value that is retained after the dimension reduction step',round(np.sum(selector.estimator.feature_importances_[selector.get_support()]),3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"citric acid does not provide too much information on the split of the data, which has a feature importance about 0.044. We will drop in the following section.","metadata":{}},{"cell_type":"markdown","source":"# Refit the model with selected features","metadata":{}},{"cell_type":"markdown","source":"Use the selected features to build a new Random forest regressor. The hyperparameter are same as the previous one. So we can compare the models with same setting with the training data is the only difference.\n\nSubset the training set, only the features with importance values higher than 0.05 are kept.","metadata":{}},{"cell_type":"code","source":"#subset the features\nreduced_X_train = X_train.loc[:,feature_selected]\nreduced_X_test = X_test.loc[:,feature_selected]\n\nrfr, rfr_test_pred,rfr_train_pred,mae_test,mae_train = build_forest(reduced_X_train,y_train,reduced_X_test,y_test)\n\nprint('The MAE for the training set on the model with feature selection is ',round(mae_train,3))\nprint('The MAE for the test set on the model with feature selection is ',round(mae_test,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the raw error are similar in training set and the test set. The model predicts the quality perfectly in most of the case. However, the model is doing better in the training set(graph on the right), especially when predicting wine quality about -1 and about 1 comparing to the test set. And hence, there are some error equal to 2 and -2 for the test set but not in training set. \n\nIn general, the model performs better in the training set, which is expected.","metadata":{}},{"cell_type":"markdown","source":"There are no features with importance values smaller than 0.05, as a result the total feature importance value is 1. It is becuase no feature need to be dropped.","metadata":{}},{"cell_type":"code","source":"selector = SelectFromModel(estimator=rfr,prefit=True,threshold=0.05)\nfeature_selected = np.array(reduced_X_train.columns)[selector.get_support()]\nremoved_feature= np.array(reduced_X_train.columns)[~selector.get_support()]\n\nprint('These are the features with importance values higher than 0.05')\nprint('\\n')\nprint(', '.join([str(feature) for feature in feature_selected]))\nprint('\\n')\nprint('The total number of features have been removed is', len(reduced_X_train.columns)-len(feature_selected),'.')\nprint('\\n')\nprint('The total feature importance value that is retained after the dimension reduction step',np.sum(selector.estimator.feature_importances_[selector.get_support()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation of the graphs below:(how good the model is)**\n\nThe prediction are fairly good at the training set, the data points are closer to the line in the middle, which mean their are close to the true value. In constract, the prediction at test set is not good as the training set since the data points are more far away from the middle line. And hence, there are only 3 unique predicted values in the test set.","metadata":{}},{"cell_type":"code","source":"fitted_vs_actual(rfr,reduced_X_test,reduced_X_train,y_test,y_train,['Test Data','Training Data'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dis_error(rfr,reduced_X_test,reduced_X_train,y_test,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the raw error are similar in training set and the test set. The model predicts the quality perfectly in most of the case. However, the model is doing better in the training set(graph on the right), especially when predicting wine quality about -1 and about 1 comparing to the test set. The number of raw error for -1 and 1 in training set is much more lower than that in the test set. \n\nAdditionally, there are some error equal to 2 and -2 (overestimate and underesimate) for the test set but not in training set. \n\nIn general, the model performs better in the training set, which is expected.","metadata":{}}]}