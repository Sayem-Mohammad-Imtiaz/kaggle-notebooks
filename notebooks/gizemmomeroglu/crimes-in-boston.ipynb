{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OVERVÄ°EW\n\n---\n\n### Hey there! I want to show my study about crimes in boston dataset. Before analyzing the data, I want to fix the shortcomings of dataset. You can see it below. I describe this step as research.\n\n                                                                *   Upload data\n                                                                *   Looking at general information (type etc.)\n                                                                *   Filling in blank values\n\n<img src=\"https://media1.tenor.com/images/55f2496603cfa0243089afc587956eaa/tenor.gif?itemid=9938974\" width=\"273\" height=\"273\" alt=\"Research GIF - Research GIFs\" style=\"max-width: 273px; background-color: rgb(63, 63, 63);\">\n\n## **Let's do some research!**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata= pd.read_csv('../input/crimes-in-boston/crime.csv',encoding=\"iso-8859-1\")\n\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I rearranged the lat AND long values according to the values in the location column and arranged the dates so that I could draw a time series. then I filled in the blanks.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Lat-1','Long-1']]=data['Location'].str.split(\",\", expand = True)\ndata['Lat'] = data['Lat-1'].str[1:]\ndata['Long'] = data['Long-1'].str[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Lat']=data['Lat'].astype(float)\ndata['Long']=data['Long'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\ndata['OCCURRED_ON_DATE']=pd.to_datetime(data['OCCURRED_ON_DATE'])\ndata['Date']=data['OCCURRED_ON_DATE'].dt.strftime('%Y-%m-%d')\ndata['Date']=pd.to_datetime(data['Date'])\ndata['MONTH'] = data.Date.dt.strftime(\"%m\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().any()[lambda x: x] # I wanted to determine which columns have NaN values, then I will examine these columns one by one.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['DISTRICT'].value_counts() #I will write 'unclear' in nan places.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['DISTRICT'].fillna('unclear',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['SHOOTING'].value_counts() # I understand that they wanted to label it as Yes and No, so I'll write N to Nan values.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['SHOOTING'].fillna('N',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['UCR_PART'].value_counts() # I will call those who are NaN \"uncertain\".","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['UCR_PART'].fillna('uncertain',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['STREET'].value_counts() # I will call those who are NaN \"uncertain\".","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['STREET'].fillna('uncertain',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All of them is filled. Great!\n\n## LET'S DIVE IN!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe() #there is something wrong with lat and long. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data.groupby([\"Lat\",\"Long\"]).count()[['INCIDENT_NUMBER']]).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# There are question patterns where we can reach general information about data. WHO? WHERE? WHEN? WHY? HOW?\n\n# WHO?                        \n\n * Which of the offense code group is more?\n\n# WHERE?  (What can you say about the distribution of different offenses over the city?)\n\n * In which district more?\n * Which street is more?\n * Which reporting area is the most?\n\n# WHEN?\n\n * Has it increased over the years?\n * Is it variable by day and month?\n * Does the clock affect? Is it more evening?\n\n# HOW?\n\n * How does it affect whether it is shooting or not?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nx=pd.pivot_table(data, index='OFFENSE_CODE_GROUP',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nx = x.reindex(x.sort_values(by=['INCIDENT_NUMBER'], ascending=False).index)\nx=x[:10]\nfig_dims = (8, 7)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"INCIDENT_NUMBER\",y=\"OFFENSE_CODE_GROUP\", data=x)\n\n#We can see that the most of the crimes occured named Motor Vehicle Accident Response. Larceny and medical assistance follow it.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. How has crime changed over the years?\n\n * While there has been an increase over the years since 2015, we see that it decreased in 2018.\n * When we look at the year as a month; There were obvious decreases in the 6th month of 2015 and the 9th month of 2018.\n * There is no extreme difference according to the days. They are all close to each other.\n * The highest hours in the evening according to the hours. at least between 3-5 at night.\n\n\n## When we say all of above, we could be wrong. Because in some years, some of months are missing.So,we cannot reach the conclusion just by looking at these.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index='YEAR',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nax = sns.barplot(x=\"YEAR\", y=\"INCIDENT_NUMBER\", data=x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import altair as alt\nx=pd.pivot_table(data, index='MONTH',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nax = sns.barplot(x=\"MONTH\", y=\"INCIDENT_NUMBER\", data=x)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import altair as alt\nx=pd.pivot_table(data, index='DAY_OF_WEEK',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nax = sns.barplot(x=\"DAY_OF_WEEK\", y=\"INCIDENT_NUMBER\", data=x)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index='HOUR',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\n\nax = sns.barplot(x=\"HOUR\", y=\"INCIDENT_NUMBER\", data=x)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now, we can look average of crime in months,year...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()\ndata['Day']=data.OCCURRED_ON_DATE.dt.day\nx=pd.pivot_table(data, index=['YEAR','MONTH','Day'],values=\"INCIDENT_NUMBER\", aggfunc='count', fill_value=0)\nx=x.reset_index()\nY=pd.pivot_table(x, index=['YEAR'],values=\"INCIDENT_NUMBER\", aggfunc=np.mean, fill_value=0)\nY=Y.reset_index()\nY.INCIDENT_NUMBER.round(0)\n\n\na=pd.pivot_table(data, index=['YEAR','MONTH','Day'],values=\"INCIDENT_NUMBER\", aggfunc='count', fill_value=0)\na=a.reset_index()\nb=pd.pivot_table(a, index=['MONTH'],values=\"INCIDENT_NUMBER\", aggfunc=np.mean, fill_value=0)\nb=b.reset_index()\nb.INCIDENT_NUMBER.round(0)\n\nc=pd.pivot_table(data, index=['YEAR','MONTH','Day','DAY_OF_WEEK'],values=\"INCIDENT_NUMBER\", aggfunc='count', fill_value=0)\nc=c.reset_index()\nd=pd.pivot_table(c, index=['DAY_OF_WEEK'],values=\"INCIDENT_NUMBER\", aggfunc=np.mean, fill_value=0)\nd=d.reset_index()\nd.INCIDENT_NUMBER.round(0)\n\na4_dims = (20, 5)\nfig, (ax1, ax2,ax3) = plt.subplots(ncols=3, sharey=True,figsize=a4_dims)\n\nax = sns.barplot(x=\"YEAR\", y=\"INCIDENT_NUMBER\", data=Y,ax=ax1,color='lightblue')\n\nax = sns.barplot(x=\"MONTH\", y=\"INCIDENT_NUMBER\", data=b,ax=ax2,color='lightgreen')\n\nax = sns.barplot(x=\"DAY_OF_WEEK\", y=\"INCIDENT_NUMBER\", data=d,ax=ax3,color='lightseagreen')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, my comment is;\n\n## Yearly\n\n### * I can not say obvious comment. But, I want to look at deeply.Maybe we can do statistical test for this.\n\n## Monthly\n\n### * We can see that crimes increase towards May and June. But we cannot say that there is an increase in the whole summer. Because, our average daily number of thefts decreased in the months after June.\n\n## Day of Week\n\n### * When we look at the average daily numbers of thefts on the chart, there is a clear decrease on Sunday. But no interpretation can be made for other days like this.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# GENERAL TIME LINE\n\n### We can see that the average daily crimes in months rise during the summer months. Especially in the summer of 2017, there is a huge increase in numbers. and we can see cycles. In the winter months, it increases less in summer and decreases again towards the end of the year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index=['Date','YEAR','MONTH','Day'],values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\n\nx['MONTH'] = x['MONTH'].astype(str)\nx['YEAR']=x['YEAR'].astype(str)\nx['YEARMONTH'] = x['YEAR'].str.cat(x['MONTH'],sep=\"\")\n\ny=pd.pivot_table(x, index=['YEARMONTH'],values=\"INCIDENT_NUMBER\", aggfunc=\"mean\", fill_value=0)\ny=y.reset_index()\ny['YEARMONTH']=y['YEARMONTH'].astype(str)\ny = y.reindex(y.sort_values(by=['YEARMONTH'], ascending=[True]).index)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 5)\nsns.set(style=\"darkgrid\")\ng=sns.lineplot(x=\"YEARMONTH\", y=\"INCIDENT_NUMBER\",data=y)\nplt.xticks(rotation=70)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM ANALYSIS\n\n### I wanted to apply a pareto analysis to offence groups. In this way, if a measure is to be taken. By giving priority to the A group that has the most impact, an 80% efficiency can be achieved.\n\n### Group A came out as follows:\n\n*   Motor Vehicle Accident Response\n*   Larceny\n*   Medical Assistance\n*   Investigate Person\n*   Other\n*   Drug Violation\n*   Simple Assault\n*   Vandalism\n*   Verbal Disputes\n*   Towed\n*   Investigate Property\n*   Larceny From Motor Vehicle\n*   Property Lost\n*   Warrant Arrests\n*   Aggravated Assault\n*   Violations\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"abc=pd.pivot_table(data, index=['OFFENSE_CODE_GROUP'],values=\"INCIDENT_NUMBER\",  aggfunc='count', fill_value=0)\nabc=abc.reset_index()\nabc = abc.reindex(abc.sort_values(by='INCIDENT_NUMBER', ascending=False).index)\nabc['cumsum'] = abc['INCIDENT_NUMBER'].cumsum()\nabc['sum'] = abc['INCIDENT_NUMBER'].sum()\nabc['percentage'] =  (abc['cumsum']/abc['sum'])*100\n\ndef ABC_segmentation(perc):\n   \n     if perc > 0 and perc < 80:\n        return 'A'\n     elif perc >= 80 and perc < 95:\n        return 'B'\n     elif perc >= 95:\n        return 'C'\n\n\n\nabc['clustering'] = abc['percentage'].apply(ABC_segmentation)\nabc[abc['clustering']=='A']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Change of offense code statuses by years\n\n### We know that in the crime data, some of the months are missing. After, we look at the data, it can be seen in year 2016 and 2017 all of the months avaible. So, I found the increase percentages of offense codes between 2016-2017 values. There are extremely increasing or decreasing varieties here. Some of them have a high effect even if their numbers are small. For instance, there is obvious changing  on Prostituon in 2017. It has changed in the direction of an increase of 40%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(\"MONTH\")[\"YEAR\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index=['OFFENSE_CODE_GROUP','YEAR','MONTH','Day'],values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nY=pd.pivot_table(x, index='OFFENSE_CODE_GROUP',columns='YEAR',values=\"INCIDENT_NUMBER\", aggfunc=np.mean, fill_value=0)\nY=Y.reset_index()\nY.columns=['OFFENSE_CODE_GROUP','YIL2015','YIL2016','YIL2017','YIL2018']\nY['DEGISIM']=((Y['YIL2017']-Y['YIL2016'])/Y['YIL2016'])*100\nY=Y.replace([np.inf,-np.inf], np.nan)\nY=Y.dropna()\nY = Y.reindex(Y.sort_values(by=\"DEGISIM\", ascending=False).index)\nfig_dims = (10, 13)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"DEGISIM\", y=\"OFFENSE_CODE_GROUP\", data=Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# HYPOTHESIS - IS THERE A DIFFERENCE  IN 2016 AND 2017?\n\n## I have said that in the above analyzes, statistical methods can be used to understand whether there is a difference over the years. I wanted to run a hypothesis test to see if there was a difference in daily crime numbers between 2016 and 2017.\n\n## Method: We can use MANN WHITNEY-U test for compare  of 2 years.\n\n### H0: Î¼d = 0 \n### H1: Î¼d â  0","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.first we should seperate years 2016 and 2017","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hypothesis=data[(data['YEAR']==2016) | (data['YEAR']==2017)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. We should found the crime number in year-month-day format","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot=pd.pivot_table(hypothesis, index=['YEAR','MONTH','Day'],values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nboxplot=boxplot.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. We can draw a boxplot for see the general situation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x=\"YEAR\", y=\"INCIDENT_NUMBER\", data=boxplot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. We should do MANN WHITNEY-U to testing the hypthesis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import mannwhitneyu\ntest=pd.pivot_table(hypothesis, index=['Day'],columns='YEAR',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\ntest.columns=['YIL2016','YIL2017']\nstat, p = mannwhitneyu(test['YIL2016'], test['YIL2017'])\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Same distribution (fail to reject H0)')\nelse:\n\tprint('Different distribution (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We find p value <0,05. So, the findings are statistically significant! One can reject the H0 hypothesis in support of the alternative.So statistically, there is a difference between years 2016 and 2017 for daily crimes.\n\n\n### Then, we look boxplot above and in year 2017 there are less outlier then year 2016 and the median of crime is higher too. We can say that crimes increased in 2017.\n\n<img alt=\"The Simpsons Crime GIF - Find &amp; Share on GIPHY\" class=\"n3VNCb\" src=\"https://media1.giphy.com/media/l2Je5OTIcc8fqrjs4/giphy.gif\" data-noaft=\"1\" jsname=\"HiaYvf\" jsaction=\"load:XAeZkd,gvK6lb;\" style=\"width: 428px; height: 322.783px; margin: 0px;\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. What can you say about the distribution of different offenses over the city?\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index='DISTRICT',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nx = x.reindex(x.sort_values(by=['INCIDENT_NUMBER'], ascending=False).index)\nax = sns.barplot(x=\"DISTRICT\", y=\"INCIDENT_NUMBER\", data=x)\n#We can see that the crimes are concentrated in the B2 district.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index='STREET',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nx = x.reindex(x.sort_values(by=['INCIDENT_NUMBER'], ascending=False).index)\nx.head()\n#We can see that the crimes are concentrated in WASHINGTON ST. Crimes with an uncertain values follow it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index='REPORTING_AREA',values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nx = x.reindex(x.sort_values(by=['INCIDENT_NUMBER'], ascending=False).index)\nx.head()\n#There are records with an empty reporting area section.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.pivot_table(data, index=['DISTRICT','OFFENSE_CODE_GROUP'],values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\nx=x.reset_index()\nx['D-O'] = x['DISTRICT'].str.cat(x['OFFENSE_CODE_GROUP'],sep=\"-\")\nx = x.reindex(x.sort_values(by=['INCIDENT_NUMBER','DISTRICT'], ascending=False).index)\nxvalue=x[:5]\nfig_dims = (5, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"INCIDENT_NUMBER\", y=\"D-O\", data=xvalue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## When we look at the top 30, the types of crimes in the B2, B3, C11 and D4 regions draw our attention.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pivfordis=pd.pivot_table(data, index=['DISTRICT','OFFENSE_CODE_GROUP'],values=\"INCIDENT_NUMBER\", aggfunc=\"count\", fill_value=0)\npivfordis=pivfordis.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivfordisB2=pivfordis[(pivfordis['DISTRICT']== 'B2') ]\npivfordisB2['D-O'] = pivfordisB2['DISTRICT'].str.cat(pivfordisB2['OFFENSE_CODE_GROUP'],sep=\"-\")\npivfordisB2 = pivfordisB2.reindex(pivfordisB2.sort_values(by=['DISTRICT','INCIDENT_NUMBER'], ascending=False).index)\nxvalue=pivfordisB2[:5]\nfig_dims = (5, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"INCIDENT_NUMBER\", y=\"D-O\", data=xvalue)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivfordisC11=pivfordis[(pivfordis['DISTRICT']== 'C11') ]\npivfordisC11['D-O'] = pivfordisC11['DISTRICT'].str.cat(pivfordisC11['OFFENSE_CODE_GROUP'],sep=\"-\")\npivfordisC11 = pivfordisC11.reindex(pivfordisC11.sort_values(by=['DISTRICT','INCIDENT_NUMBER'], ascending=False).index)\nxvalue=pivfordisC11[:5]\n\nfig_dims = (5, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"INCIDENT_NUMBER\", y=\"D-O\", data=xvalue)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivfordisD4=pivfordis[(pivfordis['DISTRICT']== 'D4') ]\npivfordisD4['D-O'] = pivfordisD4['DISTRICT'].str.cat(pivfordisD4['OFFENSE_CODE_GROUP'],sep=\"-\")\npivfordisD4 = pivfordisD4.reindex(pivfordisD4.sort_values(by=['DISTRICT','INCIDENT_NUMBER'], ascending=False).index)\nxvalue=pivfordisD4[:5]\nfig_dims = (5, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"INCIDENT_NUMBER\", y=\"D-O\", data=xvalue)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivfordisB3=pivfordis[(pivfordis['DISTRICT']== 'B3') ]\npivfordisB3['D-O'] = pivfordisB3['DISTRICT'].str.cat(pivfordisB3['OFFENSE_CODE_GROUP'],sep=\"-\")\npivfordisB3 = pivfordisB3.reindex(pivfordisB3.sort_values(by=['DISTRICT','INCIDENT_NUMBER'], ascending=False).index)\nxvalue=pivfordisB3[:5]\nfig_dims = (5, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"INCIDENT_NUMBER\", y=\"D-O\", data=xvalue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### When we look at the selected regions, we can say that high crimes are generally similar. these:\n### * Motor Vehicle Accident Response\n### * Verbal Disputes\n### * Medical Asistanse\n\n### Only for D4 region **larceny** comes first.\n\n# If you noticed, we also detected these crimes with pareto analysis.\n\n### Let's try some more mapping methods.\n\n<img src=\"https://media1.tenor.com/images/0c7d7c3c22d522674057187eae35cd22/tenor.gif?itemid=13892258\" width=\"273\" height=\"174.72\" alt=\"Lost Kermit GIF - Lost Kermit Map GIFs\" style=\"max-width: 273px; background-color: rgb(63, 63, 63);\">\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### (-1,-1) and (0,0) coordinates coincide with the middle of the ocean. That's why I drop values.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(data.groupby([\"Lat\",\"Long\"]).count()[['INCIDENT_NUMBER']]).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_folium=data.drop(data[(data['Lat']==-1) & (data['Long']==-1)].index)\ndata_folium1=data_folium.drop(data_folium[(data_folium['Lat']==0) & (data_folium['Long']==0)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\ncrime_map = folium.Map(location=[42.3125,-71.0875], \n                       tiles = \"Stamen Terrain\",\n                      zoom_start = 11)\ndf_drop=data_folium1.dropna(subset=['Lat', 'Long', 'DISTRICT'])\n# Add data for heatmp \ndata_heatmap = df_drop[df_drop[\"YEAR\"]==2016]\ndata_heatmap = df_drop[['Lat','Long']]\ndata_heatmap = df_drop.dropna(axis=0, subset=['Lat','Long'])\ndata_heatmap = [[row['Lat'],row['Long']] for index, row in data_heatmap.iterrows()]\nHeatMap(data_heatmap, radius=10).add_to(crime_map)\n\n# Plot!\ncrime_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### When looked separately at the Engine Vehicle Accident Response, it seems that it has spread everywhere.\n\n### The intensity of crime is higher on the main streets.But,the cops may have deliberately written the main streets. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.Is it possible to predict where or when a crime will be committed?\n\n### When a detailed forecast is required, for example, hourly work will become increasingly difficult. Because there are manual entries in the data, the margin of error can be high.\n\n### In general, As a first step, I think that data with a lot of categorical variables can be started with logistic regression and then looked at in detail. We can use month, day, street variables. In this way, a simple estimate can be made as a first step.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# THANK YOU!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}