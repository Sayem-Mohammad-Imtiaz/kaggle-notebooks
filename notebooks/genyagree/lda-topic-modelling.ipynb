{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Intro\nIn natural language processing, the **Latent Dirichlet Allocation** (LDA) is a generative statistical model that allows to divide a collection of texts into N-number of subgroups, where each subgroup is characterized by a set of X-number of keywords, and this set of keywords is associated with a topic. Both the topic (as a set of words) and the text (as a set of topics describing the text) are described by Dirichlet-distribution. \n\nLet's say we have three texts:<br>\n\"Dogs like playing.\"<br>\n\"Cats like milk.\"<br>\n\"Cats and dogs like eating and playing. I love dogs. They are adorable.\" <br>\n\nThe results from LDA model could be the following:<br>\n\nText1: 100% Topic1 + 0% Topic2<br>\nText2: 100% Topic2 + 0% Topic1<br>\nText3: 70% Topic2 + 30% Topic1<br>\n\nWhere each topic represented by a set of words (from most to least relevant), which forms the topic:<br>\n\nTopic1: 30% dog, 30% playing, 20% like 10% adorable 10% love<br>\nTopic2: 50% cat, 30% milk, 20% like<br>\n\n# When to apply LDA topic modeling\n\nWhen we have a collection of documents and wish to understand what the collection/archive contains without necessarily reading every document.\n   - If we are working with a small number of documents (or even a single document), word frequency counts (or TF-IDF) might be sufficient in order to get an idea what the text is about. <br>\n   - However, if we have a large number of documents, then topic modeling might be a good approach.\n   \n# Theory: How LDA model works and what's behind it\nIn latent diriclet allocaton (LDA) model, each document is considered to be characterized by a set of topics that is following the Dirichlet distribution. \n\n\nIn LDA probabilistic topic modeling:\n- a collection of documents (texts) $D$ is given\n- each document $d$ from the collection is a sequence of words $W_{d} = (w_{1}, ..., w_{n_{d}})$ from dictionary $W$, where $n_{d}$ - length of document d. \n- each document may be related to one or several themes\n- order of documents in collection is not important\n- order of words in documents is neglected, each document is considered as \"bag of words\"\n- document collection is considered as set of pairs \"document-word\" $(d,w), d \\in D, w \\in W_{d}$\n- each topic $t\\in T$, where $T$ - set of topics, is described by Dirichlet-distribution $p(w|t)$ on the range of $w\\in W$, in other words there are topic vectors: $\\phi_{t} = (p(w|t):w \\in W)$\n- each document $d\\in D$ is described by Dirichlet-distribution $p(t|d)$ on the range of $t\\in T$, in other words there are document vectors: $\\theta_{d} = (p(t|d):t \\in T)$\n<br>\n\nProbability of a pair \"document-word\" to occure can be written as: \n$$\np(d,w)=\\sum\\limits_{t\\in T}p(t)p(w|t)p(d|t)\n$$\n\nTo build a topic model means to find matrices $\\Phi = ||p(w|t)||$ and $\\Theta = ||p(t|d)||$ given collection $D$.\n\nIn order to find a solution, we need to solve the optimization problem, i.e. to maximaze the function: \n$$\n\\sum\\limits_{d\\in D}\\sum\\limits_{w\\in d}n_{dw}logp(w|d)\\to\\max\\limits_{\\Phi,\\Theta},\n$$\nwhere $n_{dw}$ is frequency of word $w$ in document $d$.\n\n# The goals:\n\nThis notebook aimes to investigate the capabilities of LDA topic modelling techniques on Russian texts. The main goals will be:\n- to test LDA and Mallet LDA on Russian texts\n- to find whether filtering of most common and most rare words will increase model performance\n- to find most frequent topics(e.g. first 20) and their sets of keywords for the given collection of documents"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing the package for lemmatization (Russian language)\n!pip install pymorphy2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\nimport json\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis\nimport pyLDAvis.gensim  \nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom matplotlib import rcParams\n%matplotlib inline\nimport nltk\nimport pymorphy2\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstopwordsrus = set(stopwords.words('russian'))\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking up Kaggle directories, loading the dataset\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nrus_data = pd.read_csv(\"/kaggle/input/corpus-of-russian-news-articles-from-lenta/lenta-ru-news.csv\")\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's have a look at the data:\nrus_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrus_data['topic'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's shorten the dataset and exclude some of the topics: \n\nrusdata = rus_data['text'][(rus_data['topic']!='Библиотека')&(rus_data['topic']!='Бывший СССР')&(rus_data['topic']!='69-я параллель')].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These Russian news articles we will use for LDA analysis:\nrusdata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading stopwords and combining them:\nstops1 = []\nwith open('/kaggle/input/russianstopwords/RussianStopWords.txt', \"r\", encoding=\"utf-8\", newline=None) as readfile:\n     [stops1.append(line.rstrip()) for line in readfile]\n\nstops2 = []\nwith open('/kaggle/input/stopwords/stopwords.txt', \"r\", encoding=\"utf-8\", newline=None) as readfile:\n     [stops2.append(line.rstrip()) for line in readfile]\n\nstopw_all = stops1 + stops2 + list(stopwordsrus)\nstopwordsru = list(dict.fromkeys(stopw_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing and removing stopwords:\ndef process(text):\n    return list(t.lower() for t in word_tokenize(text) if t.isalpha() and t.lower() not in stopwordsru)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take only first 10000 articles for our analysis:\ndata = [process(t) for t in rusdata[:10000]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatizer for russian language:\nmorph = pymorphy2.MorphAnalyzer()\ndef lemmatizer(texts):\n    return [[morph.parse(word)[0] for word in text] for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"morph_data = lemmatizer(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need only lemma of the word, without additional information, so let's extract it:\ndef extract_lemma(texts):\n    norm = []\n    for t in texts:\n        res = []\n        for word in t:\n            n = word.normal_form\n            res.append(n)\n        norm.append(res)\n    return norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is our lemmatized data ready to be used further:\ndata_norm = extract_lemma(morph_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's build the bigram and trigram models using gensim\nbigram = gensim.models.Phrases(data_norm, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_norm], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndata_words_trigrams = make_trigrams(data_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that bigrams and trigrams are build successfully \nprint(data_words_trigrams[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create dictionary of all our unique words from the dataset using corpora from gensim\ndictionary = corpora.Dictionary(data_words_trigrams)\n# Now let's create corpus where we count occurances for each word from dictionary in texts\ncorpus = [dictionary.doc2bow(doc) for doc in data_words_trigrams]\n# We will also try to filter unimportant words by their tf-idf score, so let's create the tf-idf scores here too\ntfidf = gensim.models.TfidfModel(corpus, id2word = dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A word from our dictionary:\ndictionary[8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see what are the max and min values of tf-idf score:\ntf_max = round(max([max([value for id, value in tfidf[corpus[x]]]) for x in range(len(corpus))]), 4)\ntf_min = round(min([min([value for id, value in tfidf[corpus[x]]]) for x in range(len(corpus))]), 4)\nprint(tf_max, tf_min)\ntfidf_range = [round(num, 3) for num in np.arange(tf_min, tf_max, 0.005).tolist()]\n# We will be cutting the highest and the lowest tf-idf, e.g. <10 and >95% of all tf-idf values, so let's obtain those values:\nprint(np.percentile(tfidf_range, 95), np.percentile(tfidf_range, 10))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's select high and low tfidf threshold values, and filter them out. \n# Thus, we will filter out very common (low tf-idf) and very rare (big tf-idf) words\n\nlow_value = np.percentile(tfidf_range, 5) \nhigh_value = np.percentile(tfidf_range, 95) \n\nfiltered_corpus = []\nfor i in range(0, len(corpus)):\n        \n    filter_ids = [id for id, value in tfidf[corpus[i]] if value < low_value or value > high_value ]\n   \n    new_bow = [(index, value) for (index, value) in corpus[i] if index not in filter_ids] \n\n      \n    filtered_corpus.append(new_bow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have prepered the data. We have two types of corpus:\n# original without tf-idf filtering called \"corpus\", and the one with tf-idf filtering called \"filtered_corpus\"\n\n# Firtst, let's build LDA model from Gensim without tf-idf filtering.\n# Since the number of texts is quite large (10000), let's chose number of topics equal to 80.\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=dictionary,\n                                           num_topics=80, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Topics\npprint(lda_model.show_topics(formatted=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. The lower the better.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's build basic Gensim LDA model with tf-idf filtering:\nlda_model = gensim.models.ldamodel.LdaModel(corpus=filtered_corpus,\n                                           id2word=dictionary,\n                                           num_topics=80, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Topics\npprint(lda_model.show_topics(formatted=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(filtered_corpus))  # a measure of how good the model is. lower the better.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mallet LDA Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip mallet-2.0.8.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmallet_path = '/kaggle/working/mallet-2.0.8/bin/mallet'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's run Mallet LDA model with Nr. of topics 145\nldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=145, id2word=dictionary, random_seed=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Topics\npprint(ldamallet.show_topics(formatted=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Coherence Score\ncoherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_ldamallet = coherence_model_ldamallet.get_coherence()\nprint('\\nCoherence Score: ', coherence_ldamallet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_topics_sentences(ldamodel, corpus, texts):\n    # Init output\n    sent_topics_df = pd.DataFrame (index=range(10000), columns = ['Dominant_Topic1', 'Dominant_Topic2', '%Topic_Contribution1', '%Topic_Contribution2', 'Topic_Keywords1', 'Topic_Keywords2'])\n    #sent_topics_df = pd.DataFrame()\n    \n\n    # Get main topic in each document\n    for i, text in enumerate(ldamodel[corpus]):\n        text = sorted(text, key=lambda x: (x[1]), reverse=True) #sort % contributions of topic  \n        # Get the Dominant topic, % of topic contribution and Keywords for each document\n        for j, (topic_num, topic_contrib) in enumerate(text):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df.Dominant_Topic1[i] = int(topic_num)\n                sent_topics_df['%Topic_Contribution1'][i] = round(topic_contrib,4)\n                sent_topics_df['Topic_Keywords1'][i] = topic_keywords\n                \n                #sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n                \n            elif j == 1:  # => second dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df.Dominant_Topic2[i] = int(topic_num)\n                sent_topics_df['%Topic_Contribution2'][i] = round(topic_contrib,4)\n                sent_topics_df['Topic_Keywords2'][i] = topic_keywords\n                \n            else:\n                break\n    \n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=rusdata[:10000])\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index(drop = True)\n\n\n# Show\ndf_dominant_topic.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we can select a text and see its first and second most dominant topics (keywords of those topics)\nindex = 0\ndf_dominant_topic['text'][index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dominant_topic['Topic_Keywords1'][index]+'// '+df_dominant_topic['Topic_Keywords2'][index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LDA Mallet with high and low tf-idf filtered out"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's build Mallet LDA with tf-idf -filtered corpus:\n\nldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=filtered_corpus, num_topics=145, id2word=dictionary, random_seed=0)#150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Coherence Score\ncoherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_ldamallet = coherence_model_ldamallet.get_coherence()\nprint('\\nCoherence Score: ', coherence_ldamallet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Topics\npprint(ldamallet.show_topics(formatted=False))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=filtered_corpus, texts=rusdata[:10000])\n\ndf_dominant_topic_filtered_idfs = df_topic_sents_keywords.reset_index(drop = True)\n\n# Show\ndf_dominant_topic_filtered_idfs.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To see an example of how the model worked: we can pick a text by index, and see the keywords of most dominant topic\n# (with higest contribution, as well as the second one)\nindex = 2\ndf_dominant_topic_filtered_idfs['text'][index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Two most dominint topics (their keywords)\ndf_dominant_topic_filtered_idfs['Topic_Keywords1'][index]+'// '+df_dominant_topic_filtered_idfs['Topic_Keywords2'][index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's plot 20 most frequent topics from our 10000 texts, as well as the second most frequent topic\n# Let's organize data for the plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counting texts for each topic number\ncounts = []\nn_topics = 145\nfor x in range(0, n_topics):\n    z = df_dominant_topic['Dominant_Topic1'][df_dominant_topic['Dominant_Topic1'] == x].count()\n    counts.append([x,z])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sorting by number of texts\nsorted_counts = sorted(counts, key=lambda x: int(x[1]), reverse=True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting most popular n-themes\nn_themes = 25\nmost_popular = [sorted_counts[x][0] for x in range(n_themes)] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting keywords for most popular n-themes\ntheme_keywords = [df_dominant_topic['Topic_Keywords1'][df_dominant_topic['Dominant_Topic1']==x].unique().tolist() for x in most_popular]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count number of texts for second topics\ntopic_counts = []\nn_topics = 145\nfor x in range(0, n_topics):\n    for y in range(0, n_topics):\n        z = df_dominant_topic['Dominant_Topic1'][(df_dominant_topic['Dominant_Topic1'] == x) & (df_dominant_topic['Dominant_Topic2'] == y)].count()\n        topic_counts.append([x,y,z])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sorting \ntwo_themes_count = sorted(topic_counts, key=lambda x: int(x[2]), reverse=True) \n# Selecting second topics for first popular n-topics\nmost_frequent = [two_themes_count[ind] for ind in range(len(two_themes_count)) if two_themes_count[ind][0] in most_popular]\nmost_frequent_sorted = sorted(most_frequent, key=lambda x: int(x[0]), reverse=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting most frequent second topics that follow those most frequent first topics\nsecond_topic = []\nfor x in most_popular:\n    for y in range(len(most_frequent_sorted)):\n        if most_frequent_sorted[y][0] == x:\n            second_topic.append(most_frequent_sorted[y:y+1][0])\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting array of second topic numbers\nsecond_topic_num = [second_topic[x][1] for x in range(n_themes)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Corresponding keywords for second topics\ntheme_keywords_second_topic = [df_dominant_topic['Topic_Keywords2'][df_dominant_topic['Dominant_Topic2']==x].unique().tolist() for x in second_topic_num]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['font.size'] = 9\nrcParams['axes.titlesize'] = 14\nplt.rc('xtick', labelsize=14)\nplt.rc('ytick', labelsize=14)\nplt.rc('axes', labelsize=14)\nrcParams['figure.dpi']= 600\n\ntheme = [sorted_counts[x][0] for x in range(n_themes)]\ntext_count = [sorted_counts[x][1] for x in range(n_themes)]\ny_pos = np.arange(len(theme))\n\n\nfig, ax = plt.subplots(figsize=(16, 20)) # 16 14\n\nplot = ax.barh(y_pos, text_count, align='center')\n\nplt.xlim(0, 500)\n\nax.set_yticks(y_pos)\nax.set_yticklabels(theme)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('N of texts')\nax.set_ylabel('Topic number')\nax.set_title('25 most frequent topics')\n\nx_offset = -100\ny_offset = 0.3\n\nfor ind, bar in enumerate(plot):\n    \n    yloc = bar.get_y() + bar.get_height() / 2\n    width = int(bar.get_width())\n    ax.annotate('T1: '+', '.join(theme_keywords[ind][0].split(', ')[:3])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[4:7])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[8:10]), xy=(width+x_offset, yloc+y_offset), fontsize=10)\n\n\n\n#for ind, p in enumerate(ax.patches):\n   # b = p.get_bbox()\n    #val = \"{:.0f}\".format(b.x1)        \n    #ax.annotate('T1: '+', '.join(theme_keywords[ind][0].split(', ')[:4])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[5:10]), (5, b.y1 + y_offset), fontsize=4)\n    \n\nx_offset = 30\ny_offset = 0.3\nfor ind, bar in enumerate(plot):\n    #b = p.get_bbox()\n    \n    #val = \"{:.0f}\".format(b.x1) \n    yloc = bar.get_y() + bar.get_height() / 2\n    width = int(bar.get_width())\n    #ax.annotate('T2: '+', '.join(theme_keywords_second_topic[ind][0].split(', ')[:4])+'\\n'+', '.join(theme_keywords_second_topic[ind][0].split(', ')[5:10]), (500, b.y1 + y_offset), fontsize=4)\n    ax.annotate('T2: '+', '.join(theme_keywords_second_topic[ind][0].split(', ')[:3])+'\\n'+', '.join(theme_keywords_second_topic[ind][0].split(', ')[4:7])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[8:10]), xy=(width+x_offset, yloc+y_offset), fontsize=10)\n\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Each document can be described by a distribution of topics [T1 + T2 + T3 + ... + T150] and each topic can be described by a distribution of words, where T1 - topic with highest contribution, thus T1 considered as the prevalent topic of text.\n<br>\n- Hovewer it makes sence to look at the second or even third topic in order to get broader overview and better idea of what the text is about.\n<br>\n- In the graph, the most frequent topics (T1) are represented, as well as most frequent T2, following that T1. \n- In this test, words filtering by their tf-idf score (most rare and most common words) didn't lead to accuracy improvement. Hovewer more investigation is needed.\n- More investigation on optimizing the number of topics is required\n- Further word filtering might be required to improve accuracy of the model"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}