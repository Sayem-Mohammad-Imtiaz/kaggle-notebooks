{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets import Libraries for EDA and data visualizations"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data size seems to be very small. So we should try simple models Here.\nLets see what the data looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see Pandas Profiling on this Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"pp.ProfileReport(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is a mixture of categorical and continuous values. Thats cool.\nLets see the distribution of target."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA DESCRIPTION**\n\n(This data description is given on UCI website for this dataset[ Link](https://archive.ics.uci.edu/ml/datasets/heart+disease))\n\n*  age: age in years\n*  sex: sex (1 = male; 0 = female)\n*  cp: chest pain type\n    -- Value 1: typical angina\n    -- Value 2: atypical angina\n    -- Value 3: non-anginal pain\n    -- Value 4: asymptomatic\n*  trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n*  chol: serum cholestoral in mg/dl\n*  fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n*  restecg: resting electrocardiographic results\n    -- Value 0: normal\n    -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n    -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n*  thalach: maximum heart rate achieved\n*  exang: exercise induced angina (1 = yes; 0 = no)\n*  oldpeak = ST depression induced by exercise relative to rest\n*  slope: the slope of the peak exercise ST segment\n    -- Value 1: upsloping\n    -- Value 2: flat\n    -- Value 3: downsloping\n*  ca: number of major vessels (0-3) colored by flourosopy\n*  thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n*  target: Heart disease (0 = no, 1 = yes)"},{"metadata":{},"cell_type":"markdown","source":"Lets see if there any null value in data. If there is any, We have to handle it here."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So no null values in data. Seems like small and clean dataset. Lets proceed..."},{"metadata":{},"cell_type":"markdown","source":"Now we can see how many unique values each feature have. So to get the idea of categorical and numerical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in df.columns:\n    print(column,df[column].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cool...\nNow we can see the correlation matrix of data. Lets see if there is any high correlated feature that we can pay attention to."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (16, 14)\n# plt.style.use('ggplot')\nsns.heatmap(df.corr(), annot = True, cmap = 'PiYG')\nplt.title('Heatmap of Data', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nope. Seems like no such highly correlated feature."},{"metadata":{},"cell_type":"markdown","source":"**PLOTS**"},{"metadata":{},"cell_type":"markdown","source":"Okay. Time to make some plots to get the feel of data. This part is very important before modelling. Maybe fo small datasets you can get away without any plots and visualizations. But when the data size is huge and lots of features are there. You will have to make some plots to get better idea of the dataset."},{"metadata":{},"cell_type":"markdown","source":"Lets first plot numeric features and see there distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\n\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak']):\n    sns.distplot(df[feature], ax=ax[i//2,i%2], hist=True, color= 'y' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems fine."},{"metadata":{},"cell_type":"markdown","source":"Now lets plot categorical features and see there distribution. We are using countplot here."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(4,2,figsize=(10,8))\n\nfor i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\n    sns.countplot(x=feature,data=df,ax=ax[i//2,i%2], alpha=0.8, edgecolor=('white'), linewidth=2)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay. so some featues are not evenly distributed here. For small datasets, this problem can be there."},{"metadata":{},"cell_type":"markdown","source":"We have visualized each features individually, Now we can plot there relation with the target variable to see their impact on target. This can give how a feature can be important in predicting target."},{"metadata":{},"cell_type":"markdown","source":"Lets first see numeric features interaction with target"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['age'], palette = 'colorblind')\nplt.title('Age vs Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this distribution we can see Age is not a good feature in deciding target. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['thalach'], palette = 'colorblind')\nplt.title('thalach vs Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here Target 1 seems to higher mean and also low \"maximum heart rate achieved\" values means no Heart Disease. It makes sense to us. Great."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['chol'], palette = 'colorblind')\nplt.title('chol vs Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this plot, we can see people with heart diseases might have some high cholesterol values. Good feature!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['trestbps'], palette = 'colorblind')\nplt.title('trestbps vs Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this plot shows that people not suffering from Heart disease might have little high blood pressure."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['oldpeak'], palette = 'colorblind')\nplt.title('oldpeak vs Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this plot shows that people not suffering from Heart disease might have high ST depression. "},{"metadata":{},"cell_type":"markdown","source":"Now lets see relation of categorical features with target."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['restecg']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of ECG measurement with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\nHere we can see 0 and 2 values of ECG are more common in People with no heart disease. And value of 1 is more common in People with disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['fbs']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of blood sugar with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not any such difference."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['sex'])\ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of Gender with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So you can say Women are more prone to heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['cp']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of  chest pain with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\nHere we can see count of 0 is high in people with no heart disease whereas count of other values are high in people with heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['exang']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of Exercise induced angina with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here also we can see relation with target. count of 0 is high in people with disease and count of 1 is high in people without disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['slope']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of slope with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also related. Count of 1 is higher in people without disease and count of 2 is higher in people with disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['ca']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of major vessels with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot is also very clear. Count of  is higher in people with disease and  other counts are higher in people without disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['thal']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation thalassemia with Target', fontsize = 20, fontweight = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot is also very clear. Count of 2 is higher in people with disease and count of 3 is higher in people without disease"},{"metadata":{},"cell_type":"markdown","source":"So these were some important plots. You can also make bivariate plot for multiple feature interactions. But I wanted to keep it simple."},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = ['sex','cp','fbs','restecg','exang','slope','ca','thal']\nnumeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_cols:\n    print(i,'\\n', df[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PREPROCESSING**"},{"metadata":{},"cell_type":"markdown","source":"In preprocessing step, we will standard scale all numeric features and one-hot encode all multilabel features. This is pretty simple and straight forward Preprocessing. You can also do some outlier removal."},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_label_cols = [i for i in categorical_cols if df[i].nunique()>2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std = StandardScaler()\ndf[numeric_cols] = std.fit_transform(df[numeric_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(data = df,columns = multi_label_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop(['target'],axis=1)\ny = df['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets do test train split for model training and evaluation. You can also try k-fold cross validation here."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TRAINING AND EVALUATING MODELS**"},{"metadata":{},"cell_type":"markdown","source":"Since the dataset is small, We will try simple models to reduce overfitting here. But can try XGBoost just for fun. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nsvm = SVC(probability=True)\nrf = RandomForestClassifier(n_estimators=100, max_depth=5)\nxg = xgb.XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = ['lr','svm','rf','xg']\nfor model in models:\n    clf = eval(model)\n    clf.fit(x_train, y_train)\n    y_pred_prob = clf.predict_proba(x_test)[:, 1]\n    y_pred = clf.predict(x_test)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    auc = roc_auc_score(y_test, y_pred_prob)\n    # evaluating the model\n    print(f\"Training Accuracy for model {model} is: \", clf.score(x_train, y_train))\n    print(f\"Testing Accuracy for model {model} is:\", clf.score(x_test, y_test))\n    print(f\"AUC Score for model {model} is: {auc}\")\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure()\n    sns.heatmap(cm, annot = True)\n    print(classification_report(y_test, y_pred))\n    plt.figure()\n    plt.plot(fpr, tpr)\n    plt.title(f'ROC for model {model}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the Logistic regression is doing a good job and not overfitting the data.\n"},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameter tuning**\n\nHere we are trying Grid search for Logistic regression which did a good job. But you can also try it on different models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid={\"C\":np.logspace(-3,3,10), \"penalty\":[\"l1\",\"l2\"]}\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x,y)\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's it. Seems like a decent accuracy with minimal work. \n\n\n\nPlease give it an upvote if you like this notebook. Also if you have any questions or comments, Please post. Will answer surely. \n\nThank you."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}