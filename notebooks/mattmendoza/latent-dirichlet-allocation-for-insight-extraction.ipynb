{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Load libraries\n\nimport numpy as np \nimport pandas as pd \nimport json\nimport glob\nimport re\nimport pickle\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generate lists of all the json's of all the papers in each \n\nbiorxiv_list = glob.glob(\"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/*.json\")\ncomm_list = glob.glob(\"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/*.json\")\nnoncomm_list = glob.glob(\"/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/*.json\")\npmc_list = glob.glob(\"/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/*.json\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#These functions will parse the different fields in the jsons\n\ndef ParseAbstract(full_paper):\n    try:\n        return full_paper['abstract'][0]['text']\n    except:\n        return 'No Abstract'\n\n    \ndef ParseRefEntries(full_paper):\n    try:\n        return pd.DataFrame(full_paper['ref_entries']).iloc[0].values\n    except:\n        return \"No Entries\"\n\ndef TagKeyword(jsonfile_str):\n    full_paper = json.load(open(jsonfile_str, \"r\"))\n    paper_id = full_paper['paper_id']\n    title = full_paper['metadata']['title']\n    abstract = ParseAbstract(full_paper)\n    body = full_paper['body_text'][0]['text']\n    ref_entries = ParseRefEntries(full_paper)\n    bib_entries = full_paper['bib_entries']\n    back_matter = full_paper['back_matter']\n    #print(full_paper.keys())\n    return [paper_id,title,abstract,body,ref_entries,bib_entries,back_matter]\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#aggregate all of the papers into one np array\ndf = []\nprint('running on biorxiv')\nfor i in range(len(biorxiv_list)):\n    df.append(TagKeyword(biorxiv_list[i]))\n\n    \nprint('running on common use subset')\nfor i in range(len(comm_list)):\n    df.append(TagKeyword(comm_list[i]))\n\n    \nprint('running on noncommon use subset')\nfor i in range(len(noncomm_list)):\n    df.append(TagKeyword(noncomm_list[i]))\n\n    \nprint('running on pmc custom license')\nfor i in range(len(pmc_list)):\n    df.append(TagKeyword(pmc_list[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert np array to data frame, label columns\nalltext_df = pd.DataFrame(df,columns=['paper_id','title','abstract','body','ref_entries','bib_entries','back_matter'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle alltext_df so that it can be easily accessed without running the code above every session\n#comment out the pickle dump so it doesn't overwrite, load the pickle back to the original df\n#pickle.dump(alltext_df,open( \"alltext_dataframe.pkl\", \"wb\" ))\nalltext_df = pickle.load(open('alltext_dataframe.pkl','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if the df loaded correctly\nalltext_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will flag keywords in corpus that will allow for brute force clustering of papers across all sources. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#flag texts with specific keywords in the title, abstract, or body text\nalltext_df['CoV_flag'] = alltext_df['title'].str.contains('CoV') | alltext_df['title'].str.contains('coronavirus', case=False) | \\\n                         alltext_df['abstract'].str.contains('CoV') | alltext_df['abstract'].str.contains('coronavirus', case=False) | \\\n                         alltext_df['body'].str.contains('CoV') | alltext_df['body'].str.contains('coronavirus', case=False)\n\nalltext_df['SARS_flag'] = alltext_df['title'].str.contains('SARS') | \\\n                          alltext_df['abstract'].str.contains('SARS') | \\\n                          alltext_df['body'].str.contains('SARS')\n\nalltext_df['MERS_flag'] = alltext_df['title'].str.contains('MERS') | \\\n                          alltext_df['abstract'].str.contains('MERS') | \\\n                          alltext_df['body'].str.contains('MERS')\n\n#truncate \"antibod\" to include both singular and plural\nalltext_df['antibody_flag'] = alltext_df['title'].str.contains('antibod') | \\\n                              alltext_df['abstract'].str.contains('antibod') | \\\n                              alltext_df['body'].str.contains('antibod')\n\nalltext_df['vaccine_flag'] = alltext_df['title'].str.contains('vaccin') | \\\n                             alltext_df['abstract'].str.contains('vaccin') | \\\n                             alltext_df['body'].str.contains('vaccin')\n\nalltext_df['treatment_flag'] = alltext_df['title'].str.contains('treatment') | \\\n                             alltext_df['abstract'].str.contains('treatment') | \\\n                             alltext_df['body'].str.contains('treatment')\n\nalltext_df['cure_flag'] = alltext_df['title'].str.contains('cure') | \\\n                             alltext_df['abstract'].str.contains('cure') | \\\n                             alltext_df['body'].str.contains('cure')\n\nalltext_df['theraputics_flag'] = alltext_df['title'].str.contains('therap') | \\\n                             alltext_df['abstract'].str.contains('therap') | \\\n                             alltext_df['body'].str.contains('therap')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By creating a subset of texts that contain keywords of interest, we can then add certain obvious words to the stop words list, thus leaving the remaining tf-idf and LDA analysis to extract less common keywords shared amongst the subset of literature. In this way, keywords of interest can be whittled down from a larger list of keywords to see frequently discussed concepts in the subset of texts of interest. We are extracting additional concepts and keywords that are found in a subset of texts known to have an initial keyword in common that we are interested in. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words):  \n    #this function parses the extracted keywords from the LDA model\n    extracted_keywords = []\n    for topic_idx, topic in enumerate(model.components_):\n        extracted_keywords.extend([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n    return extracted_keywords\n\n\n\ndef LDAwithExclusions(alltext_df,keyword_exclusion_list,no_features=50,no_topics=20,no_top_words=20): \n    #this function fits the input subset of text to Latent Dirichlet Allocation\n    keyword_exclusion_list = text.ENGLISH_STOP_WORDS.union(keyword_exclusion_list)\n\n    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n    tf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=10, max_features=no_features, stop_words=keyword_exclusion_list)\n    tf = tf_vectorizer.fit_transform(alltext_df[alltext_df['vaccine_flag']==True]['title'])\n    tf_feature_names = tf_vectorizer.get_feature_names()\n\n    # Run LDA\n    lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n    return display_topics(lda,tf_feature_names,no_top_words)\n\n\ndef FlagNewWords(alltext_df,list_of_words): \n    #this function allows for the brute force flagging and subsetting of additional keywords in order to force the model away or towards specific words\n    for word in list_of_words:\n        alltext_df[word+'_flag'] = alltext_df['title'].str.contains(word) | \\\n                                   alltext_df['abstract'].str.contains(word) | \\\n                                   alltext_df['body'].str.contains(word)\n    return alltext_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"e.g. below, I'm looking for texts that have to do with antibodies. At first, with no stop words, it returns topics that are relatively obvious such as \"respiratory\" or \"protein\". After iteratively running the LDAwithExclusions function more and more 'obvious' and/or unhelpful keywords are added to the stop words list and the LDA extracts less obvious keywords that the antibody subset of texts have in common.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"text_subset = alltext_df[alltext_df['antibody_flag']==1] \nkeyword_exclusion_list = ['coronavirus','cov','protein','influenza','human','sars','virus','viral','vaccine',\n                          'infection','antibody','neutralizing','respiratory','antibodies','immune','vaccines',\n                          'infectious','responses','response','syndrome','development','dna','vaccination',\n                          'immunity','protection','cell','mice','disease','specific']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, counts of the keywords in each topic extracted by the LDA show the relative strength of the partner keywords.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dftest = LDAwithExclusions(text_subset,keyword_exclusion_list)\n\nkeyword_counts = pd.DataFrame([[term,dftest.count(term)] for term in pd.Series(dftest).unique()],columns=['Keyword','Count']).sort_values('Count',ascending=False)\nkeyword_counts.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that 'recombinant', 'neutralization', and 'immunogenicity' are topics that are strongly related to the original topic of inquiry: 'antibodies'. This strength corresponds to how much of the literature discusses the related topic within the subset.\n\nThis process of subsetting, fitting, adding stopwords, and refitting can be iterated over by subject matter experts in order to extract concepts from the body of texts\n\nFor example, below I pivot on the concept of \"recombinant\" and rerun the model which extracts the concepts of things like 'antgens' and 'spike' which can inspire new pivots and subsearches within the body of literature"},{"metadata":{"trusted":true},"cell_type":"code","source":"recombinant_subset = FlagNewWords(alltext_df,['recombinant','neutralization'])\nrecombinant_subset = recombinant_subset[(recombinant_subset['recombinant_flag']==1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keyword_exclusion_list.append('recombinant')\ndftest = LDAwithExclusions(recombinant_subset,keyword_exclusion_list)\n\nkeyword_counts = pd.DataFrame([[term,dftest.count(term)] for term in pd.Series(dftest).unique()],columns=['Keyword','Count']).sort_values('Count',ascending=False)\nkeyword_counts.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Here I started with the concept of \"antibodies\" which drew interest to \"recombinant\" and lead me to \"immunogenicity\", \"antigens\", \"attenuated\", etc. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}