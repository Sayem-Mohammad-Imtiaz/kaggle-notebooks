{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA PREPROCESSING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping the Columns which are having Null values**\n\ndrop columns with empty data > 10%"},{"metadata":{"trusted":true},"cell_type":"code","source":"#amount empty data\ncol_empty = df.apply(lambda x: f'{(x.isnull().sum()/df.shape[0]).round(2)} %').sort_values()\ncol_empty","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns with empty data > 10%\ndf.drop(col_empty.index.to_list()[-4:], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cols(df) -> list:\n    '''\n    function return list of name numbers and categorials columns\n    '''\n    categorical_feature_mask = df.dtypes == object\n    number_feature_mask = df.dtypes != object\n    numbers_cols = df.columns[number_feature_mask].tolist()\n    categorical_cols = df.columns[categorical_feature_mask].tolist()\n    return [numbers_cols, categorical_cols]\n\nnum_cols, cat_cols = get_cols(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill na data\nfrom sklearn.impute import SimpleImputer\n\n\nimp_mean_num = SimpleImputer(strategy='mean')\nimp_mean_cat = SimpleImputer(strategy='most_frequent')\n\nfor col in df.columns.to_list():\n    if col in num_cols:\n        df[col] = imp_mean_num.fit_transform(df[[col]])\n    else:\n        df[col] = imp_mean_cat.fit_transform(df[[col]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check previous step\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['RainTomorrow'].abs().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Features****\n* Date-Date of occurance\n* Location-Where does it rain\n* MinTemp-The lowest temperature recorded during the day          \n* MaxTemp-The highest temperature recorded during the day          \n* Rainfall-The numerics of rainfall in scalable format        \n* Evaporation-The Evaporation of water in scalable format      \n* Sunshine-The sunshine chances         \n* WindGustDir-Wind Direction       \n* WindGustSpeed-The speed of wind gust    \n* WindDir9am-Wind direction during 9.00 am      \n* WindDir3pm-Wind direction during 3.00 pm        \n* WindSpeed9am-Wind direction during 9.00 am      \n* WindSpeed3pm-Wind direction during 3.00 pm      \n* Humidity9am-Humidity during 9.00 am      \n* Humidity3pm-Humidity during 3.00 pm       \n* Pressure9am-Air Pressure during 9.00 am      \n* Pressure3pm-Air Pressure during 3.00 pm      \n* Cloud9am-Clouds Visbility during 9.00 am         \n* Cloud3pm-Cloud Visbility during 3.00 pm         \n* Temp9am-Temperture during 9.00 am           \n* Temp3pm-Temperture during 3.00 pm          \n* RainToday-chance of rain during Today         \n* RainTomorrow-chance of rain during tomorrow      "},{"metadata":{},"cell_type":"markdown","source":"**Approaching Categorical Features**\n\nCategorical variables/features are any feature type can be classified into two major types:\n\n* Nominal\n* Ordinal\n\nNominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.\n\nOrdinal variables on the other hand, have “levels” or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important."},{"metadata":{},"cell_type":"markdown","source":"**Feature Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor=df.corr()\nplt.figure(figsize=(20,12))\nsns.heatmap(cor,annot=True,cmap='coolwarm')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Label Encoding** refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.\n![](https://ekababisong.org/assets/seminar_IEEE/LabelEncoder.png)  \n\nWe can do label Encoding From LabelEncoder of scikit-Learn but to do so first we have to impute missing values in data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndf['RainTomorrow'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize LabelEncoder\nlbl_enc = LabelEncoder()\n\n# fit label encoder and transform values on ord_2 column\ndf.loc[:, \"RainTomorrow\"] = lbl_enc.fit_transform(df['RainTomorrow'].values)\n\ndf['RainTomorrow'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit label encoder and transform values on ord_2 column\ndf.loc[:, \"Location\"] = lbl_enc.fit_transform(df['Location'].values)\n\ndf['Location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['WindGustDir'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit label encoder and transform values on ord_2 column\ndf.loc[:, \"WindGustDir\"] = lbl_enc.fit_transform(df['WindGustDir'].values)\n\ndf['WindGustDir'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['WindDir9am'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit label encoder and transform values on ord_2 column\ndf.loc[:, \"WindDir9am\"] = lbl_enc.fit_transform(df['WindDir9am'].values)\n\ndf['WindDir9am'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['WindDir3pm'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit label encoder and transform values on ord_2 column\ndf.loc[:, \"WindDir3pm\"] = lbl_enc.fit_transform(df['WindDir3pm'].values)\n\ndf['WindDir3pm'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['RainToday'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit label encoder and transform values on ord_2 column\ndf.loc[:, \"RainToday\"] = lbl_enc.fit_transform(df['RainToday'].values)\n\ndf['RainToday'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Date'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we split the data into train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop(['RainTomorrow'], axis = 1)\ny = df['RainTomorrow']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion matrix\n\nThe result is telling us that we have 35592+4609 correct predictions and 1775+6026 incorrect predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compute precision, recall, F-measure and support**\n\nTo quote from Scikit Learn:\n\nThe precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n\nThe recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n\nThe F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_test."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**\n\nThe receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}