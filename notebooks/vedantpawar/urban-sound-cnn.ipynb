{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import IPython.display as ipd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\nfrom tensorflow.keras.utils import to_categorical \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DOG_BARK\nfilename = '../input/urbansound8k/fold1/101415-3-0-2.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nprint\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Car_Horn\nfilename = '../input/urbansound8k/fold10/100648-1-1-0.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nmetadata = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\nmetadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metadata.classID.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import struct\n\nclass WavFileHelper():\n    \n    def read_file_properties(self, filename):\n\n        wave_file = open(filename,\"rb\")\n        \n        riff = wave_file.read(12)\n        fmt = wave_file.read(36)\n        \n        num_channels_string = fmt[10:12]\n        num_channels = struct.unpack('<H', num_channels_string)[0]\n\n        sample_rate_string = fmt[12:16]\n        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n        \n        bit_depth_string = fmt[22:24]\n        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n\n        return (num_channels, sample_rate, bit_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load various imports \n\nimport pandas as pd\nimport os\nimport librosa\nimport librosa.display\n\n#from helpers.wavfilehelper import WavFileHelper\n\nwavfilehelper = WavFileHelper()\n\naudiodata = []\nfor index, row in metadata.iterrows():\n    \n    file_name = os.path.join(os.path.abspath('../input/urbansound8k/'),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n    data = wavfilehelper.read_file_properties(file_name)\n    audiodata.append(data)\n\n# Convert into a Panda dataframe\naudiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information about num of channels \n\nprint(audiodf.num_channels.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information about sample rates \n\nprint(audiodf.sample_rate.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information about bit depth\n\nprint(audiodf.bit_depth.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sample rate conversion to same value for all the audios \n\nimport librosa \nfrom scipy.io import wavfile as wav\nimport numpy as np\n\nfilename = '../input/urbansound8k/fold1/101415-3-0-2.wav'\n\nlibrosa_audio, librosa_sample_rate = librosa.load(filename) \nscipy_sample_rate, scipy_audio = wav.read(filename) \n\nprint('Original sample rate:', scipy_sample_rate) \nprint('Librosa sample rate:', librosa_sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bit_Depth_Normalisation \nprint('Original audio file min~max range:', np.min(scipy_audio), 'to', np.max(scipy_audio))\nprint('Librosa audio file min~max range:', np.min(librosa_audio), 'to', np.max(librosa_audio))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No need for concversion to mono channel as it is already single channel signal\nimport matplotlib.pyplot as plt\n\n# Original audio with 1 channels \nplt.figure(figsize=(12, 4))\nplt.plot(scipy_audio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 40 MFCC values are collected over 173 frames\nmfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc=40)\nprint(mfccs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import librosa.display\nlibrosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_pad_len = 174\n\ndef extract_features(file_name):\n   \n    try:\n        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n        pad_width = max_pad_len - mfccs.shape[1]\n        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n        \n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file_name)\n        return None \n     \n    return mfccs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load various imports \nimport pandas as pd\nimport os\nimport librosa\n\n# Set the path to the full UrbanSound dataset \nfulldatasetpath = '../input/urbansound8k/'\n\nmetadata = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\n\nfeatures = []\n\n# Iterate through each sound file and extract the features \nfor index, row in metadata.iterrows():\n    \n    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n    #print(file_name)\n    #print(file_name)\n    \n    class_label = row[\"classID\"]\n    data = extract_features(file_name)\n  #  print(len(data))\n    \n    features.append([data ,class_label])\n\n# Convert into a Panda dataframe \nfeaturesdf = pd.DataFrame(features, columns=['feature','class_label'])\n\nprint('Finished feature extraction from ', len(featuresdf) , ' files' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#All the 8732 files are converted into the 40 mfcc's at 174 frames \nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\n# Convert features and corresponding classification labels into numpy arrays\nX = np.array(featuresdf.feature.tolist())\ny = np.array(featuresdf.class_label.tolist())\n\nprint(len(X))\nprint(X.shape)\n\n# Encode the classification labels\nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y)) \n\n# split the dataset \nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate/extract Log-MEL Spectrogram coefficients with LibRosa in case we want to experiment \ndef get_mel_spectrogram(file_path, mfcc_max_padding=0, n_fft=2048, hop_length=512, n_mels=128):\n    try:\n        # Load audio file\n        y, sr = librosa.load(file_path)\n\n        # Normalize audio data between -1 and 1\n        normalized_y = librosa.util.normalize(y)\n\n        # Generate mel scaled filterbanks\n        mel = librosa.feature.melspectrogram(normalized_y, sr=sr, n_mels=n_mels)\n\n        # Convert sound intensity to log amplitude:\n        mel_db = librosa.amplitude_to_db(abs(mel))\n\n        # Normalize between -1 and 1\n        normalized_mel = librosa.util.normalize(mel_db)\n\n        # Should we require padding\n        shape = normalized_mel.shape[1]\n        if (mfcc_max_padding > 0 & shape < mfcc_max_padding):\n            xDiff = mfcc_max_padding - shape\n            xLeft = xDiff//2\n            xRight = xDiff-xLeft\n            normalized_mel = np.pad(normalized_mel, pad_width=((0,0), (xLeft, xRight)), mode='constant')\n\n    except Exception as e:\n        print(\"Error parsing wavefile: \", e)\n        return None \n    return normalized_mel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train1 = x_train \nx_test1 = x_test\ny_train1 = y_train\ny_test1 = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as keras_backend\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Flatten, LeakyReLU, SpatialDropout2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils, to_categorical, plot_model\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.regularizers import l2\nnum_rows = 40\nnum_columns = 174\nnum_channels = 1\nnum_labels = y_train.shape[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(spatial_dropout_rate_1=0, spatial_dropout_rate_2=0, l2_rate=0):\n\n    # Create a secquential object\n    model = Sequential()\n\n\n    # Conv 1\n    model.add(Conv2D(filters=32, \n                     kernel_size=(3, 3), \n                     kernel_regularizer=l2(l2_rate), \n                     input_shape=(num_rows, num_columns, num_channels)))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(BatchNormalization())\n\n    model.add(SpatialDropout2D(spatial_dropout_rate_1))\n    model.add(Conv2D(filters=32, \n                     kernel_size=(3, 3), \n                     kernel_regularizer=l2(l2_rate)))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(BatchNormalization())\n\n\n    # Max Pooling #1\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(SpatialDropout2D(spatial_dropout_rate_1))\n    model.add(Conv2D(filters=64, \n                     kernel_size=(3, 3), \n                     kernel_regularizer=l2(l2_rate)))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(BatchNormalization())\n    model.add(SpatialDropout2D(spatial_dropout_rate_2))\n    model.add(Conv2D(filters=64, \n                     kernel_size=(3,3), \n                     kernel_regularizer=l2(l2_rate)))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(BatchNormalization())\n\n\n    # Reduces each h×w feature map to a single number by taking the average of all h,w values.\n    model.add(GlobalAveragePooling2D())\n\n    # Softmax output\n    model.add(Dense(num_labels, activation='softmax'))\n    \n    return model\n\n# Regularization rates\nspatial_dropout_rate_1 = 0.07\nspatial_dropout_rate_2 = 0.14\nl2_rate = 0.001\n\nmodel = create_model(spatial_dropout_rate_1, spatial_dropout_rate_2, l2_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adam = Adam(lr=1e-4, beta_1=0.99, beta_2=0.999)\nmodel.compile(\n    loss='categorical_crossentropy', \n    metrics=['accuracy'], \n    optimizer=adam)\n\n# Model architecture summary \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_path = os.path.abspath('./models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#you need to tell Conv2D that there is only 1 feature map, and add an extra dimension to the input vector\nx_train = x_train.reshape(x_train.shape + (1,))\nprint(x_train.shape)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adam = Adam(lr=1e-4, beta_1=0.99, beta_2=0.999)\n#model.compile(\n #   loss='categorical_crossentropy', \n  #  metrics=['accuracy'], \n   # optimizer=adam)\ndef get_apply_grad_fn():\n    @tf.function\n    def apply_grad(X, Y, model, loss_fn, optimizer):\n        with tf.GradientTape() as t:\n\n            output = model(X)\n\n            loss = loss_fn(Y, output)\n\n        grads = t.gradient(loss, model.trainable_weights)\n\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n        return loss\n    return apply_grad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom sklearn import metrics \n\nnum_rows = 40\nnum_columns = 174\nnum_channels = 1\n\nx_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\nx_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\nprint(x_train.shape)\n\nnum_labels = yy.shape[1]\nfilter_size = 2\n\n# Construct model \nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\nmodel.add(GlobalAveragePooling2D())\n\nmodel.add(Dense(num_labels, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model\nmodel.compile(loss = 'categorical_crossentropy', metrics=['accuracy'], optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n\n# Calculate pre-training accuracy \nscore = model.evaluate(x_test, y_test, verbose=1)\naccuracy = 100*score[1]\n\nprint(\"Pre-training accuracy: %.4f%%\" % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint \nfrom datetime import datetime \n\n#num_epochs = 12\n#num_batch_size = 128\n\nnum_epochs = 100\nnum_batch_size = 256\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n                               verbose=1, save_best_only=True)\nstart = datetime.now()\n\nmodel.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data = (x_test, y_test), callbacks=[checkpointer], verbose=1)\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the model on the training and testing set\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: \", score[1])\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Redefining prediction function\ndef print_prediction(file_name):\n\n    prediction_feature = extract_features(file_name) \n    \n    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n\n    predicted_vector = model.predict_classes(prediction_feature)\n    predicted_class = le.inverse_transform(predicted_vector) \n#    print(\"The predicted class is:\", predicted_class[0], '\\n') \n\n    predicted_proba_vector = model.predict_proba(prediction_feature) \n    predicted_proba = predicted_proba_vector[0]\n    for i in range(len(predicted_proba)): \n        category = le.inverse_transform(np.array([i]))\n        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sciffer Wav File splitting \n#Audio Splitting for better analysis \nfrom scipy.io import wavfile\nimport os\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\n\n# Utility functions\n\ndef windows(signal, window_size, step_size):\n    if type(window_size) is not int:\n        raise AttributeError(\"Window size must be an integer.\")\n    if type(step_size) is not int:\n        raise AttributeError(\"Step size must be an integer.\")\n    for i_start in np.arange(0, len(signal), step_size):\n        i_end = i_start + window_size\n        if i_end >= len(signal):\n            break\n        yield signal[i_start:i_end]\n\ndef energy(samples):\n    return np.sum(np.power(samples, 2.)) / float(len(samples))\n\ndef rising_edges(binary_signal):\n    previous_value = 0\n    index = 0\n    for x in binary_signal:\n        if x and not previous_value:\n            yield index\n        previous_value = x\n        index += 1\n\n\n\n\n#args = parser.parse_args()\ninput_filename =  '../input/test-audio/Test_Video.wav' \noutput_dir = '/kaggle/working/'\nstep_duration = None\nmin_silence_length = 0.01\nsilence_threshold = 0.000001\n\n\nwindow_duration = min_silence_length\nif step_duration is None:\n    step_duration = window_duration / 10.\nelse:\n    step_duration = step_duration\nsilence_threshold = silence_threshold\noutput_dir = output_dir\noutput_filename_prefix = os.path.splitext(os.path.basename(input_filename))[0]\ndry_run = 0\n\n\nprint(\"Splitting {} where energy is below {}% for longer than {}s.\".format(\n    input_filename,\n    silence_threshold * 100.,\n    window_duration\n)\n     )\n# Read and split the file\n\nsample_rate, samples = input_data=wavfile.read(filename=input_filename, mmap=True)\n\nmax_amplitude = np.iinfo(samples.dtype).max\nmax_energy = energy([max_amplitude])\n\nwindow_size = int(window_duration * sample_rate)\nstep_size = int(step_duration * sample_rate)\n\nsignal_windows = windows(\n    signal=samples,\n    window_size=window_size,\n    step_size=step_size\n)\n\nwindow_energy = (energy(w) / max_energy for w in tqdm(\n    signal_windows,\n    total=int(len(samples) / float(step_size))\n))\n\nwindow_silence = (e > silence_threshold for e in window_energy)\n\ncut_times = (r * step_duration for r in rising_edges(window_silence))\n\n# This is the step that takes long, since we force the generators to run.\nprint(\"Finding silences...\")\ncut_samples = [int(t * sample_rate) for t in cut_times]\ncut_samples.append(-1)\n\ncut_ranges = [(i, cut_samples[i], cut_samples[i+1]) for i in np.arange(len(cut_samples) - 1)]\n\nfor i, start, stop in tqdm(cut_ranges):\n    output_file_path = \"{}_{:03d}.wav\".format(\n        os.path.join(output_dir, output_filename_prefix),\n        i\n    )\n    if not dry_run:\n        print (\"Writing file {}\".format(output_file_path))\n        wavfile.write(\n            filename = output_file_path,\n            rate=sample_rate,\n            data=samples[start:stop]\n        )\n    else:\n        print(\"Not writing file {}\".format(output_file_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DoG Bark\nfilename = '/kaggle/working/Test_Video_001.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dog Barking\nfilename = '/kaggle/working/Test_Video_001.wav'\nprint_prediction(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Traffic\nfilename = '/kaggle/working/Test_Video_002.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Redefining prediction function\ndef prediction(file_name):\n\n    prediction_feature = extract_features(file_name) \n    \n    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n\n    predicted_vector = model.predict_classes(prediction_feature)\n    predicted_class = le.inverse_transform(predicted_vector) \n#    print(\"The predicted class is:\", predicted_class[0], '\\n') \n\n    predicted_proba_vector = model.predict_proba(prediction_feature) \n    predicted_proba = predicted_proba_vector[0]\n    for i in range(len(predicted_proba)): \n        category = le.inverse_transform(np.array([i]))\n       # print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )\n        return(list(predicted_proba[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pydub import AudioSegment\n\n#Let's do voting strategy to find out probability \n#Gives the most to street music \ncount = 0 \n#List of list to store values of predictions \nval = [[] for i in range(11)]\nfrom pydub import AudioSegment\nt1 = 0 * 500 #Works in milliseconds\nt2 = 1 * 500\nnewAudio = AudioSegment.from_wav('/kaggle/working/Test_Video_002.wav')\nnewAudio = newAudio[t1:t2]\nnewAudio.export('new.wav', format=\"wav\") #Exports to a wav file in the current path.\n\nt1 = 1 * 500 #Works in milliseconds\nt2 = 1 * 1000\nnewAudio = AudioSegment.from_wav('/kaggle/working/Test_Video_002.wav')\nnewAudio = newAudio[t1:t2]\nnewAudio.export('new1.wav', format=\"wav\") #Exports to a wav file in the current path.\n\nt1 = 1 * 1000 #Works in milliseconds\nt2 = 1 * 15000\nnewAudio = AudioSegment.from_wav('/kaggle/working/Test_Video_002.wav')\nnewAudio = newAudio[t1:t2]\nnewAudio.export('new2.wav', format=\"wav\") #Exports to a wav file in the current path.\n\nt1 = 1 * 1500 #Works in milliseconds\nt2 = 1 * 2000\nnewAudio = AudioSegment.from_wav('/kaggle/working/Test_Video_002.wav')\nnewAudio = newAudio[t1:t2]\nnewAudio.export('new3.wav', format=\"wav\") #Exports to a wav file in the current path.\n\nt1 = 1 * 2000 #Works in milliseconds\nt2 = 1 * 2500\nnewAudio = AudioSegment.from_wav('/kaggle/working/Test_Video_002.wav')\nnewAudio = newAudio[t1:t2]\nnewAudio.export('new4.wav', format=\"wav\") #Exports to a wav file in the current path.\n\nt1 = 1 * 2500 #Works in milliseconds\nt2 = 1 * 3000\nnewAudio = AudioSegment.from_wav('/kaggle/working/Test_Video_002.wav')\nnewAudio = newAudio[t1:t2]\nnewAudio.export('new5.wav', format=\"wav\") #Exports to a wav file in the current path.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = '/kaggle/working/new.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val = [[] for i in range(11)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilename = '/kaggle/working/new.wav'\nprint_prediction(filename)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilename = '/kaggle/working/new1.wav'\nprint_prediction(filename)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilename = '/kaggle/working/new2.wav'\nprint_prediction(filename)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilename = '/kaggle/working/new3.wav'\nprint_prediction(filename)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilename = '/kaggle/working/new4.wav'\nprint_prediction(filename)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilename = '/kaggle/working/new5.wav'\nprint_prediction(filename)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Max Mean average prediction : \n#Car_Horn ~ 40% \n#Which is different as compared to original prediction which came out to be Street Noise ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FFT BASED NOISE REDUCTION METHOD\npip install noisereduce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import noisereduce as nr\n# load data\nfilename = (\"/kaggle/working/Test_Video_002.wav\")\ndata,sample_rate = librosa.load(filename)\nprint(len(data))\n# select section of data that is noise\nnoisy_part = data\n#print(data)\n# perform noise reduction\nreduced_noise = nr.reduce_noise(audio_clip=data, noise_clip=noisy_part, verbose=True)\n\n#reduced_noise.export('hope.wav', format=\"wav\") #Exports to a wav file in the current path.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_noise\n_ = librosa.display.waveplot(reduced_noise,sr=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction on this part of the signal gives us --->\nfrom pydub import AudioSegment\nt1 = 1 * 900 #Works in milliseconds\nt2 = 1 * 2000\nnewAudio = AudioSegment.frnewAudio = AudioSegment.from_wav(\"../input/test-audio/Test_Video.wav\")\nnewAudio = newAudio[t1:t2]\nprint(type(newAudio))\nnewAudio.export('significant.wav', format=\"wav\") #Exports to a wav file in the current path.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = '/kaggle/working/significant.wav'\nprint_prediction(filename)\n\n#Here it shows the car horn highest probability ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnp.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = x_train.reshape((6985, 174, 40))\n#y_train.reshape((6985, 174, 40))\n#print(y_train.shape)\n\ny_train = to_categorical(y_train , 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(k.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AUGMENTATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport librosa\nimport librosa.display\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib as plt\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display\nimport librosa.display\nfrom scipy.io import wavfile\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#path to the dataset\n#us8k_path = os.path.abspath('../input/urbansound8k/')\n\nmetadata = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\naudio_path = '../input/urbansound8k/'\naugmented_path = '/kaggle/working/'\n\n# Metadata\n#metadata_path = os.path.join(us8k_path, 'metadata/UrbanSound8K.csv')\n#metadata_augmented_path = os.path.abspath('data/augmented-data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nmetadata = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\nmetadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Time Scaling\nrates = [0.81, 1.07]\ntotal = len(metadata) * len(rates)\ncount = 0\nfor rate in rates: \n    # Generate new stretched audio file\n    for index, row in metadata.iterrows(): \n        fulldatasetpath = '../input/urbansound8k/'\n\n     #   file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n        #print(file_name)\n        #print(file_name)\n    \n      #  class_label = row[\"classID\"]\n     #   data = extract_features(file_name)\n        curr_fold = str(row['fold'])\n        curr_file_path = audio_path + '/fold' + curr_fold + '/' + row['slice_file_name']\n        \n        # Speed sub-dir inside current fold dir\n        curr_rate_path = augmented_path + '/fold' + '/speed_' + str(int(rate*100))\n\n        \n        # Create sub-dir if it does not exist\n        if not os.path.exists(curr_rate_path):\n            os.makedirs(curr_rate_path)\n                    \n        output_path = curr_rate_path + '/' + row['slice_file_name']\n        \n        # Skip when file already exists\n        if (os.path.isfile(output_path)):\n            count += 1 \n            continue\n        \n        y, sr = librosa.load(curr_file_path)  \n        y_changed = librosa.effects.time_stretch(y, rate=rate)\n        print(y_changed)\n       # librosa.output.write_wav(output_path, y_changed, sr)\n        wavfile.write(\n            filename = output_path,\n            rate = sr,\n            data = y_changed\n        )\n        \n        count += 1 \n        \n        clear_output(wait=True)\n        print(\"Progress: {}/{}\".format(count, total))\n        print(\"Last file: \", row['slice_file_name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pitch_Shifting \ntone_steps = [-1, -2, 1, 2]\ntotal = len(metadata) * len(tone_steps)\ncount = 0\nfor tone_step in tone_steps:\n    # Generate new pitched audio\n    for index, row in metadata.iterrows():        \n        curr_fold = str(row['fold'])\n        curr_file_path = audio_path + '/fold' + curr_fold + '/' + row['slice_file_name']\n\n        # Pitch Shift sub-dir inside current fold dir\n        curr_ps_path = augmented_path + '/fold' + curr_fold + '/pitch_' + str(tone_step)\n\n        # Create sub-dir if it does not exist\n        if not os.path.exists(curr_ps_path):\n            os.makedirs(curr_ps_path)\n        \n        output_path = curr_ps_path + '/' + row['slice_file_name']\n        \n        # Skip when file already exists\n        if (os.path.isfile(output_path)):\n            count += 1 \n            continue\n        \n        y, sr = librosa.load(curr_file_path)  \n        y_changed = librosa.effects.pitch_shift(y, sr, n_steps=tone_step)\n        librosa.output.write_wav(output_path, y_changed, sr)\n        \n        count += 1 \n        \n        clear_output(wait=True)\n        print(\"Progress: {}/{}\".format(count, total))\n        print(\"Last file: \", row['slice_file_name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Noise addition \nimport random\n\ndef add_noise(data):\n    noise = np.random.rand(len(data))\n    noise_amp = random.uniform(0.005, 0.008)\n    data_noise = data + (noise_amp * noise)\n    return data_noise\n\ntotal = len(metadata)\ncount = 0\n\n# Generate new noised audio\nfor index, row in metadata.iterrows():        \n    curr_fold = str(row['fold'])\n    curr_file_path = audio_path + '/fold' + curr_fold + '/' + row['slice_file_name']\n    \n    # Noised sub-dir inside current fold dir\n    curr_noise_path = augmented_path + '/fold' + curr_fold + '/noise'\n\n    # Create sub-dir if it does not exist\n    if not os.path.exists(curr_noise_path):\n        os.makedirs(curr_noise_path)\n        \n    output_path = curr_noise_path + '/' + row['slice_file_name']\n        \n    # Skip when file already exists\n    if (os.path.isfile(output_path)):\n        count += 1 \n        continue\n        \n    y, sr = librosa.load(curr_file_path)  \n    y_changed = add_noise(y)\n    librosa.output.write_wav(output_path, y_changed, sr)\n    \n    count += 1 \n\n    clear_output(wait=True)\n    print(\"Progress: {}/{}\".format(count, total))\n    print(\"Last file: \", row['slice_file_name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_files_recursive(path):\n    # create a list of file and sub directories names in the given directory \n    file_list = os.listdir(path)\n    all_files = list()\n    # Iterate over all the entries\n    for entry in file_list:\n        # Create full path\n        full_path = os.path.join(path, entry)\n        # If entry is a directory then get the list of files in this directory \n        if os.path.isdir(full_path):\n            all_files = all_files + get_files_recursive(full_path)\n        else:\n            all_files.append(full_path)\n                \n    return all_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get every single file within the tree\nfiles = get_files_recursive(augmented_path)\n\n# Define metadata columns\nnames = []\nclasses = []\nfolds = []\naugmentations = []\n\n# Iterate and collect name, fold and class\nfor file in files:\n    pieces = file.split(\"/\")\n    file = pieces[len(pieces) - 1]\n    fold = pieces[len(pieces) - 3] \n    augment = pieces[len(pieces) - 2] \n    fold_num = fold[4:len(fold)]\n    class_id = file.split(\"-\")[1]\n\n    # Push records\n    names.append(file)\n    folds.append(fold_num)\n    classes.append(class_id)\n    augmentations.append(augment)\n\n# Create a dataframe with the new augmented data\nnew_meta = pd.DataFrame({'file': names, 'fold': folds, 'class_id': classes, 'augment': augmentations })\n\n# Make sure class_id is int\nnew_meta['class_id'] = new_meta['class_id'].astype(np.int64)\n\nprint(len(new_meta), \"new entries\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = pd.DataFrame({\n    'class_id': range(0,10),\n    'class': [\n        'air_conditioner',\n        'car_horn',\n        'children_playing',\n        'dog_bark',\n        'drilling',\n        'engine_idling',\n        'gun_shot',\n        'jackhammer',\n        'siren',\n        'street_music'\n    ]\n})\n\nnew_meta = pd.merge(new_meta, classes, on='class_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_meta.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modify original data to fit the new structure\ndel metadata['fsID'], metadata['start'], metadata['end'], metadata['salience']\nmetadata.columns = ['file', 'fold', 'class_id', 'class']\nmetadata['augment'] = 'none'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concat the two dataframes\nfull_meta = pd.concat([metadata, new_meta])\n\n# Verify lengths\nif (len(full_meta) == len(metadata) + len(new_meta)):\n    print(\"Dataframes merged correctly!\")\nelse:\n    print(\"Error! Lengths do not match.\")\n\nprint(\"Initial data:\", len(metadata))\nprint(\"New data:\", len(new_meta))\nprint(\"Merged data:\", len(full_meta))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the new metadata\nfull_meta.to_csv(metadata_augmented_path, index=False, encoding=\"utf-8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Couldn't perform Augmentation cause of memory allocation got full in here. Might try the same in Google Collab in the future ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}