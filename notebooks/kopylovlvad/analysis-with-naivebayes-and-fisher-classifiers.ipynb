{"cells":[{"metadata":{"trusted":true,"_uuid":"db9137670e7bd8eb86830ee9039478592e873df6"},"cell_type":"markdown","source":"It is my try to write spam detector with Naive Bayes and Fisher classifiers. Here is a link to [GitHub repo](https://github.com/kopylovvlad/text_classifier_kv) with source-code of implementation Naive Bayes and Fisher classifiers.\n\nThe main goal is set up classifiers for maximum accuracy. The classifier should filter spam correct and has minimum mistakes with 'ham' category.\n\nFile spam.csv has 5Â 572 messages. For each experiment, I will divide data from the file to 2 subsets: train subset with 3000 messages and test subset with 2572 messages. Let's overview how many 'ham' and 'spam' categories in every subset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import text_classifier_kv as tc # package with naivebayes and fisher classifiers\nfrom typing import List, Dict, Tuple\nimport csv\nimport os\nprint(os.listdir(\"../input\"))\n# print(text_classifier_kv.fisherclassifier())\n# Any results you write to the current directory are saved as output.\n\nrow_limit: int = 3000\n# (category, text)\ntrain_data: List[Tuple[str, str]] = []\ntrain_data_ham: int = 0\ntrain_data_spam: int = 0\ntest_data: List[Tuple[str, str]] = []\ntest_data_ham: int = 0\ntest_data_spam: int = 0\nreader = csv.reader(open('../input/spam.csv', newline='', encoding='latin-1'))\ni: int = 0\nfor row in reader:\n    i += 1\n    if i == 1:\n        continue\n    if i < row_limit + 2:\n        train_data.append((row[0], row[1]))\n        if row[0] == 'ham':\n            train_data_ham += 1\n        else:\n            train_data_spam += 1\n    else:\n        test_data.append((row[0], row[1]))\n        if row[0] == 'ham':\n            test_data_ham += 1\n        else:\n            test_data_spam += 1\n\nprint('Data overview:')\nprint('Train data size: %d' % len(train_data))\nprint('Train data ham: %d' % train_data_ham)\nprint('Train data spam: %d' % train_data_spam)\nprint('')\nprint('Test data size: %d' % len(test_data))\nprint('Test data ham: %d' % test_data_ham)\nprint('Test data spam: %d' % test_data_spam)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false,"_kg_hide-output":true},"cell_type":"markdown","source":"Train subset has 2591 messages with 'ham' category and 409 messages with 'spam' category. The ratio is 13:2,4.\nTest subset has 2234 messages with 'ham' category and 338 messages with 'spam' category. The ratio is 11,1:1,7."},{"metadata":{"_uuid":"2385e9b8e9e1e938eb628a3dd3e6f615ddc621ac"},"cell_type":"markdown","source":"Experiment #1: run train and test with Naive Bayes and Fisher classifiers."},{"metadata":{"trusted":true,"_uuid":"2530028247dedaad23759a2b7b7f4c3a7b8b41d5"},"cell_type":"code","source":"import text_classifier_kv as tc # package with naivebayes and fisher classifiers\nfrom typing import List, Dict, Tuple\nimport csv\nimport os\n# Any results you write to the current directory are saved as output.\n\nrow_limit: int = 3000\n# (category, text)\ntrain_data: List[Tuple[str, str]] = []\ntest_data: List[Tuple[str, str]] = []\nreader = csv.reader(open('../input/spam.csv', newline='', encoding='latin-1'))\ni: int = 0\nfor row in reader:\n    i += 1\n    if i == 1:\n        continue\n    if i < row_limit + 2:\n        train_data.append((row[0], row[1]))\n    else:\n        test_data.append((row[0], row[1]))\n\n\ndef experiment(\n    classifier,\n    train_data: List[Tuple[str, str]],\n    test_data: List[Tuple[str, str]],\n    show_not_equal_ham: bool = False,\n    show_not_equal_spam: bool = False\n) -> None:\n    # main function train, test and print result\n    [classifier.train(text, cat) for (cat, text) in train_data]\n    stat: Dict[str, int] = {\n        'equal': 0,\n        'not_equal': 0,\n        'not_equal_ham': 0,\n        'not_equal_spam': 0\n    }\n    for (cat, text) in test_data:\n        predict = classifier.classify(text)\n        if cat == predict:\n            stat['equal'] += 1\n        else:\n            stat['not_equal'] += 1\n            if cat == 'ham':\n                stat['not_equal_ham'] += 1\n                if show_not_equal_ham == True:\n                    print(text)\n            else:\n                stat['not_equal_spam'] += 1\n                if show_not_equal_spam == True:\n                    print(text)\n    print(stat)\n    ac: float = stat['equal'] / \\\n        ((stat['equal']+stat['not_equal'])/100)\n    print('Accuracy: %f' % ac)\n    return None\n\n\nprint('Experiment #1.1 - naivebayes')\ncl = tc.naivebayes()\nexperiment(cl, train_data, test_data)\n\nprint('Experiment #1.2 - fisherclassifier')\ncl = tc.fisherclassifier()\nexperiment(cl, train_data, test_data)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e34415b6d5822f2427228c6dddeca947c77b976"},"cell_type":"markdown","source":"**Results:**\nNaive Bayes classifier has accuracy 94.86%. Fisher classifier has accuracy 95.37%. \nAccuracy is great, but each classifier has mistakes with detecting 'ham' category. Test subset has 2234 messages with 'ham' category.\nThat means Naive Bayes classifier has 5% mistakes with 'ham' category. And Fisher classifier has 4.7% mistakes with 'ham' category.\nWith 'spam' category, Naive Bayes classifier has 5,6% mistakes and Fisher classifier has 4.1% mistakes.\n\nHow to increase it? We have two ways:\n1.  Change algorithm for .train-method\n2. Customise .classify-method by set up thresholds for each category\n\n**Change algorithm for .train-method**\nBy-default .train method take a text-message. Transform text to Dict of unique words. Increase values in Dict of features and Dict of categories. We can try to increase size of Dict of unique words by ignoring common words. We can take all messages in train subset and use only words with frequency bigger than 10% and less than 50%.\n\n**Customize .classify-method by set up thresholds for each category**\nFor Fisher classifier, we can set up minimum thresholds for 'spam' category. Other words, you can claim it as 'spam' only if you really sure."},{"metadata":{"_uuid":"b225b6ca1c36e3718add0d720a66754bbdec9b47"},"cell_type":"markdown","source":"Experiment #2: run Naive Bayes and Fisher classifiers with ignoring commin words."},{"metadata":{"trusted":true,"_uuid":"17152f9f9d0d4d68ce02916f951f147e8d0b52f2"},"cell_type":"code","source":"import text_classifier_kv as tc # package with naivebayes and fisher classifiers\nfrom typing import List, Dict, Tuple, Callable\nimport csv\nimport os\n# Any results you write to the current directory are saved as output.\n\nrow_limit: int = 3000\n# (category, text)\ntrain_data: List[Tuple[str, str]] = []\ntest_data: List[Tuple[str, str]] = []\nreader = csv.reader(open('../input/spam.csv', newline='', encoding='latin-1'))\ni: int = 0\nfor row in reader:\n    i += 1\n    if i == 1:\n        continue\n    if i < row_limit + 2:\n        train_data.append((row[0], row[1]))\n    else:\n        test_data.append((row[0], row[1]))\n\n        \ndef generate_text_vector(\n    text_list: List[str],\n    min_frequency: float=0.1,\n    max_frequency: float=0.5,\n    getfeatures: Callable[[str], Dict[str, int]] = tc.getwords,\n) -> Callable[[str], Dict[str, int]]:\n    '''\n    Get a Dict of words from array of text between max and mix frequency\n    '''\n    text_list_len: int = len(text_list)\n    text_vector: List[str] = []\n    # Dict of uniq words for each text\n    apcount: Dict[str, int] = {}\n\n    for text in text_list:\n        for word, count in getfeatures(text).items():\n            apcount.setdefault(word, 0)\n            if count > 0:\n                apcount[word] += 1\n\n    # Dict of uniq words for all texts\n    # all words are between max and mix frequency\n    fr_set: List[float] = []\n    for word, count in apcount.items():\n        frac = float(float(count) / float(text_list_len))\n        fr_set.append(frac)\n        if frac > min_frequency and frac < max_frequency:\n            text_vector.append(word)\n\n    def get_match_words(text: str) -> Dict[str, int]:\n        new_dict: Dict[str, int] = {}\n        for text, i in getfeatures(text).items():\n            if text in text_vector:\n                new_dict[text] = i\n\n        return new_dict\n    return get_match_words\n\n\ndef experiment(\n    classifier,\n    train_data: List[Tuple[str, str]],\n    test_data: List[Tuple[str, str]],\n    show_not_equal_ham: bool = False,\n    show_not_equal_spam: bool = False\n) -> None:\n    # main function train, test and print result\n    [classifier.train(text, cat) for (cat, text) in train_data]\n    stat: Dict[str, int] = {\n        'equal': 0,\n        'not_equal': 0,\n        'not_equal_ham': 0,\n        'not_equal_spam': 0\n    }\n    for (cat, text) in test_data:\n        predict = classifier.classify(text)\n        if cat == predict:\n            stat['equal'] += 1\n        else:\n            stat['not_equal'] += 1\n            if cat == 'ham':\n                stat['not_equal_ham'] += 1\n                if show_not_equal_ham == True:\n                    print(text)\n            else:\n                stat['not_equal_spam'] += 1\n                if show_not_equal_spam == True:\n                    print(text)\n    print(stat)\n    ac: float = stat['equal'] / \\\n        ((stat['equal']+stat['not_equal'])/100)\n    print('Accuracy: %f' % ac)\n    return None\n\ntrain_texts: List[str] = [text for _c, text in train_data]\ngetweatures_ignoring_common_words: Callable[[str], Dict[str, int]] \ngetweatures_ignoring_common_words = generate_text_vector(\n    train_texts, \n    min_frequency=0.0003,\n    max_frequency=0.05\n)\nprint('Experiment #2.1 - naivebayes with words frequency limit')\ncl = tc.naivebayes(getfeatures=getweatures_ignoring_common_words)\nexperiment(cl, train_data, test_data)\n\nprint('Experiment #2.2 - fisherclassifier with words frequency limit')\ncl = tc.fisherclassifier(getfeatures=getweatures_ignoring_common_words)\nexperiment(cl, train_data, test_data)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44a2cd24b932abdcd607f5a11fb8c8546263aab2"},"cell_type":"markdown","source":"**Result:**\n\nResults are little better, but not excellent. Let's try with thresholds. Unfortunately, it is not useful for Naive Bayes: we will try with only Fisher Classifier."},{"metadata":{"trusted":true,"_uuid":"2a1b980025b0b79add1b60cbbae06bd0e1587209"},"cell_type":"code","source":"import text_classifier_kv as tc # package with naivebayes and fisher classifiers\nfrom typing import List, Dict, Tuple, Callable\nimport csv\nimport os\n# Any results you write to the current directory are saved as output.\n\nrow_limit: int = 3000\n# (category, text)\ntrain_data: List[Tuple[str, str]] = []\ntest_data: List[Tuple[str, str]] = []\nreader = csv.reader(open('../input/spam.csv', newline='', encoding='latin-1'))\ni: int = 0\nfor row in reader:\n    i += 1\n    if i == 1:\n        continue\n    if i < row_limit + 2:\n        train_data.append((row[0], row[1]))\n    else:\n        test_data.append((row[0], row[1]))\n\n        \ndef generate_text_vector(\n    text_list: List[str],\n    min_frequency: float=0.1,\n    max_frequency: float=0.5,\n    getfeatures: Callable[[str], Dict[str, int]] = tc.getwords,\n) -> Callable[[str], Dict[str, int]]:\n    '''\n    Get a Dict of words from array of text between max and mix frequency\n    '''\n    text_list_len: int = len(text_list)\n    text_vector: List[str] = []\n    # Dict of uniq words for each text\n    apcount: Dict[str, int] = {}\n\n    for text in text_list:\n        for word, count in getfeatures(text).items():\n            apcount.setdefault(word, 0)\n            if count > 0:\n                apcount[word] += 1\n\n    # Dict of uniq words for all texts\n    # all words are between max and mix frequency\n    fr_set: List[float] = []\n    for word, count in apcount.items():\n        frac = float(float(count) / float(text_list_len))\n        fr_set.append(frac)\n        if frac > min_frequency and frac < max_frequency:\n            text_vector.append(word)\n\n    def get_match_words(text: str) -> Dict[str, int]:\n        new_dict: Dict[str, int] = {}\n        for text, i in getfeatures(text).items():\n            if text in text_vector:\n                new_dict[text] = i\n\n        return new_dict\n    return get_match_words\n\n\ndef experiment(\n    classifier,\n    train_data: List[Tuple[str, str]],\n    test_data: List[Tuple[str, str]],\n    show_not_equal_ham: bool = False,\n    show_not_equal_spam: bool = False\n) -> None:\n    # main function train, test and print result\n    [classifier.train(text, cat) for (cat, text) in train_data]\n    stat: Dict[str, int] = {\n        'equal': 0,\n        'not_equal': 0,\n        'not_equal_ham': 0,\n        'not_equal_spam': 0\n    }\n    for (cat, text) in test_data:\n        predict = classifier.classify(text)\n        if cat == predict:\n            stat['equal'] += 1\n        else:\n            stat['not_equal'] += 1\n            if cat == 'ham':\n                stat['not_equal_ham'] += 1\n                if show_not_equal_ham == True:\n                    print(text)\n            else:\n                stat['not_equal_spam'] += 1\n                if show_not_equal_spam == True:\n                    print(text)\n    print(stat)\n    ac: float = stat['equal'] / \\\n        ((stat['equal']+stat['not_equal'])/100)\n    print('Accuracy: %f' % ac)\n    return None\n\ntrain_texts: List[str] = [text for _c, text in train_data]\ngetweatures_ignoring_common_words: Callable[[str], Dict[str, int]] \ngetweatures_ignoring_common_words = generate_text_vector(\n    train_texts, \n    min_frequency=0.0003,\n    max_frequency=0.05\n)\n\n\nprint('Experiment #3.1 - fisherclassifier with word frequency limit and cat-minimum')\ncl = tc.fisherclassifier(getfeatures=getweatures_ignoring_common_words)\ncl.setminimum('spam', 0.929)\nexperiment(cl, train_data, test_data)\n\nprint('Experiment #3.2 - fisherclassifier with cat-minimum')\ncl = tc.fisherclassifier()\ncl.setminimum('spam', 0.949)\nexperiment(cl, train_data, test_data)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a65d961667a98ad48ae2745f3b1e18b295f5461a"},"cell_type":"markdown","source":"Results are better.\nFisher Classifier with word frequency limit and cat-minimum has 97.51% accuracy.\nThere are 6 mistakes with 'ham' category and 58 mistakes with 'spam'. It has 0,26% mistakes with 'ham' and 17,15% with 'spam'.\n\nFisher Classifier with cat-minimum has 98.09% accuracy.\nThere are 1 mistake with 'ham' category and 48 mistakes with 'spam'. It has 0,04% mistakes with 'ham' and 14,2% with 'spam'."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}