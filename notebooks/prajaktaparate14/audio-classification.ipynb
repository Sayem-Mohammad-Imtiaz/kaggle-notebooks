{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T15:51:04.859262Z","iopub.execute_input":"2021-06-08T15:51:04.859674Z","iopub.status.idle":"2021-06-08T15:51:10.217389Z","shell.execute_reply.started":"2021-06-08T15:51:04.859593Z","shell.execute_reply":"2021-06-08T15:51:10.19924Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Audio Classification\nObjective : Use a simple Neural Network to classify audio samples in their category based on features extracted using LIBROSA.","metadata":{}},{"cell_type":"code","source":"## Import required libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\n'''Librosa is a special library used for audio analysis'''\nimport librosa\nimport librosa.display","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:51:15.187747Z","iopub.execute_input":"2021-06-08T15:51:15.188071Z","iopub.status.idle":"2021-06-08T15:51:17.113858Z","shell.execute_reply.started":"2021-06-08T15:51:15.18804Z","shell.execute_reply":"2021-06-08T15:51:17.1129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Understanding and Exploration","metadata":{}},{"cell_type":"code","source":"#let's take a random audio file from the data.\naudio_file_path='../input/urbansound8k/fold1/101415-3-0-2.wav'\n\n#let's view the waveplot \nplt.figure(figsize=(14,3))\ny, sr = librosa.load(audio_file_path)\nlibrosa.display.waveplot(y, sr = sr)\nipd.Audio(audio_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:51:23.565447Z","iopub.execute_input":"2021-06-08T15:51:23.565777Z","iopub.status.idle":"2021-06-08T15:51:25.032889Z","shell.execute_reply.started":"2021-06-08T15:51:23.565747Z","shell.execute_reply":"2021-06-08T15:51:25.032045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is a sound of dog barking.","metadata":{}},{"cell_type":"code","source":"print('Time series data :- ',y)\nprint('Sample rate :- ',sr)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:51:40.843694Z","iopub.execute_input":"2021-06-08T15:51:40.844024Z","iopub.status.idle":"2021-06-08T15:51:40.850332Z","shell.execute_reply.started":"2021-06-08T15:51:40.843995Z","shell.execute_reply":"2021-06-08T15:51:40.849215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A `time series` is a series of data points indexed in time order.\nHere, time series of an audio signal represented as a one-dimensional numpy.ndarray of floating-point values. y[t] corresponds to amplitude of the waveform at sample t.\n\n\nA `sample rate` or sampling rate defines how many times per second a sound is sampled. The default sampling rate used by Librosa is 22050","metadata":{}},{"cell_type":"code","source":"# Load the meta data\nmetadata = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\nmetadata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:51:50.747442Z","iopub.execute_input":"2021-06-08T15:51:50.747763Z","iopub.status.idle":"2021-06-08T15:51:50.794115Z","shell.execute_reply.started":"2021-06-08T15:51:50.747734Z","shell.execute_reply":"2021-06-08T15:51:50.793406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check whether the dataset is imbalanced\nmetadata['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:51:54.102532Z","iopub.execute_input":"2021-06-08T15:51:54.102848Z","iopub.status.idle":"2021-06-08T15:51:54.113575Z","shell.execute_reply.started":"2021-06-08T15:51:54.102819Z","shell.execute_reply":"2021-06-08T15:51:54.112784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"##### Feature Extration\n\nLet's extract the Mel-frequency cepstral coefficients from the raw signal y","metadata":{}},{"cell_type":"code","source":"mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\nprint(mfccs.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:42:34.950477Z","iopub.execute_input":"2021-06-08T16:42:34.950982Z","iopub.status.idle":"2021-06-08T16:42:34.981188Z","shell.execute_reply.started":"2021-06-08T16:42:34.950929Z","shell.execute_reply":"2021-06-08T16:42:34.979856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output of librosa.feature.mfcc function is the matrix, which is a numpy.ndarray of shape (n_mfcc, T) where T denotes the track duration in frames. ","metadata":{}},{"cell_type":"code","source":"# Extracting MFCC's for every audio file\nimport pandas as pd\nimport os\nimport librosa\n\naudio_dataset_path = '../input/urbansound8k'\nmetadata = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\nmetadata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:46:30.15433Z","iopub.execute_input":"2021-06-08T16:46:30.154689Z","iopub.status.idle":"2021-06-08T16:46:30.186835Z","shell.execute_reply.started":"2021-06-08T16:46:30.154658Z","shell.execute_reply":"2021-06-08T16:46:30.186099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def features_extractor(file):\n    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n    return mfccs_scaled_features","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:46:31.996808Z","iopub.execute_input":"2021-06-08T16:46:31.997117Z","iopub.status.idle":"2021-06-08T16:46:32.004116Z","shell.execute_reply.started":"2021-06-08T16:46:31.997087Z","shell.execute_reply":"2021-06-08T16:46:32.003251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n## Now we iterate through every audio file and extract features\n## using MeL-Frequency cepstral Coefficients\nextracted_features=[]\nfor index_num, row in tqdm(metadata.iterrows()):\n    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/', str(row['slice_file_name']))\n    final_class_labels = row['class']\n    data = features_extractor(file_name)\n    extracted_features.append([data,final_class_labels])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:46:47.095369Z","iopub.execute_input":"2021-06-08T16:46:47.095676Z","iopub.status.idle":"2021-06-08T17:02:10.764709Z","shell.execute_reply.started":"2021-06-08T16:46:47.095648Z","shell.execute_reply":"2021-06-08T17:02:10.763823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Converting extracted_features to pandas dataframe\nextracted_features_df = pd.DataFrame(extracted_features, columns=['feature','class'])\nextracted_features_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:32.480718Z","iopub.execute_input":"2021-06-08T17:04:32.48113Z","iopub.status.idle":"2021-06-08T17:04:32.511301Z","shell.execute_reply.started":"2021-06-08T17:04:32.481095Z","shell.execute_reply":"2021-06-08T17:04:32.510442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the dataset into independent and dependent dataset\nX=np.array(extracted_features_df['feature'].tolist())\ny= np.array(extracted_features_df['class'].tolist())","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:39.202704Z","iopub.execute_input":"2021-06-08T17:04:39.203023Z","iopub.status.idle":"2021-06-08T17:04:39.215863Z","shell.execute_reply.started":"2021-06-08T17:04:39.202992Z","shell.execute_reply":"2021-06-08T17:04:39.214866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:42.251792Z","iopub.execute_input":"2021-06-08T17:04:42.252117Z","iopub.status.idle":"2021-06-08T17:04:42.257613Z","shell.execute_reply.started":"2021-06-08T17:04:42.252086Z","shell.execute_reply":"2021-06-08T17:04:42.256776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Label Encoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ny = to_categorical(labelencoder.fit_transform(y))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:43.45661Z","iopub.execute_input":"2021-06-08T17:04:43.456956Z","iopub.status.idle":"2021-06-08T17:04:47.849771Z","shell.execute_reply.started":"2021-06-08T17:04:43.456927Z","shell.execute_reply":"2021-06-08T17:04:47.848928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:47.851393Z","iopub.execute_input":"2021-06-08T17:04:47.851725Z","iopub.status.idle":"2021-06-08T17:04:47.859298Z","shell.execute_reply.started":"2021-06-08T17:04:47.85169Z","shell.execute_reply":"2021-06-08T17:04:47.858411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:47.860874Z","iopub.execute_input":"2021-06-08T17:04:47.861386Z","iopub.status.idle":"2021-06-08T17:04:47.869716Z","shell.execute_reply.started":"2021-06-08T17:04:47.861326Z","shell.execute_reply":"2021-06-08T17:04:47.868942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:51.051205Z","iopub.execute_input":"2021-06-08T17:04:51.05156Z","iopub.status.idle":"2021-06-08T17:04:51.059969Z","shell.execute_reply.started":"2021-06-08T17:04:51.05153Z","shell.execute_reply":"2021-06-08T17:04:51.058985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:52.110839Z","iopub.execute_input":"2021-06-08T17:04:52.111138Z","iopub.status.idle":"2021-06-08T17:04:52.117152Z","shell.execute_reply.started":"2021-06-08T17:04:52.11111Z","shell.execute_reply":"2021-06-08T17:04:52.116089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:54.81121Z","iopub.execute_input":"2021-06-08T17:04:54.811573Z","iopub.status.idle":"2021-06-08T17:04:54.817245Z","shell.execute_reply.started":"2021-06-08T17:04:54.811541Z","shell.execute_reply":"2021-06-08T17:04:54.816284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Creation","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:04:58.252261Z","iopub.execute_input":"2021-06-08T17:04:58.252602Z","iopub.status.idle":"2021-06-08T17:04:58.25731Z","shell.execute_reply.started":"2021-06-08T17:04:58.25257Z","shell.execute_reply":"2021-06-08T17:04:58.256204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:05:00.218734Z","iopub.execute_input":"2021-06-08T17:05:00.219061Z","iopub.status.idle":"2021-06-08T17:05:00.224999Z","shell.execute_reply.started":"2021-06-08T17:05:00.219031Z","shell.execute_reply":"2021-06-08T17:05:00.223903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### No of classes\nnum_labels = y.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:05:07.230315Z","iopub.execute_input":"2021-06-08T17:05:07.230748Z","iopub.status.idle":"2021-06-08T17:05:07.234465Z","shell.execute_reply.started":"2021-06-08T17:05:07.230712Z","shell.execute_reply":"2021-06-08T17:05:07.23343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\n### first Layer\nmodel.add(Dense(100, input_shape=(40,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n### second Layer\nmodel.add(Dense(200))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n### third Layer\nmodel.add(Dense(100))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\n### final Layer\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:05:07.906284Z","iopub.execute_input":"2021-06-08T17:05:07.906702Z","iopub.status.idle":"2021-06-08T17:05:10.010303Z","shell.execute_reply.started":"2021-06-08T17:05:07.906659Z","shell.execute_reply":"2021-06-08T17:05:10.009466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:05:13.974144Z","iopub.execute_input":"2021-06-08T17:05:13.974484Z","iopub.status.idle":"2021-06-08T17:05:13.986902Z","shell.execute_reply.started":"2021-06-08T17:05:13.974451Z","shell.execute_reply":"2021-06-08T17:05:13.985009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:05:18.27294Z","iopub.execute_input":"2021-06-08T17:05:18.273248Z","iopub.status.idle":"2021-06-08T17:05:18.289148Z","shell.execute_reply.started":"2021-06-08T17:05:18.273219Z","shell.execute_reply":"2021-06-08T17:05:18.288371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime\n\nnum_epochs = 100\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath='./', verbose = 1, save_best_only=True)\nstart = datetime.now()\n\nmodel.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data = (X_test, y_test), callbacks=[checkpointer])\n\nduration = datetime.now() - start\nprint('Training completed in time: ', duration)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:05:24.004892Z","iopub.execute_input":"2021-06-08T17:05:24.005205Z","iopub.status.idle":"2021-06-08T17:07:40.664037Z","shell.execute_reply.started":"2021-06-08T17:05:24.005176Z","shell.execute_reply":"2021-06-08T17:07:40.663189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluate the model","metadata":{}},{"cell_type":"code","source":"test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(test_accuracy[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:41.163331Z","iopub.execute_input":"2021-06-08T17:09:41.163683Z","iopub.status.idle":"2021-06-08T17:09:41.277998Z","shell.execute_reply.started":"2021-06-08T17:09:41.163653Z","shell.execute_reply":"2021-06-08T17:09:41.277212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so, this model give 74.81% accuracy on test data.","metadata":{}},{"cell_type":"markdown","source":"### Testing some test audio data\nsteps:\n- Preprocess the new audio data\n- predict the classes\n- Inverse transform your Predicted Label","metadata":{}},{"cell_type":"code","source":"filename = '../input/urbansound8k/fold6/108638-9-0-1.wav'\naudio, sample_rate = librosa.load(filename, res_type='kaiser_fast')\nmfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate, n_mfcc=40)\nmfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n\nprint(mfccs_scaled_features)\nmfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)\nprint(mfccs_scaled_features)\nprint(mfccs_scaled_features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:17:20.015272Z","iopub.execute_input":"2021-06-08T17:17:20.015625Z","iopub.status.idle":"2021-06-08T17:17:20.078868Z","shell.execute_reply.started":"2021-06-08T17:17:20.015595Z","shell.execute_reply":"2021-06-08T17:17:20.077988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_label = model.predict_classes(mfccs_scaled_features)\nprint(predicted_label)\nprediction_class = labelencoder.inverse_transform(predicted_label)\nprediction_class","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:17:20.42209Z","iopub.execute_input":"2021-06-08T17:17:20.422417Z","iopub.status.idle":"2021-06-08T17:17:20.465793Z","shell.execute_reply.started":"2021-06-08T17:17:20.422386Z","shell.execute_reply":"2021-06-08T17:17:20.465047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model predicted given audio file as `street_music`. Let's check out what audio it is.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\ndata, sample_rate = librosa.load(filename)\nlibrosa.display.waveplot(data, sr = sample_rate)\nipd.Audio(filename)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:17:21.45705Z","iopub.execute_input":"2021-06-08T17:17:21.45733Z","iopub.status.idle":"2021-06-08T17:17:21.788144Z","shell.execute_reply.started":"2021-06-08T17:17:21.457303Z","shell.execute_reply":"2021-06-08T17:17:21.787222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, model predicted it correctly. It is street music.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}