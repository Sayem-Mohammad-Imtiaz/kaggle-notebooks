{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a starting kernel to convert the documents into a meaningful dataframe and visualize texts and the most meaningful words from it to serve for the further analysis\n\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Generating a dataframe from the documents (json files)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#let's imprt the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Get all the files saved into a list and then iterate over them like below to extract relevant information\n# hold this information in a dataframe and then move forward from there. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating an empty dataframe with only column names to fill it with files content\ndf = pd.DataFrame(columns=['Doc_ID', 'Title', 'Text', 'Source'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grabbing the files from the repositories using glob library\n\njson_filenames = glob.glob(f'/kaggle/input/CORD-19-research-challenge/2020-03-13/**/**/*.json', recursive=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking a look at the first 10 filenames path ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"json_filenames[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we just iterate over the files and populate the data frame. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df(json_filenames, df):\n\n    for file_name in json_filenames:\n\n        row = {\"Doc_ID\": None, \"Title\": None, \"Text\": None, \"Source\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n            \n            #getting the column values for this specific document\n            row['Doc_ID'] = data['paper_id']\n            row['Title'] = data['metadata']['title']            \n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \" \".join(body_list)\n            row['Text'] = body\n            \n            # Now just add to the dataframe. \n            row['Source'] = file_name.split(\"/\")[5]\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataframe = get_df(json_filenames, df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataframe.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataframe.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = corona_dataframe.to_csv('kaggle_CORD-19_csv_format.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NER extraction from Text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here's we'll visualize the extraction of entities from some text in the dataframe generated previously","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NER extraction using Spacy library\ndoc = nlp(corona_dataframe[\"Text\"][10])\nspacy.displacy.render(doc, style='ent',jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LDA Topic modeling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the necessary libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Load the regular expression library\nimport re\nfrom wordcloud import WordCloud\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's first clean our text datas with some basic operations \n#It may be improved more and more \n\n#Remove punctuation\ncorona_dataframe['Text'] = corona_dataframe['Text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n#Convert to lowercase\ncorona_dataframe['Text'] = corona_dataframe['Text'].map(lambda x: x.lower())\n#Print out the first rows of papers\ncorona_dataframe['Text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's have an idea about what reveal the titles of the papers\n\n\n#Join the different processed titles together.\nlong_string = ','.join(list(corona_dataframe['Title'].values))\n#Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n#Generate a word cloud\nwordcloud.generate(long_string)\n#Visualize the word cloud\nwordcloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's take a look at the distribution of the most significant words of the text corpus\n\n#Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n    \n    plt.figure(2, figsize=(15, 15/1.6180))\n    plt.subplot(title='10 most common words')\n    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n    sns.barplot(x_pos, counts, palette='husl')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.show()\n#Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n#Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(corona_dataframe['Text'])\n#Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LDA model training and results visualization\n#To keep things simple, we will only tweak the number of topic parameters.\n#The first 5 topics mention the most meaningful terms related in the text of all papers \n \n#Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n#Tweak the two parameters below\nnumber_topics = 5\nnumber_words = 10\n#Create and fit the LDA model imported from sklearn library\nlda = LDA(n_components=number_topics, n_jobs=1)\nlda.fit(count_data)\n#Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I hope you enjoy it ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}