{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install -U ppscore","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport ppscore as pps\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams['figure.figsize'] = 8, 5\nplt.style.use(\"fivethirtyeight\")\npd.options.plotting.backend = \"plotly\"\n\ndata = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Content","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<ul style=\"font-size: 18px\">\n    <li>Facts related to heart Failure</li>\n    <li>Description of Data</li>\n    <li>Correlation in the Data</li>\n    <li>Outliers in Data</li>\n    <li>Relation between variables</li>\n    <li>Pairplot</li>\n    <li>Standardizing data</li>\n    <li>Deciding Model</li>\n    <li>Splitting the Data</li>\n    <li>DT Classifier</li>\n    <li>RF Classifier</li>\n    <li>Comparison of Models</li>\n    <li>Conclusion</li>\n</ul>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1>Heart Failure</h1>\n\n<p>Heart failure happens when the heart cannot pump enough blood and oxygen to support other organs in your body. Heart failure is a serious condition, but it does not mean that the heart has stopped beating.</p>\n\n<h1>Facts About Heart Failure</h1>\n\n<ul>\n    <li>About 6.5 million adults in the United States have heart failure.</li>\n    <li>Heart failure was a contributing cause of 1 in 8 deaths in 2017.</li>\n    <li>Heart failure costs the nation an estimated $30.7 billion in 2012. This total includes the cost of health care services, medicines to treat heart failure, and missed days of work.<a href=\"https://www.cdc.gov/heartdisease/heart_failure.htm\">Source</a></li>\n</ul>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://timesofindia.indiatimes.com/thumb/msid-71058199,width-1200,height-900,resizemode-4/.jpg\" height=\"500px\" width=\"500px\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Description of Data","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = data.nunique().reset_index().plot(kind='bar', x='index', y=0, color=0)\nfig.update_layout(title='Unique Value Count Plot', xaxis_title='Variables', yaxis_title='Unique value count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = data.isnull().sum().reset_index().plot(kind='bar', x='index', y=0)\nfig.update_layout(title='Missing Value Plot', xaxis_title='Variables', yaxis_title='Missing value count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">No missing values present. That's a good news. Let's check the data types as well to make sure variables don't have mized type of data here.</p>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = data.dtypes.value_counts().reset_index()\ndf['index'] = df['index'].astype('str')\nsns.barplot(df['index'], df[0])\nplt.title('DataType Count')\nplt.xlabel('DataTypes')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation in the data","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr())\nplt.title('Correelation in data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"matrix_df = pps.matrix(data)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)\nplt.title('PPS Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Outliers in the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:17px\">The variables which I will need to check for outliers are mostly the continuous variables and not the categorical ones.<br><br>Since I have already confirmed the datatypes, categorical variables are already clean.<br><br>\nSo i will check thing for age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium and time</p>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.FacetGrid(data, hue=\"DEATH_EVENT\", height=6,).map(sns.kdeplot, \"age\",shade=True).add_legend()\nplt.title('Age Distribution Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"px.box(data, x='DEATH_EVENT', y='creatinine_phosphokinase', color='smoking', title='Creatinine Phosphokinase Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">The variable creatinine_phosphokinase possibly denotes the amount of creatinine phosphokinase(an enzyme in human body) in the body of the person tested.<br><br>From the above plot it can be inferred that any amount of the enzyme higher than 4000 is an outlier.</p>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"px.violin(data, x='ejection_fraction', color='DEATH_EVENT', title='Ejection Fraction Distribution')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"px.box(data, x='DEATH_EVENT', y='platelets', color='diabetes', points='all', title='Platelets Distribution')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"px.box(data, x='DEATH_EVENT', y='time', color='smoking', notched=True, title='Time under observation Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Relation in the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">From general convention the relation among the variables that might be interesting are<br><br>1. high_blood_pressure and platelets<br>2. diabetes, serum_sodium and serum_creatinine<br>3. time and DEATH_EVENT</p>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=data, x='high_blood_pressure', y='platelets', hue='DEATH_EVENT')\nplt.title('high_blood_pressure vs platelets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=data, x='serum_creatinine', y='serum_sodium', hue='diabetes')\nplt.title('serum_creatinine vs serum_sodium')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.stripplot(data=data, x=\"DEATH_EVENT\", y=\"time\")\nplt.title('Time vs Death Event')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pairplot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">The pairplot below is for checking the relation between all the continuous variables as it will help in deciding the model.</p>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(data=data[['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time','DEATH_EVENT']], hue='DEATH_EVENT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardizing the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">The variables that needs to be standardized are age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, and time.<br><br>Leaving out the categorical variables.<br><br>I will be using the StandardScaler module provided by the sklearn api</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ncols = ['age','creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ndata[cols] = scaler.fit_transform(data[cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">Since the number of dimensions in the data isn't much high, we don't need any dimensionality reduction in this case.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Deciding Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">If we look at a glance at the data then it will seem that the data contains very few number of observations.<br><br>But if looked closely the number of observations is still a lot more than the number of features.<br><br>So there is a chance that low bias/high variance algorithms like KNN, Decision Trees and kernel SVM will perform better on this type of data.<br><br>Also if I use decision trees I won't needs to process the binary categorical variables in the data.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">Now in this case since it is a prediction of whether or not a person has chances of heart attack, high accuracy is the main goal whatsoever.<br><br>So in this I will be using a flexible model since the interpretability can be sacrificed.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">Since the number of observations is not much high them it is quite feasible to use any algorithm without worrying much about the speed or training time.<br><br>In our case most of the variables are not very much linear so it will not be a good idea to use logistic regression or SVM. Rather models like random forest or neural net will be better.<br><br>I will be using decision tree classifier and random forest classifier on the data.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('DEATH_EVENT', axis=1)\ny = data['DEATH_EVENT'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get the model checking metric modules as well","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve,accuracy_score,plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree CLassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(min_samples_split=2, class_weight={0:2,1:7}, random_state=13)\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Decision Tree Classifier is', accuracy_score(prediction,test_y))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(model, test_X, test_y)\nplt.title('Decision Tree Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier(min_samples_split=2, class_weight={0:2,1:7}, random_state=13)\nmodel.fit(train_X, train_y)\ny_pred_prob = model.predict_proba(test_X)[:,1]\nfpr, tpr, thresholds = roc_curve(test_y, y_pred_prob)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='DT')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(min_samples_split=2, class_weight={0:2,1:7}, random_state=13)\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Random Forest Classifier is', accuracy_score(prediction,test_y))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(model, test_X, test_y)\nplt.title('Random Forest Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(min_samples_split=2, class_weight={0:2,1:7}, random_state=13)\nmodel.fit(train_X, train_y)\ny_pred_prob = model.predict_proba(test_X)[:,1]\nfpr, tpr, thresholds = roc_curve(test_y, y_pred_prob)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='RF')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison of models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<ol style='font-size:18px'>\n    <li>Random Forest provides a better accuracy(89.3%) than decision trees</li>\n    <li>RF is more efficient in prediction the negative labels in the data as can be seen from the confusion matrices of the two models</li>\n    <li>ROC score of RF is much better than DT</li>\n</ol>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px\">Do you fear a heart attack? Then obviouly make use of random forest. Also in case this notebook gave some hint on using the model, do leave a heart here. I mean an upvote :)</p>\n\n<p style=\"font-size:18px\">And let me know in comments for any improvement.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}