{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as skl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and Read the dataset","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"../input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns #gives column names in the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape #shows no of rows and columns in the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so we can see that we have total 13 attributes out of which 12 attributes are Independent variables and 1 attribute (Loan_Status) is dependent variable.\nwe can also see the datatype of each variable. ","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"#let`s check the missing values with in the dataset\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fill the missing values for numerical terms - mean\nLoanAmountMean = df[\"LoanAmount\"].mean()\nLoanAmountTermMean = df[\"Loan_Amount_Term\"].mean()\ndf[\"LoanAmount\"] = df[\"LoanAmount\"].fillna(LoanAmountMean)\ndf[\"Loan_Amount_Term\"] = df[\"Loan_Amount_Term\"].fillna(LoanAmountTermMean)\n# I have replaced missing values in Credit_History column with most frequent value - 1.0\ndf[\"Credit_History\"] = df[\"Credit_History\"].fillna(1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fill the missing values for categorical terms - mode\ndf[\"Gender\"] = df[\"Gender\"].fillna(df[\"Gender\"].mode()[0])\ndf[\"Married\"] = df[\"Married\"].fillna(df[\"Married\"].mode()[0])\ndf[\"Dependents\"] = df[\"Dependents\"].fillna(df[\"Dependents\"].mode()[0])\ndf[\"Self_Employed\"] = df[\"Self_Employed\"].fillna(df[\"Self_Employed\"].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum() #now we can see that their are no missing values in the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating New Features","metadata":{}},{"cell_type":"code","source":"# ApplicantIncome and CoapplicantIncome can be combined together \n# so we are adding these two columns and making a new column called TotalIncome\n# and we will drop ApplicantIncome and CoapplicantIncome columns\ndf[\"TotalIncome\"] = df[\"ApplicantIncome\"] + df[\"CoapplicantIncome\"]\ncols=[\"ApplicantIncome\",\"CoapplicantIncome\",\"Loan_ID\"]\ndf.drop(cols,axis=1,inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#categorical attriburtes visualization\nsns.countplot(df[\"Gender\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so we can analyse from this plot that majority of the data is for Male","metadata":{}},{"cell_type":"code","source":"sns.countplot(df[\"Married\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so majority of the applicants are married","metadata":{}},{"cell_type":"code","source":"sns.countplot(df[\"Dependents\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so most of the applicants have 0 dependents and very few have 3+ dependents","metadata":{}},{"cell_type":"code","source":"sns.countplot(df[\"Education\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the above graph that most of the applicants are Graduate.","metadata":{}},{"cell_type":"code","source":"sns.countplot(df[\"Self_Employed\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the applicants are not self employed.","metadata":{}},{"cell_type":"code","source":"# numerical attributes visualization\nsns.distplot(df[\"TotalIncome\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MAjority of the TotalIncome of applicants is between 0-10,000 , only few are from 20,000 onwards.\ngraph is left skewed i.e most of the applicants are on the left side which is not a good distribution for training the model.so we will apply the log function in the column to normalize the attribute and make a bell curve.\n\nIf you see the graph \"left skewed or right skewed\", you can apply\n1. log transformation\n2. Min-Max Normalization\n3. Standarization\nThese are the common techniques to normalize the distribution in order to train the model better.","metadata":{}},{"cell_type":"code","source":"# apply log transformation to the attribute\ndf[\"TotalIncome\"]= np.log(df[\"TotalIncome\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"TotalIncome\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"LoanAmount\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so the distribution for CoapplicantIncome is also left skewed. we will apply log transformation here as well.","metadata":{}},{"cell_type":"code","source":"df[\"LoanAmount\"] = np.log(df[\"LoanAmount\"])\nsns.distplot(df[\"LoanAmount\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so now the distribution of LoanAmount is better than before.","metadata":{}},{"cell_type":"code","source":"sns.distplot(df[\"Loan_Amount_Term\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"Credit_History\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No need to apply transformation here because values are already in the range of 0-1","metadata":{}},{"cell_type":"markdown","source":"# Correlation Matrix","metadata":{}},{"cell_type":"markdown","source":"Correlation Matrix is used to see the relationship between variables. if the correlation between two variables is high , drop any one of the variable (This is the best practice). ","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize=(12,9))\nsns.heatmap(corr, annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoding ","metadata":{}},{"cell_type":"markdown","source":"## Converting Categorical variables into numerical using label encoder","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = [\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Property_Area\",\"Loan_Status\",\"Dependents\"]\nle = LabelEncoder()\nfor col in cols:\n    df[col] = le.fit_transform(df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see all the columns are converted into numerical columns and we can now easily train our model.","metadata":{}},{"cell_type":"markdown","source":"# Train-Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(\"Loan_Status\",axis=1)\nY = df[\"Loan_Status\"]\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have splitted the data like 20% for testing and 80% for training.","metadata":{}},{"cell_type":"markdown","source":"# Model Training (Logistic Regression, Decision Tree, Random Forest)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier()\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning","metadata":{}},{"cell_type":"code","source":"# Let`s fine tune the hyper parameters of RandomForest \nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25,max_depth=7,max_features=1)\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_predicted = model.predict(x_test)\ncm = confusion_matrix(y_test,y_predicted)\nsns.heatmap(cm,annot=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}