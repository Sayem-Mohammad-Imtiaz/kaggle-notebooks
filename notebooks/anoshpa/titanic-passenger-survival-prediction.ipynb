{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"# let`s first import all the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and Read the dataset","metadata":{}},{"cell_type":"code","source":"# let`s now load the titanic dataset\ndata=pd.read_csv(\"../input/titanicdataset-traincsv/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head() # look at the starting five rows of the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape # rows and columns in the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe() #gives the statistical summary of the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"1. Fill the missing values\n2. Convert the data type of age (float64) to int64","metadata":{}},{"cell_type":"code","source":"data.isnull().sum() #look for the missing value within the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As the \"age,cabin and Embarked column have missing values\" so let`s fill the missing values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let`s look at the cabin column\ndata[\"Cabin\"].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so we have 687 missing values in cabin column so let`s drop this column\ndata.drop([\"Cabin\"],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let`s now look at the `Age` column\ndata[\"Age\"].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In Age column we have 177 missing value so let`s fill these values with mean or median , \n# I will use mean to fill it\nAge_mean=data[\"Age\"].mean()\ndata[\"Age\"]=data[\"Age\"].fillna(Age_mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let`s have look at Age column\ndata[\"Age\"].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.dropna() # Embarked has only 2 missing values so let`s drop the rows ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# By looking at the data we can analyse that \"PassengerId,Name,SibSp,Parch,Ticket ,Fare,Embarked\" \n# can not effect on the survival of passenger\nfeatures=data[[\"Pclass\",\"Sex\",\"Age\",\"Fare\"]]\ntarget=data[\"Survived\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As \"Sex\" is categorical data so let`s use LabelEncoder to covert it into Numerical data\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le=LabelEncoder() #let`s create object of LabelEncoder\nfeatures[\"Sex_n\"]=le.fit_transform(features[\"Sex\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features\n# Sex column is converted into numerical column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let`s drop the original \"Sex\" column\nfeatures.drop([\"Sex\"],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL BUILDING","metadata":{}},{"cell_type":"code","source":"# before building the model let`s split the dataset using train_test_split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(features,target,test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train) #fit method is used to train the model so here we are training our model\nmodel.score(x_test,y_test)  # score method will take \"x_test\" and will calculate survival rate\n# and will compare it with y_test and will give the accuracy of the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree","metadata":{}},{"cell_type":"code","source":"# let`s now use decision tree algorithm for prediction\nfrom sklearn import tree\nmodel=tree.DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier()\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning","metadata":{}},{"cell_type":"markdown","source":"From the above models we can observe that without fine tuning any of the model, the Random Forest give us the best accuracy so lets fine tune the hyper parameters of the Random Forest.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_estimators=50,max_depth=10,random_state=42)\nmodel.fit(x_train,y_train)\nprint(\"The accuracy of model is: \",model.score(x_test,y_test)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can see that after fine tuning our accuracy is increased \nNow our accuracy is 86% which is pretty good. yayyyy :D","metadata":{}},{"cell_type":"markdown","source":"Lets now predict the survival of the pessengers using our Random Forest model.","metadata":{}},{"cell_type":"code","source":"y_predicted= model.predict(x_test)\ny_predicted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matric","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_predicted)\nsns.heatmap(cm,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}