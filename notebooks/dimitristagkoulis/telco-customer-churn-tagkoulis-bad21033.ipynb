{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T19:03:10.00932Z","iopub.execute_input":"2021-07-20T19:03:10.010077Z","iopub.status.idle":"2021-07-20T19:03:10.02192Z","shell.execute_reply.started":"2021-07-20T19:03:10.010012Z","shell.execute_reply":"2021-07-20T19:03:10.020537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard\nimport pandas as pd\nimport numpy as np\nfrom pandas_profiling import ProfileReport\n\n# Plots\nimport seaborn as sns\nfrom plotly.offline import iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\n\npio.templates.default = \"plotly_white\"\ninit_notebook_mode()\n\n# Preprocessing tools\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom scipy.stats import zscore\n\n#Modeling Tools\nimport xgboost as xgb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Tuning Tools\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import plot_importance\n\n# Extras\nfrom datetime import date\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport shap\n\n# Datapath and Setup\ndata_path = \"/kaggle/input/telco-customer-churn/\"\nrandom_seed = 1","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.061616Z","iopub.execute_input":"2021-07-20T19:03:10.062018Z","iopub.status.idle":"2021-07-20T19:03:10.246629Z","shell.execute_reply.started":"2021-07-20T19:03:10.061983Z","shell.execute_reply":"2021-07-20T19:03:10.24571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########################\n## DISABLE WARNINGS\n###########################\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.248044Z","iopub.execute_input":"2021-07-20T19:03:10.248498Z","iopub.status.idle":"2021-07-20T19:03:10.252446Z","shell.execute_reply.started":"2021-07-20T19:03:10.248461Z","shell.execute_reply":"2021-07-20T19:03:10.251731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Customer churn is one of the most important aspects of a business. Especially in a telecommunication setting where a customer produces monthly recurrent revenue, the loss  of customers can have a significant impact on the long term financial standing of the company. This notebook aims to provide a holistic approach to the telecommunications company. One the one hand, through exploratory data analysis, it aims to provide valuable feedback regarding the reasons that customers churn, \nthus providing useful insight for the company to minimize those reasons. On the other hand, it provides an accurate model to predict potentially churning customers, thus giving the company one more opportunity to follow a proactive approach to keep the customers.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(data_path+\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.254076Z","iopub.execute_input":"2021-07-20T19:03:10.254536Z","iopub.status.idle":"2021-07-20T19:03:10.334902Z","shell.execute_reply.started":"2021-07-20T19:03:10.2545Z","shell.execute_reply":"2021-07-20T19:03:10.334038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#Finding the columns and the type of each column\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.336287Z","iopub.execute_input":"2021-07-20T19:03:10.336817Z","iopub.status.idle":"2021-07-20T19:03:10.345819Z","shell.execute_reply.started":"2021-07-20T19:03:10.336741Z","shell.execute_reply":"2021-07-20T19:03:10.344798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TotalCharges is an object type instead of float64. This happened because there are some blank spaces in this column which caused Python to force the data type as object. To fix that, we will have to trim blank spaces before changing the data type.","metadata":{}},{"cell_type":"code","source":"# replace blanks with np.nan\ndf['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)\n\n# convert to float64\ndf['TotalCharges'] = df['TotalCharges'].astype('float64')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.347239Z","iopub.execute_input":"2021-07-20T19:03:10.34777Z","iopub.status.idle":"2021-07-20T19:03:10.367404Z","shell.execute_reply.started":"2021-07-20T19:03:10.347735Z","shell.execute_reply":"2021-07-20T19:03:10.366483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check missing values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.369041Z","iopub.execute_input":"2021-07-20T19:03:10.369798Z","iopub.status.idle":"2021-07-20T19:03:10.411189Z","shell.execute_reply.started":"2021-07-20T19:03:10.369758Z","shell.execute_reply":"2021-07-20T19:03:10.410003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we striped blank spaces Total charges now has 11 missing values. Since the missing values are just sucha a small part of the dataset they can be droped without affecting the final results.","metadata":{}},{"cell_type":"code","source":"# check missing values\ndf.dropna(subset= ['TotalCharges'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.413313Z","iopub.execute_input":"2021-07-20T19:03:10.413969Z","iopub.status.idle":"2021-07-20T19:03:10.426493Z","shell.execute_reply.started":"2021-07-20T19:03:10.413921Z","shell.execute_reply":"2021-07-20T19:03:10.425329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run pandas profiling for Exploratory Data Analysis \nprofile = ProfileReport(df, title=\"Telco Customer Churn Profiling Report\", explorative=True, )\nprofile.to_file(\"Churn_EDA_report.html\")\n\n#Due to the fact that Kaggle sometimes produces an error on the pandas profiling module the html report can be found in the output folder.","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:10.42827Z","iopub.execute_input":"2021-07-20T19:03:10.42877Z","iopub.status.idle":"2021-07-20T19:03:36.113095Z","shell.execute_reply.started":"2021-07-20T19:03:10.42872Z","shell.execute_reply":"2021-07-20T19:03:36.111967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Churn Distribution","metadata":{}},{"cell_type":"code","source":"px.pie(df,\"Churn\", opacity = 0.8, title = \"<b>Client Churn Distribution<b>\")","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:36.114639Z","iopub.execute_input":"2021-07-20T19:03:36.114987Z","iopub.status.idle":"2021-07-20T19:03:36.259058Z","shell.execute_reply.started":"2021-07-20T19:03:36.114953Z","shell.execute_reply":"2021-07-20T19:03:36.258002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's some imbalance on Churn Distribution, 26.5% of the clients have churned, and small occurences of a label could lead to bad predictor.\\\nIt's possible to choose some ways to work with this case with Stratification, which will be used in the Cross Validation step of the process by default.\\\nChoosing a metric that deals with imbalanced datasets, in this case the AUC score. AUC is sensitive to class imbalance in the sense that when there is a minority class and it will have a strong impact on the AUC value. The Churn problem is about client retention, so is worth to check about false positives, so precision and recall metrics are a must for this situtation.","metadata":{}},{"cell_type":"code","source":"\nfig = px.scatter(x=df['tenure'], y=df['TotalCharges'], \n                 color = df['Churn'], template = 'presentation', \n                 opacity = 0.5, facet_col = df['Contract'], \n                 title = 'Customer Churn by Tenure, Charges, and Contract Type',\n                 labels = {'x' : 'Customer Tenure', 'y' : 'Total Charges $'})\n                \nfig.show()\n#some of the plottly express visualizations sometimes are not displayed properly on Kaggle on browsers other than chrome","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:36.260519Z","iopub.execute_input":"2021-07-20T19:03:36.26084Z","iopub.status.idle":"2021-07-20T19:03:36.450508Z","shell.execute_reply.started":"2021-07-20T19:03:36.260807Z","shell.execute_reply":"2021-07-20T19:03:36.449462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most churn can be seen in the contracts that are “Month-to-Month”. Makes sense, of course. \nAlso, I can see that as the tenure increases and so are the total charges, the likelihood of customers with high tenure and low charges is less compared to customers with high tenure and high charges.","metadata":{}},{"cell_type":"code","source":"df_churn = df.query('(Churn == \"Yes\")')\ndf_no_churn = df.query('(Churn == \"No\")')\n\nfig1 = px.histogram(df_churn, x=\"MonthlyCharges\", color=\"Contract\", marginal=\"rug\",\n                         title = \"Churned customers\")\n\nfig2 = px.histogram(df_no_churn, x=\"MonthlyCharges\", color=\"Contract\", marginal=\"rug\",\n                         title = \"Not Churned customers\")\nfig1.show()\nfig2.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:36.453294Z","iopub.execute_input":"2021-07-20T19:03:36.453654Z","iopub.status.idle":"2021-07-20T19:03:36.784053Z","shell.execute_reply.started":"2021-07-20T19:03:36.453619Z","shell.execute_reply":"2021-07-20T19:03:36.782972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1 = px.histogram(df_churn, x=\"TotalCharges\", color=\"Contract\", marginal=\"rug\",\n                         title = \"Churned customers\")\n\nfig2 = px.histogram(df_no_churn, x=\"TotalCharges\", color=\"Contract\", marginal=\"rug\",\n                         title = \"Not Churned customers\")\nfig1.show()\nfig2.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:36.786031Z","iopub.execute_input":"2021-07-20T19:03:36.786358Z","iopub.status.idle":"2021-07-20T19:03:37.113278Z","shell.execute_reply.started":"2021-07-20T19:03:36.786325Z","shell.execute_reply":"2021-07-20T19:03:37.112068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Monthly Charges have a high concentration of churned customer in higher values. \\\nTotal Charges have similar distributions, but the ‘No churn’ distribution have lower values. \\\nMaybe the amount of chage value could lead the client to leave the service.","metadata":{}},{"cell_type":"markdown","source":"### Correlations","metadata":{}},{"cell_type":"code","source":"# define the mask to set the values in the upper triangle to True\nnp.triu(np.ones_like(df.corr()))\nmask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n\n# Increase the size of the heatmap.\nplt.figure(figsize=(40, 18))\n\n# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\nheatmap = sns.heatmap(df.corr(),mask=mask, vmin=-1, vmax=1, annot=True, cmap='RdBu')\n\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Churn Correlation Heatmap', fontdict={'fontsize':12}, pad=12)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:37.114968Z","iopub.execute_input":"2021-07-20T19:03:37.11529Z","iopub.status.idle":"2021-07-20T19:03:38.195763Z","shell.execute_reply.started":"2021-07-20T19:03:37.115257Z","shell.execute_reply":"2021-07-20T19:03:38.194599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"In order to properly fit the data to the models, they need to be uniform. \\\nSo, using the MultiColumnLabelEncoder class I convert the categorical features into numerical values, by using the scikit-learn.preprossecing classes OneHotEncoder and LabelEncoder. \\\nThe churn and the customer ID columns are not coppied in the fetures array since the fisrst is the label and the second does not provide any value. \\\nAs for the numerical values, they are normalized, by using the scikit-learn scikit-learn.preprossecing class StandardScaler in the MultiColumnLabelScaler class, to avoid having differences in scales of the measures that could affect the model. \\\nIt is also important that there were no outliers detected in the profiling report so there was no preprocessing to replace outliers.\n","metadata":{}},{"cell_type":"code","source":"#remove customer ID column from the dataset\nX = df.copy().drop(['Churn','customerID'] , axis = 1)\ny = df['Churn'].copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.197523Z","iopub.execute_input":"2021-07-20T19:03:38.198006Z","iopub.status.idle":"2021-07-20T19:03:38.20783Z","shell.execute_reply.started":"2021-07-20T19:03:38.197959Z","shell.execute_reply":"2021-07-20T19:03:38.206825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Class for Encoding the Numerical Values\nclass MultiColumnLabelScaler:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        StandardScaler(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = StandardScaler().fit_transform(output[col].values.reshape(-1, 1))\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = StandardScaler().fit_transform(col.values.reshape(-1, 1))\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.20922Z","iopub.execute_input":"2021-07-20T19:03:38.209664Z","iopub.status.idle":"2021-07-20T19:03:38.219835Z","shell.execute_reply.started":"2021-07-20T19:03:38.209614Z","shell.execute_reply":"2021-07-20T19:03:38.218623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Class for Encoding the Categorical Variables\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        Stan(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.22174Z","iopub.execute_input":"2021-07-20T19:03:38.222198Z","iopub.status.idle":"2021-07-20T19:03:38.234655Z","shell.execute_reply.started":"2021-07-20T19:03:38.222144Z","shell.execute_reply":"2021-07-20T19:03:38.233579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding the Categorical Variables\nX = MultiColumnLabelEncoder(columns = [\"gender\",\"Partner\",\"Dependents\",\"PhoneService\",\"MultipleLines\",\"InternetService\", \"OnlineSecurity\",\n                                        \"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\n                                        \"PaperlessBilling\",\"PaymentMethod\"]).fit_transform(X)\n\ny = LabelEncoder().fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.236529Z","iopub.execute_input":"2021-07-20T19:03:38.237006Z","iopub.status.idle":"2021-07-20T19:03:38.30458Z","shell.execute_reply.started":"2021-07-20T19:03:38.23696Z","shell.execute_reply":"2021-07-20T19:03:38.303456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding the Numerical Variables\nX = MultiColumnLabelScaler(columns = [\"TotalCharges\", \"MonthlyCharges\", \"tenure\"]).fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.307111Z","iopub.execute_input":"2021-07-20T19:03:38.307591Z","iopub.status.idle":"2021-07-20T19:03:38.317194Z","shell.execute_reply.started":"2021-07-20T19:03:38.307542Z","shell.execute_reply":"2021-07-20T19:03:38.315977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the dataset\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.318773Z","iopub.execute_input":"2021-07-20T19:03:38.319159Z","iopub.status.idle":"2021-07-20T19:03:38.344142Z","shell.execute_reply.started":"2021-07-20T19:03:38.319124Z","shell.execute_reply":"2021-07-20T19:03:38.342902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One other preprocessing technique that was used on previous iterations of the notebook but proved to harm both the overall model perfomance and the explainability of the model was Principal Component Analysis with 17 components since under 17, the explained variance dropped lower than 99%. The technique rendered the model unexplainable since the fetured where projected into new ones and also lowered all the scores of the models so it was omitted on the final iteration of the notebook.","metadata":{}},{"cell_type":"markdown","source":"### Modeling","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:15:24.448639Z","iopub.execute_input":"2021-07-20T18:15:24.449Z","iopub.status.idle":"2021-07-20T18:15:25.043291Z","shell.execute_reply.started":"2021-07-20T18:15:24.448964Z","shell.execute_reply":"2021-07-20T18:15:25.040744Z"}}},{"cell_type":"markdown","source":"In the testing phase multiple algorithms were tested, like Decision Trees, Random Forests, Adaboost, Bagging, Extra Trees and Gradient Boosting. The best and second-best classifiers in my experiments were Gradient Boosting and XGBoost respectively. For this reason, I run a Grid Search for tuning the hyperparameters on both XGBoost and Gradient Boosting Classifier. I have tried different approaches at stacking the classifiers, which even though outperformed most of the \nmodels, it was consistently less accurate than all of the enseble base models.","metadata":{}},{"cell_type":"code","source":"#Initializing models\ndt_classifier = DecisionTreeClassifier(max_depth=20)\nrf_classifier = RandomForestClassifier(n_estimators = 5, criterion = 'entropy')\nada_classifier = AdaBoostClassifier()\nbag_classifier = BaggingClassifier()\nxtrees_classifier = ExtraTreesClassifier()\ngrad_classifier = GradientBoostingClassifier()\nxgbclass = xgb.XGBClassifier(eval_metric='auc')\n\n# hyperparameters from gridsearch below\ngrad_enh = GradientBoostingClassifier(learning_rate = 0.1, max_depth = 1, n_estimators = 500)\n\n# hyperparameters from gridsearch below\nxgbenhanced = xgb.XGBClassifier(colsample_bytree= 0.7,gamma= 0.5, learning_rate= 0.1, max_depth= 3, min_child_weight= 3, eval_metric='auc')\n\n#Setting up the stacking model\ndef get_stacking():\n  # define the base models\n  level0 = list()\n  level0.append(('Random Forest', rf_classifier))\n  level0.append(('Bagging Classifier', bag_classifier))\n  level0.append(('Gradient Boosting Classifier', grad_classifier))\n  # define meta learner model\n  level1 = xgbclass\n  # define the stacking ensemble\n  model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n  return model\n\nstack = get_stacking()\n\n#Setting up the function to call the algorithms\ndef check_algorithms():\n  models = []\n  models.append(('Decision Tree', dt_classifier))\n  models.append(('Random Forest', rf_classifier))\n  models.append(('AdaBoost Classifier', ada_classifier))\n  models.append(('Bagging Classifier', bag_classifier))\n  models.append(('Extra Trees Classifier', xtrees_classifier))\n  models.append(('Gradient Boosting Classifier', grad_classifier))\n  models.append(('Gradient Boosting Enhanced', grad_enh))\n  models.append(('XGB Classifier', xgbclass))\n  models.append(('XGB Enhanced', xgbenhanced))\n  models.append(('Stacking', get_stacking()))\n  return models","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.347715Z","iopub.execute_input":"2021-07-20T19:03:38.348082Z","iopub.status.idle":"2021-07-20T19:03:38.362085Z","shell.execute_reply.started":"2021-07-20T19:03:38.348047Z","shell.execute_reply":"2021-07-20T19:03:38.361121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.36522Z","iopub.execute_input":"2021-07-20T19:03:38.365558Z","iopub.status.idle":"2021-07-20T19:03:38.385994Z","shell.execute_reply.started":"2021-07-20T19:03:38.365521Z","shell.execute_reply":"2021-07-20T19:03:38.384934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBoost Hyperparameter tuning\n\n#xgbclass = xgb.XGBClassifier(tree_method='gpu_hist')\n#xgboost_paramgrid = {'learning_rate': [0.05, 0.10, 0.25, 0.20], \n#                     'max_depth': [3, 4, 5, 6, 8], \n#                     'min_child_weight': [1, 3, 5, 6], \n#                     'gamma': [0.1, 0.2, 0.3, 0.4, 0.5],\n#                     'colsample_bytree' : [0.4, 0.5, 0.7]\n#                     }\n#xgb_grid = GridSearchCV(xgbclass, xgboost_paramgrid, scoring='roc_auc', cv=5, verbose =3, n_jobs = 4)\n#xgb_grid.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.387858Z","iopub.execute_input":"2021-07-20T19:03:38.388214Z","iopub.status.idle":"2021-07-20T19:03:38.397002Z","shell.execute_reply.started":"2021-07-20T19:03:38.388177Z","shell.execute_reply":"2021-07-20T19:03:38.395681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the best params for xgboost to set them on the model rotation\n#xgb_grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.400012Z","iopub.execute_input":"2021-07-20T19:03:38.400498Z","iopub.status.idle":"2021-07-20T19:03:38.417709Z","shell.execute_reply.started":"2021-07-20T19:03:38.400448Z","shell.execute_reply":"2021-07-20T19:03:38.416505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gradient Booster Hyperparameter tuning\n\n#gradenhanced = xgb.XGBClassifier(tree_method='gpu_hist')\n#grad_paramgrid = {'learning_rate': [0.05, 0.10, 0.20, 0.30], \n#                  'max_depth': [1, 3, 4, 5, 6, 7, 8, 9, 10], \n#                  \"n_estimators\":[50,250,500]}\n#grad_grid = GridSearchCV(gradenhanced, grad_paramgrid, scoring='roc_auc', cv=5, verbose =3, n_jobs = 4)\n#grad_grid.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.419188Z","iopub.execute_input":"2021-07-20T19:03:38.419578Z","iopub.status.idle":"2021-07-20T19:03:38.433933Z","shell.execute_reply.started":"2021-07-20T19:03:38.419542Z","shell.execute_reply":"2021-07-20T19:03:38.433014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the best params for Gradient Booster\n#grad_grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.435531Z","iopub.execute_input":"2021-07-20T19:03:38.435947Z","iopub.status.idle":"2021-07-20T19:03:38.451216Z","shell.execute_reply.started":"2021-07-20T19:03:38.435904Z","shell.execute_reply":"2021-07-20T19:03:38.449988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Results\\n')\nresults = []\nnames = []\nfor name, model in check_algorithms():\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %s %f %s (%f)' % (name,'AUC', cv_results.mean(),', Standart Deviation', cv_results.std()))\nprint('-'*64)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:03:38.452946Z","iopub.execute_input":"2021-07-20T19:03:38.453294Z","iopub.status.idle":"2021-07-20T19:05:25.692271Z","shell.execute_reply.started":"2021-07-20T19:03:38.453261Z","shell.execute_reply":"2021-07-20T19:05:25.690974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the results\nfig_res = results\nfig_names = ['DT', 'RF', 'ADA', 'BG', 'XTR', 'GB', 'GBE', 'XGB', 'XGBE', 'STCK']\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison AUC')\nax = fig.add_subplot(111)\nplt.boxplot(fig_res)\nax.set_xticklabels(fig_names)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:25.694154Z","iopub.execute_input":"2021-07-20T19:05:25.694765Z","iopub.status.idle":"2021-07-20T19:05:25.97526Z","shell.execute_reply.started":"2021-07-20T19:05:25.694724Z","shell.execute_reply":"2021-07-20T19:05:25.974396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad_enh.fit(X_train, y_train)\ny_pred_gbe = grad_enh.predict(X_test)\n\n#y_pred = np.where(y_pred == 0, 'no', y_pred)\n#y_pred = np.where(y_pred == 1, 'yes', y_pred)\n#y_test = np.where(y_test == 0, 'no', y_test)\n#y_test = np.where(y_test == 1, 'yes', y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:25.97637Z","iopub.execute_input":"2021-07-20T19:05:25.97679Z","iopub.status.idle":"2021-07-20T19:05:27.988967Z","shell.execute_reply.started":"2021-07-20T19:05:25.976758Z","shell.execute_reply":"2021-07-20T19:05:27.988033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print confusion matrix to get a clearer picture of the model's performance\nconfusion_matrix = metrics.confusion_matrix(y_test, y_pred_gbe)\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n\nplt.xlabel(\"Predicted Label\", fontsize= 12)\nplt.ylabel(\"True Label\", fontsize= 12)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:27.990623Z","iopub.execute_input":"2021-07-20T19:05:27.991284Z","iopub.status.idle":"2021-07-20T19:05:28.275461Z","shell.execute_reply.started":"2021-07-20T19:05:27.991233Z","shell.execute_reply":"2021-07-20T19:05:28.274368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred_gbe, labels = [0, 1]))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:28.277121Z","iopub.execute_input":"2021-07-20T19:05:28.27756Z","iopub.status.idle":"2021-07-20T19:05:28.292086Z","shell.execute_reply.started":"2021-07-20T19:05:28.277513Z","shell.execute_reply":"2021-07-20T19:05:28.290913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbe_pred_proba = grad_enh.predict_proba(X_test)[:,1]\n\ngbe_roc_auc = metrics.roc_auc_score(y_test, gbe_pred_proba)\nprint('ROC_AUC: ', gbe_roc_auc)\n\ngbe_fpr, gbe_tpr, thresholds = metrics.roc_curve(y_test, gbe_pred_proba)\n\nplt.plot(gbe_fpr,gbe_tpr, label = 'ROC_AUC = %0.3f' % gbe_roc_auc)\n\nplt.xlabel(\"False Positive Rate\", fontsize= 12)\nplt.ylabel(\"True Positive Rate\", fontsize= 12)\nplt.legend(loc=\"lower right\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:28.293796Z","iopub.execute_input":"2021-07-20T19:05:28.294174Z","iopub.status.idle":"2021-07-20T19:05:28.473292Z","shell.execute_reply.started":"2021-07-20T19:05:28.29414Z","shell.execute_reply":"2021-07-20T19:05:28.472267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbenhanced.fit(X_train, y_train)\ny_pred_xgbe = xgbenhanced.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:28.474864Z","iopub.execute_input":"2021-07-20T19:05:28.475221Z","iopub.status.idle":"2021-07-20T19:05:28.70751Z","shell.execute_reply.started":"2021-07-20T19:05:28.475171Z","shell.execute_reply":"2021-07-20T19:05:28.706337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print confusion matrix to get a clearer picture of the models performance\nconfusion_matrix = metrics.confusion_matrix(y_test, y_pred_xgbe)\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n\nplt.xlabel(\"Predicted Label\", fontsize= 12)\nplt.ylabel(\"True Label\", fontsize= 12)\nplt.title(\"Confusion Matrix for Enchanced XGB\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:28.709025Z","iopub.execute_input":"2021-07-20T19:05:28.709388Z","iopub.status.idle":"2021-07-20T19:05:28.995397Z","shell.execute_reply.started":"2021-07-20T19:05:28.709354Z","shell.execute_reply":"2021-07-20T19:05:28.994492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred_xgbe, labels = [0, 1]))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:28.996666Z","iopub.execute_input":"2021-07-20T19:05:28.997179Z","iopub.status.idle":"2021-07-20T19:05:29.011156Z","shell.execute_reply.started":"2021-07-20T19:05:28.997127Z","shell.execute_reply":"2021-07-20T19:05:29.009521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbe_pred_proba = xgbenhanced.predict_proba(X_test)[:,1]\n\nxgbe_roc_auc = metrics.roc_auc_score(y_test, xgbe_pred_proba)\nprint('ROC_AUC: ', xgbe_roc_auc)\n\nxgbe_fpr, xgbe_tpr, thresholds = metrics.roc_curve(y_test, xgbe_pred_proba)\n\nplt.plot(xgbe_fpr,xgbe_tpr, label = 'ROC_AUC = %0.3f' % xgbe_roc_auc)\n\nplt.xlabel(\"False Positive Rate\", fontsize= 12)\nplt.ylabel(\"True Positive Rate\", fontsize= 12)\nplt.legend(loc=\"lower right\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:05:29.012703Z","iopub.execute_input":"2021-07-20T19:05:29.013054Z","iopub.status.idle":"2021-07-20T19:05:29.191156Z","shell.execute_reply.started":"2021-07-20T19:05:29.01302Z","shell.execute_reply":"2021-07-20T19:05:29.190157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explainability","metadata":{}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(grad_enh)\nshap_values = explainer.shap_values(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:06:45.423475Z","iopub.execute_input":"2021-07-20T19:06:45.424188Z","iopub.status.idle":"2021-07-20T19:06:45.536447Z","shell.execute_reply.started":"2021-07-20T19:06:45.424122Z","shell.execute_reply":"2021-07-20T19:06:45.53545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_test, plot_type=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-07-20T19:06:46.474512Z","iopub.execute_input":"2021-07-20T19:06:46.475152Z","iopub.status.idle":"2021-07-20T19:06:46.830712Z","shell.execute_reply.started":"2021-07-20T19:06:46.475097Z","shell.execute_reply":"2021-07-20T19:06:46.829954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finaly we can use the Shap explainer library to extract the impact that each feature had on the model ouptut. \\\nThis insight in my opinion is as valuable as the predictions themselves since it can provide recomendations to the telecom carriers for future offers to avoid churning. \\\nFor example since contract, tenure and Monthly charges are the biggest drivers of the churning behaviour they should be prioritized and reexamined in the current offers by offering Lower Monthly Charges to Month to Month subscribers so they can stay longer and the lower individual payments will be subsedized by additional months before churning. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}