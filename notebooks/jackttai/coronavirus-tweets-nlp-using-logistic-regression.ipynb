{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Coronavirus tweets NLP - Text Classification\n\n- (2020/12) I am a self-taught learner of data science and finished my NLP online course. Try to apply what i have learnt to this project.\n\n### Corona Virus Tagged Data\n\nData from:https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n\n\nPerform Text Classification on the data. The tweets have been pulled from Twitter and manual tagging has been done then.\nThe names and usernames have been given codes to avoid any privacy concerns.\n\n\nColumns in Data:\n- Location\n- Tweet At\n- Original Tweet\n- Label"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the tools \nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Read the train and test file \ntrain_df = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='ISO-8859-1', parse_dates=['TweetAt'])\ntest_df = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding='ISO-8859-1',parse_dates=['TweetAt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check missing data\ntrain_df.isnull().sum(), train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we combined the train_df and test_df into one dataframe for preprocessing\n\n# Create new column to identify the test data\ntrain_df['is_test'] = 0\ntest_df['is_test'] = 1\n\n# combine \ncomp_df = pd.concat([train_df, test_df])\ncomp_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data EDA and formatting"},{"metadata":{},"cell_type":"markdown","source":"### Grouping the labels to positive(2), negative(0) and neutral (1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a look on the target features\ncomp_df.Sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_df['Sentiment'] = comp_df.Sentiment.str.replace('Extremely Positive', 'Positive')\ncomp_df['Sentiment'] = comp_df.Sentiment.str.replace('Extremely Negative', 'Negative')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_df.Sentiment.value_counts().plot.bar(figsize=(7,4))\nplt.xticks(rotation=None)\nplt.title('Number of tweets in different sentiments',fontsize=12)\nplt.xlabel('Number of tweets', fontsize=12)\nplt.ylabel('Sentiment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map the sentiment into 0 , 1, 2\ncomp_df['Sentiment'] = comp_df['Sentiment'].map({'Positive':2, 'Negative':0, 'Neutral':1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get the month of the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_df['month'] = comp_df['TweetAt'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual the date with the labels\npd.crosstab(comp_df.month, comp_df.Sentiment).plot.bar()\nplt.ylabel('Number of tweets')\nplt.xticks(rotation=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop the other columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this task we will focus on the text data only, so we drop the other columns\ncomp_df = comp_df[['OriginalTweet','Sentiment','is_test']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modify the tweet contents"},{"metadata":{},"cell_type":"markdown","source":"#### Have a look on the  tweets's content"},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_df['OriginalTweet'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The elements we would like to remove from the tweet's content\n\n- URL\n- punctuations\n- \\# tags\n- @ tags\n- extra space"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change columns name for easy access\ncomp_df.columns =['tweet','label','is_test']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove @ tags\ncomp_df.tweet = comp_df.tweet.str.replace(r'(@\\w*)','')\n\n#Remove URL\ncomp_df.tweet = comp_df.tweet.str.replace(r\"http\\S+\", \"\")\n\n#Remove # tag\ncomp_df.tweet = comp_df.tweet.str.replace(r'#\\w+',\"\")\n\n#Remove all non-character\ncomp_df.tweet = comp_df.tweet.str.replace(r\"[^a-zA-Z ]\",\"\")\n\n# Remove extra space\ncomp_df.tweet = comp_df.tweet.str.replace(r'( +)',\" \")\ncomp_df.tweet = comp_df.tweet.str.strip()\n\n# Change to lowercase\ncomp_df.tweet = comp_df.tweet.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_df.tweet[60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenize and Lemmatize the word in data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new columns for storing\ncomp_df['corpus'] = [nltk.word_tokenize(text) for text in comp_df.tweet]\nlemma = nltk.WordNetLemmatizer()\ncomp_df.corpus = comp_df.apply(lambda x: [lemma.lemmatize(word) for word in x.corpus], axis=1)\ncomp_df.corpus = comp_df.apply(lambda x: \" \".join(x.corpus),axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize the text data using wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = comp_df.corpus.values\nwordcloud = WordCloud(max_words=500,background_color='white', stopwords=stop_words, colormap='rainbow',height=300)\nwordcloud.generate(str(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.set_figheight(6)\nfig.set_figwidth(10)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the tools we need\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data back to train and test set\ntrain_df = comp_df[comp_df.is_test==0]\ntest_df = comp_df[comp_df.is_test==1]\ntrain_df.drop('is_test',axis=1, inplace=True)\ntest_df.drop('is_test',axis=1, inplace=True)\ntest_df.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data in X and y dataset\n\nx_df = train_df.corpus\ny_df = train_df['label']\n\nx_test = test_df.corpus\ny_test =test_df['label']\n\n# Split to train and validation\nx_train, x_val, y_train, y_val = train_test_split(x_df,y_df, test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the shape\nx_train.shape, x_val.shape, y_train.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the vectorizer\nvectorizer = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=5).fit(comp_df.corpus)\n\n# transform both train and valid data\nx_train_vector = vectorizer.transform(x_train)\nx_val_vector = vectorizer.transform(x_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Start training models\n- Logistric Regression\n- Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(LogisticRegression(random_state=42), x_train_vector, y_train, cv=10, verbose=1, n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(MultinomialNB(alpha=0.01), x_train_vector, y_train, cv=10, verbose=1, n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=42).fit(x_train_vector, y_train)\nprint(classification_report(y_val, model.predict(x_val_vector)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The performance of logistric regression is better, now try to tune the hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#params = {\n    #'solver':['liblinear','saga','newton-cg','lbfgs'],\n   # 'C':[0.001,0.01,0.1,1,10,100],\n    # 'penalty':['l1','l2']\n}\n\n#lr_grid = GridSearchCV(LogisticRegression(random_state=42),params, cv=5, verbose=2, n_jobs=-1)\n#lr_grid.fit(x_train_vector, y_train)\n\n#print(classification_report(y_val, lr_grid.predict(x_val_vector)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use tf-idf as vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1,2),stop_words='english').fit(comp_df.corpus)\n\nx_train_tf = vectorizer.transform(x_train)\nx_val_tf = vectorizer.transform(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the top 20 words \nfeature_weight = x_train_tf.sum(axis=0).tolist()[0]\nfeatures = pd.DataFrame(feature_weight)\nfeatures.index =  list(vectorizer.get_feature_names())\nfeatures.sort_values(by=[0],ascending=False).head(30).plot.barh(figsize=(20,10))\nplt.xlabel('Weight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model training\n- logistric regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=42).fit(x_train_tf,y_train)\nprint(classification_report(y_val, model.predict(x_val_tf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameters tuning using gridsearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"#params = {\n    #'solver':['liblinear','saga','newton-cg','lbfgs'],\n    #'C':[0.001,0.01,0.1,1,10,100],\n    #'penalty':['l1','l2']\n}\n\n#lr_grid02 = GridSearchCV(LogisticRegression(random_state=42),params, cv=10, verbose=2, n_jobs=-1)\n#lr_grid02.fit(x_train_tf, y_train)\n\n#print(classification_report(y_val, lr_grid02.predict(x_val_tf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lr_grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To skip the training time on kaggle, I use the best parameter found in my notebook directly\nbest_model = LogisticRegression(C=1, penalty='l1', random_state=42, solver='saga')\nbest_model.fit(x_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best model performance on validation dataset\nprint(classification_report(y_val, best_model.predict(x_val_tf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now do prediction on the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_tf = vectorizer.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model.predict(x_test_tf)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Heat map of the prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6,4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d',annot_kws={'size':17}, cmap='Reds')\nplt.ylabel('True')\nplt.xlabel('Predicted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you very much"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}