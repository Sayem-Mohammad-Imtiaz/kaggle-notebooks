{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WARNING\n**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**","metadata":{}},{"cell_type":"markdown","source":"# Preliminaries\nWrite requirements to file, anytime you run it, in case you have to go back and recover dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n\nRequirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp","metadata":{}},{"cell_type":"code","source":"!pip freeze > kaggle_image_requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read and Preprocess Fake News Data\n\nThe data preprocessing steps are the same as those in sections 4.2/4.4\n\nRead in the \"true\" and \"fake\" data\n\nIn quotes, because that has the potential to simply replicate the biases of the labeler, so should be carefully evaluated","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Read the data into pandas DataFrames\nDataTrue = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nDataFake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\n\nprint(\"Data labeled as True:\")\nprint(DataTrue.head())\nprint(\"\\n\\n\\nData labeled as Fake:\")\nprint(DataFake.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assemble the two different kinds of data (1000 samples from each of the two classes into a Pandas DataFrame)","metadata":{}},{"cell_type":"code","source":"Nsamp =1000 # number of samples to generate in each class - 'true', 'fake'\nDataTrue = DataTrue.sample(Nsamp)\nDataFake = DataFake.sample(Nsamp)\nraw_data = pd.concat([DataTrue,DataFake], axis=0)\n\n# corresponding labels\nCategories = ['True','False']\nheader = ([1]*Nsamp)\nheader.extend(([0]*Nsamp))\n\n# put label in the same dataframe as a new column, drop any extra columns\nraw_data['label'] = header\nraw_data = raw_data.drop(['subject','date','title'],axis=1)\n\n# shuffle it\nraw_data = raw_data.sample(frac=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.head() # take a peek","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do not need to split into train and test sets when using v2 of fast.ai, as we will see next...","metadata":{}},{"cell_type":"markdown","source":"# ULMFiT Experiments\n\nImport the fast.ai library, written by the ULMFiT authors","metadata":{}},{"cell_type":"code","source":"from fastai.text.all import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TextDataLoaders Class for Language Model/Task Classifier Consumption","metadata":{}},{"cell_type":"markdown","source":"Data from data frames in fast.ai v2 is consumed using the TextDataLoaders.from_df class/method. Construct an instance of this class, observing that it will do the train/validation data split for us:","metadata":{}},{"cell_type":"code","source":"data_lm = TextDataLoaders.from_df(raw_data, is_lm=True, valid_pc = 0.3, path = \"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Construct an instance of this object for task-specific classifier consumption.","metadata":{}},{"cell_type":"markdown","source":"## Fine-Tune Language Model\n\nIn ULMFiT, language models are trained using the *language_model_learner* class. \n\nWe initialize an instance of this class, opting to go with ASGD Weight-Dropped LSTM (AWD_LSTM) model architecture. This is just the usual LSTM with some weights randomly set to 0, analogously to what is done to activations in Dropout layers. More info can be found here - https://docs.fast.ai/text.models.awdlstm","metadata":{}},{"cell_type":"code","source":"learn = language_model_learner(data_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=\"\", wd=0.3, pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the initialization of this model also loads weights pretrained on the Wikitext 103 benchmark dataset (The WikiText Long Term Dependency Language Modeling Dataset - https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/).\n\nWe can find a suggested maximum learning rate using the following commands. Instead of selecting the lowest point on the curve, note that the chosen point is where the curve is changing the fastest.","metadata":{}},{"cell_type":"code","source":"rate,lr_steep = learn.lr_find() # find best learning rate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The optimal rate (where graph above is changing fastest) is:\")\nprint(rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fine-tune using slanted trangular learning rates for one epoch, which are already built into the fit_one_cycle() method in fast.ai","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(1, rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discriminative Fine-Tuning\n\nThe call *learn.unfreeze()* makes all the layers trainable. We can use the *slice()* function to train the last layer at a specified rate, while the layers below will have reducing learning rates. We set the lower bound of the range at two orders of magnitude smaller, i.e., divide the maximum rate by 100.","metadata":{}},{"cell_type":"code","source":"learn.unfreeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(1, slice(rate/100,rate))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the accuracy slightly increased!\n\nWe can use the resulting language model to predict some words in a sequence using the following command (predicts next 10 words)","metadata":{}},{"cell_type":"code","source":"learn.predict(\"This is news about\", n_words=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plausible!\n\nSave the fine-tuned language model!","metadata":{}},{"cell_type":"code","source":"learn.save_encoder('fine-tuned_language_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Task Classifier Fine-tuning\n\nRecall that data from data frames in fast.ai v2 is consumed using the TextDataLoaders.from_df class/method. Construct an instance of this class for task classifier fine-tuning, observing that it will do the train/validation data split for us.\n\nWe use the same settings as the language model we fine-tuned above, so we can load that fine-tuned model without issues. We also load the fine-tuned language model into the instance below.","metadata":{}},{"cell_type":"code","source":"data_clas = TextDataLoaders.from_df(raw_data, valid_pc = 0.3, text_vocab=data_lm.vocab, bs=32)\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3, metrics=accuracy) # use the same settings as the language model we fine-tuned, so we can load without problems\nlearn.load_encoder('fine-tuned_language_model') # load saved encoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Figure out the learning best rate as before.","metadata":{}},{"cell_type":"code","source":"rate,lr_steep = learn.lr_find() # find best learning rate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the fake news classifier","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(1, rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A good score is achieved!","metadata":{}},{"cell_type":"markdown","source":"### Gradual Unfreezing\nThe idea is to keep the initial layers of model as untrainable in the beginning, slowly decreasing how many are untrainable as the training process proceeds.\n\nWe can use the following command to only unfreeze the last layer:","metadata":{}},{"cell_type":"code","source":"learn.freeze_to(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the following command to only unfreeze the last two layers","metadata":{}},{"cell_type":"code","source":"learn.freeze_to(-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, gradual unfreezing to a depth=2 would involve doing something like this:","metadata":{}},{"cell_type":"code","source":"depth = 2\nfor i in range(1,depth+1): # freeze progressively fewer layers, up to a depth of 2, training for one cycle each time\n    learn.freeze_to(-i)\n    learn.fit_one_cycle(1, rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results improve, and speak for themselves!","metadata":{}}]}