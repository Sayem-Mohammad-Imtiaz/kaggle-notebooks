{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WARNING\n**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**\n\nNote that this version of the notebook uses fast.ai version 1. For version 2 code, please see https://www.kaggle.com/azunre/tlfornlp-chapter9-ulmfit-adaptation-fast-aiv2\n\nAlso note that while fast.ai version 2 documentation is available at https://docs.fast.ai/, the fast.ai version 1 documentation is available at https://fastai1.fast.ai/ ","metadata":{}},{"cell_type":"markdown","source":"# Preliminaries\nWrite requirements to file, anytime you run it, in case you have to go back and recover dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n\nRequirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp","metadata":{}},{"cell_type":"code","source":"!pip freeze > kaggle_image_requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read and Preprocess Fake News Data\n\nThe data preprocessing steps are the same as those in sections 4.2/4.4\n\nRead in the \"true\" and \"fake\" data\n\nIn quotes, because that has the potential to simply replicate the biases of the labeler, so should be carefully evaluated","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Read the data into pandas DataFrames\nDataTrue = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nDataFake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\n\nprint(\"Data labeled as True:\")\nprint(DataTrue.head())\nprint(\"\\n\\n\\nData labeled as Fake:\")\nprint(DataFake.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assemble the two different kinds of data (1000 samples from each of the two classes)","metadata":{}},{"cell_type":"code","source":"Nsamp =1000 # number of samples to generate in each class - 'true', 'fake'\nDataTrue = DataTrue.sample(Nsamp)\nDataFake = DataFake.sample(Nsamp)\nraw_data = pd.concat([DataTrue,DataFake], axis=0).values\n\n# combine title, body text and topics into one string per document\n#raw_data = [sample[0].lower() + sample[1].lower() + sample[3].lower() for sample in raw_data]\n\nprint(\"Length of combined data is:\")\nprint(len(raw_data))\nprint(\"Data represented as numpy array (first 5 samples) is:\")\nprint(raw_data[:5])\n\n# corresponding labels\nCategories = ['True','False']\nheader = ([1]*Nsamp)\nheader.extend(([0]*Nsamp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shuffle data, split into train and test sets...","metadata":{}},{"cell_type":"code","source":"# function for shuffling data\ndef unison_shuffle(a, b):\n    p = np.random.permutation(len(b))\n    data = np.asarray(a)[p]\n    header = np.asarray(b)[p]\n    return data, header\n\nraw_data, header = unison_shuffle(raw_data, header)\n\n# split into independent 70% training and 30% testing sets\nidx = int(0.7*raw_data.shape[0])\n\n# 70% of data for training\ntrain_x = raw_data[:idx]\ntrain_y = header[:idx]\n# remaining 30% for testing\ntest_x = raw_data[idx:]\ntest_y = header[idx:]\n\nprint(\"train_x/train_y list details, to make sure it is of the right form:\")\nprint(len(train_x))\n#print(train_x)\nprint(train_y[:5])\nprint(train_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ULMFiT Experiments\n\nImport the fast.ai library, written by the ULMFiT authors","metadata":{}},{"cell_type":"code","source":"from fastai.text import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Bunch Class for Language Model/Task Classifier Consumption","metadata":{}},{"cell_type":"markdown","source":"We prepare train and test/validation dataframes first.","metadata":{}},{"cell_type":"code","source":"train_df = pd.DataFrame(data=[train_y,train_x]).T\ntest_df = pd.DataFrame(data=[test_y,test_x]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check their shape:","metadata":{}},{"cell_type":"code","source":"train_df.shape\ntest_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data in fast.ai is consumed using the *TextLMDataBunch* class. Construct an instance of this class for language model consumption.","metadata":{}},{"cell_type":"code","source":"data_lm = TextLMDataBunch.from_df(train_df = train_df, valid_df = test_df, path = \"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Construct an instance of this object for task-specific classifier consumption.","metadata":{}},{"cell_type":"code","source":"data_clas = TextClasDataBunch.from_df(path = \"\", train_df = train_df, valid_df = test_df, vocab=data_lm.train_ds.vocab, bs=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tune Language Model\n\nIn ULMFiT, language models are trained using the *language_model_learner* class. \n\nWe initialize an instance of this class, opting to go with ASGD Weight-Dropped LSTM (AWD_LSTM) model architecture. This is just the usual LSTM with some weights randomly set to 0, analogously to what is done to activations in Dropout layers. More info can be found here - https://docs.fast.ai/text.models.awdlstm","metadata":{}},{"cell_type":"code","source":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the initialization of this model also loads weights pretrained on the Wikitext 103 benchmark dataset (The WikiText Long Term Dependency Language Modeling Dataset - https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). You can see the execution log above for confirmation of this. \n\nWe can find a suggested maximum learning rate using the following commands. Instead of selecting the lowest point on the curve, note that the chosen point is where the curve is changing the fastest.","metadata":{}},{"cell_type":"code","source":"learn.lr_find() # find best rate\nlearn.recorder.plot(suggestion=True) # plot it","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fetch the optimal rate as follows.","metadata":{}},{"cell_type":"code","source":"rate = learn.recorder.min_grad_lr\nprint(rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fine-tune using slanted trangular learning rates, which are already built into the *fit_one_cycle()* method in fast.ai","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(1, rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discriminative Fine-Tuning\n\nThe call *learn.unfreeze()* makes all the layers trainable. We can use the *slice()* function to train the last layer at a specified rate, while the layers below will have reducing learning rates. We set the lower bound of the range at two orders of magnitude smaller, i.e., divide the maximum rate by 100.","metadata":{}},{"cell_type":"code","source":"learn.unfreeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(1, slice(rate/100,rate))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the accuracy slightly increased!\n\nWe can use the resulting language model to predict some words in a sequence using the following command (predicts next 10 words)","metadata":{}},{"cell_type":"code","source":"learn.predict(\"This is a news article about\", n_words=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plausible!\n\nSave the fine-tuned language model!","metadata":{}},{"cell_type":"code","source":"learn.save_encoder('fine-tuned_language_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Task Classifier Fine-tuning\n\nIn ULMFiT, target task classifier fine-tuning is carried out using the *text_classifier_learner* class. Recall that the target task here is predicting whether a given article is \"fake news\" or not.\n\nWe instantiate it below, using the same settings as the language model we fine-tuned above, so we can load that fine-tuned model without issues. We also load the fine-tuned language model into the instance below.","metadata":{}},{"cell_type":"code","source":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3) # use the same settings as the language model we fine-tuned, so we can load without problems\nlearn.load_encoder('fine-tuned_language_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Figure out the learning best rate as before.","metadata":{}},{"cell_type":"code","source":"learn.lr_find() # find best rate\nlearn.recorder.plot(suggestion=True) # plot it","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rate = learn.recorder.min_grad_lr\nprint(rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the fake news classifier","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(1, rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A nearly perfect score is achieved!","metadata":{}},{"cell_type":"markdown","source":"### Gradual Unfreezing\nThe idea is to keep the initial layers of model as untrainable in the beginning, slowly decreasing how many are untrainable as the training process proceeds.\n\nWe can use the following command to only unfreeze the last layer:","metadata":{}},{"cell_type":"code","source":"learn.freeze_to(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the following command to only unfreeze the last two layers","metadata":{}},{"cell_type":"code","source":"learn.freeze_to(-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, gradual unfreezing to a depth=2 would involve doing something like this:","metadata":{}},{"cell_type":"code","source":"depth = 2\nfor i in range(1,depth+1): # freeze progressively fewer layers, up to a depth of 2, training for one cycle each time\n    learn.freeze_to(-i)\n    learn.fit_one_cycle(1, rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we actually achieved the perfect score here! These results speak for themselves!","metadata":{}}]}