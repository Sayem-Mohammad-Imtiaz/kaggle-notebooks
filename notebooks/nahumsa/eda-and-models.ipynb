{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA and simple baselines \n\nIn this notebook I will do the Exploratory Data Analysis of this dataset and create some baselines with Linear Regression, Random Forest, Logistic regression, Gradient Boost, and XGBoost."},{"metadata":{},"cell_type":"markdown","source":"# 0. Importing dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Defining plotting style\nsns.set(style=\"ticks\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. EDA"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Getting the data\nFirst, let's create a dataframe from the `.csv` file."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Checking statistics and correlations of the data"},{"metadata":{},"cell_type":"markdown","source":"Let's check the statistics and see if there is any null values in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not df.isnull().values.any():\n    print(\"No missing values in the data.\")\nelse: \n    print(\"There is missing values in the data, you need to preprocess those values.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Pregnancies\", y=\"Insulin\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to visualize the correlation of variables, let's do a pair plot using seaborn! I choose to do only the lower triangular "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue=\"Outcome\", markers=[\"o\", \"s\"], corner=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the pairplot, we see that there is some data with Blood Pressure = 0, which seems odd... Let's look it further."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.BloodPressure.dropna());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysing this, we see that the it is higly likely that null values are written as 0, thus we can remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['BloodPressure'] == 0].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there is 35 counts that doesn't have data for insulin and blood pressure, since those are important factors for diabetes, I choose to delete this data since they seem to be out of place. Also we shall delete other data points which have 0 as the value, such as BMI and Glucose. Let's analyze the distributions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, figsize=(20,10))\nsns.distplot(df.Glucose, ax = axs[0])\nsns.distplot(df.BMI, ax = axs[1])\nsns.distplot(df.Insulin, ax = axs[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So analysing those plots, we see that we can discart the 0 values for Glucose and BMI, because they are probably null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df[df['BloodPressure'] != 0]\ndf_clean = df_clean[df_clean['BMI'] != 0]\ndf_clean = df_clean[df_clean['Glucose'] != 0]\ndf_clean.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Creating a training/test split\n\nNow we create a training/test split in order to see how our model works for unseen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df_clean.drop(\"Outcome\", axis=1)\ny = df_clean[\"Outcome\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Creating models\n\nLet's create some models to see which one works better."},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'\n      % r2_score(y_test, diabetes_y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's consider y_pred > 0.5 as pacients with diabetes and y_pred < 0.5 as pacients without diabetes and evaluate our accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (diabetes_y_pred > 0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Accuracy: {np.around(sum(y_pred == y_test)/len(y_test)*100,1)}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2 Evaluating the model using ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\ndef roc(y_test, y_pred, model_name, title=\"ROC\"):\n    \"\"\"Creates and plots the roc for a model.\n    \"\"\"\n    \n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr)\n    lw = 2\n    plt.plot(fpr, tpr,\n             lw=lw, label=f'{model_name} ROC curve area = {roc_auc:0.2f}')\n    plt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=0)\nrandomforest.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_RF = randomforest.predict_proba(X_test)\nprint(f\"Accuracy: {np.around(sum(np.argmax(y_pred_RF, axis=1) == y_test)/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_RF[:,1], \"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\n\nModel = LogisticRegression();\n\n# Let's rescale the data\nX_scaled = preprocessing.scale(X_train)\nModel.fit(X_scaled, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scale_test = preprocessing.scale(X_test)\ny_pred_Log = Model.predict_proba(X_scale_test)\nprint(f\"Accuracy: {np.around(sum(np.argmax(y_pred_Log, axis=1) == y_test)/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_Log[:, 1], \"Logistic regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nGB = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.05, max_features=3, max_depth = 10, random_state = 0)\nGB.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_GB = GB.predict_proba(X_test)\nprint(f\"Accuracy: {np.around(sum(np.argmax(y_pred_GB, axis=1) == y_test)/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_GB[:,1], \"Gradient Boost\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.5 XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor, XGBClassifier\n\n# Define the model\nXGBR = XGBRegressor(n_estimators=1000, learning_rate=0.05) # Your code here\nXGBC = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n\n# Fit the model\nXGBR.fit(X_train, y_train,\n               early_stopping_rounds=5,\n              eval_set=[(X_test, y_test)],\n              verbose=0)\n\nXGBC.fit(X_train, y_train,\n               early_stopping_rounds=5,\n              eval_set=[(X_test, y_test)],\n              verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_XGBC = XGBC.predict_proba(X_test)\nprint(f\"Classifier Accuracy: {np.around(sum(np.argmax(y_pred_XGBC, axis=1) == y_test)/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_XGBC[:, 1], \"Gradient Boost Classifier\")\n\ny_pred_XGBR = XGBR.predict(X_test)\ny_pred = (y_pred_XGBR > 0.5).astype(int)\nprint(f\"Regressor Accuracy: {np.around(sum(y_pred == y_test)/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_XGBR, \"Gradient Boost Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing all models\n\nWe see that all the models have similar ROC curves and the one that have the best accuracy and ROC curve area is the XGBoost Regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"roc(y_test, diabetes_y_pred, \"Linear regression\")\nroc(y_test, y_pred_RF[:,1], \"Random Forest\")\nroc(y_test, y_pred_Log[:, 1], \"Logistic regression\")\nroc(y_test, y_pred_GB[:,1], \"Gradient Boost\")\nroc(y_test, y_pred_XGBR, \"XGBoost Regressor\")\nroc(y_test, y_pred_XGBC[:, 1], \"XGBoost Classifier\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If you like this notebook, please upvote! :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}