{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n### 1.1  Importing necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 messages intuition\nsms messages classification as spam or not spam using uci data set from our intuition about spam sms we usuallly know that they contain url about the offer or concerned website. SMS spam messages are usually bulk messages containing short a short description, url, an offer/deal which is targeting a large set of users so the messages are generally short;"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing sms data set\nsms = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding='latin-1')\nsms.head()\n\nsms = sms.rename(columns={\"v1\":\"label\", \"v2\":\"message\"})\nsms.head()\nsms.describe()\nsms.groupby('label').describe()\nsms = sms.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.describe()\nsms.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3 priliminary examination of the sms\nwe can infer that there are 747 spam messages and  4825 not spam messages and some messages have repeated more than once\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Dropping duplicate sms in both spam and not spam"},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.drop_duplicates(inplace=True)\nsms.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.5 after dropping duplicate messages\n\nthe number of not spam messages dropped from 4825 to 4516 and and spam messages from 747 to 653\n\nfor further analysis from here we have more not spam messages than spam messages\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 1.6 To check if all messages have appropriate labels\n\nIt is necessary to check if all the 5169 messages have appropriate labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"countspam=0\ncountham=0\nunlabelled=0\nfor x in sms['label']:\n    if x=='spam': \n        countspam=countspam+1\n    elif x=='ham':\n        countham=countham+1\n    else:\n        unlabeled=unlabeled+1\nprint(\"Total number of spam messages\",countspam)        \nprint(\"Total number of ham messages\",countham)  \nprint(\"Total number of unlabelled messages\",unlabelled)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.7 Length of messages in both spam and not spam\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['length'] = sms['message'].apply(len)\nsms.head();\nplt.title('Histogram distribution of the spam and not spam messages')\nsms['length'].hist(bins=60,figsize=(12,5))\nplt.xlim(-40,900)\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()\n\n\n\nsms['length'].hist(by=sms['label'],bins=60,figsize=(12,5))\nplt.xlim(-40,900);\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.7.1 Inference from the length of sms\nfrom the histogram distribution of both spam and not spam messages we can conclude that majority of the spam messages have a greater length. we can at this stage also conclude that if a message has a length greater than 150 we can infer that the sms will be a spam, but the accuracy of our ML model will be very less; hence for further analysis we should clean and preprocess the data;\n\nIt is important here to note that we calculated the total number of characters in the entire message that is if a message is \"i'll call you back\" thaen the total length of the message 18. But language is not character that includes spaces, letters but word as a whole, hence lets split the sentence snd find the exact number of words in a given message."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find number of words in the message\ndef word_counter(msg):\n    no_words = sum([i.isalpha() for i in msg.split()])\n    return no_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['word count']=sms['message'].apply(word_counter)\n\n\nplt.title('Histogram distribution of number of words in both spam and not spam messages')\nsms['length'].hist(bins=100,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()\n\n\n\nsms['word count'].hist(by=sms['label'],bins=50,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.7.2 Inference from the length of sms\n"},{"metadata":{},"cell_type":"markdown","source":"1. From the above graph we can clearly make out that ham(not spam) messages are primarily having less number of words from the graph word count between 0-20 have a higher frequency and usually simple short messages;\n2. But spam messages have a lot of words beacause from the graph we can see that a lot of messages have a word count between 12-22 and have higher frequency in this interval."},{"metadata":{},"cell_type":"markdown","source":"### 1.8.1 Binarising labels has ham(not spam) as 0 and spam as 1\n\nrepresenting spam and not Spam(ham) as 1 and 0 makes it efficient for classifing messages"},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['labels'] = sms['label'].map({'ham': 0, 'spam': 1})\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.0 SMS Cleaning and Wrangling\n\nby simply going through the sms we see that they contain a lot of puntations, words which hinder the learning capabilities of the ML;\nhence it is necessary to clean the sms;\nto clean the sms:\n1. remove punctuation using python's string \n2. remove word like 'him', 'and' which do not add much meaning to the message using stop words from nltk\n3. re\n"},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 Delete punctuations by using python string"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to delete punctuations by python string \nprint('List of supported punctuations',string.punctuation)\ndef delete_punctuation(msg):\n    new_msg=''.join([p for p in msg if p not in string.punctuation])\n    return new_msg\nsms['del_p_message']=sms['message'].apply(delete_punctuation)\n\nsms['del_p_length'] = sms['del_p_message'].apply(len)\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.1 Visulizing sms in terms of length before and after removing punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Histogram distribution of total characters spam and not spam messages after deleting punctuations')\nsms['del_p_length'].hist(bins=250,figsize=(12,5))\nplt.xlim(-40,900)\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()\n\n\nsms['del_p_length'].hist(by=sms['label'],bins=200,figsize=(12,5))\nplt.xlim(-40,900);\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['words after removing punct.']=sms['del_p_message'].apply(word_counter)\n\n\nplt.title('Histogram distribution of number of words in both spam and not spam messages after removing punctuation')\nsms['words after removing punct.'].hist(bins=100,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()\n\n\n\nsms['words after removing punct.'].hist(by=sms['label'],bins=50,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Cleaning SMS by using STOP WORDS which are not informative enough to classify as spam or not spam"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords.words(\"english\")\n\n# definie function to delete stop words\ndef delete_stopwords(msgp):\n    msg_no_stopwords=' '.join([x for x in msgp.split() if x.lower() not in stopwords.words(\"english\")])\n    \n    return msg_no_stopwords\n\nsms['del_SW_message']=sms['del_p_message'].apply(delete_stopwords)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['del_SW_length']=sms['del_SW_message'].apply(len)\nsms['del_SW_message'].head()\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.1 SMS cleaning and wrangling Visulization and comparing histogram with Punctuation removed words\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Histogram distribution of the spam and not spam messages after deleting Stop words')\nsms['del_SW_length'].hist(bins=50,figsize=(12,5))\nplt.xlim(-40,900)\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()\n\n\nsms['del_SW_length'].hist(by=sms['label'],bins=60,figsize=(12,5))\nplt.xlim(-40,400);\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()\n\nsms['del_SW_length'].hist(bins=150,alpha=0.8,figsize=(12,5),label='no stopwords')\nsms['del_p_length'].hist(bins=150,alpha=0.5,figsize=(12,5),label='punctuation length')\nsms['length'].hist(bins=150,alpha=0.2,figsize=(12,5),label='true length')\nplt.legend(loc='upper right')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['words after removing SW']=sms['del_SW_message'].apply(word_counter)\n\n\nplt.title('Histogram distribution of number of words in both spam and not spam messages after removing STOPWORDS')\nsms['words after removing SW'].hist(bins=100,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()\n\n\n\nsms['words after removing SW'].hist(by=sms['label'],bins=50,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3 SMS word visulization using wordcount"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pictorially represent spam and not spam words using WordCloud\nfrom wordcloud import WordCloud\nspam_words = ' '.join(list(sms[sms['labels'] == 1]['message']))\nspam_wc = WordCloud(width = 512,height = 512).generate(spam_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(spam_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()\n\nnotspam_words = ' '.join(list(sms[sms['labels'] == 0]['message']))\nnotspam_wc = WordCloud(width = 512,height = 512).generate(notspam_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(notspam_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"#### 2.5 Stemming Words\nStemming is the process of giving a similar meaning to the words in a given sentence;\nword like programming, programmed, programmer, program can be collectively called program"},{"metadata":{},"cell_type":"markdown","source":"from nltk.stem import SnowballStemmer \nstemmer= SnowballStemmer(\"english\")\ndef stemming(msg):\n    stemmed_sentence =' '.join([x for x in msg in stemmer.stem(x)])\n    return stemmed_sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import SnowballStemmer \nstemmer= SnowballStemmer(\"english\")\ndef stemming(text):\n    text = text.split()\n    words = \"\"\n    for i in text:\n            stemmer = SnowballStemmer(\"english\")\n            words =words + (stemmer.stem(i))+\" \"\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['stemmed_message']=sms['del_SW_message'].apply(stemming)\nsms['stem_message_length']= sms['stemmed_message'].apply(len)\nsms['stemmed_word_length']= sms['stemmed_message'].apply(word_counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.5.1 Visulization of message length after removing stem words with histogram "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Histogram distribution of the spam and not spam messages after Stemming')\nsms['stem_message_length'].hist(bins=150,figsize=(12,5))\nplt.xlim(-40,700)\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()\n\n\nsms['stem_message_length'].hist(by=sms['label'],bins=150,figsize=(12,5))\nplt.xlim(-40,700);\nplt.xlabel('length')\nplt.ylabel('frequency')\nplt.show()\n\n\nsms['length'].hist(bins=150,alpha=0.4,figsize=(12,5),label='true length msg')\nsms['del_SW_length'].hist(bins=150,alpha=0.5,figsize=(12,5),label='no stopwords msg')\nsms['stem_message_length'].hist(bins=150,alpha=0.6,figsize=(12,5),label='stem msg length')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.5.2 Visulization of Number of words after removing stem words with histogram "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Histogram distribution of number of words in both spam and not spam messages after stemming')\nsms['stemmed_word_length'].hist(bins=100,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()\n\n\n\nsms['stemmed_word_length'].hist(by=sms['label'],bins=50,figsize=(12,5))\nplt.xlabel('No. of words')\nplt.ylabel('frequency')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.5.3 Inference from Visulization of Number of words after removing stem words with histogram \nFrom the above graph we see that the number of words before and after doing preprocessing and cleaning the number of words in spam are much higher in the interval 10 - 20 and in the ham(not spam) the number of words in the interval 0-20 are higher\nhence we can conclude that word count is a feature which can be further used in our ML model."},{"metadata":{},"cell_type":"markdown","source":"### 2.6 Tokenizing and Vectorizing words\nTokenizing is a process of giving a unique id to each word our data set;\nit is easier to implement to implement machine learning models on numerical values which map a particular word\nafter Tokenizing the occurence of each word in our data set is calculted\ncertain words repeat a number of times and do not add much value to our dataset hence they have to dealt with by normalizing and weighing them\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()\ncount_v=count_vector.fit_transform(sms['stemmed_message'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.6.1 Term frequency inverse document frequency - TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\nsms_tfidf = TfidfTransformer().fit_transform(count_v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.0 Splitting the words into training set and test test\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting data into training and test data\nfrom sklearn.model_selection import train_test_split\n\nxtrain, xtest, ytrain, ytest=train_test_split(sms_tfidf,sms['labels'], random_state=1)\nprint('Training messages size',xtrain.shape[0])\nprint('Testing messages size',xtest.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.0 Applying Different ML models and tabulating resultsand finding the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#training data using naive bayes from sklearn\n\nfrom sklearn.naive_bayes import MultinomialNB\nnaive_bayes=MultinomialNB()\nnaive_bayes.fit(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing data and making predictions\nprediction=naive_bayes.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#testing data and making predictions\nprediction=naive_bayes.predict(xtest)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate metrics of the algorithm\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nprint('accuracy score ',accuracy_score(ytest,prediction))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1.1 Inference from naive bayes MultinomialNB() model\nwe have a receceived a high accuracy score of 95.82 %. now scale the test and train data with MinMax scaler if it further increases the accuracy of Naive Bayes ML model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nxxtrain=xtrain.A\nxxtest=xtest.A\nscaler = MinMaxScaler()\nscaled_xtrain = scaler.fit_transform(xxtrain)\nscaled_xtest  = scaler.transform(xxtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_bayes_with_scalling = MultinomialNB().fit(scaled_xtrain, ytrain)\npred_NB_scaled = naive_bayes_with_scalling.predict(scaled_xtest)\naccuracy_NB_scalling = accuracy_score(ytest, pred_NB_scaled)\nprint('accuracy score with scalling ',accuracy_NB_scalling)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1.2 Inference from naive bayes MultinomialNB() model after scalling\nwe have a receceived a high accuracy score of 98.37 % after scalling the messages with MinMaxScaler.\nAfter visually studying the influence of number of words in spam and not spam messages let us fit with current model and see if it further increases the accuracy of the Naive Bayes ML model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import  hstack\nsms_stack_len = hstack((sms_tfidf ,np.array(sms['stemmed_word_length'])[:,None])).A\n\nxl_train, xl_test, yl_train, yl_test = train_test_split(sms_stack_len,sms['labels'], random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_bayes_withlength=MultinomialNB()\nnaive_bayes_withlength.fit(xl_train,yl_train)\npred_naive_bayes_withlength=naive_bayes_withlength.predict(xl_test)\nprint('accuracy score with word count feature without scalling ',accuracy_score(yl_test,pred_naive_bayes_withlength))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2_tfidf_train = xl_train\nX2_tfidf_test  =  xl_test\n\nscaler = MinMaxScaler()\nX2_tfidf_train = scaler.fit_transform(X2_tfidf_train)\nX2_tfidf_test  = scaler.transform(X2_tfidf_test)\n\nnaive_bayes_with_sc_len = MultinomialNB().fit(X2_tfidf_train, yl_train)\npred_NB_sc_len = naive_bayes_with_sc_len.predict(X2_tfidf_test)\naccuracy_NB_sc_len = accuracy_score(yl_test, pred_NB_sc_len)\nprint('accuracy score with word count feature with scalling',accuracy_NB_sc_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1.3 Inference from naive bayes MultinomialNB() model after scalling and implementing Number of words\nwe have a receceived an accuracy score of 98.29 % after implementing number of words with the model. we can infer that the number of words almost had negligible effect on the ML model. without including the no of words feature we had a accuracy score of 98.37 %. hence for better results it better to use ML model without considering number of words feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}