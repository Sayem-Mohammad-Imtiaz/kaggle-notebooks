{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel you can find exploratory data analysis done before entering the data into a model. I have tried clubbing few parameters of features based on its similarility and effect on class of mushroom(used countplot, catplot for that). It will reduce the number of features generated after onehot coding done on the dataset. \n\nIn models, I have used Logistic regression, SVM , Random Forest and Xgboost. RF and Xgboost is giving the best result(AUC=1).\n\nIf you like my kernal plz upvote, and if you are reusing my code, kindly quote my kernal.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport shutil\nimport os\nimport pandas as pd\nimport matplotlib\nmatplotlib.use(u'nbAgg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pickle\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing\nimport pandas as pd\nfrom multiprocessing import Process# this is used for multithreading\nimport multiprocessing\nimport codecs# this is used for file operations \nimport random as r\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/mushroom-classification/mushrooms.csv\")\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class']=data['class'].map({'p':0,'e':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['class'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CAP-SHAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='cap-shape',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cap-shape',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-shape'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['cap-shape']=='c']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion:- 1)A mushroom with sunken cap-shape is most likely to be edible. Although the sample size is very less(only 32)to conclude that sunken cap-shape cannot be poisonous.\n2)Highest number of samples collected is of x(convex) and f(flat) and both has 50-50% chance of being poisonous and edible.\n3)bell=b cap shape mushroom(5% of sample) has almost 90% chance of being edible.\n4)K and c are most likly to be poisonous\n\nIdea:\n\n1)Reduce the number of variable. Keep x, f,merge(k+c),merge(b+c). Total 4 variable from 6 .\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-shape']=data['cap-shape'].map({'x':'x','f':'f','k':'KC','c':'KC','b':'BC','c':'BC'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-shape'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# cap-surface\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='cap-surface',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cap-surface',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-surface'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['cap-surface']=='g']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nConclusion:- 1)A mushroom with grooves cap-surface is most likely to be poisonous . Although the sample size is very less(only 4)to conclude that grooves cap-surface cannot be edible. \n\n2)y(scaly) and s(smooth)both has 45% chance of being edible.\n\n3)fibrous=f cap surface mushroom has almost 65% chance of being edible. \n\nIdea:\n1)Merge g with s. This will reduce one varaible(4 samples) and will not have any affect on model.\n2)I was thinking of merging y with s. But I am not sure , as although y and s has same effect towards the edibility it may change the its effect on other variable when entering the model. We can check it later. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-surface']=data['cap-surface'].map({'y':'y','s':'SG','g':'SG','f':'f'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-surface'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# cap-color\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='cap-color',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cap-color',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will merge c+u+r(total 76) with w (count 1040) as w has the highest chance of being edible among all, thus will be able to reduce three colors having lowest count\n\nAlso merge p,b with y.Less chance of being edible."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-color']=data['cap-color'].map({'n':'n','g':'g','e':'e','y':'PBY','p':'PBY','b':'PBY','w':'CURW','c':'CURW','u':'CURW','r':'CURW'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cap-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# bruises\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='bruises',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='bruises',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['bruises'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Presence of bruises increases the chance of the mushroom being edible."},{"metadata":{},"cell_type":"markdown","source":"# odor\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='odor',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='odor',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['odor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the countplot we can observe that p,f,c,y,s,m odor will be surely poisonous. We can club this together.\na and l are surely edible,club it together.n is 95% edible, we will keep it as seperate."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['odor']=data['odor'].map({'n':'n','p':'POI','f':'POI','c':'POI','y':'POI','s':'POI','m':'POI','a':'EDI','l':'EDI'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['odor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# gill-attachment\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='gill-attachment',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='gill-attachment',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gill-attachment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data['gill-attachment']=='a')&(data['class']==1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data['gill-attachment']=='a')&(data['class']==1)]['class'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data['gill-attachment']=='a')]['class'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Edible_percent=data[(data['gill-attachment']=='a')&(data['class']==1)]['class'].sum()/data[(data['gill-attachment']=='a')]['class'].count()*100\nprint(\"The percentage of mushroom which are edible when gill is attached is:\",Edible_percent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If gill attachment is in free mode ,there is a 50% chance of the mushroom to be poisonous. However if the gill is attached then 91% \nof mushroom are edible."},{"metadata":{},"cell_type":"markdown","source":"# gill-spacing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='gill-spacing',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='gill-spacing',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gill-spacing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"close gill-spacing has 40% chance to be edible. However if the spacing is crowded then chances that it is edible is very hight(~95%)"},{"metadata":{},"cell_type":"markdown","source":"# gill-size\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='gill-size',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='gill-size',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gill-size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"gill-size which is broad has 70% chances to be edible whereas narrow gill size has only 10% chances"},{"metadata":{},"cell_type":"markdown","source":"# gill-color\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='gill-color',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='gill-color',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gill-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1)b+r=BR, both has zero probability of edibilty.\n2)o+e+k+n+u=OEKNU, have high probababilty of edibilty\n3)g+h=GH, low probability of around ~30%\n4)w+y=WY,high probability of around ~80%"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gill-color']=data['gill-color'].map({'b':'BR','p':'p','w':'WY','n':'OEKNU','g':'GH','h':'GH','u':'OEKNU','k':'OEKNU','e':'OEKNU','y':'WY','o':'OEKNU','r':\"BR\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gill-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='gill-color',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# stalk-shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='stalk-shape',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='stalk-shape',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-shape'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# stalk-root"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='stalk-root',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='stalk-root',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-root'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-root']=data['stalk-root'].map({'r':'RC','c':'RC','b':'b','?':'missing','e':'e'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-root'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# stalk-surface-above-ring"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='stalk-surface-above-ring',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='stalk-surface-above-ring',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-surface-above-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-surface-above-ring']=data['stalk-surface-above-ring'].map({'s':'SFY','f':'SFY','y':'SFY','k':'k'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-surface-above-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# stalk-surface-below-ring"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='stalk-surface-below-ring',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='stalk-surface-below-ring',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-surface-below-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-surface-below-ring']=data['stalk-surface-below-ring'].map({'s':'SFY','f':'SFY','y':'SFY','k':'k'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-surface-below-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# stalk-color-above-ring"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='stalk-color-above-ring',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='stalk-color-above-ring',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-color-above-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-color-above-ring']=data['stalk-color-above-ring'].map({'e':'EOG','o':'EOG','g':'EOG','n':'NBCY','b':'NBCY','c':'NBCY','y':'NBCY','w':'w','p':'p'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-color-above-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# stalk-color-below-ring"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='stalk-color-below-ring',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='stalk-color-below-ring',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-color-below-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-color-below-ring']=data['stalk-color-below-ring'].map({'e':'EOG','o':'EOG','g':'EOG','n':'BYCN','b':'BYCN','y':'BYCN','c':'BYCN','w':'w','p':'p'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-color-below-ring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# veil-type"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='veil-type',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='veil-type',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(['veil-type'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All values are p , so no impact on model. we can drop the column"},{"metadata":{},"cell_type":"markdown","source":"# veil-color"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='veil-color',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='veil-color',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['veil-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 2.5% values are not w. So dropping it , will not impact our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(['veil-color'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ring-number"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='ring-number',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='ring-number',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ring-number'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ring-number']=data['ring-number'].map({'o':'ON','n':'ON','t':'t'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ring-number'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ring-type"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='ring-type',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='ring-type',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ring-type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ring-type']=data['ring-type'].map({'p':'PF','f':'PF','e':'e','l':'LN','n':'LN'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ring-type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# spore-print-color"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='spore-print-color',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='spore-print-color',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['spore-print-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['spore-print-color']=data['spore-print-color'].map({'o':'OYBU','y':'OYBU','b':'OYBU','u':'OYBU','h':'HR','r':'HR','w':'w','k':'KN','n':'KN'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['spore-print-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# population"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='population',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='population',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['population'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['population']=data['population'].map({'n':'NAC','a':'NAC','c':'NAC','s':'SY','y':'SY','v':'v'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['population'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# habitat"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='habitat',hue='class',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='habitat',y='class',data=data,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['habitat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['habitat']=data['habitat'].map({'g':'GD','d':'GD','u':'UL','l':'UL','m':'MW','w':'MW','p':'p'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['habitat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dum=data.copy()\n#data_dum=data_dum.drop(columns=['class'],axis=0)\ndata_dum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data.columns:\n    if (str(i)!=str('class')):\n        data_dum=pd.get_dummies(data_dum,columns=[i],prefix=[i])\ndata_dum        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saleprice correlation matrix\nk = 50 #number of variables for heatmap\nplt.figure(figsize=(16,8))\ncorrmat = data_dum.corr()\n# picking the top 15 correlated features\ncols = corrmat.nlargest(k, 'class')['class'].index\ncm = np.corrcoef(data_dum[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=data_dum[cols]\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data_out=dataset['class']\ninput_data=dataset.drop(['class'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_y = result['Class']\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nX_train, X_test, y_train, y_test = train_test_split(input_data,Data_out,stratify=Data_out,test_size=0.10)\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train,stratify=y_train,test_size=0.10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of data points in train data:', X_train.shape[0])\nprint('Number of data points in test data:', X_test.shape[0])\nprint('Number of data points in cross validation data:', X_cv.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)#it will count how many 0 and how many 1 present.\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\nprint(\"-\"*10, \"Distribution of output variable in test data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[0])/test_len, \"Class 1: \",int(test_distr[1])/test_len)\nprint(train_distr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    #print(\"Number of misclassified points \",(len(test_y)-np.trace(C))/len(test_y)*100)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n    labels = [0,1]\n    cmap=sns.light_palette(\"green\")\n    # representing A in heatmap format\n    #print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n    #plt.figure(figsize=(10,5))\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    #plt.show()\n\n    #print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n    #plt.figure(figsize=(10,5))\n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    #plt.show()\n    print(\"Sum of columns in precision matrix\",B.sum(axis=0))\n    \n    # representing B in heatmap format\n    #print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n    #plt.figure(figsize=(10,5))\n    plt.subplot(1, 3, 3)\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    plt.show()\n    print(\"Sum of rows in precision matrix\",A.sum(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_len = X_test.shape[0]\ncv_data_len = X_cv.shape[0]\n#print(test_data_len)\n#print(cv_data_len)\ncv_predicted_y = np.zeros((cv_data_len,1))\n#print(cv_predicted_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_probs = np.random.rand(1,2)\n#print(rand_probs)\n#print(sum(sum(rand_probs)))\ncv_predicted_y = ((rand_probs/sum(sum(rand_probs)))[0])\ncv_predicted_y_12 = ((rand_probs/sum(sum(rand_probs))))\n#print(cv_predicted_y)\n#print(cv_predicted_y_12)\n\ncv_predicted_y_1 = np.zeros((cv_data_len,1))\n#print(cv_predicted_y_1)\n#print(y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_len = X_test.shape[0]\ncv_data_len = X_cv.shape[0]\n\n# we create a output array that has exactly same size as the CV data\ncv_predicted_y = np.zeros((cv_data_len,2))\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,2)\n    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_cv,cv_predicted_y, eps=1e-15))\n#print(cv_predicted_y.shape)\n#print(y_cv.shape)\n\n# Test-Set error.\n#we create a output array that has exactly same as the test data\ntest_predicted_y = np.zeros((test_data_len,2))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,2)\n    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n\n\npredicted_y =np.argmax(test_predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)\nprint(\"Scores using the AUC model\",roc_auc_score(y_test,predicted_y))\n\n#print(cv_predicted_y)\n#print(test_predicted_y.shape)\n#print(predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression with hyperparameter tuning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nalpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nprint(\"Scores using the AUC model\",roc_auc_score(y_test,predicted_y))\n\n\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear SVM with hyperparameter tuning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nprint(\"Scores using the AUC model\",roc_auc_score(y_test,predicted_y))\n\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\npredicted_y =np.array(predict_y>0.5,dtype=int)\nprint(\"Total number of data points :\", len(predicted_y))\nprint(\"Scores using the AUC model\",roc_auc_score(y_test,predicted_y))\n\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n# --------------------------------\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(X_train, y_train)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(X_train, y_train)\n        sig_clf_probs = sig_clf.predict_proba(X_cv)\n        cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(y_cv, sig_clf_probs)) \n\n'''fig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i/2)],max_depth[int(i%2)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_cv)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y =np.argmax(predict_y,axis=1)\nprint(\"Scores using the AUC model\",roc_auc_score(y_test,predicted_y))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}