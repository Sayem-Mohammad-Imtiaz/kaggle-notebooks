{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport re\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()#to check is there any null value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()#checking na value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nfeature = df['review'].values\nlabel = df['sentiment'].values\n\nX_train, X_test, y_train, y_test = train_test_split(feature, label, test_size = 0.2)#split the data\n\ny_train = le.fit_transform(y_train)\ny_test = le.fit_transform(y_test)\ndf['sentiment'] = le.fit_transform(df['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#then we try to remove our special character \n\n#only store the char and space\ndef special_chars(text):\n        alphanumeric=\"\"\n        for character in text:\n            if character.isalpha() or character==\" \":\n                alphanumeric += character\n        return alphanumeric\n    \n#change the special char into whitespace\ndef tags(text):\n     return re.compile(r\"<[^>]+>\").sub(\" \", text)\n\n#change the number char into whitespace\ndef num(text):\n     return \"\".join(re.sub(r\"([0â€“9]+)\",\" \",text))\n\n#jalankan fungsi\ndf.review=df.review.apply(lambda x : tags(x))\ndf.review=df.review.apply(lambda x : num(x))\ndf.review=(df.review).apply(special_chars)\n\n#menampilkan data hasil fungsi\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#before we remove the special char\n#A wonderful little production. <br /><br />The...\t\n\n#after we remove the special char \n\n#A wonderful little production The filming te...\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = df['sentiment'] == 1\nneg = df['sentiment'] == 0\ntemp = [pos.sum(),neg.sum()]\nplt.pie(temp, labels = ['positive', 'negative'],autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizer\ntokenizer = Tokenizer(num_words=10000,oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1\n\n#sequencing\ntraining_sequence = tokenizer.texts_to_sequences(X_train)\ntesting_sequence = tokenizer.texts_to_sequences(X_test)\n\n#padding\ntrain_pad_sequence = pad_sequences(training_sequence,maxlen = 500,truncating= 'post',padding = 'pre')\ntest_pad_sequence = pad_sequences(testing_sequence,maxlen = 500,truncating= 'post',padding = 'pre')\nprint('Total Unique Words : {}'.format(len(word_index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip glove.6B.zip.2\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#embedding\nembedded_words = {}\nwith open ('./glove.6B.200d.txt') as file:\n  for line in file:\n    words, coeff = line.split(maxsplit=1)\n    coeff = np.array(coeff.split(),dtype = float)\n    embedded_words[words] = coeff\n\nembedding_matrix = np.zeros((len(word_index) + 1,200))\nfor word, i in word_index.items():\n  embedding_vector = embedded_words.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the sequential and Bi-LSTM model\nmodel = tf.keras.Sequential([tf.keras.layers.Embedding(len(word_index) + 1,200,weights=[embedding_matrix],input_length=500,\n                            trainable=False),\n                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n                             tf.keras.layers.Dropout(0.5),\n                             tf.keras.layers.Dense(256,activation = 'relu',),\n                             tf.keras.layers.Dense(128,activation = 'relu'),\n                             tf.keras.layers.Dropout(0.5),\n                             tf.keras.layers.Dense(1,activation = tf.nn.sigmoid)])\nmodel.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating callback\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.98):\n      print(\"\\n Accuracy == 98%!\")\n      self.model.stop_training = True\ncallbacks = myCallback()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 30\nhistory = model.fit(train_pad_sequence,\n                    y_train,\n                    epochs = num_epochs,\n                    validation_data=(test_pad_sequence,y_test),\n                    callbacks=[callbacks])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}