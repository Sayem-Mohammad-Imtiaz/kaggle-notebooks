{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading the data and exploring its shape and values","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-04T15:14:40.506825Z","iopub.execute_input":"2021-07-04T15:14:40.507483Z","iopub.status.idle":"2021-07-04T15:14:40.52683Z","shell.execute_reply.started":"2021-07-04T15:14:40.507378Z","shell.execute_reply":"2021-07-04T15:14:40.525966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test = pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv',header=None)\nmit_train = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\nptb_abnormal = pd.read_csv('/kaggle/input/heartbeat/ptbdb_abnormal.csv', header=None)\nptb_normal = pd.read_csv('/kaggle/input/heartbeat/ptbdb_normal.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:40.528531Z","iopub.execute_input":"2021-07-04T15:14:40.528856Z","iopub.status.idle":"2021-07-04T15:14:51.846271Z","shell.execute_reply.started":"2021-07-04T15:14:40.528828Z","shell.execute_reply":"2021-07-04T15:14:51.84516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:51.848509Z","iopub.execute_input":"2021-07-04T15:14:51.84897Z","iopub.status.idle":"2021-07-04T15:14:51.895771Z","shell.execute_reply.started":"2021-07-04T15:14:51.848896Z","shell.execute_reply":"2021-07-04T15:14:51.894874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:51.897168Z","iopub.execute_input":"2021-07-04T15:14:51.897442Z","iopub.status.idle":"2021-07-04T15:14:51.927742Z","shell.execute_reply.started":"2021-07-04T15:14:51.897415Z","shell.execute_reply":"2021-07-04T15:14:51.926597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptb_abnormal.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:51.929163Z","iopub.execute_input":"2021-07-04T15:14:51.929472Z","iopub.status.idle":"2021-07-04T15:14:51.960833Z","shell.execute_reply.started":"2021-07-04T15:14:51.929444Z","shell.execute_reply":"2021-07-04T15:14:51.959895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptb_normal.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:51.961988Z","iopub.execute_input":"2021-07-04T15:14:51.962291Z","iopub.status.idle":"2021-07-04T15:14:51.992765Z","shell.execute_reply.started":"2021-07-04T15:14:51.962262Z","shell.execute_reply":"2021-07-04T15:14:51.991921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test.rename(columns={187:\"Class\"}, inplace=True)\nmit_train.rename(columns={187:\"Class\"}, inplace=True)\nptb_abnormal.rename(columns={187:\"Class\"}, inplace=True)\nptb_normal.rename(columns={187:\"Class\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:51.994811Z","iopub.execute_input":"2021-07-04T15:14:51.995159Z","iopub.status.idle":"2021-07-04T15:14:52.005507Z","shell.execute_reply.started":"2021-07-04T15:14:51.995125Z","shell.execute_reply":"2021-07-04T15:14:52.004313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at how many classes are there in each dataset\nThe MIT dataset has 5 clases:\n* 0 = N  (Normal Beat)\n* 1 = S  (Supraventricular premature beat)\n* 2 = V  (Premature ventricular contraction)\n* 3 = F  (Fusion of ventricular and normal beat)\n* 4 = Q  (Unclassifiable beat)\n\nCompared to the PTB dataset which is 1 for abnormal and 0 for normal\n","metadata":{}},{"cell_type":"code","source":"print (\"MIT Train classes: \\n\", mit_train[\"Class\"].value_counts())\nprint (\"\\nMIT Test classes: \\n\", mit_test[\"Class\"].value_counts())\nprint (\"\\nPTB Abnormal classes: \\n\", ptb_abnormal[\"Class\"].value_counts())\nprint (\"\\nPTB Normal classes: \\n\", ptb_normal[\"Class\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:52.009095Z","iopub.execute_input":"2021-07-04T15:14:52.009421Z","iopub.status.idle":"2021-07-04T15:14:52.031659Z","shell.execute_reply.started":"2021-07-04T15:14:52.00939Z","shell.execute_reply":"2021-07-04T15:14:52.030642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting Dictionary to define the type of Heartbeat for both datasets\nMIT_Outcome = {0. : 'Normal Beat',\n               1. : 'Supraventricular premature beat',\n               2. : 'Premature ventricular contraction',\n               3. : 'Fusion of ventricular and normal beat',\n               4. : 'Unclassifiable beat'}\nPTB_Outcome = {0. : 'Normal',\n               1. : 'Abnormal'}","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:52.033494Z","iopub.execute_input":"2021-07-04T15:14:52.033792Z","iopub.status.idle":"2021-07-04T15:14:52.038794Z","shell.execute_reply.started":"2021-07-04T15:14:52.033764Z","shell.execute_reply":"2021-07-04T15:14:52.037676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Plots of some of the samples in the dataset","metadata":{}},{"cell_type":"code","source":"#Plotting 10 random samples from the MIT training dataset with their classification\nplt.figure(figsize=(25,10))\nnp_count = np.linspace(0,186,187)\nnp_time = np.tile(np_count,(10,1))\nrnd = np.random.randint(0,mit_train.shape[0],size=(10,))\n\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    ax.plot(mit_train.iloc[rnd[i],np_time[i,:]])\n    ax.set_title(MIT_Outcome[mit_train.loc[rnd[i],'Class']])\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:52.040492Z","iopub.execute_input":"2021-07-04T15:14:52.040923Z","iopub.status.idle":"2021-07-04T15:14:53.107409Z","shell.execute_reply.started":"2021-07-04T15:14:52.040874Z","shell.execute_reply":"2021-07-04T15:14:53.106664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting 10 random samples from the PTB training dataset with their classification\nplt.figure(figsize=(25,10))\nrnd = np.random.randint(0,ptb_normal.shape[0],size=(5,))\nrnd1 = np.random.randint(0,ptb_abnormal.shape[0], size=(5,))\n\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    if (i < 5):\n        ax.plot(ptb_normal.iloc[rnd[i],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_normal.loc[rnd[i],'Class']])\n    else:\n        ax.plot(ptb_abnormal.iloc[rnd1[i-5],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_abnormal.loc[rnd1[i-5],'Class']])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:53.108477Z","iopub.execute_input":"2021-07-04T15:14:53.108773Z","iopub.status.idle":"2021-07-04T15:14:54.109188Z","shell.execute_reply.started":"2021-07-04T15:14:53.108744Z","shell.execute_reply":"2021-07-04T15:14:54.108499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimenting with Classifiers for PTB Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import normalize\nfrom sklearn.svm import SVC \nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nptb_full = pd.concat([ptb_normal, ptb_abnormal], axis=0).reset_index()\nptb_full.drop(columns='index', inplace=True)\nptb_full = ptb_full.sample(ptb_full.shape[0], random_state=42)\nlearn_ptb, test_ptb, out_learn_ptb, out_test_ptb = train_test_split(ptb_full.iloc[:,:187], ptb_full.iloc[:,-1], test_size=0.15, random_state=42)\ntrain_ptb, valid_ptb, out_train_ptb, out_valid_ptb = train_test_split(learn_ptb, out_learn_ptb, test_size=0.2, random_state=42 )","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:54.110258Z","iopub.execute_input":"2021-07-04T15:14:54.110693Z","iopub.status.idle":"2021-07-04T15:14:55.506743Z","shell.execute_reply.started":"2021-07-04T15:14:54.110648Z","shell.execute_reply":"2021-07-04T15:14:55.505674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Traing dataset size: \", train_ptb.shape)\nprint(\"Validation dataset size: \", valid_ptb.shape)\nprint(\"Test dataset size: \", test_ptb.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:55.508152Z","iopub.execute_input":"2021-07-04T15:14:55.508477Z","iopub.status.idle":"2021-07-04T15:14:55.513843Z","shell.execute_reply.started":"2021-07-04T15:14:55.508444Z","shell.execute_reply":"2021-07-04T15:14:55.512993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalizing the training & test data \ntrain_ptb = normalize(train_ptb, axis=0, norm='max')\nvalid_ptb = normalize(valid_ptb, axis=0, norm='max')\ntest_ptb = normalize(test_ptb, axis=0, norm='max')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:55.515765Z","iopub.execute_input":"2021-07-04T15:14:55.516315Z","iopub.status.idle":"2021-07-04T15:14:55.564791Z","shell.execute_reply.started":"2021-07-04T15:14:55.516272Z","shell.execute_reply":"2021-07-04T15:14:55.563806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validating that the training data has a sample from both classess\nnp.unique(out_train_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:55.566249Z","iopub.execute_input":"2021-07-04T15:14:55.566678Z","iopub.status.idle":"2021-07-04T15:14:55.574297Z","shell.execute_reply.started":"2021-07-04T15:14:55.566635Z","shell.execute_reply":"2021-07-04T15:14:55.573034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running SVM\n\nWe use a GridSearchCV to find the bets parameters for SVM model with F1 micro scoring (F1 score \"weighted\" based on the data, so that the imbalanced data doesn't skew it towards the more abundant class)","metadata":{}},{"cell_type":"code","source":"#Looking at the plots we can see that there are a lot \"zero\" values which will not likely help our classification.  Eyeballing the data I chose 100 features to keep.\nsvc = SVC(kernel='rbf', class_weight='balanced')\n\nparam_grid = {'C': [1, 5, 10]}\ngrid_svc = GridSearchCV (svc, param_grid, verbose=2, scoring='f1_micro')\n\n# Train the grid of models. Time this process.\n%time grid_svc.fit(train_ptb, out_train_ptb)\n\n# Print the parameters which yield the best model performance\nprint (grid_svc.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:14:55.57551Z","iopub.execute_input":"2021-07-04T15:14:55.575802Z","iopub.status.idle":"2021-07-04T15:16:28.632925Z","shell.execute_reply.started":"2021-07-04T15:14:55.575773Z","shell.execute_reply":"2021-07-04T15:16:28.631737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the best parameters from the previos GridSearchCV and predicting values on our validation set.\nsvc = grid_svc.best_estimator_\npred_svc = svc.predict(valid_ptb)\n\n\nprint(classification_report(out_valid_ptb, pred_svc, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:16:28.634692Z","iopub.execute_input":"2021-07-04T15:16:28.635129Z","iopub.status.idle":"2021-07-04T15:16:30.392437Z","shell.execute_reply.started":"2021-07-04T15:16:28.635083Z","shell.execute_reply":"2021-07-04T15:16:30.391446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validating that the predictions contained both classes\nnp.unique(pred_svc)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:16:30.393714Z","iopub.execute_input":"2021-07-04T15:16:30.394035Z","iopub.status.idle":"2021-07-04T15:16:30.399307Z","shell.execute_reply.started":"2021-07-04T15:16:30.394003Z","shell.execute_reply":"2021-07-04T15:16:30.398637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM observations:\n\nThe SVM model performed quite well with F1 score of 0.91 (for Normal) and 0.96 (for Abnormal) considering that the data was imblalanced.  The overall accuracy was 95% which is also impressive.  I don't believe there is anything tha would stop us from using such a model but lets see if other models can outperform it.","metadata":{}},{"cell_type":"markdown","source":"# Running ExtraTreesClassifier\n\nConsidering that this is a time series dataset and how the prior value impacts the current value our intuition is that this model would perform badly as \"randomly\" selecting features and making decisions based on these values would make for an archiac model.","metadata":{}},{"cell_type":"code","source":"forest = ExtraTreesClassifier (criterion='entropy', max_samples=10, class_weight='balanced', random_state=42)\n\nparam_grid = {'n_estimators': [10, 20, 30],\n             'max_depth' : [5, 10, 15, 20]}\ngrid_forest = GridSearchCV(forest, param_grid, scoring='f1_micro', verbose=2)\n\ngrid_forest.fit(train_ptb, out_train_ptb)\n\nprint(grid_forest.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:16:30.400509Z","iopub.execute_input":"2021-07-04T15:16:30.400816Z","iopub.status.idle":"2021-07-04T15:16:45.905076Z","shell.execute_reply.started":"2021-07-04T15:16:30.400788Z","shell.execute_reply":"2021-07-04T15:16:45.903872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the best estimator from the GridSearchCV into our model\nforest = grid_forest.best_estimator_\n\n# predicting the outcome by using the best model\npred_forest = forest.predict(valid_ptb)\nprint(classification_report(out_valid_ptb, pred_forest, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:16:45.906658Z","iopub.execute_input":"2021-07-04T15:16:45.907101Z","iopub.status.idle":"2021-07-04T15:16:45.94698Z","shell.execute_reply.started":"2021-07-04T15:16:45.907055Z","shell.execute_reply":"2021-07-04T15:16:45.945888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validating that the model wasn't able to predict any record as normal\nnp.unique(pred_forest)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:16:45.948241Z","iopub.execute_input":"2021-07-04T15:16:45.948525Z","iopub.status.idle":"2021-07-04T15:16:45.956076Z","shell.execute_reply.started":"2021-07-04T15:16:45.948497Z","shell.execute_reply":"2021-07-04T15:16:45.954985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ExtraTreesClassifier Observation\n\nOut intution was quite off and the model did even better than SVM.  The resuls are even more impressive with an F1 score of 0.94 (for Normal) and 0.98 (for Abnormal), this is setting the bar quite high so lets see if any other model can outperform it.","metadata":{}},{"cell_type":"markdown","source":"# Running Logistic Regression\n\nNow we run the standard Logistic Regression model, our intuition is that it would perform well since it will take the data as is (without randomization) and just try to predict an outcome.  The results should be comparable to SVM","metadata":{}},{"cell_type":"code","source":"logistic = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n#clf_log = make_pipeline(pca, logistic)\n\nlogistic.fit(train_ptb, out_train_ptb)\npred_log = logistic.predict(valid_ptb)\nprint(classification_report(out_valid_ptb, pred_log, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:16:45.958118Z","iopub.execute_input":"2021-07-04T15:16:45.95869Z","iopub.status.idle":"2021-07-04T15:16:46.578074Z","shell.execute_reply.started":"2021-07-04T15:16:45.95864Z","shell.execute_reply":"2021-07-04T15:16:46.577031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression observation:\n\nThe Logistic Regression model so far has been the poorest performing model out of the 3 evaluated so far.\n","metadata":{}},{"cell_type":"markdown","source":"# Running TPOTClassifier to determine best algorithm\n### This part of the code takes roughly 3.5-4 hours to run, it has been commented out to avoid the long run times.  \n#The results are below, you can convert this cell from markdown to code to run it if desired\n\n#===================    Begin Code here ======================================\nfrom tpot import TPOTClassifier\n\ntpot = TPOTClassifier (generations=5, population_size=40, verbosity=2, random_state=42, scoring='f1_micro')\ntpot.fit(train_ptb, out_train_ptb)\n\n#evaluate the classifier against the validation set\nprint(tpot.score(valid_ptb, out_valid_ptb))\n\n#export the model to a file\ntpot.export('PTB_Data_Classifier.py')\n\n#===================   End Code here ==========================================","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:16:46.582226Z","iopub.execute_input":"2021-07-04T15:16:46.582864Z","iopub.status.idle":"2021-07-04T18:52:45.606281Z","shell.execute_reply.started":"2021-07-04T15:16:46.58282Z","shell.execute_reply":"2021-07-04T18:52:45.605274Z"}}},{"cell_type":"markdown","source":"======================== OUTPUT =============================\n\nGeneration 1 - Current best internal CV score: 0.9667508842849925\n\nGeneration 2 - Current best internal CV score: 0.9710965133906013\n\nGeneration 3 - Current best internal CV score: 0.9751389590702374\n\nGeneration 4 - Current best internal CV score: 0.9755432036382011\n\nGeneration 5 - Current best internal CV score: 0.9780697321879737\n\n\nBest pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.5, max_depth=8, max_features=0.1, min_samples_leaf=4, min_samples_split=15, n_estimators=100, subsample=0.9000000000000001)\n\n0.9773645917542442\n\n\n\n\n\n================== END OF OUTPUT =============================","metadata":{}},{"cell_type":"markdown","source":"########  PTB_Data_Classifier.Py File content ########\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\n#NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = train_test_split(features, tpot_data['target'], random_state=42)\n\n#Average CV score on the training set was: 0.9780697321879737\nexported_pipeline = GradientBoostingClassifier(learning_rate=0.5, max_depth=8, max_features=0.1, min_samples_leaf=4, min_samples_split=15, n_estimators=100, subsample=0.9000000000000001)\n\n#Fix random state in exported estimator\nif hasattr(exported_pipeline, 'random_state'):\n    setattr(exported_pipeline, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\n\nresults = exported_pipeline.predict(testing_features)\n\n\n########  End of File content ########","metadata":{}},{"cell_type":"markdown","source":"# Running GradientBosstingClassifier\n\nFinally lets run the GradientBosstingClassifier that AutoML has recommended for us and see what we observer.  We're expecting this to be our best performing model so far.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradboost = GradientBoostingClassifier(learning_rate=0.5, max_depth=8, max_features=0.1, \n                                       min_samples_leaf=4, min_samples_split=15, n_estimators=100, \n                                       subsample=0.9000000000000001)\ngradboost.fit(train_ptb, out_train_ptb)\npred_gradboost = gradboost.predict(valid_ptb)\n\nprint(classification_report(out_valid_ptb, pred_gradboost, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T20:57:06.141703Z","iopub.execute_input":"2021-07-04T20:57:06.142076Z","iopub.status.idle":"2021-07-04T20:57:15.201069Z","shell.execute_reply.started":"2021-07-04T20:57:06.142045Z","shell.execute_reply":"2021-07-04T20:57:15.200068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GradientBoostingClassifier:\n\nAutoML has recommeded a very high performing model. It would be very difficult to beat, Deep Learning might be able to produce a better performing model but would the minimal gains in results come with an acceptable performance hit?","metadata":{}},{"cell_type":"markdown","source":"# Analayzing the MIT Heartbeat Data\nNow we repeat the same analysis with the different dataset and tweak accordingly","metadata":{}},{"cell_type":"code","source":"# Since the MIT dataset already comes as a train set and test set, we just split 20% of the training set for validation\ntrain_mit, valid_mit, out_train_mit, out_valid_mit = train_test_split(mit_train.iloc[:,:187], mit_train.iloc[:,-1], test_size=0.20, random_state=42)\n\n#we remove the targets from the test set\ntest_mit, out_test_mit = mit_test.iloc[:,:187], mit_test.iloc[:,-1]\n\n#Normalizing the training & test data \ntrain_mit = normalize(train_mit, axis=0, norm='max')\nvalid_mit = normalize(valid_mit, axis=0, norm='max')\ntest_mit = normalize(test_ptb, axis=0, norm='max')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T18:53:03.099999Z","iopub.execute_input":"2021-07-04T18:53:03.100434Z","iopub.status.idle":"2021-07-04T18:53:03.752612Z","shell.execute_reply.started":"2021-07-04T18:53:03.100389Z","shell.execute_reply":"2021-07-04T18:53:03.751381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running SVM\n\nWe use a GridSearchCV to find the bets parameters for SVM model with F1 micro scoring (F1 score \"weighted\" based on the data, so that the imbalanced data doesn't skew it towards the more abundant class)","metadata":{}},{"cell_type":"code","source":"#Looking at the plots we can see that there are a lot \"zero\" values which will not likely help our classification.  Eyeballing the data I chose 100 features to keep.\nsvc = SVC(kernel='rbf', class_weight='balanced')\n\nparam_grid = {'C': [1, 5, 10]}\ngrid_svc = GridSearchCV (svc, param_grid, verbose=2, scoring='f1_micro')\n\n# Train the grid of models. Time this process.\n%time grid_svc.fit(train_mit, out_train_mit)\n\n# Print the parameters which yield the best model performance\nprint (grid_svc.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T18:53:03.754047Z","iopub.execute_input":"2021-07-04T18:53:03.754356Z","iopub.status.idle":"2021-07-04T20:46:48.659387Z","shell.execute_reply.started":"2021-07-04T18:53:03.754327Z","shell.execute_reply":"2021-07-04T20:46:48.658589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the best parameters from the previos GridSearchCV and predicting values on our validation set.\nsvc = grid_svc.best_estimator_\npred_svc_mit = svc.predict(valid_mit)\n\nprint(classification_report(out_valid_mit, pred_svc_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T20:46:48.660432Z","iopub.execute_input":"2021-07-04T20:46:48.66084Z","iopub.status.idle":"2021-07-04T20:48:28.10878Z","shell.execute_reply.started":"2021-07-04T20:46:48.660809Z","shell.execute_reply":"2021-07-04T20:48:28.107718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running ExtraTreesClassifier\n\nConsidering that this is a time series dataset and how the prior value impacts the current value our intuition is that this model would perform badly as \"randomly\" selecting features and making decisions based on these values would make for an archiac model.","metadata":{}},{"cell_type":"code","source":"forest_mit = ExtraTreesClassifier (criterion='entropy', max_samples=10, class_weight='balanced', random_state=42)\n\nparam_grid = {'n_estimators': [10, 20, 30],\n             'max_depth' : [5, 10, 15, 20]}\ngrid_forest_mit = GridSearchCV(forest_mit, param_grid, scoring='f1_micro', verbose=2)\n\ngrid_forest_mit.fit(train_mit, out_train_mit)\n\nprint(grid_forest_mit.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T20:48:28.109986Z","iopub.execute_input":"2021-07-04T20:48:28.110284Z","iopub.status.idle":"2021-07-04T20:50:49.837717Z","shell.execute_reply.started":"2021-07-04T20:48:28.110256Z","shell.execute_reply":"2021-07-04T20:50:49.836711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the best estimator from the GridSearchCV into our model\nforest_mit = grid_forest_mit.best_estimator_\n\n# predicting the outcome by using the best model\npred_forest_mit = forest_mit.predict(valid_mit)\nprint(classification_report(out_valid_mit, pred_forest_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T20:50:49.83905Z","iopub.execute_input":"2021-07-04T20:50:49.839361Z","iopub.status.idle":"2021-07-04T20:50:50.087818Z","shell.execute_reply.started":"2021-07-04T20:50:49.83933Z","shell.execute_reply":"2021-07-04T20:50:50.08675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running Logistic Regression\n​\nNow we run the standard Logistic Regression model, our intuition is that it would perform well since it will take the data as is (without randomization) and just try to predict an outcome.  The results should be comparable to SVM","metadata":{}},{"cell_type":"code","source":"logistic_mit = LogisticRegression(random_state=42, class_weight='balanced', max_iter=10000)\n#clf_log = make_pipeline(pca, logistic)\n\nlogistic_mit.fit(train_mit, out_train_mit)\npred_log_mit = logistic_mit.predict(valid_mit)\nprint(classification_report(out_valid_mit, pred_log_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T20:50:50.089077Z","iopub.execute_input":"2021-07-04T20:50:50.089354Z","iopub.status.idle":"2021-07-04T20:52:03.200927Z","shell.execute_reply.started":"2021-07-04T20:50:50.089327Z","shell.execute_reply":"2021-07-04T20:52:03.199752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running GradientBoostingClassifier\n\nThis time we're not going to run AutoML as its very time consuming and specially with 5 classes so we're just going to run it as the PTB dataset and see how it fares.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradboost_mit = GradientBoostingClassifier(learning_rate=0.5, max_depth=8, max_features=0.1, \n                                       min_samples_leaf=4, min_samples_split=15, n_estimators=100, \n                                       subsample=0.9000000000000001)\ngradboost_mit.fit(train_mit, out_train_mit)\npred_gradboost_mit = gradboost_mit.predict(valid_mit)\n\nprint(classification_report(out_valid_mit, pred_gradboost_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:01:39.976801Z","iopub.execute_input":"2021-07-04T21:01:39.97727Z","iopub.status.idle":"2021-07-04T21:07:54.433643Z","shell.execute_reply.started":"2021-07-04T21:01:39.97723Z","shell.execute_reply":"2021-07-04T21:07:54.432527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}