{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"This notebook contains code used to build an Text Spam Classifier using a Multinomial Naive Bayes Algorithm."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing important libraries\nimport numpy as np\nimport pandas as pd\n\nimport re\nfrom collections import defaultdict\n\nfrom nltk.corpus import stopwords\n\n\n# Printing files in input folder\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import shuffle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and Cleaning Data"},{"metadata":{},"cell_type":"markdown","source":"In this section, I load the data used in the model and clean it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data from CSV\ndata = pd.read_csv(\"../input/spam.csv\", encoding = \"latin-1\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting and renaming first 2 columns\ndata = data[['v1','v2']]\ndata.columns = ['label','text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting ham and spam to 0 and 1 respectively\ndata['label'] = data['label'].map({'ham':0,'spam':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing number of ham and spam emails\ndata['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n- There's clearly a lot more spam emails than ham emails\n- In the model, I will try to train it on an equal amount of spam and ham emails; in order to reduce bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new shuffled dataset with equal ham and spam emails\nham = data[data['label'] == 0]\nspam = data[data['label'] == 1]\nnew_ham = ham.sample(len(spam), random_state = 5)\nnew_data = pd.concat([new_ham,spam],axis = 0)\ndata = shuffle(new_data, random_state = 5).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important Functions"},{"metadata":{},"cell_type":"markdown","source":"In this section, I define some important functions that will be used in the ML model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a text parsing function which will tokenize the text. It removes all punctuation, spaces, and stopwords\ndef textParser(text):\n    tokens = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', \"\", text).split(\" \")\n    tokens = list(filter(lambda x: len(x) > 0 , map(str.lower,tokens)))\n    tokens = list(filter(lambda x: x not in stopwords.words(\"english\"),tokens))\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the ML Pipeline"},{"metadata":{},"cell_type":"markdown","source":"In this section, I build the separate parts of the ML pipeline"},{"metadata":{},"cell_type":"markdown","source":"## Count Vectorizer\nIn this section, I use CountVectorizer to tokenize the text (according to the text parser above) and then convert each text into a vectorized format by words and their counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting each text into a vector format\nbow_data = CountVectorizer(analyzer = textParser).fit_transform(data['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tfidf Transformation\nSince longer texts tend to have more words, I normalize for this by using Tfidf transformer on each text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing the vectorized texts by text length\ntfidf_data = TfidfTransformer().fit_transform(bow_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting Data\nIn this section, I will split data into test and training sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the normalized, vectorized texts into train and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(tfidf_data,data[['label']], test_size=0.3, random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining a Multinomial Model\nIn this section, I define a Multinomial Model to classify the texts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a Gaussian model\nmodel = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model to the training data\nfitted_model = model.fit(X_train.toarray(), np.array(Y_train).ravel())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting and Evaluating the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting on the test data and printing the accuracy\npred = fitted_model.predict(X_test.toarray())\nacc_MNB = accuracy_score(np.array(Y_test).ravel(), pred)\nacc_MNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the classification report\nprint(classification_report(np.array(Y_test).ravel(),pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline"},{"metadata":{},"cell_type":"markdown","source":"In this section, I create a pipeline that mimics the above models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the training pipeline\ntraining_pipe = Pipeline(\n    steps = [\n        ('bow', CountVectorizer(analyzer = textParser)),\n        ('tfdif', TfidfTransformer()),\n        ('model',MultinomialNB())\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating training data from the unvectorized data\nX_train, X_test, Y_train, Y_test = train_test_split(data['text'], data['label'], test_size = 0.3, random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting model and predicting on data\ntraining_pipe.fit(X_train,Y_train)\npred_test_MNB = training_pipe.predict(X_test)\nprint(\"Accuracy (%):\",training_pipe.score(X_test, Y_test)*100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}