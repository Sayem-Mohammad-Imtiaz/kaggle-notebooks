{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/housesalesprediction/kc_house_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Columns\n* **id** - Unique ID for each home sold\n* **date** - Date of the home sale\n* **price** - Price of each home sold\n* **bedrooms** - Number of bedrooms\n* **bathrooms** - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n* **sqft_living** - Square footage of the apartments interior living space\n* **sqft_lot** - Square footage of the land space\n* **floors** - Number of floors\n* **waterfront** - A dummy variable for whether the apartment was overlooking the waterfront or not\n* **view** - An index from 0 to 4 of how good the view of the property was\n* **condition** - An index from 1 to 5 on the condition of the apartment,\n* **grade** - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n* **sqft_above** - The square footage of the interior housing space that is above ground level\n* **sqft_basement** - The square footage of the interior housing space that is below ground level\n* **yr_buil**t - The year the house was initially built\n* **yr_renovated** - The year of the houseâ€™s last renovation\n* **zipcode** - What zipcode area the house is in\n* **lat** - Lattitude\n* **long** - Longitude\n* **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors\n* **sqft_lot15** - The square footage of the land lots of the nearest 15 neighbors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.distplot(df[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df[\"bedrooms\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# price between 0 - 2M and bedrooms 2 - 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()[\"price\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.scatterplot(x=\"price\",y=\"sqft_living\",data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.scatterplot(x=\"long\",y=\"lat\",data=df,hue=\"price\",palette=\"RdYlGn\",alpha=0.2,edgecolor=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottom_99 = df.sort_values(\"price\",ascending=False).iloc[216:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.scatterplot(x=\"long\",y=\"lat\",data=bottom_99,hue=\"price\",palette=\"RdYlGn\",alpha=0.2,edgecolor=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from the coordinates we can see that water side houses are more expenseive as natural. \nsns.boxplot(x=\"waterfront\",y=\"price\",data=bottom_99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so far we did a corraletion analysis, inspect lat/long - price relationship. exclude most expensive %1 of the houses.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what we do is to drop unusable columns and transform remaining ones as necessary. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"id column is not necessary for prediction model. Also we need to transform date columns as year and month.Even seperating them should benefit us more for seasonal movements. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(\"id\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"date\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"date\"] = pd.to_datetime(df[\"date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"date\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"year\"] = df[\"date\"].apply(lambda date: date.year)\ndf[\"month\"] = df[\"date\"].apply(lambda date: date.month)\ndf = df.drop(\"date\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.boxenplot(x=\"month\",y=\"price\",data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\ndf.groupby(\"month\").mean()[\"price\"].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the figures above month is not playing a vital role but still some months are higher average especially in spring-summer season","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\ndf.groupby(\"year\").mean()[\"price\"].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we excpected year/price have lineer relatinship. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Also zipcode requires domain experince about the area itself. Normally we can categorize them.\n#However, it wiil create 70 category which is too much to handle. \n#But if you are familiar with the area you can group them, and reduce category bin size. \n#I prefer to drop that column too. (Accept Bias on my model.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(\"zipcode\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"yr_renovated\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we can say most of the data is equal to \"0\" so it is not a good variable in the context of quality\n#It is possible to think that the building has not been renovated,that's why it is \"0\". \n#For example basement sqft value has also many \"0\" but you can think like there is no basement.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So lets begin\nX = df.drop(\"price\",axis=1).values\ny = df[\"price\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we split our data to test/train groups we need to scale them. I chose MinMaxScaler from sklearn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I did not fit the scaler on test set in order to prevent data leakage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(19,activation =\"relu\"))\nmodel.add(Dense(19,activation =\"relu\"))\nmodel.add(Dense(19,activation =\"relu\"))\nmodel.add(Dense(19,activation =\"relu\"))\n\nmodel.add(Dense(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The shape of the train set contains 19 variables so we insert 19 neurons as a start, 4 hidden layers with one output layer which is a signle neuron due to single value expected. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=\"adam\",loss=\"mse\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),batch_size=128,epochs=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_df = pd.DataFrame(model.history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if we overfit on test data and also see optimum epochs\nloss_df.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error,mean_squared_error,explained_variance_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_test,preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This not mean anything if don't compare with the actual data, price avg value is 540K and the error 104K\n#We are %20 percent wrong.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance_score(y_test,preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test,preds)\nplt.plot(y_test,y_test,\"r\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model works better in lower price houses and errors are quite larger at higher prices. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets do everything with bottom_99 data set.But we re-define from our transformed and explored data df\nbottom_99 = df.sort_values(\"price\",ascending=False).iloc[216:]\nX1 =bottom_99.drop(\"price\",axis=1).values\ny1 = bottom_99[\"price\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler1  =  MinMaxScaler()\nX1_train =  scaler1.fit_transform(X1_train)\nX1_test  =  scaler1.transform(X1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Dense(19,activation =\"relu\"))\nmodel1.add(Dense(19,activation =\"relu\"))\nmodel1.add(Dense(19,activation =\"relu\"))\nmodel.add(Dense(19,activation =\"relu\"))\n\nmodel1.add(Dense(1))\n\nmodel1.compile(optimizer=\"adam\",loss=\"mse\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(x=X1_train,y=y1_train,validation_data=(X1_test,y1_test),batch_size=64,epochs=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_bott99 = pd.DataFrame(model1.history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_bott99.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds1 = model1.predict(X1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y1_test,preds1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance_score(y1_test,preds1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y1_test,preds1)\nplt.plot(y1_test,y1_test,\"r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So we get worse around %30. So we need to optimize our model. Try different epochs and batch sizes, but we may need to update our data set, make more cleaning\n#Also find the optimum neuron structure. Will be continued. ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}