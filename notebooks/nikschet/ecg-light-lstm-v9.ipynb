{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport random\nimport pickle\nimport itertools\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nfrom sklearn.utils import shuffle\n\nfrom scipy.signal import resample\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nimport math\nimport random\nimport pickle\nimport itertools\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nnp.random.seed(42)\nimport tensorflow as tf\nimport tensorflow.keras as keras\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.  # DATA ACQUISITION *"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.getcwd())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mit_test_data = pd.read_csv(\"../input/heartbeat/mitbih_test.csv\", header=None)\nmit_train_data = pd.read_csv(\"../input/heartbeat/mitbih_train.csv\", header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PRODUCE BALANCED DATASET train_df , test_df *"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is a huge difference in the balanced of the classes.\n# Better choose the resample technique more than the class weights for the algorithms.\nfrom sklearn.utils import resample\n\ndf_1=mit_train_data[mit_train_data[187]==1]\ndf_2=mit_train_data[mit_train_data[187]==2]\ndf_3=mit_train_data[mit_train_data[187]==3]\ndf_4=mit_train_data[mit_train_data[187]==4]\ndf_0=(mit_train_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_4_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntrain_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])\n\n\ndf_11=mit_test_data[mit_train_data[187]==1]\ndf_22=mit_test_data[mit_train_data[187]==2]\ndf_33=mit_test_data[mit_train_data[187]==3]\ndf_44=mit_test_data[mit_train_data[187]==4]\ndf_00=(mit_test_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n\ndf_11_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_22_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_33_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_44_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntest_df=pd.concat([df_00,df_11_upsample,df_22_upsample,df_33_upsample,df_44_upsample])\n\n\nequilibre=train_df[187].value_counts()\nprint(equilibre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ALL Train data\")\nprint(\"Type\\tCount\")\nprint((mit_train_data[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"ALL Test data\")\nprint(\"Type\\tCount\")\nprint((mit_test_data[187]).value_counts())\n\nprint(\"ALL Balanced Train data\")\nprint(\"Type\\tCount\")\nprint((train_df[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"ALL Balanced Test data\")\nprint(\"Type\\tCount\")\nprint((train_df[187]).value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ONE HOT Encoding *"},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encoding for categorical target\n#Since we will be using neural networks for our classification model, \n#our output classes need to be turned into a numerical representation. We use one hot encoding (from sklearn package) to do this.\n\n\n\n#train_target = mit_train_data[187]\n#train_target = train_target.values.reshape(87554,1)\ntrain_target = train_df[187]\ntrain_target = train_target.values.reshape(100000,1)\n\n\n\n\n#one hot encode train_target\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\n# TODO: create a OneHotEncoder object, and fit it to all of X\n\n# 1. INSTANTIATE\nenc = preprocessing.OneHotEncoder()\n\n# 2. FIT\nenc.fit(train_target)\n\n# 3. Transform\nonehotlabels = enc.transform(train_target).toarray()\nonehotlabels.shape\n\ntarget = onehotlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove ground truth labels from training df\n#train/test split\n\n\nfrom sklearn.model_selection import train_test_split\n\n#X = mit_train_data\nX = train_df\nX = X.drop(axis=1,columns=187)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X,target, test_size = 0.25, random_state = 36)\nX_train = np.asarray(X_train)\nX_valid = np.asarray(X_valid)\nY_train = np.asarray(Y_train)\nY_valid = np.asarray(Y_valid)\n\n#X_train.reshape((1, 2403, 187))\nX_train = np.expand_dims(X_train, axis=2)\nX_valid = np.expand_dims(X_valid, axis=2)\nprint(X_train.shape)\nprint(Y_train.shape)\n# 2,403 training heartbeats and 802 validation heartbeats \n# for a 75:25 train-test split. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 MODEL NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL 1 https://www.kaggle.com/freddycoder/heartbeat-categorization\n# Separate features and targets\n\nfrom keras.utils import to_categorical\n\nprint(\"--- X ---\")\n# X = mit_train_data.loc[:, mit_train_data.columns != 187]\nX = train_df.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\n# y = mit_train_data.loc[:, mit_train_data.columns == 187]\ny = train_df.loc[:, mit_train_data.columns == 187]\ny = to_categorical(y)\n\nprint(\"--- testX ---\")\n#testX = mit_test_data.loc[:, mit_test_data.columns != 187]\ntestX = test_df.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\n#testy = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = test_df.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keras model to make prediction\n\n#The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n#The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n# softmax is used to categorize \n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential()\n\nmodel.add(Dense(50, activation='relu', input_shape=(187,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\nmodel.compile(optimizer='Adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X, y, epochs=10)\n\nprint(\"Evaluation: \")\nmse, acc = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ARDUINO SAMPLES"},{"metadata":{},"cell_type":"markdown","source":"# USE IF SAMPLES ARE IN A MATRIX FORM"},{"metadata":{"trusted":true},"cell_type":"code","source":"test= pd.read_csv(\"../input/arduino3a/AS3.txt\", header=None)\ntest\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(test.iloc[100,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalizing Arduino Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMALIZING TEST DATA AMPLITUDE\nfrom sklearn.preprocessing import MinMaxScaler\n# load the dataset and print the first 5 rows\n# prepare data for normalization\nvalues = test.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(values)\nnormalized = scaler.transform(values)\n\ndfnormalized = pd.DataFrame(normalized)\ndfnormalized.index = [x for x in range(1, len(dfnormalized.values)+1)]\nplt.plot(dfnormalized.iloc[30,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicing Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"# category= model.predict_classes(test) #not Normalized\ncategory= model.predict_classes(dfnormalized) #Normalized\nplt.plot(category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean of Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# USE IF SAMPLES ARE IN A ROW"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/arduinorow1a/test44.txt\", header=None)\n#test = pd.read_csv(\"../input/arduinorow2a/marnelakis.txt\", header=None)\n#test = test.iloc[0,0:len(test.T)-1] # Remove last line cause it might be a Nan\ntest = pd.DataFrame(test)\ntest=test.T\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMALIZING TEST DATA AMPLITUDE\nfrom sklearn.preprocessing import MinMaxScaler\n# load the dataset and print the first 5 rows\n# prepare data for normalization\nvalues = test.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(values)\nnormalized = scaler.transform(values)\nnormalized = pd.DataFrame(normalized) \nnormalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TESTING FOR ONE X\n#x=0\n#normalized = pd.DataFrame(normalized.T) ## CAUTION!!! needs to run only once \n#normtest=normalized.iloc[0, 0+x:187+x] \n#normtest=pd.DataFrame(normtest)\n#category = model.predict_classes(normtest.T)\n#category","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category= pd.DataFrame()\ncategory=category.dropna()\nlst_seq = np.arange(0,len(normalized.T)-190)\nfor x in lst_seq:\n    normtest=normalized.iloc[0, 0+x:187+x] \n    normtest=pd.DataFrame(normtest)\n    category[x] = model.predict_classes(normtest)\ncategory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MEAN OF CATEGORIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(category.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PLOT OF CATEGORIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(category.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display frequency of each predicted category as evaluated by model"},{"metadata":{"trusted":true},"cell_type":"code","source":"category = pd.DataFrame(category)\ntemp1= category.iloc[0,:].value_counts()\nprint(\"Categories vs Value Count\")\nprint(temp1)\nprint(\"Categories vs Frequency\")\nprint(temp1/(len(category.T)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. NEW MODEL CNN"},{"metadata":{},"cell_type":"markdown","source":"## https://www.kaggle.com/gregoiredc/arrhythmia-on-ecg-classification-using-cnn"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_train=train_df[187]\ntarget_test=test_df[187]\ny_train=to_categorical(target_train)\ny_test=to_categorical(target_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=train_df.iloc[:,:186].values\nX_test=test_df.iloc[:,:186].values\n#for i in range(len(X_train)):\n#    X_train[i,:186]= add_gaussian_noise(X_train[i,:186])\nX_train = X_train.reshape(len(X_train), X_train.shape[1],1)\nX_test = X_test.reshape(len(X_test), X_test.shape[1],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def network(X_train,y_train,X_test,y_test):\n    \n\n    im_shape=(X_train.shape[1],1)\n    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n    conv1_1=Convolution1D(64, (6), activation='relu', input_shape=im_shape)(inputs_cnn)\n    conv1_1=BatchNormalization()(conv1_1)\n    pool1=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n    conv2_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool1)\n    conv2_1=BatchNormalization()(conv2_1)\n    pool2=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n    conv3_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool2)\n    conv3_1=BatchNormalization()(conv3_1)\n    pool3=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n    flatten=Flatten()(pool3)\n    dense_end1 = Dense(64, activation='relu')(flatten)\n    dense_end2 = Dense(32, activation='relu')(dense_end1)\n    main_output = Dense(5, activation='softmax', name='main_output')(dense_end2)\n    \n    \n    model = Model(inputs= inputs_cnn, outputs=main_output)\n    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n    \n    \n    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\n    history=model.fit(X_train, y_train,epochs=30,callbacks=callbacks, batch_size=32,validation_data=(X_test,y_test))\n    model.load_weights('best_model.h5')\n    return(model,history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(history,X_test,y_test,model):\n    scores = model.evaluate((X_test),y_test, verbose=0)\n    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n    \n    print(history)\n    fig1, ax_acc = plt.subplots()\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model - Accuracy')\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.show()\n    \n    fig2, ax_loss = plt.subplots()\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model- Loss')\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.show()\n    target_names=['0','1','2','3','4']\n    \n    y_true=[]\n    for element in y_test:\n        y_true.append(np.argmax(element))\n    prediction_proba=model.predict(X_test)\n    prediction=np.argmax(prediction_proba,axis=1)\n    cnf_matrix = confusion_matrix(y_true, prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nmodel,history=network(X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(history,X_test,y_test,model)\ny_pred=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame()\ncategory= pd.DataFrame()\ncategory=category.dropna()\nlst_seq = np.arange(0,len(normalized.T)-190)\nfor x in lst_seq:\n    temp=normalized.iloc[0,0+x:186+x]\n    temp=pd.DataFrame(temp) \n    temp=temp.values\n    temp=temp.reshape(1,186,1)\n    category=pd.DataFrame(model.predict(temp))\n    df = pd.DataFrame(category)\n    df1=df1.append(df)\n    \ncategory=df1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean of each Catecory"},{"metadata":{"trusted":true},"cell_type":"code","source":"category=pd.DataFrame(category)\ncategory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat1=category[0].mean()\ncat2=category[1].mean()\ncat3=category[2].mean()\ncat4=category[3].mean()\ncat5=category[4].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. MODEL RNN LSTM GRU"},{"metadata":{},"cell_type":"markdown","source":"# 3.1 USE IF SAMPLES ARE IN A ROW"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/arduinorow1a/test44.txt\", header=None)\ntest = test.iloc[0,0:len(test.T)-1] # Remove last line cause it might be a Nan\ntest = pd.DataFrame(test)\n# NORMALIZING TEST DATA AMPLITUDE\nfrom sklearn.preprocessing import MinMaxScaler\n# load the dataset and print the first 5 rows\n# prepare data for normalization\nvalues = test.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(values)\nnormalized = scaler.transform(values)\nnormalized = pd.DataFrame(normalized)\nnormalized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL LSTM RNN"},{"metadata":{},"cell_type":"markdown","source":"## https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n## https://www.hindawi.com/journals/jhe/2019/6320651/\n## https://www.mathworks.com/help/signal/examples/classify-ecg-signals-using-long-short-term-memory-networks.html\n##"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnp.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL 1 https://www.kaggle.com/freddycoder/heartbeat-categorization\n# Separate features and targets\n\nfrom keras.utils import to_categorical\n\nprint(\"--- X ---\")\n# X = mit_train_data.loc[:, mit_train_data.columns != 187]\nX = train_df.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\n# y = mit_train_data.loc[:, mit_train_data.columns == 187]\ny = train_df.loc[:, mit_train_data.columns == 187]\ny = to_categorical(y)\n\nprint(\"--- testX ---\")\n#testX = mit_test_data.loc[:, mit_test_data.columns != 187]\ntestX = test_df.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\n#testy = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = test_df.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model.\nfrom keras.callbacks import History \nhistory = History()\nembedding_vecor_length = 187\nmodel = Sequential()\n#model = Bidirectional(model)\n\nmodel.add(Embedding(100000, embedding_vecor_length, input_length=187))\nmodel.add(LSTM(187))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nhistory = model.fit(X, y, validation_data=(testX, testy), epochs=3, batch_size=8)\n\n\n#Dropout is a powerful technique for combating overfitting in your LSTM models \n#model = Sequential()\n#model.add(Embedding(1000, embedding_vecor_length, input_length=187))\n#model.add(LSTM(50, dropout=0.001, recurrent_dropout=0.001))\n#model.add(Dense(5, activation='softmax'))\n#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n#print(model.summary())\n#history = model.fit(X, y, validation_data=(testX, testy), epochs=50, batch_size=128)\n\n\n\n## SAVE MODEL ##\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"1model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"1model.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model"},{"metadata":{},"cell_type":"raw","source":"mse, acc = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# The history for the validation dataset is labeled test by convention as it is indeed a test dataset for the model.\n#The plots can provide an indication of useful things about the training of the model, such as:\n#*It’s speed of convergence over epochs (slope).\n#*Whether the model may have already converged (plateau of the line).\n#*Whether the mode may be over-learning the training data (inflection for validation line)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Αccuracy and prediction scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(testX, batch_size=1000)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nprint(classification_report(testy.argmax(axis=1), y_pred.argmax(axis=1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict category of Arduino sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"category= pd.DataFrame()\ncategory=category.dropna()\nlst_seq = np.arange(0,len(normalized.T)-190)\nfor x in lst_seq:\n    normtest=normalized.iloc[0, 0+x:187+x] \n    normtest=pd.DataFrame(normtest)\n    category[x] = model.predict_classes(normtest.T)\ncategory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MEAN OF CATEGORIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(category.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PLOT OF CATEGORIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(category.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display frequency of each predicted category as evaluated by model"},{"metadata":{"trusted":true},"cell_type":"code","source":"category = pd.DataFrame(category)\ntemp1= category.iloc[0,:].value_counts()\nprint(\"Categories vs Value Count\")\nprint(temp1)\nprint(\"Categories vs Frequency\")\nprint(temp1/(len(category.T)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOAD MODEL "},{"metadata":{"trusted":true},"cell_type":"code","source":"json_file = open(\"../working/model.json\", 'r')\nmodel_json = json_file.read() \njson_file.close()\n\nfrom keras.models import model_from_json\nmodel = model_from_json(model_json)\nmodel.load_weights(\"../working/model.h5\")\n\n#model.compile(loss='binary_crossentropy', optimizer='adam')\n#prediction = model.predict(x_test, batch_size=2048)[0].flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}