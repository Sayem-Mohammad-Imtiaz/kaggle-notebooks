{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart.csv\")\ndata.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72f2e694b257717fa84f4ddb366afd0cef9c6cdc"},"cell_type":"code","source":"df = pd.DataFrame(data)\ndf.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24e7008c63946516fcaa52147714f7a8330be20f"},"cell_type":"code","source":"df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28263b3b61279c87085e95911d4d4b80780e1e68"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf29ee1af46b8b084a090e33bf838bde531abec4"},"cell_type":"code","source":"import seaborn as sns # used for plot interactive graph.\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,8))\nsns.countplot(data['age'],label =\"count\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57fb89c1109df0cdd546a688db9be22938b3ebb4"},"cell_type":"code","source":"#correlation graph to find the correlation\ncorr = data.corr()\nplt.figure(figsize = (14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           cmap= 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"623c317d57dabfaeb5afaf71e02e41ff1398b530"},"cell_type":"code","source":"#conclusion, none of the features are highlt correltaed, so we could use all ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"feb2ff0c114bca0cd5346fc9770fa019f8c6862e"},"cell_type":"code","source":"selected_features = ['age', 'cp', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal']\n\n\nplt.figure(figsize = (14,14))\ncolor_function = {0: \"blue\", 1: \"red\"} # Here Red color will be 1 which means M and blue foo 0 means B\ncolors = data[\"target\"].map(lambda x: color_function.get(x))# mapping the color fuction with diagnosis column\npd.scatter_matrix(data[selected_features], c=colors, alpha = 0.5, figsize = (15, 15)); # plotting scatter plot matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80e201383923a818a14c3db2e4b498d9a8b52b4e"},"cell_type":"code","source":"#First get an general overview\ndata.hist(figsize=(15,20))\nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24363c72de9cf42870476b99f29c336ee2b713c5"},"cell_type":"code","source":"#cp: chest pain type\ndata[\"cp\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7108b7dd700375273cc9e4498648d4d6ba624847"},"cell_type":"code","source":"plt.xlabel('pain type')\nplt.ylabel('count')\nplt.title('Four type chest pain')\nsns.countplot(data['cp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"912f816f3e5e57b9090cd5946d8eaeb2002d9592","_kg_hide-output":true},"cell_type":"code","source":"import plotly\nimport plotly.graph_objs as go\n\nx_data = data['age']\ny_data = data['thalach']\ncolors = np.random.rand(2938)\nsz = np.random.rand(2000)*30\n\nfig = go.Figure()\nfig.add_scatter(x = x_data,\n                y = y_data,\n                mode = 'markers',\n                marker = {'size': sz,\n                         'color': colors,\n                         'opacity': 0.6,\n                         'colorscale': 'Portland'\n                       })\nplotly.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96b910935bfd7d4555eb3625c6fc24f37ecef025"},"cell_type":"code","source":"#adding filter to data\ndata[(data['thalach']>190)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f94bce6bfd4d27c395f5e77a4a184d06a005a42"},"cell_type":"code","source":"#adding filter to data\n\ndata[(data['age']>40) & (data['sex']==0)]\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc037ea7bbd10c56191e26f45745d405a354fb9"},"cell_type":"code","source":"#using swarmplot to learn about age, gender ad orobability of having heart disearse.\n#first append a new row about age measuremnet based on probability\nthreshold = sum(data.age)/len(data.age)\nthreshold_chol = sum(data.chol)/len(data.chol)\nprint(\"threshold of age:\" , threshold)\nprint(\"threshold of chol: \", threshold_chol)\n\ndata['probability'] = ['high' if i> threshold else 'low' for i in data.age]\ndata['probability']\n\nsns.swarmplot(x = 'sex', y = 'age', hue = \"probability\", data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a591ef8063e10fe44c1633243a9348b7aa9c85bb"},"cell_type":"code","source":"data_original = pd.read_csv(\"../input/heart.csv\")\ndata['sex'] = data_original.sex\ndata['sex']=['female' if i == 0 else 'male' for i in data.sex]\nplt.figure(figsize=(14,8))\nsns.swarmplot(x = 'age', y = 'chol', hue = \"sex\", data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ebe8360283314fa627bed77f368620b766ef5fe"},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89c61a2810e92a120ce81e1a317ab762480946ac"},"cell_type":"code","source":"#aplit the dataset ready for training and testing, and apply machine leaning ALG\nfrom sklearn.model_selection import train_test_split\nx,y = data.loc[:,data.columns != 'target'], data.loc[:,'target']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4840262ab0f07ae077f07199992a5fd8d1996011"},"cell_type":"code","source":"data_orginal = pd.read_csv(\"../input/heart.csv\")\ndata['sex']=data_orginal.sex\ndata.dtypes.sample(10)\n#data.select_dtypes(exclude=['object'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1de28138bd6ba56c037dd009dbfeb2b17adbc9bb"},"cell_type":"code","source":"#Machine learning by KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nknn = KNeighborsClassifier(n_neighbors = 3)\n#x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nx_train_one_hot_encoded = pd.get_dummies(x_train)\nx_test_one_hot_encoded = pd.get_dummies(x_test)\nknn.fit(x_train_one_hot_encoded,y_train)\nprediction = knn.predict(x_test_one_hot_encoded)\nacc = accuracy_score(y_test, prediction)\nk = knn.score(x_test_one_hot_encoded,y_test)\nprint(\"acc score with one hot encoded: \", acc)\nprint(\"knn score: \", k)\nprint(\"just only drop the non-numrical feature--->\")\nx_train_2 = x_train.select_dtypes(exclude=['object'])\nx_test_2 = x_test.select_dtypes(exclude=['object'])\nknn.fit(x_train_2,y_train)\nprediction = knn.predict(x_test_2)\nacc = accuracy_score(y_test, prediction)\nprint(\"acc score without one hot encoded: \", acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ae6c25b6c252c7d7a33b1bbc2e3b5f3f2955e5a"},"cell_type":"code","source":"# the training score is relativly low. I will apply Grid Search to find the best parameters of KNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"055a4bd76cfefd411516564b1521aa72dc1a10c3"},"cell_type":"code","source":"x_train = x_train.select_dtypes(exclude=['object'])\nx_test = x_test.select_dtypes(exclude=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"392651ac00bdd134a6529958749659e424cac58e"},"cell_type":"code","source":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,100)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78b775951c228cbc9238b5290043031c39b60446"},"cell_type":"code","source":"#After apply GridSearch the score is not much impoved","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"985a5ef59934381f05426fe912224b0a6278f4fe"},"cell_type":"code","source":"#apply sklearn decision tree classifier on the dataset\n#reference https://www.kaggle.com/drgilermo/playing-with-the-knobs-of-sklearn-decision-tree\nimport time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.metrics import confusion_matrix\nfrom subprocess import check_output\nfrom sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\nclassifier = DecisionTreeClassifier(max_depth = 3)\nclassifier.fit(x_train, y_train)\n\nprint(\"Decision tree score : {}\".format(classifier.score(x_test, y_test))) \n\nclf = DecisionTreeClassifier(max_depth = 3, criterion = \"entropy\")\nclf.fit(x_train,y_train)\nprint(\"Decision tree score : {}\".format(clf.score(x_test, y_test))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2df567943d72c81e82dc889b0400c3370175fcc8"},"cell_type":"code","source":"#apply best split and random split\nt = time.time()\nclf_random = DecisionTreeClassifier(max_depth = 3, splitter = 'random')\nclf_random.fit(x_train,y_train)\nprint('random Split accuracy...',clf_random.score(x_test,y_test))\nclf_best = DecisionTreeClassifier(max_depth = 3, splitter = 'best')\nclf_best.fit(x_train,y_train)\nprint('best Split accuracy...',clf_best.score(x_test,y_test))\n# conclusion: random split is not nessesary worse than best split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26b6600ca2822ee0248305723bb01475a82ea959"},"cell_type":"code","source":"with open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(clf_best,\n                              out_file=f,\n                              max_depth = 5,\n                              impurity = False,# true will show thw im-purity of each node~ gini value\n                              feature_names = x_test.columns.values,\n                             class_names = ['No', 'Yes'],\n                            #  class_names = True,\n                              rounded = True,\n                              filled= True )#False no color indication\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")\n# we have generared rhe random decision tree classifier, see below.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd9edcb676c3bf82d78c19bb28b546ca264ae1e0"},"cell_type":"markdown","source":"Conculsion, when using the best decision tree classifier, we have reached the best score around 70%. We could make  the following conclustions from the best_classifier decision tree:\n1. When a patient has typical anagina (cp type 1), but there no obvious result of anagina pain during excercise, then the patient have a high chance NO heart disease. (target as NO)\n2. When a patient shows not -typical anagina, with a low ST depression induced by exercise (oldpeak <= 2.1), the patinet is probabaly HAS heart disease. (target as YES)\n3. When a patient shows typical anagina (cp type 1), no obvious result of anagina pain during excercise, but shows a low number of major vessels (0-3) colored by flourosopy  (ca <= 0.5), then this patient has more than 50% HAVE heart disease. (target as YES)\n\nBesides, I have tried to generate decison tree by other data mining tool, such as RapidMiner, two of the results are shown below:\n"},{"metadata":{"trusted":true,"_uuid":"b818c6b0d22653bac6a132b73e3cd42e32e830bc"},"cell_type":"markdown","source":"\n![](C:/Users/s134225/Downloads/wetransfer-2abfb3/tree.JPG)"},{"metadata":{"_uuid":"14eafee87e1b6724da9e879ac30ffe807da94d1a"},"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/xuanxuanzhang/image/master/tree.JPG)\n"},{"metadata":{"_uuid":"b06b7df965be8387e813753168126771cbc06418"},"cell_type":"markdown","source":" Doc reference: https://docs.rapidminer.com/latest/studio/operators/modeling/predictive/trees/parallel_decision_tree.html\nThis decison tree is generated spilting attribute selection on:gain_ratio, and the maximal depth set to 4. \nFrom the 1st tree of RapidMiner, we could conclude the following:\n1. When a patient has non-typical anagina (cp type 2-4 , cp>0.5), and when resting with blood pressure (trestbps) higher than 179, ST depression(oldoeak) smaller than 2.45, have a high chance diagnose as heart disease. (target as YES)\n2. When a patient has typical anagina (cp type 1, cp< 0.9), alone with the maximum heart rate (thalach) reached over 181.50, then the patient as a high chance have heart disease. (target as YES)\n\n"},{"metadata":{"_uuid":"6cdf1dca4a277d6956f4130cbbe30153666ffc8c"},"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/xuanxuanzhang/image/master/tree2.JPG)"},{"metadata":{"_uuid":"5716bec5a2a42b0ed14aa2fc1ce20e812d6580d0"},"cell_type":"markdown","source":"When I have adjusted the settings with set the spliting attribute on the least entropy one (information_gain), maximum depth kept as 4, then we get the second decison tree, which age, sex ,ca, and thal involved for prediction. Then we could get the following conclusion:\n1. When a patient has non-typical anagina (cp type 2-4 , cp>0.5), but he/she is younger than 56, then he/she has a high chance to diagnose as heart disease.(target as YES)\n2. When a female patient has non-typical anagina (cp type 2-4 , cp>0.5) and older than 56, then she has a high chance to diagnose as heart disease.(target as YES)\n3. When a patient has a typical anagina (cp type 1, cp<= 0.5),  and he/she has number of major vessels bigger than 0.5 (ca> 0.5, maority patient has ca smaller than 0.4), then he/she probablty has NO heart disease. (target as NO)\n"},{"metadata":{"_uuid":"987192266e578f5e6a23dbdb9d7ea3d90d230b02"},"cell_type":"markdown","source":"\n"},{"metadata":{"_uuid":"1b5a9f53a2cbe475442adec169016853121dcc25"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}