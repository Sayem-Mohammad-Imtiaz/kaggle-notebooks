{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        URL = os.path.join(dirname, filename)\n        print(URL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport spacy\n\nimport datetime\nimport random\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.util.fix_random_seed(0)\nnp.random.seed(0)\nrandom.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(URL)\nprint(len(data))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop Columns/NA values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (data.drop(data.columns[0], axis=1)).dropna()\nprint(len(data))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordCloud of Insults & Targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_text = \" \".join(data.tweet)\n\nwordcloud = WordCloud(width=1500, height=500).generate(tweet_text)\n\nplt.figure( figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_text = \" \".join(data.target)\n\nwordcloud = WordCloud(width=1500, height=500).generate(target_text)\n\nplt.figure( figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insults per Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"(data.groupby(\"target\").target.count())\\\n    .nlargest(10)\\\n    .plot(kind=\"barh\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sorting by Date"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"date\"] = pd.to_datetime(data[\"date\"])\nprint(data[\"date\"].dtype)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"Insults over Years\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Percentage%\")\ndata[\"year\"] = data[\"date\"].dt.year\ninsults_over_year = data[\"year\"].value_counts().drop(2021) # 2021 is not completed as of yet\nsns.lineplot(insults_over_year.index, insults_over_year.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Classification to Year Tweeted"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.blank(\"en\")\n\ntextcat = nlp.create_pipe(\n              \"textcat\",\n              config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"bow\"})\n\nnlp.add_pipe(textcat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.unique(data[\"year\"].values).tolist()\nlabels = [textcat.add_label(str(label)) for label in labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = data[\"tweet\"].values\ntrain_label = [{'cats': {'2014': label == '2014',\n                          '2015': label == '2015',\n                          '2016': label == '2016',\n                          '2017': label == '2017',\n                          '2018': label == '2018',\n                          '2019': label == '2019',\n                          '2020': label == '2020',\n                          '2021': label == '2021',\n                          '2022': label == '2022'}} for label in data[\"year\"]]\n\ntrain_data = list(zip(train_text, train_label))\ntrain_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = nlp.begin_training()\n\nlosses = {}\nfor epoch in range(10):\n    random.shuffle(train_data)\n    batches = spacy.util.minibatch(train_data, size=4)\n    for batch in batches:\n        texts, labels = zip(*batch)\n        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n    print(losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = [\"GET SLEEPY JOE\", \n         \"GET HILLARY CLINTON\",]\ndocs = [nlp.tokenizer(text) for text in texts]\n\ntextcat = nlp.get_pipe('textcat')\nscores, _ = textcat.predict(docs)\n\nprint(scores)\n\npredicted_labels = scores.argmax(axis=1)\nprint([textcat.labels[label] for label in predicted_labels])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}