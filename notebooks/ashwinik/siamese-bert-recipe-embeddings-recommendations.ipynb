{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Created By : Ashwini Kumar\n### Dated : 14th Oct 2020\n#### Objective : The idea of this project is to check out the feasibility of using recipes to identify similar food item. We will use sentence embeddings to get embedding for complete recipe and the calculate cosine similarity between them. \n#### Also, we will try to use community detection algorithms to identify communities of recipes maybe used for a lot of other purposes\n\n#### Data Source : The source of data comes from food.com uploaded on kaggle which has recipes and recipes ratings csv\n"},{"metadata":{},"cell_type":"markdown","source":"#### Import all the packages required for creating this algorithms "},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install sentence-transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os \nimport ast\nimport sentence_transformers  #### This is the package which we will use for encoding recipes using pretrained embedding\nimport matplotlib.pyplot as plt \nimport networkx as nx #### Network x will be used to create graph based algorithms\nimport pickle ### We will use pickleto save files for later access\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity ### Cosine Similary\nfrom scipy import sparse ### Sparse Matrix\nmodel = SentenceTransformer('bert-large-nli-stsb-mean-tokens') ### We will use this senetnce encodings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Read the interactions csv as required \ninteractions = pd.read_csv('../input/food-com-recipes-and-user-interactions/RAW_interactions.csv')\n\nprint (\"Lets look at the basic stats about the data\")\nprint (\"The shape of the data is\")\nprint (interactions.shape)\nprint (\" The columns in the data are as follows\")\nprint (interactions.columns)\nprint (\" The first few columns are \")\nprint (interactions.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions['rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Interestingly most of the recipes are rated at highest rating. \ninteractions.groupby('recipe_id')['rating'].mean().reset_index().rating.plot(kind ='hist',title='Histograms of Avg Rating Recipe')\nplt.xlabel(\"Average Ratings\")\nplt.ylabel(\"Number of recipes\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will restrict our analysis only to those recipes which has been reviewed by more than 2 people\n### Analysis have shown that most recipes are only added but never seen"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Print for the poc purposes we will restrict\ng = {'rating' : ['mean'],'user_id' : ['nunique']}\nint_summary = interactions.groupby(['recipe_id']).agg(g).reset_index()\n### Its gives a muti index output convert it to single index by cobining bothe level\nind = pd.Index([e[0] + \"_\" +e[1] for e in int_summary.columns.tolist()])\n### Assign the column names \nint_summary.columns = ind\nint_summary.columns = ['recipe_id', 'rating_mean', 'user_id_nunique']\n### We will keep only those recipes in considerstaion which have been reviewed by more than 2 people\nint_summary_94k = int_summary[ (int_summary['user_id_nunique'] > 2)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read the recipes data and keep only data for recipes which are reviewed by more than 2 people"},{"metadata":{"trusted":true},"cell_type":"code","source":"recipes = pd.read_csv('../input/food-com-recipes-and-user-interactions/RAW_recipes.csv')\nprint (recipes.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do the inner join with subset data"},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_recipe = pd.merge(recipes,int_summary_94k,right_on = ['recipe_id'],left_on = ['id'],how = 'inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_recipe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The steps in recipes are as a list, create a single text out of it"},{"metadata":{"trusted":true},"cell_type":"code","source":"### The steps recipe is in list. We will combine list into one string\nfilter_recipe['dish_recipe'] = filter_recipe['steps'].apply(lambda x : \" \".join(ast.literal_eval(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## We will encode the recipes and store it in pickle file\nencodings_recipe= model.encode(filter_recipe['dish_recipe'])\npickle.dump(encodings_recipe,open(\"recipe_embedding.pickle\",'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Load the pickle files of encoding and create a dataframe out of it\nencodings_recipe_df = pickle.load(open(\"recipe_embedding.pickle\",'rb'))\nprint (\"Encoding are loaded\")\ndata_encoding = pd.DataFrame(encodings_recipe_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### As we don't have enough memory create cosine similary for only 5000 recipes\nencoding_sparse = sparse.csr_matrix(encodings_recipe_df[0:10000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# similarities = cosine_similarity(encoding_sparse)\n# print('pairwise dense output:\\n {}\\n'.format(similarities))\n\n#also can output sparse matrices\nimport datetime\ntime = datetime.datetime.now()\nprint (time)\nsimilarities_sparse = cosine_similarity(encoding_sparse)\n# print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\nprint (\"Time taken is :\",)\nprint (datetime.datetime.now()-time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Convert the pickle file to datafarme and dump\ndf1 = pd.DataFrame(similarities_sparse)\npickle.dump(df1,open('similarities_sparse.pickle','wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_similarity = df1.unstack().reset_index() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_similarity.columns = ['recipe1','recipe2','cosine_similarity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filter out too high score as it is cosine similarity with itself and too low scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_similarity = data_similarity[data_similarity['cosine_similarity']<0.9999]\ndata_similarity = data_similarity[data_similarity['cosine_similarity']>0.6]\nprint (data_similarity.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a hash map for dictionary and id"},{"metadata":{"trusted":true},"cell_type":"code","source":"recipe_dict = {}\nfor j,i in enumerate(filter_recipe['name']):\n    recipe_dict[j] = i\nprint (\"Dictionary is created :\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_similarity['recipe1_name'] = data_similarity['recipe1'].map(recipe_dict)\ndata_similarity['recipe2_name'] = data_similarity['recipe2'].map(recipe_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_similarity.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rank products based on similarity score"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_similarity['similarity_rank'] = data_similarity.groupby(['recipe1'])['cosine_similarity'].rank(\"dense\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_similarity = data_similarity[data_similarity['similarity_rank'] <= 5].reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create an algorithm for finding similar dishes based on recipes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_similar_dishes(list_names):\n    for i in list_names:\n        dummy_data =  data_similarity[data_similarity['recipe1_name'] == i]\n        print (\"As you liked dish :\",i)\n        print (\"You must try following 4 dishes with slight variations\")\n        dummy_data.sort_values(inplace = True,by =['similarity_rank']) \n        for j,i in enumerate(dummy_data['recipe2_name'].unique()):\n            print (\"             \", i)\n            if j == 3:\n                break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets see the results from the results\n\nAlgorithm 1 : Given a recipe return top 4 similar recipes\nIn this function we make use of cosine similarity between input recipe and all other recipes vector and return top 4 recipes with highest cosine similarity. It is interesting to note that with this we get similar recipes without any transaction data\n\nAlso, the similarity varies at various level. if you look at examples below\n\nExample 1 : You get prodcts similar based on ingredients but also based on type i.e Desserts \n\nExample 2 : It returns recipes which are similar because they follows same steps \n\nExample 3 : It returns you burgers with differenr preprations and ingredients\n\nExample 4 : It return products which have similar main ingredients i.e. Potato but different prep strategies \n\nExample 5 : For Taco, you get all mexican recipes because maybe they have similar preparation strategy"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfind_similar_dishes(['banana walnut cake','aaloo mattar   indian style peas and potatoes','avocado ranch burgers with smoked cheddar',\n                    'bird s perfect baked potatoes','bird s ultimate taco salad'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Community of recipes based on Similarity Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"G = nx.from_pandas_edgelist(data_similarity,'recipe1_name','recipe2_name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_sample = data_similarity\nimport matplotlib.pyplot as plt\nimport networkx as nx\nplt.figure(figsize=(250,250))\nplt.rcParams['axes.facecolor'] ='white'\nG = nx.Graph()\nfor i in range(0,5000):\n    G.add_edge(data_sample['recipe1_name'][i], data_sample['recipe2_name'][i], weight=data_sample['cosine_similarity'][i])\n\nelarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 0.8]\n\n\npos = nx.spring_layout(G)  # positions for all nodes\n\n# nodes\nnx.draw_networkx_nodes(G, pos, node_size=50)\n\n# edges\nnx.draw_networkx_edges(G, pos, edgelist=elarge,\n                       width=5)\n\n\n# labels\nnx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\nplt.savefig(\"Recipe_Community.pdf\", bbox_inches='tight')\nplt.axis('off')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Detect Recipe communities using graph based algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx.algorithms.community as nxcom\ncommunities = sorted(nxcom.greedy_modularity_communities(G), key=len, reverse=True)\n    # Count the communities\nprint(f\"The Recipe data has {len(communities)} communities.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in communities:\n    print (i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trying out different visualisation for communities\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = nx.spring_layout(G, k=0.1)\nplt.rcParams.update({'figure.figsize': (15, 10)})\nnx.draw_networkx(\n    G, \n    pos=pos, \n    node_size=0, \n    edge_color=\"#444444\", \n    alpha=0.05, \n    with_labels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"communities = sorted(nxcom.greedy_modularity_communities(G), key=len, reverse=True)\nlen(communities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_node_community(G, communities):\n    '''Add community to node attributes'''\n    for c, v_c in enumerate(communities):\n        for v in v_c:\n            # Add 1 to save 0 for external edges\n            G.nodes[v]['community'] = c + 1\n\ndef set_edge_community(G):\n    '''Find internal edges and add their community to their attributes'''\n    for v, w, in G.edges:\n        if G.nodes[v]['community'] == G.nodes[w]['community']:\n            # Internal edge, mark with community\n            G.edges[v, w]['community'] = G.nodes[v]['community']\n        else:\n            # External edge, mark as 0\n            G.edges[v, w]['community'] = 0\n\ndef get_color(i, r_off=1, g_off=1, b_off=1):\n    '''Assign a color to a vertex.'''\n    r0, g0, b0 = 0, 0, 0\n    n = 16\n    low, high = 0.1, 0.9\n    span = high - low\n    r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n    g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n    b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n    return (r, g, b)          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update(plt.rcParamsDefault)\nplt.rcParams.update({'figure.figsize': (15, 10)})\n\n\n# Set node and edge communities\nset_node_community(G, communities)\nset_edge_community(G)\n\n# Set community color for internal edges\nexternal = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] == 0]\ninternal = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] > 0]\ninternal_color = [\"black\" for e in internal]\nnode_color = [get_color(G.nodes[v]['community']) for v in G.nodes]\n# external edges\nnx.draw_networkx(\n    G, \n    pos=pos, \n    node_size=0, \n    edgelist=external, \n    edge_color=\"silver\",\n    node_color=node_color,\n    alpha=0.2, \n    with_labels=False)\n# internal edges\nnx.draw_networkx(\n    G, pos=pos, \n\n    edgelist=internal, \n    edge_color=internal_color,\n    node_color=node_color,\n    alpha=0.5, \n    with_labels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## For a deatiles readup on this please refer to this github link readme file : https://github.com/Ashwinikumar1/NLP-DL/tree/master/Recipe_Recommendation_Using_Recipe%20Embedding","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}