{"cells":[{"metadata":{},"cell_type":"markdown","source":"**1. IMPORTING LIBRARIES**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))\n\ndata = pd.read_csv('../input/weatherAUS.csv')\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. DATA PREPARATION**DATA PREPARATION"},{"metadata":{},"cell_type":"markdown","source":"Importing Data and Cleaning Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata.drop(['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RISK_MM'], axis=1, inplace=True)\ndata.head(5)\ndata.fillna(data.mean(), inplace=True)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.3. Converting Predictions to Binary for Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can change that day and next days'predictions (yes and no) to 1 and 0:\ndata.RainToday = [1 if each == 'Yes' else 0 for each in data.RainToday]\ndata.RainTomorrow = [1 if each == 'Yes' else 0 for each in data.RainTomorrow]\ndata.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.4. Excluding Tomorrow's Prediction from the Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.RainTomorrow.values\nx_data = data.drop('RainTomorrow', axis=1)\nx_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.5. Normalization Progress**"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to scale all the features between 0 and 1:\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\nx.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.6. Dividing Dataset for Training and Testing the Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing sklearn's library for splitting our dataset:\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=75)\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\nprint('x_train shape is: ', x_train.shape)\nprint('y_train shape is: ', y_train.shape)\nprint('x_test shape is: ', x_test.shape)\nprint('y_test shape is: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. LOGISTIC REGRESSION**"},{"metadata":{},"cell_type":"markdown","source":"**3.1. Creating the Initial Parameters (Weight and Bias)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weight_bias(dimension):\n    w = np.full((dimension,1), 0.01)    # Create a matrix by the size of (dimension,1) and fill it with the values of 0.01\n    b = 0.0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.2. Defining the Sigmoid Function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    y_head = 1 / (1 + np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.3. Defining Forward and Backward Propagation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w, b, x_train, y_train):\n    # forward propagation:\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    \n    loss = -(1 - y_train) * np.log(1 - y_head) - y_train * np.log(y_head)     # loss function formula\n    cost = (np.sum(loss)) / x_train.shape[1]                               # cost function formula\n    \n    # backward propagation:\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    \n    gradients = {'derivative_weight': derivative_weight, 'derivative_bias': derivative_bias}\n    \n    return cost, gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.4. Defining Update Parameters Method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate, nu_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Initialize for-back propagation for the number of iteration times. Then updating w and b values and writing the cost values to a list:  \n    for i in range(nu_of_iteration):\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n    \n        # Update weight and bias values:\n        w = w - learning_rate * gradients['derivative_weight']\n        b = b - learning_rate * gradients['derivative_bias']\n        # Show every 20th value of cost:\n        if i % 20 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print('Cost after iteration %i: %f' %(i,cost))\n    \n    parameters = {'weight': w, 'bias':b}\n    \n    # Visulization of cost values:\n    plt.plot(index, cost_list2)\n    plt.xlabel('Nu of Iteration')\n    plt.ylabel('Cost Function Value')\n    plt.show()\n    \n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.5. Defining Prediction Method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(w, b, x_test):\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.6. Implementing Logistic Regression Using Test Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, nu_of_iteration):\n    dimension = x_train.shape[0]\n    w, b = initialize_weight_bias(dimension)    \n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, nu_of_iteration)\n    \n    y_test_predictions = prediction(parameters['weight'], parameters['bias'], x_test) \n    \n    print('Test accuracy: {}%'.format(100 - np.mean(np.abs(y_test_predictions - y_test))*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, nu_of_iteration=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. LOGISTIC REGRESSION WITH SKLEARN LIBRARY**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\nprint('Test accuracy of sklearn logistic regression library: {}'.format(lr.score(x_test.T, y_test.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}