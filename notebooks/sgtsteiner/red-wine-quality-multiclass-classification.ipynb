{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wine Quality Prediction - Part 2 - Multiclass Classification"},{"metadata":{},"cell_type":"markdown","source":"![](https://cdn.pixabay.com/photo/2016/03/09/11/53/wine-glasses-1246240_1280.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"This notebook is part of a trilogy in which I will approach the wine quality dataset from several different approaches:\n\n+ [Part 1: Supervised Learning - Regression](https://www.kaggle.com/sgtsteiner/red-wine-quality-regression)\n+ Part 2: Supervised Learning - Multiclass Classification\n+ [Part 3: Supervised Learning - Binary Classification](https://www.kaggle.com/sgtsteiner/red-wine-quality-binary-classification)"},{"metadata":{},"cell_type":"markdown","source":"# Frame the problem"},{"metadata":{},"cell_type":"markdown","source":"We have a dataset that contains various characteristics of red and white variants of the Portuguese \"Vinho Verde\" wine. We have chemical variables, such as the amount of alcohol, citric acid, acidity, density, pH, etc; as well as a sensorial and subjective variable such as the score with which a group of experts rated the quality of the wine: between 0 (very bad) and 10 (very excellent).\n\nThey ask us to build a model that can predict the quality score given these biochemical indicators.\n\nIn the [first part of this analysis](https://www.kaggle.com/sgtsteiner/red-wine-quality-regression) we approach the problem as supervised learning - regression. We cannot consider the resulting model satisfactory. Let's consider the problem as supervised learning - multiclass classification."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score \nfrom sklearn.model_selection import cross_validate, cross_val_predict, StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn import metrics\n\nimport xgboost as xgb\n\nimport pandas_profiling\n\n%matplotlib inline\n\nseed = 42\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"red = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"red.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red.profile_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are not going to delve into data exploration, as we already did in the [first part of this analysis](https://www.kaggle.com/sgtsteiner/red-wine-quality-regression). The only preprocessing we are going to do is convert the target variable \"quality\" to categorical."},{"metadata":{"trusted":true},"cell_type":"code","source":"red[\"quality_cat\"] = red[\"quality\"].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red[\"quality_cat\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Percentage of quality scores\")\nred[\"quality_cat\"].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is significantly unbalanced. Most instances (82%) have scores of 6 or 5."},{"metadata":{},"cell_type":"markdown","source":"# Select and train models"},{"metadata":{},"cell_type":"markdown","source":"The goal of this phase is to train many models quickly and unrefined, of different categories (i.e. Random Forests, AdaBoost, Extra Tree, etc.) using the standard parameters. The idea is to have a quick overview of which models are most promising. Measure and compare the performance of all of them. Select the best models.\n\nCreate the predictor dataset and the dataset with the target variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_columns = red.columns[:-2]\npredict_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = red[predict_columns]\ny = red[\"quality_cat\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the training and test datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    random_state=42, \n                                                    test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline"},{"metadata":{},"cell_type":"markdown","source":"First, we are going to train a dummy classifier that we will use as a baseline with which to compare."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dummy = DummyClassifier(strategy=\"uniform\", random_state=seed) # Random prediction\nclf_dummy.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(clf_dummy, X_train, y_train, cv=3, \n                scoring=\"accuracy\", n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, a classifier that predicts randomly obtains an accuracy of 16%."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Always predict the most frequent class\nclf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=seed) \nclf_dummy.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(clf_dummy, X_train, y_train, cv=3, \n                scoring=\"accuracy\", n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A classifier that always predicts the most frequent class (in our case the quality score 6) obtains an accuracy of 43%. We are going to take the prediction of this dummy classifier as our baseline."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = cross_val_predict(clf_dummy, X_train, y_train, cv=3, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mx = metrics.confusion_matrix(y_train, preds)\nconf_mx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train, preds, rownames = ['Real'], colnames =['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", \n                 xticklabels=clf_dummy.classes_,\n                 yticklabels=clf_dummy.classes_,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_base = metrics.accuracy_score(y_train, preds)\nprecision_base = metrics.precision_score(y_train, preds, average='weighted', \n                                         zero_division=0)\nrecall_base = metrics.recall_score(y_train, preds, average='weighted')\nf1_base = metrics.f1_score(y_train, preds, average='weighted')\nprint(f\"Accuracy: {accuracy_base}\")\nprint(f\"Precision: {precision_base}\")\nprint(f\"Recall: {recall_base}\")\nprint(f\"f1: {f1_base}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, preds, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dummy classifier is correct only 19% of the time (precision) and detects 43% of the actual scores (recall). It is often convenient to combine precision and sensitivity into a single metric called the F1 score, particularly if we need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and sensitivity. While the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only score high on F1 if both sensitivity and precision are high. In our case, F1 = 0.26. Okay, let's take these three metrics as our initial baseline."},{"metadata":{},"cell_type":"markdown","source":"## Shortlist Promising Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(estimator, X_train, y_train, cv=5, verbose=True):\n    \"\"\"Print and return cross validation of model\n    \"\"\"\n    scoring = {\"accuracy\": \"accuracy\",\n               \"precision\": \"precision_weighted\",\n               \"recall\": \"recall_weighted\",\n               \"f1\": \"f1_weighted\"}\n    scores = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring)\n    \n    accuracy, accuracy_std = scores['test_accuracy'].mean(), \\\n                                scores['test_accuracy'].std()\n    \n    precision, precision_std = scores['test_precision'].mean(), \\\n                                scores['test_precision'].std()\n    \n    recall, recall_std = scores['test_recall'].mean(), \\\n                                scores['test_recall'].std()\n    \n    f1, f1_std = scores['test_f1'].mean(), scores['test_f1'].std()\n\n    \n    result = {\n        \"Accuracy\": accuracy,\n        \"Accuracy std\": accuracy_std,\n        \"Precision\": precision,\n        \"Precision std\": precision_std,\n        \"Recall\": recall,\n        \"Recall std\": recall_std,\n        \"f1\": f1,\n        \"f1 std\": f1_std,\n    }\n    \n    if verbose:\n        print(f\"Accuracy: {accuracy} - (std: {accuracy_std})\")\n        print(f\"Precision: {precision} - (std: {precision_std})\")\n        print(f\"Recall: {recall} - (std: {recall_std})\")\n        print(f\"f1: {f1} - (std: {f1_std})\")\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [GaussianNB(), KNeighborsClassifier(), RandomForestClassifier(random_state=seed),\n          DecisionTreeClassifier(random_state=seed), ExtraTreeClassifier(random_state=seed), \n          AdaBoostClassifier(random_state=seed), GradientBoostingClassifier(random_state=seed), \n          xgb.XGBClassifier()]\n\nmodel_names = [\"Naive Bayes Gaussian\", \"K Neighbors Classifier\", \"Random Forest\",\n               \"Decision Tree\", \"Extra Tree\", \"Ada Boost\", \n               \"Gradient Boosting\", \"XGBoost\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nfor model in range(len(models)):\n    print(f\"Step {model+1} de {len(models)}\")\n    print(f\"...running {model_names[model]}\")\n    \n    clf_scores = evaluate_model(models[model], X_train, y_train)\n    \n    accuracy.append(clf_scores[\"Accuracy\"])\n    precision.append(clf_scores[\"Precision\"])\n    recall.append(clf_scores[\"Recall\"])\n    f1.append(clf_scores[\"f1\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the performance of each of them:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = pd.DataFrame({\"Model\": model_names,\n                          \"accuracy\": accuracy,\n                          \"precision\": precision,\n                          \"recall\": recall,\n                          \"f1\": f1})\ndf_result.sort_values(by=\"f1\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to visualize the comparison of the different models / metrics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_list = [\"f1\", \"accuracy\", \"precision\", \"recall\"]\n\nfor metric in metrics_list:\n    df_result.sort_values(by=metric).plot.barh(\"Model\", metric)\n    plt.title(f\"Model by {metric}\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best performing model is Random Forest. Let's examine the execution of Random Forest a little more in detail:"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf = RandomForestClassifier(random_state=seed)\npreds = cross_val_predict(clf_rf, X_train, y_train, cv=5, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train, preds, rownames = ['Real'], colnames =['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, preds, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model is correct 66% of the time (precision) and detects 68% of the actual scores (recall). The F1 score is 0.66. Well, it has improved our baseline significantly (remember, precision = 19%, recall = 43%, and F1 = 0.26).\n\nWhen examining in detail the result of the predictions, we can see that it is terrible in the extreme scores (3, 4 and 8) and quite bad in the score 7."},{"metadata":{},"cell_type":"markdown","source":"# Fine-Tune"},{"metadata":{},"cell_type":"markdown","source":"We are going to do a hyperparameter adjustment to see if any improvement is achieved."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {\"n_estimators\": range(20, 200, 20), \n     \"bootstrap\": [True, False],\n     \"criterion\": [\"gini\", \"entropy\"],   \n     \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None],\n     \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n     \"min_samples_split\": [2, 5, 10],\n     \"min_samples_leaf\": [1, 2, 4],\n     }\n]\n\nclf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial fine-tune with Randomized Search"},{"metadata":{},"cell_type":"markdown","source":"First we do a random quick sweep:"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_random = RandomizedSearchCV(clf_rf, param_grid, n_iter = 200, cv = 5, \n                                scoring=\"f1_weighted\", verbose=2, \n                                random_state=seed, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = cross_val_predict(clf_random.best_estimator_, \n                          X_train, y_train, \n                          cv=5, n_jobs=-1)\nprint(metrics.classification_report(y_train, preds, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final fine-tune with GridSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {\"n_estimators\": range(20, 100, 10), \n     \"bootstrap\": [True, False],\n     \"criterion\": [\"gini\", \"entropy\"],   \n     \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None],\n     \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n     \"min_samples_split\": [2, 5, 10],\n     \"min_samples_leaf\": [1, 2, 4],\n     }\n]\n\n\nclf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search = GridSearchCV(clf_rf, param_grid, cv=5,\n                           scoring=\"f1_weighted\", verbose=2, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\npreds = cross_val_predict(final_model, X_train, y_train, cv=5, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicción'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adjusting hyperparameters, a very slight improvement is achieved over the default hyperparameters. It's correct 67% of the time (precision) and detects 70% of the actual scores (recall). The F1 score is 0.68. Which significantly improves our baseline (remember, precision = 19%, recall = 43% and F1 = 0.26).\n\nFinally let's see how it runs on the test set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = final_model.predict(X_test)\n\npd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicción'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's correct 63% of the time (precision) and detects 66% of the actual scores (recall). The F1 score is 0.66. Which significantly improves our baseline (remember, precision = 19%, recall = 43% and F1 = 0.26)."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mx = metrics.confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", \n                 xticklabels=final_model.classes_,\n                 yticklabels=final_model.classes_,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = final_model.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(zip(feature_importances, X_test.columns), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False)\nfeature_imp.plot(kind='bar')\nplt.title('Feature Importances');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"We are going to use RFECV to determine the number of valid features with cross-validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = RFECV(final_model, step=1, cv=StratifiedKFold())\nselector = selector.fit(X_train, y_train)\npd.DataFrame({\"Feature\": predict_columns, \"Support\": selector.support_})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel(\"No. of features selected\")\nplt.ylabel(\"Cross validation scores\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selector.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The conclusion is that all the variables are important for the model, since the maximum score is obtained with the 10 selected features."},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n\nOur starting baseline, obtained from a classifier that always predicts the most frequent class, is the following:\n\n+ Precision: **19%**\n+ Recall: **43%**\n+ Accuracy: **43%**\n+ f1: **0.26**\n\nAfter training various models, the one that has provided the best results is RandomForest. After fine-tuning the hyperparameters we obtain the following metrics:\n\n+ Precision: **67%**\n+ Recall: **70%**\n+ Accuracy: **70%**\n+ f1: **0.68**\n\nThe performance in the test set is as follows:\n\n+ Precision: **63**\n+ Recall: **66%**\n+ Accuracy: **66%**\n+ f1: **0.64**\n\nSince this is multiclass classification, we are talking about weighted scores. However, the scores obtained by each class are very different. It can be seen that the result is terrible in the extreme scores (3, 4 and 8). As we saw in the distribution of the target variable, it is very unbalanced, there are hardly any observations for the extreme values, so the model does not have enough training data for all the quality scores.\n\nAll predictor variables are relevant to the model. The three that most affect prediction are the following:\n\n+ alcohol\n+ sulphates\n+ volatile acidity\n\nIt could be interesting to evaluate the model by segmenting our target variable into quality ranges (for example, bad and good) and see if we obtain better results."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}