{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wine Quality Prediction - Part 3 - Binary Classification"},{"metadata":{},"cell_type":"markdown","source":"![](https://cdn.pixabay.com/photo/2016/03/09/11/53/wine-glasses-1246240_1280.jpg)"},{"metadata":{},"cell_type":"markdown","source":"This notebook is part of a trilogy in which I will approach the wine quality dataset from several different approaches:\n\n+ [Part 1: Supervised Learning - Regression](https://www.kaggle.com/sgtsteiner/red-wine-quality-regression)\n+ [Part 2: Supervised Learning - Multiclass Classification](https://www.kaggle.com/sgtsteiner/red-wine-quality-multiclass-classification)\n+ Part 3: Supervised Learning - Binary Classification"},{"metadata":{},"cell_type":"markdown","source":"In [the first part of this analysis](https://www.kaggle.com/sgtsteiner/red-wine-quality-regression) we approach the problem as supervised learning - regression. The resulting model was not satisfactory to us. [In a second analysis](https://www.kaggle.com/sgtsteiner/red-wine-quality-multiclass-classification), we approach the problem as supervised learning - multiclass classification. Given that the data we had was so unbalanced (the quality scores were concentrated on scores 5 or 6) the performance of our best model (RandomForest) was not quite good. In this third and final part, we will focus our analysis on a **supervised learning - binary classification** problem."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score \nfrom sklearn.model_selection import cross_validate, cross_val_predict\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn import metrics, utils\n\nimport xgboost as xgb\n\nimport pandas_profiling\n\n%matplotlib inline\n\nseed = 42\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"red = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"red.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red.profile_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are not going to delve into data exploration, as we already did in the [first part of this analysis](https://www.kaggle.com/sgtsteiner/red-wine-quality-regression). The only preprocessing that we are going to perform is to convert the target variable `quality` to categorical, indicating whether the score is good or not: `bad`: scores less than or equal to 5. `good`: scores greater than or equal to 6."},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [2, 5.5, 8]\nlabels = [\"bad\", \"good\"]\nred['quality_cat'] = pd.cut(red['quality'], bins=bins, labels=labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red[\"quality_cat\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Percentage of quality scores\")\nred[\"quality_cat\"].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red[\"quality_cat\"].value_counts().plot.pie(autopct='%1.2f%%');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Select and train models"},{"metadata":{},"cell_type":"markdown","source":"The goal of this phase is to train many models quickly and unrefined, of different categories (i.e. Random Forests, AdaBoost, Extra Tree, etc.) using the standard parameters. The idea is to have a quick overview of which models are most promising. Measure and compare the performance of all of them. Select the best models.\n\nCreate the predictor dataset and the dataset with the target variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_columns = red.columns[:-2]\npredict_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = red[predict_columns]\ny = red[\"quality_cat\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the training and test datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    random_state=42, \n                                                    test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline\n\nFirst, we are going to train a dummy classifier that we will use as a baseline with which to compare."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dummy = DummyClassifier(strategy=\"uniform\", random_state=seed) # Random prediction\nclf_dummy.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(clf_dummy, X_train, y_train, cv=3, \n                scoring=\"accuracy\", n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, a classifier that predicts randomly obtains an accuracy of 53%."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Always predict the most frequent class\nclf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=seed) \nclf_dummy.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A classifier that always predicts the most frequent class (in our case the `good` quality) also obtains an accuracy of 53%. We are going to take the prediction of this dummy classifier as our baseline."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = cross_val_predict(clf_dummy, X_train, y_train, cv=3, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mx = metrics.confusion_matrix(y_train, preds)\nconf_mx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicci√≥n'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(6,5))\nax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", \n                 xticklabels=clf_dummy.classes_,\n                 yticklabels=clf_dummy.classes_,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_base = metrics.accuracy_score(y_train, preds)\nprecision_base = metrics.precision_score(y_train, preds, \n                                         average='weighted', \n                                         zero_division=0)\nrecall_base = metrics.recall_score(y_train, preds, \n                                   average='weighted')\nf1_base = metrics.f1_score(y_train, preds, \n                           average='weighted')\nprint(f\"Accuracy: {accuracy_base}\")\nprint(f\"Precision: {precision_base}\")\nprint(f\"Recall: {recall_base}\")\nprint(f\"f1: {f1_base}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, preds, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dummy classifier is correct only 28% of the time (precision) and detects 53% of the actual scores (recall). It is often convenient to combine precision and sensitivity into a single metric called the F1 score, particularly if we need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and sensitivity. While the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only score high on F1 if both sensitivity and precision are high. In our case, F1 = 0.37. Okay, let's take these three metrics as our initial baseline."},{"metadata":{},"cell_type":"markdown","source":"## Shortlist Promising Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(estimator, X_train, y_train, cv=5, verbose=True):\n    \"\"\"Print and return cross validation of model\n    \"\"\"\n    scoring = {\"accuracy\": \"accuracy\",\n               \"precision\": \"precision_weighted\",\n               \"recall\": \"recall_weighted\",\n               \"f1\": \"f1_weighted\"}\n    scores = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring)\n    \n    accuracy, accuracy_std = scores['test_accuracy'].mean(), \\\n                                scores['test_accuracy'].std()\n    \n    precision, precision_std = scores['test_precision'].mean(), \\\n                                scores['test_precision'].std()\n    \n    recall, recall_std = scores['test_recall'].mean(), \\\n                                scores['test_recall'].std()\n    \n    f1, f1_std = scores['test_f1'].mean(), scores['test_f1'].std()\n\n    \n    result = {\n        \"Accuracy\": accuracy,\n        \"Accuracy std\": accuracy_std,\n        \"Precision\": precision,\n        \"Precision std\": precision_std,\n        \"Recall\": recall,\n        \"Recall std\": recall_std,\n        \"f1\": f1,\n        \"f1 std\": f1_std,\n    }\n    \n    if verbose:\n        print(f\"Accuracy: {accuracy} - (std: {accuracy_std})\")\n        print(f\"Precision: {precision} - (std: {precision_std})\")\n        print(f\"Recall: {recall} - (std: {recall_std})\")\n        print(f\"f1: {f1} - (std: {f1_std})\")\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [GaussianNB(), KNeighborsClassifier(), RandomForestClassifier(random_state=seed),\n          DecisionTreeClassifier(random_state=seed), ExtraTreeClassifier(random_state=seed), \n          AdaBoostClassifier(random_state=seed), GradientBoostingClassifier(random_state=seed), \n          xgb.XGBClassifier()]\n\nmodel_names = [\"Naive Bayes Gaussian\", \"K Neighbors Classifier\", \"Random Forest\",\n               \"Decision Tree\", \"Extra Tree\", \"Ada Boost\", \n               \"Gradient Boosting\", \"XGBoost\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nfor model in range(len(models)):\n    print(f\"Step {model+1} de {len(models)}\")\n    print(f\"...running {model_names[model]}\")\n    \n    clf_scores = evaluate_model(models[model], X_train, y_train)\n    \n    accuracy.append(clf_scores[\"Accuracy\"])\n    precision.append(clf_scores[\"Precision\"])\n    recall.append(clf_scores[\"Recall\"])\n    f1.append(clf_scores[\"f1\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the performance of each of them:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = pd.DataFrame({\"Model\": model_names,\n                          \"accuracy\": accuracy,\n                          \"precision\": precision,\n                          \"recall\": recall,\n                          \"f1\": f1})\ndf_result.sort_values(by=\"f1\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to visualize the comparison of the different models / metrics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_list = [\"f1\", \"accuracy\", \"precision\", \"recall\"]\n\nfor metric in metrics_list:\n    df_result.sort_values(by=metric).plot.barh(\"Model\", metric)\n    plt.title(f\"Model by {metric}\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best performing model is Random Forest. Let's examine the execution of Random Forest a little more in detail:"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf = RandomForestClassifier(random_state=seed)\npreds = cross_val_predict(clf_rf, X_train, y_train, cv=5, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train, preds, rownames = ['Real'], colnames =['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, preds, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model is correct 81% of the time (precision) and detects 81% of the actual scores (recall). The F1 score is 0.81. Well, it has improved our baseline significantly (remember, precision = 28%, recall = 53%, and F1 = 0.37)."},{"metadata":{},"cell_type":"markdown","source":"# Fine-Tune\n\nWe are going to do a hyperparameter adjustment to see if any improvement is achieved."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {\"n_estimators\": range(20, 200, 20), \n     \"bootstrap\": [True, False],\n     \"criterion\": [\"gini\", \"entropy\"],   \n     \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None],\n     \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n     \"min_samples_split\": [2, 5, 10],\n     \"min_samples_leaf\": [1, 2, 4],\n     }\n]\n\nclf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial fine-tune with Randomized Search\n\nFirst we do a random quick sweep:"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_random = RandomizedSearchCV(clf_rf, param_grid, n_iter = 200, cv = 5, \n                                scoring=\"f1_weighted\", verbose=2, \n                                random_state=seed, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = cross_val_predict(clf_random.best_estimator_, \n                          X_train, y_train, \n                          cv=5, n_jobs=-1)\nprint(metrics.classification_report(y_train, preds, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final fine-tune with GridSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {\"n_estimators\": range(20, 80, 10), \n     \"bootstrap\": [True, False],\n     \"criterion\": [\"gini\", \"entropy\"],   \n     \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None],\n     \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n     \"min_samples_split\": [2, 5, 10],\n     \"min_samples_leaf\": [1, 2, 4],\n     }\n]\n\nclf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search = GridSearchCV(clf_rf, param_grid, cv=5,\n                           scoring=\"f1_weighted\", verbose=2, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\npreds = cross_val_predict(final_model, X_train, y_train, cv=5, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train, preds, rownames = ['Real'], colnames =['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adjusting hyperparameters, a very slight improvement is achieved over the default hyperparameters. It's correct 82% of the time (precision) and detects 82% of the actual scores (recall). The F1 score is 0.82. Which significantly improves our baseline (remember, precision = 28%, recall = 53%, and F1 = 0.37).\n\nFinally let's see how it runs on the test set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = final_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_test, y_pred, rownames = ['Real'], colnames =['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's correct 78% of the time (precision) and detects 78% of the actual scores (recall). The F1 score is 0.78. Which significantly improves our baseline (remember, precision = 28%, recall = 53%, and F1 = 0.37)."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mx = metrics.confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", \n                 xticklabels=final_model.classes_,\n                 yticklabels=final_model.classes_,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = final_model.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(zip(feature_importances, X_test.columns), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False)\nfeature_imp.plot(kind='bar')\nplt.title('Feature Importances');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"We are going to use RFECV to determine the number of valid features with cross-validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = RFECV(final_model, step=1, cv=StratifiedKFold())\nselector = selector.fit(X_train, y_train)\npd.DataFrame({\"Feature\": predict_columns, \"Support\": selector.support_})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel(\"No. of features selected\")\nplt.ylabel(\"Cross validation scores\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selector.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The conclusion is that all the variables are important for the model, since the maximum score is obtained with the 10 selected features."},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n\nThis analysis has addressed the issue as a problem of supervised learning binary classification. Our starting baseline, obtained from a classifier that always predicts the most frequent class, is the following:\n\n+ Precision: **28%**\n+ Recall: **53%**\n+ Accuracy: **53%**\n+ f1: **0.37**\n\nAfter training various models, the one that has provided the best results is RandomForest. After fine-tuning the hyperparameters we obtain the following metrics:\n\n+ Precision: **82%**\n+ Recall: **82%**\n+ Accuracy: **82%**\n+ f1: **0.82**\n\nThe performance in the test set is as follows:\n\n+ Precision: **78%**\n+ Recall: **78%**\n+ Accuracy: **78%**\n+ f1: **0.78**\n\nAll predictor variables are relevant to the model. The three that most affect prediction are the following:\n\n+ alcohol\n+ sulphates\n+ volatile acidity"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}