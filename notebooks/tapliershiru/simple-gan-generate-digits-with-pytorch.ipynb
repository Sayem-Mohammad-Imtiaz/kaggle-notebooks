{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## In this notebook:\n- Training of the Vanilla GAN, for more details refer to paper: https://arxiv.org/abs/1406.2661\n- FID Metric. Calculate how good final generator with FID metric, for more details refer to paper: https://arxiv.org/abs/1706.08500","metadata":{}},{"cell_type":"markdown","source":"# Import libraries what we will use in this notebook","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport numpy as np\nimport struct\nimport pandas as pd\nimport os\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils","metadata":{"_uuid":"74a3bec8-f4bb-4860-a306-0ab4dfecec0c","_cell_guid":"5a5d67dd-355d-488b-ae18-444997026ebf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:12.566317Z","iopub.execute_input":"2021-08-15T15:13:12.566724Z","iopub.status.idle":"2021-08-15T15:13:14.437334Z","shell.execute_reply.started":"2021-08-15T15:13:12.566627Z","shell.execute_reply":"2021-08-15T15:13:14.436229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define global constants","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Noise shape for generator\nNOISE_SHAPE = 128\nBATCH_SIZE = 60\n# Image size and number of channels\nH, W, C = (28, 28, 1)\n\n# Its better then a number of images in dataset will be divided by batch size without remainder\n# So, algorithm below will print this numbers\n# By default batch size equal to 60 is okey\n\"\"\"\nfor i in range(3, 100):\n    if 60_000 % i == 0:\n        print(i)\n\"\"\"\n","metadata":{"_uuid":"dbcd8b02-0e5e-40fd-bebb-6b109df46b7a","_cell_guid":"e7b2b974-a9e2-4b5f-a2e4-531c59305b85","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:14.439312Z","iopub.execute_input":"2021-08-15T15:13:14.439759Z","iopub.status.idle":"2021-08-15T15:13:14.518946Z","shell.execute_reply.started":"2021-08-15T15:13:14.439705Z","shell.execute_reply":"2021-08-15T15:13:14.517657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data from CSV file","metadata":{}},{"cell_type":"markdown","source":"### We will map data into numpy array for better usage","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')\n# Create np array from csv\ndf_as_np = np.asarray(df)\n# Wrap images and labels\nlabels_mnist, data = (\n    df_as_np[:, 0],                      # First row - labels\n    df_as_np[:, 1:].reshape(-1, H, W, C) # Other rows - images\n)","metadata":{"_uuid":"fecb8007-326b-4f27-bc8f-cfd947228ee2","_cell_guid":"313c48c2-ca5c-4ff4-8160-c818bca2439a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:14.521348Z","iopub.execute_input":"2021-08-15T15:13:14.522326Z","iopub.status.idle":"2021-08-15T15:13:20.29026Z","shell.execute_reply.started":"2021-08-15T15:13:14.522281Z","shell.execute_reply":"2021-08-15T15:13:20.289025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define image generator","metadata":{}},{"cell_type":"markdown","source":"### Define class for DataLoader in order to create image generator","metadata":{}},{"cell_type":"code","source":"# Define class with super-class Dataset\n# We must implement two methods: getitem__ and __len__, \n#     __getitem__ - gives possibility to apply indexing for the instance of class FashionDataset\n#     __len__ - gives possibility to take size of overall dataset\n# This methods need in order to use DataLoader\nclass FashionDataset(Dataset):\n\n    def __init__(self, data, transform = None, H = 28, W = 28, C = 1):\n        self._images = np.asarray(data, dtype=np.float32).reshape(-1, H, W, C)\n        self._transform = transform\n\n    def __getitem__(self, index):\n        image = self._images[index]\n        if self._transform is not None:\n            image = self._transform(image)\n        return image\n    \n    def __len__(self):\n        return len(self._images)\n\n# Create instaince of data loader in order to load and create batches of data\n# Also we can specify number of workers in loader which can speed up process of \n# preparing data. We leave it as it is, with default value.\n# For more info refer to original docs.\ntrain_set = FashionDataset(\n    data, transform=transforms.Compose(\n        # Transform data into Tensor that has values in a range from -1 to 1\n        [transforms.ToTensor(), transforms.Normalize(128, 128)]\n    ),\n    H=H, W=W, C=C\n)\n# Create data loader\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"_uuid":"b703a1f4-6a26-4677-b056-e479049cf7d4","_cell_guid":"cbd11a5e-cc02-42a6-8ad5-75d68abef635","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:20.29337Z","iopub.execute_input":"2021-08-15T15:13:20.293859Z","iopub.status.idle":"2021-08-15T15:13:20.400149Z","shell.execute_reply.started":"2021-08-15T15:13:20.2938Z","shell.execute_reply":"2021-08-15T15:13:20.399038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test train loader. Print batch of images","metadata":{}},{"cell_type":"code","source":"# Test loader\nbatch_d = next(iter(train_loader))\ngrid = torchvision.utils.make_grid(batch_d, nrow=10)\n\nplt.figure(figsize=(15, 20))\nplt.imshow(np.transpose(grid, (1, 2, 0)).numpy().astype(np.uint8))","metadata":{"_uuid":"ec9e4dfb-9092-4ae0-8358-9ddaaf665ae2","_cell_guid":"88b4cce2-88eb-47aa-8580-73f6aaafd902","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:20.40281Z","iopub.execute_input":"2021-08-15T15:13:20.403424Z","iopub.status.idle":"2021-08-15T15:13:20.864336Z","shell.execute_reply.started":"2021-08-15T15:13:20.403378Z","shell.execute_reply":"2021-08-15T15:13:20.863211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Models","metadata":{}},{"cell_type":"markdown","source":"### Define some utils for layers/models","metadata":{}},{"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"_uuid":"5da8ecf6-2a54-4137-b9d0-74eca7aa18da","_cell_guid":"74c10de6-e317-4a0c-a5e7-1d9ce8a73bac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:20.865883Z","iopub.execute_input":"2021-08-15T15:13:20.866357Z","iopub.status.idle":"2021-08-15T15:13:20.874069Z","shell.execute_reply.started":"2021-08-15T15:13:20.866312Z","shell.execute_reply":"2021-08-15T15:13:20.872592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Generator model","metadata":{}},{"cell_type":"code","source":"class GeneratorNN(nn.Module):\n\n    def __init__(self):\n        super(GeneratorNN, self).__init__()\n        \n        self._model = nn.Sequential(\n            nn.Linear(NOISE_SHAPE, 256),\n            nn.BatchNorm1d(256, momentum=0.8, track_running_stats=False),\n            nn.ReLU(inplace=False),\n\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512, momentum=0.8, track_running_stats=False),\n            nn.ReLU(inplace=False),\n\n            nn.Linear(512, 1024),\n            nn.BatchNorm1d(1024, momentum=0.8, track_running_stats=False),\n            nn.ReLU(inplace=False),\n            \n            nn.Linear(1024, 2048),\n            nn.BatchNorm1d(2048, momentum=0.8, track_running_stats=False),\n            nn.ReLU(inplace=False),\n            \n            nn.Linear(2048, H * W * C),\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        x = self._model(x)\n        x = x.view(-1, C, H, W)\n        return x","metadata":{"_uuid":"d58b4636-f649-43d4-86c5-87a5523997a9","_cell_guid":"97fbb608-64f6-4409-8fa7-7e46c699ddc5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:23.071568Z","iopub.execute_input":"2021-08-15T15:13:23.071973Z","iopub.status.idle":"2021-08-15T15:13:23.081521Z","shell.execute_reply.started":"2021-08-15T15:13:23.071941Z","shell.execute_reply":"2021-08-15T15:13:23.080382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create instance of generator model and test it with noise","metadata":{}},{"cell_type":"code","source":"# Generator\ngen_nn = GeneratorNN()\ngen_nn.to(device=device)\n# Init weights of the model with certain initialization\ngen_nn.apply(weights_init)\n# Turn on training mode\ngen_nn.train()","metadata":{"_uuid":"f4e902af-de54-4e09-8df4-4118f7c36fd6","_cell_guid":"80379bb0-5d20-443a-b8fe-a665c4f7a4e3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:24.84195Z","iopub.execute_input":"2021-08-15T15:13:24.84234Z","iopub.status.idle":"2021-08-15T15:13:29.842964Z","shell.execute_reply.started":"2021-08-15T15:13:24.842307Z","shell.execute_reply":"2021-08-15T15:13:29.841929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check generator\narr = np.random.randn(BATCH_SIZE, NOISE_SHAPE).astype(np.float32)\nres = gen_nn(torch.tensor(arr).to(device=device))\nprint(res.shape)\nplt.imshow( ((res + 1.0) / 2.0)[0].cpu().detach().numpy().transpose(1, 2, 0)[..., 0])","metadata":{"_uuid":"15386b70-b512-41b0-8bbb-7266bac273d8","_cell_guid":"5f9c5434-8a88-4382-a8ed-91e9c54e8e61","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:29.846449Z","iopub.execute_input":"2021-08-15T15:13:29.846807Z","iopub.status.idle":"2021-08-15T15:13:30.954298Z","shell.execute_reply.started":"2021-08-15T15:13:29.846762Z","shell.execute_reply":"2021-08-15T15:13:30.953175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Discriminator model","metadata":{}},{"cell_type":"code","source":"class DiscriminatorNN(nn.Module):\n\n    def __init__(self):\n        super(DiscriminatorNN, self).__init__()\n\n        self._net = nn.Sequential(\n            nn.Linear(H * W * C, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Linear(512, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Dropout(p=0.2),\n            nn.Linear(512, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Dropout(p=0.3),\n            nn.Linear(512, 1),\n            nn.Sigmoid(),\n        )\n    \n    def forward(self, x):\n        b = x.shape[0]\n        return self._net(x.view(b, -1))","metadata":{"_uuid":"56571439-efc3-420f-b4c0-ecee8cd31197","_cell_guid":"3c1b3a2f-25f0-465a-a194-02549346103b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:30.95624Z","iopub.execute_input":"2021-08-15T15:13:30.956699Z","iopub.status.idle":"2021-08-15T15:13:30.966069Z","shell.execute_reply.started":"2021-08-15T15:13:30.956654Z","shell.execute_reply":"2021-08-15T15:13:30.964744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create instance of discriminator model and test it with noise","metadata":{}},{"cell_type":"code","source":"# Discriminator\ndisc_nn = DiscriminatorNN()\ndisc_nn.to(device=device)\n# Init weights of the model with certain initialization\ndisc_nn.apply(weights_init)\n# Turn on training mode\ndisc_nn.train()","metadata":{"_uuid":"168fc792-724c-4fc7-8f1a-85dbc2f1db39","_cell_guid":"81913fdd-3a52-4bcc-bedf-3a3ad994af40","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:32.770045Z","iopub.execute_input":"2021-08-15T15:13:32.770475Z","iopub.status.idle":"2021-08-15T15:13:32.800734Z","shell.execute_reply.started":"2021-08-15T15:13:32.770443Z","shell.execute_reply":"2021-08-15T15:13:32.799261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check discriminator on noise data\narr = np.random.randn(BATCH_SIZE, C, H, W).astype(np.float32)\nprint(arr.shape)\nres = disc_nn(torch.tensor(arr, device=device))\nres.cpu().detach().numpy()[:5]","metadata":{"_uuid":"ac084dda-b32a-4fce-9ce6-6292df01a5a9","_cell_guid":"c2ccec04-8c92-4cee-8950-7f2502fe1aee","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:33.415964Z","iopub.execute_input":"2021-08-15T15:13:33.416385Z","iopub.status.idle":"2021-08-15T15:13:33.450455Z","shell.execute_reply.started":"2021-08-15T15:13:33.416338Z","shell.execute_reply":"2021-08-15T15:13:33.449357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check discriminator on real data from loaded test batch above\nprint(batch_d.shape)\nres = disc_nn(torch.tensor(batch_d, device=device))\nres.cpu().detach().numpy()[:5]","metadata":{"_uuid":"df9410db-8017-43cc-984d-4e4a58b2c66c","_cell_guid":"ead05aff-560a-4458-b528-b714721a1ba8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:33.679949Z","iopub.execute_input":"2021-08-15T15:13:33.680361Z","iopub.status.idle":"2021-08-15T15:13:33.694912Z","shell.execute_reply.started":"2021-08-15T15:13:33.680328Z","shell.execute_reply":"2021-08-15T15:13:33.693647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"### Define class which control training of GAN. \n### Main method: `fit` function which start training of a GAN","metadata":{}},{"cell_type":"code","source":"class TrainGANController:\n    \n    def __init__(self, disc_nn, gen_nn, batch_size, device = None):\n        self._disc_nn = disc_nn\n        self._gen_nn = gen_nn\n        self._batch_size = batch_size\n\n        self._is_compiled = False\n        self._opt_disc = None\n        self._opt_gen = None\n        self._loss = None\n        self._device = device\n    \n    def compile(\n            self, \n            lr_disc=2e-4, lr_gen=3e-4, \n            beta_params_disc=(0.5, 0.999), beta_params_gen=(0.5, 0.999)):\n        # Init opt\n        self._opt_disc = torch.optim.Adam(\n            self._disc_nn.parameters(), lr=lr_disc, betas=beta_params_disc\n        )\n        self._opt_gen = torch.optim.Adam(\n            self._gen_nn.parameters(), lr=lr_gen, betas=beta_params_gen\n        )\n        # Losses\n        self._loss = nn.BCELoss().to(device=self._device)\n        # Set flag, in order to start train\n        self._is_compiled = True\n    \n    def train_step_disc(self, real_data, real_label=0.9, fake_label=0.0):\n        # Set real label equal to 0.9 in order to use \"Label smoothing\"\n        # Discriminator can produce better gradients, then this technique is used\n        # For more detailы about label smoothing you can find in the internet \n        \n        # For easy access\n        device = self._device\n        # Train step for discriminator\n        # Zero grads\n        self._disc_nn.zero_grad()\n        # Forward pass for real data\n        label = torch.full((self._batch_size,), real_label, dtype=torch.float, device=device)\n        fake = torch.full((self._batch_size,), fake_label, dtype=torch.float, device=device)\n        # Generate fake stuf\n        noise = torch.randn(self._batch_size, NOISE_SHAPE, device=device)\n        generated_imgs = self._gen_nn(noise)\n        # Forward pass real batch through D\n        errD_real = self._loss(self._disc_nn(real_data).view(-1), label)\n        # Forward pass fake batch through D\n        errD_fake = self._loss(self._disc_nn(generated_imgs.detach()).view(-1), fake)\n        errD = (errD_fake + errD_real) / 2.0\n        errD.backward()\n        self._opt_disc.step()\n        return errD.cpu().detach().numpy()\n\n    def train_step_gen(self, fake_label=1.0):\n        # For easy access\n        device = self._device\n        # Train step for generator\n        # Zero grads\n        self._gen_nn.zero_grad()\n        # fake labels are real for generator cost\n        label = torch.full((self._batch_size,), fake_label, dtype=torch.float, device=device)\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        # Generate batch of latent vectors\n        noise = torch.randn(self._batch_size, NOISE_SHAPE, device=device)\n        # Generate fake image batch with G\n        generated_imgs = self._gen_nn(noise)\n        # Calculate G's loss based on this output\n        errG = self._loss(self._disc_nn(generated_imgs).view(-1), label)\n        # Calculate gradients for G\n        errG.backward()\n        # Update G\n        self._opt_gen.step()\n        return errG.cpu().detach().numpy()\n\n    def fit(self, data_gen, epoch: int, print_it: int = 400):\n        for i_e in range(epoch):\n            for ii_it, single_data in enumerate(data_gen):\n                single_data = single_data.to(device=self._device)\n                # Train discriminator\n                err_d = self.train_step_disc(single_data)\n                # Train generator\n                err_g = self.train_step_gen()\n                if ii_it % print_it == 0:\n                    print(f'epoch: {i_e+1}/{epoch}, it: {ii_it}/{len(data_gen)}'\n                          f'|| Loss G: {err_g}, Loss D: {err_d}'\n                    )","metadata":{"_uuid":"5229d9e3-5b12-48b5-80d9-df1ffadbf411","_cell_guid":"95ad8e1a-eaaf-4721-8db3-edb2c4f717f1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:36.957392Z","iopub.execute_input":"2021-08-15T15:13:36.957752Z","iopub.status.idle":"2021-08-15T15:13:36.978187Z","shell.execute_reply.started":"2021-08-15T15:13:36.957719Z","shell.execute_reply":"2021-08-15T15:13:36.97667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create instance and compile controller","metadata":{}},{"cell_type":"code","source":"t_gan_c = TrainGANController(disc_nn, gen_nn, BATCH_SIZE, device=device)\nt_gan_c.compile()","metadata":{"_uuid":"3c1a908a-54a5-4e47-a17e-d29a28aec30e","_cell_guid":"468b2c10-26c9-421e-b0c6-d42a821bc411","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:13:37.655285Z","iopub.execute_input":"2021-08-15T15:13:37.655686Z","iopub.status.idle":"2021-08-15T15:13:37.661622Z","shell.execute_reply.started":"2021-08-15T15:13:37.655655Z","shell.execute_reply":"2021-08-15T15:13:37.660312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start training","metadata":{}},{"cell_type":"code","source":"t_gan_c.fit(train_loader, epoch=25)","metadata":{"_uuid":"b7470eef-7066-4770-ad49-ddb2798ba79e","_cell_guid":"f8f4165c-e5e2-44df-9e86-8dc2f34bf013","execution":{"iopub.status.busy":"2021-08-15T15:13:38.521258Z","iopub.execute_input":"2021-08-15T15:13:38.521625Z","iopub.status.idle":"2021-08-15T15:24:12.186213Z","shell.execute_reply.started":"2021-08-15T15:13:38.521592Z","shell.execute_reply":"2021-08-15T15:24:12.185138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate digits with trained model","metadata":{}},{"cell_type":"code","source":"def visualise_sheets_of_images(\n    images, prefix_name, unique_index=0,\n    show_images=False, subplot_size=(10, 10),\n    figsize=(20, 20),use_BGR2RGB=False, use_grey=False):\n    \"\"\"\n    Plot sheets of images. Usually used for generated images from GANs.\n    Parameters\n    ----------\n    images : list or np.ndarray\n        List of images that should be plotted.\n    prefix_name : str\n        Prefix name for file with sheets of images.\n    unique_index : int\n        Unique number for name of file which consist of sheets of images,\n        usually this params used for showing at which epoch this result is.\n    show_images : bool\n        If true, sheets of images will be plotted.\n    subplot_size : tuple\n        Size of raw and columns. For more detail, see plt docs.\n    figsize : tuple\n        Size of figure. For more detail, see plt docs.\n    use_BGR2RGB : bool\n        If true, `images` will be converted into RGB format (if they have BGR format).\n    use_grey : bool\n        If true, `images` will be plotted as black-white images.\n    \n    \"\"\"\n    plt.figure(figsize=figsize)\n    for z in range(min(len(images), subplot_size[0] * subplot_size[1])):\n        plt.subplot(*subplot_size, z + 1)\n        if use_BGR2RGB:\n            plt.imshow(cv2.cvtColor(images[z], cv2.COLOR_BGR2RGB))\n        elif use_grey:\n            plt.imshow(images[z], cmap='gray')\n        else:\n            plt.imshow(images[z])\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.savefig(f'{prefix_name}_{unique_index}.png')\n    if show_images:\n        plt.show()\n\n    plt.close('all')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:24:12.188132Z","iopub.execute_input":"2021-08-15T15:24:12.188585Z","iopub.status.idle":"2021-08-15T15:24:12.198658Z","shell.execute_reply.started":"2021-08-15T15:24:12.188514Z","shell.execute_reply":"2021-08-15T15:24:12.197293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check generator\narr = np.random.randn(BATCH_SIZE, NOISE_SHAPE).astype(np.float32)\ngen_nn.eval()\nres = gen_nn(torch.tensor(arr).to(device=device))\n# Unnormed images and plot big figure\nres = ((res + 1.0) / 2.0).cpu().detach().numpy().transpose(0, 2, 3, 1)\nvisualise_sheets_of_images(res, \"generated_digits\", show_images=True, use_grey=True)","metadata":{"_uuid":"c8d78da7-349b-4d9f-98bf-b4d98121080c","_cell_guid":"c54edb92-a8fa-466e-b8d1-48de3e59e2bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-15T15:24:12.201483Z","iopub.execute_input":"2021-08-15T15:24:12.202154Z","iopub.status.idle":"2021-08-15T15:24:16.574214Z","shell.execute_reply.started":"2021-08-15T15:24:12.202086Z","shell.execute_reply":"2021-08-15T15:24:16.573043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate accuracy with FID metric","metadata":{}},{"cell_type":"markdown","source":"## Import libraries and define constants","metadata":{}},{"cell_type":"code","source":"from scipy.linalg import sqrtm\nfrom sklearn.utils import shuffle\nimport cv2\nfrom tqdm import tqdm\nfrom torchvision.models import inception_v3\n\n# Number of images taken and generated\n# In order to estimate generator with FID metric\nN_IMAGES = 10_000","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:24:16.576381Z","iopub.execute_input":"2021-08-15T15:24:16.577092Z","iopub.status.idle":"2021-08-15T15:24:17.619539Z","shell.execute_reply.started":"2021-08-15T15:24:16.577034Z","shell.execute_reply":"2021-08-15T15:24:17.618252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define some useful methods","metadata":{}},{"cell_type":"markdown","source":"#### Scale list of images into certain shape","metadata":{}},{"cell_type":"code","source":"def scale_images(images, new_shape):\n    \"\"\"\n    Scale an array of images to a new size\n    \n    Parameters\n    ----------\n    images : list\n        List of images. Each image have shape - (C, H_old, W_old)\n        Where:\n            C - color dimension of the image;\n            H_old - height of the image;\n            W_old - width of the image.\n    new_shape : list or tuple\n        (H, W), Height and Width of the result image\n    \n    Return\n    ------\n    list\n        List of images with shape equal to `new_shape`\n    \n    \"\"\"\n    images_list = list()\n    for image in images:\n        # resize with nearest neighbor interpolation\n        new_image = np.transpose(image, (1, 2, 0)) # (C, H, W) --> (H, W, C)\n        new_image = cv2.resize(new_image, new_shape, interpolation = cv2.INTER_NEAREST)\n        if len(new_image.shape) == 2 or new_image.shape[-1] == 1:\n            new_image = cv2.cvtColor(new_image, cv2.COLOR_GRAY2BGR)\n        new_image = np.transpose(new_image, (2, 0, 1)) # (H, W, C) --> (C, H, W)\n        # store\n        images_list.append(new_image)\n    return np.asarray(images_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:24:17.621349Z","iopub.execute_input":"2021-08-15T15:24:17.621811Z","iopub.status.idle":"2021-08-15T15:24:17.631947Z","shell.execute_reply.started":"2021-08-15T15:24:17.621765Z","shell.execute_reply":"2021-08-15T15:24:17.630289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Collect predictions from InceptionV3\nCollect data using certain batch size in order to save memory","metadata":{}},{"cell_type":"code","source":"def calculate_fid_batched(model_inception, images1, images2, batch_size=128):\n    assert len(images1) == len(images2)\n    n_batches = len(images1) // batch_size\n    preds1 = []\n    preds2 = []\n    for i in tqdm(range(n_batches)):\n        batch_img1 = images1[i*batch_size: (i+1)*batch_size]\n        batch_img2 = images2[i*batch_size: (i+1)*batch_size]\n        # Resize images\n        resized_b_img1 = scale_images(batch_img1, (299, 299))\n        resized_b_img2 = scale_images(batch_img2, (299, 299))\n        # Normalize images\n        resized_b_img1 -= np.array([0.485, 0.456, 0.406]).reshape(1, -1, 1, 1)\n        resized_b_img1 /= np.array([0.229, 0.224, 0.225]).reshape(1, -1, 1, 1)\n        # Run though inception v3 and take prediction\n        act1 = model_inception(torch.tensor(resized_b_img1, device=device)).squeeze().cpu().detach().numpy()\n        act2 = model_inception(torch.tensor(resized_b_img2, device=device)).squeeze().cpu().detach().numpy()\n        preds1.append(act1)\n        preds2.append(act2)\n    act1 = np.concatenate(preds1, axis=0)\n    act2 = np.concatenate(preds2, axis=0)\n    \n    return act1, act2","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:24:17.633826Z","iopub.execute_input":"2021-08-15T15:24:17.63464Z","iopub.status.idle":"2021-08-15T15:24:17.64887Z","shell.execute_reply.started":"2021-08-15T15:24:17.634593Z","shell.execute_reply":"2021-08-15T15:24:17.647289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculate FID with predictions from InceptioV3","metadata":{}},{"cell_type":"code","source":"def calculate_fid(act1, act2):\n    # calculate mean and covariance statistics\n    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n    # calculate sum squared difference between means\n    ssdiff = np.sum((mu1 - mu2)**2.0)\n    # calculate sqrt of product between cov\n    covmean = sqrtm(sigma1.dot(sigma2))\n    # check and correct imaginary numbers from sqrt\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    # calculate score\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:24:17.650694Z","iopub.execute_input":"2021-08-15T15:24:17.651205Z","iopub.status.idle":"2021-08-15T15:24:17.665202Z","shell.execute_reply.started":"2021-08-15T15:24:17.651158Z","shell.execute_reply":"2021-08-15T15:24:17.663953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load InceptionV3 and calculate FID","metadata":{}},{"cell_type":"code","source":"# prepare the inception v3 model\nmodel = inception_v3(pretrained=True)\nmodel.eval()\n# Remove certain layers from output\nlayer_names = []\nfor layer in list(model.children()):\n    if layer.__class__.__name__ not in ['InceptionAux', 'Linear', 'Dropout']:\n        layer_names.append(layer)\n# Create model without some layers\nmodel = nn.Sequential(*layer_names)\nmodel.eval()\nmodel.to(device=device)\n# Define two batches of images\n# First - real data\nimages1 = shuffle(data)[:N_IMAGES]\nimages1 = np.transpose(np.asarray(images1), (0, 3, 1, 2)).astype(np.float32)\n# Images1 in range [0, 255], normalize into [0, 1]\nimages1 /= 255.0\nimages1 = torch.tensor(images1, device=device).cpu().detach().numpy()\n\nimages2_noise = torch.tensor(\n    np.random.randn(N_IMAGES, NOISE_SHAPE).astype(np.float32),\n    device=device\n)\ngen_nn.eval()\nimages2 = gen_nn(images2_noise).cpu().detach().numpy()\n# Generator generate images in range (-1, 1), normalize into [0, 1] range\nimages2 += 1.0\nimages2 /= 2.0\nprint('Prepared', images1.shape, images2.shape)\n# Calculate FID with batch size\n# fid between images1 and images1\nact1, act2 = calculate_fid_batched(model, images1, images2)\nfid_same = calculate_fid(act1, act1)\nfid = calculate_fid(act1, act2)\n\n\nprint('FID (same): %.3f' % fid_same)\n# fid between images1 and images2\nprint('FID (different): %.3f' % fid)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:24:17.668749Z","iopub.execute_input":"2021-08-15T15:24:17.669305Z","iopub.status.idle":"2021-08-15T15:26:17.376591Z","shell.execute_reply.started":"2021-08-15T15:24:17.669258Z","shell.execute_reply":"2021-08-15T15:26:17.375049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"torch.save(gen_nn.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:26:17.378729Z","iopub.execute_input":"2021-08-15T15:26:17.379265Z","iopub.status.idle":"2021-08-15T15:26:17.446474Z","shell.execute_reply.started":"2021-08-15T15:26:17.379217Z","shell.execute_reply":"2021-08-15T15:26:17.445146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to download final model - click link below.","metadata":{}},{"cell_type":"markdown","source":"<h1><a href=\"model.pth\"> Download trained generator </a></h1>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}