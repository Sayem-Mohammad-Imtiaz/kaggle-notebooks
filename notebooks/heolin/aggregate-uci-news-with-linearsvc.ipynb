{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py"}},"cells":[{"source":"from IPython.core.display import display\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","outputs":[],"execution_count":null,"metadata":{"_uuid":"a3c9d2c135dd47015774cb3892808e603de0614a","_cell_guid":"00bd9f7c-26b4-4b2b-b55c-a3ec83383dce","collapsed":true},"cell_type":"code"},{"source":"# 1. Read data","metadata":{"_uuid":"733a758d7d17159aee8a70162ba28efaadd3b80c","_cell_guid":"17d16915-1f4b-45b8-be6f-7b1ea820fa4e"},"cell_type":"markdown"},{"source":" Reading the data and selecting right columns.","metadata":{"_uuid":"40d2c0ad603eadce36372cdd5fd23f4344502aca","_cell_guid":"a845d2ae-ec64-4df4-a54e-af9ded0ece46"},"cell_type":"markdown"},{"source":"data = pd.read_csv(\"../input/uci-news-aggregator.csv\")\ndata.head()","outputs":[],"execution_count":null,"metadata":{"_uuid":"1ce4d17033d3f979e9d53c408a4ea514d71be3e3","_cell_guid":"131e25cd-c48c-4708-9777-01b6a743e634","collapsed":true},"cell_type":"code"},{"source":"X_raw, y = data['TITLE'], data['CATEGORY']","outputs":[],"execution_count":null,"metadata":{"_uuid":"76de24b5a2199d03e34af6ec69f020dd8956bab7","_cell_guid":"da74a4a4-b7c1-4b18-a70f-117ac2140e89","collapsed":true},"cell_type":"code"},{"source":"value_counts = dict(y.value_counts())\ntargets_labels = value_counts.keys()\nind = range(len(targets_labels))\nplt.bar(ind, value_counts.values())\nplt.title(\"Categories count\")\nplt.xticks(ind, targets_labels)\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_uuid":"3a052d93e04b5063ba54a38735a8a5f01c3af95c","_cell_guid":"55690878-9f0d-40a7-af56-036ad70e2c79","collapsed":true},"cell_type":"code"},{"source":"# 2. Prepare data","metadata":{"_uuid":"a36deda1a0c9bcd2e6177f7c7c8c82c6ada5fe26","_cell_guid":"7a66da65-c5b3-491e-965a-a8177910b75f"},"cell_type":"markdown"},{"source":"## 2.1. Vectorizing the data","metadata":{"_uuid":"c7d31680ba2e0cba65867c664f3de21355fc7fcb","_cell_guid":"87ad2483-13a3-4e99-b270-e62533cc9d22"},"cell_type":"markdown"},{"source":"We use a vector representation of documents (called a \"bag of words\" model). The document is represented in the form of a vector, whose dimension is equal to the number of all available words. Each word corresponds to one dimension, if the word is present in the document, the vector has a corresponding value on that dimension.\nIn the basic implementation, this is the number of occurrences of a given word in a document (CountVectorizer in sklearn).\nBelow I am using an enhanced version of this model (TfidfVectorizer) which, thanks to the TF-IDF factor, also takes into account the relevance of the word. This factor is based on the assumption that the words appearing in many different documents are less important than those that appear in fewer numbers.","metadata":{"_uuid":"bb7d3cdf106e575d908c285b7764aa4d7ad5f789","_cell_guid":"b82f5981-57ac-4afb-be62-ba26bfaba5c9"},"cell_type":"markdown"},{"source":"vectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(X_raw)","outputs":[],"execution_count":null,"metadata":{"_uuid":"21dfe4b74b2b9b09434c0a324883fc584c32ed07","_cell_guid":"cafff791-c35b-46e4-b830-1064aca86d1c","collapsed":true},"cell_type":"code"},{"source":"## 2.1. Spliting data into training and test set","metadata":{"_uuid":"d1defe08361fe92ad2c24e99ce65ed257e703ce5","_cell_guid":"6557c358-36f3-4d7d-bfd0-dd7a0308557b"},"cell_type":"markdown"},{"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","outputs":[],"execution_count":null,"metadata":{"_uuid":"1a10e194bf12bbbb8d2b97a1fb5e8ff551d5d66e","_cell_guid":"bf348124-2719-4d04-a0ee-a70fa01831ec","collapsed":true},"cell_type":"code"},{"source":"# 3. Training model","metadata":{"_uuid":"431ef141d957404d6203bc1070fc158fc07353f4","_cell_guid":"f81dce23-5f72-40b3-ba9e-24db881fd5e4","collapsed":true},"cell_type":"markdown"},{"source":"I'm using LinearSVC with is a Support Vector Classification model with linear kernel. I've tried using other kernels, but it takes to long to train, while this still gives a sufficient results. Suppor Vector Machines algorithms works well with high dimension data (like text documents).\n\nWe could have use any other popular, classification algorithm like LogisticRegression, RandomForest, GaussianNaiveBayer or MultipleLayerPerceptron. Or even a deep learning alogrithm, e.g. CNN with word embedding layer tends to get really good results for text classification.","metadata":{"_uuid":"aae1eec723503998d54e8815d695df5515d3aef2","_cell_guid":"c0c780cc-06cb-47cc-9f38-5a7fab53c10b"},"cell_type":"markdown"},{"source":"clf = LinearSVC()\nclf.fit(X_train, y_train)","outputs":[],"execution_count":null,"metadata":{"_uuid":"968ab35a82609aabf0af39c69a12ce937737c9af","_cell_guid":"84c214a0-6e56-4a24-8721-57e63d3cfca3","collapsed":true},"cell_type":"code"},{"source":"# 4. Results","metadata":{"_uuid":"ff641dbfd241dd5806609c710c5621b2118d3e52","_cell_guid":"4b9f2999-921e-4ff9-8ecb-49012d58cf28","collapsed":true},"cell_type":"markdown"},{"source":"## 4.1. Classification metrics","metadata":{"_uuid":"31ae59e170d60e82f7f2067c640893edbdafd948","_cell_guid":"f43e252a-5108-442e-8c78-07e5daea6485","collapsed":true},"cell_type":"markdown"},{"source":"y_pred = clf.predict(X_test)","outputs":[],"execution_count":null,"metadata":{"_uuid":"7718db72b66ae7907c8e06957c6f5c6010805ef0","_cell_guid":"e9ecc3b7-80aa-41f7-8a26-b2cae0dfc10f","collapsed":true},"cell_type":"code"},{"source":"print (classification_report(y_test, y_pred))","outputs":[],"execution_count":null,"metadata":{"_uuid":"7fdbd3ce51e22675b4067d89cfa39aea4321b72e","_cell_guid":"61b26f2f-77f5-47be-8f00-c9ddceb9745b","collapsed":true},"cell_type":"code"},{"source":"## 4.2. Confusion matrix","metadata":{"_uuid":"9bafd8c531aaf7db7ba32fcd37f61302b4a6c458","_cell_guid":"ad52b59e-a4b8-41ad-8472-d0710db51f0f"},"cell_type":"markdown"},{"source":"def plot_confusion_matrix(y_true, y_pred, targets_labels=None):\n    targets_labels = list(targets_labels)\n    if not targets_labels:\n        targets_labels = list(set(y_true))\n    num_classes = len(targets_labels)\n    cdata = confusion_matrix(y_true, y_pred, labels=targets_labels)\n    cdata = cdata / cdata.sum(axis=1).astype(float)\n    heatmap = plt.pcolor(cdata, cmap=\"PuBu\")\n    plt.title(\"Confusion matrix\")\n    plt.colorbar(heatmap)\n    for y in range(cdata.shape[0]):\n        for x in range(cdata.shape[1]):\n            plt.text(x + 0.5, y + 0.5, '{0:.2f}%'.format((cdata[y, x] * 100)),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     )\n\n    tick_marks = np.arange(num_classes) + 0.5\n    plt.xticks(tick_marks, targets_labels)\n    plt.yticks(tick_marks, targets_labels)\n    plt.show()\n    \nplot_confusion_matrix(y_test, y_pred, targets_labels)","outputs":[],"execution_count":null,"metadata":{"_uuid":"4897e7924d233c9b64bdc3d2fb5304ee2279c2d0","_cell_guid":"5e441466-b062-454b-ac15-ef7006373dc8","collapsed":true},"cell_type":"code"},{"source":"","metadata":{"_uuid":"303699882928520f1bb36471e26260c4b2539053","_cell_guid":"828a30c1-9810-4933-90b3-97f84592a101"},"cell_type":"markdown"}],"nbformat_minor":1,"nbformat":4}