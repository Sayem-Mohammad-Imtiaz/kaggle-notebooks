{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c68ce83b-4530-5c9a-729e-85f2a21c013f"},"source":"In this Notebook, I tried to predict the Imdb_score values based on the numeric data.\nRegression model is tried on multiple regression models with and without feature_selection."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"243d38d4-3598-0842-3cf1-ee83b10bc19d"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import Imputer,StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nmovies = pd.read_csv(\"../input/movie_metadata.csv\")\nprint (movies.shape)\nprint (movies.columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b30ea59-eb16-5cb0-1f81-3c2646979ed7"},"outputs":[],"source":"\n#drop columns which does not seem to have any effect on movie rating\n\n#get numeric data for computation and correlation purposes\nnumerical_data = movies.select_dtypes(exclude=[\"object\"])\n\n# take out the y value(imdb_score from data)\nscore_imdb= numerical_data[\"imdb_score\"]\nnumerical_data = numerical_data.drop([\"imdb_score\"],axis=1)\nyear_category = numerical_data[\"title_year\"]\nnumerical_data = numerical_data.drop([\"title_year\"],axis=1)\nnumerical_columns = numerical_data.columns\n# print (numerical_columns.shape)\n#Plot distribution of actual imdb scores\nsns.distplot(score_imdb,rug=True,label=\"IMDB Scores\").legend()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36a411e8-7bcd-3cda-9476-a41a343224c8"},"outputs":[],"source":"#fill missing values and normalize the data\nimp = Imputer(missing_values=\"NaN\",strategy=\"mean\",axis=0)      #default values\nnumerical_data[numerical_columns] = imp.fit_transform(numerical_data[numerical_columns])\n# print (numerical_data.describe())\n\n#Without StandardScaler, SVR with poly kernel will throw error of large size.\n#With standard scaling, models has seen improvement in predicting.\n#knn model is the most beneficiary of standard scaling\nscaler = StandardScaler()\nnumerical_data[numerical_columns] = scaler.fit_transform(numerical_data[numerical_columns])\n\n# print (numerical_data.describe())\n# print (numerical_data.shape)\n# numerical_data = pd.DataFrame(numerical_data)\n# print (numerical_data.describe())\n\n#get non_numeric informational content\ninformation_data = movies.select_dtypes(include=[\"object\"])\nprint (information_data.columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cec7036-0e17-738f-2e4b-2d6384d03273"},"outputs":[],"source":"\n#numpy corrcoef returns symmetric metrics of correlation coef\n#Use -from scipy.stats.stats import pearsonr   print pearsonr(a,b)\n#check attributes for correlation with movie rating\nlow_covariance_1 = []\nlow_covariance_2 = []\nlow_covariance_15 = []\nlow_covariance_2g = []\nfor x in numerical_columns:\n    z = (np.corrcoef(numerical_data[x],y=score_imdb))\n    if(np.fabs(z[0,1]) < 0.1):\n        low_covariance_1.append(x)\n    elif(np.fabs(z[0,1]) < 0.15):\n        low_covariance_15.append(x)\n    elif(np.fabs(z[0,1])<0.2):\n        low_covariance_2.append(x)\n    else:\n        low_covariance_2g.append(x)\n\nprint (low_covariance_2g, \"\\n\", low_covariance_2, \"\\n\", low_covariance_15, \"\\n\", low_covariance_1)\n#attributes with correlation coef >=0.2 . Thyese attributes will help more than other attributes in regression\n#['num_critic_for_reviews', 'duration', 'num_voted_users', 'num_user_for_reviews', 'movie_facebook_likes']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffeb65f0-10e5-13d2-26aa-bff7910439ec"},"outputs":[],"source":"from sklearn.feature_selection import SelectKBest,SelectPercentile,RFE,RFECV,SelectFromModel\nfrom sklearn.svm import SVR,SVC\nfrom sklearn.linear_model import Lasso\n#data which has high correlation with imdb_score is selected\nselect_k = SelectKBest(k=8)\nx_transformed = select_k.fit_transform(numerical_data,y=score_imdb) #x_transformed is numpy array not pandas\n#sklearn returns numpy array not pandas object\nprint (x_transformed.shape)\n# print (x_transformed.columns)\n#print (x_transformed[0,:])\n# print (numerical_data.head(1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b12d4a9-91c1-a7bc-6545-cac0c35a3054"},"outputs":[],"source":"from sklearn.svm import LinearSVR\n\n#print(\"before model selection\")\n#The underlying estimator SVR has no `coef_` or `feature_importances_` attribute.\n#  Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\nestimator = LinearSVR().fit(numerical_data,score_imdb)\nselect_model = SelectFromModel(estimator,prefit=True)\n#Uncomment this if you want to use selectFromModel with LinearSVR as feature selection algorithm\nx_transformed = select_model.transform(numerical_data)\nprint (x_transformed.shape)\n#print(\"after model selection\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a444d940-5046-dc13-b910-d14bb9fe6965"},"outputs":[],"source":"\n#RFE use recursive selecting of attributes which is a time counsuming process.\nestimator = LinearSVR()\nselector = RFE(estimator)\nselector = selector.fit(numerical_data,score_imdb)\nprint (selector.support_)\nprint (selector.ranking_)\n#Comment/Uncomment this if you want to use/not use RFE as feature selection algorithm\n#x_transformed = selector.transform(numerical_data)\nprint (x_transformed.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbbe1bbf-56d5-12a8-2610-29ff691f8d64"},"outputs":[],"source":"#some global variables to compare and select best of all methods\nbest_on_training_data = {\"training_score\":-10000,\"test_score\":0,\"model\":\"\"}\nbest_on_test_data = {\"training_score\":-10000,\"test_score\":0,\"model\":\"\"}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79e4477b-826c-1f8f-ffc9-a3cdedbc20d7"},"outputs":[],"source":"#generic utility methods\n#method to get score as mean deviation\ndef svm_score(test_y, predict_y):\n    # convert to numpy array to compare both predict and actual array\n    # Iris_test_y contain indexes from dataframe(parent)\n    iris_test_y = np.array(test_y)\n    diff = 0\n    total_size = test_y.shape[0]\n    # print (total_size,test_y.iloc[0],predict_y[0])\n    for idx in range(total_size):\n        diff += np.fabs(test_y.iloc[idx]-predict_y[idx])\n    return diff/total_size\ndef split_data(x_data,y_data,size=0.1):\n    return train_test_split(x_data,y_data,test_size=0.1)\n\ndef fit_model(model_to_print,model,training_x,test_x,training_Y,test_y,clr):\n    model.fit(X=training_x,y=training_Y)\n    predicted_y = model.predict(test_x)\n    training_score = model.score(training_x,training_Y)\n    test_score = model.score(test_x,test_y)\n    if(training_score > best_on_training_data[\"training_score\"]):\n        best_on_training_data[\"training_score\"] = training_score\n        best_on_training_data[\"test_score\"] = test_score\n        best_on_training_data[\"model\"] = model_to_print\n    if(training_score > best_on_test_data[\"test_score\"]):\n        best_on_test_data[\"training_score\"] = training_score\n        best_on_test_data[\"test_score\"] = test_score\n        best_on_test_data[\"model\"] = model_to_print\n    sns.distplot(predicted_y,hist=True,rug=True,color=clr,label=model_to_print).legend()\n    print  (model_to_print,\"Score on training data: \",training_score)\n    print  (model_to_print,\"Score on test data: \",test_score, \"\\n\")\n    # print (model_to_print,\"training\",svm_score(training_Y,model.predict(training_x)))\n    # print (model_to_print,svm_score(test_y,predicted_y))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5968d538-6ff4-2db6-0d30-8208b4974583"},"outputs":[],"source":"\n#Fit a regression model on numeric data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVR\n#On complete data without feature extraction\nsvr_model = SVR(kernel='rbf') #default\n#below kernel is taking infinite time. Hence usign LinearSVR explicitly\n#svr_linear_model = SVR(kernel=\"linear\") #infinite time(hanging)\nsvr_linear_model = LinearSVR()\nsvr_poly_model = SVR(kernel=\"poly\") #default degree is 3\n\n#Split data beofrehand so that they can be compared on same data\ntraining_x,test_x,training_Y,test_y = split_data(numerical_data,score_imdb)\n\n# plt.plot(score_imdb,label=\"original data\")\nfit_model(\"SVR rbf: \",svr_model,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"SVR linear: \",svr_linear_model,training_x,test_x,training_Y,test_y,\"yellow\")\n#ValueError: Input contains NaN, infinity or a value too large for dtype('float64'). \n#Without Standard Scaler poly kernel will throw above error.\nfit_model(\"SVR poly: \",svr_poly_model,training_x,test_x,training_Y,test_y,\"red\")\nsns.plt.show()\n\n#Calling fit on any scikit-learn estimator will forget all the previously seen data\n#So we can use same models for transformed data also\n#same model on transformed data with data selection\n\ntraining_x,test_x,training_Y,test_y = split_data(x_transformed,score_imdb)\n\nfit_model(\"transformed , svr rbf: \",svr_model,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"transformed , svr linear: \",svr_linear_model,training_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"transformed , svr poly: \",svr_poly_model,training_x,test_x,training_Y,test_y,\"red\")\nsns.plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a5ed65c-8027-41cc-173e-37c719c450cc"},"outputs":[],"source":"\n#using knn regression\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndefault_knn = KNeighborsRegressor(n_neighbors=5)\nknn_10 = KNeighborsRegressor(n_neighbors=10)\nknn_20 = KNeighborsRegressor(n_neighbors=20)\n\n#Split data beofrehand so that they can be compared on same data\ntraining_x,test_x,training_Y,test_y = split_data(numerical_data,score_imdb)\n\nfit_model(\"knn with k=5: \",default_knn,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"knn with k=10: \",knn_10,training_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"knn with k=20: \",knn_20,training_x,test_x,training_Y,test_y,\"red\")\nsns.plt.show()\n#Calling fit on any scikit-learn estimator will forget all the previously seen data\n#So we can use same models for transformed data also\n#same model on transformed data with data selection\n\ntraining_x,test_x,training_Y,test_y = split_data(x_transformed,score_imdb)\n\nfit_model(\"transformed , knn with k=5: \",default_knntraining_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"transformed , knn with k=10: \",knn_10,training_x,test_x,training_Y,test_y,\"red\")\nfit_model(\"transformed , knn with k=20: \",knn_20,training_x,test_x,training_Y,test_y,\"green\")\nsns.plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9b16b80-e0da-7408-5eb2-4fb1a73595d6"},"outputs":[],"source":"\n\n#Other regression models\n\nfrom sklearn.linear_model import LinearRegression,Ridge\n\nlinear_reg = LinearRegression()\n\n#Split data beofrehand so that they can be compared on same data\ntraining_x,test_x,training_Y,test_y = split_data(numerical_data,score_imdb)\nfit_model(\"linear regression: \",linear_reg,training_x,test_x,training_Y,test_y,\"green\")\n\n#Calling fit on any scikit-learn estimator will forget all the previously seen data\n#So we can use same models for transformed data also\ntraining_x,test_x,training_Y,test_y = split_data(x_transformed,score_imdb)\n\nfit_model(\"linear regression transformed: \",linear_reg,training_x,test_x,training_Y,test_y,\"yellow\")\nsns.plt.show()\n\n#Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients.\n#alpha is the rete of penalty\nridge_1 = Ridge(alpha=1.0)\nridget_point_5 = Ridge(alpha=0.5)\nridget_point_25 = Ridge(alpha=0.25)\n\n#Split data beofrehand so that they can be compared on same data\ntraining_x,test_x,training_Y,test_y = split_data(numerical_data,score_imdb)\n\nfit_model(\"Ridge alpha =1:\",ridge_1,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"Ridge alpha =0.5 :\",ridget_point_5,training_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"Ridge alpha =0.25:\",ridget_point_25,training_x,test_x,training_Y,test_y,\"red\")\nsns.plt.show()\n#Calling fit on any scikit-learn estimator will forget all the previously seen data\n#So we can use same models for transformed data also\ntraining_x,test_x,training_Y,test_y = split_data(x_transformed,score_imdb)\nfit_model(\"Ridge transformed alpha =1:\",ridge_1,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"Ridge transformed alpha =0.5 :\",ridget_point_5,training_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"Ridge transformed alpha =0.25:\",ridget_point_25,training_x,test_x,training_Y,test_y,\"red\")\nsns.plt.show()\n#By plotting the distribution against predicted values. You can see that values are in the middle range(5,7) and have a peak at 6.\n#While the original distribution is more randomly distributed.\n#Values of score returns the mean deviation from actual score."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af87e3c7-e8c2-09ff-957b-9876e438cf45"},"outputs":[],"source":"#Let's check best models on trainging data\nprint (\"best model on trainging score is :\")\nprint (\"training_score: \", best_on_training_data[\"training_score\"])\nprint (\"test_score:\", best_on_training_data[\"test_score\"])\nprint (\"model: \", best_on_training_data[\"model\"])\n\n\nprint (\"\\n\\n\\nbest model on test score is :\")\nprint (\"training_score: \", best_on_test_data[\"training_score\"])\nprint (\"test_score:\", best_on_test_data[\"test_score\"])\nprint (\"model: \", best_on_test_data[\"model\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eceb6db0-ae48-af45-be86-d3da61e36cbf"},"outputs":[],"source":"#Lets comprare the distribution of above types.\n\n#Calling fit on any scikit-learn estimator will forget all the previously seen data\n#So we can use same models for transformed data also\n\n#Split data beofrehand so that they can be compared on same data\ntraining_x,test_x,training_Y,test_y = split_data(numerical_data,score_imdb)\n\nfit_model(\"SVR rbf: \",svr_model,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"knn with k=5: \",default_knn,training_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"linear regression: \",linear_reg,training_x,test_x,training_Y,test_y,\"red\")\nfit_model(\"Ridge alpha =1:\",ridge_1,training_x,test_x,training_Y,test_y,\"blue\")\nsns.plt.show()\n\ntraining_x,test_x,training_Y,test_y = split_data(x_transformed,score_imdb)\n\nfit_model(\"transformed , svr rbf: \",svr_model,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"transformed , knn with k=5: \",default_knn,training_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"linear regression transformed: \",linear_reg,training_x,test_x,training_Y,test_y,\"red\")\nfit_model(\"Ridge transformed alpha =1:\",ridge_1,training_x,test_x,training_Y,test_y,\"blue\")\nsns.plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e1884b8-0275-a0dd-6386-96ef9b87b271"},"outputs":[],"source":"#Lets comprare the distribution same type on transformed and untransformed data\n\n#Calling fit on any scikit-learn estimator will forget all the previously seen data\n#So we can use same models for transformed data also\n\n#Split data beofrehand so that they can be compared on same data\ntraining_x,test_x,training_Y,test_y = split_data(numerical_data,score_imdb)\ntraining_x_t,test_x_t,training_Y_t,test_y_t = split_data(x_transformed,score_imdb)\n\nfit_model(\"SVR rbf: \",svr_model,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"transformed , svr rbf: \",svr_model,training_x_t,test_x_t,training_Y_t,test_y_t,\"yellow\")\nsns.plt.show()\nfit_model(\"knn with k=5: \",default_knn,training_x,test_x,training_Y,test_y,\"yellow\")\nfit_model(\"transformed , knn with k=5: \",default_knn,training_x_t,test_x_t,training_Y_t,test_y_t,\"blue\")\nsns.plt.show()\nfit_model(\"linear regression: \",linear_reg,training_x,test_x,training_Y,test_y,\"red\")\nfit_model(\"linear regression transformed: \",linear_reg,training_x_t,test_x_t,training_Y_t,test_y_t,\"blue\")\nsns.plt.show()\nfit_model(\"Ridge alpha =1:\",ridge_1,training_x,test_x,training_Y,test_y,\"green\")\nfit_model(\"Ridge transformed alpha =1:\",ridge_1,training_x_t,test_x_t,training_Y_t,test_y_t,\"blue\")\nsns.plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"210bfdd9-9775-9ece-806a-9b02eccf6325"},"source":"**Conclusion** :-\n\n - KNN scores better than SVM, linear regression and ridge.  Linear Regression and Ridge scores least among all. \n - Standard scaling of data increases the score on average. Most of time, increasing the value o K in KNN increases the score. \n - Still, Accuracy of system is very low. Mean deviation of test data from original data is in the range [0.5,0.9].\n - IMDB_Score predicted by the model are concentrated in the mid region of score. Most scores are in the range [5,7] with peak around 6.\n - Most models doesn't predict any value less than 5 and more than 7. \n - Features selected using SelectFromModel and RFE feature selection algorithms performs better than feature selected from SelectKBest."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}