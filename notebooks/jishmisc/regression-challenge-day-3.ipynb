{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"Welcome to the third data of the Regression 5-Day Challenge! Over the last two days of the challenge, we've:\n\n* [Learned about different types of regression (Poisson, linear and logistic) and when to use them](https://www.kaggle.com/rtatman/regression-challenge-day-1)\n* [Learned how to fit & evaluate a model with diagnostic plots](https://www.kaggle.com/rtatman/regression-challenge-day-2)\n\nToday, we're going to learn about how to interpret a model and how to tell whether our input variable actually does have a strong relationship to our output variable. To figure this out, we'll need to learn about a new concept: coefficients.\n\n## What's a coefficient?\n\nA coefficient expresses that strength of the relationship between the input value and the output value. You can read a coefficient as \"for every increase of the input value by one unit, the output value will change by [whatever number the coefficient is] units\". In linear regression, you can think of the coefficient as the slope of the line. Today we're going to fit a model and learn how to see and interpret its coefficients.\n___\n\n<center>\n[**You can check out a video that goes with this notebook by clicking here.**](https://www.youtube.com/embed/4OnNnu6GqCs)","metadata":{"_cell_guid":"9ea99ad7-2317-450b-a8fe-8f18bb7737ff","_uuid":"baf68c0b5d7bf3d9d65885dfdf41b4d31f72e6e1"}},{"cell_type":"markdown","source":"## Example: Can we predict how likely a hard drive is to fail?\n___\n\nLet's see if we can predict the probability that a hard drive will fail based on the Read Error Rate (which is in the column smart_1_normalized in this dataset). Intuitively, I'd expect that a hard drive is more likely to fail if the Read Error Rate is higher, i.e. there are more read errors. \n\nFirst we need to set up our environment, though.","metadata":{"_cell_guid":"1f5ed66f-3916-4721-b4cc-4babadc53086","_uuid":"6b70266553f191cb9643390bd25d81b9bb3708f4"}},{"cell_type":"code","source":"# read in libraries\nlibrary(tidyverse)\nlibrary(boot)\n\n# data. I'll use the hard_drives dataset & the cameras dataset is for you \n# to use as part of your exerise\nhard_drives <- read_csv(\"../input/hard-drive-test-data/harddrive.csv\", n_max = 100000)\ncameras <- read_csv(\"../input/1000-cameras-dataset/camera_dataset.csv\")","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"673245d4-89c5-4ff0-b55a-592a8fb02841","collapsed":true,"_uuid":"37dd079652c5784e2ed69715b8e795cd8277a2bd","_kg_hide-output":true}},{"cell_type":"code","source":"#Python Code\n\nimport pandas as pd\n\nhard_drives = pd.read_csv('../input/hard-drive-test-data/harddrive.csv', nrows= 90000)\ncameras = pd.read_csv(\"../input/1000-cameras-dataset/camera_dataset.csv\")\nhard_drives.head()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"Because \"failure\" is a categorical variable with two values, I'm going to use the \"binomial\" family to fit a logistic regression model. In this dataset 1 indicates that a hard drive did fail and 0 indicates that it didn't.","metadata":{"_cell_guid":"86b83dec-72bf-452d-85cd-ec88c66c02db","_uuid":"69a2f2e36ac01d6247fc4582897a0c021729c094"}},{"cell_type":"code","source":"# predict probability of failure give the read error rate\nmodel <- glm(failure ~ smart_1_normalized, data = hard_drives, family = \"binomial\")","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3d4945cf-0652-404e-b9c3-a10a6c72abf3","collapsed":true,"_uuid":"94a4127956125274e8e0e23465907a648075062f","scrolled":true}},{"cell_type":"code","source":"#Python Code\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nX = np.array(hard_drives['smart_1_normalized']).reshape(len(hard_drives),1)\ny = hard_drives['failure']\nlogit = LogisticRegression()\nlogit.fit(X,y)\nlogit.coef_\nlogit.intercept_\n\nplt.scatter(X,y)\nplt.xlabel('smart_1_normalized')\nplt.ylabel('failure')","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"# Python Code\nimport seaborn as sns\nsns.lmplot('smart_1_normalized','failure',data=hard_drives,fit_reg=True,hue='failure',scatter_kws={\"marker\": \"D\",\"s\": 100})","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"Great, so now we have a model! But how can we tell what's going on? We can do this using the \"summary\" function.\n\n> **Note:** I'd recommend doing diagnostic plots at this point, but this is a pretty big model and they took fooooorrrrever to plot, so I'm skipping them here. Don't be lazy like I'm being! :P","metadata":{"_cell_guid":"b6e1afe4-6339-4c77-a1e9-2f2785937df8","_uuid":"b39b87f65f9a40431980665945cce298986b7a26"}},{"cell_type":"code","source":"# summary of the model\nsummary(model)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"455bea88-a519-4a8d-a025-78c26daeb6fe","collapsed":true,"_uuid":"eb6ecb4ff6258694b9973e85762cad8271ba656e"}},{"cell_type":"markdown","source":"Wow, that's a lot of output! Let's take it a bit at a time and go through it together. \n\n**Call**: This is just the call that you made to the function. It will be the exact same code you typed into R. This can be helpful for seeing if you made any typos.\n\n**(Deviance) Residuals:** You can pretty much ignore these for logistic regression. For Poisson or logistic regression, you want these to be more-or-less normally distributed (which is the same thing the top two diagnostic plots are checking). You can check this by seeing if the absolute value of 1Q and 3Q are close(ish) to each other, and if the median is close to 0. The mean is not shown because it's always 0. If any of these are super off then you probably have some weird skew in your data. (This will also show up in your diagnostic plots!)\n\n**Coefficients:** This is the meat of the output.\n\n* **Intercept**: For Poisson and linear regression, this is the predicted output when all our inputs are 0. For logistic regression, this value will be further away from 0 the bigger the difference between the number of observation in each class.. The the standard error represents how uncertain we are about this (lower is better). In this case, because our intercept is far from 0 and our standard error is much smaller than the intercept, we can be pretty sure that one of our classes (failed or didn't fail) has a lot of more observations in it. (In this case it's \"didn't fail\", thankfully!)\n* **Various inputs** (each input will be on a different line): This estimate represents how much we think the output will change each time we increase this input by 1. The bigger the estimate, the bigger the effect of this input variable on the output. The standard error is how certain about it we are. Usually, we can be pretty sure an input is informative is the standard error is 1/10 of the estimate.  So in this case we're pretty sure the intercept is important.\n* **Signif. Codes**: This is a key to the significance of each :input and the intercept. These are only correct if you only ever fit one model to your data. (In other words, they’re great for experimental data if you from the start which variables you’re interested in and not as informative for data analysis or variable selection.)\n\n> **Wait, why can't we use statistical significance?** You can, I just wouldn't generally recommend it. In data science you'll often be fitting multiple models using the same dataset to try and pick the best model. If you ever run more than one test for statistical significance on the same dataset, you need to adust your p-value to make up for it. You can think about it this way: if you decide that you'll accept results that are below p = 0.05, you're basically saying that you're ok with being wrong one in twenty times. If you then do five tests, however, and for each one there's a 1/20 chance that you'll be wrong, you now have a 1/4 chance of having been wrong on at least one of those tests... but you don't know which one. You can correct for it ([by multiplying the p-value you'll accept as significant by the number of tests you'll preform](http://mathworld.wolfram.com/BonferroniCorrection.html)) but in practice I find it's generally easier to avoid using p-values altogether. \n\n**(Dispersion parameter for binomial family taken to be 1):** You'll only see this for Poisson and binomial (logistic) regression. It's just letting you know that there has been an additional scaling parameter added to help fit the model. You can ignore it. :)\n\n**Null deviance:** The null deviance tells us how well we can predict our output *only* using the intercept. Smaller is better.\n\n**Residual deviance:** The residual deviance tells us how well we can predict our output using the intercept and our inputs. Smaller is better. The bigger the difference between the null deviance and residual deviance is, the more helpful our input variables were for predicting the output variable.\n\n**AIC:** The AIC is the \"Akaike information criterion\" and it's an estimate of how well your model is describing the patterns in your data. It's mainly used for comparing models trained on the same dataset. If you need to pick between models, the model with the *lower* AIC is doing a better job describing the variance in the data.\n\n**Number of Fisher Scoring iterations:** This is just a measure of how long it took to fit you model. You can safely ignore it. \n\nOK, that was a lot! Let's get the summary of our model again and quickly go over what this tells us.","metadata":{"_cell_guid":"ba4ec79a-8d22-4c22-abe1-755c104cfc8a","_uuid":"0282c4f8f71b3f4c7fb7ba555e1554eb52cc4532"}},{"cell_type":"code","source":"# summary of the model\nsummary(model)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"56385b7b-7fe4-44f7-b67e-272040b672aa","collapsed":true,"_uuid":"9295597ce0e118d1f83c15774465688a99605d0d"}},{"cell_type":"markdown","source":"From looking at the Deviance Residuals, we can tell that there's a really big skew in our residuals. Since the mean of the residuals is always 0 (due to the way that the model is fit) the median is lower than the mean. For that matter, the 3rd quartile is lower than the mean! However, since this is logistic regression and we don't need our residuals to be normally distributed, this doesn't really matter.\n\nFrom looking at the coefficients, we can tell that our classes are pretty unbalanced (because our intercept is far from 0 and our standard error is much smaller than it). We can also tell that our input, smart_1_normalized, doesn't have a very strong relationship to our output because the standard error (0.009) is more than 1/10 of our estimate (0.02). Our suspicions are confirmed when we look at the difference between the null & residual deviance: adding the smart_1_normalized term barely improved our deviance at all! \n\nSo, based on this model, we can say that it seems like smart_1_normalized, on its own, isn't a really good predictor of whether a hard drive is going to fail or not. We can double check this intution by plotting our model.","metadata":{"_cell_guid":"e7bd9068-9b59-4c7e-b5fc-9d9e68c07ba0","_uuid":"0ecc236e1dbc8a6f7c7a64c1f64365ee7fc97d38"}},{"cell_type":"code","source":"ggplot(hard_drives, aes(x = smart_1_normalized, y = failure)) + # draw a \n    geom_point() + # add points\n    geom_smooth(method = \"glm\", # plot a regression...\n    method.args = list(family = \"binomial\")) # ...from the binomial family","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"248fdaf9-68f1-4f33-ace7-9a739098cb33","collapsed":true,"_uuid":"e0786eb15b42f3471936eedda10d03078dcdceea"}},{"cell_type":"markdown","source":"This plot does confirm what we've learned from our model: our classes are very imbalanced and the input variable is not that helpful. It looks like this model is pretty much always just predicting that a hard drive *won't* fail, since most hard drives don't.","metadata":{"_cell_guid":"61423db1-9f3d-44aa-97e7-6e34329581aa","_uuid":"567d6661b8bc5b90be4730ef045953566c442916"}},{"cell_type":"markdown","source":"## Your turn!\n___\n\nNow it's your turn to come up with a model and interpret it!\n\n1. Pick a question to answer to using the Cameras dataset. Pick a variable to predict and one variable to use to predict it.\n2. Fit a GLM model of the appropriate family. (Check out [Monday's challenge](https://www.kaggle.com/rtatman/regression-challenge-day-1) if you need a refresher).\n3. *Optional but recommended:* Plot diagnostic plots for your model. Does it seem like your model is a good fit for your data? If you're fitting a linear or Poisson model, are the residuals normally distributed (no patterns in the first plot and the points in the second plot are all in a line)? Are there any influential outliers?\n4. Check out your model using the summary() function. Does your input variable have a strong relationship to the output variable you're predicting?\n5. Write a couple sentences describing what you've learned from your model. (It could just be that it's not a very good model!)\n5. Plot your two variables & use \"geom_smooth\" and the appropriate family to fit and plot a model. Does this confirm what you learned from examining your model?\n6. *Optional:* If you want to share your analysis with friends or to ask for help, you’ll need to make it public so that other people can see it.\n    * Publish your kernel by hitting the big blue “publish” button. (This may take a second.)\n    * Change the visibility to “public” by clicking on the blue “Make Public” text (right above the “Fork Notebook” button).\n    * Tag your notebook with 5daychallenge","metadata":{"_cell_guid":"1da05bb5-a89d-42c9-8f86-f3a9f8ecf3ff","_uuid":"aa5fa3edfaca1ac1591a7fb9abbaa7547f115833"}},{"cell_type":"code","source":"# your work goes here! :)\n# Python Code for working with Cameras Dataset\ncameras.columns\ncameras.describe()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9cca4d4d-2c04-4089-9b32-b61bd14c8c29","_uuid":"07eeb52df68f7ac14740d7774ca945086feb1a58"}},{"cell_type":"code","source":"# Python Code\n# Find the outliers in the camera prices i.e. any values lying within 3 std dev.\ncam_price_outliers = cameras[~(np.abs(cameras.Price-cameras.Price.mean())<=(3*cameras.Price.std()))]\ncam_price_outliers.head()\n","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"cameras['Release date'].unique()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"log = LogisticRegression()\nx = np.array(cameras['Max resolution']).reshape(len(cameras),1)\ny = cameras['Price']\nlog.fit(x, y)\n\nsns.lmplot('Max resolution','Price',data=cameras,fit_reg=True)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"\n\nWant more? Ready for a different dataset? [This notebook](https://www.kaggle.com/rtatman/datasets-for-regression-analysis/) has additional dataset suggestions for you to practice regression with. ","metadata":{"_cell_guid":"40502f05-f3f3-4a22-b65b-59e0f50b9055","_uuid":"5ba713b5dd6927e72fa2ee5a09ee6c928530542e"}}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}