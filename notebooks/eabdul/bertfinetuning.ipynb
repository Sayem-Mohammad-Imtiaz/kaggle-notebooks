{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install torch pytorch_lightning datasets wandb torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport tqdm\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertConfig, BertTokenizer, BertTokenizerFast\nfrom datasets import load_dataset\nimport pytorch_lightning as pl\nimport wandb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom dataset class \nclass SentimentDataset(Dataset):\n    def __init__(self, tokenizer, text, target, max_len=512):\n        self.tokenizer = tokenizer\n        self.text = text\n        self.target = target\n        self.max_len =  max_len\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        text  = self.text[idx]\n        target = self.target[idx]\n        \n        # encode the text and target into tensors return the attention masks as well\n        encoding = self.tokenizer.encode_plus(\n            text=text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n        \n        return {\n          'text': text,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BERTModel PyTorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertClassifier(torch.nn.Module):\n    \n    def __init__(self, config, model, dim=256, num_classes=2):\n        super(BertClassifier, self).__init__()\n        \n        # create the model config and BERT initialize the pretrained BERT, also layers wise outputs\n        self.config = config\n        self.base = model\n        \n        # classifier head [not useful]\n        self.head = torch.nn.Sequential(*[\n            torch.nn.Dropout(p=self.config.hidden_dropout_prob),\n            torch.nn.Linear(in_features=self.config.hidden_size, out_features=dim),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=self.config.hidden_dropout_prob),\n            torch.nn.Linear(in_features=dim, out_features=num_classes)\n        ])\n    \n    def forward(self, input_ids, attention_mask=None):\n        \n        # first output is top layer output, second output is context of input seq and third output will be layerwise tokens \n        top_layer, pooled, layers = self.base(input_ids, attention_mask)\n        outputs = self.head(pooled)\n        return top_layer, outputs, layers\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lightning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertFinetuner(pl.LightningModule):\n    \n    def __init__(self, model=None, tokenizer=None, data_file=\"./data/twitter/train.csv\", use_cols=['review_text', 'sentiment'], batch_size=32):\n        super(BertFinetuner, self).__init__()\n        \n        # initialize the BERT model c\n        self.model = model\n        self.data_file = data_file\n        self.use_cols = use_cols\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        \n        self.f_score= Fbeta()\n    \n    def accuracy(self, outputs, targets):\n        correct = 0\n        for i in range(outputs.shape[0]):\n            if outputs[i]==targets[i]:\n                correct+=1\n        return correct/outputs.shape[0]\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        top_layer, outputs, layers =  self.model(input_ids, attention_mask)\n        return top_layer, outputs, layers\n    \n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(params=self.parameters(), lr=1e-5)\n    \n    def train_dataloader(self):\n        # first 30% data reserved for validation\n        train = load_dataset(\"csv\", data_files=self.data_file, split='train[20%:]')\n        text, target = train['review_text'], train['sentiment']\n        dataset = SentimentDataset(tokenizer=self.tokenizer, text=text, target=target)\n        loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n        return loader\n        \n    def training_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets =  batch['input_ids'], batch['attention_mask'], batch['targets']\n        _, logits, _ = self(input_ids, attention_mask)\n        loss = F.cross_entropy(logits, targets)\n        acc = self.accuracy(logits.argmax(dim=1), targets)\n        wandb.log({\"Loss\": loss, \"Accuracy\": torch.tensor(acc)})\n        return {\"loss\": loss, \"accuracy\": torch.tensor(acc)}\n    \n    def val_dataloader(self):\n        # first 30% data reserved for validation\n        val = load_dataset(\"csv\", data_files=self.data_file, split='train[:20%]')\n        text, target = val['review_text'], val['sentiment']\n        dataset = SentimentDataset(tokenizer=self.tokenizer, text=text, target=target)\n        loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n        return loader\n        \n    def validation_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets =  batch['input_ids'], batch['attention_mask'], batch['targets']\n        _, logits, _ = self(input_ids, attention_mask)\n        loss = F.cross_entropy(logits, targets)\n        acc = self.accuracy(logits.argmax(dim=1), targets)\n#         wandb.log({\"val_loss\":loss, \"val_accuracy\":acc})\n        self.f_score(logits.argmax(dim=1), targets)\n        return {\"val_loss\": loss, \"val_accuracy\": torch.tensor(acc)}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n        avg_f_score = self.f_score.compute()\n        wandb.log({\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_fb\":avg_f_score})\n        return {'val_accuracy': avg_loss, 'val_accuracy': avg_acc, \"val_fb\":avg_f_score}\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.metrics import Fbeta \nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, ProgressBar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT_DIR = \"../input/amazonproductsreview/amazon-review/\"\nDATASET = \"books\"\nNUM_CLASSES = 2\nBATCH_SIZE = 16\nMAX_LEN = 512\nEPOCH = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logger \nlogger = WandbLogger(\n    name=DATASET,\n    save_dir=\"../working/\",\n    project=\"domain-adaptation\",\n    log_model = True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# callbacks\nearly_stopping = EarlyStopping(\n    monitor=\"val_accuracy\",\n)\nmodel_checkpoint = ModelCheckpoint(\n    filepath=\"{epoch}-{val_accuracy:.2f}-{val_loss:.2f}\",\n    monitor=\"val_accuracy\",\n    save_top_k=1,\n)\nprogress_bar = ProgressBar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the BERTConfig, BERTTokenizer, and BERTModel \nmodel_name = \"bert-base-uncased\"\nconfig = BertConfig.from_pretrained(model_name, output_hidden_states=True)\ntokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\nbert = BertModel.from_pretrained(model_name, config=config)\nclassifier = BertClassifier(config=config, model=bert, num_classes=NUM_CLASSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertFinetuner(\n    model=classifier,\n    data_file=os.path.join(ROOT_DIR, DATASET+\".csv\"),\n    tokenizer=tokenizer,\n    batch_size=BATCH_SIZE\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner = pl.Trainer(\n    logger=logger,\n    gpus=[0],\n    checkpoint_callback=model_checkpoint,\n    max_epochs=EPOCH,\n    precision=16\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.fit(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Save trained state dictionary\n- See section 4 : https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Books"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH =  DATASET+\"-512\"+\".pt\"\n# save the model \ntorch.save(classifier.state_dict(), PATH)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Load from state dictionary\nclassifier_trained = BertClassifier(config=config, model=bert, num_classes=NUM_CLASSES)\nclassifier_trained.load_state_dict(torch.load(PATH))\n\n# you can evaluate the model on top 20% data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### DVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH =  DATASET+\"-512\"+\".pt\"\n# save the model \ntorch.save(classifier.state_dict(), PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Load from state dictionary\nclassifier_dvd = BertClassifier(config=config, model=bert, num_classes=NUM_CLASSES)\nclassifier_dvd.load_state_dict(torch.load(PATH))\n\n# you can evaluate the model on top 20% data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## There you go ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}