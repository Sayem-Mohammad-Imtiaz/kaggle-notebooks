{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":">>> import matplotlib.pyplot as plt\n>>> import pandas as pd\n>>> from sklearn import (\n...     ensemble,\n...     preprocessing,\n...     tree,\n... )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":">>> from sklearn.metrics import (\n...     auc,\n...     confusion_matrix,\n...     roc_auc_score,\n...     roc_curve,\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from sklearn.model_selection import (\n...     train_test_split,\n...     StratifiedKFold,\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from yellowbrick.classifier import (\n...     ConfusionMatrix,\n...     ROCAUC,\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from yellowbrick.model_selection import (\n...     LearningCurve,\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> url = (\n...     \"http://biostat.mc.vanderbilt.edu/\"\n...     \"wiki/pub/Main/DataSets/titanic3.xls\"\n... )\n>>> df = pd.read_excel(url)\n>>> orig_df = df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> # import pandas_profiling\n>>> # pandas_profiling.ProfileReport(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().iloc[:,:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking missing values by index\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get count of missing feature for each sample\ndf.isnull().sum(axis=1).loc[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['sex'].value_counts(dropna=False), df['embarked'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> df = df.drop(\n...     columns=[\n...         \"name\",\n...         \"ticket\",\n...         \"home.dest\",\n...         \"boat\",\n...         \"body\",\n...         \"cabin\",\n...     ]\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# During dummmy creation we have to drop firstcolumn to avoid multi collinearity \ndf = pd.get_dummies(df, drop_first=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> y = df.survived\n>>> X = df.drop(columns=\"survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Test split \n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.3, random_state=42\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using iterative imputer from SKlearn \n>>> from sklearn.experimental import (\n...     enable_iterative_imputer,\n... )\n>>> from sklearn import impute\n>>> num_cols = [\n...     \"pclass\",\n...     \"age\",\n...     \"sibsp\",\n...     \"parch\",\n...     \"fare\",\n...     \"sex_male\",\n... ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> imputer = impute.IterativeImputer()\n>>> imputed = imputer.fit_transform(X_train[num_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Once we have the fit method on train data we can use it on test set using only transform , we will not do fit on test set otherwise model will leak the information from train to test \nX_train.loc[:,num_cols]=imputed\nimputed=imputer.transform(X_test[num_cols])\nX_test.loc[:,num_cols]=imputed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n>>> sca = preprocessing.StandardScaler()\n>>> X_train = sca.fit_transform(X_train)\n>>> X_train = pd.DataFrame(X_train)\n>>> X_test = sca.transform(X_test)\n>>> X_test = pd.DataFrame(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Smoothing of the code using function , one function for dropping unwanted columns and get dummies for categorical to numeric \n>>> def red_titanic(df):\n...     df = df.drop(\n...         columns=[\n...             \"name\",\n...             \"ticket\",\n...             \"home.dest\",\n...             \"boat\",\n...             \"body\",\n...             \"cabin\",\n...         ]\n...     ).pipe(pd.get_dummies, drop_first=True)\n...     return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Second function for train test split ,imputing the missing values and scaling the numerical columns \n>>> def get_train_test_X_y(\n...     df, y_col, size=0.3, std_cols=None\n... ):\n...     y = df[y_col]\n...     X = df.drop(columns=y_col)\n...     X_train, X_test, y_train, y_test = train_test_split(\n...         X, y, test_size=size, random_state=42)\n...     cols = X.columns\n...     num_cols = [\n...         \"pclass\",\n...         \"age\",\n...         \"sibsp\",\n...         \"parch\",\n...         \"fare\"]\n...     fi = impute.IterativeImputer()\n...     X_train.loc[:, num_cols] = fi.fit_transform(X_train[num_cols])\n...     X_test.loc[:, num_cols] = fi.transform(X_test[num_cols])\n...     if std_cols:\n...         std = preprocessing.StandardScaler()\n...         X_train.loc[\n...             :, std_cols\n...         ] = std.fit_transform(\n...             X_train[std_cols]\n...         )\n...         X_test.loc[\n...             :, std_cols\n...         ] = std.transform(X_test[std_cols])\n...\n...     return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> ti_df = red_titanic(orig_df)\n>>> std_cols = \"pclass,age,sibsp,fare\".split(\",\")\n>>> X_train, X_test, y_train, y_test = get_train_test_X_y(\n...     ti_df, \"survived\", std_cols=std_cols\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline model with dummy classifier \n\nfrom sklearn.dummy import DummyClassifier\nbm = DummyClassifier()\nbm.fit(X_train, y_train)\nbm.score(X_test, y_test)  # accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from sklearn import metrics\n>>> metrics.precision_score(\n...     y_test, bm.predict(X_test)\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the various classifier models to compare the AUC. Model with less score but still tighter std dev can be a good modelfor us \n\nX = pd.concat([X_train, X_test])\ny = pd.concat([y_train, y_test])\nfrom sklearn import model_selection\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> for model in [\n...     DummyClassifier,\n...     LogisticRegression,\n...     DecisionTreeClassifier,\n...     KNeighborsClassifier,\n...     GaussianNB,\n...     SVC,\n...     RandomForestClassifier,\n...     xgboost.XGBClassifier]:\n...     cls = model()\n...     kfold = model_selection.KFold(n_splits=10, random_state=42)\n...     s = model_selection.cross_val_score(cls, X, y, scoring=\"roc_auc\", cv=kfold)\n...     print(f\"{model.__name__:22}  AUC: \"f\"{s.mean():.3f} STD: {s.std():.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to improve the model we may use stacking ,it takes other model and use their output to predict the target class \n>>> from mlxtend.classifier import (\n...     StackingClassifier,\n... )\n>>> clfs = [\n...     x()\n...     for x in [\n...         LogisticRegression,\n...         DecisionTreeClassifier,\n...         KNeighborsClassifier,\n...         GaussianNB,\n...         SVC,\n...         RandomForestClassifier,\n...     ]\n... ]\n>>> stack = StackingClassifier(\n...     classifiers=clfs,\n...     meta_classifier=LogisticRegression(),\n... )\n>>> kfold = model_selection.KFold(\n...     n_splits=10, random_state=42\n... )\n>>> s = model_selection.cross_val_score(\n...     stack, X, y, scoring=\"roc_auc\", cv=kfold\n... )\n>>> print(\n...     f\"{stack.__class__.__name__}  \"\n...     f\"AUC: {s.mean():.3f}  STD: {s.std():.2f}\"\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and fit the modelusing RF classfier \n>>> rf = ensemble.RandomForestClassifier(\n...     n_estimators=100, random_state=42\n... )\n>>> rf.fit(X_train, y_train)\n>>> rf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the other metrics \n>>> metrics.precision_score(\n...     y_test, rf.predict(X_test)\n... )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the feature importance of model\n>>> for col, val in sorted(\n...     zip(X_train.columns,rf.feature_importances_),\n...     key=lambda x: x[1],reverse=True)[:5]:\n...     print(f\"{col:10}{val:10.3f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimize the model using hyper parameter tuning with Grid search \n>>> rf4 = ensemble.RandomForestClassifier()\n>>> params = {\n...     \"max_features\": [0.4, \"auto\"],\n...     \"n_estimators\": [15, 200],\n...     \"min_samples_leaf\": [1, 0.1],\n...     \"random_state\": [42],\n... }\n>>> cv = model_selection.GridSearchCV(\n...     rf4, params, n_jobs=-1\n... ).fit(X_train, y_train)\n>>> print(cv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the best params to fit and test the score \n>>> rf5 = ensemble.RandomForestClassifier(\n...     **{\n...         \"max_features\": 0.4,\n...         \"min_samples_leaf\": 1,\n...         \"n_estimators\": 200,\n...         \"random_state\": 42,\n...     }\n... )\n>>> rf5.fit(X_train, y_train)\n>>> rf5.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting confusion matrix \n>>> from sklearn.metrics import confusion_matrix\n>>> y_pred = rf5.predict(X_test)\n>>> confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> mapping = {0: \"died\", 1: \"survived\"}\n>>> fig, ax = plt.subplots(figsize=(6, 6))\n>>> cm_viz = ConfusionMatrix(\n...     rf5,\n...     classes=[\"died\", \"survived\"],\n...     label_encoder=mapping,\n... )\n>>> cm_viz.score(X_test, y_test)\n>>> cm_viz.poof()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Roc curve \n>>> y_pred = rf5.predict(X_test)\n>>> roc_auc_score(y_test, y_pred)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the learning to see if we need more data in case CV score continues to rise \n>>> import numpy as np\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> cv = StratifiedKFold(12)\n>>> sizes = np.linspace(0.3, 1.0, 10)\n>>> lc_viz = LearningCurve(\n...     rf5,\n...     cv=cv,\n...     train_sizes=sizes,\n...     scoring=\"f1_weighted\",\n...     n_jobs=4,\n...     ax=ax,\n... )\n>>> lc_viz.fit(X, y)\n>>> lc_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deploying model using python pickle topersist the model and load them \n>>> import pickle\n>>> pic = pickle.dumps(rf5)\n>>> rf6 = pickle.loads(pic)\n>>> y_pred = rf6.predict(X_test)\n>>> roc_auc_score(y_test, y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}