{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Give me some credit dataset"},{"metadata":{},"cell_type":"markdown","source":"-------------------------------------------------------------------------\n       Matthieu\n--------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"## Chargement des librairies"},{"metadata":{},"cell_type":"markdown","source":"Pour cette analyse nous allons avoir besoin des librairies de calculs (panda et numpy), de graphiques (matplotlib et seaborn) et de scikit learn pour le machine learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Scikit learn librairies\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import accuracy_score\n\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous chargons les bases de données dont nous allons avoir besoin."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test= pd.read_csv(\"../input/give-me-some-credit-dataset/cs-test.csv\")\ndf= pd.read_csv(\"../input/give-me-some-credit-dataset/cs-training.csv\")\ndftmp=pd.read_csv(\"../input/give-me-some-credit-dataset/cs-training.csv\")\nsample_entry=pd.read_csv(\"../input/give-me-some-credit-dataset/sampleEntry.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Commencons par afficher les dimensions de la base de donnée."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Puis les 10 premiers éléments du dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Enfin une description mathématique des données."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regardons à présent les 5 premières valeurs du dataframe sample_entry.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_entry.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il s'agit de la probabilité de défaut calculé par la banque pour chaque individu. Nous n'utiliserons évidement pas cette donnée puisqu'elle a déjà été calculée par la banque elle même !"},{"metadata":{},"cell_type":"markdown","source":"Affichons les colonnes de notre base."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous nous intéressons au type de chaque variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il n'y a que des valeurs numériques, cela facilitera grandement le traitement de la base."},{"metadata":{},"cell_type":"markdown","source":"Regardons maintenant le nombre de valeurs nulles par colonne"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous remarquons quelque chose de problématique. Il manque près de 30 000 valeurs pour le salaire, soit environ 20% des valeurs. Il est pourtant évident que cette donnée est cruciale pour réussir notre classification. Après plusieurs essais, nous en avons conclu que la méthode de la médiane était la plus efficace pour gérer ces valeurs manquantes. Nous effectuerons cette opération par la suite."},{"metadata":{},"cell_type":"markdown","source":"Afin d'avoir un vue claire de la répartition des données, nous avons écrit une fonction afin d'afficher la valeur de chaque cellule pour chaque catégorie."},{"metadata":{"trusted":true},"cell_type":"code","source":"#A function to print every graph with the ID as \ndef print_all_values():\n    df1=df.drop('Unnamed: 0',axis=1)\n    cols=df1.columns\n    for col in cols:\n        if (df[col].dtypes !='object'):\n\n            fig1=plt.figure()\n            ax1=plt.axes()\n            plt.scatter(df[[col]],df['Unnamed: 0'],alpha=1,s=0.5)\n            plt.title(col)\n            ax1 = ax1.set(xlabel=col, ylabel='ID')\n            plt.show()\n            \n            \nprint_all_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous pouvons supprimer les valeurs \"aberrantes\" de la base de donnée. Cela nous aidera à améliorer les prédictions de nos modèles. Pour nous aider dans cette tâche nous avons créé une fonction qui supprime les valeurs qui paraissent trop extrêmes. Elle prend en paramètre la limite maximum qu'une valeur peut prendre en fonction des autres valeurs du même feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\ndef delete_absurd_values(df_transformed,cols,max_value,percentage):\n        \n        \n        for col in cols:\n            if (df_transformed[col].dtypes !='object'):\n                       \n                q99=df_transformed[col].quantile(q=percentage)\n                q01=df_transformed[col].quantile(q=(1-percentage))\n                for i in df_transformed.index:\n                    \n                    if (df_transformed.loc[i,col]> max_value*q99 or df_transformed.loc[i,col]< q01/max_value):\n                        df_transformed=df_transformed.drop(index=i)\n        \n        return df_transformed\n\ncols=['DebtRatio', 'MonthlyIncome',\n       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n       'NumberRealEstateLoansOrLines',\n       'NumberOfDependents']\ndf=delete_absurd_values(df,cols,4,0.999)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On fini par supprimer manuellement les valeurs qui parraissent toujours aberrantes."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[df.RevolvingUtilizationOfUnsecuredLines <30000]\ndf=df[df.DebtRatio <100000]\ndf=df[df.MonthlyIncome <15000000]\ndf=df[df.NumberRealEstateLoansOrLines <40]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En ce qui concerne les valeurs manquantes, comme nous l'avons dit précédement, nous avons décidé de les remplacer par la valeur médiane de chaque classe. Avant cela, nous avons essayé plusieurs techniques, parmis elles : de les supprimer, supprimer la colonne, remplacer par la moyenne, remplacer par un constante."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(df.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On vérifie qu'il ne reste plus de valeur nulles"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vérifions que la répartition de la target est inchangée"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig11=plt.figure()\nax11=plt.axes()\nthe_target = dftmp['SeriousDlqin2yrs']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax11 = ax11.set(xlabel='Default proportion', ylabel='Number of people')\nthe_target.value_counts().plot.pie(startangle=90, autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Graphiques comparés"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = 'whitegrid', context = 'notebook', rc={'figure.figsize':(20,15)})\n\n\ncols = ['SeriousDlqin2yrs',\n       'RevolvingUtilizationOfUnsecuredLines', 'age',\n       'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',\n       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n       'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n       'NumberOfDependents']\n\n#sns.pairplot(df[cols])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le nombre important de features rend difficile une lecture intelligible de ce graphique. Nous préférerons utiliser une matrice de corrélation pour comprendre le lien entre les features."},{"metadata":{},"cell_type":"markdown","source":"#### Matrice de Corrélation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Matrix calcul\ncorr_mat = df.corr()\n\nfig2=plt.figure()\nsns.set(rc={'figure.figsize':(25,15)})\nk = 20\ncols = corr_mat.nlargest(k, 'SeriousDlqin2yrs')['SeriousDlqin2yrs'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Correlation Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grâce à cette matrice de corrélation on remarque que les features qui ont la plus grande influence sur la target sont l'age et d'avoir fait défaut pas le passer. Etonnement le salaire ne semble pas à première vue avoir un grand impact. Cependant ces coefficients restent relativement faibles.\n\nOn notera également qu'il y a un très grande corrélation entre les differents temps de défaut de crédit."},{"metadata":{},"cell_type":"markdown","source":"## Séparer les données entre les donnée d'entrainement et de test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('SeriousDlqin2yrs',axis=1)\ny = df['SeriousDlqin2yrs']  \n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On utilise 80% du dataset pour entrainer notre algorithme et 20% pour effectuer les tests. On donne un random_state pour qu'à chaque execution de la fonction, celle-ci sépare les données de la meme facon et ainsi on introduit un biais constant à chaque itération."},{"metadata":{},"cell_type":"markdown","source":"---------------------------------------------------------------------------------------------\n#                  Algorithmes de Machine Learning\n----------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"## Regression Logistique"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100,random_state=0)\nlogisticRegr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Score et erreur"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ERROR\nerror = (1 - logisticRegr.score(X_test, y_test))*100\nprint('Score  = ',logisticRegr.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda=LinearDiscriminantAnalysis(solver='svd',shrinkage=None,store_covariance=True)\nlda.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Score et erreur"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ERROR\nerror = (1 - lda.score(X_test, y_test))*100\nprint('Score  = ',lda.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=0)\nrf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Score et erreur"},{"metadata":{"trusted":true},"cell_type":"code","source":"error = (1 - rf.score(X_test, y_test))*100\nprint('Score  = ',rf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tree Decision Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Score et erreur"},{"metadata":{"trusted":true},"cell_type":"code","source":"error = (1 - clf.score(X_test, y_test))*100\nprint('Score  = ',clf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Résultats"},{"metadata":{},"cell_type":"markdown","source":"Après entrainement de nos algorithmes, nous avons obtenu les résultats suivants sur le jeu de test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Taux de réussite par modèle:\\n\\nRégression Logistique:',logisticRegr.score(X_test, y_test)*100,'%','\\n\\nLDA:',lda.score(X_test, y_test)*100,'%','\\n\\nRandom Forest Classifier:',rf.score(X_test, y_test)*100,'%','\\n\\nDecision Tree Classifier:',clf.score(X_test, y_test)*100,'%')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}