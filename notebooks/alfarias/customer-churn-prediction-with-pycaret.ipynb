{"cells":[{"metadata":{"tags":[],"cell_id":"7088943d-f7b9-4b97-b640-6a775e1e8450"},"cell_type":"markdown","source":"![](https://res.cloudinary.com/dn1j6dpd7/image/fetch/f_auto,q_auto,w_736/https://www.livechat.com/wp-content/uploads/2016/04/customer-churn@2x.jpg)\n<i>Image Source:</i> [What Is Churn Rate and Why It Will Mess With Your Growth](https://www.livechat.com/success/churn-rate/)","execution_count":null},{"metadata":{"tags":[],"cell_id":"9b238085-0cbf-4c9e-afa5-88b72a835fae"},"cell_type":"markdown","source":"## 1. Introduction\nCustomer Churn is when customers leave a service in a given period of time, what is bad for business.<br>\nThis work has as objective to build a machine learning model to predict what customers will leave the service. Also, an Exploratory Data Analysis is made to a better understand about the data. \nAnother point on this work is use the [PyCaret](https://pycaret.org/) Python Module to make all the experiment pipeline. ","execution_count":null},{"metadata":{"tags":[],"cell_id":"1e78a237-948f-480d-ba96-3503917949ad"},"cell_type":"markdown","source":"### 1.1 Enviroment Setup\nThe Modules used for this work, highlights for PyCaret and good plots by [Plotly](https://plotly.com/).","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install pycaret","execution_count":null,"outputs":[]},{"metadata":{"cell_id":"464a9356-824f-4c4d-9945-224698b09877","output_cleared":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Standard\nimport pandas as pd\nimport numpy as np\n# Pycaret\nfrom pycaret.classification import *\n# Plots\nfrom plotly.offline import iplot\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\n# Sklearn tools\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import *\n# Extras\nfrom datetime import date\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Datapath and Setup\ndata_path = \"/kaggle/input/telco-customer-churn/\"\nrandom_seed = 142","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"2bcfc94d-a561-405d-ac35-65ac706a54dc"},"cell_type":"markdown","source":"And the helper functions used on this notebook.","execution_count":null},{"metadata":{"tags":[],"cell_id":"28308802-8246-4878-aa00-e65ee5bd184a","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Helper functions for structured data\n## Get info about the dataset\ndef dataset_info(dataset, dataset_name: str):\n    print(f\"Dataset Name: {dataset_name} | Number of Samples: {dataset.shape[0]} | Number of Columns: {dataset.shape[1]}\")\n    print(30*\"=\")\n    print(\"Column             Data Type\")\n    print(dataset.dtypes)\n    print(30*\"=\")\n    missing_data = dataset.isnull().sum()\n    if sum(missing_data) > 0:\n        print(missing_data[missing_data.values > 0])\n    else:\n        print(\"No Missing Data on this Dataset!\")\n    print(30*\"=\")\n    print(f\"Memory Usage: {np.round(dataset.memory_usage(index=True).sum() / 10e5, 3)} MB\")\n## Dataset Sampling\ndef data_sampling(dataset, frac: float, random_seed: int):\n    data_sampled_a = dataset.sample(frac=frac, random_state=random_seed)\n    data_sampled_b =  dataset.drop(data_sampled_a.index).reset_index(drop=True)\n    data_sampled_a.reset_index(drop=True, inplace=True)\n    return data_sampled_a, data_sampled_b   \n## Bar Plot\ndef bar_plot(data, plot_title: str, x_axis: str, y_axis: str):\n    colors = [\"#0080ff\",] * len(data)\n    colors[0] = \"#ff8000\"\n    trace = go.Bar(y=data.values, x=data.index, text=data.values, \n                    marker_color=colors)\n    layout = go.Layout(autosize=False, height=600,\n                    title={\"text\" : plot_title,\n                       \"y\" : 0.9,\n                       \"x\" : 0.5,\n                       \"xanchor\" : \"center\",\n                       \"yanchor\" : \"top\"},  \n                    xaxis={\"title\" : x_axis},\n                    yaxis={\"title\" : y_axis},)\n    fig = go.Figure(data=trace, layout=layout)\n    fig.update_layout(template=\"simple_white\")\n    fig.update_traces(textposition=\"outside\",\n                    textfont_size=14,\n                    marker=dict(line=dict(color=\"#000000\", width=2)))                \n    fig.update_yaxes(automargin=True)\n    iplot(fig)\n## Plot Pie Chart\ndef pie_plot(data, plot_title: str):\n    trace = go.Pie(labels=data.index, values=data.values)\n    layout = go.Layout(autosize=False,\n                    title={\"text\" : plot_title,\n                       \"y\" : 0.9,\n                       \"x\" : 0.5,\n                       \"xanchor\" : \"center\",\n                       \"yanchor\" : \"top\"})\n    fig = go.Figure(data=trace, layout=layout)\n    fig.update_traces(textfont_size=14,\n                    marker=dict(line=dict(color=\"#000000\", width=2)))\n    fig.update_yaxes(automargin=True)            \n    iplot(fig)\n## Histogram\ndef histogram_plot(data, plot_title: str, y_axis: str):\n    trace = go.Histogram(x=data)\n    layout = go.Layout(autosize=False,\n                    title={\"text\" : plot_title,\n                       \"y\" : 0.9,\n                       \"x\" : 0.5,\n                       \"xanchor\" : \"center\",\n                       \"yanchor\" : \"top\"},  \n                    yaxis={\"title\" : y_axis})\n    fig = go.Figure(data=trace, layout=layout)\n    fig.update_traces(marker=dict(line=dict(color=\"#000000\", width=2)))\n    fig.update_layout(template=\"simple_white\")\n    fig.update_yaxes(automargin=True)\n    iplot(fig)\n# Particular case: Histogram subplot (1, 2)\ndef histogram_subplot(dataset_a, dataset_b, feature_a: str,\n                        feature_b: str, title: str, title_a: str, title_b: str):\n    fig = make_subplots(rows=1, cols=2, subplot_titles=(\n                        title_a,\n                        title_b\n                        )\n                    )\n    fig.add_trace(go.Histogram(x=dataset_a[feature_a], showlegend=False), row=1, col=1)\n    fig.add_trace(go.Histogram(x=dataset_b[feature_b], showlegend=False), row=1, col=2)\n    fig.update_layout(template=\"simple_white\")\n    fig.update_layout(autosize=False,\n                        title={\"text\" : title,\n                        \"y\" : 0.9,\n                        \"x\" : 0.5,\n                        \"xanchor\" : \"center\",\n                        \"yanchor\" : \"top\"},  \n                        yaxis={\"title\" : \"<i>Frequency</i>\"})\n    fig.update_traces(marker=dict(line=dict(color=\"#000000\", width=2)))\n    fig.update_yaxes(automargin=True)\n    iplot(fig)\n# Calculate scores with Test/Unseen labeled data\ndef test_score_report(data_unseen, predict_unseen):\n    le = LabelEncoder()\n    data_unseen[\"Label\"] = le.fit_transform(data_unseen.Churn.values)\n    data_unseen[\"Label\"] = data_unseen[\"Label\"].astype(int)\n    accuracy = accuracy_score(data_unseen[\"Label\"], predict_unseen[\"Label\"])\n    roc_auc = roc_auc_score(data_unseen[\"Label\"], predict_unseen[\"Label\"])\n    precision = precision_score(data_unseen[\"Label\"], predict_unseen[\"Label\"])\n    recall = recall_score(data_unseen[\"Label\"], predict_unseen[\"Label\"])\n    f1 = f1_score(data_unseen[\"Label\"], predict_unseen[\"Label\"])\n\n    df_unseen = pd.DataFrame({\n        \"Accuracy\" : [accuracy],\n        \"AUC\" : [roc_auc],\n        \"Recall\" : [recall],\n        \"Precision\" : [precision],\n        \"F1 Score\" : [f1]\n    })\n    return df_unseen\n# Confusion Matrix\ndef conf_mat(data_unseen, predict_unseen):\n    unique_label = data_unseen[\"Label\"].unique()\n    cmtx = pd.DataFrame(\n        confusion_matrix(data_unseen[\"Label\"], predict_unseen[\"Label\"], labels=unique_label), \n        index=['{:}'.format(x) for x in unique_label], \n        columns=['{:}'.format(x) for x in unique_label]\n    )\n    ax = sns.heatmap(cmtx, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n    ax.set_ylabel('Predicted')\n    ax.set_xlabel('Target');\n    ax.set_title(\"Predict Unseen Confusion Matrix\", size=14);","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"5ae1c017-271e-410e-abd5-0be3b8727766"},"cell_type":"markdown","source":"## 2. Load Data\n\nThe Dataset is load as a Pandas dataframe and show a gimplse of the data.\nA good thing about Deepnote is that the displayed dataframes shows the column type, helping to understand the features.","execution_count":null},{"metadata":{"tags":[],"cell_id":"cec0aaf1-7373-484e-bcdd-e39fa067e8ad","output_cleared":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(data_path+\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for duplicated samples.","execution_count":null},{"metadata":{"tags":[],"cell_id":"8827849f-c74e-419a-adff-0a0d71e75f1c","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataset[dataset.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"9b3cb07f-e86e-494a-87ec-04aeafbc5144"},"cell_type":"markdown","source":"Is needed so more information about the dataset as the number of samples, memory size allocation, etc.<br>\nThe result is showed on the following output.","execution_count":null},{"metadata":{"tags":[],"cell_id":"7a4a0d96-1085-4179-9523-fe1774edff40","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataset_info(dataset, \"customers\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"f8a7621d-34b8-49d8-85a6-ac7b71a545ba"},"cell_type":"markdown","source":"The dataset has a small memory size allocation (1.183 MB) and is composed for many Categorical (object) features and only a few numeric, but one of the categorical features doesn't look right, the `TotalCharges`, as showed on the displayed dataframe, the festure is numeric.<br>\n`TotalCharges` is converted from Object to float64, the same of `MonthlyCharges` feature.","execution_count":null},{"metadata":{"tags":[],"cell_id":"3520f897-c87b-4a90-9814-8edfd3220119","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataset[\"TotalCharges\"] = pd.to_numeric(dataset[\"TotalCharges\"], errors=\"coerce\")\nprint(f\"The Feature TotalCharges is type {dataset.TotalCharges.dtype} now!\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"74bc6ebf-5467-4c13-af9f-4d2744ba11ec"},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis","execution_count":null},{"metadata":{"tags":[],"cell_id":"aceeb11e-7a7d-4367-9492-1714689c6c5a"},"cell_type":"markdown","source":"### 3.1 Churn Distribution\nThe Client Churn Distribution is checked for any imbalance, as the feature is the target, it's important to choose what strategy to adopt when dealing with imbalanced classes.<br>\nBelow, a Pie Chart shows the feature distribution.\n","execution_count":null},{"metadata":{"tags":[],"cell_id":"0e06fe3a-30b9-4ff2-b803-951dd0b8e46c","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pie_plot(dataset[\"Churn\"].value_counts(), plot_title=\"<b>Client Churn Distribution<b>\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"e30ae2f1-f982-462f-80f3-1bc64e1d31d1"},"cell_type":"markdown","source":"There's some imbalance on Churn Distribution, 26.5% of the clients have churned, and small occurences of a label could lead to bad predictor.<br>\nIt's possible to choose some ways to work with this case:\n* Make a random over-sampling, duplicating some samples of the minority class until this reach a balance, but this could lead to an overfitted model.\n* Make a random down-sampling, removing some samples from the majority class until this reach a balance, but this leads to information loss and not feeding the model with the collected samples.\n* Make a random down-sampling, removing some samples from the majority class until this reach a balance, but this leads to information loss and not feeding the model with the collected samples.\n* Another resampling technique, as SMOTE.\n* Choosing a metric that deals with imbalanced datasets, like F1 Score.\n\nThe Churn problem is about client retention, so is worth to check about false positives, so precision and recall metrics are a must for this situtation.<br>\nF1 Score is used to check the quality of the model predictions, as the metric is an harmonic mean of precision and recall. ","execution_count":null},{"metadata":{"tags":[],"cell_id":"61904ad8-8f19-4ed5-bf91-8fbabcb40de1"},"cell_type":"markdown","source":"### 3.2 Analysis of the Contract Type\n","execution_count":null},{"metadata":{"tags":[],"cell_id":"84546349-680a-41c9-9a0f-3dc2f376a8b2"},"cell_type":"markdown","source":"The contract type is a good feature to analyze what happens to a client churn from that service, a plot from the contract types of not churned clients is showed below.","execution_count":null},{"metadata":{"tags":[],"cell_id":"b560ac2d-e6c1-49ec-980e-da39712c2f85","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_aux = dataset.query('Churn == \"No\"')\ndf_aux = df_aux[\"Contract\"].value_counts()\nbar_plot(df_aux, \"<b>Contract Types of not Churned Clients</b>\", \"<i>Contract</i>\", \"<i>Count</i>\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"a5778511-9798-45f7-8022-db6210e76fc6"},"cell_type":"markdown","source":"Is showed that a Month-to-month contract is the firts when compared to annual contracts, but the difference between the number of contracts is not so big.<br>\nTo a better comparation, the same plot is showed for the churned clients.","execution_count":null},{"metadata":{"tags":[],"cell_id":"d73d0256-a7f9-48af-9b08-741ece81ff43","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_aux = dataset.query('Churn == \"Yes\"')\ndf_aux = df_aux[\"Contract\"].value_counts()\nbar_plot(df_aux, \"<b>Contract Types of Churned Clients</b>\", \"<i>Contract</i>\", \"<i>Count</i>\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"da76f779-776c-4d75-bd18-d0e9c4e8e439"},"cell_type":"markdown","source":"Now, the difference between a Month-to-month and annual contractts is bigger, and can lead to a conclusion that annual contracts are better to retain the clients, perhaps fidelity promotions could aid to reduce the churn rate.<br>\nAs the problem can be examined more deep on Month-to-month contract types, a good idea is see the Monthly Charges and Total Charges distribution for the not churned clients of this contract.\n","execution_count":null},{"metadata":{"tags":[],"cell_id":"632a627e-7764-4629-9408-93e86f15d99e","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_aux = dataset.query('(Contract == \"Month-to-month\") and (Churn == \"No\")')\nhistogram_subplot(df_aux, df_aux, \"MonthlyCharges\", \"TotalCharges\", \"<b>Charges Distribution for Month-to-month contracts for not Churned Clients</b>\",\n                \"(a) Monthtly Charges Distribution\", \"(b) Total Charges Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"af9de48f-100b-441c-98c0-76a28a29fc46"},"cell_type":"markdown","source":"From the plots, can be said that many clients just got charged with a few values, principally for the Total Charges.<br>\nOn the following plots, the same features are analyzed, but for churned clients.","execution_count":null},{"metadata":{"tags":[],"cell_id":"3a7d59f0-0bfa-40a1-80bc-d8208382cf9c","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_aux = dataset.query('(Contract == \"Month-to-month\") and (Churn == \"Yes\")')\nhistogram_subplot(df_aux, df_aux, \"MonthlyCharges\", \"TotalCharges\", \"<b>Charges Distribution for Month-to-month contracts for Churned Clients</b>\",\n                \"(a) Monthtly Charges Distribution\", \"(b) Total Charges Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"0a9867c9-7fbc-491c-ac50-c033404f654f"},"cell_type":"markdown","source":"Total Charges had the same behaviour, but the Monthly Charges for many churned clients was high, maybe the amount of chage value could lead the client to leave the service.<br>\nStill on the Month-to-month contract, it's time to analyze the most used Payment methods of churned clients.","execution_count":null},{"metadata":{"tags":[],"cell_id":"5b9d7be6-7b0a-4276-bf80-0e955af80524","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_aux = dataset.query(('Contract == Month-to-month') and ('Churn == \"Yes\"'))\ndf_aux = df_aux[\"PaymentMethod\"].value_counts()\nbar_plot(df_aux, \"<b>Payment Method of Month-to-month contract Churned Clients</b>\", \"<i>Payment Method</i>\", \"<i>Count</i>\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"0b85ae6a-63cf-457b-a105-f013d1798415"},"cell_type":"markdown","source":"Many Churned Clients used to pay with electronic checks, automatic payments, as bank transfers or credit card have a few churned clients. A good idea could make promotions to clients that use automatic payment methods. <br>\nLastly, the tenure of the churned clients.","execution_count":null},{"metadata":{"tags":[],"cell_id":"0bcdd312-51fd-4748-8035-889060017e5c","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_aux = dataset.query(('Contract == Month-to-month') and ('Churn == \"Yes\"'))\ndf_aux = df_aux[\"tenure\"].value_counts().head(5)\nbar_plot(df_aux, \"<b>Tenure of Month-to-month contract for Churned Clients</b>\", \"<i>Tenure</i>\", \"<i>Count</i>\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"007f6510-4a74-46a4-952b-225af9f37d03"},"cell_type":"markdown","source":"Most clients just used the service for one month, seems like the clients used to service to check the quality or the couldn't stay for the amount of charges, as the Monthly Charges for these clients was high and the Total Charges was small, as the client just stayed a little time.  ","execution_count":null},{"metadata":{"tags":[],"cell_id":"8546f71b-652c-4909-b31b-b11cd7cc52c1"},"cell_type":"markdown","source":"## 4. Setting up PyCaret","execution_count":null},{"metadata":{"tags":[],"cell_id":"f96ae182-aead-4514-af36-6ffcf5df7e2e"},"cell_type":"markdown","source":"Before setting up PyCaret, a random sample of 10% size of the dataset will be get to make predictions with unseen data. ","execution_count":null},{"metadata":{"tags":[],"cell_id":"d6011a29-86c1-44e5-b754-8d952ddce4cc","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data, data_unseen = data_sampling(dataset, 0.9, random_seed)\nprint(f\"There are {data_unseen.shape[0]} samples for Unseen Data.\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"506eea27-2ffc-48ba-a05f-19ba290d3d1e"},"cell_type":"markdown","source":"The PyCaret's setup is made with 90% of data samples and just use one function (`setup`) from the module.<br>\nIt's possible configure with variuos options, as data pre-processing, feature engineering, etc. The easy and efficient of PyCaret buy a lot of time when prototyping models.<br>\nEach setup is an experiment and for this problem, is used the following options:\n* Normalization of the numerical features with Z-Score.\n* Feature Selection with permutation importance techniques.\n* Outliers Removal.\n* Features Removal based on Multicollinearity.\n* Features Scalling Transformation.\n* Ignore low variance on Features.\n* PCA for Dimensionality Reduction, as the dataset has many features.\n* Numeric binning on the features `MonthlyCharges` and `TotalCharges`.\n* 70% of samples for Train and 30% for test.","execution_count":null},{"metadata":{"tags":[],"cell_id":"6f4ed29e-1357-47b2-a4e4-637ce4119bbc","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"exp01 = setup(data=data, target=\"Churn\", session_id=random_seed, ignore_features=[\"customerID\"], \n                numeric_features=[\"SeniorCitizen\"], normalize=True,\n                feature_selection=True, remove_outliers=True, remove_multicollinearity=True,\n                transformation=True, ignore_low_variance=True, pca=True, \n                bin_numeric_features=[\"MonthlyCharges\", \"TotalCharges\"])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"8d33c0f2-51f2-4b29-8548-7b354dd14b00"},"cell_type":"markdown","source":"PyCaret shows at first if all features types are with it correspondent type, if everything is right, press enter on the blank bar and the setup is finished showing a summary of the experiment. ","execution_count":null},{"metadata":{"tags":[],"cell_id":"4ae241d1-57c6-4dcc-91c0-5d6f10b11bf1"},"cell_type":"markdown","source":"## 5. Model Build","execution_count":null},{"metadata":{"tags":[],"cell_id":"cc8f9047-81a4-46cc-902e-5450348b5b33"},"cell_type":"markdown","source":"A great tool on PyCaret is build many models and compare a metric for the bests! <br>\nDue the target class imbalance, the models are sorted by F1 Score.","execution_count":null},{"metadata":{"tags":[],"cell_id":"922945dc-4001-4a47-b782-9515570044cf","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"compare_models(fold=10, sort=\"F1\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"0bdc63ac-21e4-4d6f-afa3-a12cce4e2023"},"cell_type":"markdown","source":"The best model suggested by PyCaret is the Quadratic Discriminant Analysis (QDA), with a F1 Score around 0.6 and a good Recall, around 0.7.<br>\nLet's stick with QDA and create the model.","execution_count":null},{"metadata":{"tags":[],"cell_id":"be5c3da4-91f5-43f1-a5dd-a36802be9dc5","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"base_alg = \"qda\"\nbase_model = create_model(base_alg)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"09d865c0-22e4-4beb-90de-97995150e751"},"cell_type":"markdown","source":"And see the hyper-parameters used for build the base model.","execution_count":null},{"metadata":{"tags":[],"cell_id":"b19ec42a-7997-4c7d-8049-f65285545275","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_model(base_model, plot=\"parameter\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"041fc47a-947b-4bc2-8491-f998e1d3eb64"},"cell_type":"markdown","source":"It's possible to tune the base model and optmize a metric, for this case, F1 Score.","execution_count":null},{"metadata":{"tags":[],"cell_id":"59d5cd22-35bd-419d-9491-9a3ddae2ae71","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tuned_model = tune_model(base_alg, fold=10, optimize=\"F1\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"f02f080a-5f37-44b7-87ca-1e8280a61ef9"},"cell_type":"markdown","source":"There's an improvement from the base model on F1 Score! Now, time to see what hyper-parameters were used by the tuned model.","execution_count":null},{"metadata":{"tags":[],"cell_id":"194c9fad-7f9e-4688-adab-64923b1fadbb","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_model(tuned_model, plot=\"parameter\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"590b1194-08b9-4c9e-9329-9b1efab65302"},"cell_type":"markdown","source":"The `reg_param` was higher on the tuned model.<br>\nThe Only problem with using QDA is that not possible to get the Features Importance plot of the model, but the model is used as some good insights about the data were got on the EDA.<br> \nPyCaret also has functions to make ensembles, for this implementation, a bagged model is build.","execution_count":null},{"metadata":{"tags":[],"cell_id":"66bf4246-2a40-494b-bd2e-69bd8897ff23","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"bagged_model = ensemble_model(tuned_model)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"282646c1-03c4-4ad2-a39c-cc99f0fafddb"},"cell_type":"markdown","source":"The bagged model improved a bit the F1 Score, it's also possible make blended and stacked models with PyCaret, both models are created using the the tuned and bagged models with the soft method.","execution_count":null},{"metadata":{"tags":[],"cell_id":"bb52afc0-9e6b-4be8-8e41-c51e0f88317c","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"blended_model = blend_models(estimator_list=[tuned_model, bagged_model], method=\"soft\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"1ba2d156-82f0-4628-9d9a-d6f021b8565e","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"stacked_model = stack_models([tuned_model, bagged_model], method=\"soft\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"a76c2608-fb6d-4ee9-b706-88391ff104cc"},"cell_type":"markdown","source":"Although the stacked model had improved precision on a good amount, the blended still got a better F1 and it is saved as the best model.<br>\nThis model is saved and its ROC Curves are plotted below. ","execution_count":null},{"metadata":{"tags":[],"cell_id":"b112eea9-4394-414c-8aec-00a046bc7782","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best_model = blended_model\nplot_model(best_model, plot=\"auc\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"c1d1172b-74f5-445e-bfa7-d06b05834269"},"cell_type":"markdown","source":"## 6. Prediction on Test Data","execution_count":null},{"metadata":{"tags":[],"cell_id":"3c3a666c-a86e-4315-ba38-c659e60f9a18"},"cell_type":"markdown","source":"The test is made with the remaining 30% of data that PyCaret got on the setup, it's important to see that the model is not overfitting.","execution_count":null},{"metadata":{"tags":[],"cell_id":"49f5255a-960e-4245-a7bd-6b86c5018e91","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"predict_model(best_model);","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"80c031f4-190f-4991-a95c-1a00055a80a6"},"cell_type":"markdown","source":"As everything is right with the model, it's time to finalize it fitting all the data.","execution_count":null},{"metadata":{"tags":[],"cell_id":"d0909c9b-9391-4ad0-bd22-f9a5bbcd679f","trusted":true},"cell_type":"code","source":"final_model = finalize_model(best_model)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"4d403f19-594d-40d7-92be-84094ccac0ba"},"cell_type":"markdown","source":"## 7. Prediction on Unseen Data","execution_count":null},{"metadata":{"tags":[],"cell_id":"949f3cc4-95ff-45c2-ba2c-e9fdaa356aa6"},"cell_type":"markdown","source":"The remaining 10% data is used to make predictions with unseen samples, what could include some outliers, it's how real world data works.<br>\nJust Kappa Score is not showed, as the focus is the F1 Score.<br>\nIt's not necessary to make any transformation on the data, PyCaret do this.","execution_count":null},{"metadata":{"tags":[],"cell_id":"f2774b74-956e-4869-a3fd-3b041c16f22e","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"predict_unseen = predict_model(final_model, data=data_unseen);\nscore_unseen = test_score_report(data_unseen, predict_unseen)\nprint(score_unseen.to_string(index=False))\nconf_mat(data_unseen, predict_unseen);","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"44ccff9f-85ca-415a-8057-aec964fdf547"},"cell_type":"markdown","source":"And the Unseen Data predicts as the trained model! The model was sucessful built! ","execution_count":null},{"metadata":{"tags":[],"cell_id":"ee75d6f1-e02e-410a-b705-27503849659e"},"cell_type":"markdown","source":"## 8. Save Experiment and Model","execution_count":null},{"metadata":{"tags":[],"cell_id":"52e2e634-b24b-4a3a-b75f-ed22f217d916"},"cell_type":"markdown","source":"PyCaret allows to save all the pipeline experiment and the model to deploy.<br>\nIt's recommended to save with date of the experiments.","execution_count":null},{"metadata":{"tags":[],"cell_id":"a774416a-ff50-44c6-b542-eb25a3d4daa6","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"save_experiment(\"expQDA_\"+date.today().strftime(\"%m-%d-%Y\"))\nsave_model(tuned_model, \"modelQDA_\"+date.today().strftime(\"%m-%d-%Y\"))","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"b62c91e5-db7d-49f8-9747-7e9f0c71cf47"},"cell_type":"markdown","source":"## 9. Conclusion","execution_count":null},{"metadata":{"tags":[],"cell_id":"90128e4b-60f7-4c8d-8add-f76174294f61"},"cell_type":"markdown","source":"From the results and explanations presented here, some conclusion can be draw:\n* The type of contract has a strict relationship with churned clients, Month-to-month contracts with high amount of charges could lead a client to leave the service.\n* For the predictions made by the model and based on the precision and recall scores, as F1 Score try to show a balance between these two metrics, the precision was near 50%, what means that the model predict correctly 54% of classified clients as churned, on other hand, the recall was good, where around 80% of the actually churned clients was predict correctly.\n* The metrics must be used in favor of the business interests, is needed a more correct prediction of the churned clients or get more a part of these clients?\n* One possible way to improve the results is SMOTE as data resampling.\n\nFrom the tools and enviroment used:\n* PyCaret is incredible, it speed up the model build a lot and the pre-processing is very useful, beyond of the functions to save and deploy the model.\n\nThanks for you reading!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}