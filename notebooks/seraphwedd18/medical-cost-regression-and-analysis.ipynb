{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Peek"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport time\nstart_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/insurance/insurance.csv\")\nprint(df.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Mapping"},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_map = {'male':0, 'female':1}\nsmoker_map = {'no':0, 'yes':1}\nregion_map = {'southwest':0, 'northwest':1, 'northeast':2, 'southeast':3}\ndf.sex = df.sex.map(sex_map)\ndf.smoker = df.smoker.map(smoker_map)\ndf.region = df.region.map(region_map)\nactual_charges = df.charges.values.tolist()\ndf.charges = df.charges.map(lambda x: np.log(x))\nprint(df.head(10))\nprint(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 17025\nnp.random.seed(random_seed)\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Average, Multiply, LeakyReLU\nfrom tensorflow.keras.layers import Input, BatchNormalization, concatenate\nfrom tensorflow.keras.optimizers import RMSprop, Adam, Nadam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom skimage.transform import resize\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Design"},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_reg_model(in_shape):\n    #Input layer\n    INPUT = Input(in_shape)\n    #Filter Layer\n    k = 16\n    f = []\n    for i in range(k):\n        f.append(Dense(256, activation='relu', kernel_initializer='normal')(INPUT))\n    for i in range(k):\n        f[i] = Dense(128, activation='relu')(f[i])\n        f[i] = Dropout(0.25)(f[i])\n    y = []\n    for i in range(k-2):\n        y.append(concatenate(f[i:i+2], axis=0))\n    x = Average()(f)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1)(x)\n    \n    model = Model(inputs=INPUT, outputs=[x])\n    \n    optimizer = Adam(lr=0.01, decay=1e-5)\n    \n    #Compile model\n    model.compile(optimizer,\n                  loss='msle',#'mse',\n                  metrics=['mae']\n                 )\n    return model\n\nmodel = new_reg_model(df.shape[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"lrr = ReduceLROnPlateau(monitor = 'val_mae',\n                         patience = 10,\n                         verbose = 1,\n                         factor = 0.5,\n                         min_lr = 0.00001)\n\nes = EarlyStopping(monitor='val_loss',\n                   mode='min',\n                   verbose=1,\n                   patience=50,\n                   restore_best_weights=True)\n\ncols = df.columns[:-1]\n#['age', 'sex', 'bmi', 'children', 'smoker']\nprint(cols)\nx_train, x_val, y_train, y_val = train_test_split(df[cols], df.charges, test_size=0.25, shuffle=True, random_state=101)\nmodel = new_reg_model(x_train.shape[1:])\nhistory = model.fit(x_train, y_train,\n                    epochs = 2000,\n                    validation_data = (x_val, y_val),\n                    verbose=1,\n                    callbacks=[lrr, es]\n                   )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:')\nplt.plot(history.history['mae'][10:])\nplt.plot(history.history['val_mae'][10:])\nplt.title('Model MAE')\nplt.ylabel('MAE')\n#plt.gca().set_ylim([0, 20000])\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.savefig('history_mae.png')\nplt.show()\n\nprint('Loss:')\nplt.plot(history.history['loss'][10:])\nplt.plot(history.history['val_loss'][10:])\nplt.title('Model Loss')\nplt.ylabel('Loss')\n#plt.gca().set_ylim([0, 5e7])\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.savefig('history_loss.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(df[cols])\nprint(\"Actual Value vs Predicted Value:\\tDifference:\")\ndiff = []\nfor a, b in zip(df.charges, predictions):\n    a = np.exp(a)\n    b = np.exp(b)\n    diff.append(a-b[0])\n    print(\"%.2f \\t %.2f \\t\\t %.2f\" %(a, b[0], a-b[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum Deviation: %.2f\" %max(abs(x) for x in diff))\nprint(\"Minimum Deviation: %.2f\" %min(abs(x) for x in diff))\nprint(\"Average Deviation: %.2f\" %np.mean([abs(x) for x in diff]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from eli5.sklearn import PermutationImportance\nimport sklearn, eli5\nprint(sklearn.metrics.SCORERS.keys())\nperm = PermutationImportance(model, random_state=1, scoring='neg_mean_absolute_error').fit(df[cols], df.charges)\neli5.show_weights(perm, feature_names = cols.values.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retrain Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['age', 'smoker', 'children', 'bmi']\nprint(cols)\nx_train, x_val, y_train, y_val = train_test_split(df[cols], df.charges, test_size=0.25, shuffle=True, random_state=101)\nmodel = new_reg_model(x_train.shape[1:])\nhistory = model.fit(x_train, y_train,\n                    epochs = 2000,\n                    validation_data = (x_val, y_val),\n                    verbose=1,\n                    callbacks=[lrr, es]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:')\nplt.plot(history.history['mae'][10:])\nplt.plot(history.history['val_mae'][10:])\nplt.title('Model MAE Adjusted')\nplt.ylabel('MAE')\n#plt.gca().set_ylim([0, 20000])\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.savefig('history_mae_adjusted.png')\nplt.show()\n\nprint('Loss:')\nplt.plot(history.history['loss'][10:])\nplt.plot(history.history['val_loss'][10:])\nplt.title('Model Loss Adjusted')\nplt.ylabel('Loss')\n#plt.gca().set_ylim([0, 5e7])\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.savefig('history_loss_adjusted.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(df[cols])\nprint(\"Actual Value vs Predicted Value:\\tDifference:\")\ndiff = []\nfor a, b in zip(df.charges, predictions):\n    a = np.exp(a)\n    b = np.exp(b)\n    diff.append(a-b[0])\n    print(\"%.2f \\t %.2f \\t\\t %.2f\" %(a, b[0], a-b[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum Deviation: %.2f\" %max(abs(x) for x in diff))\nprint(\"Minimum Deviation: %.2f\" %min(abs(x) for x in diff))\nprint(\"Average Deviation: %.2f\" %np.mean([abs(x) for x in diff]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End"},{"metadata":{"trusted":true},"cell_type":"code","source":"end_time = time.time()\ntotal_time = end_time - start_time\nh = total_time//3600\nm = (total_time%3600)//60\ns = total_time%60\nprint(\"Total Time: %i hours, %i minutes and %i seconds.\" %(h, m, s))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}