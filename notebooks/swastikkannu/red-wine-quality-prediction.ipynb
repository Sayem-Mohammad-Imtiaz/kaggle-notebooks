{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Red Wine Quality"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read CSV file"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for the null value\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see there is not null value in the dataset. So we can proceed with the same data"},{"metadata":{},"cell_type":"markdown","source":"- quality > 6.5 => \"good\"\n- quality < 6.5 => \"bad\"\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = (2,6.5,8)\nlabels = ['bad','good']\ndf['quality'] = pd.cut(df['quality'],bins=bins,labels=labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"Let visualize each column in deep for better understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unistats(df):\n    output_df = pd.DataFrame(columns=['Count','Missing','NUnique','Unique','Dtype', 'Numeric','Mode','Mean','Min','25%','Median','75%','Max','Std', 'Skew', 'Kurt'])\n\n\n    for col in df:\n        if pd.api.types.is_numeric_dtype(df[col]):\n            output_df.loc[col] = [df[col].count(), df[col].isnull().sum(), df[col].nunique(), df[col].unique(), df[col].dtype, pd.api.types.is_numeric_dtype(df[col]),\n                                  df[col].mode().values[0], df[col].mean(), df[col].min(), df[col].quantile(0.25), df[col].median(), df[col].quantile(0.75),\n                                  df[col].max(), df[col].std(),df[col].skew(), df[col].kurt()]\n        else:\n            output_df.loc[col] = [df[col].count(),df[col].isnull().sum(),df[col].nunique(), df[col].unique(), df[col].dtype, pd.api.types.is_numeric_dtype(df[col]),\n                                  df[col].mode().values[0],'','','','','','','','','']\n    return output_df.sort_values(by=['Numeric','Skew', 'NUnique'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unistats(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def univaritePlot(df,col,vartype):\n    if vartype==0:\n        sns.set(style=\"darkgrid\")\n        fig, ax=plt.subplots(nrows =1,ncols=2,figsize=(20,8))\n        ax[0].set_title(col.upper() + \" DISTRIBUTION PLOT\")\n        sns.distplot(df[col],ax=ax[0])\n        ax[1].set_title(col.upper() + \" BOX PLOT\")\n        sns.boxplot(data =df, x=col,ax=ax[1],orient='v')\n        plt.show()\n    if vartype==1:\n        fig, ax = plt.subplots()\n        fig.set_size_inches(len(df[col].unique())+10 , 7)\n        ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index) \n        for p in ax.patches:\n            percentage = '{:.1f}%'.format(100 * p.get_height()/len(df))\n            x = p.get_x() + p.get_width() / 2 - 0.05\n            y = p.get_y() + p.get_height()\n            ax.annotate(percentage, (x, y), size = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univaritePlot(df=df,col='quality',vartype=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univaritePlot(df=df,col='chlorides',vartype=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univaritePlot(df=df,col='density',vartype=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univaritePlot(df=df,col='residual sugar',vartype=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the correlation between variables\ndf.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(df,hue='quality')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the class bad as 0 and good as a 1\ndf['quality'] = df['quality'].map({'bad':0, 'good':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['quality']\nX = df.drop(['quality'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into train and test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape : \", X_test.shape)\n\ny_train_imb = (y_train != 0).sum()/(y_train == 0).sum()\ny_test_imb = (y_test != 0).sum()/(y_test == 0).sum()\nprint(\"Imbalance in Train Data : \", y_train_imb)\nprint(\"Imbalance in Test Data : \", y_test_imb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see the data class is imbalance so we need to handle this with using SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balancing DataSet\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_train,y_train = sm.fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train Shape\", X_train.shape)\nprint(\"y_train Shape\", y_train.shape)\n\nimb = (y_train != 0).sum()/(y_train == 0).sum()\nprint(\"Imbalance in Train Data : \",imb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom imblearn.metrics import sensitivity_specificity_support\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\n\n\nlr.fit(X_train,y_train)\npreds = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Accuracy Score:\",accuracy_score(preds,y_test))\nprint(\"classification Report:\\n\",classification_report(preds,y_test))\nprint(\"confusion Matrix:\\n\",confusion_matrix(preds,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity, specificity, _ = sensitivity_specificity_support(y_test, preds, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, preds),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc= SVC()\nsvc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds1= svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score:\",accuracy_score(preds1,y_test))\nprint(\"classification Report:\\n\",classification_report(preds1,y_test))\nprint(\"confusion Matrix:\\n\",confusion_matrix(preds1,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity, specificity, _ = sensitivity_specificity_support(y_test, preds1, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, preds1),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# random forest - the class weight is used to handle class imbalance - it adjusts the cost function\nforest = RandomForestClassifier(class_weight={0:0.1, 1: 0.9}, n_jobs = -1)\n\n# hyperparameter space\nparams = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n\n# create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# create gridsearch object\nmodel = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict churn on test data\ny_pred = model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy Score:\",accuracy_score(y_pred,y_test))\n\nprint(\"classification Report:\\n\",classification_report(y_pred,y_test))\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Please Upvote "},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}