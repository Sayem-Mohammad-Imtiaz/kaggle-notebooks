{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Attack Prediction with Classifier Algorithms\n\nHeart attack is a serious disease. It can be caused death. Some features (age,anaemia,creatinine_phosphokinase,diabetes,high_blood_pressure etc.) can trigger heary attack. We can see, whether a person has a heart attack or doesn't have, looking this features. In this notebook I developed a model that predicted the number of death because of hearth attack. I used to various metrics in order to can find true model.\n\n## CONTENT\n\n[1.Exploratory Data Analysis](#1) <br/>\n[2.Train And Test Split](#2) <br/>\n[3.Create Model](#3) <br/>\n    [3.1.Logistic Regression](#3.1) <br/>\n    [3.2.K Nearest Neighbors](#3.2) <br/>\n    [3.3.Support Vector Machine](#3.3) <br/>\n    [3.4.Native Bayes](#3.4) <br/>\n    [3.5.Decision Tree Classifier](#3.5) <br/>\n    [3.6.Random Forest Classifier](#3.6) <br/>\n    [3.7.Gradient Boosting Classifier](#3.7) <br/>\n    [3.8.XG Boosting Classifier](#3.8) <br/>\n[4.Evaluation Models](#4) <br/>\n[5.Conclusion](#5) <br/>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\n# Model Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n## Exploratory Data Analysis","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Before the model doesn't set up, We let look features.**\n\nIn this dataset:\n\n* It contains 299 rows (patient information).\n* It contains 13 columns (features about heart attack).\n* 10 features are integer type.\n* 3 features are float type.\n* It doesn't have missing value.\n\n### **FEATURES**\n\n**age:** age of patient <br/>\n**anaemia:** Decrease of red blood cells or hemoglobin <br/>\n**creatinine_phosphokinase:** Level of the CPK enzyme in the blood (mcg/L) <br/>\n**diabetes:** If the patient has diabetes <br/>\n**ejection_fraction:** Percentage of blood leaving the heart at each contraction (percentage) <br/>\n**high_blood_pressure:** If the patient has hypertension \n**platelets:** Platelets in the blood                \n**serum_creatinine:** Level of serum creatinine in the blood (mg/dL)         \n**serum_sodium:** Level of serum sodium in the blood (mEq/L)              \n**sex:** Woman or man (binary)                       \n**smoking:** If the patient smokes or not                   \n**time:** Follow-up period (days)                     \n**DEATH_EVENT:** If the patient deceased during the follow-up period ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data.corr(),annot=True, linewidths=.5, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above correlation matrix, we see features relationship each other. This relationships can be useful to set up model. If the relationship how is close and is strong, it can be impact to use them in order to set up true model. In this dataset, we will look relationship with death_evet other features. If relationship between them is big from 0.1, This features can be important features,which heart attack triggers. While my model set up, I will use features,which correlation coffience is big from 0.1.   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = data.corr() \ncorr_target = abs(cor[\"DEATH_EVENT\"])\nrelevant_features = corr_target[corr_target>0.1]\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n## Train and Test Split\n\nI splitted as 20% test dataset and 80% train dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_list = []\nalgorithm = []\npredict_list = []\n\nX_train,X_test,Y_train,Y_test = train_test_split(data.loc[:,{\"age\",\"ejection_fraction\",\"serum_creatinine\",\"serum_sodium\",\"time\"}]\n                                                 ,data[\"DEATH_EVENT\"],test_size=0.2)\nprint(\"X_train shape :\",X_train.shape)\nprint(\"Y_train shape :\",Y_train.shape)\nprint(\"X_test shape :\",X_test.shape)\nprint(\"Y_test shape :\",Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n## Create Model\n\nIn this section, accuracy values and the number of patient, which predicted, of the models,which I set up,is seen. Accuracy value is not enough in order to set up actual model. Sometimes, low accuracy value models can predict more actual result than high accuracy value. You see all models comparisons in below. The algorithm I have used:\n\n* Logistic Regression\n* K Nearest Neighbors \n* Support Vector Machine\n* Native Bayes \n* Decision Tree Classifier\n* Random Forest Classifier\n* Gradient Boosting Classifier\n* XG Boosting Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LogisticRegression(max_iter=1000)\nreg.fit(X_train,Y_train)\naccuracy_list.append(reg.score(X_test,Y_test))\nalgorithm.append(\"Logistic Regression\")\nprint(\"test accuracy \",reg.score(X_test,Y_test))\n\ncm = confusion_matrix(Y_test,reg.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n### K Nearest Neighbors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors': np.arange(1, 25)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=4)\nknn_gscv.fit(X_train, Y_train)\nprint(\"Best K Value is \",knn_gscv.best_params_)\n\naccuracy_list.append(knn_gscv.score(X_test,Y_test))\nprint(\"test accuracy \",knn_gscv.score(X_test,Y_test))\nalgorithm.append(\"K Nearest Neighbors Classifier\")\n\ncm = confusion_matrix(Y_test,knn_gscv.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n### Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC()\nsvm.fit(X_train,Y_train)\nprint(\"test accuracy: \",svm.score(X_test,Y_test))\naccuracy_list.append(svm.score(X_test,Y_test))\nalgorithm.append(\"Support Vector Machine\")\n\ncm = confusion_matrix(Y_test,svm.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n### Native Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train,Y_train)\nprint(\"test accuracy: \",nb.score(X_test,Y_test))\naccuracy_list.append(nb.score(X_test,Y_test))\nalgorithm.append(\"Native Bayes Classifier\")\n\ncm = confusion_matrix(Y_test,nb.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.5\"></a>\n### Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier()\ndt.fit(X_train,Y_train)\nprint(\"test accuracy: \",dt.score(X_test,Y_test))\naccuracy_list.append(dt.score(X_test,Y_test))\nalgorithm.append(\"Decision Tree Classifier\")\n\ncm = confusion_matrix(Y_test,dt.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.6\"></a>\n### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': np.arange(10, 100, 10)}\nrf = RandomForestClassifier(random_state = 42)\nrf_gscv = GridSearchCV(rf, param_grid, cv=4)\nrf_gscv.fit(X_train, Y_train)\nprint(\"Best K Value is \",rf_gscv.best_params_)\n\nprint(\"test accuracy: \",rf_gscv.score(X_test,Y_test))\naccuracy_list.append(rf_gscv.score(X_test,Y_test))\nalgorithm.append(\"Random Forest Classifier\")\n\ncm = confusion_matrix(Y_test,rf_gscv.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.7\"></a>\n### Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': [10,20,50],'learning_rate': [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1],'max_features': [2],'max_depth': [2]}\ngb = GradientBoostingClassifier()\ngb_gscv = GridSearchCV(gb, param_grid, cv=4)\ngb_gscv.fit(X_train,Y_train)\nprint(\"The best parameters are \",gb_gscv.best_params_)\nprint(\"------------------------------------------------------\")\nprint(\"test accuracy is \",gb_gscv.score(X_test,Y_test))\naccuracy_list.append(gb_gscv.score(X_test,Y_test))\nalgorithm.append(\"Gradient Boosting Classifier\")\n\ncm = confusion_matrix(Y_test,gb_gscv.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.8\"></a>\n### XGBoosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, Y_train)\nprint(\"test accuracy is \",xgb_clf.score(X_test,Y_test))\naccuracy_list.append(xgb_clf.score(X_test,Y_test))\nalgorithm.append(\"XGBClassifier\")\n\ncm = confusion_matrix(Y_test,xgb_clf.predict(X_test))\npredict_list.append(cm.item(0)+cm.item(2))\nsns.heatmap(cm,annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n## Evaluation Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classifier Accuracy\nf,ax = plt.subplots(figsize = (15,7))\nsns.barplot(x=accuracy_list,y=algorithm,palette = sns.cubehelix_palette(len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Classifier\")\nplt.title('Classifier Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classifier Predict Death Event Count\nf,ax = plt.subplots(figsize = (15,7))\nsns.barplot(x=predict_list,y=algorithm,palette = sns.cubehelix_palette(len(accuracy_list)))\nplt.xlabel(\"Predict Death Event Count\")\nplt.ylabel(\"Classifier\")\nplt.title('Classifier Predict Death Event Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n## Conclusion\n\n* Correlation coffience is effective in classification problems.\n* The models give actual result to use features that have high correlation coffience.\n* It isn't enough to look accuracy value in order to choose model. It needs to apply other metrics. Especially, the number of class, which predicted true, is the important metric. Sometimes, although lower accuracy value can give more accurate predictions. \n* For example Support Vector Machine, even though it has lower accuracy value, it gave more accurate prediction according to other algorithms.\n* If my notebook is benefit for you, please don't forget to upvote. :))","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}