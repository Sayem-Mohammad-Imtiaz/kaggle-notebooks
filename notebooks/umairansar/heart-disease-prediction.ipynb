{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Disease Classification using Logistic Regression\n## We use logistic regression for this problem because we don't just need class labels but also the confidence percentage behind those predicted class labels. Our current problem exactly fits these needs.","execution_count":null},{"metadata":{"_uuid":"0bea601e-d063-40a8-b337-4fe800cd41d1","_cell_guid":"33114a94-9af7-40f8-8844-e2ea6860026a","trusted":true},"cell_type":"code","source":"# Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pylab as pl\nimport matplotlib.pyplot as plt\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, log_loss\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\n\nfrom collections import Counter\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the dataset","execution_count":null},{"metadata":{"_uuid":"2ffc91e6-e73f-4295-bcd2-090933a88125","_cell_guid":"2e3d36a2-1e5d-4c7a-b256-6258ab284f60","trusted":true},"cell_type":"code","source":"rpt = pd.read_csv('../input/heart-disease-prediction-using-logistic-regression/framingham.csv')\nrpt[20:41]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"_uuid":"df9b64a2-3501-4781-9c27-ecf042555581","_cell_guid":"840bad09-9881-421e-a832-d8e5ba64dfaf","trusted":true},"cell_type":"code","source":"# Checkig datatypes of each column and shape\nprint(rpt.dtypes, '\\n')\nprint(\"Shape: \", rpt.shape)\n\n# Checking for NaN and correcting\nprint(\"NaN exists: \", rpt.isnull().values.any())\nprint(\"NaN count: \", rpt.isnull().values.sum())\nprint(\"NaN count for each attriue:\")\nprint(rpt.isnull().sum())\nprint(\"\\n\")\n\n# Dropping all rows with any NaN\nrpt.dropna(axis = 0, inplace = True)\nprint(\"New Shape after dropping rows: \", rpt.shape)\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Depictong Correlations\nrpt.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X and y\nX = np.asarray(rpt[['male', 'age', 'cigsPerDay', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']])\ny = np.asarray(rpt['TenYearCHD']) #do not do [['TenYearCHD']] it will give shape (2924,1) instead of needed (2924,)\n\n# Normalizing X\nX = preprocessing.StandardScaler().fit_transform(X)\nprint(\"Shape X:\", X.shape)\nprint(\"Shape y:\", y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualizeIn2D(X, y):\n    '''\n    Visualizing Data in 2 dim using features: sysBP, diaBP. \n    '''\n    # Class labels in a dict\n    counter = Counter(y)\n    print(counter)\n    \n    plt.figure(figsize = (15, 10))\n    for label, _ in counter.items():\n        row = np.where(y == label)[0]\n        plt.scatter(X[row, 0], X[row, 1], label=str(label))\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating number of classes\nNumOfClasses = len(rpt.groupby('TenYearCHD').size().values)\n\n#Creating new X1 because X is already ndarray and normalized\nX1 = np.asarray(rpt[['sysBP', 'diaBP']])\n\n# Visualize\nvisualizeIn2D(X1, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Logistic Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def Ktimes_train_LR(LR, kf, X, y):\n    '''\n    Since we are not using train_test_split where say 80% was used for training and 20% was used for testing,\n    in KFold our training is done on k-1 sets and testing is done on 1 set. So if n_sets equals 5, then 4 sets\n    (or 80%) are used for training and 1 set (or 20%) is used for testing.\n    '''\n    scores = list()\n    \n    for train_index, test_index in kf.split(X):\n        \n        # Splitting Data into train test\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # Train\n        LR.fit(X_train, y_train)\n        \n        # Predicting\n        y_hat = LR.predict(X_test)\n        y_hat_prob = LR.predict_proba(X_test)\n        \n        # Scoring\n        scores.append(accuracy_score(y_test, y_hat))\n        \n    '''\n    The y_hat and y_hat_proba returned below hold the values from the \"LAST\" iteration for KFold training.\n    '''\n    report = (X_train, X_test, y_train, y_test, y_hat, y_hat_prob, scores)\n    return report\n\n\n# Init the main Logistic model - the lesser the C, the greater the regularization \nLR = LogisticRegression(C = 0.0000001, solver = 'liblinear')\n\n# Init KF\nkf = KFold(n_splits = 5, random_state = 7)\n\n# Call for K times training and predicting\n(X_train, X_test, y_train, y_test, y_hat, y_hat_prob, scores) = Ktimes_train_LR(LR, kf, X, y)\n\n# Priting the jaccard_scores\nprint(\"Scores: \", scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nThe above block of code and these 3 lines of code below do exactly the same work, except that the we do not\nhave access to (or do not need) X_train, X_test, y_train, y_test, y_hat, y_hat_prob. We just get back scores.\nOf course, we can always split the data using other functions like ShuffleSplit or train_test_split to get\naccess of X_train, X_test and so on.\n'''\nkfold = KFold(n_splits = 5, random_state = 7)\ncv_result = cross_val_score(LR, X, y, cv = kfold, scoring = 'accuracy')\nprint(cv_result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the confusion matrix\nconfusion_matrix(y_test, y_hat, labels = [1, 0])\nplot_confusion_matrix(LR, X_test, y_test, labels = [1, 0], cmap=plt.cm.Blues) #test acc\nplot_confusion_matrix(LR, X_train, y_train, labels = [1, 0], cmap=plt.cm.Blues) #train acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report\nprint(\"\\t\\t\\t *TEST REPORT*\")\nprint(classification_report(y_test, y_hat)) #test acc\nprint('\\n')\nprint(\"\\t\\t\\t *TRAIN REPORT*\")\nprint(classification_report(y_train, LR.predict(X_train))) #train acc\n\n# Log-loss\nprint(\"LogLoss: \", log_loss(y_hat, y_hat_prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since the logloss error is high (i.e. close to 69%) and f1_scores of accuracy for label 1 are very low (i.e. close to 30% compared to label 0's 88%), we need to do some more preprocessing on the dataset. The technique we will use is called SMOTE (discussed later).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Label Imballance\nprint(\"Label Imballance:\")\nprint(rpt.groupby('TenYearCHD').size())\nprint(\"\\n\")\n\n# Checking Gender Discrepancy \nprint(\"Gender Distribution:\")\nprint(rpt.groupby('male').size())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It seems that our data has a lot of label imballance, cases for label 0 are 3000 vs 550 for label 1. This means that our model learns features of label 0 too well but for label 1 not so much. So what's the solution? \n\n### We can do many things. \n\n1. ### Add synthetic cases for mnority class to our dataset\n2. ### Reduce size of minority class by throwing out some cases\n3. ### Do both task 1 and 2 *(SMOTE Technique)*\n4. ### Since we are using Logistic Regression, change the threshold for y_hat_prob","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# How Smote Works?\n### SMOTE picks K-Nearest Neighbours (usually 5) at random in the feature space and draws a line between any two points from KNNs. Finally it picks, a new sample at a point along that line.\n\n>  *The simplest approach involves duplicating examples in the minority class, although these examples donâ€™t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short.\" ~[Jason Brownlee](http://https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Applying SMOTE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Additional Libraries\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining Pipeline\nover = SMOTE(sampling_strategy = 0.4, k_neighbors = 3, random_state = 7)\nunder = RandomUnderSampler(sampling_strategy = .55)\nsteps = [('o', over), ('u', under), ('model', LR)]\npipe1 = Pipeline(steps = steps)\n\n# Training and Evaluating\ncv_result = cross_val_score(pipe1, X, y, cv = kfold, scoring = 'accuracy')\nprint(\"Mean Accuracy: \", np.mean(cv_result))\nprint('\\n')\n\npipe = Pipeline(steps =[('o', over), ('u', under)])\nX2, y1 = pipe.fit_resample(X1, y)\ncounter = Counter(y1)\nprint(counter)\n\n# Call for K times training and predicting\n(X_train, X_test, y_train, y_test, y_hat, y_hat_prob, scores) = Ktimes_train_LR(pipe1, kf, X, y)\nplot_confusion_matrix(pipe1, X_test, y_test, labels = [1, 0], cmap=plt.cm.Blues) #test acc\nvisualizeIn2D(X2, y1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_hat)) #test acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the new report above, it is clear that:\n* ### Our f1_score for true postive (1) has increased from 0.31 to 0.35. \n* ### This has come at a cost of compromizing f1_Score for true negative (0) from 0.88 to 0.78\n* ### But our True Positive is still very poor, we care for it more than True Negartive Rate. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tinkering with threshold value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import binarize\n\ncm2 = 0\ny_hat2 = binarize(y_hat_prob,0.4999856)[:,1]\ncm2 = confusion_matrix(y_test,y_hat2)\nprint ('With',0.4999856,'threshold the Confusion Matrix is ','\\n',cm2,)\nprint(classification_report(y_test, y_hat2)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the new report, it is clear that:\n* ### Our f1_score for true postive (1) has further increased from 0.35 to 0.38. \n* ### We need more data to improve this result.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, _ = roc_curve(y_test, y_hat_prob[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('Heart Disease Predictor (ROC curve)')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Area under curve: \", roc_auc_score(y_test,y_hat_prob[:,1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finally, our Area Under Curve (AUC) shows that if we want to increase \"true positie rate\" (detecting patients who truly have a heart disease) we also end having large \"false positive rate\" which is problematic (many more patients who are actually healthy end up getting diagnosed with a heart disease). This scenario makes it infeasible for deployment in a real-world setting at the moment. More data and less label imballance can help here.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}