{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis and Machine Learning Classification on Heart Disease Prediction"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I classified the Heart Disease dataset with the DecisionTreeClassifier. The purpose is to convey what I know about the Decision Tree and Assessment Criteria. I used Seaborn library in Basic Data Analysis, Correlation and Data Visualization sections. I leave the inferences that can be made from the graphics to the reader. I focused on providing information and making applications about Decision Tree and Evaluation Metrics. Hope it will be useful to you."},{"metadata":{},"cell_type":"markdown","source":"### If you have questions please ask them on the comment section.\n\n### I will be glad if you can give feedback."},{"metadata":{},"cell_type":"markdown","source":"Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n1. [Basic Data Analysis](#4)\n   1. [age](#5)\n   1. [sex](#6)\n   1. [cp](#7)\n   1. [trestbps](#8)\n   1. [chol](#9)\n   1. [fbs](#10)\n   1. [restecg](#11)\n   1. [thalach](#12)\n   1. [exang](#13)\n   1. [oldpeak](#14)\n   1. [slope](#15)\n   1. [ca](#16)\n   1. [thal](#17)\n   1. [target](#18)\n1. [Correlation](#19)\n1. [Pandas Profiling](#20)\n1. [Data Visualization](#21)\n1. [Train-Test Split](#22)\n1. [Decision Tree Classifier](#23)\n   1. [Evaluation Metrics](#24)\n      1. [Confusion Matrix](#25)\n      1. [Classification Report](#26)\n      1. [ROC Curve](#27)\n   1. [Decision Tree Visualization](#28)\n      1. [Visualize Decision Tree with graphviz](#29)\n      1. [Print Text Representation](#30)\n   1. [k-Fold Cross Validation](#31)\n   1. [Hyper-Parameter Optimization](#32)\n      1. [GridSearchCV](#33)\n      1. [RandomizedSearchCV](#34)\n   1. [Feature Importance](#35)\n1. [Conclusion](#36)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> \n# Importing the Necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> \n# Read Datas & Explanation of Features & Information About Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pandas.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> \n## Variable Descriptions"},{"metadata":{},"cell_type":"markdown","source":"* age\n* sex\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg/dl\n* fasting blood sugar > 120 mg/dl\n* resting electrocardiographic results (values 0,1,2)\n* maximum heart rate achieved\n* exercise induced angina\n* oldpeak = ST depression induced by exercise relative to rest\n* the slope of the peak exercise ST segment\n* number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\nSource: https://www.kaggle.com/ronitf/heart-disease-uci"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> \n# Basic Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"At this stage, the aim is to obtain statistical information about the features. In addition, subjects are grouped according to their attributions and their distribution is desired to be seen. I used Pie Plot and Histograms for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_int64 = (dataset.dtypes == \"int64\")\nnumerical_int64_list = list(numerical_int64[numerical_int64].index)\n\nprint(\"Categorical variables:\")\nprint(numerical_int64_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(variable):\n    sns.set_style('darkgrid')\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in numerical_int64_list:\n    plot_hist(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> \n## Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"ticks\")\n\n\nsns.histplot(\n    dataset,\n    x=\"age\", hue=\"target\",\n    multiple=\"stack\",\n    palette=\"light:m_r\",\n    edgecolor=\".3\",\n    linewidth=.9,\n    #log_scale=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> \n## Sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"sex\",\"target\"]].groupby([\"sex\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code snippet taken from Bharti Prasad's notebook via https://www.kaggle.com/bhartiprasad17/student-academic-performance-analysis (with her permission)\n\nplt.figure(figsize=(14, 7))\nlabels=['Male', 'Female']\nplt.pie(dataset['sex'].value_counts(),labels=labels,explode=[0.1,0.1],\n        autopct='%1.2f%%',colors=['#E37383','#FFC0CB'], startangle=90)\nplt.title('Gender')\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> \n## Cp"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"cp\",\"target\"]].groupby([\"cp\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"ticks\")\n\nsns.set_style('darkgrid')\nsns.histplot(\n    dataset,\n    x=\"cp\", hue=\"target\",\n    multiple=\"stack\",\n    palette=\"gist_rainbow\",\n    edgecolor=\".3\",\n    linewidth=.9,\n    #log_scale=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> \n## Trestbps"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"ticks\")\n\nsns.set_style('darkgrid')\nsns.histplot(\n    dataset,\n    x=\"trestbps\", hue=\"target\",\n    multiple=\"stack\",\n    palette=\"prism\",\n    edgecolor=\".3\",\n    linewidth=.9,\n    #log_scale=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> \n## Chol"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\", palette=\"pastel\")\nsns.boxplot(x=\"target\", y=\"chol\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> \n## Fbs"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"fbs\",\"target\"]].groupby([\"fbs\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"fbs\",\"target\"]].groupby([\"fbs\"], as_index = False).count().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"ticks\")\n\nsns.set_style('darkgrid')\nsns.histplot(\n    dataset,\n    x=\"fbs\", hue=\"target\",\n    multiple=\"stack\",\n    palette=\"OrRd\",\n    edgecolor=\".3\",\n    linewidth=.9,\n    #log_scale=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a> \n## Restecg"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"restecg\",\"target\"]].groupby([\"restecg\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['restecg'].value_counts().index\nsizes = dataset['restecg'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'restecg'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> \n## Thalach"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"thalach\",\"target\"]].groupby([\"thalach\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a> \n## Exang"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"exang\",\"target\"]].groupby([\"exang\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['exang'].value_counts().index\nsizes = dataset['exang'].value_counts().values\nsns.set_theme(style=\"darkgrid\", palette=\"pastel\")\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'exang'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a> \n## Oldpeak"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"ticks\")\n\nsns.set_style('darkgrid')\nsns.histplot(\n    dataset,\n    x=\"oldpeak\", hue=\"target\",\n    multiple=\"stack\",\n    palette=\"OrRd\",\n    edgecolor=\".3\",\n    linewidth=.9,\n    #log_scale=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a> \n## Slope"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"slope\",\"target\"]].groupby([\"slope\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['slope'].value_counts().index\nsizes = dataset['slope'].value_counts().values\nsns.set_theme(style=\"darkgrid\", palette=\"pastel\")\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'slope'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a> \n## Ca"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"ca\",\"target\"]].groupby([\"ca\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['ca'].value_counts().index\nsizes = dataset['ca'].value_counts().values\nsns.set_theme(style=\"darkgrid\", palette=\"pastel\")\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'ca'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a> \n## Thal"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"thal\",\"target\"]].groupby([\"thal\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['thal'].value_counts().index\nsizes = dataset['thal'].value_counts().values\nsns.set_theme(style=\"darkgrid\", palette=\"pastel\")\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'thal'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a> \n## Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['target'].value_counts().index\nsizes = dataset['target'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'target'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"19\"></a> \n# Correlation"},{"metadata":{},"cell_type":"markdown","source":"Correlation specifies the direction and strength of the linear relationship between two random variables in correlation, probability theory and statistics. In general statistical use, correlation shows how far the state of independence has been moved away."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heat maps visualize data with color changes. When applied to the table format, its variables are placed in rows and columns. Coloring the boxes in the table is useful for examining multivariate crosstab data. Heat maps are good for showing more than one variable, showing any patterns, or showing if any variables are alike, and detecting whether there is any correlation between them."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.set_style('white')\nmask = np.triu(np.ones_like(dataset.corr(), dtype=np.bool))\nheatmap = sns.heatmap(dataset.corr(), mask=mask,annot=True, cmap='BrBG', linewidths = 2)\nheatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':30}, pad=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nheatmap = sns.heatmap(dataset.corr(),  annot=True, cmap='Blues_r', linewidths = 2)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':30}, pad=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"20\"></a> \n# Pandas Profiling"},{"metadata":{},"cell_type":"markdown","source":"Pandas profiling is a useful library that generates interactive reports about the data. With using this library, we can see types of data, distribution of data and various statistical information. This tool has many features for data preparing. Pandas Profiling includes graphics about specific feature and correlation maps too. You can see more details about this tool in the following url: https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"21\"></a> \n# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(6.5, 6.5))\nsns.despine(f, left=True, bottom=True)\nsns.set_theme(style=\"whitegrid\")\nsns.scatterplot(x=dataset['trestbps'], y=dataset['chol'],\n                hue=dataset['slope'], \n                size=\"sex\",\n                palette=\"tab20\",\n                hue_order=dataset['slope'],\n                sizes=(2,16), \n                linewidth=0,\n                data=dataset, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.JointGrid(data=dataset, x=\"trestbps\", y=\"chol\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"r\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"r\", alpha=.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.JointGrid(data=dataset, x=\"trestbps\", y=\"thalach\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"g\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"g\", alpha=.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\nsns.set_theme(style=\"darkgrid\")\n\nplt.subplot(2,2,1)\nsns.histplot(dataset['age'], color = 'red', kde = True).set_title('age Interval and Counts')\n\nplt.subplot(2,2,2)\nsns.histplot(dataset['trestbps'], color = 'green', kde = True).set_title('trestbps Interval and Counts')\n\nplt.subplot(2,2,3)\nsns.histplot(dataset['chol'], kde = True, color = 'blue').set_title('chol Interval and Counts')\n\nplt.subplot(2,2,4)\nsns.histplot(dataset['thalach'], kde = True, color = 'black').set_title('thalach Interval and Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\")\nsns.pairplot(dataset, hue = 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"22\"></a> \n# Train - Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:,0:13].values \ny = dataset.iloc[:,13:].values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardization is a method in which the mean value is 0 and the standard deviation is 1, and the distribution approaches the normal. The formula is as follows, we subtract the average value from the value we have, then divide it by the variance value."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"23\"></a> \n# Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"Tree-based learning algorithms are among the most used supervised learning algorithms. In general, they can be adapted to the solution of problems such as classification and regression.\n\nDecision tree algorithm is one of the data mining classification algorithms. They have a predefined target variable. They offer a strategy from the top to the bottom due to their structure.\n\n**A decision tree is a structure used to divide a data set containing a large number of records into smaller sets by applying a set of decision rules. In other words, it is a structure that is used by applying simple decision-making steps, dividing large amounts of records into very small groups of records.**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"decTree_model = DecisionTreeClassifier()\ndecTree_model.fit(X_train, y_train)\n\ntrain_score2 = decTree_model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score2}')\n\nvalidation_score2 = decTree_model.score(X_valid, y_valid)\nprint(f'Validation score of trained model: {validation_score2}')\n\ntest_score2 = decTree_model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score2}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Advantages of Decision Trees:**\n\n* It is easy to understand and interpret.\n\n* It requires little data preparation.\n\n* The cost of the tree used is logarithmic with the number of data points used to train the tree.\n\n* It can process both numerical and categorical data.\n\n* They can handle multi-output problems.\n\n* It is possible to validate a model using statistical tests."},{"metadata":{},"cell_type":"markdown","source":"**Disadvantages of Decision Trees:**\n\n* Not very good at estimating persistent attribute values.\n\n* Modeling is not very successful when the number of classes is high and the number of learning cluster examples is low.\n\n* Time and place complexity depends on the number of learning set instances, the number of attributes and the structure of the resulting tree.\n\n* Both tree forming complexity and tree pruning complexity are high."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"24\"></a> \n## Evaluation Metrics"},{"metadata":{},"cell_type":"markdown","source":"Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model. These include classification accuracy, logarithmic loss, confusion matrix, and others. \n\n***Why is this Useful?***\n\nIt is very important to use multiple evaluation metrics to evaluate your model. This is because a model may perform well using one measurement from one evaluation metric, but may perform poorly using another measurement from another evaluation metric. Using evaluation metrics are critical in ensuring that your model is operating correctly and optimally. \n\n***Applications of Evaluation Metrics***\n* Statistical Analysis\n\n* Machine Learning\n\nSource: https://deepai.org/machine-learning-glossary-and-terms/evaluation-metrics"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"25\"></a> \n### Confusion Matrix"},{"metadata":{},"cell_type":"markdown","source":"Complexity matrix is a measurement tool that provides information about the accuracy of predictions.\nThe logic behind it is actually simple, but it is often used especially in classification algorithms as it provides easy to understand information about the accuracy of the measurement."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predictions = decTree_model.predict(X_test)\n\nconf_matrix = confusion_matrix(y_predictions, y_test)\n\n\nprint(f'Accuracy: {accuracy_score(y_predictions, y_test)*100}')\nprint()\nprint(f'Confussion matrix: \\n{conf_matrix}\\n')\n\nsns.heatmap(conf_matrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TP - True Positive:** The model correctly predicted the positive class as a positive class.\n\n**FP - False Positive:** The model predicted the negative class as a false positive class.\n\n**FN - False Negative:** The model predicted the positive class as false, negative class.\n\n**TN - True Negative:** The model predicted the negative class correctly."},{"metadata":{"trusted":true},"cell_type":"code","source":"tn = conf_matrix[0,0]\nfp = conf_matrix[0,1]\ntp = conf_matrix[1,1]\nfn = conf_matrix[1,0]\n\ntotal = tn + fp + tp + fn\nreal_positive = tp + fn\nreal_negative = tn + fp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy Rate:** A measure of how often the classifier predicts correctly.\n\n**Precision:** It shows how many of the values we guess as Positive are actually Positive.\n\n**Recall:** It is a measure of how much the classifier correctly predicts the true positive value. Also known as Sensitivity, Accuracy or Recall. (Sensitivity, Hit Rate or Recall) It should be as high as possible.\n\n**F1 Score:** F1 Score value shows the harmonic mean of Precision and Recall values. The reason why it is a harmonic average instead of a simple average is that we should not ignore extreme cases. If there was a simple average calculation, the F1 Score of a model with a Precision value of 1 and a Recall value of 0 would come as 0.5, and this would mislead us.\n\n**Specificity:** It is a measure of how much the classifier correctly predicted the true negative value.\n\n**Misclassification Rate (Error Rate):** It is a measure of how often the classifier guesses incorrectly. Also known as Error Rate.\n\n**Prevalence:** It is the measure of how often a value of 1 is found at the end of the estimation.\n\n**Miss Rate:** It is the ratio of those predicted to be 0 despite the real value being 1. Also known as loss rate.\n\n**Fall out:** It is the ratio of those predicted to be 1 even though the real value is 0. "},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy  = (tp + tn) / total # Accuracy Rate\nprecision = tp / (tp + fp) # Positive Predictive Value\nrecall    = tp / (tp + fn) # True Positive Rate\nf1score  = 2 * precision * recall / (precision + recall)\nspecificity = tn / (tn + fp) # True Negative Rate\nerror_rate = (fp + fn) / total # Missclassification Rate\nprevalence = real_positive / total\nmiss_rate = fn / real_positive # False Negative Rate\nfall_out = fp / real_negative # False Positive Rate\n\nprint(f'Accuracy    : {accuracy*100}')\nprint(f'Precision   : {precision*100}')\nprint(f'Recall      : {recall*100}')\nprint(f'F1 score    : {f1score*100}')\nprint(f'Specificity : {specificity*100}')\nprint(f'Error Rate  : {error_rate*100}')\nprint(f'Prevalence  : {prevalence*100}')\nprint(f'Miss Rate   : {miss_rate*100}')\nprint(f'Fall Out    : {fall_out*100}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"26\"></a> \n### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = decTree_model.predict(X_test)\n\n\nprint(classification_report(predictions, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"27\"></a> \n### ROC Curve"},{"metadata":{},"cell_type":"markdown","source":"AUC - ROC curve is used to evaluate performance in machine learning and classification problems. It is one of the most important evaluation criteria to check the performance of any classification model. It is one of the most widely used metrics to evaluate the performance of machine learning algorithms, especially in cases where there are unstable data sets. This curve explains how well the model is at its prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC' )\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = decTree_model.predict_proba(X_test)\nprobs = probs[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the most commonly used metrics is the AUC-ROC curve. AUC stands for \"Area under the ROC Curve\". The scope of this area is AUC. The larger the area covered, the better the machine learning models at discriminating the classes given. The ideal value for AUC is 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(y_test, probs)\nprint('AUC: ', auc*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, probs)\nplt.legend(loc = 'lower right')\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"28\"></a> \n## Decision Tree Visualization"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"29\"></a> \n### Visualize Decision Tree with graphviz"},{"metadata":{"trusted":true},"cell_type":"code","source":"exported_tree = tree.export_graphviz(decTree_model,  \n                                     filled = True, rounded = True,  \n                                     special_characters = True) \ntree_plot = graphviz.Source(exported_tree) \ntree_plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"30\"></a> \n### Print Text Representation"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_representation = tree.export_text(decTree_model)\nprint(text_representation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"31\"></a> \n## k-Fold Cross Validation"},{"metadata":{},"cell_type":"markdown","source":"Cross Validation will enable us to see whether we are facing an overfitting problem and also to see the quality of our model. Thus, it will enable us to test the performance of our model before encountering high error rates in the test data set that we have not seen yet. It is a method that is frequently used because it is easy to apply."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cross_val_score(decTree_model, X = X_train, y = y_train, cv = 15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = cross_val_score(estimator = decTree_model, X = X_train, y = y_train, cv = 100)\nprint(\"Accuracy (mean):\", accuracies.mean()*100, \"%\")\nprint(\"std: \", accuracies.std()*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"32\"></a> \n## Hyper-Parameter Optimization"},{"metadata":{},"cell_type":"markdown","source":"Unlike parameters, hyperparameters are not learned during training the model. They are determined by the data scientist before the modeling phase. For example, KNN algorithm, which is one of the non-parametric classification algorithms, makes classification by looking at the nearest k neighbors to the desired value. Here, the k number (n_neighbors:) and the distance metric (metric:) to be used are the hyperparameters that should be specified by the data scientist before the modeling, which increases the performance of the model.\n\nHyperparameter optimization is the process of finding the most suitable hyperparameter combination according to the success metric specified for a machine learning algorithm.\n\nGiven that there are dozens of hyperparameters for a machine learning algorithm and dozens of values these hyperparameters can take, it's clear how difficult it will be to try all combinations one by one and pick the best combination. For this reason, different methods have been developed for hyperparameter optimization. GridSearcCV and RandomizedSearchCV are among these methods."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"33\"></a> \n### GridSearchCV\n\nFor the hyperparameters and their values that are desired to be tested in the model, a separate model is established with all combinations and the most successful hyperparameter set is determined according to the specified metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'criterion': ['gini', 'entropy'],\n              'splitter': ['best', 'random'],\n              'max_depth': range(1,14), \n              'min_samples_split': range(2,8), \n              'min_samples_leaf': range(1,3),\n             'max_features': ['auto', 'sqrt', 'log2'],\n             }\n\ngcv = GridSearchCV(decTree_model, parameters, cv=10).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Best Estimator: {gcv.best_estimator_}\")\nprint(f\"Best Parameter: {gcv.best_params_}\")\nprint(f\"Best Score: {gcv.best_score_}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"34\"></a> \n### RandomizedSearchCV\n\nA set of hyperparameters is randomly selected and tested by cross-validation and the model set up. These steps continue until the specified calculation time limit or the number of iterations is reached."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nparams = {'criterion': ['gini', 'entropy'],\n              'splitter': ['best', 'random'],\n              'max_depth': range(1,14), \n              'min_samples_split': range(2,8), \n              'min_samples_leaf': range(1,3),\n             'max_features': ['auto', 'sqrt', 'log2'],\n             }\n\nrandomizedcv = RandomizedSearchCV(decTree_model, params, n_iter=1000, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\n\nprint(f'RandomizedSearchCV Best Score: {randomizedcv.best_score_*100}')\nprint(f'RandomizedSearchCV Best Estimator: {randomizedcv.best_estimator_}')\nprint(f'RandomizedSearchCV Best Params: {randomizedcv.best_params_}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"35\"></a> \n## Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_feature = pd.DataFrame({'Feature': ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal'], 'Importance': decTree_model.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"Feature Importance for DecisionTreeClassifier\")\nplt.xlabel(\"Importance \")\nplt.ylabel(\"Features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'rgbkymc')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features = SelectFromModel(decTree_model)\nbest_features.fit(X_train, y_train)\n\ntransformedX = best_features.transform(X_train)\nprint(f\"Old Shape: {X_train.shape} New shape: {transformedX.shape}\")\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"36\"></a> \n# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I examined Heart Disease Dataset. Firstly, I made Exploratory Data Analysis, Visualization, then I applied Desicion Tree Classifiet to this dataset. I visualized Decision Tree and give some explanation and examples about evaluation metrics. I performed k-Fold Cross Validation and GridSearchView. Lastly I showed Feature Importance Graphic.\n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, I will be glad to be informed :)\n\nThank you for your time."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}