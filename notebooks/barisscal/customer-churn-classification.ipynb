{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis and Machine Learning Classification on Customer Churn\n\nIn this notebook, I performed EDA on the 'Customer Churn Dataset'. I visualized the data using the Seaborn library. I created pipelines with Machine Learning algorithms. I applied k-Fold Cross Validation each of them and evaluated their results. Lastly, I determined best features for some algorithms. I hope this notebook will be useful to you.\n\n### If you have questions please ask them on the comment section.\n\n### I will be glad if you can give feedback.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n   1. [Univariate Variable Analysis](#4)\n      1. [Categorical Variables](#5)\n      1. [Numerical Variables](#6)\n1. [Basic Data Analysis](#7)\n   1. [Distribution of Each Feature](#8)\n   1. [Distributions of Each Feature According to 'Churn'](#9)\n1. [Data Visualization](#10)   \n1. [Pandas Profiling](#11)\n1. [Correlation](#12)\n1. [Skewness](#13)\n1. [Encoding](#14)\n   1. [Uniqueness of each Feature](#22)\n   1. [Label Encoding](#15)\n   1. [One-Hot Encoding](#16)\n1. [Train-Test Split](#17)\n1. [Pipelines](#18)\n   1. [k-Fold Cross Validation](#19)\n   1. [Best Features Selection](#20)\n1. [Gradient Boosting Classifier](#23)\n   1. [Confusion Matrix](#24)\n   1. [Classification Report](#25)\n   1. [ROC Curve](#26)\n   1. [Visualization](#27)\n1. [Conclusion](#21)      ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# Importing the Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nimport lightgbm\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n\n# Read Datas & Explanation of Features & Information About Datasets","metadata":{}},{"cell_type":"code","source":"dataset = pandas.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndataset.head(490)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.drop(\"customerID\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['TotalCharges'] = dataset['TotalCharges'].apply(lambda x: 0 if x == ' ' else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[\"TotalCharges\"] = pd.to_numeric(dataset[\"TotalCharges\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n\n## Variable Descriptions\n\n* gender    -->    Whether the customer is a male or a female     \n* SeniorCitizen   -->    Whether the customer is a senior citizen or not (1, 0)\n* Partner       -->       Whether the customer has a partner or not (Yes, No)\n* Dependents       -->   Whether the customer has dependents or not (Yes, No)\n* tenure            -->  Number of months the customer has stayed with the company\n* PhoneService      -->  Whether the customer has a phone service or not (Yes, No)\n* MultipleLines     -->  Whether the customer has multiple lines or not (Yes, No, No phone service)\n* InternetService   -->  Customer’s internet service provider (DSL, Fiber optic, No)\n* OnlineSecurity    -->  Whether the customer has online security or not (Yes, No, No internet service)\n* OnlineBackup      -->  Whether the customer has online backup or not (Yes, No, No internet service)\n* DeviceProtection  -->  Whether the customer has device protection or not (Yes, No, No internet service)\n* TechSupport       -->  Whether the customer has tech support or not (Yes, No, No internet service)\n* StreamingTV       -->  Whether the customer has streaming TV or not (Yes, No, No internet service)\n* StreamingMovies   -->  Whether the customer has streaming movies or not (Yes, No, No internet service)\n* Contract           -->  The contract term of the customer (Month-to-month, One year, Two year)\n* PaperlessBilling  -->  Whether the customer has paperless billing or not (Yes, No)\n* PaymentMethod     -->  The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n* MonthlyCharges    -->  The amount charged to the customer monthly\n* TotalCharges      -->  The total amount charged to the customer\n* Churn              -->  Whether the customer churned or not (Yes or No)","metadata":{}},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n\n## Univariate Variable Analysis\n\n* Categorical Variables: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']\n\n* Numerical Variables: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n\n### Categorical Variables","metadata":{}},{"cell_type":"code","source":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in categorical_list:\n    bar_plot(c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n\n### Numerical Variables","metadata":{}},{"cell_type":"code","source":"numerical_int64 = (dataset.dtypes == \"int64\")\nnumerical_int64_list = list(numerical_int64[numerical_int64].index)\n\nprint(\"Categorical variables:\")\nprint(numerical_int64_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n in numerical_int64_list:\n    plot_hist(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n in numerical_float64_list:\n    plot_hist(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Basic Data Analysis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n\n## Distribution of Each Feature\n\nThese graphs show the distribution of each feature within itself.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(50,50))\nj = 0\n\nfor i in categorical_list:\n    colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fbdf70','#ac9fd0','#8b7470']\n    \n    labels = dataset[i].value_counts().index\n    sizes = dataset[i].value_counts().values\n    \n    unique = len(dataset[i].unique())\n    if(unique == 2):\n        myexplode = [0.1, 0]\n    if(unique == 3):\n        myexplode = [0.1, 0,0]\n    if(unique == 4):\n        myexplode = [0.1,0,0,0]\n    \n    plt.subplot(5,4,j+1)\n    plt.pie(sizes, labels=labels, explode = myexplode, shadow = True, startangle=90, colors=colors, autopct='%1.1f%%',textprops={'fontsize': 25})\n    plt.title(f'Distribution of {i}',color = 'black',fontsize = 30)\n    j += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n\n## Distributions of Each Feature According to 'Churn'\n\nThese graphs show the distribution of the variable in each feature according to 'Churn'.","metadata":{}},{"cell_type":"markdown","source":"### Gender","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('gender = Female')\ndataset.groupby('gender').Churn.value_counts().loc['Female'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('gender = Male')\ndataset.groupby('gender').Churn.value_counts().loc['Male'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Partner ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('Partner = Yes')\ndataset.groupby('Partner').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('Partner = No')\ndataset.groupby('Partner').Churn.value_counts().loc['No'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dependents","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('Dependents = Yes')\ndataset.groupby('Dependents').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('Dependents = No')\ndataset.groupby('Dependents').Churn.value_counts().loc['No'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PhoneService","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('PhoneService = Yes')\ndataset.groupby('PhoneService').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('PhoneService = No')\ndataset.groupby('PhoneService').Churn.value_counts().loc['No'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MultipleLines","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('MultipleLines = Yes')\ndataset.groupby('MultipleLines').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('MultipleLines = No')\ndataset.groupby('MultipleLines').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('MultipleLines = No phone service')\ndataset.groupby('MultipleLines').Churn.value_counts().loc['No phone service'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### InternetService","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('InternetService = DSL')\ndataset.groupby('InternetService').Churn.value_counts().loc['DSL'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('InternetService = No')\ndataset.groupby('InternetService').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('InternetService = Fiber optic')\ndataset.groupby('InternetService').Churn.value_counts().loc['Fiber optic'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OnlineSecurity","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('OnlineSecurity = Yes')\ndataset.groupby('OnlineSecurity').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('OnlineSecurity = No')\ndataset.groupby('OnlineSecurity').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('OnlineSecurity = No internet service')\ndataset.groupby('OnlineSecurity').Churn.value_counts().loc['No internet service'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OnlineBackup","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('OnlineBackup = Yes')\ndataset.groupby('OnlineBackup').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('OnlineBackup = No')\ndataset.groupby('OnlineBackup').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('OnlineBackup = No internet service')\ndataset.groupby('OnlineBackup').Churn.value_counts().loc['No internet service'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DeviceProtection","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('DeviceProtection = Yes')\ndataset.groupby('DeviceProtection').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('DeviceProtection = No')\ndataset.groupby('DeviceProtection').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('DeviceProtection = No internet service')\ndataset.groupby('DeviceProtection').Churn.value_counts().loc['No internet service'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TechSupport","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('TechSupport = Yes')\ndataset.groupby('TechSupport').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('TechSupport = No')\ndataset.groupby('TechSupport').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('TechSupport = No internet service')\ndataset.groupby('TechSupport').Churn.value_counts().loc['No internet service'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### StreamingTV","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('StreamingTV = Yes')\ndataset.groupby('StreamingTV').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('StreamingTV = No')\ndataset.groupby('StreamingTV').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('StreamingTV = No internet service')\ndataset.groupby('StreamingTV').Churn.value_counts().loc['No internet service'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### StreamingMovies","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('StreamingMovies = Yes')\ndataset.groupby('StreamingMovies').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('StreamingMovies = No')\ndataset.groupby('StreamingMovies').Churn.value_counts().loc['No'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('StreamingMovies = No internet service')\ndataset.groupby('StreamingMovies').Churn.value_counts().loc['No internet service'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Contract","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('Contract = Month-to-month')\ndataset.groupby('Contract').Churn.value_counts().loc['Month-to-month'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('Contract = One year')\ndataset.groupby('Contract').Churn.value_counts().loc['One year'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('Contract = Two year')\ndataset.groupby('Contract').Churn.value_counts().loc['Two year'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PaperlessBilling","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('PaperlessBilling = Yes')\ndataset.groupby('PaperlessBilling').Churn.value_counts().loc['Yes'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('PaperlessBilling = No')\ndataset.groupby('PaperlessBilling').Churn.value_counts().loc['No'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PaymentMethod","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\nsns.set_theme(style=\"darkgrid\")\n\nplt.subplot(2,2,1)\nplt.title('PaymentMethod = Electronic check')\ndataset.groupby('PaymentMethod').Churn.value_counts().loc['Electronic check'].plot(kind='bar')\n\nplt.subplot(2,2,2)\nplt.title('PaymentMethod = Mailed check')\ndataset.groupby('PaymentMethod').Churn.value_counts().loc['Mailed check'].plot(kind='bar')\n\nplt.subplot(2,2,3)\nplt.title('PaymentMethod = Bank transfer (automatic)')\ndataset.groupby('PaymentMethod').Churn.value_counts().loc['Bank transfer (automatic)'].plot(kind='bar')\n\nplt.subplot(2,2,4)\nplt.title('PaymentMethod = Credit card (automatic)')\ndataset.groupby('PaymentMethod').Churn.value_counts().loc['Credit card (automatic)'].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n# Data Visualization","metadata":{}},{"cell_type":"markdown","source":"### Numerical values, value ranges and distributions.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.histplot(dataset['MonthlyCharges'], color = 'red', kde = True).set_title('MonthlyCharges Interval and Counts')\n\nplt.subplot(2,3,2)\nsns.histplot(dataset['TotalCharges'], color = 'green', kde = True).set_title('TotalCharges Interval and Counts')\n\nplt.subplot(2,3,3)\nsns.histplot(dataset['tenure'], color = 'blue', kde = True).set_title('tenure Interval and Counts')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The relationship between 'MonthlyCharges' and 'TotalCharges' and the correlation with 'gender' and 'tenure'.","metadata":{}},{"cell_type":"code","source":"sns.set_style('darkgrid')\nf, ax = plt.subplots(figsize=(20,10))\nsns.despine(f, left=True, bottom=True)\nsns.set_theme(style=\"darkgrid\")\nsns.scatterplot(x=dataset['MonthlyCharges'], y=dataset['TotalCharges'],\n                hue=dataset['gender'], \n                size=\"tenure\",\n                palette='tab20',\n                hue_order=dataset['gender'],\n                sizes=(20, 50), \n                linewidth=0,\n                data=dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Samples According to 'Churn' = Yes or 'Churn' = No with Histograms","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(50,50))\nj = 0\nsns.set_theme(style=\"whitegrid\")\nfor i in categorical_list:\n    \n    plt.subplot(5,4,j+1)\n    sns.histplot(dataset, x=\"Churn\",  hue=dataset[i], multiple=\"stack\", palette=\"light:m_r\", edgecolor=\".3\", linewidth=.5)\n    plt.title(f'Distribution of {i}',color = 'black',fontsize = 25)\n    j += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a>\n\n# Pandas Profiling\n\nPandas profiling is a useful library that generates interactive reports about the data. With using this library, we can see types of data, distribution of data and various statistical information. This tool has many features for data preparing. Pandas Profiling includes graphics about specific feature and correlation maps too. You can see more details about this tool in the following url: https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/","metadata":{}},{"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a>\n\n# Correlation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Implications:\n\n* As seen from Heat Map, there is a high correlation between 'tenure' and 'TotalCharges'.\n\n* Another notable correlation is between 'MonthlyCharges' and 'TotalCharges'.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(dataset, hue = 'Churn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"13\"></a>\n\n# Skewness","metadata":{}},{"cell_type":"code","source":"dataset.agg(['skew'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skews = ['MonthlyCharges']\nfrom scipy.stats import norm, skew, boxcox\nfor i in skews:\n    sns.set_style('darkgrid')\n    sns.distplot(dataset[i], fit = norm)\n    plt.title('Skeweed')\n    plt.show()\n    (mu, sigma) = norm.fit(dataset[i])\n    print(\"mu {} : {}, sigma {} : {}\".format(i, mu, i, sigma))\n    print()\n    \n    dataset[i], lam = boxcox(dataset[i])\n\n    sns.set_style('darkgrid')\n    sns.distplot(dataset[i], fit = norm)\n    plt.title('Transformed')\n    plt.show()\n    (mu, sigma) = norm.fit(dataset[i])\n    print(\"mu {} : {}, sigma {} : {}\".format(i, mu, i, sigma))\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"14\"></a>\n\n# Encoding","metadata":{}},{"cell_type":"markdown","source":"<a id=\"22\"></a>\n## Uniqueness of each Feature","metadata":{}},{"cell_type":"code","source":"label_encoding = []\none_hot = []\n\nfor x in categorical_list:\n    a = dataset[x].unique()\n    print(f'Unique Values for {x}: ', dataset[x].unique())\n    if(len(a) == 2):\n        label_encoding.append(x)\n    else:\n        one_hot.append(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"15\"></a>\n\n## Label Encoding\n\nLabel Encoding is an encoding technique for handling categorical variables. In this technique, each data is assigned a unique integer.","metadata":{}},{"cell_type":"code","source":"for y in label_encoding:\n    var = dataset[y].unique()\n    y_mapping = {var[0]: 0, var[1]: 1}\n    dataset[y] = dataset[y].map(y_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"16\"></a>\n\n## One-Hot Encoding\n\nOne Hot Encoding is the binary representation of categorical variables. This process requires categorical values to be mapped to integer values first. Next, each integer value is represented as a binary vector with all values zero except the integer index marked with 1.\n\nOne Hot Encoding makes the representation of categorical data more expressive and easy. Many machine learning algorithms cannot work directly with categorical data, so categories must be converted to numbers. This operation is required for input and output variables that are categorical.\n\nIn this part, I converted categorical datas to the binary values. This operation increases the accuracy.\n","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(one_hot)):\n    dataset[f'{one_hot[i]}'] = pd.Categorical(dataset[f'{one_hot[i]}'])\n    dummies = pd.get_dummies(dataset[f'{one_hot[i]}'], prefix = f'{one_hot[i]}_encoded')\n    dataset.drop([f'{one_hot[i]}'], axis=1, inplace=True)\n    dataset = pd.concat([dataset, dummies], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"17\"></a>\n# Train - Test Split","metadata":{}},{"cell_type":"code","source":"columns = dataset.columns.drop('Churn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = columns\nlabel = ['Churn']\n\nX = dataset[features]\ny = dataset[label]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"18\"></a>\n# Pipelines","metadata":{}},{"cell_type":"code","source":"pipeline_GaussianNB = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_GaussianNB\",GaussianNB())])\n\npipeline_BernoulliNB = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_BernoulliNB\",BernoulliNB())])\n\npipeline_LogisticRegression = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_LogisticRegression\",LogisticRegression())])\n\npipeline_RandomForest = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_RandomForest\",RandomForestClassifier())])\n\npipeline_SVM = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_SVM\",SVC())])\n\npipeline_DecisionTree = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_DecisionTree\",DecisionTreeClassifier())])\n\npipeline_KNN = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_KNN\",KNeighborsClassifier())])\n\npipeline_GBC = Pipeline([(\"scaler\",StandardScaler()), (\n                        \"pipeline_GBC\",GradientBoostingClassifier())])\n\npipeline_SGD = Pipeline([(\"scaler\",StandardScaler()), \n                        (\"pipeline_SGD\",SGDClassifier(max_iter=5000, random_state=0))])\n\npipeline_LGBM = Pipeline([(\"scaler\",StandardScaler()), \n                        (\"pipeline_NN\",lightgbm.LGBMClassifier())])\n\npipelines = [pipeline_GaussianNB, pipeline_BernoulliNB, pipeline_LogisticRegression, pipeline_RandomForest, pipeline_SVM, pipeline_DecisionTree, pipeline_KNN, pipeline_GBC, pipeline_SGD, pipeline_LGBM]\n\npipe_dict = {0: \"GaussianNB\", 1: \"BernoulliNB\", 2: \"LogisticRegression\",3: \"RandomForestClassifier\", 4: \"SupportVectorMachine\", 5: \"DecisionTreeClassifier\",\n            6: \"KNeighborsClassifier\", 7: \"GradientBoostingClassifier\", 8:\"Stochastic Gradient Descent\", 9: \"LGBM\"}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'LGBM']\n\ni= 0\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor pipe in pipelines:\n    pipe.fit(X_train, y_train)\n    print(f'{pipe_dict[i]}')\n    print(\"Train Score of %s: %f     \" % (pipe_dict[i], pipe.score(X_train, y_train)*100))\n    trainScores.append(pipe.score(X_train, y_train)*100)\n    \n    print(\"Validation Score of %s: %f\" % (pipe_dict[i], pipe.score(X_valid, y_valid)*100))\n    validationScores.append(pipe.score(X_valid, y_valid)*100)\n    \n    print(\"Test Score of %s: %f      \" % (pipe_dict[i], pipe.score(X_test, y_test)*100))\n    testScores.append(pipe.score(X_test, y_test)*100)\n    print(\" \")\n    \n    y_predictions = pipe.predict(X_test)\n    conf_matrix = confusion_matrix(y_predictions, y_test)\n    print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n    \n    tn = conf_matrix[0,0]\n    fp = conf_matrix[0,1]\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n\n    total = tn + fp + tp + fn\n    real_positive = tp + fn\n    real_negative = tn + fp\n\n    accuracy  = (tp + tn) / total # Accuracy Rate\n    precision = tp / (tp + fp) # Positive Predictive Value\n    recall    = tp / (tp + fn) # True Positive Rate\n    f1score  = 2 * precision * recall / (precision + recall)\n    specificity = tn / (tn + fp) # True Negative Rate\n    error_rate = (fp + fn) / total # Missclassification Rate\n    prevalence = real_positive / total\n    miss_rate = fn / real_positive # False Negative Rate\n    fall_out = fp / real_negative # False Positive Rate\n    \n    print('Evaluation Metrics:')\n    print(f'Accuracy    : {accuracy}')\n    print(f'Precision   : {precision}')\n    print(f'Recall      : {recall}')\n    print(f'F1 score    : {f1score}')\n    print(f'Specificity : {specificity}')\n    print(f'Error Rate  : {error_rate}')\n    print(f'Prevalence  : {prevalence}')\n    print(f'Miss Rate   : {miss_rate}')\n    print(f'Fall Out    : {fall_out}')\n\n    print(\"\") \n    print(f'Classification Report: \\n{classification_report(y_predictions, y_test)}\\n')\n    print(\"\")\n\n    print(\"*****\"*20)\n    i +=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'LGBM']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table = pd.DataFrame({'Model': modelNames, 'Train': trainScores, 'Validation': validationScores, 'Test': testScores})\ntable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id=\"19\"></a>\n## Cross Validation","metadata":{}},{"cell_type":"code","source":"cv_results_acc = []\n\nfor i, model in enumerate(pipelines):\n    cv_score = cross_val_score(model, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    cv_results_acc.append(cv_score.mean()*100)\n    print(\"%s: %f\" % (pipe_dict[i], cv_score.mean()*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table_cv = pd.DataFrame({'Model': modelNames, 'CV Score': cv_results_acc})\ntable_cv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('CV Scores Means', fontweight='bold', size = 24)\n\nbarWidth = 0.5\n \nbars2 = cv_results_acc\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\n \nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='black',  yerr=0.5,ecolor=\"black\",capsize=10)\n\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Light GBM']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"20\"></a>\n## Best Features Selection","metadata":{}},{"cell_type":"code","source":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_valid = sc.fit_transform(X_valid)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Light GBM': lightgbm.LGBMClassifier(),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")\n\n  imp_feature = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_})\n  plt.figure(figsize=(15,10))\n  plt.title(\"Feature Importance Graphic\")\n  plt.xlabel(\"importance \")\n  plt.ylabel(\"features\")\n  plt.barh(imp_feature['Feature'],imp_feature['Importance'])\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"23\"></a>\n# Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"gbc_model = GradientBoostingClassifier()\ngbc_model.fit(X_train, y_train)\n\ntrain_score = gbc_model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\nvalidation_score = gbc_model.score(X_valid, y_valid)\nprint(f'Validation score of trained model: {validation_score*100}')\n\ntest_score = gbc_model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"24\"></a>\n## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"y_predictions = gbc_model.predict(X_test)\n\nconf_matrix = confusion_matrix(y_predictions, y_test)\n\nprint(f'Accuracy: {accuracy_score(y_predictions, y_test)*100}')\nprint()\nprint(f'Confussion matrix: \\n{conf_matrix}\\n')\n\nsns.heatmap(conf_matrix, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn = conf_matrix[0,0]\nfp = conf_matrix[0,1]\ntp = conf_matrix[1,1]\nfn = conf_matrix[1,0]\n\ntotal = tn + fp + tp + fn\nreal_positive = tp + fn\nreal_negative = tn + fp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy  = (tp + tn) / total # Accuracy Rate\nprecision = tp / (tp + fp) # Positive Predictive Value\nrecall    = tp / (tp + fn) # True Positive Rate\nf1score  = 2 * precision * recall / (precision + recall)\nspecificity = tn / (tn + fp) # True Negative Rate\nerror_rate = (fp + fn) / total # Missclassification Rate\nprevalence = real_positive / total\nmiss_rate = fn / real_positive # False Negative Rate\nfall_out = fp / real_negative # False Positive Rate\n\nprint(f'Accuracy    : {accuracy}')\nprint(f'Precision   : {precision}')\nprint(f'Recall      : {recall}')\nprint(f'F1 score    : {f1score}')\nprint(f'Specificity : {specificity}')\nprint(f'Error Rate  : {error_rate}')\nprint(f'Prevalence  : {prevalence}')\nprint(f'Miss Rate   : {miss_rate}')\nprint(f'Fall Out    : {fall_out}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"25\"></a>\n## Classification Report","metadata":{}},{"cell_type":"code","source":"predictions = gbc_model.predict(X_test)\n\nprint(classification_report(predictions, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"26\"></a>\n## ROC Curve","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC' )\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = gbc_model.predict_proba(X_test)\nprobs = probs[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_test, probs)\nprint('AUC: ', auc*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, probs)\nplt.legend(loc = 'lower right')\nplot_roc_curve(fpr, tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"27\"></a>\n## Visualization","metadata":{}},{"cell_type":"code","source":"!pip install pydotplus","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#####\n# This code snippet was taken from this url: https://stackoverflow.com/questions/44974360/how-to-visualize-an-sklearn-gradientboostingclassifier\n#####\n\nimport pydotplus\nfrom sklearn.tree import export_graphviz\nfrom pydotplus import graph_from_dot_data\nfrom IPython.display import Image\n\nsub_tree = gbc_model.estimators_[10, 0]\ndot_data = export_graphviz(sub_tree, out_file=None, filled=True, \n                           rounded=True, special_characters=True, proportion=True, impurity=True)\n\ngraph = graph_from_dot_data(dot_data)\nImage(graph.create_png())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"21\"></a>\n# Conclusion\n\nI made Visualization and Machine Learning on this notebook. If you like my visualization and you want to know how I made them, you can check my other notebooks which are about Seaborn and Plotly libraries. You can see them via this links:\n\n**EDA: Visualization with Plotly for Beginners**\n\n* https://www.kaggle.com/barisscal/eda-visualization-with-plotly-for-beginners\n\n\n**EDA: Visualization with Seaborn**\n\n* https://www.kaggle.com/barisscal/eda-visualization-with-seaborn\n\n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, please let me know :)\n\nThank you for your time.","metadata":{}}]}