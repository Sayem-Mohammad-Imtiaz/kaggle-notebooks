{"cells":[{"metadata":{"_uuid":"f6c9323855c88af478ff8439c729275498bb85b1"},"cell_type":"markdown","source":"# Summary:\n* #  [Introduction](#first-bullet)\n\n* #  [I. Data preprocessing](#second-bullet)\n\n* #  [II. First Modeling : Multi Layer Perceptron ](#third-bullet)\n* ##  [ 1) Modeling  ](#fifth-bullet)\n* ##  [ 2) First Results ](#sixth-bullet)\n\n* #  [III. Second Modeling : LSTM ](#fourth-bullet)\n* ##  [ 1) Modeling  ](#seventh-bullet)\n* ##  [ 2) First Results ](#eighth-bullet)\n\n* #  [IV. Third Modeling : CNN ](#ninth-bullet)\n* ##  [ 1) Modeling  ](#tenth-bullet)\n* ##  [ 2) First Results ](#eleventh-bullet)"},{"metadata":{"_uuid":"a4ac4fff650d4f8c717fe45c00a99d2359f5bdfc"},"cell_type":"markdown","source":"# Introduction : importing librairies and defining parameters <a class=\"anchor\" id=\"first-bullet\"></a>\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom keras.datasets import reuters\nfrom keras.models import Sequential\nfrom keras.preprocessing import sequence\nfrom keras.layers import Dense, Activation, Embedding, Dropout\nfrom keras.layers import LSTM\nfrom keras.utils import np_utils\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.core import Masking\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D\nimport collections\nPATH = \"../input/Tweets.csv\"\n\ndata=pd.read_csv(PATH)\ndata= data.copy()[['airline_sentiment', 'text']]\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e7001a140837dc5762224554de5dd86bd9817e7"},"cell_type":"code","source":"max_words = 10000\nbatch_size = 32\nnb_epoch = 10\nmaxlen = 12\nmax_features = 10000\nnb_filter = 250\nfilter_length = 3\nhidden_dims = 250\ntotal=0\ncoall=collections.Counter()\n\n\n","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"92bcb6677df951d5a76991869df26de8c8d1b5b0"},"cell_type":"markdown","source":"# I.  Data preprocessing<a class=\"anchor\" id=\"second-bullet\"></a>"},{"metadata":{"_uuid":"bbf1fe9827273497a529bb0ca372fb02d311527f"},"cell_type":"markdown","source":"### First look at the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"340d7c7512370446348954a2a0f22dfcb5cc8ab8"},"cell_type":"code","source":"Counter(data.airline_sentiment)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"0dfd5251ae0257ab545e16d6bdbfe2ee349c6916"},"cell_type":"markdown","source":"### Data preparation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1888b09479439e43df5a572d351a00e46cb1f335"},"cell_type":"code","source":"def review_to_words( review ):\n    review_text = review\n    no_hasthtags = re.sub(\"#\\w+\", \" \", review_text)\n    no_url = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", no_hasthtags )\n    no_tag = re.sub(\"@\\w+\", \" \", no_url)\n    no_punctions = re.sub(\"[^a-zA-Z]\", \" \", no_tag) \n    wordslower= no_punctions.lower()\n    words = word_tokenize(wordslower)  \n    stopswd = set(stopwords.words(\"english\"))                  \n    meaningful_wd = [w for w in words if not w in stopswd]\n    str=' '.join(meaningful_wd)   \n    return(str) ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ddfaee3746fa7ec70df44a7a7eee66b97f0033c9"},"cell_type":"code","source":"clean_text = []\nfor tweet in data['text']:\n    clean= review_to_words(tweet)\n    clean_text.append(clean)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5b4aaa652f0f2e3996f47f8f4d34b601d52e02d","scrolled":true,"collapsed":true},"cell_type":"code","source":"for i in range(0,len(data['airline_sentiment'])): \n     if data['airline_sentiment'][i]=='negative':\n        data['airline_sentiment'][i]=0\n     if data['airline_sentiment'][i]=='positive':\n        data['airline_sentiment'][i]=1\n     if data['airline_sentiment'][i]=='neutral':\n        data['airline_sentiment'][i]=2\n","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"b1e1a6094bc3c9e213724177582c1ca70b0b5ec7"},"cell_type":"markdown","source":"# II. First Modeling : Multi Layer Perceptron <a class=\"anchor\" id=\"third-bullet\"></a>"},{"metadata":{"_uuid":"c5591639d53d2666efbe3fc1349be2269369d55d"},"cell_type":"markdown","source":"## 1) Modeling : <a class=\"anchor\" id=\"fifth-bullet\"></a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"49a96de3ecff3380a8813826074cd965f2758730"},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001, verbose=1, mode='auto')\ncallbacks_list = [earlystop]\ndata['text'] = clean_text\ntotal=0","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"086bff55d0ec5dd1b5b7582e9f00c6070d2da1d5","scrolled":true},"cell_type":"code","source":"history = []\nfor t in range (0,10):\n     cv=10\n     k = [int((len(data['text']))/cv*j) for j in range(cv+1)]\n     X_test, y_test= data['text'][k[t]:k[t+1]], data['airline_sentiment'][k[t]:k[t+1]]\n     X_train, y_train =pd.concat([data['text'][:k[t]],data['text'][k[t+1]:]]), pd.concat([data['airline_sentiment'][:k[t]],data        ['airline_sentiment']  [k[t+1]:]])\n     nb_classes = 3\n     train_data=[]\n     for i in X_train:\n        train_data.append(i)\n     test_data=[]\n     for i in X_test:\n        test_data.append(i)\n     tokenizer = Tokenizer(num_words=max_words)\n     X_train = tokenizer.fit_on_texts(train_data)\n     X_test = tokenizer.fit_on_texts(test_data)\n     X_train = tokenizer.texts_to_matrix(train_data,mode='binary')\n     X_test = tokenizer.texts_to_matrix(test_data,mode='binary')\n     y_train = np_utils.to_categorical(y_train, nb_classes)\n     y_test = np_utils.to_categorical(y_test, nb_classes)\n     model = Sequential()\n     model.add(Dense(10, input_shape=(max_words,)))\n     model.add(Activation('relu'))\n     model.add(Dropout(0.5))\n     model.add(Dense(3))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n    \n     history.append(model.fit(X_train, y_train,\n                    epochs=nb_epoch, batch_size=batch_size,\n                    callbacks = callbacks_list, validation_split=0.1))\n     score = model.evaluate(X_test, y_test,\n                       batch_size=batch_size, verbose=1)\n     total=total+score[1]\n     t=t+1\n\n\nprint(model.summary())\nprint(total)\naccuracy=total/10\nprint(accuracy)\n\n","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"cbeba5bf862bbf61bccaabadba5972c6a24ad89c"},"cell_type":"markdown","source":"## 2) First Results: <a class=\"anchor\" id=\"sixth-bullet\"></a>"},{"metadata":{"_uuid":"3674ebec2d512cca9fa017d241d5b5c5bea002fc"},"cell_type":"markdown","source":"#### Function to plot the model\nFrom http://parneetk.github.io/blog/neural-networks-in-keras/"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"48447d825524f4b96dd70482ce773d6c8143b2c1"},"cell_type":"code","source":"def plot_model_history(model_history):\n    fig, axs = plt.subplots(1,2,figsize=(15,5))\n    # summarize history for accuracy\n    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].set_xlabel('Epoch')\n    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n    axs[0].legend(['train', 'val'], loc='best')\n    # summarize history for loss\n    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss')\n    axs[1].set_xlabel('Epoch')\n    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n    axs[1].legend(['train', 'val'], loc='best')\n    plt.show()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c5e0057e3527b2661b0e0d827b98197ff55359f"},"cell_type":"code","source":"for x in history :\n    plot_model_history(x)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f7895f7af96c503525411b11bd42888f1335abf"},"cell_type":"markdown","source":"# II. Second Modeling : LSTM <a class=\"anchor\" id=\"fourth-bullet\"></a>"},{"metadata":{"_uuid":"37dfa0b5294149ddf2d93b008d52e95ead532843"},"cell_type":"markdown","source":"## 1) Modeling : <a class=\"anchor\" id=\"seventh-bullet\"></a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eab09f18829c5824c11eef7eb416a7da12617041"},"cell_type":"code","source":"def language_preprocessing(x_train,x_test,x_dev):\n   each_critique=[]\n   train=[]\n   test=[]\n   dev=[]\n   t=Tokenizer()\n   t.fit_on_texts(data['text'])\n   dictionnary=t.word_index\n   for element in x_train:\n       words = word_tokenize(element)\n       for element in words:\n                each_critique.append(dictionnary[element])\n       train.append(each_critique)\n       each_critique=[]\n   for element in x_test:\n       words = word_tokenize(element)\n       for element in words:\n                each_critique.append(dictionnary[element])\n       test.append(each_critique)\n       each_critique=[]\n   for element in x_dev:\n       words = word_tokenize(element)\n       for element in words:\n                each_critique.append(dictionnary[element])\n       dev.append(each_critique)\n       each_critique=[]\n   return(train,test,dev)\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82f44085b90e03be2de0e5ba5826cd2eed0d3bcf","scrolled":false},"cell_type":"code","source":"histoLSTM = []\nfor t in range (0,10):\n     cv=10\n     data['text'] = clean_text\n     k = [int((len(data['text']))/cv*j) for j in range(cv+1)]\n     X_test, y_test= data['text'][k[t]:k[t+1]], data['airline_sentiment'][k[t]:k[t+1]]\n     X_train, y_train =pd.concat([data['text'][:k[t]],data['text'][k[t+1]:]]), pd.concat([data['airline_sentiment'][:k[t]],data        ['airline_sentiment']  [k[t+1]:]])\n     X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n     y_train = np_utils.to_categorical(y_train, 3)\n     y_test = np_utils.to_categorical(y_test, 3)\n     y_dev = np_utils.to_categorical(y_dev, 3)\n     train_data=[]\n     for i in X_train:\n       train_data.append(i)\n     test_data=[]\n     for i in X_test:\n       test_data.append(i)\n     dev_data=[]\n     for i in X_dev:\n       dev_data.append(i)\n     X_train,X_test,X_dev=language_preprocessing(train_data,test_data,dev_data)\n     X_train= sequence.pad_sequences(X_train, maxlen=maxlen)\n     X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n     X_dev= sequence.pad_sequences(X_dev, maxlen=maxlen)\n     model = Sequential() \n     model.add(Embedding(max_features, 50, mask_zero=True,input_length=12))\n     model.add(LSTM(3, dropout=0.3, recurrent_dropout=0.05))\n     model.add(Dense(3))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n     histoLSTM.append(model.fit(X_train, y_train, batch_size=batch_size, epochs=10, callbacks = callbacks_list,\n         validation_data=(X_dev, y_dev)))\n     score, acc = model.evaluate(X_test, y_test,\n                           batch_size=batch_size)\n     total=total+acc\n     t=t+1\n\n\nprint(\"************************************\")\nprint(model.summary()) \nprint(total)\naccuracy=total/10\nprint(accuracy)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"50e50864bcd30993bad31425fc2e4bdc57910db7"},"cell_type":"markdown","source":"## 2) First Results: <a class=\"anchor\" id=\"eighth-bullet\"></a>"},{"metadata":{"trusted":true,"_uuid":"2b1231c1135dc0754b5ad903583dec71c8947a6d"},"cell_type":"code","source":"for x in histoLSTM:\n    plot_model_history(x)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e909227f53599c649a665a8faffa22439cea7854"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f04feb96e2c67afc9393d7627d94f78c18d244da"},"cell_type":"markdown","source":"# II. Third Modeling : CNN <a class=\"anchor\" id=\"ninth-bullet\"></a>"},{"metadata":{"_uuid":"179d42ee3a9a4ce36fbbb0ebc270ebb1b8f81d8d"},"cell_type":"markdown","source":"## 1) Modeling : <a class=\"anchor\" id=\"tenth-bullet\"></a>"},{"metadata":{"trusted":true,"_uuid":"758e353b691c57913a5b220203ba39935c31f3cd"},"cell_type":"code","source":"histoCNN=[]\nfor t in range (0,10):\n     cv=10\n     data['text'] = clean_text\n     k = [int((len(data['text']))/cv*j) for j in range(cv+1)]\n     X_test, y_test= data['text'][k[t]:k[t+1]], data['airline_sentiment'][k[t]:k[t+1]]\n     X_train, y_train =pd.concat([data['text'][:k[t]],data['text'][k[t+1]:]]), pd.concat([data['airline_sentiment'][:k[t]],data        ['airline_sentiment']  [k[t+1]:]])\n     X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n     y_train = np_utils.to_categorical(y_train, 3)\n     y_test = np_utils.to_categorical(y_test, 3)\n     y_dev = np_utils.to_categorical(y_dev, 3)\n     train_data=[]\n     for i in X_train:\n       train_data.append(i)\n     test_data=[]\n     for i in X_test:\n       test_data.append(i)\n     dev_data=[]\n     for i in X_dev:\n       dev_data.append(i)\n     X_train,X_test,X_dev=language_preprocessing(train_data,test_data,dev_data)\n     X_train= sequence.pad_sequences(X_train, maxlen=maxlen)\n     X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n     X_dev= sequence.pad_sequences(X_dev, maxlen=maxlen)\n     model = Sequential()\n     embedding_layer = Embedding(max_features, 50, input_length=12)\n     model.add(embedding_layer)\n     model.add(Convolution1D(nb_filter=nb_filter,\n                        filter_length=filter_length,\n                        border_mode='valid',\n                        activation='relu',\n                        subsample_length=1))\n     model.add(GlobalMaxPooling1D())\n     model.add(Dense(hidden_dims))\n     model.add(Dropout(0.2))\n     model.add(Activation('relu'))\n     model.add(Dense(3))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n     histoCNN.append(model.fit(X_train, y_train,\n         batch_size=batch_size, callbacks=callbacks_list, \n         nb_epoch=nb_epoch,\n         validation_data=(X_dev, y_dev)))\n     acc = model.evaluate(X_test, y_test,\n                           batch_size=batch_size)\n     print('Test accuracy:', acc[1])\n     total=total+acc[1]\n     t=t+1\n\n\n\n\nprint(\"************************************\")\nprint(model.summary()) \nprint(total)\naccuracy=total/10\nprint(accuracy)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"93530f80a587e9ccf589744162d62a136c27b153"},"cell_type":"markdown","source":"## 2) First Results : <a class=\"anchor\" id=\"eleventh-bullet\"></a>"},{"metadata":{"trusted":true,"_uuid":"a8a0e94933ae9c0ccc0ed52c8cd93a3304852aed"},"cell_type":"code","source":"for x in histoCNN:\n    plot_model_history(x)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b7988b5286cafe26e03b6cc5a4785926cd71bf59"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}