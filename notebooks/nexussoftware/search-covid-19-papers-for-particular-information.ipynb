{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A look through the data sources included in the dataset\nThis notebook provides a few ways to browse through the dataset and become familiar with the contents, prior to doing analysis.\n\nThere are 29500 papers in the dataset. These are listed in the all_sources_metadata file. Here we add utility functions to load, browse and search the research papers.\n\nThe papers are also listed in lots of JSON files, but the all_sources_metadata is the most convenient way to browse the papers.\n\n\n## Highlights\nThis notebook has the following useful features\n1. Viewing the papers in the metdata csv as a dataframe\n2. Selecting individual papers\n3. Search using a simple search index"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Before we begin\nThis notebook runs code that assume that the **Internet** is **ON**. Make sure to turn Internet to ON in the notebook **Settings**"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path, PurePath\nimport pandas as pd\nimport requests\nfrom requests.exceptions import HTTPError, ConnectionError\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt\")\npd.options.display.max_colwidth = 500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the All Sources Metadata file\nHere we load the csv file containing the metadata for the SARS-COV-2 papers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Where are all the files located\ninput_dir = PurePath('../input/CORD-19-research-challenge/2020-03-13')\n\n# The all sources metadata file\nmetadata = pd.read_csv(input_dir / 'all_sources_metadata_2020-03-13.csv', \n                      dtype={'Microsoft Academic Paper ID': str,\n                             'pubmed_id': str})\n\n# Convert the doi to a url\ndef doi_url(d): return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\nmetadata.doi = metadata.doi.fillna('').apply(doi_url)\n\n# Set the abstract to the paper title if it is null\nmetadata.abstract = metadata.abstract.fillna(metadata.title)\n\n# A list of columns to limit the display\nMETADATA_COLS = ['title', 'abstract', 'doi', 'publish_time',\n                 'authors', 'journal', 'has_full_text']\n\ndef show_metadata(ShowAllColumns=False):\n    return metadata if ShowAllColumns else metadata[METADATA_COLS]\n\n# Use ipywidgets to limit the sources\ninteract(show_metadata);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some Data Classes for Research Papers\nThese classes make it easier to navigate through the datasources. There is a class called **ResearchPapers** that wraps the entire dataset an provide useful functions to navigate through it, and **Paper**, that make it easier to view each paper."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get(url, timeout=6):\n    try:\n        r = requests.get(url, timeout=timeout)\n        return r.text\n    except ConnectionError:\n        print(f'Cannot connect to {url}')\n        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n    except HTTPError:\n        print('Got http error', r.status, r.text)\n\nclass ResearchPapers:\n    \n    def __init__(self, metadata: pd.DataFrame):\n        self.metadata = metadata\n        \n    def __getitem__(self, item):\n        return Paper(self.metadata.iloc[item])\n    \n    def __len__(self):\n        return len(self.metadata)\n    \n    def head(self, n):\n        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n    \n    def tail(self, n):\n        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n    \n    def abstracts(self):\n        '''\n        :return: a list of the abstracts\n        '''\n        return self.metadata.abstract.dropna()\n    \n    def titles(self):\n        return self.metadata.title.dropna()\n        \n    def _repr_html_(self):\n        return self.metadata._repr_html_()\n    \nclass Paper:\n    \n    '''\n    A single research paper\n    '''\n    def __init__(self, item):\n        self.paper = item.to_frame().fillna('')\n        self.paper.columns = ['Value']\n    \n    def doi(self):\n        return self.paper.loc['doi'].values[0]\n    \n    def html(self):\n        '''\n        Load the paper from doi.org and display as HTML. Requires internet to be ON\n        '''\n        text = get(self.doi())\n        return widgets.HTML(text)\n    \n    def text(self):\n        '''\n        Load the paper from doi.org and display as text. Requires Internet to be ON\n        '''\n        text = get(self.doi())\n        return text\n    \n    def abstract(self):\n        return self.paper.loc['abstract'].values[0]\n    \n    def title(self):\n        return self.paper.loc['title'].values[0]\n    \n    def authors(self, split=False):\n        '''\n        Get a list of authors\n        '''\n        authors = self.paper.loc['authors'].values[0]\n        if not authors:\n            return []\n        if not split:\n            return authors\n        if authors.startswith('['):\n            authors = authors.lstrip('[').rstrip(']')\n            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n        \n        # Todo: Handle cases where author names are separated by \",\"\n        return [a.strip() for a in authors.split(';')]\n        \n    def _repr_html_(self):\n        return self.paper._repr_html_()\n    \n\npapers = ResearchPapers(metadata)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Authors"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers[1620].authors(split=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examples of Loading a Paper from DOI\n\nTo run the code below the **Internet** must be set to **ON**. This setting is on the **right pane** or at the **bottom****** of the notebook."},{"metadata":{},"cell_type":"markdown","source":"### Show a paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load a paper\nThe following code loads the **doi** url and displays it inside an **ipywidget HTML widget**. \nNote that there seems to be a side effect on some papers where the notebook takes the CSS style of the research paper. I am looking into this."},{"metadata":{"trusted":true},"cell_type":"code","source":"papers[0].html()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load a paper as text"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers[0].text()[:1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select the first 2 papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Abstracts"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.head(2).abstracts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Titles\nShow the titles of the first 2 papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.head(2).titles()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create a simple search index\nWe will create a simple search index that will just match search tokens in a document. First we tokenize the abstract and store it in a dataframe. Then we just match search terms against it."},{"metadata":{"trusted":true},"cell_type":"code","source":"english_stopwords = list(set(stopwords.words('english')))\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words if word.isalnum() \n                                             and not word in english_stopwords\n                                             and not (word.isnumeric() and len(word) < 4)]))\ndef preprocess(string):\n    return tokenize(string.lower())\n\nclass SearchResults:\n    \n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n        \n    def _repr_html_(self):\n        #display_cols = [col for col in self.results.columns if not col == 'index']\n        return self.results._repr_html_()\n\nclass WordTokenIndex:\n    \n    def __init__(self, \n                 metadata: pd.DataFrame, \n                 columns=['title', 'abstract', 'doi', 'authors', 'journal', 'has_full_text']):\n        self.metadata = metadata\n        self.index = metadata.abstract.fillna('').apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = metadata.index\n        self.columns = columns\n    \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        results = self.metadata[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        return SearchResults(results, self.columns + ['paper'])\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Word Token Index\n\nCreating a Word Token Index on the entire dataset takes over a minute. As a todo I will try to speed it up.\nFor now let's create the word_token_index on the first 10000 records"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_token_index = WordTokenIndex(metadata.head(10000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = word_token_index.search('Guidance')\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Show the paper for the 4th item in the search results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results[3].title()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Covid Research Tasks\nThis dataset has a number of tasks. We will try to organize the papers according to the tasks\n\n1. **What is known about transmission, incubation, and environmental stability?**\n2. **What do we know about COVID-19 risk factors?**\n3. **What do we know about virus genetics, origin, and evolution?**\n4. **What has been published about ethical and social science considerations?**\n5. **What do we know about diagnostics and surveillance?**\n6. **What has been published about medical care?**\n7. **What do we know about non-pharmaceutical interventions?**\n8. **What has been published about information sharing and inter-sectoral collaboration?**\n9. **What do we know about vaccines and therapeutics?**\n10. **Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = [('What is known about transmission, incubation, and environmental stability?', \n        'transmission incubation environment'),\n        ('What do we know about COVID-19 risk factors?', 'risk factors'),\n        ('What do we know about virus genetics, origin, and evolution?', 'genetics origin evolution'),\n        ('What has been published about ethical and social science considerations','ethics ethical social'),\n        ('What do we know about diagnostics and surveillance?','diagnose diagnostic surveillance'),\n        ('What has been published about medical care?', 'medical care'),\n        ('What do we know about vaccines and therapeutics?', 'vaccines vaccine vaccinate therapeutic therapeutics'),\n        ('Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.', 'EUA, CLIA') ] \ntasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Research Papers for each task\nHere we add a dropdown that allows for selection of tasks and show the search results."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_task(Task):\n    print(Task)\n    keywords = tasks[tasks.Task == Task].Keywords.values[0]\n    search_results = word_token_index.search(keywords)\n    return search_results\n    \nresults = interact(show_task, Task = tasks.Task.tolist());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TODO\n1. Allow selection of individual papers from the search results dropdown. This will probably use the **interactive** widget instead of **interac**.\n2. Create a new search index based on **bm25**. This will be more accurate"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}