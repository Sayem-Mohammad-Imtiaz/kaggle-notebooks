{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pywsd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport re\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport nltk\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.manifold import MDS\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import casual_tokenize\nimport string\nfrom pywsd.utils import lemmatize_sentence\nfrom sklearn.feature_extraction.text import TfidfVectorizer,ENGLISH_STOP_WORDS\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport catboost\nimport os\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Briefly Look at Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(r'../input/coronavirus-tweets/Corona.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_raw = data['OriginalTweet']\nsns.countplot(y=data['Sentiment']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation Pipeline: \n# Removing References -> Lower Cse -> Lemmatizing -> Removing Punctuation,Stop Words -> Vectorizing"},{"metadata":{},"cell_type":"markdown","source":"# Removing the References"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = tweets_raw.apply(lambda x: re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', str(x)))\ntweets.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To Lower Case"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = tweets.apply(lambda x : x.lower())\ntweets.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_lemmatized = tweets.apply(lambda x : lemmatize_sentence(x))\ntweets = tweets_lemmatized.apply(lambda x: ' '.join(x))\ntweets.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"Punctuation = string.punctuation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Stop-Words and Punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"ENGLISH_STOP_WORDS_PUNCTUATION = list(ENGLISH_STOP_WORDS)\nfor i in Punctuation:\n    ENGLISH_STOP_WORDS_PUNCTUATION.append(i)\n    \ncorona_Stop =[ '#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19',\n'#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19','. .',\n '. . .',\n '. . . . .',\n '..',\n '...',\n '... .',\n '... ..',\n '... ...',\n '... ... .',\n '0',\n '00',\n '000',\n '1',\n '1,000',\n '1.5',\n '1/2',\n '1/3',\n '10',\n '10,000',\n '100',\n'100,000',\n '1000',\n '101',\n '10downingstreet',\n '11',\n '12',\n '13',\n '14',\n '15',\n '150',\n '16',\n '17',\n '18',\n '19',\n '19au',\n '19aus',\n '19australia',\n '19canada',\n '19india',\n '19nz',\n '19on',\n '19out',\n '19outbreak',\n '19pandemic',\n '19sa',\n '19southafrica',\n '19th',\n '19ug',\n '19uk',\n '19us',\n '19usa',\n '1st',\n '2',\n '2-3',\n '2.italy',\n '2/2',\n '20',\n '200',\n '2000',\n '2003',\n '2008',\n '2016',\n '2018',\n '2019',\n '2019au',\n '2020',\n '2021',\n '21',\n '21daylockdown',\n '21dayslockdown',\n '22',\n '23',\n '24',\n '\\x92',\n '24/7',\n '25',\n '26',\n '27',\n '28',\n '2m',\n '2nd',\n '2x',\n '3',\n '3.wuhan',\n '30',\n '300',\n '31',\n '32',\n '33',\n '34',\n '35',\n '35,000',\n '36',\n '37',\n '3d',\n '3m',\n '3rd',\n '4',\n '40',\n '400',\n '42',\n '43',\n '45',\n '48',\n '4all',\n '4th',\n '5',\n '50',\n '500',\n '55',\n '5g',\n '6',\n '60',\n '600',\n '65',\n '66',\n '6am',\n '6ft',\n '7',\n '70',\n '700',\n '71',\n '72',\n '75',\n '77',\n '7am',\n '7news',\n '8',\n '80',\n '800',\n '85',\n '8am',\n '8p',\n '9',\n '90',\n '900',\n '95',\n '99',\n '9am',\n '_19',\n '_19uk',\n '__19' ]\n\nfor i in corona_Stop:\n    ENGLISH_STOP_WORDS_PUNCTUATION.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorizing\nIn order to compare accuracy, 4 vector representations were created: unigrams, bigrams, threegrams and 1-3grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(lowercase=True,\n                             stop_words=ENGLISH_STOP_WORDS_PUNCTUATION,\n                            tokenizer = casual_tokenize,\n                            min_df = 0.0004,\n                            max_df=0.65)\n\nvectorizer_bigrams = TfidfVectorizer(lowercase=True,\n                             stop_words=ENGLISH_STOP_WORDS_PUNCTUATION,\n                            tokenizer = casual_tokenize,\n                            min_df = 0.0004,\n                            max_df=0.65,\n                            ngram_range=(2,2))\n\nvectorizer_threegrams = TfidfVectorizer(lowercase=True,\n                             stop_words=ENGLISH_STOP_WORDS_PUNCTUATION,\n                            tokenizer = casual_tokenize,\n                            min_df = 0.0004,\n                            max_df=0.65,\n                            ngram_range=(3,3))\n\nvectorizer_13 = TfidfVectorizer(lowercase=True,\n                             stop_words=ENGLISH_STOP_WORDS_PUNCTUATION,\n                            tokenizer = casual_tokenize,\n                            min_df = 0.0004,\n                            max_df=0.65,\n                            ngram_range=(1,3))\n\nrepresentation = vectorizer.fit_transform(tweets).toarray()\nrepresentation2 = vectorizer_bigrams.fit_transform(tweets).toarray()\nrepresentation3 = vectorizer_threegrams.fit_transform(tweets).toarray()\nrepresentation13 = vectorizer_13.fit_transform(tweets).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_bigrams.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_threegrams.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_13.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"unigrams size: \",representation.shape[1])\nprint(\"bigrams size: \",representation2.shape[1])\nprint(\"threegrams size: \",representation3.shape[1])\nprint(\"uni-three grams size: \",representation13.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepating Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(representation,\n                                                 data['Sentiment'])\n\nx_train2,x_test2,y_train2,y_test2 = train_test_split(representation2,\n                                                 data['Sentiment'])\n\nx_train3,x_test3,y_train3,y_test3 = train_test_split(representation3,\n                                                 data['Sentiment'])\n\nx_train13,x_test13,y_train13,y_test13 = train_test_split(representation13,\n                                                 data['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LDA. SVD Solver. Comparing Accuracy on different datasets.\nAccording to result, unigrams showed the best perfomance among all"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model_4 = LinearDiscriminantAnalysis(n_components=4).fit(x_train,\n                                                             y_train)\n\nlda_model_4_predictions = lda_model_4.predict(x_test)\n\nprint(accuracy_score(y_test,lda_model_4_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model_4_bigrams = LinearDiscriminantAnalysis(n_components=4).fit(x_train2,\n                                                             y_train2)\n\nlda_model_4_predictions_bigrams = lda_model_4_bigrams.predict(x_test2)\n\nprint(accuracy_score(y_test2,lda_model_4_predictions_bigrams))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model_4_threegrams = LinearDiscriminantAnalysis(n_components=4).fit(x_train3,\n                                                             y_train3)\n\nlda_model_4_predictions_threegrams = lda_model_4_threegrams.predict(x_test3)\n\nprint(accuracy_score(y_test3,lda_model_4_predictions_threegrams))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\nAmong supervised algorithms were used, Logistic Regression with ElasticNET regularization showed the best perfomance.\nUnfortunately, because of lack of the computational resources and huge dataset size, I can not use complicated model like Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_model_check = LogisticRegression(penalty='elasticnet',solver=\"saga\",l1_ratio=0.9,n_jobs=-1).fit(x_train,y_train)\n\nprint(accuracy_score(logreg_model_check.predict(x_test),y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier().fit(x_train,y_train)\nprint(accuracy_score(rf_model.predict(x_test),y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA. Dimension Reduction\nDimension Reduction methods allow us to interpret multidimenstional data into 2 or 3 dimensions where we can visualize it. Also they allow to avoid \"dimension curse\".\nThese methods use Singular Matrix Decomposition. The Decomposition spreads matrix into 3 other matrix (2 ortodiagonal and 1 symmetrical). The eigenvectors and eigenvalues of this matrix are used to construct new representation. As we can see on plots, even 4000+ dimension data can be representated as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_model = PCA(n_components=2).fit_transform(x_train)\nplt.hexbin(pca_model[:,0],pca_model[:,1],bins='log');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Truncated SVD","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"truncated_model = TruncatedSVD(n_components=2).fit_transform(x_train)\nplt.hexbin(truncated_model[:,0],truncated_model[:,1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}