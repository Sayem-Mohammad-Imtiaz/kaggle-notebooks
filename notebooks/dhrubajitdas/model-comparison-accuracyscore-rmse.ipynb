{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f3489588-f947-8b6d-2eb2-5f017ee52d0b"},"source":"So, in this kernel, I am going to build models by evaluating the accuracy score and the RMSE (root mean squared error) of each model. \nThe final model will be the one which has the highest accuracy score and the lowest RMSE.\n\nFirst, I will compare all default models and select the models which perform best for the TEST data. Then I will tune those models to try to get a better prediction result, only if the default models doesn't give a perfect score.\n\nIn order to divide our dataset into training and testing data, it can be done in 2 ways:\n1. KFold\n2. train_test_split\n\nI will show both."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e90dcdb-9405-e2d3-1d58-7643073e38cb"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nimport warnings\nwarnings.filterwarnings('ignore')\ntry:\n    t_file = pd.read_csv('../input/mushrooms.csv', encoding='ISO-8859-1')\n    print('File load: Success')\nexcept:\n    print('File load: Failed')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d20343d-bdd0-961d-329e-39d7353881d6"},"outputs":[],"source":"#importing necessary modules\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, classification_report, mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.cross_validation import train_test_split"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b55d302-1847-90ea-8ec3-992c104109f4"},"outputs":[],"source":"mushroom = t_file.copy()\nmushroom.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c79885d3-c5dd-9b3b-a0d6-bf421384dc2c"},"outputs":[],"source":"# checking how many labels are available for the 'class'\nmushroom['class'].unique()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67ed60e1-a2fd-7b1c-0c95-1aee84f63b83"},"outputs":[],"source":"#Now using labelencoder, convert the categorical values to numeric\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor col in mushroom.columns:\n    mushroom[col] = le.fit_transform(mushroom[col])\nmushroom.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"046e85d8-b6cc-1a46-ae37-a193508fcfde"},"source":"Here, in the column 'class':\nlabel 'p' --> 1\nlabel 'e' --> 0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf097be7-0994-6292-f34a-8fbf6a28b0b5"},"outputs":[],"source":"X = mushroom.iloc[:,1:23]\ny = mushroom.iloc[:,0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13aa0f36-ca7e-e6b3-c708-680bb1ace7bd"},"outputs":[],"source":"# Prepare default models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA', QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier(n_estimators=1000)))\nmodels.append(('SVM', SVC()))\nmodels.append(('NB', GaussianNB()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ed3dd569-d40f-2bdf-f555-03db6ef62528"},"source":"## Using KFold ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b0e7caa-9bb6-44db-d8cb-657efbf46413"},"outputs":[],"source":"# evaluating each model\nresults = []\nname = []\nfor names, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    mod_results = cross_val_score(model, X,y, cv=kfold, scoring='accuracy')\n    results.append(mod_results)\n    name.append(names)\n    print(\"%s : %f\" %(names, mod_results.mean()))\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(name)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d79c994-9c1d-4332-05f3-65f6633c93b5"},"source":"As we can see, RandomForest is giving perfect classification rate. Followed by DecisionTree, KNN and SVM with almost 100% accuracy."},{"cell_type":"markdown","metadata":{"_cell_guid":"1c3510ee-d817-1725-131b-af699658dff9"},"source":"## using train_test_split ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8994a00-40df-fd53-3a4e-536556761112"},"outputs":[],"source":"result = []\naccuracy = []\nrmse = []\nnames = []\nfor name, model in models:\n    xtrain,xtest,ytrain,ytest = train_test_split(X,y, test_size=0.3, random_state=3)\n    model.fit(xtrain,ytrain)\n    mod_pred = model.predict(xtest)\n    accu = accuracy_score(ytest,mod_pred)\n    error = np.sqrt(mean_squared_error(ytest,mod_pred))\n    accuracy.append(accu)\n    rmse.append(error)\n    names.append(name)\n    a = pd.DataFrame(accuracy)\n    b = pd.DataFrame(rmse)\n    print(\"%s : %f (%f)\" %(name, accu,error))\n#plot - accuracy\nplt.figure(figsize=[7,4])\na.plot(kind='bar', alpha=0.7, color='g', rot=0)\nplt.xticks([0,1,2,3,4,5,6,7], names)\nplt.ylim(0.5,1)\nplt.xlabel('Models')\nplt.ylabel(\"Accuracy Score\")\nplt.legend(\"\")\nplt.show()\n\n#plot - RMSE\nplt.figure(figsize=[7,4])\nb.plot(kind='bar', alpha=0.7, color='g', rot=0)\nplt.xticks([0,1,2,3,4,5,6,7], names)\nplt.ylim(0,0.7)\nplt.xlabel('Models')\nplt.ylabel(\"RMSE\")\nplt.legend(\"\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"66ec9ced-39dd-7726-0244-b7d7cf6f8d42"},"source":"In case of train_test_split, 3 models mentioned in KFold section having perfect/almost-perfect accuracy rates are giving 100% accuracy and 0% rmse, i.e (DT,RF and SVM). Whereas KNN has RMSE of 0.02\n\nSo, for this dataset, these 4 models performs best."},{"cell_type":"markdown","metadata":{"_cell_guid":"a76974dc-4824-87b7-d726-cb7cf7a86d84"},"source":"## Tuning model ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"afdb6641-c9f1-3312-6a63-cda1d4ea0650"},"source":"Since we are getting perfect scores with those 4 default models, we donot need to tune anything. \nYou can select anyone model among those 4 to be your final model.\n\nIn case you want to learn how to tune models, follow the kernel submitted by Niraj Verma. A great kernel to learn and apply GridSearchCV and RandomizedSearchCV to find the best parameters for your model."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}