{"cells":[{"metadata":{},"cell_type":"markdown","source":"Do Lego sets have more specialized parts than they used to?\n===========================================================\n\n![](https://raw.githubusercontent.com/colbeseder/resources/master/lego_mariachi_crate.jpg)\n_Some pretty specializd pieces_\n\nI've recently come back to Lego (ie. I have kids) and it seems to me that modern lego sets have more specialized pieces.\n\nIn my day (ðŸ‘´) we had to use our imaginations to make new things out of generic bricks. Or did we?\n\nHere's my investigation to whether new lego sets really are introducing more specialized bricks than they used to.\n\n\n\n\nAims\n----\n\nTo evaluate whether newer Lego sets introduce more less-useful parts than they did in the '80s. With _useful_ losely defined as \"can be used to make lots of other things\".\n\n### Some notes on the data\n\nI've excluded 2017 as the data was not complete.\nIt's expected that in the first years of Lego (created in 1949), there was a rush of new bricks (likely generic ones).\nI've ignored the colour of the pieces.\n\n### The data\nLet's join some tables and take a look at the basic data we're going to be looking at.\n\nEach row is the first appearance of a Lego piece (ignoring colour).\n* part_num - The distinct part\n* color_id - The piece's orignal colour\n* set_num - The first set that the piece appeared in\n* year - The year that the peice was first released\n* theme_id - The theme that the piece's first set was part of (eg. 250:  _Prisoner of Azkaban_)\n* parent_theme_id - The broader theme (eg. 246: _Harry Potter_)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport pprint, math\nimport matplotlib.pyplot as plt\n\n# Read in the data files\n\n# For the part number:\ninv_parts_data = pd.read_csv(\"../input/lego-database/inventory_parts.csv\")\n\n# Sets and the year they were released\nsets_data = pd.read_csv(\"../input/lego-database/sets.csv\")\n\n# To assist the join between part-number, and year introduced \ninv_data = pd.read_csv(\"../input/lego-database/inventories.csv\")\n\n# We'll need this to track parent themes\ntheme_data = pd.read_csv(\"../input/lego-database/themes.csv\")\n\n# We want to know the type of part\npart_cat_data = pd.read_csv(\"../input/lego-database/parts.csv\")\n\n# Join data into a table mapping part-number (with duplicates) to year\nyears_set_data = pd.merge(inv_data, sets_data,on='set_num')\nyears = years_set_data[['id', 'set_num','year', 'theme_id']]\ndata = pd.merge(inv_parts_data, years, left_on='inventory_id', right_on='id')\ndata = pd.merge(part_cat_data, data, on='part_num')\ndata = data.drop(['id', 'quantity', 'is_spare', 'name'], axis='columns').sort_values(by=['year'])\n\n# Clean up parts that appear in duplicate rows for a single set (each color is listed separately)\ndata = data.drop_duplicates(subset=['part_num', 'set_num'], keep='first')\n\n# Add the parent theme\n\nmemo = {}\ndef get_parent_theme(theme):\n    if theme in memo:\n        return memo[theme]\n    \n    parent = theme_data.loc[theme_data['id'] == theme]['parent_id']\n    if math.isnan(parent):\n        return theme\n    else:\n        parent = int(parent)\n    if parent == theme:\n        r = theme\n    else:\n        r = get_parent_theme(parent)\n    \n    memo[theme] = r\n    return r\n\ndata['parent_theme_id'] = data['theme_id'].apply(lambda x: get_parent_theme(x) )\n\nprint(data.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's organize some of this data by year.\n\nWe can see how many new parts were introduced that year.\n\nFor all the parts that originated in that year: how many sets have those parts appeared in since?\nOn average, how many new parts were in each set?","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Find the year each part was introduced\n\npart_sum = {} # part_num : (sets_containing, year_introduced)\nfor _, inv in data.iterrows():\n    part_num = inv['part_num']\n\n    if part_num in part_sum:\n        s = part_sum[part_num][0] + 1\n        yr = min(part_sum[part_num][1], inv['year'])\n    else:\n        s = 1\n        yr = inv['year']\n    part_sum[part_num] = (s, yr)\n\n\n# Find number of sets released each year\n\nset_releases = {}\nfor _, row in sets_data.iterrows():\n    yr = row['year']\n    if yr in set_releases:\n        set_releases[yr] +=1\n    else:\n        set_releases[yr] = 1\n\n#print(\"New sets released by year:\")\n#pprint.pprint(set_releases)\n\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Parts introduced each year\nyear_part_count = {}\n\n# For each part, number of sets that it appears in, summed per year (ie. sum of set-appearances for all parts originating in year)\npart_appearances_by_year_first_seen = {}\n\nfor part_num in part_sum:\n    yr = part_sum[part_num][1]\n    s = part_sum[part_num][0]\n    \n    if yr in year_part_count:\n        year_part_count[yr] += 1\n        part_appearances_by_year_first_seen[yr] += s\n    else:\n        year_part_count[yr] = 1\n        part_appearances_by_year_first_seen[yr] = s\n\nyears = []\npart_appearances = []\nfor yr in sorted(part_appearances_by_year_first_seen):\n    years.append(yr)\n    part_appearances.append(part_appearances_by_year_first_seen[yr])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfirst_seen = []\nnew_parts = []\nnew_parts_per_set = []\nnew_sets = []\nsets_per_new_part = []\nyear_reuses = {}\nfor yr in year_part_count:\n    if yr == 2017:\n        continue # Seems data was collected mid-year\n    first_seen.append(yr)\n    new_parts.append(year_part_count[yr])\n    new_sets.append(set_releases[yr])\n    sets_per_new_part.append(part_appearances_by_year_first_seen[yr] / year_part_count[yr])\n    year_reuses[yr] = part_appearances_by_year_first_seen[yr] / year_part_count[yr]\n    new_parts_per_set.append(year_part_count[yr] / set_releases[yr])\n\nfirst_seen_parts = pd.DataFrame.from_dict({'year': first_seen, 'new_parts': new_parts, 'new_sets': new_sets, 'sets_per_new_part': sets_per_new_part, 'new_parts_per_set': new_parts_per_set}).sort_values(by=['year'])\n\nprint(first_seen_parts.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Below, we'll see the number of new bricks introduced per new Lego set, by year.\n\nThe colours show the number of new sets that were released that year.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#fig, ax = plt.subplots()\n#first_seen_parts.plot.scatter(x='year',y='new_parts_per_set', c='new_sets', colormap='Wistia', ax=ax);\n\nprint(\"Zoom in on last 35 years\")\n#fig, ax = plt.subplots()\nfirst_seen_parts[-35:].plot.bar(x='year',y='new_parts_per_set');#, c='new_sets', colormap='Wistia', ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There definitely seems to be an increase in average number of new bricks per set, over the last few years.**\n\nI wonder if we can spot a trend of newer bricks being in fewer sets?","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#All time\n#fig, ax = plt.subplots()\n#first_seen_parts.plot.scatter(x='year',y='sets_per_new_part', c='new_sets', colormap='Wistia', ax=ax);\n\nprint(\"Zoom in on last 35 years\")\nfig, ax = plt.subplots()\n#first_seen_parts[-35:].plot.scatter(x='year',y='sets_per_new_part', c='new_sets', colormap='Wistia', ax=ax);\nsns.regplot(x=first_seen_parts[-35:]['year'], y=first_seen_parts[-35:]['sets_per_new_part'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a very steep decline. But it's likely skewed by the fact that newer bricks cannot be included in older sets. \n\nSo now, we'll normalize the plot to show what percentage of sets the brick appeared in, starting from the year it was originated.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_sets = sum(new_sets)\nt = 0\ntotal_sets = []\n\nfor x in new_sets:\n    t += x\n    total_sets.append(all_sets - t)\n\nfirst_seen_parts['sets_to_be_released'] = total_sets\nfirst_seen_parts['percent_of_sets_containing_part'] = first_seen_parts['sets_per_new_part'] / first_seen_parts['sets_to_be_released'] * 100\n\n#print(first_seen_parts)\nfig, ax = plt.subplots()\n\nprint(\"Decline in reuse of new bricks\")\nprint(\"Zoom in on last 35 years\")\nfirst_seen_parts[-35:].plot.scatter(x='year',y='percent_of_sets_containing_part', c='sets_per_new_part', colormap='Wistia', ax=ax);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's definitely a downward trend in the reuse of new bricks from the 80's to around 2007. But this is followed by a small bounce back.  This could be a blip, or the start of a climb back to the top.\n\nI'm interested to see if this bounce-back is related to themes.\n\n**Is the increase in reusability over the last 10 years cross-theme usability?** Or are these pieces only relevant to their themes?","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# For each part, let's count how many themes it appeared in\n\nthemes_for_part = {} # part_num : list of themes\nparent_themes_for_part = {} # part_num : list of parent themes\nfor _, inv in data.iterrows():\n    part_num = inv['part_num']\n    theme = inv['theme_id']\n    parent_theme = get_parent_theme(theme)\n    if part_num not in themes_for_part:\n        themes_for_part[part_num] = [theme]\n        parent_themes_for_part[part_num] = [parent_theme]\n    else:\n        if theme not in themes_for_part[part_num]:\n            themes_for_part[part_num].append(theme)\n        if parent_theme not in parent_themes_for_part[part_num]:\n            parent_themes_for_part[part_num].append(parent_theme)\n\n\nthemes_for_year = {} # year: themes * parts\nparent_themes_for_year = {} # year: themes * parts\nfor part_num in part_sum:\n        yr = part_sum[part_num][1]\n        themes = len(themes_for_part[part_num])\n        parent_themes = len(parent_themes_for_part[part_num])\n        if yr in themes_for_year:\n            themes_for_year[yr] += themes\n            parent_themes_for_year[yr] += parent_themes\n        else:\n            themes_for_year[yr] = themes\n            parent_themes_for_year[yr] = parent_themes\nyears = []\ntheme_count = []\nparent_theme_count = []\nfor yr in sorted(themes_for_year):\n    if yr == 2017:\n        continue\n    years.append(yr)\n    theme_count.append(themes_for_year[yr])\n    parent_theme_count.append(parent_themes_for_year[yr])\n\nfirst_seen_parts['themes_per_part'] = theme_count / first_seen_parts['new_parts']\nfirst_seen_parts['parent_themes_per_part'] = parent_theme_count / first_seen_parts['new_parts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#print(first_seen_parts.head())\n\n# Plot parent themes - very similar results to themes\n#sns.regplot(x=first_seen_parts[-35:]['year'], y=first_seen_parts[-35:]['parent_themes_per_part'])\n\nsns.regplot(x=first_seen_parts[-35:]['year'], y=first_seen_parts[-35:]['themes_per_part'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear from this that the number of cross theme parts is dropping. Pieces introduced more recently are, on average, in less than 2 themes. Plotting against parent themes shows the same picture.","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# An Aside: how many themes per parent (root) theme?\nthemes_per_parent = {}\nfor t in memo:\n    p = memo[t]\n    if p in themes_per_parent:\n        themes_per_parent[p] +=1\n    else:\n        themes_per_parent[p] = 1\n\npparents = []\npcount = []\nfor p in sorted(themes_per_parent):\n    pparents.append(p)\n    pcount.append(themes_per_parent[p])\n\ndf = pd.DataFrame.from_dict({'parent_theme': pparents, 'themes_count': pcount})\ndf.plot.bar(x='parent_theme',y='themes_count');\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nConclusion\n----------\n\nI got my first lego kit in 1987. As I suspected, new sets are contain more new bricks shapes than they did back then. There's also been a decline in reuse of the bricks that have been \"invented\" during that time. The slight bounce back in the last few years seems to be related to pieces that are used in multiple sets, but only one theme. \n\nAs a further investigation, it would be interesting to investigate what effect we would see from including colour in this study. I'd also be interested to if we could predict the likely future reuse of a new piece.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Appendix\n--------\n\nHere's my attempt at a model predicting future use of a new brick. How accurately can we predict the expected number of sets that will contain a piece, based on its:\n* part category\n* First colour\n* Year of release\n* Theme\n* Parent theme\n\nI've excluded Typical ammount of reuse for a brick from that year ( _reuse\\_for\\_year_ )\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try to make some predictions\n\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\n# We want to make a prediction based only on the information that's available at part origination\npart_data = data.copy()\npart_data = part_data[part_data.year < 2017 ]\n\npart_data = part_data.drop(['inventory_id', 'set_num'], axis='columns').sort_values(by=['part_num'])\npart_data = part_data.drop_duplicates(subset=['part_num'], keep='first')\n    \npart_data['reuse_for_year'] = part_data['year'].apply(lambda yr: year_reuses[yr] )\n\npart_data['uses'] = part_data['part_num'].apply(lambda x: part_sum[x][0] )\n\n# We'll class pieces with over 30 reuses as \"high use\" and bunch them together\npart_data['uses'] = part_data['uses'].clip(1, 31)\n\npart_data.reset_index(drop=True, inplace=True)\nprint(part_data.head())\n\n# Convert part categories, colours, themes and parent themes to One Hot Encoding\n\ndef convert_to_one_hot(df, key, enc):\n    enc_df = pd.DataFrame(enc.fit_transform(df[[key]]).toarray())\n    df = pd.concat([df, enc_df], axis=1)\n    df = df.drop([key], axis='columns')\n    return df\n\n\nenc = OneHotEncoder()\nfor k in ['part_cat_id', 'color_id', 'theme_id', 'parent_theme_id']:\n    part_data = convert_to_one_hot(part_data, k, enc)\n\n# Randomize row order before splitting\npart_data = shuffle(part_data, random_state=3)\n\ny = part_data['uses']\n\npart_data = part_data.drop(['uses', 'part_num'], axis='columns')\nX = part_data\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=2)\n\nprint(\"%s rows of training data\"%(len(train_y)))\nprint(\"%s rows of validation data\"%(len(val_y)))\n# Define and fit the model.\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(train_X, train_y)\n\n# Make predictions on the validation data\npredictions = rf_model.predict(val_X)\n\n# Calculate the error of the predictions\nrf_val_mae = mean_absolute_error(val_y, predictions)\nrf_val_rmse = mean_squared_error(val_y, predictions)\n\nprint(\"\\n\")\nprint(\"Validation RMSE: {}\".format(rf_val_rmse))\n\nprint(\"\\n\\n** The Mean Average Error on the validation set is {}! **\".format(rf_val_mae))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><br><br><br><br><br><br><br><br><br>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}