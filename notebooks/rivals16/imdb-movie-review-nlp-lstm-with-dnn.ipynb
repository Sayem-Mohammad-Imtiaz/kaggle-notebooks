{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing necessary libraries\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Data\nAs we can see our data is distributed evenly 25k positive reviews and 25k negative reviews count plot is shown in the figure.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nnegative = len(dataset[dataset['sentiment']=='positive'])\npositive = len(dataset) - negative\nsns.countplot(dataset['sentiment'])\nprint('Positive reviews are {} and negative reviews are {} of total {} '.format(positive,negative,len(dataset)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the labels positve and negative as 1,0 so that they can be fed to the neural network to predict whether the given review is a positive or negative. Splitting of data 80% for the training and remaining 20% for testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ntraining_reviews,testing_reviews,training_labels,testing_labels  = train_test_split(dataset['review'].values,dataset['sentiment'].values,test_size = 0.2)\ntraining_labels = le.fit_transform(training_labels)\ntesting_labels = le.fit_transform(testing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Processing The Text\nUsing tokenizer to produce token for a given word and taking maximum length of 200 character of a review and after we simply truncate the input review and then padded the input to max len of 200. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000,oov_token='<OOV>')\ntokenizer.fit_on_texts(training_reviews)\nword_index = tokenizer.word_index\ntraining_sequence = tokenizer.texts_to_sequences(training_reviews)\ntesting_sequence = tokenizer.texts_to_sequences(testing_reviews)\ntrain_pad_sequence = pad_sequences(training_sequence,maxlen = 200,truncating= 'post',padding = 'pre')\ntest_pad_sequence = pad_sequences(testing_sequence,maxlen = 200,truncating= 'post',padding = 'pre')\nprint('Total Unique Words : {}'.format(len(word_index)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using glove vectors for embedding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_words = {}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt') as file:\n    for line in file:\n        words, coeff = line.split(maxsplit=1)\n        coeff = np.array(coeff.split(),dtype = float)\n        embedded_words[words] = coeff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1,200))\nfor word, i in word_index.items():\n    embedding_vector = embedded_words.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating The Model\nlayer1: Embedding Layer using glove weights \n\nlayer2: Using a Bidirectional LSTM\n\nlayer3: A dropout Layer\n\nlayer4: A Dense layer of 256 neurons with 'relu' activation\n\nlayer5: A Dense Layer of 128 neurons with 'relu' activation\n\nlayer6: Again a dropout layer. \n\nlayer7: Sigmoid activation layer to classify it positive and negative.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([tf.keras.layers.Embedding(len(word_index) + 1,200,weights=[embedding_matrix],input_length=200,\n                            trainable=False),\n                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n                             tf.keras.layers.Dropout(0.8),\n                             tf.keras.layers.Dense(256,activation = 'relu',),\n                             tf.keras.layers.Dense(128,activation = 'relu'),\n                             tf.keras.layers.Dropout(0.8),\n                             tf.keras.layers.Dense(1,activation = tf.nn.sigmoid)])\nmcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = tf.keras.losses.BinaryCrossentropy() , optimizer='Adam' , metrics = 'accuracy')\nhistory = model.fit(train_pad_sequence,training_labels,epochs = 30, validation_data=(test_pad_sequence,testing_labels),\n                   callbacks=[mcp_save])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.Model.save_weights(model, filepath='weight.hdf5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Accuracy and Losses","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend(loc=0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy: {}'.format(max(acc)))\nprint('Validation Accuracy: {}'.format(max(val_acc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n1 - We have great accuracy and we can increase it training for much longer and tune other hyperparameters\n2 - DNN LSTM have a deep impact on NLP problems and we can see that this model performs quite well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}