{"cells":[{"metadata":{"_uuid":"194421db-5ea5-4411-9b6d-8a59f6f58dc1","_cell_guid":"ed5a75a5-0aba-4646-9892-3d0569a80a3c","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization \n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ndf= pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_train.csv\")\ndf.head(10)\ndf.shape\ndf.info()\n\n\ndf.isna().sum() # so lots of missing values are there","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## Lets start by handling missing values\ndf['gender'].loc[df['gender'].isnull()==True]='Undefined' #since nan values is really high we will treat it as a seperate category\ndf.gender.value_counts()\ndf.gender.isna().sum()\n\n\n\ndf.enrolled_university .value_counts()\ndf.enrolled_university .isna().sum()\ndf['enrolled_university'].loc[df['enrolled_university'].isnull()==True]='no_enrollment'  \n#Here the nan values are small so we just add it to majority class\n\n\n\n\ndf.education_level.value_counts()\ndf.education_level.isna().sum()\ndf['education_level'].loc[df['education_level'].isnull()==True]='Graduate' \n# the nan values are very small so we will just add into graduate category\n\n\n\ndf.major_discipline.value_counts()\ndf['major_discipline'].loc[df['major_discipline'].isnull()==True]='STEM'\ndf.major_discipline.isna().sum()\n\n\n\n\ndf.experience.replace('>20','22',inplace=True) # replacing special chars(like >,+) with numbers\ndf.experience.replace('<1','0',inplace=True)\ndf.experience=pd.to_numeric(df.experience)\ndf['experience']=np.where(df['experience']>10,'Senior-level', np.where(df['experience']>3,'Intermediate-level' ,'Entry-Level'))\n# here we are creating a class interval for each level of experience\ndf['experience'].value_counts().sum() \n\n\n\ndf.company_size.replace('<10','9',inplace=True) # replacing special chars(like >,+) with numbers\ndf.company_size.replace('10/49','20',inplace=True)\ndf.company_size.replace('50-99','55',inplace=True)\ndf.company_size.replace('100-500','300',inplace=True)\ndf.company_size.replace('10000+','10001',inplace=True)\ndf.company_size.replace('500-999','600',inplace=True)\ndf.company_size.replace('5000-9999','6000',inplace=True)\ndf.company_size.replace('1000-4999','3000',inplace=True)\ndf.company_size= pd.to_numeric(df.company_size)\ndf['company_size']=np.where(df['company_size']>2000,'Large-org.', np.where(df['company_size']>1,'Small & Medium-org.','Undefined'))\n# here we are creating a class interval for various company sizes\ndf['company_size'].value_counts()\n\n\ndf.company_type.value_counts()\ndf.company_type.isna().sum()\ndf['company_type'].loc[df['company_type'].isnull()==True]='Pvt Ltd'\n\n\n\n\ndf.last_new_job.value_counts()      \ndf.last_new_job.replace('>4','5',inplace=True)\ndf.last_new_job.replace('never','0',inplace=True)\ndf.last_new_job.fillna(1,inplace=True)    #Replace with majority category of 1 year diff\ndf.last_new_job=df.last_new_job.astype(int)\n\ndf.isna().sum() # no column has nan value left we can proceed now","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## Time to encode the categories\n\ncategorical_cols=[name for name in df.columns if df[name].dtype not in ['int','float']]  \ncategorical_cols\n#fetches categorical features needed to be encoded\n\n\n\nnew_df=df.copy() #lets make a new df to which we want to make changes\n\nnew_df.relevent_experience= new_df.relevent_experience.map({\"Has relevent experience\":1,\"No relevent experience\":0})\n\n\n\n#Ordinal encoder because these categories follow a rank like 1st,2nd,3rd etc\nfrom sklearn.preprocessing import OrdinalEncoder\nenc_ordered_cat=OrdinalEncoder(categories=[['Undefined',\"Small & Medium-org.\",'Large-org.'],[\"Entry-Level\",\"Intermediate-level\" ,'Senior-level'],['Primary School','High School',\"Graduate\",'Masters','Phd']])\nordinal_feat=enc_ordered_cat.fit_transform(new_df[['company_size','experience','education_level']])\nenc_ordered_cat.categories_\n\n\n\n#OneHot encode because these categories are independent of each other\none_hot_cat=new_df[['gender','major_discipline', 'company_type','enrolled_university']]\none_hot=pd.get_dummies(one_hot_cat)\n\n\nnew_df.drop([\"company_type\",\"gender\",\"major_discipline\",\"enrolled_university\"],axis=1,inplace=True)\nnew_df.drop(['company_size','experience','education_level'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Combining all dataframes to get a final df"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfull_df=pd.concat([new_df,one_hot],axis=1)\n\n\nordinal_feat_df= pd.DataFrame(data=ordinal_feat,dtype='int32')      #converting the np array of ordinal encoded feat to dataframe\nordinal_feat_df.columns=['company_size','experience','education_level']\n\nall_df= pd.concat([full_df,ordinal_feat_df],axis=1) # here is the final dateframe that we are going to work with \nall_df.head() \n\n\nall_df.drop(['enrollee_id', 'city'],inplace=True,axis=1)\nall_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation of features with target class "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import scale  #training hours can have a magnitude as it shows relatively high numbers\n\nall_df['training_hours']=scale(all_df['training_hours'])     #scales down to unit variance\nall_df['training_hours']= np.floor(all_df['training_hours'])\n\nall_df.corr()['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treating imbalanced targets\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=all_df['target']\nX=all_df.drop('target',axis=1)\n\n\ny.value_counts()   # the number people that will change job are way less than those who wont -DATA IS IMBALANCED\nsns.countplot(y) \n\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\n\nrand=RandomOverSampler(random_state=42)\nx_ros, y_ros = rand.fit_resample(X, y)\nprint(f\"Imbalanced target class: {Counter(y)} Balanced target class:{Counter(y_ros)}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and testing data  split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x_ros,y_ros,test_size=0.3,shuffle=True)\n#make sure shuffle is set to true because we dont want to get data belonging to one class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier  # In my opinion RF will be ideal as it we have many feat like yes or no questions which can be use in decision making \nrf=RandomForestClassifier()\nrf.get_params() #fetches rf params\n\nrf.fit(X_train,y_train)\n\npredictions= rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets see how well our model performed!"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,classification_report\nprint(f\"Roc-Auc score: {roc_auc_score(y_test,predictions)},f1_score: {f1_score(y_test,predictions)},Accuracy: {accuracy_score(y_test,predictions)}\")\nprint(classification_report(y_test,predictions))\n# The precision-recall, accuracy scores are very promising but wait until we cross check it with the validation set\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score,StratifiedKFold        # lets validate our val_accuracy\nskfold = StratifiedKFold(n_splits=3, random_state=42,shuffle=True)\nscores=cross_val_score(rf,X_test,y_test,cv=skfold)\nprint(\"best score:{:.3f}\".format(np.mean(scores)))              # validation looks good","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing on new testset"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.to_csv(\"all_test.csv\")\nnew_test= pd.read_csv('./all_test.csv')\nnew_test_X=new_test.drop([\"Unnamed: 0\",'target'],axis=1)\nnew_test_y=new_test[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=rf.predict(new_test_X) \n#fitting our random forest into new test data and geting the probability of candidate leaving or staying \npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,classification_report\nprint(f\"Roc-Auc score: {roc_auc_score(new_test_y,pred)},f1_score: {f1_score(new_test_y,pred)},Accuracy: {accuracy_score(new_test_y,pred)}\")\nprint(classification_report(new_test_y,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score,StratifiedKFold  # lets validate our val_accuracy\nskfold = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\nscores=cross_val_score(rf,new_test_X,new_test_y,cv=skfold)\nprint(\"best score:{:.3f}\".format(np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n# save your precious model :)\nfilename = 'finalized_model.sav'\npickle.dump(rf, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Please upvote this so i can make moreof these. Cheers!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}