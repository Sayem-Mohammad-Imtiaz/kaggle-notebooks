{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Forecast of Australian beer production\nFor the purpouse of AI Tech Trainee time-series session","metadata":{"ExecuteTime":{"end_time":"2020-05-26T16:14:59.179251Z","start_time":"2020-05-26T16:14:59.173269Z"}}},{"cell_type":"markdown","source":"## Forecast of Australian beer production \n\n**The Story**\n- Lets go back in time and space. It is as last date of the year 1969 and you live in Australia.\n- You're a passionate beer drinker and you made a bet with your friends that you're going to drink one milionth of Australian overall beer production each month for the next 24 months. You can choose one friend to help you with the consumption. \n- If you succeed, all the bills will be covered for you. If you fail on any month, you'll need be be sober for the next 24 months.\n- Now, you woke up on New Year's Eve with a headache and imediatelly remembered your yesterday bet.\n- Luckily, you have an access to the relevant data (from 1956-1969), PC, and a strong thirst.\n- Do you have any chance of winning the bet? How much beer do you need to drink each month?\n\n**TL;DR story:**\n- you have monthly data on Australian beer production starting 1956 till 1969 (180 months)\n- you want to create a forecast for years 1970-1971 (24 months)\n\n\n**Plan:**\n- inspect the data\n- explore patterns in the historical data\n- make predictions using:\n    - Naive models\n    - Exponentials Smoothing models\n    - tuned Exponentials Smoothing models","metadata":{}},{"cell_type":"markdown","source":"\n![img1](https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/australian-beer-tap-gej-jones.jpg)\n","metadata":{}},{"cell_type":"code","source":"# Source libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nimport warnings\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = 15, 7","metadata":{"ExecuteTime":{"end_time":"2020-06-01T23:13:20.320011Z","start_time":"2020-06-01T23:13:20.307046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data\n- Dataset: Australian beer production on 1956 - 1995","metadata":{}},{"cell_type":"code","source":"df_raw = pd.read_csv(\"/kaggle/input/time-series-datasets/monthly-beer-production-in-austr.csv\", \n                   parse_dates=['Month'], index_col='Month')\ndf_raw.columns = ['Production']\n\n# Select only data till 1970 (including)\ndf = df_raw.loc[df_raw.index < '1970']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Inspection\n","metadata":{}},{"cell_type":"code","source":"df","metadata":{"ExecuteTime":{"end_time":"2020-06-01T23:13:20.669081Z","start_time":"2020-06-01T23:13:20.658107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"ExecuteTime":{"end_time":"2020-06-01T23:13:20.698Z","start_time":"2020-06-01T23:13:20.673067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# full time series\ndf.plot(figsize=(15, 7))\nplt.title('Beer Production')\nplt.ylabel('megalitres')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data looks fine, no missing values, no outliers -> we can proceed with modeling.","metadata":{}},{"cell_type":"markdown","source":"## Time Series Decomposition\n\nHere we're going to use a classical decomposition approach: \n1. compute trend using moving average  (-> loose first and last 6 months)\n2. detrend data  (subtract trend line from the data)\n3. compute average seasonal effect of each month\n\nor use `seasonal_decompose` function that does it for you.\n\nMore on the topic here: https://otexts.com/fpp2/classical-decomposition.html","metadata":{}},{"cell_type":"code","source":"# Compute centred moving averages\ndf_ma = df.rolling(window=12, center=True).mean()\n\n# Plot including moving average (orange line)\nplt.figure(figsize=(15, 7))\nplt.plot(df)\nplt.plot(df_ma)\nplt.title('Beer Production')\nplt.ylabel('megalitres')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detrended data\ndetrended_df = df - df_ma\n\nplt.figure(figsize=(15, 7))\nplt.plot(detrended_df)\nplt.title('Beer Production')\nplt.ylabel('megalitres')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Yearly seasonality plot\ndetrended_df['year'] = detrended_df.index.year\ndetrended_df['month'] = detrended_df.index.month\ndf_pivot = pd.pivot_table(detrended_df, values='Production', index='month', columns='year', aggfunc='mean')\ndf_pivot.plot(figsize=(15, 7))\nplt.legend().remove()\nplt.xlabel('Month')\nplt.ylabel('Beer Production')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2020-06-01T23:13:22.581964Z","start_time":"2020-06-01T23:13:22.178052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average seasonal effect computed\ndetrended_df.groupby('month')['Production'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Or we can do the same thing bit more easily with these few lines\ndecomposition = seasonal_decompose(df['Production'], freq=12, model='additive')\n\ndecomposition.plot()\nplt.show();","metadata":{"ExecuteTime":{"end_time":"2020-06-01T23:13:22.174055Z","start_time":"2020-06-01T23:13:21.219607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Forecasting models\n- So far we only looked in to the past, trying to see its patterns\n- Now, we'll need to focus on using the past information to predict the future months","metadata":{}},{"cell_type":"code","source":"# Basic prediction settings\nseasonal_period = 12     # what is the length of seasonal cycle\nprediction_horizon = 24  # how many months to predict\n\nprediction_index = pd.date_range(\"1970-01-01\", periods=prediction_horizon, freq=\"MS\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Naïve average model   \n- **average all values**","metadata":{}},{"cell_type":"code","source":"predictionN1 = df.mean()\npredictionN1 = predictionN1.repeat(prediction_horizon).to_frame()\npredictionN1.index = prediction_index\n\nplt.plot(df, label='training')\nplt.plot(predictionN1, label='prediction')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Simple Naïve model – random walk  ()\n- **use latest observation**","metadata":{}},{"cell_type":"code","source":"predictionN2 = df.iloc[-1]\npredictionN2 = predictionN2.repeat(prediction_horizon).to_frame()\npredictionN2.index = prediction_index\n\nplt.plot(df, label='training')\nplt.plot(predictionN2, label='prediction')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Naïve seasonal model\n- **repeat values 12 months back**","metadata":{}},{"cell_type":"code","source":"predictionN3 = df.iloc[-seasonal_period:]\npredictionN3 = predictionN3.append(predictionN3)\npredictionN3.index = prediction_index\n\nplt.plot(df, label='training')\nplt.plot(predictionN3, label='prediction')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exponential smoothing models\n\n- Starting from simple versions of the model to more complex ones:\n  - Simple Exponential Smoothing\n  - Double Exponential Smoothing\n  - Tripple Exponential Smoothing  (a.k.a. Holt-Winters model)\n\nNo parameter tuning at this stage.","metadata":{}},{"cell_type":"code","source":"# Simple Exponential Smoothing  (level only)\nexp_model_1 = (ExponentialSmoothing(df,\n                                   trend=None, \n                                   seasonal=None,\n                                   seasonal_periods=None)\n               .fit(smoothing_level=0.3))\n\npredictionE1 = exp_model_1.forecast(prediction_horizon)\n\nplt.plot(df)\nplt.plot(predictionE1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Double Exponential Smoothing  (level + trend)\nexp_model_2 = (ExponentialSmoothing(df,\n                                   trend='add', \n                                   seasonal=None,\n                                   seasonal_periods=None)\n               .fit(smoothing_level=0.3,\n                    smoothing_trend=0.3))\n\npredictionE2 = exp_model_2.forecast(prediction_horizon)\n\nplt.plot(df)\nplt.plot(predictionE2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Triple Exponential Smoothing  (level + trend + seasonality)\nexp_model_3 = (ExponentialSmoothing(df,\n                                   trend='add', \n                                   seasonal='add',\n                                   seasonal_periods=seasonal_period).\n               fit(smoothing_level=0.3,\n                   smoothing_trend=0.3,\n                   smoothing_seasonal=0.3))\n\npredictionE3 = exp_model_3.forecast(prediction_horizon)\n\nplt.plot(df)\nplt.plot(predictionE3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parameter tuning and error measures (accuracy)\n- Instead of manually tweaking the parameters of Exponential smoothing, you better automate the process of choosing the best parameter\n- First we need to select some data for the tuning (e.g. last 12 or 24 months)","metadata":{}},{"cell_type":"code","source":"tuning_test_period = 12\n\n# Test-train data split\ndf_train = df[df.index < '1969']\ndf_test = df[df.index >= '1969']\n\nplt.plot(df_train)\nplt.plot(df_test, 'g-')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple Exponential Smoothing  (level only) with Parameter tuning\n\nsmoothing_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n\nplt.plot(df_train)\nplt.plot(df_test, 'g-')\n\nfor i in smoothing_levels:\n    exp_model_t1 = (ExponentialSmoothing(df_train,\n                                       trend=None, \n                                       seasonal=None,\n                                       seasonal_periods=None)\n                   .fit(smoothing_level=i))\n\n    predictionEt1 = exp_model_t1.forecast(tuning_test_period)\n\n    plt.plot(predictionEt1)\n    \n    print(f'smoothing_levels={i}  -  {np.sqrt(mean_squared_error(df_test, predictionEt1))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple Exponential Smoothing  (level only)  -  USE tuned parameters\nexp_model_1 = (ExponentialSmoothing(df,\n                                   trend=None, \n                                   seasonal=None,\n                                   seasonal_periods=None)\n               .fit(smoothing_level=0.1))\n\npredictionET1 = exp_model_1.forecast(prediction_horizon)\n\nplt.plot(df)\nplt.plot(predictionET1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Double Exponential Smoothing  (level only) with Parameter tuning\n\nsmoothing_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\nsmoothing_trends = [0.1, 0.3, 0.5, 0.7, 0.9]\n\nplt.plot(df_train)\nplt.plot(df_test, 'g-')\n\nfor i in smoothing_levels:\n    for j in smoothing_trends:\n        exp_model_t2 = (ExponentialSmoothing(df_train,\n                                           trend='add', \n                                           seasonal=None,\n                                           seasonal_periods=None)\n                       .fit(smoothing_level=i,\n                           smoothing_trend=j))\n\n        predictionEt2 = exp_model_t2.forecast(tuning_test_period)\n\n        plt.plot(predictionEt2)\n\n        print(f'level={i} & trend={j}  -  {np.sqrt(mean_squared_error(df_test, predictionEt2))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Double Exponential Smoothing  (level + trend)  -  USE tuned parameters\nexp_model_T2 = (ExponentialSmoothing(df,\n                                   trend='add', \n                                   seasonal=None,\n                                   seasonal_periods=None)\n               .fit(smoothing_level=0.1,\n                    smoothing_trend=0.5))\n\npredictionET2 = exp_model_T2.forecast(prediction_horizon)\n\nplt.plot(df)\nplt.plot(predictionET2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Triple Exponential Smoothing  (level only) with Parameter tuning\n\nsmoothing_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\nsmoothing_trends = [0.1, 0.3, 0.5, 0.7, 0.9]\nsmoothing_seasonal = [0.1, 0.3, 0.5, 0.7, 0.9]\n\nplt.plot(df_train)\nplt.plot(df_test, 'g-')\n\nfor i in smoothing_levels:\n    for j in smoothing_trends:\n        for k in smoothing_seasonal:\n            exp_model_t3 = (ExponentialSmoothing(df_train,\n                                               trend='add', \n                                               seasonal='add',\n                                               seasonal_periods=seasonal_period)\n                           .fit(smoothing_level=i,\n                               smoothing_trend=j,\n                               smoothing_seasonal=k))\n\n            predictionEt3 = exp_model_t3.forecast(tuning_test_period)\n\n            plt.plot(predictionEt3)\n\n            print(f'level={i} & trend={j} & seasonal={k} -  {np.sqrt(mean_squared_error(df_test, predictionEt3))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Triple Exponential Smoothing  (level + trend + seasonality)  -  USE tuned parameters\nexp_model_T3 = (ExponentialSmoothing(df,\n                                   trend='add', \n                                   seasonal='add',\n                                   seasonal_periods=seasonal_period).\n               fit(smoothing_level=0.7,\n                   smoothing_trend=0.5,\n                   smoothing_seasonal=0.5))\n\npredictionET3 = exp_model_T3.forecast(prediction_horizon)\n\nplt.plot(df)\nplt.plot(predictionET3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final predictions\n- on the tuning test data we could see that the Tripple exponential smoothing were doing the best from the Exponential smoothing model family. Therefore, we can assume that their predictions will be the most accurate also for future months (1970 and further)","metadata":{}},{"cell_type":"code","source":"predictionET3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\n- now we have a good idea about the future of Australian beer production\n- do you think you would stand a chance to the bet? \n\nFast forward to the year 1972, this is how the beer production developed.\n\n### Now we can check if our predictions back in 1969 were any good","metadata":{}},{"cell_type":"code","source":"# full time series\ndf_raw.loc[df_raw.index < '1972'].plot(figsize=(15, 7))\npredictionET3.plot()\nplt.title('Beer Production')\nplt.ylabel('megalitres')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final prediction evaluation\ndf_test = df_raw[(df_raw.index >= '1970-01-01') & (df_raw.index <= '1971-12-31')]\n\nprint(\"NAIVE MODELS\")\nfor pred in [predictionN1, predictionN2, predictionN3]:\n    print(f'RMSE={np.sqrt(mean_squared_error(df_test, pred))}, MAPE={mean_absolute_percentage_error(df_test, pred)}')   \n\nprint(\"\\nEXPONENTIAL SMOOTHING MODELS\")\nfor pred in [predictionE1, predictionE2, predictionE3]:\n    print(f'RMSE={np.sqrt(mean_squared_error(df_test, pred))}, MAPE={mean_absolute_percentage_error(df_test, pred)}')   \n\nprint(\"\\nTUNED EXPONENTIAL SMOOTHING MODELS\")\nfor pred in [predictionET1, predictionET2, predictionET3]:\n    print(f'RMSE={np.sqrt(mean_squared_error(df_test, pred))}, MAPE={mean_absolute_percentage_error(df_test, pred)}')   \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# full time series\ndf_raw.plot(figsize=(15, 7))\npredictionET3.plot()\nplt.title('Beer Production')\nplt.ylabel('megalitres')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n![img2](https://i.dailymail.co.uk/1s/2020/02/18/23/24907772-0-image-a-16_1582069652685.jpg)","metadata":{}},{"cell_type":"markdown","source":"## What else could be done to improve predictions\n- different parameter choices (of Exponential models), also trying multiplicative trend and/or seasonality\n- different test data length, or better do a cross validation (e.g. https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection)\n- try different models (e.g. SARIMA, FB Prophet)\n- and many more...","metadata":{}}]}