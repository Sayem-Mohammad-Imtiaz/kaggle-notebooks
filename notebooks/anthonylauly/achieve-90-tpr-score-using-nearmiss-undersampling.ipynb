{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install seaborn==0.11.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries needed"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Basic Libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n#Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Ensemble Model Library\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n\n#Evaluation Library\nfrom sklearn import metrics\n\n#Imbalanced Libraries\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Find Insight From Dataset"},{"metadata":{},"cell_type":"markdown","source":"Lets load the dataset, drop all Naive_Bayes Column because those aren't used, and see the small sample of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\ndataset.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the descriptive statistics of our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From our descriptive statistics, we can see that the scale of our data is not the same for all features. But we won't be bothered, because the model that we are trying to trained is logical based which is not sensitive to input's scale. Another thing that could be noticed is if we are looking at features where currency is involved, the std deviation value tends to be bigger (sometimes even surpassed the mean value).\n\nNow lets visualize each feature distribution in respect to our target which is Attrited Customer or Existing Customer. For continuous feature I prefer to plot the kde (kernel density estimation) while for discrete feature I use histogram plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Customer_Age\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=dataset, x=\"Gender\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=dataset, x=\"Dependent_count\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xticks(rotation = 315)\nsns.countplot(data=dataset, x=\"Education_Level\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=dataset, x=\"Marital_Status\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xticks(rotation=315)\nsns.countplot(data=dataset, x=\"Income_Category\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=dataset, x=\"Card_Category\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Months_on_book\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=dataset, x=\"Total_Relationship_Count\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=dataset, x=\"Months_Inactive_12_mon\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=dataset, x=\"Contacts_Count_12_mon\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Credit_Limit\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Total_Revolving_Bal\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Avg_Open_To_Buy\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Total_Amt_Chng_Q4_Q1\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Total_Trans_Amt\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Total_Trans_Ct\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Total_Ct_Chng_Q4_Q1\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=dataset, x=\"Avg_Utilization_Ratio\", hue=\"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, some features have (almost) normal distribution like *Total_Ct_Chng_Q4_Q1* and *Total_Trans_Amt*, some have Poisson distribution, some have other type of distribution.\n\nNow lets plot the correlation matrix to get another insight."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = dataset.drop('CLIENTNUM', axis=1).corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr, mask=mask, cmap='BrBG', vmin=-1, vmax=1, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation matrix we could see some features are correlated between each other, like *Avg_Open_To_Buy* is highly dependent/correlated to *Credit Limit*, well if we take a look at the definition it is not a shocking fact. It is surprise me that *Months_on_book* is quite correlate with the *Customer_Age*. Well, for instance we could drop one of those correlated feature and just pick one of them because correlated features are just redundant information. But for this notebook, I will keep all those features. I will deal with correlated features for further investigation."},{"metadata":{},"cell_type":"markdown","source":"# Preprocess Stage"},{"metadata":{},"cell_type":"markdown","source":"Now, lets one-hot encode the categorical features using *get_dummies* by *pandas*  "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.concat([dataset.drop(['Dependent_count', 'Marital_Status', 'Income_Category', 'Card_Category', \n                                   'Education_Level'], axis=1), pd.get_dummies(dataset.Dependent_count), \n                     pd.get_dummies(dataset.Marital_Status), pd.get_dummies(dataset.Income_Category), \n                     pd.get_dummies(dataset.Card_Category), pd.get_dummies(dataset.Education_Level)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Gender'] = dataset.Gender.map({'M':1, 'F':0})\ndataset['Attrition_Flag'] = dataset.Attrition_Flag.map({'Existing Customer':0, 'Attrited Customer':1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seperate our features from our target, and drop unuse feature like *CLIENTNUM* which is just an id in database."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset.Attrition_Flag\nX = dataset.drop(['Attrition_Flag', 'CLIENTNUM'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets rename our feature's columns. This is neccesary because we got similar name for several features caused by one-hot encoded process"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns = ['Customer_Age','Gender', 'Months_on_book', 'Total_Relationship_Count','Months_Inactive_12_mon',\n             'Contacts_Count_12_mon','Credit_Limit','Total_Revolving_Bal','Avg_Open_To_Buy','Total_Amt_Chng_Q4_Q1',\n             'Total_Trans_Amt', 'Total_Trans_Ct','Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio', 0, 1, 2, 3, 4, 5,\n             'Divorced','Married', 'Single', 'Marriage_Unknown','$120K +', '$40K - $60K', '$60K - $80K','$80K - $120K',\n             'Less than $40K','Income_Category_Unknown', 'Blue', 'Gold','Platinum', 'Silver', 'College','Doctorate',\n             'Graduate','High School','Post-Graduate', 'Uneducated','Education_Unknown']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split our dataset which will result in 70% of training set and 30% of test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=68, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now train a simple Random Forest Classifier and evaluate its performace, I leave all parameters to default except for the max_depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=10)\nclf.fit(X_train, y_train)\ny_pred_proba = clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf, X_test, y_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we are working with imbalanced data I prefer to use ROC_AUC_score instead of accuracy. Then we also interested to see True Positive Rate (TPR) and we also want to avoid False Negative. From the matrix confusion, we got:\nTPR = TP/(TP + FN)\n    = 357/(357+151)\n    = 0.716867\n\nThis result isn't good enough. Now lets try to use Adaboost. Adaboost works as an ensemble learning. Adaboost combines plenty of weak classifier to produce a strong classifier. In this sample, we will use our random forest model as our weak classifier, then adaboost will try to combine this model to give a better result. "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_adaboost = AdaBoostClassifier(base_estimator=clf, n_estimators=100, random_state=0)\nclf_adaboost.fit(X_train, y_train)\ny_pred_proba = clf_adaboost.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, y_pred_proba[:,1]))\nmetrics.plot_confusion_matrix(clf_adaboost, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adaboost Model has been successful to produce a greater learner. ROC_AUC_score has shown a better result. And also our TPR has been improved compared to the random forest classifier. We achieve TPR score of 0.80315. The next session, I will try to do something with the imbalanced data."},{"metadata":{},"cell_type":"markdown","source":"# # Undersampling the Majority Class"},{"metadata":{},"cell_type":"markdown","source":"This time I will try to use NearMiss algorithm to undersampling the *Existing_Customer*. First let take a look at the distribution of our target values. It is shown that Attrited Customer is only 15.7% of our train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()/len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nm = NearMiss()\nX_under, y_under = nm.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_under.value_counts()/len(y_under)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After appling Near Miss algorithm, I have a balanced data between Attrited Customer and Existing Customer. Lets try to train Random Forest and Adaboost again on this undersample dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=10, random_state=0)\nclf.fit(X_under, y_under)\ny_pred_proba = clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a worse ROC_AUC_score, but on the other hand, we got a better TPR score as what have been asked in the task. We achieve 0.87992 TPR Score. \n\nNow, lets try Adaboost Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_adaboost = AdaBoostClassifier(base_estimator=clf, n_estimators=200, random_state=0)\nclf_adaboost.fit(X_under, y_under)\ny_pred_proba = clf_adaboost.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf_adaboost, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We achieved a better ROC_AUC_score and also the highest TPR value which is 0.9035. So far, undersampling the dataset works well for this dataset. Next section I will try to overpsampling the minority class."},{"metadata":{},"cell_type":"markdown","source":"# Oversampling the Minority Class"},{"metadata":{},"cell_type":"markdown","source":"This time, I will try to use SMOTE algorithm to oversample the minority class and train the Random Forest and Adaboost classifier again and see whether it will give a better result than the Near Miss undersampling."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()/len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(sampling_strategy='auto', random_state=1234)\nX_over, y_over = sm.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_over.value_counts()/len(y_over)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SMOTE gives us the balanced train distribution, just like what Near Miss did. Now train the Random Forest Classifier using SMOTE dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=10, random_state=0)\nclf.fit(X_over, y_over)\ny_pred_proba = clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf, X_test, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared to the undersampling dataset, we achieved a better ROC_AUC_score but worse TPR value which is 0.8307.\n\nNow lets train the AdaBoost Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_adaboost = AdaBoostClassifier(base_estimator=clf, n_estimators=200, random_state=0)\nclf_adaboost.fit(X_over, y_over)\ny_pred_proba = clf_adaboost.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf_adaboost, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We achieved TPR Score of 0.84843. Overall, if we compared Upsampling SMOTE and undersampling Near Miss dataset, we achieved a better result of AUC score. But if our target is to get higher TPR then, I will choose Near Miss dataset and use AdaBoost classifier which give me the best TPR score."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}