{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fake and Real News Dataset\n\n\n# **<span style=\"color:#6daa9f;\">IMPORT LIBRARY & PACKAGES </span>**\n","metadata":{}},{"cell_type":"markdown","source":"### Hi there!ðŸ˜„ I am new to data science and this is my try on the Fake and Real News dataset. Feel free to comment if you have any questions, insights or advice on this or any data science related :) Upvote if you find my work useful for you! Thank you!","metadata":{}},{"cell_type":"code","source":"#import package\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport re\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#6daa9f;\">EXPLORATORY DATA ANALYSIS </span>**\n","metadata":{}},{"cell_type":"code","source":"# Reading from file \nfake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\ntrue = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(true.shape)\nprint(true.info())\ntrue.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(fake.shape)\nprint(fake.info())\nfake.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake['Label'] = 1\ntrue['Label'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([true,fake],axis=0,ignore_index=True)\nprint(data.shape)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text']=data['title']+data['text']\ndata=data.drop(['title'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data.Label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.subject.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#6daa9f;\">DATA CLEANING </span>**\n\n**Lowercase words, remove the word 'Reuters', remove square brackets, links, words containing numbers and punctuations**\n\n* Cleaning our text data is important so that the model wont be fed noises that would not help with the prediction. \n* The word reuters was removed as it always appear in the real news article therefore I removed it as it is an obvious indicator to the model ","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    \n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('Reuters','',text)\n    return text\n\ndata['text'] = data['text'].apply(lambda x:clean_text(x))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove stop words**","metadata":{}},{"cell_type":"code","source":"stop = stopwords.words('english')\ndata['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatize words**\n\nWords were lemmatized so that only root words are retain in the data and fed into the model ","metadata":{}},{"cell_type":"code","source":"def lemmatize_words(text):\n    wnl = nltk.stem.WordNetLemmatizer()\n    lem = ' '.join([wnl.lemmatize(word) for word in text.split()])    \n    return lem\n\ndata['text'] = data['text'].apply(lemmatize_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split data into train and test set**","metadata":{}},{"cell_type":"code","source":"y = data['Label']\nX_train, X_test, y_train, y_test = train_test_split(data['text'], y,test_size=0.33,random_state=53)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using Bag of words model for data transformation**\n\nSince we are dealing with text data, we cannot fed it directly to our model. Therefore, I am using bag of words model to extract features from our text data and convert it into numerical feature vectors that can be fed directly to the algorithm","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words='english')\ncount_train = count_vectorizer.fit_transform(X_train.values)\ncount_test = count_vectorizer.transform(X_test.values)\nprint(count_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#6daa9f;\">MODEL </span>**\n\nUsing 2 different model with different parameter for parameter investigation values of alpha and c\n\n**Naive Bayes**","metadata":{}},{"cell_type":"code","source":"# Model 1 - default parameter \nfrom sklearn.metrics import classification_report\n\nnb_classifier1 = MultinomialNB()\nnb_classifier1.fit(count_train, y_train)\n\npred1 = nb_classifier1.predict(count_test)\n\nprint(classification_report(y_test, pred1, target_names = ['Fake','True']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model 2\nnb_classifier2 = MultinomialNB(alpha = 1000)\nnb_classifier2.fit(count_train, y_train)\n\npred2 = nb_classifier2.predict(count_test)\n\nprint(classification_report(y_test, pred2, target_names = ['Fake','True']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Support Vector Machine (SVM)**","metadata":{}},{"cell_type":"code","source":"# 1\nfrom sklearn.svm import SVC\n\nsvc_model1 = SVC(C=1, kernel='linear', gamma= 1)\nsvc_model1.fit(count_train, y_train)\n\nprediction1 = svc_model1.predict(count_test)\n\nprint(classification_report(y_test, prediction1, target_names = ['Fake','True']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2\nsvc_model2 = SVC(C= 100, kernel='linear', gamma= 1)\nsvc_model2.fit(count_train, y_train)\n\nprediction2 = svc_model2.predict(count_test)\n\nprint(classification_report(y_test, prediction2, target_names = ['Fake','True']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}