{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport datetime as dt\nimport calendar,warnings,itertools,matplotlib,keras,shutil\nimport tensorflow as tf\nimport statsmodels.api as sm\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict\nfrom sklearn import svm,metrics,tree,preprocessing,linear_model\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge,LinearRegression,LogisticRegression,ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingRegressor,BaggingClassifier,ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score,mean_squared_error,recall_score,confusion_matrix,f1_score,roc_curve, auc\nfrom sklearn.datasets import load_iris,make_regression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.kernel_ridge import KernelRidge\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom IPython.core import display as ICD\nwarnings.filterwarnings('ignore') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing Dataset using pandas\ndataset=pd.read_csv(\"../input/dataco-smart-supply-chain-for-big-data-analysis/DataCoSupplyChainDataset.csv\",header= 0,encoding= 'unicode_escape')\ndataset.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping Columns that are irrelevant and repetitive\n\n## Concatenating first and last name so that one could be dropped\ndataset['Customer Full Name'] = dataset['Customer Fname'].astype(str)+dataset['Customer Lname'].astype(str) \n\ndata=dataset.drop(['Customer Email','Product Status','Customer Password','Customer Street','Customer Fname','Customer Lname',\n           'Latitude','Longitude','Product Description','Product Image','Order Zipcode','shipping date (DateOrders)','Customer Zipcode', 'order date (DateOrders)','Delivery Status' ],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.apply(lambda x: sum(x.isnull())) #Checking missing values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heatmap for correlation matrix","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24,12)) # figsize\nsns.heatmap(data.corr(),annot=True,linewidths=.5,fmt='.1g',cmap= 'coolwarm') # Heatmap for correlation matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.plot(x='Product Price', y='Sales per customer',linestyle='dotted',\n     markerfacecolor='blue', markersize=12) \nplt.title('Product Price vs Sales per customer')#title\nplt.xlabel('Product Price')  # X-axis title\nplt.ylabel('Sales per customer') # Y=axis title\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Casestudy 1: Fraud Order Classification ","metadata":{}},{"cell_type":"code","source":"# Creating a new column with binary classification of fraud status\ntrain_data['fraud'] = np.where(train_data['Order Status'] == 'SUSPECTED_FRAUD', 1, 0)\ntrain_data.drop(['Order Status'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking data tyoes so that relevant encosing couldcould be done \n\ntrain_data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Label encoder all the column with datatype as object is been transformed\n\nle = preprocessing.LabelEncoder()\ntrain_data['Customer Country']  = le.fit_transform(train_data['Customer Country'])\ntrain_data['Market']            = le.fit_transform(train_data['Market'])\ntrain_data['Type']              = le.fit_transform(train_data['Type'])\ntrain_data['Product Name']      = le.fit_transform(train_data['Product Name'])\ntrain_data['Customer Segment']  = le.fit_transform(train_data['Customer Segment'])\ntrain_data['Customer State']    = le.fit_transform(train_data['Customer State'])\ntrain_data['Order Region']      = le.fit_transform(train_data['Order Region'])\ntrain_data['Order City']        = le.fit_transform(train_data['Order City'])\ntrain_data['Category Name']     = le.fit_transform(train_data['Category Name'])\ntrain_data['Customer City']     = le.fit_transform(train_data['Customer City'])\ntrain_data['Department Name']   = le.fit_transform(train_data['Department Name'])\ntrain_data['Order State']       = le.fit_transform(train_data['Order State'])\ntrain_data['Shipping Mode']     = le.fit_transform(train_data['Shipping Mode'])\ntrain_data['Order Country']     = le.fit_transform(train_data['Order Country'])\ntrain_data['Customer Full Name']= le.fit_transform(train_data['Customer Full Name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test):\n    model_f=model_f.fit(xf_train,yf_train) # Fitting train data for fraud detection\n    model_l=model_l.fit(xl_train,yl_train) # Fitting train data for predection of late delivery\n    yf_pred=model_f.predict(xf_test)\n    yl_pred=model_l.predict(xl_test)  \n    accuracy_f=accuracy_score(yf_pred, yf_test) #Accuracy for fraud detection\n    accuracy_l=accuracy_score(yl_pred, yl_test) #Accuracy for predection of late delivery\n    recall_f=recall_score(yf_pred, yf_test) #Recall score for  fraud detection\n    recall_l=recall_score(yl_pred, yl_test)# Recall score for predection of late delivery\n    conf_f=confusion_matrix(yf_test, yf_pred)# fraud detection\n    conf_l=confusion_matrix(yl_test, yl_pred)#predection of late delivery\n    f1_f=f1_score(yf_test, yf_pred)#fraud detection\n    f1_l=f1_score(yl_test, yl_pred)#predection of late delivery\n    print('Model paramters used are :',model_f)\n    print('Accuracy of fraud status is        :', (accuracy_f)*100,'%')\n    print('Recall score of fraud status is        :', (recall_f)*100,'%')\n    print('Conf Matrix of fraud status is        :\\n',  (conf_f))\n    print('F1 score of fraud status is        :', (f1_f)*100,'%')\n    print('Accuracy of late delivery status is:', (accuracy_l)*100,'%')\n    print('Recall score of late delivery status is:', (recall_l)*100,'%')\n    print('Conf Matrix of late delivery status is: \\n',(conf_l))\n    print('F1 score of late delivery status is:', (f1_l)*100,'%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.layers.BatchNormalization()\nclassifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(1024, activation='relu',kernel_initializer='random_normal', input_dim=38)) #Since we have 44 columns\nclassifier.add(Dropout(0.3))\n#Third Hidden Layer\nclassifier.add(Dense(512, activation='relu',kernel_initializer='random_normal'))\nclassifier.add(Dropout(0.3))\n#Fourth Hidden Layer\nclassifier.add(Dense(256, activation='relu',kernel_initializer='random_normal'))\nclassifier.add(Dropout(0.3))\n#Fifth Hidden Layer\nclassifier.add(Dense(128, activation='relu',kernel_initializer='random_normal'))\nclassifier.add(Dropout(0.3))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid',kernel_initializer='random_normal'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#All columns expect fraud\nxf=train_data.loc[:, train_data.columns != 'fraud']\n#Only fraud column\nyf=train_data['fraud']\n#Splitting the data into two parts in which 80% data will be used for training the model and 20% for testing\nxf_train, xf_test,yf_train,yf_test = train_test_split(xf,yf,test_size = 0.2,random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preparing vaidation set for finding the optimun number of epochs required.\n\nxf_val = xf[:28000] # reserving the first 10,000 reviews for validation\nxf_train_small = xf[28000:] # reserving the remaining 15,000 reviews for training\nyf_val = yf[:28000]\nyf_train_small = yf[28000:]\nhistory = classifier.fit(xf_train_small, \n                    yf_train_small, \n                    epochs=20, \n                    batch_size=216,\n                   validation_data=(xf_val, yf_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = classifier.history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.fit(xf_train,yf_train, batch_size=216, epochs=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(classifier, to_file='Case1.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_evaluate=classifier.evaluate(xf_train, yf_train)\ntest_evaluate=classifier.evaluate(xf_test, yf_test)\nprint('accuracy for Train set is',train_evaluate)\nprint('accuracy for Test set is',test_evaluate)# evaluation of model.\nyf_pred1=classifier.predict(xf_test,batch_size=512,verbose=1)\nyf_pred=np.argmax(yf_pred1,axis=1)\nprint(f1_score(yf_test,yf_pred,average=\"weighted\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Casestudy 2: Late Delivery Classification ","metadata":{}},{"cell_type":"code","source":"xl=train_data.loc[:, train_data.columns != 'Late_delivery_risk']\n#Only fraud column\nyl=train_data['Late_delivery_risk']\n#Splitting the data into two parts in which 80% data will be used for training the model and 20% for testing\nxl_train, xl_test,yl_train,yl_test = train_test_split(xl,yl,test_size = 0.1, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nxl_train=sc.fit_transform(xl_train)\nxl_test=sc.transform(xl_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xl_val = xl[:28000] # reserving the first 10,000 reviews for validation\nxl_train_small = xl[28000:] # reserving the remaining 15,000 reviews for training\nyl_val = yl[:28000]\nyl_train_small = yl[28000:]\nhistory = classifier.fit(xl_train_small, \n                    yl_train_small, \n                    epochs=20, \n                    batch_size=216,\n                   validation_data=(xl_val, yl_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = classifier.history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.fit(xl_train,yl_train, batch_size=216, epochs=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_evaluate=classifier.evaluate(xl_train, yl_train)\ntest_evaluate=classifier.evaluate(xl_test, yl_test)\nprint('accuracy for Train set is',train_evaluate)\nprint('accuracy for Test set is',test_evaluate)# evaluation of model.\nyl_pred1=classifier.predict(xl_test,batch_size=216,verbose=1)\nyl_pred=np.argmax(yl_pred1,axis=1)\nprint(f1_score(yl_test,yl_pred,average=\"weighted\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"keras.layers.BatchNormalization()\nclassifier_l= Sequential()\n#First Hidden Layer\nclassifier_l.add(Dense(1024, activation='relu',kernel_initializer='random_normal', input_dim=38)) #Since we have 44 columns\nclassifier_l.add(Dropout(0.3))\n#Third Hidden Layer\nclassifier_l.add(Dense(512, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Fourth Hidden Layer\nclassifier_l.add(Dense(256, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Fifth Hidden Layer\nclassifier_l.add(Dense(128, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Sixth Hidden Layer\nclassifier_l.add(Dense(64, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Seventh Hidden Layer\nclassifier_l.add(Dense(32, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Eight Hidden Layer\nclassifier_l.add(Dense(16, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Ninth Hidden Layer\nclassifier_l.add(Dense(8, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Tenth Hidden Layer\nclassifier_l.add(Dense(4, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Eleventh Hidden Layer\nclassifier_l.add(Dense(2, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Output Layer\nclassifier_l.add(Dense(1, activation='sigmoid',kernel_initializer='random_normal'))","metadata":{}},{"cell_type":"markdown","source":"# Casestudy 3: Regression on Sales Data","metadata":{}},{"cell_type":"code","source":"xs=train_data.loc[:, train_data.columns != 'Sales']\nys=train_data['Sales']\nxs_train, xs_test,ys_train,ys_test = train_test_split(xs,ys,test_size = 0.3, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler=MinMaxScaler()\nxs_train=scaler.fit_transform(xs_train)\nxs_test=scaler.transform(xs_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def regressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test):\n    model_s=model_s.fit(xs_train,ys_train)#Fitting train data for sales\n    model_q=model_q.fit(xq_train,yq_train)#Fitting train data for order quantity\n    ys_pred=model_s.predict(xs_test)#predicting sales with test data\n    yq_pred=model_q.predict(xq_test)#predicting order quantity with test data\n    print('Model parameter used are:',model_s) #Printing the model to see which parameters are used\n    #Printing mean absolute error for predicting sales\n    print(\"MAE of sales is         :\", metrics.mean_absolute_error(ys_test,ys_pred))\n    #Printing Root mean squared error for predicting sales\n    print(\"RMSE of sales is        :\",np.sqrt(metrics.mean_squared_error(ys_test,ys_pred)))\n    #Printing mean absolute error for predicting order quantity\n    print(\"MAE of order quantity   :\", metrics.mean_absolute_error(yq_test,yq_pred))\n    #Printing Root mean squared error for predicting order quantity\n    print(\"RMSE of order quantity  :\",np.sqrt(metrics.mean_squared_error(yq_test,yq_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor = Sequential()\n\n#First Hidden Layer\nregressor.add(Dense(512, activation='relu',kernel_initializer='normal',input_dim=38))\nregressor.add(Dropout(0.3))\n#Second  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\nregressor.add(Dropout(0.3))\n#Third  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\nregressor.add(Dropout(0.3))\n#Fourth  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\nregressor.add(Dropout(0.3))\n#Fifth  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\n#Output Layer\nregressor.add(Dense(1, activation='linear'))# Linear activation is used.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = array(X).reshape(20, 1, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the LSTM network model\nmodel = Sequential()\nmodel.add(LSTM(units=512, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, activation='relu',kernel_initializer='random_normal', input_shape=(38, 1)))\nmodel.add(Dense(units=1,activation='linear'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs_val = xs[:28000] # reserving the first 10,000 reviews for validation\nxs_train_small = xs[28000:] # reserving the remaining 15,000 reviews for training\nys_val = ys[:28000]\nys_train_small = ys[28000:]\nhistory = classifier.fit(xs_train_small, \n                    ys_train_small, \n                    epochs=30, \n                    batch_size=216,\n                   validation_data=(xs_val, ys_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = classifier.history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor.compile(optimizer='rmsprop',loss='mean_absolute_error',metrics=['mean_absolute_error'])\nregressor.fit(xs_train,ys_train, batch_size=216, epochs=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(xs_train.shape)\nprint(xs_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='rmsprop',loss='mean_absolute_error',metrics=['mean_absolute_error'])\nmodel.fit(xs_train,ys_train, batch_size=216, epochs=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_train_s= regressor.predict(xs_train)\npred_s_test= regressor.predict(xs_test)\nprint('MAE Value train data:',regressor.evaluate(xs_train,ys_train))\nprint('RMSE of train data:',np.sqrt(mean_squared_error(ys_train,pred_train_s)))\nprint('MAE Value test data:',regressor.evaluate(xs_test,ys_test))\nprint('RMSE of test data:',np.sqrt(mean_squared_error(ys_test,pred_s_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"from keras.layers import LSTM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xt=train_data.loc[:, train_data.columns != 'Sales']\nyt=train_data['Sales']\nxt_train, xt_test,yt_train,yt_test = train_test_split(xt,yt,test_size = 0.3, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler=MinMaxScaler()\nxt_train=scaler.fit_transform(xt_train)\nxt_test=scaler.transform(xt_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytt = train_data['Sales'].values.reshape(-1, 1) #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nytt_scaled = min_max_scaler.fit_transform(ytt)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xt_train, xt_test,yt_train,yt_test = train_test_split(xt,ytt_scaled,test_size = 0.3, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yt_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape input to be [samples, time steps, features] which is required for LSTM\nxt_train1 =xt_train.reshape(xt_train.shape[0],xt_train.shape[1] , 1)\nxt_test1 = xt_test.reshape(xt_test.shape[0],xt_test.shape[1] , 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape input to be [samples, time steps, features] which is required for LSTM\nyt_train1 =yt_train.reshape(yt_train.shape[0],yt_train.shape[1] , 1)\nyt_test1 = yt_test.reshape(yt_test.shape[0],yt_test.shape[1] , 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the LSTM network model\nmodel = Sequential()\nmodel.add(LSTM(units=512, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, activation='relu',kernel_initializer='random_normal', input_shape=(38, 1)))\nmodel.add(Dense(units=1,activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mean_absolute_error'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xt_train,yt_train, batch_size=216, epochs=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xt_train.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_predict=model.predict(xt_train1)\ntest_predict=model.predict(xt_test1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modeva= model.evaluate(xt_train1,yt_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MAE Value train data:',modeva)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modevatest=model.evaluate(xt_test1,yt_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MAE Value test data:',modevatest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}