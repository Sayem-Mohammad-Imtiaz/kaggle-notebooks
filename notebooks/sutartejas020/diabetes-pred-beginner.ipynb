{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking for imbalanced dataset**","metadata":{}},{"cell_type":"code","source":"df[\"Outcome\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA**","metadata":{}},{"cell_type":"code","source":"df.hist(figsize=(30,30))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.distplot(df[\"Pregnancies\"],color=\"darkred\",bins=40)\nsns.catplot(x=\"Outcome\",y=\"Pregnancies\",kind=\"violin\",data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From above graph it shows that more impact of pregnancies for diabetes lies between 0-5**","metadata":{}},{"cell_type":"code","source":"sns.distplot(df[\"Glucose\"],color=\"darkred\",bins=40)\nsns.catplot(x=\"Outcome\",y=\"Glucose\",kind=\"violin\",data=df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Glucose is main source for diabetes from above graph it shows glucose level**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"sns.distplot(df[\"SkinThickness\"],color=\"darkred\",bins=40)\nsns.catplot(x=\"Outcome\",y=\"SkinThickness\",kind=\"violin\",data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"SkinThickness\"],color=\"darkred\",bins=40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"Insulin\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"Outcome\",y=\"Insulin\",kind=\"violin\",data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"BMI\"])\nsns.catplot(x=\"Outcome\",y=\"BMI\",kind=\"violin\",data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BMI lies between 20-55 contains diabetes**","metadata":{}},{"cell_type":"code","source":"sns.distplot(df[\"DiabetesPedigreeFunction\"])\nsns.catplot(x=\"Outcome\",y=\"DiabetesPedigreeFunction\",kind=\"violin\",data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"Age\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[\"Age\"],kde=False,color=\"darkred\",bins=40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"Outcome\",y=\"Age\",kind=\"violin\",data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,25))\nsns.heatmap(df.corr(),cmap=\"Dark2\", annot=True,)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Replacing \"0\" with \"NaN\" as \"0\" doesn't contribute so considering it as Null values**","metadata":{}},{"cell_type":"code","source":"df1[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']]=df1[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0,np.NaN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imputing null values with mean values.**","metadata":{}},{"cell_type":"code","source":"df1[\"Glucose\"].fillna(df1[\"Glucose\"].mean(),inplace=True)\ndf1[\"BloodPressure\"].fillna(df1[\"BloodPressure\"].mean(),inplace=True)\ndf1[\"SkinThickness\"].fillna(df1[\"SkinThickness\"].mean(),inplace=True)\ndf1[\"Insulin\"].fillna(df1[\"Insulin\"].mean(),inplace=True)\ndf1[\"BMI\"].fillna(df1[\"BMI\"].mean(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking with outliers in data with the help of Boxplots**","metadata":{}},{"cell_type":"code","source":"for col in df1.iloc[:,:-1].columns:\n    print(col)\n    sns.boxplot(x=df1[col],data=df1)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df1,hue='Outcome', diag_kind=\"hist\");\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transforming outliers with IQR**","metadata":{}},{"cell_type":"code","source":"def boxoutlier(var):\n    for x in var.iloc[:,:-1].columns :        \n        Q1=var[x].quantile(0.25)\n        Q3=var[x].quantile(0.75)\n        IQR=Q3-Q1\n        Lower = Q1-(1.5*IQR)\n        Upper = Q3+(1.5*IQR)\n        var.loc[:,x]=np.where(var[x].values > Upper,Upper,var[x].values)\n        var.loc[:,x]=np.where(var[x].values < Lower,Lower,var[x].values)\n        \n    return var\ndf1=boxoutlier(df1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df1.iloc[:,:-1].columns:\n    print(col)\n    sns.boxplot(x=df1[col],data=df1)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df1.drop([\"Outcome\"],axis=1)\nY=df1[\"Outcome\"] \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def models(X_train,Y_train):\n    \n    ###Logistics Regression\n    from sklearn.linear_model import LogisticRegression\n    le=LogisticRegression()\n    log_re=le.fit(X_train,Y_train)\n    \n    \n    ### Random Forest\n    from sklearn.ensemble import RandomForestClassifier\n    rclf=RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 10)\n    RF=rclf.fit(X_train,Y_train)\n    \n    ###KNN\n    from sklearn.neighbors import KNeighborsClassifier\n    knn=KNeighborsClassifier(n_neighbors=7)\n    KNN=knn.fit(X_train,Y_train)\n    \n    ###SVM\n    from sklearn.svm import SVC\n    svl=SVC(kernel=\"linear\",random_state=0)\n    LINSVM=svl.fit(X_train,Y_train)\n    \n    svp=SVC(kernel=\"poly\",random_state=0)\n    POLSVM=svp.fit(X_train,Y_train)\n    \n    svrbf=SVC(kernel=\"rbf\",random_state=0)\n    RBFSVM=svrbf.fit(X_train,Y_train)\n    \n    print(\"[0]Logistic Regression Accuracy:\",log_re.score(X_train,Y_train))\n\n    print(\"[1]Random Forest:\",RF.score(X_train,Y_train))\n    \n    print(\"[2]KNN:\",KNN.score(X_train,Y_train))\n    \n    print(\"[3]Linear SVM:\",LINSVM.score(X_train,Y_train))\n    \n    print(\"[4]Polynomial SVM:\",POLSVM.score(X_train,Y_train))\n    \n    print(\"[5]RBF SVM:\",RBFSVM.score(X_train,Y_train))\n    \n    return log_re,RF,KNN,LINSVM,POLSVM,RBFSVM\n\nmodel=models(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(model)):\n    print(\"Model\",i)\n    print(classification_report(Y_test,model[i].predict(X_test)))\n    print(accuracy_score(Y_test, model[i].predict(X_test)))\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=model[5].predict(X_test)\nprint(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}