{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"missing_values = [\"n/a\", \"na\", \"NaN\",\"nan\"] #these values will be considered as missing values\nbiden = pd.read_csv('/kaggle/input/us-election-2020-tweets/hashtag_joebiden.csv',na_values = missing_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trump = pd.read_csv('/kaggle/input/us-election-2020-tweets/hashtag_donaldtrump.csv',na_values = missing_values, engine=\"python\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"quick look at biden dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"biden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"quick look at trump dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"trump.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some columns are not so useful for our work so we'll remove them**"},{"metadata":{"trusted":true},"cell_type":"code","source":"biden = biden.drop(columns = ['created_at', 'tweet_id','source','user_id','user_name','user_screen_name','user_description','user_location','lat','long','city','state_code','collected_at']) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trump = trump.drop(columns = ['created_at', 'tweet_id','source','user_id','user_name','user_screen_name','user_description','user_location','lat','long','city','state_code','collected_at']) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some rows contains null values. Since dataset is quite large we can manage to remove these rows**"},{"metadata":{"trusted":true},"cell_type":"code","source":"biden = biden.dropna(axis = 0, how = 'any') #it will remove all rows containing 'any' value as null(at least one)\nprint(len(biden)) #size after removing rows\nprint(biden.isnull().sum()) #count of null values in each column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After removing unuseful columns and rows**"},{"metadata":{"trusted":true},"cell_type":"code","source":"biden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Same procedure for trump dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"trump = trump.dropna(axis = 0, how = 'any')\nprint(len(trump))\nprint(trump.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#retwwet_count and user_followers_count columns conatins values as object/string\n#so it'll be better to convert into float\nbiden = biden.astype({\"retweet_count\": float, \"user_followers_count\":float})\ntrump = trump.astype({\"retweet_count\": float, \"user_followers_count\":float})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below function will fill null values according to percentage of non-null values in that column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" import random\ndef fill_missing(column):\n    col = column.value_counts().to_dict()\n    prob = random.choices(list(col.keys()), weights = list(col.values()), k=100)\n    null_val = column.isnull()\n    for i in range(len(null_val)):\n        if null_val[i]== True:\n            column[i] = prob[i%100]\n    return column\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below function will make dictionary for certain column. Keys will be its elements and values will be count of elements in that columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_dictionary(dataframe,column):\n    dictt = {}\n    for i in list(dataframe[column]):\n        if i in dictt:\n            dictt[i] = dictt.get(i) + 1\n        else:\n            dictt[i] = 1\n    return dictt\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making dictionaries for certain columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_biden = make_dictionary(biden,'country')\nstate_biden = make_dictionary(biden,'state')\ncontinent_biden = make_dictionary(biden,'continent')\ncontinent_biden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_trump = make_dictionary(trump,'country')\nstate_trump = make_dictionary(trump,'state')\ncontinent_trump = make_dictionary(trump,'continent')\ncontinent_trump","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since there're so many keys in each dictionary, we can delete some keys with values less than certain threshold number**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#all keys with values less than 1000 will be deleted\nfor i in list(country_biden):\n    if country_biden.get(i)<1000:\n        del country_biden[i]\n#now arranging keys according to values in asc order\ncountry_biden  = {k: v for k, v in sorted(country_biden.items(), key=lambda item: item[1])}\ncountry_biden\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(country_trump):\n    if country_trump.get(i)<1000:\n        del country_trump[i]\ncountry_trump  = {k: v for k, v in sorted(country_trump.items(), key=lambda item: item[1])}\ncountry_trump\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\np1 = plt.bar(np.arange(len(country_biden)),list(country_biden.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per country for biden')\nplt.xticks(np.arange(len(country_biden)), country_biden.keys(),rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p11 = plt.bar(np.arange(len(country_trump)),list(country_trump.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per country for trump')\nplt.xticks(np.arange(len(country_trump)), country_trump.keys(),rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_biden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**finding correlation between winning in particular state with no. of tweets from that state**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 for winning, 0 for losing in that state for biden\nbiden_won_states = {'Georgia':1, 'Arizona':1,'Florida':0,'Iowa':0,'Michigan':1,'Nevada':1,'New Hampshire':1,\n                   'North Carolina':0,'Ohio':0,'Pennsylvania':1,'Texas':0,'Wisconsin':1,'Washington':1,\n                   'Idaho':0,'Oregon':1,'Utah':0,'New Mexico':1,'Oklahoma':0,'Nebraska':0,\n                   'Wyoming':0,'Montana':0,'North Dakota':0,'South Dakota':0,'Minnesota':1,'Illinois':1,\n                   'Indiana':0,'West Virginia':0,'Virginia':1,'Maine':1,'Massachusetts':1,'Alabama':0,\n                   'Mississippi':0,'Lousiana':0,'Michigan':1,'South Carolina':0,'Maryland':1,\n                    'District Of Columbia':1,'Delaware':1,'New Jersey':1,'Connecticut':1,'Rhode Island':1,\n                   'Vermont':1,'Arkansas':0,'Kansas':0,'Missouri':0,'Tennessee':0,'Kentucky':0}\nst_x = [] #stores number of tweets from each state\nst_y = [] #stores 1 for winning, 0 for losing in each state\nfor i in state_biden.keys():\n    if i in biden_won_states.keys():\n        st_x.append(state_biden.get(i))\n        st_y.append(biden_won_states.get(i))\nprint(st_x)\nprint(st_y)\ncorr = np.corrcoef(st_x,st_y) #correlation b/w tweets and winning in particular state\nprint(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As you can see we got very less value for corr b/w tweets and winning. Now we'll try using vote percent instead of 0/1 and see the corr b/w tweets and vote percent**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#vote percent of biden in each state\nbiden_won_states_percent = {'Georgia':49.5, 'Arizona':49.4,'Florida':47.9,'Iowa':45.0,'Michigan':50.6,'Nevada':50.1,'New Hampshire':52.8,\n                   'North Carolina':48.7,'Ohio':45.2,'Pennsylvania':50.0,'Texas':46.5,'Wisconsin':49.6,'Washington':58.4,\n                   'Idaho':33.1,'Oregon':56.9,'Utah':37.7,'New Mexico':54.3,'Oklahoma':32.3,'Nebraska':39.4,\n                   'Wyoming':26.7,'Montana':40.5,'North Dakota':31.9,'South Dakota':35.6,'Minnesota':52.6,'Illinois':57.4,\n                   'Indiana':41.0,'West Virginia':29.7,'Virginia':54.3,'Maine':52.9,'Massachusetts':65.6,'Alabama':36.6,\n                   'Mississippi':40.5,'Lousiana':39.9,'Michigan':50.6,'South Carolina':43.4,'Maryland':65.4,\n                    'District Of Columbia':92.9,'Delaware':58.8,'New Jersey':57.2,'Connecticut':59.3,'Rhode Island':59.6,\n                   'Vermont':66.4,'Arkansas':34.8,'Kansas':41.3,'Missouri':41.3,'Tennessee':37.4,'Kentucky':36.2}\nst_x_per = [] #stores tweets count of each state\nst_y_per = [] #store vote percent of each state for biden\nfor i in state_biden.keys():\n    if i in biden_won_states_percent.keys():\n        st_x_per.append(state_biden.get(i))\n        st_y_per.append(biden_won_states_percent.get(i))\nprint(st_x_per)\nprint(st_y_per)\ncorr1 = np.corrcoef(st_x_per,st_y_per)\nprint(corr1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we can see we got slightly more correlation using vote percent.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#deleting some keys with less values or state which are not in USA\nfor i in list(state_biden):\n    if state_biden.get(i)<2000:\n        del state_biden[i]\ndel state_biden['Istanbul']\ndel state_biden['Berlin']\ndel state_biden['Maharashtra']\ndel state_biden['Delhi']\ndel state_biden['Ontario']\ndel state_biden['Ile-de-France']\ndel state_biden['England']\n\nstate_biden  = {k: v for k, v in sorted(state_biden.items(), key=lambda item: item[1])} #sorting dict\nstate_biden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(state_trump):\n    if state_trump.get(i)<2500:\n        del state_trump[i]\ndel state_trump['Istanbul']\ndel state_trump['Berlin']\ndel state_trump['Maharashtra']\ndel state_trump['Delhi']\ndel state_trump['Ontario']\ndel state_trump['Ile-de-France']\ndel state_trump['England']\ndel state_trump['North Rhine-Westphalia']\ndel state_trump['British Columbia']\ndel state_trump['Scotland']\nstate_trump  = {k: v for k, v in sorted(state_trump.items(), key=lambda item: item[1])}\nstate_trump","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p2 = plt.bar(np.arange(len(state_biden)),list(state_biden.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per state for biden')\nplt.xticks(np.arange(len(state_biden)), state_biden.keys(),rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p3 = plt.bar(np.arange(len(state_trump)),list(state_trump.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per state for trump')\nplt.xticks(np.arange(len(state_trump)), state_trump.keys(),rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**deleting unuseful keys of continent dict**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(continent_biden):\n    if continent_biden.get(i)<100:\n        del continent_biden[i]\ncontinent_biden  = {k: v for k, v in sorted(continent_biden.items(), key=lambda item: item[1])}\ncontinent_biden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(continent_trump):\n    if continent_trump.get(i)<100:\n        del continent_trump[i]\ncontinent_trump  = {k: v for k, v in sorted(continent_trump.items(), key=lambda item: item[1])}\ncontinent_trump","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting percentage of tweets from diff continents**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for biden\nplt.pie([v for v in continent_biden.values()],labels = [k for k in continent_biden.keys()],autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for trump\nplt.pie([v for v in continent_trump.values()],labels = [k for k in continent_trump.keys()],autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment analysis"},{"metadata":{},"cell_type":"markdown","source":"Combining all text from tweets into single string"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_biden = \" \".join(biden.tweet)\nlen(text_biden)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**dividing large text into lists each containing 10000 chars**"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 10000\ntext_list_biden = [text_biden[i:i+n] for i in range(0, len(text_biden), n)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text_list_biden) #total lists each containing 10000 chars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_trump = \" \".join(trump.tweet)\nn = 10000\ntext_list_trump = [text_trump[i:i+n] for i in range(0, len(text_trump), n)]\nlen(text_list_trump)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first, we import the relevant modules from the NLTK library\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n# next, we initialize VADER so we can use it within our Python script\nsid = SentimentIntensityAnalyzer()\nscores_biden = {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}\n#compound for overall sentiment\nfor i in range(1000): #we're analysing only 1000 list for saving time instead of ~5000\n    score = sid.polarity_scores(text_list_biden[0]) #sentiment score of each list\n    #adding this score to total score\n    scores_biden['compound'] = scores_biden['compound'] + score['compound']\n    scores_biden['neg'] = scores_biden['neg'] + score['neg']\n    scores_biden['neu'] = scores_biden['neu'] + score['neu']\n    scores_biden['pos'] = scores_biden['pos'] + score['pos']\n#since we've added scores of 1000 lists \n#divide each score by 1000 to get mean score\nscores_biden['compound'] = scores_biden['compound']/1000\nscores_biden['neg'] = scores_biden['neg'] /1000\nscores_biden['neu'] = scores_biden['neu'] /1000\nscores_biden['pos'] = scores_biden['pos'] /1000\nscores_biden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we got overall sentiment as -0.97 which indicates that mostly tweets are negative rather than positive**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_trump = {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}\nfor i in range(1000):\n    score = sid.polarity_scores(text_list_trump[0])\n    scores_trump['compound'] = scores_trump['compound'] + score['compound']\n    scores_trump['neg'] = scores_trump['neg'] + score['neg']\n    scores_trump['neu'] = scores_trump['neu'] + score['neu']\n    scores_trump['pos'] = scores_trump['pos'] + score['pos']\nscores_trump['compound'] = scores_trump['compound'] /1000\nscores_trump['neg'] = scores_trump['neg'] /1000\nscores_trump['neu'] = scores_trump['neu'] /1000\nscores_trump['pos'] = scores_trump['pos'] /1000\nscores_trump","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we got overall sentiment for trump as -0.99 which is more negative than biden(-0.97) which concludes that people are saying more negative things for trump.\nThere may be some tweets for #biden in which negative words are meant for trump and vice-versa. \nSo we can get more accurate results by considering only those tweets which are only for either biden or trump.\nBut I think still there'll be more negative tweets for trump ;)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}