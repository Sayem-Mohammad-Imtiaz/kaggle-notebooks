{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data insights**"},{"metadata":{},"cell_type":"markdown","source":"Read the data and store it in a dataframe"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if there's any null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"200 rows and 5 features(columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape\n      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the describe we get, mean of the spending score is 50 which is not that good! The spending score gives the quality of the customers. It's based on customer behaviour and purchasing data. \nAverage age of the customers is 38."},{"metadata":{},"cell_type":"markdown","source":"Checking if there's any categorical variable then we've to convert into numeric."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking two features: \n1. Annual Income (index location: 3)\n2. Spending Score (index location: 4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, [3, 4]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualise data points\nplt.scatter(X[:, 0], X[:,1], marker = '+')\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score out of 10')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[:,1] #Spending score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[:, 0] #Annual income","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In some cases, if the initialization of clusters is not appropriate, K-Means can result in arbitrarily bad clusters. This is where K-Means++ helps. It specifies a procedure to initialize the cluster centers before moving forward with the standard k-means clustering algorithm.\n\nUsing the K-Means++ algorithm, we optimize the step where we randomly pick the cluster centroid. We are more likely to find a solution that is competitive to the optimal K-Means solution while using the K-Means++ initialization.\nThe steps to initialize the centroids using K-Means++ are:\n\nThe first cluster is chosen uniformly at random from the data points that we want to cluster. This is similar to what we do in K-Means, but instead of randomly picking all the centroids, we just pick one centroid here\nNext, we compute the distance (D(x)) of each data point (x) from the cluster center that has already been chosen\nThen, choose the new cluster center from the data points with the probability of x being proportional to (D(x))2\nWe then repeat steps 2 and 3 until k clusters have been chosen"},{"metadata":{"trusted":true},"cell_type":"code","source":"#KMeans Algorithm to decide the optimum cluster number , KMeans++ using Elbow Mmethod\n#to figure out K for KMeans, I will use ELBOW Method on KMEANS++ Calculation\nfrom sklearn.cluster import KMeans\nSSE = []\n\n#we always assume the max number of cluster would be 10\n#you can judge the number of clusters by doing averaging\n###Static code to get max no of clusters\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 0)\n    kmeans.fit(X)\n    SSE.append(kmeans.inertia_)\n\n    #inertia_ is the formula used to segregate the data points into clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the ELBOW method to get the optimal value of K \n\nplt.plot(range(1,11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('No of clusters')\nplt.ylabel('SSE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If you zoom out this curve then you will see that last elbow comes at k=5\n#no matter what range we select ex- (1,21) also i will see the same behaviour but if we chose higher range it is little difficult to visualize the ELBOW\n#that is why we usually prefer range (1,11)\n##Finally we got that k=5\n\n#Model Build\nkmeansmodel = KMeans(n_clusters = 5, init ='k-means++', random_state = 0)\ny_kmeans = kmeansmodel.fit_predict(X)\n\n#For unsupervised learning we use \"fit_predict()\" wherein for supervised learning we use \"fit_tranform()\"\n#y_kmeans is the final model . Now how and where we will deploy this model in production is depends on what tool we are using.\n#This use case is very common and it is used in BFS industry(credit card) and retail for customer segmenattion.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(y_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing all the clusters \n\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###Model Interpretation \n#Cluster 1 (Red Color) -> earning high but spending less\n#cluster 2 (Blue Colr) -> average in terms of earning and spending \n#cluster 3 (Green Color) -> earning high and also spending high [TARGET SET]\n#cluster 4 (cyan Color) -> earning less but spending more\n#Cluster 5 (magenta Color) -> Earning less , spending less\n\n\n######We can put Cluster 3 into some alerting system where email can be send to them on daily basis as these re easy to converse ######\n#wherein others we can set like once in a week or once in a month"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('[0, 0] = ', X[y_kmeans == 0, 0])\nprint('[0, 1] = ', X[y_kmeans == 0, 1])\nprint('[1, 0] = ', X[y_kmeans == 1, 0])\nprint('[1, 1] = ', X[y_kmeans == 1, 1])\nprint('[2, 0] = ', X[y_kmeans == 2, 0])\nprint('[2, 1] = ', X[y_kmeans == 2, 1])\nprint('[3, 0] = ', X[y_kmeans == 3, 0])\nprint('[3, 1] = ', X[y_kmeans == 3, 1])\nprint('[4, 0] = ', X[y_kmeans == 4, 0])\nprint('[4, 1] = ', X[y_kmeans == 4, 1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(kmeans.cluster_centers_[:, 0])\nprint(kmeans.cluster_centers_[:, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Source: [https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/](http://)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}