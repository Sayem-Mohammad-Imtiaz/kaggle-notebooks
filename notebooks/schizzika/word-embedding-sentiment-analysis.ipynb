{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv', encoding='utf-8')\n\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['sentiment'] == 'positive', 'sentiment'] = 1\ndf.loc[df['sentiment'] == 'negative', 'sentiment'] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nreview_lines = list()\nlines = df['review'].values.tolist()\n\nfor line in lines:   \n    tokens = word_tokenize(line)\n    # convert to lower case\n    tokens = [w.lower() for w in tokens]\n    # remove punctuation from each word    \n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter out stop words    \n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    review_lines.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(review_lines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim \n\nEMBEDDING_DIM = 100\n# train word2vec model\nmodel = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n# vocab size\nwords = list(model.wv.vocab)\nprint('Vocabulary size: %d' % len(words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model in ASCII (word2vec) format\nfilename = 'imdb_embedding_word2vec.txt'\nmodel.wv.save_word2vec_format(filename, binary=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us try some utility functions of gensim word2vec more details here \n\nmodel.wv.most_similar('horrible')#, topn =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let’s see the result of semantically reasonable word vectors (king - man + woman)\nmodel.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Let’s see the result of semantically reasonable word vectors (king - man + woman)\nmodel.wv.most_similar(positive=['woman', 'king'], negative=['man'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#odd word out\nprint(model.wv.doesnt_match(\"woman king queen movie\".split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.similar_by_word(\"cat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.similarity('boy', 'girl'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nembeddings_index = {}\nf = open(os.path.join('', 'imdb_embedding_word2vec.txt'),  encoding = \"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:])\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df.loc[:24999, 'review'].values\ny_train = df.loc[:24999, 'sentiment'].values\nX_test = df.loc[25000:, 'review'].values\ny_test = df.loc[25000:, 'sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_reviews = X_train + X_test\nmax_length = 100 # try other options like mean of sentence lengths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nVALIDATION_SPLIT = 0.2\n\n# vectorize the text samples into a 2D integer tensor\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(review_lines)\nsequences = tokenizer_obj.texts_to_sequences(review_lines)\n\n# pad sequences\nword_index = tokenizer_obj.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nreview_pad = pad_sequences(sequences, maxlen=max_length)\nsentiment =  df['sentiment'].values\nprint('Shape of review tensor:', review_pad.shape)\nprint('Shape of sentiment tensor:', sentiment.shape)\n\n# split the data into a training set and a validation set\nindices = np.arange(review_pad.shape[0])\nnp.random.shuffle(indices)\nreview_pad = review_pad[indices]\nsentiment = sentiment[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n\nX_train_pad = review_pad[:-num_validation_samples]\ny_train = sentiment[:-num_validation_samples]\nX_test_pad = review_pad[-num_validation_samples:]\ny_test = sentiment[-num_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of X_train_pad tensor:', X_train_pad.shape)\nprint('Shape of y_train tensor:', y_train.shape)\n\nprint('Shape of X_test_pad tensor:', X_test_pad.shape)\nprint('Shape of y_test tensor:', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM =100\nnum_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    if i > num_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(num_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.initializers import Constant\n\n# define model\nmodel = Sequential()\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length,\n                            trainable=False)\n\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())\n\n# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# fit the model\nmodel.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# evaluate the model\nloss, accuracy = model.evaluate(X_test_pad, y_test, batch_size=128)\nprint('Accuracy: %f' % (accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us test some  samples\n# load the dataset but only keep the top n words, zero the rest\n\ntest_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\ntest_sample_2 = \"Good movie!\"\ntest_sample_3 = \"Maybe I like this movie.\"\ntest_sample_4 = \"Not to my taste, will skip and watch another movie\"\ntest_sample_5 = \"if you like action, then this movie might be good for you.\"\ntest_sample_6 = \"Bad movie!\"\ntest_sample_7 = \"Not a good movie!\"\ntest_sample_8 = \"This movie really sucks! Can I get my money back please?\"\ntest_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n\ntest_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\ntest_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n\n#predict\nmodel.predict(x=test_samples_tokens_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GRU\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\n\n# define model\nmodel = Sequential()\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length,\n                            trainable=False)\nmodel.add(embedding_layer)\nmodel.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint('Summary of the built model...')\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train...')\n\nmodel.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Testing...')\nscore, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\nprint(\"Accuracy: {0:.2%}\".format(acc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}