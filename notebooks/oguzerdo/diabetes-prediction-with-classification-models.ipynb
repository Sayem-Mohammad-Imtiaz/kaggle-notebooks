{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://www.oguzerdogan.com/\">\n    <img src=\"https://www.oguzerdogan.com/wp-content/uploads/2020/10/logo_oz.png\" width=\"200\" align=\"right\">\n</a>"},{"metadata":{},"cell_type":"markdown","source":"<center><h1><strong>Pima Indians Diabetes Classification Project</strong></h1>\n<img\nsrc=\"https://cdn.britannica.com/s:700x500/42/93542-050-E2B32DAB/women-Pima-shinny-game-field-hockey.jpg\">\n</center>"},{"metadata":{},"cell_type":"markdown","source":"# &#127919; Objective of Kernel"},{"metadata":{},"cell_type":"markdown","source":"Diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. \n\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are **females at least 21 years old of Pima Indian heritage.**\n\n--------------------"},{"metadata":{},"cell_type":"markdown","source":"**Details about the dataset:**\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n**Pregnancies:** Number of times pregnant  \n**Glucose:** Plasma glucose concentration a 2 hours in an oral glucose tolerance test  \n**BloodPressure:** Diastolic blood pressure (mm Hg)  \n**SkinThickness:** Triceps skin fold thickness (mm)  \n**Insulin:** 2-Hour serum insulin (mu U/ml)  \n**BMI:** Body mass index (weight in kg/(height in m)^2)  \n**DiabetesPedigreeFunction:** Diabetes pedigree function  \n**Age:** Age (years)  \n**Outcome:** Class variable ( 0 - 1)\n\n-----------\n\n**Number of Observation Units:** 768  \n**Variable Number:** 9\n\nResult; The model with the highest score after hyper parameter optimization was LGBM with 0.90 cross validation score."},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"%%html\n<style> \n@import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');\na {color: #37c9e1; font-family: 'Roboto';} \nh1 {color: #C20E69; font-family: 'Poppins'} \nh2, h3 {color: #25B89B; font-family: 'Poppins';}\nh4 {color: #818286; font-family: 'Roboto';}\n                                      \n</style>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# &#128217; Load Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main\n\nimport pandas as pd\nimport numpy as np\nimport warnings\n\n# Plots\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport itertools\nplt.style.use('fivethirtyeight')\npy.offline.init_notebook_mode(connected=True)\n\n# Models & Others\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nwarnings.simplefilter(action=\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# &#128214; Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(r\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## &#128270; Descriptive Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe([0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<ul>\n<li><strong>pregnancies</strong> mean &gt; median, <strong>right skewed</strong> | <strong>99% quartile value:</strong> 13, <strong>max value:</strong> 17 <strong>might be outlier</strong></li>\n<li><strong>glucose</strong> mean &gt; median, <strong>right skewed</strong> | <strong>%99</strong> and <strong>max</strong> values close each other</li>\n<li><strong>bloodpressure</strong> mean = median nearly, looks <strong>gaussian distribution</strong>. There is a 16 units difference between <strong>%99</strong> and <strong>max value. might be outlier</strong></li>\n<li><strong>skinthickness</strong> mean &gt; median, <strong>right skewed</strong> | <strong>min value:</strong> 0 | <strong>99% quartile value:</strong> 51 <strong>max value:</strong> 99 <strong>there is outlier</strong></li>\n<li><strong>insulin</strong> mean(79) &gt; median(30), <strong>right skewed</strong> | <strong>min value:</strong> 0, <strong>99% quartile value:</strong> 519 <strong>max value:</strong> 846</li>\n<li><strong>BMI</strong> mean(31) =~ median(32), looks <strong>gaussian distribution</strong> | <strong>min value:</strong> 0 | <strong>99% quartile value:</strong> 50, <strong>max value:</strong> 67</li>\n<li><strong>DPF</strong> mean(0.47) &gt; median (0.37) | <strong>min value:</strong> 0.078 | <strong>99% quartile value:</strong> 1.69 <strong>max value:</strong> 2.42</li>\n<li><strong>Age</strong> mean(33)&gt;median(29) <strong>right skewed</strong> | <strong>min value:</strong> 21 | <strong>99% quartile value:</strong> 67 <strong>max value:</strong> 2.42</li>\n\n</ul>\n\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Shape\nprint(\"There are {} observation and {} features \".format(df.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## &#128918; Target Variable Counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------COUNT-----------------------\ndef target_count():\n    trace = go.Bar( x = df['Outcome'].value_counts().values.tolist(), \n                    y = ['healthy','diabetic' ], \n                    orientation = 'h', \n                    text=df['Outcome'].value_counts().values.tolist(), \n                    textfont=dict(size=20),\n                    textposition = 'auto',\n                    opacity = 0.8,marker=dict(\n                    color=['#25B89B', '#C20E69'],\n                    line=dict(color='#FFFFFF',width=1.5)))\n\n    layout = dict(title =  'Count of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\ntarget_count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## &#9703; Categorical & Numerical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cols():\n    cat_cols = [col for col in df.columns if df[col].dtypes == \"O\"]\n    if len(cat_cols) == 0:\n        print(\"There is not Categorical Column\")\n    else:\n        print(\"Number of Categorical Column: \", len(cat_cols),\"\\n\",cat_cols)\n    \n    num_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n    if len(num_cols) == 0:\n        print(\"There is not Numerical Column\")\n    print(\"Number of Numerical Columns: \", len(num_cols),\"\\n\",num_cols)\ncols()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# &#128270; Missing Values"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-danger\" role=\"alert\">\n    In this data set <b>NA</b> are filled with 0\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Values Table Function\ndef missing_values_table(dataframe):\n    \n    variables_with_na = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[variables_with_na].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[variables_with_na].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    dtypes = dataframe.dtypes\n    dtypesna = dtypes.loc[(np.sum(dataframe.isnull()) != 0)]\n    missing_df = pd.concat([n_miss, np.round(ratio, 2), dtypesna], axis=1, keys=['n_miss', 'ratio', 'type'])\n    if len(missing_df)>0:\n        print(missing_df)\n        print(\"\\nThere are {} columns with missing values\\n\".format(len(missing_df)))\n    else:\n        print(\"\\nThere is no missing value\") \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This features can not be 0!\nmissing = [\"Glucose\", \"BMI\", \"BloodPressure\", \"SkinThickness\", \"Insulin\"]\n\nfor i in missing:\n    df[i] = np.where(df[i] == 0, np.nan, df[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## &#9850; Filling NA Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The missing values will be filled with the median values of each variable\ndef median_target(variable):   \n    temp = df[df[variable].notnull()]\n    temp = temp[[variable, 'Outcome']].groupby(['Outcome'])[[variable]].median().reset_index()\n    return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The values to be given for incomplete observations are given the median value of people who are not sick and the median values of people who are sick\ncolumns = df.columns\ncolumns = columns.drop(\"Outcome\")\nfor i in columns:\n    median_target(i)\n    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]\n    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------------"},{"metadata":{},"cell_type":"markdown","source":"# &#128202; Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(bins=20,color = \"#F19C1F\",edgecolor='white',figsize = (15,15));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Between Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot = True, fmt = \".2f\", cmap = \"viridis\", figsize=(11,11))\nplt.title(\"Correlation Between Features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------------\n# &#9823; Base Models | LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split X and y\ny = df[\"Outcome\"]\nX = df.drop([\"Outcome\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_model = LogisticRegression().fit(X,y)\ny_pred = log_model.predict(X)\nprint(\"Accuracy Score:\", accuracy_score(y, y_pred), \"\\n\")\nprint(classification_report(y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GBM',GradientBoostingClassifier()))\nmodels.append(('XGB', GradientBoostingClassifier()))\nmodels.append((\"LightGBM\", LGBMClassifier()))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    \n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## &#128202; Base Model: LightGBM Feature Importance"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"Importance = pd.DataFrame({'Importance':LGBMClassifier().fit(X, y).feature_importances_*100}, \n                          index = X.columns)\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = '#25B89B', figsize=(10,6))\n\nplt.xlabel('LightGBM Feature Importance')\nplt.gca().legend_ = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------------------------\n# &#128296; Feature Engineering\n"},{"metadata":{},"cell_type":"markdown","source":"## Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.Insulin.quantile(0.25)\nQ3 = df.Insulin.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"Insulin\"] > upper,\"Insulin\"] = upper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.boxplot(x = df[\"Insulin\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# According to feature importance i'm creating new features.\ndf2 = df.copy()\n\ndf2[\"Insulin/Age\"]=df2[\"Insulin\"]/df2[\"Age\"]\ndf2[\"BMI/Age\"]=df2[\"BMI\"]/df2[\"Age\"]\ndf2[\"Pregnancies/Age\"]=df2[\"Pregnancies\"]/df2[\"Age\"]\ndf2[\"Ins*Glu\"]=df2[\"Insulin\"]* df2[\"Glucose\"]\ndf2.drop([\"Age\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Edit Features"},{"metadata":{},"cell_type":"markdown","source":"### - BMI Levels"},{"metadata":{},"cell_type":"markdown","source":"<center><img\nsrc=\"https://www.oguzerdogan.com/wp-content/uploads/2020/10/BMI.jpg\">\n</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['New_BMI'] = pd.cut(x = df['BMI'], bins = [0,18.5, 24.9, 29.9, 100], labels = [\"Underweight\", \n                                                                                  \"NormalWeight\", \n                                                                                  \"Overweight\", \n                                                                                  \"Obes\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Blood Pressure Levels"},{"metadata":{},"cell_type":"markdown","source":"<center><img\nsrc=\"https://www.oguzerdogan.com/wp-content/uploads/2020/10/blood.jpg\">\n</center>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['New_BloodPressure'] = pd.cut(x = df['BloodPressure'], bins = [0,80, 90, 120, 122], labels = [\"Normal\", \n                                                                                                \"Hyper_St1\", \n                                                                                                \"Hyper_St2\", \n                                                                                                \"Hyper_Emer\"])\n#reference: American Heart Association","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Glucose Levels"},{"metadata":{},"cell_type":"markdown","source":"<center><img\nsrc=\"https://www.oguzerdogan.com/wp-content/uploads/2020/10/glucose.jpg\">\n</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2[\"New_Glucose\"] = pd.cut(x = df[\"Glucose\"], bins = [0,140,200,300], labels = [\"Normal\",\n                                                                                \"Prediabetes\",\n                                                                                \"Diabetes\"])\n#reference: https://emedicine.medscape.com/article/2049402-overview","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Insulin Levels"},{"metadata":{"trusted":true},"cell_type":"code","source":"#A categorical variable creation process is performed according to the insulin value.\ndef set_insulin(row): \n    if row[\"Insulin\"] >= 100 and row[\"Insulin\"] <= 126:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2.assign(NewInsulinScore=df2.apply(set_insulin, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------\n# &#8690; One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encoder(dataframe, categorical_columns, nan_as_category=False):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe, columns=categorical_columns,\n                               dummy_na=nan_as_category, drop_first=True)\n    new_columns = [col for col in dataframe.columns if col not in original_columns]\n    return dataframe, new_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = [col for col in df2.columns\n                           if len(df2[col].unique()) <= 10\n                      and col != \"Outcome\"]\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2, new_cols_ohe = one_hot_encoder(df2,categorical_columns)\nnew_cols_ohe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------\n# &#128270; LOF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\nlof =LocalOutlierFactor(n_neighbors= 20)\nlof.fit_predict(df2)\ndf_scores = lof.negative_outlier_factor_\nnp.sort(df_scores)[0:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"th = np.sort(df_scores)[6]\nth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We delete those that are higher than the threshold value\ndf2 = df2[df_scores > th]\ndf2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------\n# &#9822; New Features Model | LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df2[\"Outcome\"]\nX = df2.drop([\"Outcome\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [('RF', RandomForestClassifier()),\n          ('GBM',GradientBoostingClassifier()),\n          ('XGBM', XGBClassifier()),\n          (\"LightGBM\", LGBMClassifier())]\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------------\n# &#9881;Model Tuning Steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGBM Tuned Model\n\nlgbm_tuned = LGBMClassifier(colsample_bytree = 0.5, \n                            learning_rate = 0.01,\n                            max_depth = 6,\n                            n_estimators = 500).fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GBM Tuned Model\n\ngbm_tuned = GradientBoostingClassifier(learning_rate = 0.1,\n                                      max_depth = 3,\n                                      n_estimators = 200,\n                                      subsample = 0.8).fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBoost Tuned Model\n\nxgb_tuned = XGBClassifier(learning_rate = 0.01,\n                         max_depth = 3,\n                         n_estimators =1000,\n                         subsample = 1.0).fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model in turn\nmodels = [(\"GBM\", gbm_tuned),\n         (\"XGBoost\", xgb_tuned),\n         (\"LightGBM\", lgbm_tuned)]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------\n# &#9812;Final Model | LightGBM: 0.90"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('LightGBM: 0.90 Accuracy')\nplt.title(\"Feature Importance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Overfitting\nlog_model = LogisticRegression().fit(X,y)\ny_pred = log_model.predict(X)\nprint(accuracy_score(y, y_pred))\nprint(classification_report(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\" style=\"margin-top: 20px\">\n\n<h1>REPORT</h1>\n\n<p><strong>The aim of this study</strong> was to create classification models for the diabetes data set and to predict whether a person is sick by establishing models and to obtain maximum validation scores in the established models. Here the steps;</p>\n<p><strong>Exploratory Data Analysis:</strong>  The data set&#39;s structural data were checked. The types of variables in the dataset were examined. Size information of the dataset was accessed. The 0 values in the data set are missing values. Primarily these 0 values were replaced with NaN values. Descriptive statistics of the data set were examined.</p>\n<p><strong>Data Preprocessing section;</strong> The NaN values missing observations were filled with the median values of whether each variable was diabetic or not. The outliers were determined by LOF and dropped. </p>\n<p><strong>In model building;</strong> first, the base model was create and the test results were checked. Then categorical variables were edited and new features were added to the model.</p>\n<p><strong>During Model Building;</strong> Logistic Regression, KNN, CART, Random Forests, GBM, XGBoost, LightGBM like using machine learning models Cross Validation Score were calculated. </p>\n<p><strong>According to test results;</strong> GBM, XGBoost, LightGBM hyperparameter optimizations optimized to increase Cross Validation value.</p>\n<p><strong>The model with the highest score after Hyper Parameter optimization was LGBM with 0.90 cross validation score</strong></p>\n\n\n\n</div>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}