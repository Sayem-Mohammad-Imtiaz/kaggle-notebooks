{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Mohammed Furkhan, Shaikh"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchtext\nimport torch.nn as nn\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport random\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load The dataset and preprocess\nOnly require comment and rating columns from the reviews csv file"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_df = pd.read_csv('/kaggle/input/boardgamegeek-reviews/bgg-15m-reviews.csv',usecols=[ \"rating\", \"comment\"])[[\"comment\", \"rating\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There seems to missing values in comment column. Lets remove all those rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data_df[data_df['comment'].notna()]\n### https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-in-a-certain-column-is-nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have removed the missing values from the dataset. Lets clean it up so we get only english characters.\nI'm also gonna round-up the ratings because I'm trying to solve the problem as classification problem and not regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['rating'] = df['rating'].apply(lambda x: round(x))\ndf['comment'] = df['comment'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should be able to see the class values below"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You see that there are some characters other than english alphabets and numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = re.compile(\"[^a-zA-Z ]+\")\ndf[\"comment\"] = df['comment'].map(lambda x: pattern.sub('', x))\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets also drop rows which are having very few characters or words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop rows with comment length <= 10\ndf = df[df['comment'].map(len) > 10]\nprint(len(df))\ndf = df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"thats done. Lets find the maximum comment length"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['comment'].map(len).max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a lot and we dont need all of it to predict a rating. So we will fix the length during training."},{"metadata":{},"cell_type":"markdown","source":"## Create training and testing datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rn out of RAM!\n#training_df, testing_df =train, test = train_test_split(df, test_size=0.30)\ntraining_df, testing_df = df.loc[:0.5*len(df)], df.loc[0.75*len(df):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data_df\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_df.to_csv(\"training.csv\", index=False)\ntesting_df.to_csv(\"testing.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare the dataset for pytorch torchtext\nData should be tokenized and numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = lambda x: x.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True, batch_first=True, fix_length=200)\nLABEL = torchtext.data.LabelField(dtype=torch.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fields = [('comment',TEXT),('rating', LABEL)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = torchtext.data.TabularDataset(\"training.csv\",\"csv\", fields, skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = torchtext.data.TabularDataset(\"testing.csv\",\"csv\", fields, skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see if we loaded the data properly"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.examples[0].comment, train_data.examples[0].rating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del training_df\ndel testing_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEXT.build_vocab(train_data, vectors=torchtext.vocab.GloVe(name='6B', dim=100,cache = 'output/kaggle/working/vector_cache'))\n# TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\") #some url error. Due to permissions I believe\nTEXT.build_vocab(train_data, vectors=torchtext.vocab.Vectors(\"/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\", cache = '../output/working/vector_cache'))\nLABEL.build_vocab(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embeddings = TEXT.vocab.vectors\nword_embeddings.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## create validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, valid_data = train_data.split()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Iterators for training and evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_iter, valid_iter, test_iter = torchtext.data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=32,\n                                                               sort_key=lambda x: len(x.comment),\n                                                               repeat=False,\n                                                               shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(TEXT.vocab)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(vocab_size, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embeddings.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(word_embeddings, \"word_embeddings.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dill","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"TEXT.Field\", \"wb\") as f:\n    dill.dump(TEXT, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassifierModel(nn.Module):\n    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n        super(ClassifierModel, self).__init__()\n        \"\"\"\n        output_size : 2 = (pos, neg)\n        \"\"\"\n        self.batch_size = batch_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_length = embedding_length\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)  # Initiale the look-up table.\n        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assign pre-trained GloVe word embedding.\n        self.lstm = nn.LSTM(embedding_length, hidden_size)\n        self.label = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_sentence, batch_size=None):\n        \"\"\" \n        final_output.shape = (batch_size, output_size)\n        \"\"\"\n        input = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n        if batch_size is None:\n            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n        else:\n            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n        final_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n\n        return final_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## for gradients"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clip_gradient(model, clip_value):\n    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n    for p in params:\n        p.grad.data.clamp_(-clip_value, clip_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, train_iter, epoch):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    model.to(device)\n    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n    steps = 0\n    model.train()\n    for idx, batch in enumerate(train_iter):\n        text = batch.comment[0]\n        target = batch.rating\n        target = torch.autograd.Variable(target).long()\n        if torch.cuda.is_available():\n            text = text.cuda()\n            target = target.cuda()\n        if (text.size()[0] != 32):# One of the batch has length different than 32.\n            continue\n        optim.zero_grad()\n        prediction = model(text)\n        loss = loss_fn(prediction, target)\n        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n        acc = 100.0 * num_corrects/len(batch)\n        loss.backward()\n        clip_gradient(model, 1e-1)\n        optim.step()\n        steps += 1\n        \n        if steps % 100 == 0:\n            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n        \n        total_epoch_loss += loss.item()\n        total_epoch_acc += acc.item()\n        \n    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model, val_iter):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    model.eval()\n    with torch.no_grad():\n        for idx, batch in enumerate(val_iter):\n            text = batch.comment[0]\n            if (text.size()[0] != 32):\n                continue\n            target = batch.rating\n            target = torch.autograd.Variable(target).long()\n            if torch.cuda.is_available():\n                text = text.cuda()\n                target = target.cuda()\n            prediction = model(text)\n            loss = loss_fn(prediction, target)\n            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n            acc = 100.0 * num_corrects/len(batch)\n            total_epoch_loss += loss.item()\n            total_epoch_acc += acc.item()\n\n    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\noutput_size = 11\nhidden_size = 256\nembedding_length = 100\nmodel = ClassifierModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#architecture\nprint(model)\n\n#No. of trianable parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \nprint(f'The model has {count_parameters(model):,} trainable parameters')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.001\nloss_fn = F.cross_entropy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(5):\n    train_loss, train_acc = train_model(model, train_iter, epoch)\n    val_loss, val_acc = eval_model(model, valid_iter)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = eval_model(model, test_iter)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save the weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'saved_weights.pt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## check on custom input text"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sent = \"This game is interesting\"\ntest_sent = TEXT.preprocess(test_sent)\ntest_sent = [[TEXT.vocab.stoi[x] for x in test_sent]]\ntest_sent = np.asarray(test_sent)\ntest_sent = torch.LongTensor(test_sent)\ntest_tensor = Variable(test_sent)\ntest_tensor = test_tensor.cuda()\nmodel.eval()\noutput = model(test_tensor, 1)\nout = F.softmax(output, 1)\nout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"rating\",torch.argmax(out[0]).item())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References"},{"metadata":{},"cell_type":"markdown","source":"#### https://pytorch.org/text/stable/data.html\n#### https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n#### https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n#### https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/main.py\n#### https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n#### https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496"},{"metadata":{},"cell_type":"markdown","source":"## Contribution and Findings"},{"metadata":{},"cell_type":"markdown","source":"1. Data cleaning and preparation\n2. Explicityly based on torchtext and self preprocessed dataset.\n3. Different WordEmbedding Vectors and parameters\n4. Optimized the hyperparameters empirically\n5. Classifier based on 11 classes 0 - 10\n6. Deploying on cloud\n7. Faster processing using sclied data"},{"metadata":{},"cell_type":"markdown","source":"- Hyperparameters\n1. The values for embedding vectors and their dimensions can increase the number of parameters required by the program.\n2. Batch Size can be 16, 32, 64, .. In this notebook I have used 32.\n3. The number of layers in the model can be increased but not necessarily may have better results.\n4. The input length has been fixed at 200 characters but can be increased. The smaller text will be padded by default."},{"metadata":{},"cell_type":"markdown","source":"- Overfitting\n1. The model training accuracy and loss are closely related to the validation accuracy and loss\n2. The model does not overfit. Also I had to use less amount of data due to resource limits"},{"metadata":{},"cell_type":"markdown","source":"## Why use embedding vectors?/What does the embeddings do?"},{"metadata":{},"cell_type":"markdown","source":"The Embedding vectors defines the relations between different words based on several features. For example King is related to Queen just like a Man is related to Women. Another generic example is oange and apple, both are fruits and the relation is defined by embeddings."},{"metadata":{},"cell_type":"markdown","source":"## What is LSTM?"},{"metadata":{},"cell_type":"markdown","source":"LSTM (Long Short Term Memory) is recurrent neural network model and is mostly used for processing sequential data. Like in our case the text data is sequential by nature. Hence LSTM is usefull for NLP tasks. It is also a powerful model compared to Vanilla RNN. There are different variants of LSTM which can be experimented with"},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{},"cell_type":"markdown","source":"After 10 epochs on the dataset, average training accuracy was around 39.7% and validation accuracy about 36%. Surely this numbers can be increased by tuning the hyperparameters defined above, and training more."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}