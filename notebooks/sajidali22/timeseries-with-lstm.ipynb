{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/london-bike-sharing-dataset/london_merged.csv',\n                   parse_dates=['timestamp'], index_col='timestamp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['hour'] = data.index.hour\ndata['day_of_the_week'] = data.index.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['day_of_month'] = data.index.day\ndata['month'] = data.index.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x=data.index, y='cnt', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_by_month = data.resample('M').sum()\nsns.lineplot(x=data_by_month.index, y='cnt', data=data_by_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pointplot(data=data, x='hour', y='cnt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6-10 high\n16-19 another spike\n0-5 low \nafter 19 dip due to night"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pointplot(data=data, x='hour', y='cnt', hue='is_holiday')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"diff btw is holiday"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pointplot(data=data, x='day_of_the_week', y='cnt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int(len(data) * 0.9)\ntest_size = len(data) - train_size\ntrain , test = data.iloc[:train_size], data.iloc[train_size:]\n\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nf_columns = ['t1', 't2', 'hum', 'wind_speed']\n\nf_transformer = RobustScaler()\ncnt_transformer = RobustScaler()\n\nf_transformer = f_transformer.fit(train[f_columns].to_numpy())\ncnt_transformer = cnt_transformer.fit(train[['cnt']])\n\ntrain.loc[:, f_columns] = f_transformer.transform(train[f_columns].to_numpy())\ntrain['cnt'] = cnt_transformer.transform(train[['cnt']])\n\ntest.loc[:, f_columns] = f_transformer.transform(test[f_columns].to_numpy())\ntest['cnt'] = cnt_transformer.transform(test[['cnt']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_transformer.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(X, y, time_steps=1):\n    Xs, ys= [], []\n    for i in range(len(X)-time_steps):\n        v = X.iloc[i:i+time_steps].values\n        Xs.append(v)\n        ys.append(y.iloc[i+time_steps])\n    return np.array(Xs), np.array(ys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TIME_STEPS = 24\n\nX_train, y_train = create_dataset(train, train.cnt, time_steps=TIME_STEPS)\nX_test , y_test = create_dataset(test, test.cnt, time_steps=TIME_STEPS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#[samples, time_steps, n_features]\nprint(X_train.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(keras.layers.Bidirectional(\n            keras.layers.LSTM(\n            units=128,\n            input_shape=(X_train.shape[1], X_train.shape[2])\n            )))\nmodel.add(keras.layers.Dropout(rate=0.2))\nmodel.add(keras.layers.Dense(units=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='mean_squared_error', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=30, batch_size=32,\n                   validation_split=0.1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_inv = cnt_transformer.inverse_transform(y_train.reshape(1, -1))\ny_test_inv = cnt_transformer.inverse_transform(y_test.reshape(1, -1))\ny_pred_inv = cnt_transformer.inverse_transform(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_test_inv.flatten(), marker='.', label='true')\nplt.plot(y_pred_inv.flatten(),'r', label='prediciton')\nplt.ylabel('Bike Count')\nplt.xlabel('Time Step')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.arange(0, len(y_train)), y_train_inv.flatten(), 'g', label=\"history\")\nplt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_test_inv.flatten(), marker='.', label=\"true\")\nplt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_pred_inv.flatten(), 'r', label=\"prediction\")\nplt.ylabel('Bike Count')\nplt.xlabel('Time Step')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_inv = (tf.squeeze(y_pred_inv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.metrics.mean_absolute_error(y_pred_inv, y_test).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=(X_train.shape[1], X_train.shape[2])),\n  tf.keras.layers.Bidirectional(keras.layers.LSTM(128)),\n  tf.keras.layers.Dense(1)\n])\n\noptimizer = keras.optimizers.Adam()\nnew_model.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nnew_history = new_model.fit(X_train, y_train, epochs=30, batch_size=32,\n                   validation_split=0.1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cyclic Learning Rates"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_conv_BI_model():\n    return tf.keras.models.Sequential([\n        tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=(X_train.shape[1], X_train.shape[2])),\n  tf.keras.layers.Bidirectional(keras.layers.LSTM(128)),\n  tf.keras.layers.Dense(1)\n])\n\ndef compile_and_fit_model(model,optimizer, epochs, batch_size, callback):\n    model.compile(loss=tf.keras.losses.Huber(),\n                  optimizer=optimizer,\n                  metrics=[\"mae\"])\n    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n                   validation_split=0.1,callbacks=callback, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import Callback\nimport keras.backend as K\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass LRFinder(Callback):\n    def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None, \n                 reload_weights=True, batches_lr_update=5):\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.mom = mom\n        self.reload_weights = reload_weights\n        self.batches_lr_update = batches_lr_update\n        if stop_multiplier is None:\n            self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n                                                       # 10 if mom=0\n        else:\n            self.stop_multiplier = stop_multiplier\n        \n    def on_train_begin(self, logs={}):\n        p = self.params\n        try:\n            n_iterations = p['epochs']*p['samples']//p['batch_size']\n        except:\n            n_iterations = p['steps']*p['epochs']\n            \n        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n                                           num=n_iterations//self.batches_lr_update+1)\n        self.losses=[]\n        self.iteration=0\n        self.best_loss=0\n        if self.reload_weights:\n            self.model.save_weights('tmp.hdf5')\n        \n    \n    def on_batch_end(self, batch, logs={}):\n        loss = logs.get('loss')\n        \n        if self.iteration!=0: # Make loss smoother using momentum\n            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n        \n        if self.iteration==0 or loss < self.best_loss: \n                self.best_loss = loss\n                \n        if self.iteration%self.batches_lr_update==0: # Evaluate each lr over 5 epochs\n            \n            if self.reload_weights:\n                self.model.load_weights('tmp.hdf5')\n          \n            lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n            K.set_value(self.model.optimizer.lr, lr)\n\n            self.losses.append(loss)            \n\n        if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n            self.model.stop_training = True\n                \n        self.iteration += 1\n    \n    def on_train_end(self, logs=None):\n        if self.reload_weights:\n                self.model.load_weights('tmp.hdf5')\n                \n        plt.figure(figsize=(12, 6))\n        plt.plot(self.learning_rates[:len(self.losses)], self.losses)\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Loss\")\n        plt.xscale('log')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_finder = LRFinder(min_lr=1e-4, max_lr=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_conv_BI_model()\noptimizer = keras.optimizers.Adam()\ncompile_and_fit_model(model, optimizer, 5, 32, lr_finder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implement One Cycle Policy Algorithm in the Keras Callback Class\n\nfrom sklearn.metrics import log_loss, roc_auc_score, accuracy_score\nfrom keras.losses import binary_crossentropy\nfrom keras.metrics import binary_accuracy\nfrom keras import backend as K\nfrom keras.callbacks import *\n\nclass CyclicLR(keras.callbacks.Callback):\n    \n    def __init__(self,base_lr, max_lr, step_size, base_m, max_m, cyclical_momentum):\n \n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.base_m = base_m\n        self.max_m = max_m\n        self.cyclical_momentum = cyclical_momentum\n        self.step_size = step_size\n        \n        self.clr_iterations = 0.\n        self.cm_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n        \n    def clr(self):\n        \n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        \n        if cycle == 2:\n            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)          \n            return self.base_lr-(self.base_lr-self.base_lr/100)*np.maximum(0,(1-x))\n        \n        else:\n            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0,(1-x))\n    \n    def cm(self):\n        \n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        \n        if cycle == 2:\n            \n            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1) \n            return self.max_m\n        \n        else:\n            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n            return self.max_m - (self.max_m-self.base_m)*np.maximum(0,(1-x))\n        \n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n            \n        if self.cyclical_momentum == True:\n            if self.clr_iterations == 0:\n                K.set_value(self.model.optimizer.momentum, self.cm())\n            else:\n                K.set_value(self.model.optimizer.momentum, self.cm())\n            \n            \n    def on_batch_begin(self, batch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n        \n        if self.cyclical_momentum == True:\n            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n        \n        if self.cyclical_momentum == True:\n            K.set_value(self.model.optimizer.momentum, self.cm())\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam, SGD\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# CLR parameters\n\nbatch_size = 32\nepochs = 30\nmax_lr = 1e-2\nbase_lr = 1e-3\nmax_m = 0.98\nbase_m = 0.85\ncyclical_momentum = False\naugment = True\ncycles = 2.35\n\niterations = round(X_train.shape[0]/batch_size*epochs)\niterations = list(range(0,iterations+1))\nstep_size = len(iterations)/(cycles)\n\nclr =  CyclicLR(base_lr=base_lr,\n                max_lr=max_lr,\n                max_m=max_m,\n                base_m=base_m,\n                step_size=step_size,\n                cyclical_momentum=cyclical_momentum)\n    \ncallbacks = [clr,\n            ModelCheckpoint(filepath='best_model.h5', monitor='mae',mode='min',verbose=1,save_best_only=True)]\n\nmodel = get_conv_BI_model()\noptimizer = keras.optimizers.Adam()\ncompile_and_fit_model(model, optimizer, epochs, batch_size, callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(clr.history['iterations'], clr.history['lr'])\nplt.xlabel('Training Iterations')\nplt.ylabel('Learning Rate')\nplt.title(\"One Cycle Policy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loss = history.history['val_loss']\nloss = history.history['loss']\nplt.plot(range(len(val_loss)),val_loss,'c',label='Validation loss')\nplt.plot(range(len(loss)),loss,'m',label='Train loss')\n\nplt.title('Training and validation losses')\nplt.legend()\nplt.xlabel('epochs')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, test):\n    y_pred = model.predict(test)\n    y_train_inv = cnt_transformer.inverse_transform(y_train.reshape(1, -1))\n    y_test_inv = cnt_transformer.inverse_transform(y_test.reshape(1, -1))\n    y_pred_inv = cnt_transformer.inverse_transform(y_pred)\n    y_pred_inv = (tf.squeeze(y_pred_inv))\n    return y_predict_inv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred_inv = cnt_transformer.inverse_transform(y_pred)\ny_pred_inv = (tf.squeeze(y_pred_inv))\nkeras.metrics.mean_absolute_error(y_pred_inv, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"improved from 999 to 952"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_test_inv.flatten(), marker='.', label='true')\nplt.plot(y_pred_inv,'r', label='prediciton')\nplt.ylabel('Bike Count')\nplt.xlabel('Time Step')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}