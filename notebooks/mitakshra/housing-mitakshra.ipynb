{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/house-price-prediction-challenge'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport pandas as pd \nimport math\nimport seaborn as sns\n#from sklearn.preprocessing import Imputer\nfrom sklearn.metrics import precision_recall_fscore_support, roc_curve, auc,mean_squared_error\n# Managing Warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n#statistics package\nimport statsmodels.formula.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as s\ntest = pd.read_csv(\"test.csv\")\ntrain = pd.read_csv(\"train.csv\")\ndata = pd.read_csv('train.csv')\ntestt = pd.read_csv('test.csv')\nprint(data.describe())\n#print(len(data))\nsample = data.iloc[0:7000,:]\nprint(data.info())\ny = sample['TARGET(PRICE_IN_LACS)']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"pd.set_option('display.max_columns',None)#####to display all columns\ndata.describe(include='object')\n\nsample.corr()\n#univariate analysis\nprint(sns.countplot(x = 'UNDER_CONSTRUCTION',data=sample))\nprint(pd.crosstab(sample['UNDER_CONSTRUCTION'],columns='count'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(sns.countplot(x='RERA',data=sample))\nprint(pd.crosstab(sample['RERA'],columns='count'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(sns.countplot(x = 'READY_TO_MOVE',data=sample))\nprint(pd.crosstab(sample['READY_TO_MOVE'],columns='count'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#univariate analysis\n#betterway to visulaize house price\nfig = plt.figure(figsize=(10,7))\nfig.add_subplot(2,1,1)\n#sns.displot(sample['TARGET(PRICE_IN_LACS)'])\n#fig.add_subplot(2,1,2)\n#sns.boxplot(sample['TARGET(PRICE_IN_LACS)'])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nmodel = LinearRegression()\ncol = ['SQUARE_FT','READY_TO_MOVE']\nx = sample[col].values\ny = sample['TARGET(PRICE_IN_LACS)'].values\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)\nmodel.fit(x_train,y_train)\npred = model.predict(x_test)\n\nprint(mean_squared_error(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nmodel = LinearRegression()\ncol = ['SQUARE_FT','READY_TO_MOVE']\nx = sample[col].values\ny = sample['TARGET(PRICE_IN_LACS)'].values\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)\nmodel.fit(x_train,y_train)\npred = model.predict(x_test)\n\nprint(mean_squared_error(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot residuals or errors to show that the error distribution is normal\n# which further shows the dependent and independent have a linear relationship\nerror = (y_test-pred)\n#sns.displot(error,kde=True)\n#on plotting we have found that error is normally distributed\nfrom sklearn.preprocessing import StandardScaler\ns_scaler = StandardScaler()\nx_train = s_scaler.fit_transform(x_train.astype(np.float))\nx_test = s_scaler.fit_transform(x_test.astype(np.float))\nmodel.fit(x_train,y_train)\npred = model.predict(x_test)\nprint(mean_squared_error(y_test,pred))\nerror = (y_test-pred)\n#sns.displot(error,kde=True)\n#on plotting we have found that error is normally distributed\ncol = ['SQUARE_FT','READY_TO_MOVE']\nxx = data[col].values\nyy = data['TARGET(PRICE_IN_LACS)'].values\nprint(len(data))\n#remove outlier \n#WE CAN SEE FROM BOX PLOT THAT PRICE HAS LOT OF OUTLIER\n#WE HAVE TO REMOVE THEM\n#LOWER LIMIT 38-1.5IQR  = 38-31 = 7\n#UPPER LIMIT 100+1.5IQR = 100+3 = 193\n\ndata = data[data['TARGET(PRICE_IN_LACS)']<165.0]\nprint(len(data))\ncol = ['SQUARE_FT','READY_TO_MOVE']\nxx = data[col].values\nyy = data['TARGET(PRICE_IN_LACS)'].values\nprint(xx.shape,yy.shape)\ntrain_x,test_x,train_y,test_y = train_test_split(xx,yy,test_size=0.4)\nmodel.fit(train_x,train_y)\npredd = model.predict(test_x)\nmean_squared_error(test_y,predd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nnp.set_printoptions(threshold=np.inf)\nkf = KFold(n_splits=5)\nl=[]\nfor train_index,test_index in kf.split(xx):\n  #print(train_index,test_index)\n  xx_train,xx_test=xx[train_index],xx[test_index]\n  yy_train,yy_test=yy[train_index],yy[test_index]\n  model.fit(xx_train,yy_train)\n  predd = model.predict(xx_test)\n  errorr = mean_squared_error(yy_test,predd)\n  l.append(error)\n  #print(sum(l))\n  #print(mean_squared_error(yy_test,predd))\n#print(l)#6204\ncol = ['SQUARE_FT','READY_TO_MOVE']\nt = testt[col].values\npd.set_option('display.max_columns',None)#####to display all columns\ny_pred = model.predict(t)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5)\nl=[]\nregressor = DecisionTreeRegressor(random_state=0)\nfor train_index,test_index in kf.split(xx):\n  #print(train_index,test_index)\n  xx_train,xx_test=xx[train_index],xx[test_index]\n  yy_train,yy_test=yy[train_index],yy[test_index]\n  regressor.fit(xx_train,yy_train)\n  predd = regressor.predict(xx_test)\n  errorr = mean_squared_error(yy_test,predd)\n  l.append(error)\n  #print(sum(l))\n  print(mean_squared_error(yy_test,predd))\nregressor = DecisionTreeRegressor(random_state=0.0)#6346","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}