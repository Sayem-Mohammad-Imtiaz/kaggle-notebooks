{"cells":[{"metadata":{},"cell_type":"markdown","source":"# HR Analytics - Classifying Employees Attrition\n## goals:\nThe main goal of this project is to get some useful information and insights about what contributes to employees feeling burnout, fatigue and attrition.\nEmployees attrition has a great amount of consequences to the company, and having the ability to know beforehand which employee is more proned to leave, can help mitigating some of those negative effects.\nThe area of Human Resource Analysis has a big weight in the field of labour market, thus expanding the knowledge about it is important.\n\n\n## data sources:\nthe data we're about to analyze is taken from Kaggle: https://www.kaggle.com/vjchoudhary7/hr-analytics-case-study and contains a number of indicators about each Employee(N=4410), as well as his attrition status('Yes' or No).\nThe data contained 30 features after cleaning was done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, RandomizedSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score, accuracy_score, roc_auc_score, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### working stages:\n\n#### 1) load the data\n\n#### 2) clean the data\n\n#### 3) explore the data\n\n#### 4) handle null values\n\n#### 5) feature engeneering\n\n#### 6) model comparison\n\n#### 7) model selection\n\n#### 8) tuning the model\n\n#### note: part 2-3-4 may often come in a different order, depends on the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nemp_data = pd.read_csv(\"../input/hr-analytics-case-study/employee_survey_data.csv\", index_col='EmployeeID')\ngen_data = pd.read_csv(\"../input/hr-analytics-case-study/general_data.csv\",index_col='EmployeeID')\nmanager_data = pd.read_csv(\"../input/hr-analytics-case-study/manager_survey_data.csv\",index_col='EmployeeID')\nin_time_data = pd.read_csv(\"../input/hr-analytics-case-study/in_time.csv\")\nout_time_data = pd.read_csv(\"../input/hr-analytics-case-study/out_time.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time_data.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)\nin_time_data.set_index('EmployeeID', inplace=True)\nin_time_data\nout_time_data.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)\nout_time_data.set_index('EmployeeID', inplace=True)\nout_time_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time_data = in_time_data.apply(pd.DatetimeIndex)\nout_time_data = out_time_data.apply(pd.DatetimeIndex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times = pd.concat([in_time_data, out_time_data], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times = times.applymap(lambda x: x.hour+0.01*x.minute)\ntimes['avg_in'] = round(times.iloc[:, :261].mean(axis=1),2)\ntimes['avg_out'] = round(times.iloc[:, 261:].mean(axis=1),2)\ntimes['med_in'] = round(times.iloc[:, :261].median(axis=1),2)\ntimes['med_out'] = round(times.iloc[:, 261:].median(axis=1),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,3, figsize = (12,4))\nsns.distplot(times.iloc[4, :261], ax=axs[0]).set(xlabel = 'In time', ylabel = 'Frequency',xlim=(7,12))\nsns.distplot(times.iloc[72, :261], ax=axs[1]).set(xlabel = 'In time', ylabel = 'Frequency',xlim=(7,12))\nsns.distplot(times.iloc[102, :261], ax=axs[2]).set(xlabel = 'In time', ylabel = 'Frequency',xlim=(7,12))\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\ng = plt.plot(times.iloc[4, :261])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\ng = plt.plot(times.iloc[25, :261])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times['total'] = times['med_out'] - times['med_in']\ntime_feats = times[['avg_in', 'avg_out', 'med_in','med_out','total']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manager_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_data.loc[emp_data['EnvironmentSatisfaction'].isnull()]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- there's some null values, let's explore the data a little bit to see how can can handle it properly</li>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,3, figsize=(10,4))\nsns.barplot(emp_data['EnvironmentSatisfaction'], emp_data['JobSatisfaction'],ax=axs[0])\nsns.barplot(emp_data['WorkLifeBalance'], emp_data['JobSatisfaction'],ax=axs[1])\nsns.barplot(emp_data['WorkLifeBalance'], emp_data['EnvironmentSatisfaction'],ax=axs[2])\nplt.tight_layout(pad=3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(emp_data, col='WorkLifeBalance',size=2.4, aspect=2, col_wrap=2 )\ng = g.map(sns.distplot, 'JobSatisfaction', )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(emp_data, col='JobSatisfaction',row ='WorkLifeBalance', size=2.4, aspect=2 )\ng = g.map(sns.distplot, 'EnvironmentSatisfaction' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No clear connection between the variables, but in order to be on the safe side, we'll handle the nulls by the conditional mode based on the other two columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_mode(data, col, col2, col3):\n    index_nan = list(data[col][data[col].isnull()].index)\n    for i in index_nan:\n        cols_mode = data[col].mode()[0]\n        mode_fill = data[col][((data[col2] == data.loc[i][col2]) & (data[col3] == data.loc[i][col3]))].mode()[0]\n        data[col].loc[i] = mode_fill\n\n\n    \n                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_mode(emp_data, 'EnvironmentSatisfaction','JobSatisfaction','WorkLifeBalance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_mode(emp_data, 'JobSatisfaction','EnvironmentSatisfaction','WorkLifeBalance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_mode(emp_data, 'WorkLifeBalance','JobSatisfaction','EnvironmentSatisfaction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alright, we can now merge the dataframes and explore the data as a whole","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([gen_data,manager_data,emp_data,time_feats], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns.values:\n    if df[col].nunique() == 1:\n        df.drop(col, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Attrition'] = np.where(df['Attrition']=='Yes',1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.NumCompaniesWorked.fillna(0, inplace=True)\ndf.TotalWorkingYears.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.distplot(df['total']).set(xlabel = 'Total Hours Of Work', ylabel = 'Frequency')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking the exact number of people by hours of work\nprint(df['total'][df['total'] <=7].value_counts().sum())\nprint(df['total'][(df['total'] > 7) & (df['total'] <=8)].value_counts().sum())\nprint(df['total'][df['total'] > 8].value_counts().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nsns.distplot(df['avg_in'], bins=20)\nplt.subplot(1,2,2)\ns = sns.distplot(df['avg_out'], bins=20)\nplt.xticks((range(16,22)))\nplt.tight_layout(pad=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nsns.distplot(df['med_in'], bins=10)\nplt.subplot(1,2,2)\ns = sns.distplot(df['med_out'], bins=20)\nplt.xticks((range(16,22)))\nplt.tight_layout(pad=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot, 'total')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot, 'med_in')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'med_out')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'avg_in')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition' )\ng = g.map(sns.distplot , 'avg_out')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### both the median and the mean in/out time, as well as the total time, show us that people who work more, but pushing the working hours till late, are more likely to attrit","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.subplot(1,3,1)\ng = sns.barplot(df['Attrition'], df['EnvironmentSatisfaction'], hue=df['JobSatisfaction'])\nplt.legend(loc ='upper right')\nplt.subplot(1,3,2)\ns = sns.barplot(df['Attrition'], df['JobInvolvement'], hue=df['JobSatisfaction'])\nplt.legend(loc ='upper right')\nplt.subplot(1,3,3)\nf = sns.barplot(df['Attrition'], df['WorkLifeBalance'], hue=df['JobSatisfaction'])\nplt.legend(loc ='upper right')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['BusinessTravel', 'Department','EducationField', 'Gender',\n       'JobRole', 'MaritalStatus','JobInvolvement', 'PerformanceRating',\n       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobLevel']\nplt.figure(figsize=(15,18))\nfor i in range(len(cat_cols)):\n    plt.subplot(4,3,i+1)\n    sns.countplot(df[cat_cols[i]], hue=df['Attrition'])\n    if len(df[cat_cols[i]].unique()) >= 3:\n        plt.xticks(rotation=75)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Att_ratio(data, col):\n    col_values = data[col].unique()\n    print('For',col, ':')\n    for index, item in enumerate(col_values):\n        ratio = len(df.loc[(df[col] == col_values[index]) & (df['Attrition'] == 1)])/len(df.loc[(df[col] == col_values[index]) & (df['Attrition'] == 0)])\n        print('The Attrition ratio(Yes/No) under the category %s is %f' %(item, ratio))\n    print('-----------------------------------------------------------------------------------------------')\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['BusinessTravel', 'Department','EducationField', 'Gender',\n       'JobRole', 'MaritalStatus','JobInvolvement', 'PerformanceRating',\n       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobLevel']\nfor col in cat_cols:\n    Att_ratio(df, col)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Gender'].replace({'Female': 1, 'Male': 0}, inplace=True)\ndf['BusinessTravel'].replace({'Travel_Rarely': 1,'Travel_Frequently':2,'Non-Travel':0 }, inplace=True)\ncat_cols = ['Gender','BusinessTravel',\n       'JobInvolvement', 'PerformanceRating',\n       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobLevel']\nfor col in cat_cols:\n    print('The Attrition ration(Yes/All) For',col+':')\n    print(df.groupby([col]).Attrition.agg(['mean']))\n    print('----------------------------------------------------------')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# summary of the categorical variables:\n* BusinessTravel seems to have a bit of an influence on attrition.those who travel frequently have an attrition ratio of 0.331731 - Probably those who travel a lot for business purposes are having more stress and thus willing to leave\n\n\n* Department doesn't seem to be correlated strongly but Human Resources attrition ratio is quite high at 0.431818 - maybe that's because of the low salary, we'll check this one later \n\n\n* EducationField seems to be a related to Department in some way, for Human Resources the ratio is 0.687500\n\n\n* Gender doesn't seem correlated - Male attrition ratio is 0.200000 - not significantly more than females\n\n\n* JobRole - the attrition ratio for Research Director is 0.311475 - that's not surprising considering the fact that the largest department is Research and Development. Research director is a role with a lot of responsibility,and that definetely can contribute to the the overall fatigue and burnout.\n\n\n* MaritalStatus - Single status has a slightly bigger ratio with 0.342857 - maybe mediated by low salary/short term job\n\n\n* JobInvolvement - category 1 ratio is 0.276923 - not surprising but isn't much higher than the other categories\n\n\n* PerformanceRating - category 4 ratio 0.221622 - probably those who work the hardest, but again no significantly higher\n\n\n* EnvironmentSatisfaction - here the difference is quite big, the category 1.0 attrition ratio is 0.339117 - it's pretty normal in my opinion. those who aren't satisfied with their work are not going to stay for long\n\n\n* JobSatisfaction - category 1.0 ratio is 0.297134 - same explanation but here it's not significantly higher in my opinion\n\n\n* WorkLifeBalance - here, simillarly to enviroment satisfaction, the gap is quite big, for category 1.0 the ratio is 0.457317 - the reasonable explanation is that people with low work/life balance are strongly prone to burnout, have more stress, etc..\n\n\n* JobLevel - category 2 ratio 0.216401 - very simillar to other categories therefore doesn;t have a strong impact.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Let's move on to the numerical variables:\n### We'll start with income, then the Age and the seniority\n### We'll check if there's any correlation between those variables and the prone to leave","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.MonthlyIncome)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### got some outliers in terms of income. \n### lets check the income correlations with other variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'MaritalStatus', row = 'Department')\ng = g.map(sns.distplot , 'MonthlyIncome')\ng.fig.subplots_adjust(top=1,right=1.4, wspace=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### * seems like, as i hypothesised earlier, that single workers are paid the least.\n\n#### * we can also notice that divorced people are paid the most, regardless of department.\n\n#### * doesn't seem like there is a clear connection between department and salary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## we can see it here as well, it's more useful to look at the median, as the distribution of income is skewed to the right\nprint(df.groupby(['MaritalStatus']).MonthlyIncome.agg(['mean','median']))\nprint(df.groupby(['Department']).MonthlyIncome.agg(['mean','median']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\ntop_corr = df.corr().nlargest(15, 'MonthlyIncome').index\ncm = np.corrcoef(df[top_corr].values.T)\ng = sns.heatmap(cm, cbar=True, annot=True, cmap='BrBG',yticklabels = top_corr.values, xticklabels=top_corr.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No real connection between salary and any other numerical value\n### let's have a look at the connection between attrition and salary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.distplot(df['MonthlyIncome'][df['Attrition'] == 0],color='blue')\nf = sns.distplot(df['MonthlyIncome'][df['Attrition'] == 1],color='orange')\ndf.groupby('Attrition').MonthlyIncome.agg(['mean','median'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### from looking at the graph and the table there's seem to be no correlation whatsoever between salary and attrition","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### let's take a look at the age, the years from last promotion and years at the company","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\ntop_corr = df.corr().nlargest(15, 'Age').index\ncm = np.corrcoef(df[top_corr].values.T)\ng = sns.heatmap(cm, cbar=True, annot=True, cmap='BrBG',yticklabels = top_corr.values, xticklabels=top_corr.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,3, figsize = (12,4))\nsns.distplot(df['Age'], ax=axs[0])\nsns.distplot(df['YearsSinceLastPromotion'], ax=axs[1])\nsns.distplot(df['YearsAtCompany'], ax=axs[2])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### quite normal distributed ages.\n\n### skewd to the right distribution of total years at the company\n\n### skewd to the right distribution of years since last promotion","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Age'][df['Attrition'] == 0],color='blue')\nsns.distplot(df['Age'][df['Attrition'] == 1],color='orange')\nplt.legend(['No','Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'Age')\ng.fig.subplots_adjust(top=1,right=1.2)\n#plt.tight_layout()\ndf.groupby('Attrition').Age.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"seems like a tendency to attrit among younger people,that's interesting. mayber thats because they work longer hours. let's check that","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(df['Age'], df['total'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### doesn't seem like that's the case. probably related to other things like more stress, need to prove yourself etc...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'YearsSinceLastPromotion')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').YearsSinceLastPromotion.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'YearsWithCurrManager')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').YearsWithCurrManager.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'YearsAtCompany')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').YearsAtCompany.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'DistanceFromHome')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').DistanceFromHome.agg(['median','mean'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'TotalWorkingYears')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').TotalWorkingYears.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'Education')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').Education.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'PercentSalaryHike')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').PercentSalaryHike.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'NumCompaniesWorked')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').NumCompaniesWorked.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'YearsSinceLastPromotion')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').YearsSinceLastPromotion.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col = 'Attrition')\ng = g.map(sns.distplot , 'StockOptionLevel')\ng.fig.subplots_adjust(top=1,right=1.2)\ndf.groupby('Attrition').StockOptionLevel.agg(['median','mean'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary of the numerical variables:\n\n* Looks like seniority plays somewhat of a role in the attrition attribute, along with total working years and years at the company, though there is a strong correlation beteen age and total working years, as well as total working years and years at the company, therefore there's a risk for multicolinearity. we'll handle it when we get to the model building.\n\n\n* No observed impact among all the other variables on attrition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Modeling:\n### 0) A little more feature engineering like creating age groups\n### 1) seperating x and y\n### 2) convert strings to dummy variables\n### 3) scale x features\n### 4) compare scores of different models and select the two best\n### 5) perform search grid on the two best models and see if there's an improvement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets first drop some unimportant features and variables with multicolinearity such as med_in avg_in and total etc","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop(['Attrition'], axis=1).reset_index(drop=True)\ny = df['Attrition'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_todrop = ['JobLevel','Department','JobRole','NumCompaniesWorked','PercentSalaryHike','StockOptionLevel','YearsWithCurrManager','med_in', 'avg_in','avg_out']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.drop(cols_todrop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating age groups\nx.Age = pd.cut(x.Age, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.Age.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## converting categorial variables to dummies\nx = pd.get_dummies(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_copy = x.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## scaling the features\n\nscaler = preprocessing.StandardScaler()\nx = scaler.fit_transform(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## splitting the sets into train and test\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Alright we are ready to start with the predictions!\n\n### As I've mentionted above, our working process is :\n* Compare\n\n\n* Select\n\n\n* Improve\n\n\n* Check the contribution of the features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Comparison","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a function which examines each model based on the score, then show each one's score and STD, as well as graphic comparison\n# evaluate each model in turn\ndef get_scores(score1, score2):\n    models = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('LDA', LinearDiscriminantAnalysis()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('CART', DecisionTreeClassifier()))\n    models.append(('NB', GaussianNB()))\n    models.append(('SVM', SVC()))\n    models.append(('ADA', AdaBoostClassifier()))\n    models.append(('GradientBooster', GradientBoostingClassifier()))\n    models.append(('ExtraTrees', ExtraTreesClassifier()))\n    models.append(('RandomForest', RandomForestClassifier()))\n    cv_scores = []\n    test_scores = []\n    names = []\n    stds = []\n    differences = []\n    #res = pd.DataFrame(columns = {'Model',score+('(train)'), 'Std', score+('(test_score)'), 'difference'})\n    #res = res[['Model',score+('(train)'), 'Std', score+('(test_score)'), 'difference']]\n    res = pd.DataFrame()\n    for index, model in enumerate(models):\n        kfold = StratifiedKFold(n_splits=7)\n        cv_results = cross_val_score(model[1], x_train, y_train, cv=kfold, scoring=score1)\n        cv_scores.append(cv_results)\n        names.append(model[0])\n        model[1].fit(x_train,y_train)\n        predictions = model[1].predict(x_test)\n        test_score = score2(predictions, y_test)\n        test_scores.append(test_score)\n        stds.append(cv_results.std())\n        differences.append((cv_results.mean() - test_score))\n        res.loc[index,'Model'] = model[0]\n        res.loc[index,score1+('(train)')] = cv_results.mean()\n        res.loc[index,score1+('(test_score)')] = test_score\n        res.loc[index,'Std'] = cv_results.std()\n        res.loc[index,'difference'] = cv_results.mean() - test_score\n    # boxplot algorithm comparison\n    fig = plt.figure(figsize = (12,5))\n    fig.suptitle('Model Comparison')\n    ax = fig.add_subplot(121)\n    plt.boxplot(cv_scores)\n    ax.set_xticklabels(names, rotation=70)\n    axs = fig.add_subplot(122)\n    sns.barplot(names,test_scores)\n    axs.set_xticklabels(names, rotation=70)\n    plt.tight_layout(pad=5)\n    return res\n    plt.show()\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_scores('accuracy', accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection And Tuning\n### seems like our models has a strong predicting power, especially the random forest and extra tree booster. let's check if theres any way  to improve them with random search cv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000]}\nRandomForest = RandomForestClassifier()\nrandomgrid_forest = RandomizedSearchCV(estimator=RandomForest, param_distributions = params, \n                               cv=5, n_iter=25, scoring = 'accuracy',\n                               n_jobs = 4, verbose = 3, random_state = 42,\n                               return_train_score = True)\nrandomgrid_forest.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomgrid_forest.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_preds = randomgrid_forest.predict(x_test)\nroc_auc_score(forest_preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomgrid_forest.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A little bit of improvement indeed! let's try tuning it a little bit more","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### I created a function which take a model and scoring method, then shows the cross validation score for each estimator\n### and plot it next to the test score.\ndef estimators_compare(model, cv_score, metrics_score):\n    train_scores = []\n    test_scores= []\n    estimators = [80,100,200,400,600,800,1200]\n    res = pd.DataFrame(columns = {'Number Of Estimators', 'train_score', 'test_score'})\n    for ind, i in enumerate(estimators):\n        mode = model(n_estimators=i)\n        kfold = StratifiedKFold(n_splits=7)\n        cv_results = cross_val_score(mode, x_train, y_train, cv=kfold, scoring=cv_score)\n        mode.fit(x_train, y_train)\n        predictions = mode.predict(x_test)\n        train_score = cv_results.mean()\n        train_scores.append(train_score)\n        test_score = metrics_score(predictions, y_test)\n        test_scores.append(test_score)\n        res.loc[ind,'Number Of Estimators'] = i\n        res.loc[ind,'train_score'] = train_score\n        res.loc[ind,'test_score'] = test_score\n\n    plt.plot(estimators, train_scores, color='red')\n    plt.plot(estimators, test_scores, color='blue')\n    legs = ['train', 'test']\n    plt.legend(legs)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators_compare(RandomForestClassifier, 'accuracy', accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### let's compare 100 estimators with 600 like the grid search provided","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_random_forest = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=40, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nfinal_random_forest.fit(x_train, y_train)\nfinal_random_forest.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Well, Looks like our randomized grid search produced simmilar results. before we move on to the extra trees, let's have a look at the contribution of each feature to our prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"featuers_coefficients = final_random_forest.feature_importances_.tolist()\nfeature_names = x_copy.columns\nfeats = pd.DataFrame(pd.Series(featuers_coefficients, feature_names).sort_values(ascending=False),columns=['Coefficient'])\nfeats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## same process for the Extra Trees model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params2 = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000]}\nExtraTress = ExtraTreesClassifier()\nrandomgrid_extrees = RandomizedSearchCV(estimator=ExtraTress, param_distributions = params2, \n                               cv=5, n_iter=25, scoring = 'accuracy',\n                               n_jobs = 4, verbose = 3, random_state = 42,\n                               return_train_score = True)\nrandomgrid_extrees.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomgrid_extrees.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomgrid_extrees.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators_compare(ExtraTreesClassifier, 'accuracy', accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's go with 100 estimators!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_extra_trees_A = ExtraTreesClassifier(max_depth=40, max_features='sqrt', n_estimators=600)\nfinal_extra_trees_A.fit(x_train, y_train)\nprint(final_extra_trees_A.score(x_test, y_test))\nfinal_extra_trees_B = ExtraTreesClassifier(max_depth=40, max_features='sqrt', n_estimators=100)\nfinal_extra_trees_B.fit(x_train, y_train)\nprint(final_extra_trees_B.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### again, our random grid search had it on point when combining the parameters together. let's check the coefficients than wrap it up.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"featuers_coefficients = final_extra_trees.feature_importances_.tolist()\nfeature_names = x_copy.columns\nfeats = pd.DataFrame(pd.Series(featuers_coefficients, feature_names).sort_values(ascending=False),columns=['Coefficient'])\nfeats","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}