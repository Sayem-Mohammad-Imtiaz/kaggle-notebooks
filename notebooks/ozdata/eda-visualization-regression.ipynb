{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nimport warnings\n# suppress warnings later\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport matplotlib\nimport seaborn as sns\nimport missingno\nfrom collections import Counter\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/insurance/insurance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df['charges']\ndf = df.drop('charges', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df, target, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\n## First look at distributions within attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"matplotlib.style.use('seaborn')\nfigure, ax = plt.subplots(nrows=3, ncols=2, figsize = (8,8))\n\nplt.subplot(3,2,1)\nplt.hist(X_train.age, bins = 20)\nplt.title('Age distribution')\nplt.subplot(3,2,2)\nplt.bar(list(Counter(X_train.sex).keys()), height = list(Counter(X_train.sex).values()))\nplt.title('Distribution by gender')\nplt.subplot(3,2,3)\nplt.bar(list(Counter(X_train.children).keys()), height = list(Counter(X_train.children).values()))\nplt.title('Number of children')\nplt.subplot(3,2,4)\nplt.hist(X_train.bmi, bins = 15)\nplt.title('BMI distribution')\nplt.subplot(3,2,5)\nplt.bar(list(Counter(X_train.smoker).keys()), height = list(Counter(X_train.smoker).values()))\nplt.title('Smoker distribution')\nplt.subplot(3,2,6)\nplt.bar(list(Counter(X_train.region).keys()), height = list(Counter(X_train.region).values()))\nplt.title('Region Distribution')\nfigure.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a dataset with pretty balanced attributes - region, sex and age are fairly uniform. The big majority are non smokers and the number of children covered by insurance decays as expected. "},{"metadata":{},"cell_type":"markdown","source":"## The Correlation Matrix \n\nLet's first start by transforming all categorical variables to have numeric values.\n\nAfter producing the correlation matrix we will dive deeper into a few things we will hopefully infer from it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_transform(df):\n    for att in ['sex', 'smoker','region']:\n        # transform data type to category\n        df[att] = df[att].astype('category')\n        # use cat.codes for encoding into numeric\n        df[att] = df[att].cat.codes\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = cat_transform(X_train)\nX_test = cat_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting correlation matrix over all attributes including the target\nall_att = X_train.join(y_train)\nall_att.head()\ncorrelations = all_att.corr()\nmask = np.triu(np.ones_like(correlations, dtype = np.bool))\n\nf, ax = plt.subplots(figsize = (7,7))\ncmap = sns.diverging_palette(220,10, as_cmap = True)\n\nsns.heatmap(correlations, mask = mask, cmap = cmap, \n            vmax= .3, center=0, square = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that very little correlation between the attributes themselves is observed. Region has no correlation with the target column. \n\nThe target(charges) column has some positive correlation with age and smoker columns, but neither value goes above 0.30. Let's have a closer look at the two relationships."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (10,5))\nplt.subplot(121)\nsns.boxplot(x = \"smoker\", y = 'charges', data = all_att, palette = 'Set3')\nplt.subplot(122)\nsns.violinplot(x = \"smoker\", y = 'charges', data = all_att, palette = 'Set3')\nplt.suptitle('Charges distribution wrt Smoker status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both plots reflect the same relationship, but the violin plot on the right showcases where the majority of values lie in a more apparent fashion.\n\nWe can see that being a smoker makes a huge difference!\n\nNow let's visualize the relationship between charges and age and incorporate BMI in there too. \n\nFrom the [NHS website](https://www.nhs.uk/common-health-questions/lifestyle/what-is-the-body-mass-index-bmi/), we get a breakdown of how we can interpret BMI. For most adults, a BMI of:\n\n- up to 18.5 suggests you're underweight\n- 18.5 to 24.9 means you're a healthy weight\n- 25 to 29.9 means you're overweight\n- 30 to 39.9 means you're obese\n- 40 or above means you're severely obese\n\nLet's remind ourselves what the distribution looked like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = X_train.bmi.mean()\nmedian = X_train.bmi.median()\nplt.axvline(mean, color='r', linestyle='-')\nplt.axvline(median, color='g', linestyle='--')\nsns.distplot(X_train.bmi)\nplt.legend({'Mean':mean,'Median':median})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The BMI isn't the best metric, but let's create new categories based on this classification from the NHS."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bmi_split(df):\n    df['underweight'] = df['bmi'] < 18.5\n    df['healthyweight'] = (df['bmi'] >= 18.5) & (df['bmi'] < 25)\n    df['overweight'] = (df['bmi'] >= 25) & (df['bmi'] < 30)\n    df['obese'] = (df['bmi'] >= 30) & (df['bmi'] < 40)\n    df['severelyobese'] = (df['bmi'] >= 40)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = bmi_split(X_train)\nX_test = bmi_split(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add new columns to all_att df\nall_att = X_train.join(y_train)\n\nf, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (15,5))\nplt.subplot(231)\nsns.scatterplot(x = 'age', y = 'charges', data = all_att, legend = None)\nplt.title('All values')\nbmi_types = ['underweight', 'healthyweight', 'overweight', 'obese','severelyobese']\nfor i,htype in enumerate(bmi_types):\n    plt.subplot(2,3,i+2)\n    sns.scatterplot(x = 'age', y = 'charges', data = all_att, \n                hue = htype, size = htype,\n                size_order = [True, False], legend = None)\n    plt.title( '{} highlighted'.format(htype))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The trend is fairly clear. The scatterplot containing all values is separable into 3 'stripes' of distinct price categories, each of which increase in price as patient's age increases. \n\nNow for a breakdown within BMI 'types' we see that people who are considered to have healthy weight almost never appear in the highest price stripe. Highest charges all fell into the 'obese' category where we find a big portion of our data points. \n\nMy suspicion is that the stripes are created by the varying complexity of treatment / severity of the problem, but this data is not available to us. \n"},{"metadata":{},"cell_type":"markdown","source":"There is also some existing positive correlation between the region and bmi. I found that a little bit surprising, so let's explore it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['Reds', 'Blues', 'Purples', 'Greens']\nline_color = ['b', 'g', 'y', 'r']\nfigure, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (10,10), sharex = True)\nfigure.suptitle('Bi-variate distribution of Charges and BMI across regions', fontsize = 20)\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    median_bmi = all_att[all_att['region'] == i].bmi.median()\n    mean_bmi = all_att[all_att['region'] == i].bmi.mean()\n    median_charge = all_att[all_att['region'] == i].charges.median()\n    mean_charge = all_att[all_att['region'] == i].charges.mean()\n    plt.axvline(median_bmi, color=line_color[i], linestyle='--')\n    plt.axvline(mean_bmi, color=line_color[i], linestyle='-')\n    plt.axhline(median_charge, color=line_color[i], linestyle='--')\n    plt.axhline(mean_charge, color=line_color[i], linestyle='-')\n    sns.kdeplot(all_att[all_att['region'] == i].bmi, all_att[all_att.region == i].charges,\n                cmap = colors[i], shade = True)\n    plt.xlim(10,50)\n    plt.ylim(-5000, 60000)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can observe that there are similar trends across all regions, but the distributions have varying skewness and centrality metrics, with 3 regions having a slight positive skew across BMI distributions (Mean is a straight line, whereas median is dashed). This certainly indicates that region will still have a role to play when it comes to regression!"},{"metadata":{},"cell_type":"markdown","source":"Lastly, let's see how we can visualize the slight correlation between charges and the number of children. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,7))\nplt.subplot(121)\nsns.scatterplot(x = 'age', y = 'charges', data = all_att, hue = 'children', size = 'children', legend = 'full')\nplt.subplot(122)\nsns.stripplot(x = \"children\", y = 'charges', data = all_att, palette = sns.cubehelix_palette())\nplt.suptitle('Highlighting the number of children covered by insurance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think the best we can conclude from this is that we don't have enough observations for insurances covering 4 or more children to say whether these distributions are different from the rest. Maybe we would observe more of a heavy tail in both of them too if we had more datapoints.  "},{"metadata":{},"cell_type":"markdown","source":"# The regression problem\n\nGiven all the attributes we have in this dataset we would like to create a model that predicts the individual medical costs billed by health insurance given the attributes. \n\nThe simplicity of this dataset is an opportunity for a deep dive into regression models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start off by log transforming the target variable so that error will be more even across all price predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.log1p(y_train)\ny_test = np.log1p(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale numeric features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX_train.loc[:, ['age', 'bmi']] = sc.fit_transform(X_train.loc[:, ['age', 'bmi']])\nX_test.loc[:, ['age', 'bmi']] = sc.fit_transform(X_test.loc[:, ['age', 'bmi']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use RMSE as our error measure. "},{"metadata":{"trusted":true},"cell_type":"code","source":"scorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression, no regularizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)\n\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression, Ridge:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = RidgeCV(alphas = [0.001, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\n# Selecting more values, concentrated around the best value of alpha \nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4, alpha * 1.5, alpha * 1.55], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very litte improvement! Let's do the same for L1 regularization and compare. "},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression, Lasso:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\n# Selecting more values, concentrated around the best value of alpha \nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4, alpha * 1.5, alpha * 1.55], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding regularization provided very little progress. \n\nThis time let's try Decision Tree Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ncurr_err = np.inf\nbest_depth = 0\nfor i in range(10):\n    dtr = DecisionTreeRegressor(max_depth=i+1)\n    dtr.fit(X_train, y_train)\n    if rmse_cv_train(dtr).mean() < curr_err:\n        curr_err = rmse_cv_train(dtr).mean()\n        best_depth = i \nprint('The best maximum depth is :', best_depth)\ndtr = DecisionTreeRegressor(max_depth=best_depth)\ndtr.fit(X_train, y_train)\nprint(\"Decision Tree Regressor RMSE on Test set :\", rmse_cv_test(dtr).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ToDo: Regression Model Stacking & Conclusion"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}