{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rain in Australia","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### This kernel predicts whether it will rain tomorrow.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### I'm a newbie in ML,if you like it, please upvote :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\n#Showing full path of datasets\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n#Disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Head","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shape","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of rows and columns in our dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#The 24 columns \ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop Risk_MM column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#As mentioned in the dataset description , \n#we should exclude the variable Risk-MM when training a binary classification model.\n#Not excluding it will leak the answers to your model and reduce its predictability.\n\ndf.drop(['RISK_MM'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic Information of dataset\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Before looking at the description of the data\n#We can see that there are few columns with very less data\n#Evaporation,Sunshine,Cloud9am,Cloud3pm\n#It is better to remove these four columns as it will affect our prediction even if we\n#fill the na values...\n\n#Date and Location is also not required\n#As we are predicting rain in australia and not when and where in australia\n\n\ndrop_cols = ['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date','Location']\n\ndf.drop(columns=drop_cols,inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data after dropping columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic description of our data\n#Numerical features first\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The count is different for all the features\n* We can see difference in mean and max is huge in many features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Including Categorical features with include object\ndf.describe(include='object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here too there are null values in our categorical data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now including all the features\ndf.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Null Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Our dataset consists of 142193 rows and the count for many features is less than 142193.\n#This shows presence of Null values.\n#Let's look at the null values..\n\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Except Date,Location and our target feature,\n* All the other features have null values\n* We'll deal with these later in the notebook..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Skewness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The features with skewness values near zero may follow gaussian distribution.\n* Rainfall is strongly right skewed (9.88)\n* We'll have a look at the distributions for further clarity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling missing values\n\n#We can see that there are outliers in our data\n#So the best way to fill the na values in our numerical features is with median\n#Because median deals the best with outliers\n\n#Let's separate numerical and categorical\n#data type of numerical features is equal to float64\n#With the help of following list comprehension we separate the numerical features...\n\nnum = [col for col in df.columns if df[col].dtype==\"float64\"]\n\nfor col in num:\n    df[col].fillna(df[col].median(),inplace=True)\n    \ncat = [col for col in df.columns if df[col].dtype==\"O\"]\nfor col in cat:\n    df[col].fillna(df[col].mode()[0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check missing values\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are no missing values present now and we can start our analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr().style.background_gradient(cmap=\"Reds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heatmap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#With the use of heatmap\ncorr = df.corr()\n\nfig = plt.figure(figsize=(12,12))\nsns.heatmap(corr,annot=True,fmt=\".1f\",linewidths=\"0.1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlated features**\n\n* MinTemp -- MaxTemp (0.7)\n* MinTemp -- Temp3pm (0.7)\n* MaxTemp -- Temp9am (0.9)\n* WindGustSpeed -- WindSpeed9am (0.6)\n* WindGustSpeed -- WindSpeed3pm (0.7)\n* Humidity9am -- Humidity3pm (0.7)\n* Humidity3pm -- Temp3pm (-0.6)\n* Temp9am -- MinTemp (0.9)\n* Temp9am -- Temp3pm","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Features that are less correlated with other features**\n\n* Rainfall\n* Pressure9am\n* Pressure3pm","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Later we'll have a look at the scatterplots of these correlated features.....","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Study the numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Numerical features :: {}\\n\".format(num))\nprint(\"No of Numerical features :: {}\".format(len(num)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distributions of each numerical feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\ncolors = ['Red','Blue','Green','Cyan',\n         'Red','Blue','Green','Cyan',\n         'Red','Blue','Green','Cyan']\nj=0\nfor col in num:\n    plt.subplot(3,4,i)\n    a1 = sns.distplot(df[col],color=colors[j])\n    i+=1\n    j+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Rainfall and Evaporation as seen with skewness value (9.88) and (3.74) are right skewed as seen above.\n* Cloud9am and Cloud3pm behave as categorical features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Boxplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\nfor col in num:\n    plt.subplot(6,2,i)\n    a1 = sns.boxplot(data=df,x=\"RainTomorrow\",y=col)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**With the help of skewness values and above box plots,Rainfall,WindGustSpeed,WindSpeed9am and WindSpeed3pm**\n**may contain outliers**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Find the outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a loop that finds the outliers in train and test  and removes it\nfeatures_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']\n\nfor col in features_to_examine:\n    IQR = df[col].quantile(0.75) - df[col].quantile(0.25) \n    Lower_Bound = df[col].quantile(0.25) - (IQR*3)\n    Upper_Bound = df[col].quantile(0.75) + (IQR*3)\n    \n    print(\"The outliers in {} feature are values <<< {} and >>> {}\".format(col,Lower_Bound,Upper_Bound))\n    \n    minimum = df[col].min()\n    maximum = df[col].max()\n    print(\"The minimum value in {} is {} and maximum value is {}\".format(col,minimum,maximum))\n    \n    if maximum>Upper_Bound:\n          print(\"The outliers for {} are value greater than {}\\n\".format(col,Upper_Bound))\n    elif minimum<Lower_Bound:\n          print(\"The outliers for {} are value smaller than {}\\n\".format(col,Lower_Bound))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Barplots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\nfor col in num:\n    plt.subplot(6,2,i)\n    a1 = sns.barplot(data=df,x=\"RainTomorrow\",y=col)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's have a look at the correlated variables from the above histogram**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Scatterplots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplots_adjust(hspace=0.5)\n\ni=1\nfeatures_list = [\"MaxTemp\",\"Temp9am\",\"Temp3pm\"]\nfor feature in features_list:\n    plt.subplot(1,3,i)\n    sns.scatterplot(data=df,x=\"MinTemp\",y=feature,hue=\"RainTomorrow\")\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.subplots_adjust(hspace=0.5)\n\nplt.subplot(3,2,1)\nsns.scatterplot(data=df,x=\"WindSpeed9am\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,2)\nsns.scatterplot(data=df,x=\"WindSpeed3pm\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,3)\nsns.scatterplot(data=df,x=\"Humidity9am\",y=\"Humidity3pm\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,4)\nsns.scatterplot(data=df,x=\"Temp9am\",y=\"Temp3pm\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,5)\nsns.scatterplot(data=df,x=\"MaxTemp\",y=\"Temp9am\",hue=\"RainTomorrow\")\n\nplt.subplot(3,2,6)\nsns.scatterplot(data=df,x=\"Humidity3pm\",y=\"Temp3pm\",hue=\"RainTomorrow\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorical Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WindGustDir","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['WindGustDir'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nsns.countplot(data=df,x=\"WindGustDir\",hue=\"RainTomorrow\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WindDir9am","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['WindDir9am'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nsns.countplot(data=df,x=\"WindDir9am\",hue=\"RainTomorrow\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WindDir3pm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['WindDir3pm'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nsns.countplot(data=df,x=\"WindDir3pm\",hue=\"RainTomorrow\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['RainTomorrow'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df,x=\"RainTomorrow\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the data into **training** and **testing** sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\ny=df[['RainTomorrow']]\nX=df.drop(['RainTomorrow'],axis=1)\n\nX_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### First we will remove any outliers present in our data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We have found the outliers in Rainfall,WindGustSpeed,WindSpeed9am and WindSpeed3pm.\n**We Will cap these outliers now**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n**Let's have a look at their histograms.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We'll plot these four as subplots \n\nplt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\nfeatures_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']\ni=1\nfor col in features_to_examine:\n    plt.subplot(6,2,i)\n    fig = df[col].hist(bins=10)\n    fig.set_xlabel(col)\n    fig.set_ylabel('RainTomorrow')\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can clearly see right skewed histograms in all the four**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***We'll try to cap these outliers that will help us in predictions later.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_outliers(df,col,Lower_Bound,Upper_Bound):    \n    minimum = df[col].min()\n    maximum = df[col].max()\n    \n    if maximum>Upper_Bound:\n        return np.where(df[col]>Upper_Bound,Upper_Bound,df[col])\n          \n    elif minimum<Lower_Bound:\n        return np.where(df[col]<Lower_Bound,Lower_Bound,df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df1 in [X_train,X_test]:\n    df1['Rainfall'] = remove_outliers(df1,'Rainfall',-1.799,2.4)\n    df1['WindGustSpeed'] = remove_outliers(df1,'WindGustSpeed',-14.0,91.0)\n    df1['WindSpeed9am'] = remove_outliers(df1,'WindSpeed9am',-29.0,55.0)\n    df1['WindSpeed3pm'] = remove_outliers(df1,'WindSpeed3pm',-20.0,57.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If we look at their boxplots we can see that the outliers are now capped...\nplt.figure(figsize=(15,30))\nplt.subplots_adjust(hspace=0.5)\n\nfeatures_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']\ni=1\nfor col in features_to_examine:\n    plt.subplot(6,2,i)\n    fig = sns.boxplot(data=X_train,y=col)\n    fig.set_xlabel(col)\n    fig.set_ylabel('RainTomorrow')\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Describe helps us understand more about the mean and max values\n\nX_train[features_to_examine].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[features_to_examine].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encode categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Our next step is to encode all the categorical variables.\n#first we will convert our target variable\n\nfor df2 in [y_train,y_test]:\n    df2['RainTomorrow'] = df2['RainTomorrow'].replace({\"Yes\":1,\n                                                    \"No\":0})\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode **RainToday** variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\nencoder = ce.BinaryEncoder(cols=['RainToday'])\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will make our training dataset\n\nX_train = pd.concat([X_train[num],X_train[['RainToday_0','RainToday_1']],\n                    pd.get_dummies(X_train['WindGustDir']),\n                    pd.get_dummies(X_train['WindDir9am']),\n                    pd.get_dummies(X_train['WindDir3pm'])],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Same for testing set\n\nX_test = pd.concat([X_test[num],X_test[['RainToday_0','RainToday_1']],\n                    pd.get_dummies(X_test['WindGustDir']),\n                    pd.get_dummies(X_test['WindDir9am']),\n                    pd.get_dummies(X_test['WindDir3pm'])],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#our training and testing set is ready for our model\n#But ,before that we need to bring all the features to same scale with feature scaling\n#For this we will use MinMaxScaler\n#As there our negative values in our dataset and MinMaxScaler scales our data in range -1 to 1.\n\ncols = X_train.columns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(X_train,columns=cols)\nX_test = pd.DataFrame(X_test,columns=cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finally ,after removing outliers,encoding the categorical variables and scaling**\n\n**Our training and testing sets are ready**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# instantiate the model\nlogreg = LogisticRegression(solver='liblinear', random_state=0)\n\n\n# fit the model\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction on Xtest\n\ny_pred_test = logreg.predict(X_test)\n\ny_pred_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using predict_proba gives the probability value for the target feature\n\nlogreg.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#probability of getting no rain (0)\n\nlogreg.predict_proba(X_test)[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#probability of getting rain (1)\n\nlogreg.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check accuracy with accuracy_score\n\nfrom sklearn.metrics import accuracy_score\n\npredict_test = accuracy_score(y_test,y_pred_test)\n\nprint(\"Accuracy of model on test set :: {}\".format(predict_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test, y_pred_test)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The result is telling us that we have 31308+4554 correct predictions and 5068+1728 incorrect predictions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**35,862 correct predictions.**\n\n**6796 Incorrect predictions.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparing train and test accuracy\n\ny_pred_train = logreg.predict(X_train)\ny_pred_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check accuracy of our model with train set\n\npredict_train = accuracy_score(y_train,y_pred_train)\nprint(\"Accuracy of our model on train set :: {}\".format(predict_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see somewhat same score for both out training and testing datasets using this model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Overall Accuracy\n\nprint(\"Accuracy of our model :: {}\".format(logreg.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 84% accuracy is good but we can still improve it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try to improve the accuracy of our model\n\n#Let's try different C values\n\n#Now what is C","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### C is inverse of regularization strength.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Higher values of C correspond to less regularization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### By default , C is equal to 1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Now let's reduce the regularization strength","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#C=100\n\n# instantiate the model\nlogreg100 = LogisticRegression(solver='liblinear',C=100, random_state=0)\n\n\n# fit the model\nlogreg100.fit(X_train, y_train)\n\n#Prediction on Xtest\n\ny_pred_test = logreg100.predict(X_test)\n\ny_pred_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test = accuracy_score(y_test,y_pred_test)\n\nprint(\"Accuracy of model on test set :: {}\".format(predict_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Overall Accuracy\n\nprint(\"Accuracy of our model :: {}\".format(logreg100.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test, y_pred_test)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**35,886 Correct predictions**\n\n**6,792 Incorrect predictions**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We can see increase in correct predictions and decrease in incorrect predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report\nprint(classification_report(y_test, y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see a slight increase in our model with C=100","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's increase the regularization strength\n\n#C=0.01\n\n# instantiate the model\nlogreg001 = LogisticRegression(solver='liblinear',C=0.01, random_state=0)\n\n\n# fit the model\nlogreg001.fit(X_train, y_train)\n\n#Prediction on Xtest\n\ny_pred_test = logreg001.predict(X_test)\n\ny_pred_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test = accuracy_score(y_test,y_pred_test)\n\nprint(\"Accuracy of model on test set :: {}\".format(predict_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Overall Accuracy\n\nprint(\"Accuracy of our model :: {}\".format(logreg001.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see a decrease in our model accuracy with C=0.01","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Our aim is to predict whether it will rain or not tomorrow in australia.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Let's see the probability of raining with the help of histogram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# store the predicted probabilities for class 1 - Probability of rain\n\ny_pred1 = logreg100.predict_proba(X_test)[:, 1]\ny_pred0 = logreg100.predict_proba(X_test)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot histogram of predicted probabilities\n\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n\n# plot histogram with 10 bins\nplt.hist(y_pred1, bins = 10)\nplt.hist(y_pred0, bins = 10)\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities')\n\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n#Set legend\nplt.legend('upper left' , labels = ['Rain','No Rain'])\n\n# set the title\nplt.xlabel('Predicted probabilities')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above histogram is highly right skewed for rain\n* Highly left skewed for no rain \n* There is less chance that it will rain tomorrow as most of the predicted probabilities are near to zero.\n* Higher chance as probabilities are close to 1.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Conclusion :-\n    \n    * Our model predicts that there's a higher chance of not raining tomorrow in australia as seen in the above histogram.\n    \n    * Accuracy of our model is 84%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### PLEASE GIVE A UPVOTE IF YOU LIKE THIS KERNEL :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}