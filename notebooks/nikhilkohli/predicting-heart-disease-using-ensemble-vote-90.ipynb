{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Problem Definition\n\nGiven clinical parameters about a person, can we predict whether or not they have heart disease?\n\n### I have also created a Inference Pipeline using Luigi and a Streamlit web app for real time predictions. \n\nYou can try it at the below links - \n\n\nhttps://heart-disease-diagnostics.herokuapp.com/\n\nhttps://github.com/Nikhilkohli1/Heart-Disease-Diagnosis-Assistant\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### What I will be doing in this notebook - \n\n1. EDA (Exploratory Data Analysis)\n2. Data Pre-processing \n3. Predictive Modeling - I will train 4 different algorithms on 4 different feature sets after doing extensive feature selection. \n4. Model Selection \n5. Ensemble Max Vote - from the best models, I will create a simple Ensemble Max Voting approach to make predictions using top 3 best models\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n%matplotlib inline\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the UCI heart Disease Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndf_heart.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Descriptive Statistics & Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart = df_heart.rename(columns= {'cp':'chest_pain_type','trestbps':'resting_BP','chol':'serum_cholestoral','fbs':'fasting_blood_sugar','restecg':'resting_ECG',\n                                     'thalach':'max_heart_rate','exang':'exercise_induced_angina',\n                                     'ca':'major_vessels_count','thal':'thalium_stress'})\ndf_heart.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Null values,lucky!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Categorical Discrete & Continous Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = []\ncontinous_cols = []\n\nfor column in df_heart.columns:\n    if(len(df_heart[column].unique()) <= 10):\n        categorical_cols.append(column)\n    else:\n        continous_cols.append(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continous_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart_tmp = df_heart.copy()\nfor cols in categorical_cols:\n    if(cols != 'target'):\n        df_heart_tmp[cols] = df_heart_tmp[cols].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart_tmp.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart_tmp.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Redundancy(Constant & Quasi Constant variables)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart_tmp.describe(include='object')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are no variables with only 1 unique value or Quasi constant(>99% values are constant), so we are good with Redundancy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Target Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.target.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_heart['target'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is not highly imbalanced, but we can try to balance it using SMOTE Oversampling if we do not get a good accuracy with this.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Continous Features\n\nLet's check the distribution for continous features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.hist(column=continous_cols, figsize=(12,12))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, column in enumerate(continous_cols):\n    plt.figure(index)\n    sns.distplot(df_heart[column])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Old Peak seems to be highly skewed, lets see the Skew for each feature. The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart[continous_cols].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As evident from the Skew values above for continous features, oldpeak & serum cholestoral are right skewed. We can apply Log Transformation to these variables while Preprocessing data for Machine Learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Distribution of Categorical Values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.hist(column=categorical_cols, figsize=(10,10))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Associations & Correlation between variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart_tmp.describe().columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df_heart_tmp[df_heart_tmp.describe().columns], hue='target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.corr()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.heatmap(df_heart.corr(), annot=True, linewidths=1, linecolor='white', fmt=\".2f\",\n                 cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.drop('target', axis=1).corrwith(df_heart.target).plot(kind='bar', grid=True, figsize=(10, 7), color='darkgreen')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most features have a significant correlation with the Target variable except Fasting Blood Sugar, Resting ECG and Serum Cholestoral. Chest pain type and Max heart rate has a high positive correlation with the target","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Analysing Relationship between continous variables & Target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,column in enumerate(continous_cols):\n    plt.figure(index, figsize=(8,5))\n    sns.boxplot(x=df_heart.target, y=column, data=df_heart, palette='rainbow',linewidth=1)\n    plt.title('Relation of {} with target'.format(column), fontsize = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some unusual or rare values like in Cholestoral(400-500) and resting BP(200) but these are possible values and not Data collection errors. So we should not remove any of these even when they look like outliers.\n\n### Swarmplots\n\nLet me also look at the relationship between few variables and target with Swarm plots.\n\nThis approach adjusts the points along the categorical axis using an algorithm that prevents them from overlapping. It can give a better representation of the distribution of observations, although it only works well for relatively small datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,column in enumerate(continous_cols):\n    plt.figure(index,figsize=(7,5))\n    sns.catplot(x='target', y=column, hue='sex', kind='swarm', data=df_heart, palette='husl')\n    plt.title('Relationship of {} with target for each sex'.format(column), fontsize = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations -\n\n***Age***\n- On an Average people above the age of 50 are at risk of having a heart disease when combined with other factors. Age alone is not a good predictor of heart disease as evident from the box plot and Swarm plot.\n\n***Resting Blood pressure***\n- Anything above 130-140 (in mm Hg) is a cause for concern.\n\n***Serum Cholestoral***\n- Cholestoral (LDL + HDL + Triglysrides) above 300 is definitely a concern, below that is a concern when combined with other factors.\n\n***Thalach(Maximum Heart ate)***\n- There is a Strong correlation between the Heart Disease and max heart rate. People with Max heart rate above 150-160 are more likely to suffer from a Heart Disease.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Analysing Relationship between Categorical variables & Target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols.pop(8)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Labels in each Categorical feature : Cardinality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical_cols:\n    print('Cardinality of {1} is {0}'.format(len(df_heart[var].unique()), var))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,column in enumerate(categorical_cols):\n    plt.figure(index,figsize=(7,5))\n    sns.countplot(x=column, hue='target', data=df_heart, palette='rainbow')\n    plt.title('Relation of {} with target'.format(column), fontsize = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation for Categorical variables -\n\n- Sex - Females are more likely to have a Heart Disease than Males.\n\n- Chest Pain type - People with Chest Pain type 1,2,3 have more chance of having a Heart Disease.\n\n- Resting ECG - People with value 1 for resting ECG(abnormal Heart beat) are more likely to have a heart disease.\n\n- Exercise Induced Angina - Poeple with No Exercise Induced Angina(0) have heart diseases more than others who have Angina due to exercise. This seems a little contradictory between.\n\n- Slope People with Slope value equal to 2 are more likely to have a Heart Disease than people with Slope value 0 or 1\n\n- Major vessel Count - This has a negative relation with Heart disease. The lesser Number of Major vessels, the more chances are of Heart Disease.\n\n- Thalium Stress ST Depression - Poeple with value 2 or 3 are more likely to have Heart Disease","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing & Feature Engineering\nWe need to do three things as part of Data Preprocessing before we can build Machine Learning models for Classification -\n\n- 1. One Hot Encoding - Creation of dummy variables for Categorical Variables with more than 2 classes\n\n- 2. Feature Scaling - We will be using distance based algorithms as well like KNN, so scaling is required\n\nDummy Variable -\nAs Sex, Fasting Blood Sugar & Exercise induced Angina contain only 2 unique values (0, 1), we do not need to create dummy variable for them. So there are 5 variables which need to be encoded.\n\nI will also drop the first column of each as after encoding, it can cause dummy variable trap.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_dummy = [\n 'chest_pain_type',\n 'resting_ECG',\n 'slope',\n 'major_vessels_count',\n 'thalium_stress']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart = pd.get_dummies(df_heart, columns=categorical_dummy, drop_first=True )\ndf_heart.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_heart.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 23 features to play with now. Lets continue with the Preprocessing Steps","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df_heart.target\nfeatures = df_heart.drop(columns=['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling\nI have tried both Robust Scaling as well as Min Max Scaling, MinMax scalign works better for this problem, so i am using this.\nRobust Scaler is robust to outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\nfeatures_SS = scaler.fit_transform(features)\nfeatures_SS = pd.DataFrame(features_SS, columns=features.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Splitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(features_SS, target, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multicollinearity using VIF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each feature, calculate the VIF score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['VIF Factor'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['features'] = X_train.columns\nvif.round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only thalium Stress features have VIF factor greater. We can remove one and calculate the VIF again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_train.drop(columns=['thalium_stress_3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each feature, calculate the VIF score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['features'] = X.columns\nvif.round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have less VIF scores for all features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection\nI will using 3 techniques to select Features -\n\n- Recursive feature Elimination with CV using Random Forest as estimator\n- Recursive feature Elimination with CV using Logistic as estimator\n- SelectFromModel using XGBoost\n\n### Recursive Feature Elimination with Cross-validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV, SelectFromModel\nfrom xgboost import XGBClassifier\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nrf = RandomForestClassifier(n_estimators=10, random_state=40)\nrfe_rf = RFECV(estimator=rf, step=1, cv=5, n_jobs=-1)\nrfe_rf.fit_transform(X_train, Y_train)\n\nend = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Time Taken - {}'.format(str(end - start)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_rf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_rf.support_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_rf_ranks = rfe_rf.ranking_\nrfe_rf_ranks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'axes.labelsize': 280,'axes.titlesize':40, 'legend.fontsize': 18, 'xtick.labelsize': 40, 'ytick.labelsize': 50}\nplt.figure(figsize=(50,25))\nplt.rcParams.update(params)\nax = plt.bar(range(X_train.shape[1]), rfe_rf_ranks, color='green', align = 'center')\nax = plt.title('Feature importance')\nax = plt.xticks(range(X_train.shape[1]), X_train.columns, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_idx = rfe_rf.support_\nfeature_names = X_train.columns[feature_idx]\nfeature_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using RFECV gives us 19 features which are significant for prediction of Heart Diseases. The 3 features which are not so important as per this are resting_ECG_2, Major Vessel Count 3 & 4, which makes sense, as upto 2-3 vessel count is good, above 3 it does not really matter as you are less likely to have a heart disease in that case. Resting ECG as we saw, had a very low correlation with the target as we analysed through the heatmap.\n\nI will also use SelectFromModel for selecting another set of features, and then see which one works best for this data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nlogit = LogisticRegression()\nrfe_logit = RFECV(estimator=logit, step=1, cv=5, n_jobs=-1)\nrfe_logit.fit_transform(X_train, Y_train)\n\nend = time.time()\n\nprint('Time Taken - {}'.format(str(end - start)))\nrfe_logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_logit.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_logit_ranks = rfe_logit.ranking_\nrfe_logit_ranks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'axes.labelsize': 280,'axes.titlesize':40, 'legend.fontsize': 18, 'xtick.labelsize': 40, 'ytick.labelsize': 50}\nplt.figure(figsize=(50,25))\nplt.rcParams.update(params)\nax = plt.bar(range(X_train.shape[1]), rfe_logit_ranks, color='green', align = 'center')\nax = plt.title('Feature importance')\nax = plt.xticks(range(X_train.shape[1]), X_train.columns, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_idx2 = rfe_logit.support_\nfeature_names2 = X_train.columns[feature_idx2]\nfeature_names2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SelectFromModel - This is a meta transformer for selecting features based on importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\nselect_xg = SelectFromModel(estimator=xgb, threshold='median')\nselect_xg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_xg.fit_transform(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_idx3 = select_xg.get_support()\nfeature_names3 = X_train.columns[feature_idx3]\nfeature_names3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Selected features\n\nWe have 4 set of features now\n\n- Default features without any feature selection\n- Features selected from RFECV(Random Forest)\n- Features selected from RFECV(Logistic)\n- Features selected from meta transformer\n\nLets start with Modeling now, I will train algorithms on all of these feature sets and see which are the best ones.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Predictive Modeling & Hyperparameter Tuning\nI will train below Machine Learning algorithms to build models for classifying Heart Disease (binary classification) using the above 3 set of selected features.\n\n- Logistic Regression\n- Support Vector Machine\n- K-Nearest Neighbours\n- Random Forest Classifier\n- XGBoost Classifier\n\nI will use Grid Search and CV to find the best Hyperparameters for each algorithm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_2 = X_train[feature_names]\nX_train_3 = X_train[feature_names2]\nX_train_4 = X_train[feature_names3]\n\nX_test_2 = X_test[feature_names]\nX_test_3 = X_test[feature_names2]\nX_test_4 = X_test[feature_names3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train.columns), len(X_train_2.columns), len(X_train_3.columns), len(X_train_4.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline Classifiers\n\nLet's quickly run some baseline Classification without any Tuning and using all the extracted features. After this I will use Grid Search and Cross-Validation to tune the Hyperparameters for all 5 algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_clf = LogisticRegression()\nlogit_clf.fit(X_train, Y_train)\n\ny_pred = logit_clf.predict(X_test)\nprint('Accuracy Score: ', str(accuracy_score(Y_test, y_pred)))\nprint('Classification Report: ')\nprint(classification_report(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, Y_train)\n\ny_pred = knn_clf.predict(X_test)\nprint('Accuracy Score: ', str(accuracy_score(Y_test, y_pred)))\nprint('Classification Report: ')\nprint(classification_report(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf = SVC(kernel='rbf', gamma=0.1, C=1.0)\nsvm_clf.fit(X_train, Y_train)\n\ny_pred = svm_clf.predict(X_test)\nprint('Accuracy Score: ', str(accuracy_score(Y_test, y_pred)))\nprint('Classification Report: ')\nprint(classification_report(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, Y_train)\n\ny_pred = dt_clf.predict(X_test)\nprint('Accuracy Score: ', str(accuracy_score(Y_test, y_pred)))\nprint('Classification Report: ')\nprint(classification_report(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search & Hyperparameter Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_model(X_train, Y_train, X_test, Y_test, classifier_name, classifier, gridSearchParam, cv, save_model=False):\n    #setting the seed for reproducability\n    #np.random.seed(100)\n    print('Training {} algorithm.........'.format(classifier_name))\n    grid_clf = GridSearchCV(estimator=classifier,\n                            param_grid=gridSearchParam, \n                            cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)\n    grid_res = grid_clf.fit(X_train, Y_train)\n    best_params = grid_res.best_params_\n    Y_pred = grid_res.predict(X_test)\n    cm = confusion_matrix(Y_test, Y_pred)\n    \n    \n    print(Y_pred)\n    print(\"=====================================================================\")\n    print('Training Accuracy Score: ' + str(accuracy_score(Y_train, grid_res.predict(X_train))))\n    print(\"---------------------------------------------------------------------\")\n    print('Test Accuracy Score: ' + str(accuracy_score(Y_test, Y_pred)))\n    print(\"---------------------------------------------------------------------\")\n    print('Best HyperParameters: ', best_params)\n    print(\"---------------------------------------------------------------------\")\n    print('Classification Report: ')\n    print(classification_report(Y_test, Y_pred))\n    print(\"---------------------------------------------------------------------\")\n    \n    #fig, ax = plt.subplots(figsize=(7,7))\n    ax= plt.subplot()\n    #plt.figure(figsize=(6,6))\n    sns.set(font_scale=1.0) # Adjust to fit\n    label_font = {'size':'5'}\n    plt.rcParams.update({'font.size': 14})\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g', cmap='Blues')\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels') \n    ax.set_title('Confusion Matrix') \n    ax.xaxis.set_ticklabels(['No Heart Disease', 'Heart Disease'])\n    ax.yaxis.set_ticklabels(['No Heart Disease', 'Heart Disease'])\n    print(\"=====================================================================\")\n    \n    if save_model:\n        file_name = classifier_name + '.pkl'\n        pickle.dump(grid_res, open(file_name, 'wb'))\n        #joblib.dump(grid_res, file_name)\n        print('Model is saved successfully!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 5 \nhyper_params = {'C': [0.0001, 0.001, 0.1, 1, 10, 20],   #np.logspace(0, 4, 10),\n               'penalty': ['l1','l2'],\n               'solver': ['liblinear', 'saga']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 1 \nfit_model(X_train, Y_train, X_test, Y_test, 'Logistic Regression', LogisticRegression(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 2 \nfit_model(X_train_2, Y_train, X_test_2, Y_test, 'Logistic Regression', LogisticRegression(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 3 \nfit_model(X_train_3, Y_train, X_test_3, Y_test, 'Logistic Regression', LogisticRegression(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 4 \nfit_model(X_train_4, Y_train, X_test_4, Y_test, 'Logistic Regression', LogisticRegression(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 5 \nhyper_params = {'C': [0.01, 0.1, 1, 10, 100, 1000],\n                'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.1, 1, 3],\n                'kernel': ['linear', 'rbf']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 1 \nfit_model(X_train, Y_train, X_test, Y_test, 'SVM Classifier', SVC(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 2\nfit_model(X_train_2, Y_train, X_test_2, Y_test, 'SVM Classifier', SVC(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 3\nfit_model(X_train_3, Y_train, X_test_3, Y_test, 'SVM Classifier', SVC(), hyper_params, cv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 4\nfit_model(X_train_4, Y_train, X_test_4, Y_test, 'SVM Classifier', SVC(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-Nearest Neighbours","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 5 \nhyper_params = {'n_neighbors': list(range(1,20)),\n                'leaf_size': list(range(1,15)),\n                'p': [1,2]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 1 \nfit_model(X_train, Y_train, X_test, Y_test, 'KNN Classifier', KNeighborsClassifier(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 2\nfit_model(X_train_2, Y_train, X_test_2, Y_test, 'KNN Classifier', KNeighborsClassifier(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 3 \nfit_model(X_train_3, Y_train, X_test_3, Y_test, 'KNN Classifier', KNeighborsClassifier(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 4\nfit_model(X_train_4, Y_train, X_test_4, Y_test, 'KNN Classifier', KNeighborsClassifier(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 5 \nhyper_params = {'n_estimators': [10, 50, 100, 200, 500],\n                'max_depth': [2, 4, 6, 10, 15, 20, 30],\n                'min_samples_split': [2, 5, 10, 20],\n                'min_samples_leaf': [1, 2, 5, 10]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 1 \nfit_model(X_train, Y_train, X_test, Y_test, 'Random Forest', RandomForestClassifier(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 2\nfit_model(X_train_2, Y_train, X_test_2, Y_test, 'Random Forest', RandomForestClassifier(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Set 3 \nfit_model(X_train_3, Y_train, X_test_3, Y_test, 'Random Forest', RandomForestClassifier(), hyper_params, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Model & features\nLogistic Regression with feature set 4 , SVM with feature set 4 & KNN with feature set 1 are the best models with 90.16% Test accuracy and 90% f1 score\n\nBased on the analysis and looking at all aspects of Training accuracy, Testing Accuracy, Precision & Recall, these 3 are our best estimator\n\n### Best features -\n\n['sex', 'max_heart_rate', 'exercise_induced_angina', 'oldpeak', 'chest_pain_type_2',\n       'chest_pain_type_3', 'slope_1', 'major_vessels_count_1',\n       'major_vessels_count_2',\n       'thalium_stress_2']","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Save Best Model for Inference Pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 5 \nhyper_params = {'C': [0.01, 0.1, 1, 10, 100, 1000],\n                'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.1, 1, 3],\n                'kernel': ['linear', 'rbf']}\n\n\n#Saving SVM Best Model using feature Set 4\nfit_model(X_train_4, Y_train, X_test_4, Y_test, 'SVMClassifier', SVC(), hyper_params, cv, save_model=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 5 \nhyper_params = {'n_neighbors': list(range(1,20)),\n                'leaf_size': list(range(1,15)),\n                'p': [1,2]}\n\n#Saving KNN Best Model using feature Set 1\nfit_model(X_train, Y_train, X_test, Y_test, 'KNN Classifier', KNeighborsClassifier(), hyper_params, cv, save_model=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 5 \nhyper_params = {'C': [0.0001, 0.001, 0.1, 1, 10, 20],   #np.logspace(0, 4, 10),\n               'penalty': ['l1','l2'],\n               'solver': ['liblinear', 'saga']}\n\n#Saving Logistic Regression Best Model using feature Set 4\nfit_model(X_train_4, Y_train, X_test_4, Y_test, 'Logistic Regression', LogisticRegression(), hyper_params, cv, save_model=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble Technique for Prediction of Heart Disease\nWe have done extensive feature selection and ran Machine learning models on 4 different set of features. After lot of Hyperparameter tuning and cross validation, I got 3 good models with f1 score of around 90% and test accuracy of around 90.16%\n\nNow, I will create an Ensemble Max Voting of 3 best models KNN, Logistic and SVM models saved with the best hyperparameters.\n\n***Ensemble learning will make the models more generalized and reduce the bias which a algorithm mihght have learnt***\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = pickle.load(open('./SVMClassifier.pkl', 'rb'))\nlogit = pickle.load(open('./Logistic Regression.pkl', 'rb'))\nknn = pickle.load(open('./KNN Classifier.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_set1 = ['age', 'sex', 'resting_BP', 'serum_cholestoral', 'fasting_blood_sugar',\n       'max_heart_rate', 'exercise_induced_angina', 'oldpeak',\n       'chest_pain_type_2', 'chest_pain_type_3', 'resting_ECG_1', 'slope_1',\n       'slope_2', 'major_vessels_count_1', 'major_vessels_count_2',\n       'thalium_stress_2', 'thalium_stress_3']\n\nfeature_set4 = ['sex', 'exercise_induced_angina', 'oldpeak', 'chest_pain_type_2',\n       'chest_pain_type_3', 'slope_1', 'major_vessels_count_1',\n       'major_vessels_count_2', 'major_vessels_count_3', 'thalium_stress_1',\n       'thalium_stress_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_knn = knn.predict(X_test)\npred_logit = logit.predict(X_test_4)\npred_svm = svm.predict(X_test_4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Max Voting Ensemble learning ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statistics\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ensemble = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ensemble['KNN'] = pred_knn\ndf_ensemble['Logistic'] = pred_logit\ndf_ensemble['SVM'] = pred_svm\ndf_ensemble.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_vote(x):\n    vote = statistics.mode([int(x['KNN']), int(x['Logistic']), int(x['SVM'])])\n    return vote","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ensemble['Ensemble'] = df_ensemble.apply(max_vote, axis=1)\ndf_ensemble.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"---------------------------------------------------------------------\")\nprint('Test Accuracy Score: ' + str(accuracy_score(Y_test, df_ensemble.Ensemble.values)))\nprint(\"---------------------------------------------------------------------\")\nprint('Classification Report: ')\nprint(classification_report(Y_test, df_ensemble.Ensemble.values))\nprint(\"---------------------------------------------------------------------\")\n\ncm = confusion_matrix(Y_test, df_ensemble.Ensemble.values)\n\nax= plt.subplot()\n#plt.figure(figsize=(6,6))\nsns.set(font_scale=1.0) # Adjust to fit\nlabel_font = {'size':'5'}\nplt.rcParams.update({'font.size': 14})\nsns.heatmap(cm, annot=True, ax = ax, fmt='g', cmap='Blues')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('Confusion Matrix') \nax.xaxis.set_ticklabels(['No Heart Disease', 'Heart Disease'])\nax.yaxis.set_ticklabels(['No Heart Disease', 'Heart Disease'])\nprint(\"=====================================================================\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Perfect! ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### I have also created a Inference Pipeline using Luigi and a Streamlit web app for real time predictions. \n\nYou can try it at the below links - \n\n\nhttps://heart-disease-diagnostics.herokuapp.com/\n\nhttps://github.com/Nikhilkohli1/Heart-Disease-Diagnosis-Assistant\n\n\nPlease upvote the kernel if you like it! ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}