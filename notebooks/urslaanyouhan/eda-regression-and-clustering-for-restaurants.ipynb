{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analysis of the Restaurant datasets"},{"metadata":{},"cell_type":"markdown","source":"# Installing required packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install beautifulsoup4\n!pip install geocoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing all required packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport requests\nimport os\nfrom bs4 import BeautifulSoup\nfrom geopy.geocoders import Nominatim\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport descartes\nfrom shapely.geometry import Point, Polygon\nimport geoplot\nimport folium\nimport plotly.express as px\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom sklearn.cluster import DBSCAN\nfrom geopy.distance import great_circle\nfrom shapely.geometry import MultiPoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seeing all files present"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A. Starting with Top 250 Restaurants"},{"metadata":{},"cell_type":"markdown","source":"# Defining function to convert % columns to floats"},{"metadata":{"trusted":true},"cell_type":"code","source":"prefix = '/kaggle/input/restaurant-business-rankings-2020/'\n\ndef p2f(x):\n    return float(x.strip('%'))/100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.1 Loading the Top 250 dataframe"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_top250 = pd.read_csv(prefix + 'Top250.csv', converters={'YOY_Sales':p2f, 'YOY_Units':p2f})\n\ndf_top250","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.2 Checking for NaNs\nThere are plenty of NaNs. The contents column especially has relatively little value for this analysis (more data may be relevant for sentiment analysis, potentially). For now it can be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top250.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top250 = df_top250.drop('Content', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.3 Filling in missing Headquarters\nI believe the HQ location is an important feature for analysis. Where it is centrally located may well have a large bearing on how well it performs. So I do some data scraping from Wikipedia to grab as much location info as possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"url = \"https://en.wikipedia.org/wiki/List_of_restaurant_chains_in_the_United_States\"\n\nwebsite_url = requests.get(url).text\n\nsoup = BeautifulSoup(website_url,'lxml')\n\ntables = soup.find_all('table',class_=\"wikitable\")\n\ndf_list = []\nfor i, _ in enumerate(tables):\n    contents = [item.get_text() for item in tables[i].find_all('td')]\n    name = []\n    hq = []\n    for j, val in enumerate(contents):\n        if j % 7 == 0:\n            name.append(val.strip('\\n'))\n        elif (j - 3) % 7 == 0:\n            hq.append(val.strip('\\n'))\n\n    df = pd.DataFrame(list(zip(name, hq)), \n                   columns =['Restaurant', 'Headquarters_temp']) \n    \n    df_list.append(df)\n    \nname_hq = pd.concat(df_list).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.4 Merging HQ in original df with Wiki data\nSome data was missing in Wiki but present in the original table. So I simply merge the two."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = pd.merge(df_top250, name_hq, on='Restaurant', how='left')\n\ncol = new_df['Headquarters'].fillna(new_df['Headquarters_temp'])\n\nnew_df = new_df.assign(Headquarters=col)\n\ndf_top250 = new_df.drop('Headquarters_temp', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.5 Check NaNs now...\nThere are considerably less. Though still 66 is too many missing, so I will try to find another way to fill this gap."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top250.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.6 Converting locations to a fixed format\nI use geolocator to extract the locations from the city info only..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top250[['City', 'Extra']] = df_top250['Headquarters'].str.split(',', 1, expand=True)\n\ndf_top250.drop(columns = ['Extra', 'Headquarters'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geolocator = Nominatim(user_agent = \"geoapiExercises\")\n\ndef findfullad(city):\n    location = geolocator.geocode(city)\n    if location is None:\n        location = 'Unknown'\n    else:\n        location = location[0]\n    return(location)\n\ndf_top250['Headquarters'] = df_top250['City'].apply(findfullad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.7 Fixing incorrect records\n...however, there is a slight issue of cities (and countries) having names elsewhere in the world... So I fix these manually."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top250['Headquarters'] = df_top250['Headquarters'].str.replace('\\d+', '', regex = True)\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace(' ,', '', regex = True)\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace(' -,', '', regex = True)\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('University of Nottingham, Wollaton Vale, Wollaton, City of Nottingham, Nottinghamshire, East Midlands, England, NG RD, United Kingdom', 'University Park, Miami-Dade County, Florida, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('New York, United States', 'New York City, New York, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('Αθήνα, Δήμος Αθηναίων, Περιφερειακή Ενότητα Κεντρικού Τομέα Αθηνών, Περιφέρεια Αττικής, Αποκεντρωμένη Διοίκηση Αττικής, Ελλάς', 'Athens, Georgia, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('لبنان', 'Lebanon, Tennessee, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('Birmingham, West Midlands Combined Authority, West Midlands, England, United Kingdom', 'Birmingham, Alabama, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('Cheshire, England, United Kingdom', 'Cheshire, Connecticut, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('Maitland City Council, New South Wales, Australia', 'Maitland, Florida, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('Dublin, Dublin Leinster, Éire / Ireland', 'Dublin, Ohio, United States')\ndf_top250['Headquarters'] = df_top250['Headquarters'].str.replace('Toledo, Castilla-La Mancha, España', 'Toledo, Ohio, United States')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.8 Filling in final gaps\nI thought of a way of using google searches recursively on the missing restaurants to find the headquarters by web scraping, but it was not placed commonly anywhere... so I did a manual insert on those records."},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_locs = {\"Dunkin'\": \"Canton, Massachusetts, United States\",\n\"Popeyes Louisiana Kitchen\": \"Miami, Florida, United States\",\n\"Chili's Grill & Bar\": \"Dallas, Texas, United States\",\n\"Papa John's\": \"Louisville, Kentucky, United States\",\n\"Jimmy John's Gourmet Sandwiches\": \"Champaign, Illinois, United States\",\n\"Hardee's\": \"Franklin, Tennessee, United States\",\n\"Five Guys Burgers and Fries\": \"Lorton, Virginia, United States\",\n\"Red Robin Gourmet Burgers and Brews\": \"Greenwood Village, Colorado, United States\",\n\"Carl's Jr.\": \"Franklin, Tennessee, United States\",\n\"Bojangles'\": \"Charlotte, North Carolina, United States\",\n\"BJ's Restaurant & Brewhouse\": \"Huntington Beach, California, United States\",\n\"P.F. Chang's\": \"Scottsdale, Arizona, United States\",\n\"Qdoba Mexican Eats\": \"San Diego, California, United States\",\n\"Bob Evans\": \"New Albany, Ohio, United States\",\n\"Papa Murphy's Pizza\": \"Vancouver, Washington, United States\",\n\"Captain D's Seafood Kitchen\": \"Nashville, Tennessee, United States\",\n\"Perkins Restaurant & Bakery\": \"Memphis, Tennessee, United States\",\n\"Checkers Drive-In Restaurants\": \"Tampa, Florida, United States\",\n\"Jamba\": \"Atlanta, Georgia, United States\",\n\"Portillo's\": \"Oak Brook, Illinois, United States\",\n\"Potbelly sandwich Shop\": \"Chicago, Illinois, United States\",\n\"Bahama Breeze Island Grille\": \"Orlando, Florida, United States\",\n\"Pret A Manger\": \"London, United Kingdom\",\n\"Mastro's Restaurants\": \"Newport Beach, California, United States\",\n\"Uncle Julio's\": \"Irving, Texas, United States\",\n\"Rubio's\": \"Carlsbad, California, United States\",\n\"A&W All-American Food\": \"Lexington, Kentucky, United States\",\n\"Brio Tuscan Grille\": \"Columbus, Ohio, United States\",\n\"Lazy Dog Restaurant & Bar\": \"Huntington Beach, California, United States\",\n\"Souplantation & Sweet Tomatoes\": \"San Diego, California, United States\",\n\"Del Frisco's Double Eagle Steak House\": \"The Post Oak, Houston, Texas, United States\",\n\"Which Wich\": \"Dallas, Texas, United States\",\n\"Firebirds Wood Fired Grill\": \"Charlotte, North Carolina, United States\",\n\"True Food Kitchen\": \"Phoenix, Arizona, United States\",\n\"Mountain Mike's Pizza\": \"Hayward, California, United States\",\n\"Bubba Gump Shrimp Co.\": \"Houston, Texas, United States\",\n\"La Madeleine Country French Cafe\": \"Dallas, Texas, United States\",\n\"Giordano's\": \"Chicago, Illinois, United States\",\n\"Islands Fine Burgers & Drinks\": \"Carlsbad, California, United States\",\n\"Mimi's Bistro & Bakery\": \"Dallas, Texas, United States\",\n\"Beef 'O' Brady's\": \"Tampa, Florida, United States\",\n\"Metro Diner\": \"Tampa, Florida, United States\",\n\"Smokey Bones Bar & Fire Grill\": \"Tampa, Florida, United States\",\n\"LaRosa's Pizzeria\": \"Cincinnati, Ohio, United States\",\n\"Roosters\": \"Moore Park, Australia\",\n\"Great Harvest Bread Co.\": \"Dillon, Montana, United States\",\n\"Shari's Cafe and Pies\": \"Beaverton, Oregon, United States\",\n\"Grand Lux Cafe\": \"Calabasas Hills, California, United States\",\n\"Anthony's Coal Fired Pizza\": \"Fort Lauderdale, Florida, United States\",\n\"Chicken Salad Chick\": \"Auburn, Alabama, United States\",\n\"Paris Baguette\": \"Seongnam-si, South Korea\",\n\"Eat'n Park\": \"Homestead, Pennsylvania, United States\",\n\"Taziki's Mediterranean Cafe\": \"Birmingham, Alabama, United States\",\n\"Duffy's Sports Grill\": \"Lake Worth, Florida, United States\",\n\"Topgolf\": \"Dallas, Texas, United States\", \n\"Ocean Prime\": \"Columbus, Ohio, United States\",\n\"Old Country Buffet/HomeTown Buffet\": \"Hollywood Park, Texas, United States\",\n\"Nobu\": \"New York, New York, United States\",\n\"Mission BBQ\": \"Glen Burnie, Maryland, United States\",\n\"Walk-On's Sports Bistreaux\": \"Baton Rouge, Los Angeles, California, United States\",\n\"WaBa Grill\": \"Los Angeles, California, United States\",\n\"54th Street Restaurant & Drafthouse\": \"Kansas City, Missouri, United States\",\n\"Costa Vida Fresh Mexican Grill\": \"Salt Lake City, Utah, United States\",\n\"Gyu-Kaku\": \"New York City, New York, United States\",\n\"PDQ\": \"Tampa, Florida, United States\",\n\"Lupe Tortilla\": \"Houston, Texas, United States\",\n\"Cook-Out Restaurant\": \"Thomasville, North Carolina, United States\",\n\"Jollibee\":\"Quezon City, Luzon\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, value in dict_locs.items():\n    df_top250.loc[df_top250['Restaurant'] == key, 'Headquarters'] = value\n    \ndf_top250.drop(columns = ['City'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top250.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# All clean now! Time to use it..."},{"metadata":{},"cell_type":"markdown","source":"# A.9 Finding lat-long geospatial info from locations\nI find the lat-long and split them into two columns."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_top250","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def findlonglat(city):\n    location = str(geolocator.geocode(city)[1])[1:-1]\n    return(location)\n\n\n\ndf_top250['Headquarters LongLat'] = df_top250['Headquarters'].apply(findlonglat)\n\nheadlonglat = ['Headquarters_Latitude','Headquarters_Longitude']\ndf_top250[headlonglat] = df_top250['Headquarters LongLat'].str.split(',',expand=True)\ndf_top250.drop(columns = ['Headquarters LongLat'], axis = 1, inplace = True)\n\nfor entry in headlonglat:\n    df_top250[entry] = pd.to_numeric(df_top250[entry])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_top250","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.10 Plotting data on map\nThe plot is done using folium. It shows where the most dense HQ is. Clearly this is around the East Coast of the US!\n\nThe green points are in the top 50, whilst orange are all others. Apart from Starbucks on the West coast, all other top 50 companies are in the East Coast."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sales = max(df_top250['Sales'])\n\nm = folium.Map(location=[50.70, -33.94], zoom_start=1, tiles='CartoDB positron')\n\ndef color_producer(val):\n    if val < 50:\n        return 'green'\n    else:\n        return 'orange'\n\nfor _, r in df_top250.iterrows():\n    fill_color=color_producer(r['Rank'])\n    tooltip = f\"{r['Restaurant']} (Rank: {str(r['Rank'])})\"\n    location = [r['Headquarters_Latitude'], r['Headquarters_Longitude']]\n    radius = 10*r['Sales']/max_sales\n    popup = r['Headquarters']\n    \n    cm = folium.CircleMarker(location = location, radius = radius,\n                        tooltip = tooltip, popup = popup,\n                        color = fill_color)\n    \n    m.add_child(cm)\n    \nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.11 Haversine Distance\nThe spherical polar distance can be calculated using this function, giving the overall distance in km between restaurants."},{"metadata":{"trusted":true},"cell_type":"code","source":"def haversine_distance(row):\n    lat_p, lon_p = row['Headquarters_Latitude_x'], row['Headquarters_Longitude_x']\n    lat_d, lon_d = row['Headquarters_Latitude_y'], row['Headquarters_Longitude_y']\n    radius = 6371 # km\n\n    dlat = np.radians(lat_d - lat_p)\n    dlon = np.radians(lon_d - lon_p)\n    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat_p)) * np.cos(np.radians(lat_d)) * np.sin(dlon/2) * np.sin(dlon/2)\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    distance = radius * c\n\n    return distance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.12 Checking distance between Top N restaurants"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Distance_Check(df, top_n_row):\n    perm = df.head(top_n_row)[['Restaurant','Headquarters_Latitude','Headquarters_Longitude']]\n\n    df_list = []\n\n    for i in range(len(perm)):\n        x = perm.loc[i].to_frame().transpose()\n        newdf = pd.DataFrame(np.repeat(x.values,4,axis=0))\n        newdf.columns = x.columns\n\n        df_drop = perm.drop(index = i).reset_index()\n\n        df_stackable = pd.merge(newdf, df_drop, left_index=True, right_index=True).drop('index', axis = 1)\n\n        df_list.append(df_stackable)\n    \n    \n    distance_df = pd.concat(df_list).reset_index(drop = True)\n\n    distance_df['distance'] = distance_df.apply(haversine_distance, axis = 1)\n\n    distance_df = distance_df.drop_duplicates(subset=['distance'])\n\n    distance_df = distance_df.sort_values(by=['distance'], ascending=False).reset_index(drop = True)\n    \n    return(distance_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.13 Restaurant distance pair\nAn obvious pattern doesn't really exist, but it appears aside from the top few restaurants (which are on the West coast), the remaining restaurants are close together. This quantifies this closeness."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Plot_Rest_Dist():\n    distance_df['Restaurant_Pair'] = distance_df['Restaurant_x'] + \" - \" + distance_df['Restaurant_y']\n\n    sns.set_theme(style=\"whitegrid\")\n    tips = sns.load_dataset(\"tips\")\n    ax = sns.barplot(x=\"Restaurant_Pair\", y=\"distance\", data=distance_df, palette = 'ch:start=.2,rot=-.3')\n    plot = plt.setp(ax.get_xticklabels(), rotation=90)\n\ndistance_df = Distance_Check(df_top250, top_n_row = 10)\n    \nPlot_Rest_Dist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.14 Top 20 Sales by Restaurants\nThe most sales are from McDonald's."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Plot_Rest_Sales(df,var,top_n_row = 20):\n    sns.set_theme(style=\"whitegrid\")\n    tips = sns.load_dataset(\"tips\")\n    ax = sns.barplot(x=\"Restaurant\", y=var, data=df.head(top_n_row), palette = 'viridis')\n    plot = plt.setp(ax.get_xticklabels(), rotation=90)\n    \nPlot_Rest_Sales(df_top250, var = 'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.15 Top 20 Sales by Segment Category\nClearly the Quick Service & Burger is quite clearly the most favourite."},{"metadata":{"trusted":true},"cell_type":"code","source":"top = 20\n\ntop_seg = df_top250[['Sales', 'Segment_Category']].groupby(df_top250['Segment_Category']).sum().sort_values('Sales', ascending = False).head(top).reset_index()\n\nsns.set_theme(style=\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Segment_Category\", y=\"Sales\", data=top_seg, palette = \"mako\")\nplot = plt.setp(ax.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.16 Starburst plot\nThere are quite significant contributors in the Quick Service & Burgers area - it is largely composed by McDonald's. Whilst Starbucks is Rank 2, it's still significantly smaller but still bolsters Quick Service & Coffee Cafe."},{"metadata":{"trusted":true},"cell_type":"code","source":"def StarburstPlot(df, var, sortvar):\n    \n    listA = [var, sortvar]\n    listB = [var,'Restaurant']\n    \n    seg_agg = df[listA].groupby(df[var]).sum().sort_values(sortvar, ascending = False)\n\n    seg_unique = list(seg_agg.index)\n\n    df_sb = df[df[var].isin(seg_unique)]\n\n    fig = px.sunburst(df_sb, path = listB, values=sortvar)\n\n    fig.show()\n    \nStarburstPlot(df_top250, 'Segment_Category', 'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.17 Correlation map\nThis checks for the collinearity between variables. As I intend to do a regression model, this may be important."},{"metadata":{"trusted":true},"cell_type":"code","source":"def CorMap(df, cols):\n    df_corr = df.drop(columns = cols, axis = 1)\n    \n    corr = df_corr.corr()\n\n    g = sns.PairGrid(df_corr)\n    g.map_diag(plt.hist)\n    g.map_offdiag(plt.scatter);\n    \n    return(corr)\n    \ncols = ['Rank', 'Restaurant', 'Segment_Category', 'Headquarters','Headquarters_Longitude', 'Headquarters_Latitude']\n    \ncorr = CorMap(df_top250, cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.18 Collinearity matrix\nThis quantifies the correlation between variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Coll_Matrix():\n    sns.set_theme(style=\"white\")\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n    \nColl_Matrix()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.19 Segment Category count\nThere are not too many segment categories in total. The starburst plot earlier demonstrated that restaurant performance is quite heavily driven by this parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(df_top250.Segment_Category)\nplot = plt.setp(ax.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.20 Extracting State and Country\nThe level of granularity I am interested in is State level - restaurants' general location may be important to determine sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top250[[\"State\",\"Country\"]] = df_top250.Headquarters.str.rsplit(', ', 2, expand=True).drop(0, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_LocCat_top250 = df_top250.drop(columns = ['Rank','Restaurant', 'Headquarters', 'Headquarters_Latitude', 'Headquarters_Longitude'], axis = 1)\ndf_LocCat_top250.loc[152,'State'] = 'London'\ndf_LocCat_top250.loc[207,'State'] = 'Moore Park'\ndf_LocCat_top250.loc[216,'State'] = 'Seongnam-si'\ndf_LocCat_top250.loc[250,'State'] = 'Quezon City'\n\ndf_LocCat_top250.loc[152,'Country'] = 'United Kingdom'\ndf_LocCat_top250.loc[207,'Country'] = 'Australia'\ndf_LocCat_top250.loc[216,'Country'] = 'South Korea'\ndf_LocCat_top250.loc[250,'Country'] = 'Luzon'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_LocCat_top250","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.21 State count\nCertain States certainly dominate the data. This indicates clusters of restaurants in particular regions."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(df_LocCat_top250.State)\nplot = plt.setp(ax.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.22 Performing regression\nI am attempting a regression model to determine Sales from remaining parameters. There are not many records so overfitting is inevitable. However, it is a cool experiment anyway.\n\nUnits from the density plot above is exponentially decaying as are sales, so I apply a log transform. I also one-hot encode the categorical variables (State, Segment Category and Country).\n\nTogether these transforms result in data within reasonable constraints for modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_LocCat_top250.drop('Sales', axis = 1)\n\ny = df_LocCat_top250[['Sales']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\nohe_list = ['Segment_Category', 'State', 'Country']\n\nfor ohe in ohe_list:\n    ohe_df = pd.get_dummies(X_train[ohe], prefix = ohe)\n    X_train = pd.concat([X_train, ohe_df], axis=1).drop([ohe], axis=1)\n    X_train['Units'] = np.log(X_train['Units'])\n\n    ohe_df = pd.get_dummies(X_test[ohe], prefix = ohe)\n    X_test = pd.concat([X_test, ohe_df], axis=1).drop([ohe], axis=1)\n    X_test['Units'] = np.log(X_test['Units'])\n\ncol_list = X_train.append(X_test).columns.tolist()\n\nX_train = X_train.reindex(columns = col_list).fillna(0)\nX_test = X_test.reindex(columns = col_list).fillna(0)\n    \ny_train = np.log(y_train)\ny_test = np.log(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.23 Random forest model\nI run a random forest regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestRegressor(verbose = 1)\nrf_model.fit(X_train, y_train)\ny_hat_rf = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.24 XGBoost model\nI attempt an xgboost model, using $R^2$ as the objective quantity."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10)\n\nxgb_reg.fit(X_train,y_train)\n\ny_hat_xgb = xgb_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.25 Random Forest Error in model\nThis is the error in the random-forest model. Due to the log transforms, this value is difficult to interpret. It is NOT the log of the error, however."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_test, y_hat_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.26 XGBoost Error in model\nThis is the error in the xgboost model. Due to the log transforms, this value is difficult to interpret. It is NOT the log of the error, however."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_test, y_hat_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.27 Combining the results from models\nHere I merge the results from both models with the actuals to plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(data=y_hat_rf, columns=[\"Random_Forest\"])\ndf2 = pd.DataFrame(data=y_hat_xgb, columns=[\"XGBoost\"])\n\ndf = pd.concat([y_test.reset_index(drop = True), df1, df2], axis=1).sort_values(by = 'Sales', ascending = False).reset_index(drop = True)\n\nfor cols in list(df):\n    df[cols] = np.exp(df[cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.28 Viewing all estimates\nIt appears that for extreme values, the model becomes inaccurate, yet there is relatively higher accuracy for smaller values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Estimateplot(slicer, var):\n    df_slice = df.iloc[slicer:]\n        \n    df_melt = df_slice.melt(var, var_name='Model', value_name='Estimated ' + var)\n\n    g = sns.scatterplot(x=var, y=\"Estimated \" + var, hue='Model', data=df_melt)\n    g = sns.lineplot(x = df[var], y = df[var], style=True, palette=['red'], dashes=[(2,2)])\n\n    lim = g.set(xlim=(min(df_slice[var]), max(df_slice[var])), ylim=(min(df_slice[var]), max(df_slice[var])))\n    \nEstimateplot(0,'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A.29 Viewing smaller estimates\nThis removes the higher 5 values. The closness in estimated values to actuals at lower values is clearer here.\n\nThe Random forest regressor is clearly a better fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"Estimateplot(5, 'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B. Starting with Independence 100 Restaurants data"},{"metadata":{},"cell_type":"markdown","source":"# B.1 Loading Independence 100 data"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_ind100 = pd.read_csv(prefix + 'Independence100.csv')\n\ndf_ind100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.2 Finding HQ from Location data\nUsing Nominatim to get consistent location info."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ind100['Headquarters'] = df_ind100['City'].apply(findfullad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.3 Correcting small errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ind100['Headquarters'] = df_ind100['Headquarters'].str.replace('\\d+', '', regex = True)\ndf_ind100['Headquarters'] = df_ind100['Headquarters'].str.replace(' ,', '', regex = True)\ndf_ind100['Headquarters'] = df_ind100['Headquarters'].str.replace(' -,', '', regex = True)\ndf_ind100.loc[38,'Headquarters'] = 'Bal Harbour, Florida, United States'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.4 Grabbing Latitude-Longitude values\nOnce again using Nominatim to get Lat-Long values"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_ind100['Headquarters LongLat'] = df_ind100['Headquarters'].apply(findlonglat)\n\nheadlonglat = ['Headquarters_Latitude','Headquarters_Longitude']\ndf_ind100[headlonglat] = df_ind100['Headquarters LongLat'].str.split(',',expand=True)\ndf_ind100.drop(columns = ['Headquarters LongLat'], axis = 1, inplace = True)\n\nfor entry in headlonglat:\n    df_ind100[entry] = pd.to_numeric(df_ind100[entry])\n    \ndf_ind100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.5 Visualising in Folium\nPutting top 20% in Green on Nominatim."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sales = max(df_ind100['Sales'])\n\nm = folium.Map(location=[40.70, -93.94], zoom_start=3, tiles='CartoDB positron')\n\ndef color_producer(val):\n    if val < 20:\n        return 'green'\n    else:\n        return 'orange'\n\nfor _, r in df_ind100.iterrows():\n    fill_color=color_producer(r['Rank'])\n    tooltip = f\"{r['Restaurant']} (Rank: {str(r['Rank'])})\"\n    location = [r['Headquarters_Latitude'], r['Headquarters_Longitude']]\n    radius = 10*r['Sales']/max_sales\n    popup = r['Headquarters']\n    \n    cm = folium.CircleMarker(location = location, radius = radius,\n                        tooltip = tooltip, popup = popup,\n                        color = fill_color)\n    \n    m.add_child(cm)\n    \nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.6 Plotting distances between Top 20 Restaurants"},{"metadata":{"trusted":true},"cell_type":"code","source":"distance_df = Distance_Check(df_ind100, top_n_row = 20)\n\nPlot_Rest_Dist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.7 Top 20 Restaurants by Sales\nTop performer is Carmine's (Times Square)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Rest_Sales(df_ind100, var = 'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.8 Correlation map"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Rank', 'Restaurant', 'Headquarters','Headquarters_Longitude', 'Headquarters_Latitude']\n    \ncorr = CorMap(df_ind100, cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.9 Correlation Matrix\nGrabbing correlation data between fields."},{"metadata":{"trusted":true},"cell_type":"code","source":"Coll_Matrix()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.10 Replacing New York with New York City"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ind100.Headquarters = df_ind100.Headquarters.replace('New York, United States', 'New York City, New York, United States')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.11 Separating data into State and Country\nAll records are in United States so I drop the Country column."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_ind100[[\"State\",\"Country\"]] = df_ind100.Headquarters.str.rsplit(', ', 2, expand=True).drop(0, axis = 1)\n\ndf_ind100.drop(columns = ['Country', 'City'], inplace = True)\n\ndf_ind100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.12 Drop unneeded columns for Location plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_LocCat_ind100 = df_ind100.drop(columns = ['Rank','Restaurant', 'Headquarters', 'Headquarters_Latitude', 'Headquarters_Longitude'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_LocCat_ind100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.13 Viewing Location data\nMost restaurants are in New York. Clearly Location matters!"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(df_LocCat_ind100.State)\nplot = plt.setp(ax.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.14 Setting up Regression models\nI use log transforms once again to translate the fields. Technically a min-max scaler may be better here with a Yeo-Johnson transform."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_LocCat_ind100.drop('Sales', axis = 1)\n\ny = df_LocCat_ind100[['Sales']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\nohe_df = pd.get_dummies(X_train['State'], prefix = 'State')\nX_train = pd.concat([X_train, ohe_df], axis=1).drop(['State'], axis=1)\n\nohe_df = pd.get_dummies(X_test['State'], prefix = 'State')\nX_test = pd.concat([X_test, ohe_df], axis=1).drop(['State'], axis=1)\n\ncol_list = X_train.append(X_test).columns.tolist()\n\nX_train = X_train.reindex(columns = col_list).fillna(0)\nX_test = X_test.reindex(columns = col_list).fillna(0)\n\nlog_cols = ['Average Check', 'Meals Served']\n\nfor cols in log_cols:\n    X_train[cols] = np.log(X_train[cols])\n    X_test[cols] = np.log(X_test[cols])\n    \ny_train = np.log(y_train)\ny_test = np.log(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.15 Random-Forest model\nRunning a Random-Forest model to predict sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestRegressor(verbose = 1)\nrf_model.fit(X_train, y_train)\ny_hat_rf = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.16 Running XGBoost model"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10)\n\nxgb_reg.fit(X_train,y_train)\n\ny_hat_xgb = xgb_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.17 Mean Absolute error for Random Forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_test, y_hat_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.18 Mean Absolute error for XGBoost model\nDue to earlier log transform, this is difficult to directly interpret."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_test, y_hat_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.19 Join XGBoost and Random forest with actuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(data=y_hat_rf, columns=[\"Random_Forest\"])\ndf2 = pd.DataFrame(data=y_hat_xgb, columns=[\"XGBoost\"])\n\ndf = pd.concat([y_test.reset_index(drop = True), df1, df2], axis=1).sort_values(by = 'Sales', ascending = False).reset_index(drop = True)\n\nfor cols in list(df):\n    df[cols] = np.exp(df[cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.20 Estimates of Sales plotted\nThis is for XGBoost and Random forest against actuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"Estimateplot(0,'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B.21 Some estimates removed\nHigher values are more difficult to estimate. The Random forest model performs better once again."},{"metadata":{"trusted":true},"cell_type":"code","source":"Estimateplot(5, 'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C. Starting with Future 50 Restaurants data"},{"metadata":{},"cell_type":"markdown","source":"# C.1 Loading Future 50 data\nBoth YOY_Sales and YOY_Units are present as % values again so once again transforms are applied."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_fut50 = pd.read_csv(prefix + 'Future50.csv', converters={'YOY_Sales':p2f, 'YOY_Units':p2f})\ndf_fut50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.2 Extracting HQ data from Location\nNominatim is used to extract location data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fut50['Headquarters'] = df_fut50['Location'].apply(findfullad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.3 Cleaning records..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fut50['Headquarters'] = df_fut50['Headquarters'].str.replace('\\d+', '', regex = True)\ndf_fut50['Headquarters'] = df_fut50['Headquarters'].str.replace(' ,', '', regex = True)\ndf_fut50['Headquarters'] = df_fut50['Headquarters'].str.replace(' -,', '', regex = True)\n\ndf_fut50.loc[18,'Headquarters'] = 'Orlando, Florida, United States'\ndf_fut50.loc[19,'Headquarters'] = 'Orange Park, Florida, United States'\ndf_fut50.loc[20,'Headquarters'] = 'Doral, Florida, United States'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fut50.drop('Location', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.4 Finding Longitude and Latitude from HQ locations\nOnce again Nominatim is applied to find coordinates."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_fut50['Headquarters LongLat'] = df_fut50['Headquarters'].apply(findlonglat)\n\nheadlonglat = ['Headquarters_Latitude','Headquarters_Longitude']\ndf_fut50[headlonglat] = df_fut50['Headquarters LongLat'].str.split(',',expand=True)\ndf_fut50.drop(columns = ['Headquarters LongLat'], axis = 1, inplace = True)\n\nfor entry in headlonglat:\n    df_fut50[entry] = pd.to_numeric(df_fut50[entry])\n    \ndf_fut50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.5 Plotting on Folium graph\nAll Restaurants data is plotted on graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sales = max(df_fut50['Sales'])\n\nm = folium.Map(location=[40.70, -93.94], zoom_start=3, tiles='CartoDB positron')\n\ndef color_producer(val):\n    if val < 10:\n        return 'green'\n    else:\n        return 'orange'\n\nfor _, r in df_fut50.iterrows():\n    fill_color=color_producer(r['Rank'])\n    tooltip = f\"{r['Restaurant']} (Rank: {str(r['Rank'])})\"\n    location = [r['Headquarters_Latitude'], r['Headquarters_Longitude']]\n    radius = 10*r['Sales']/max_sales\n    popup = r['Headquarters']\n    \n    cm = folium.CircleMarker(location = location, radius = radius,\n                        tooltip = tooltip, popup = popup,\n                        color = fill_color)\n    \n    m.add_child(cm)\n    \nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.6 Inter-distance Restaurant\nTop 10 restaurant inter-distances found."},{"metadata":{"trusted":true},"cell_type":"code","source":"distance_df = Distance_Check(df_fut50, top_n_row = 10)\nPlot_Rest_Dist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.7 YOY_Sales plot\nTop performer is Evergreens"},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Rest_Sales(df_fut50, var = 'YOY_Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.8 Franchising as function of YOY_Sales\nMajority of top 50 are Franchises."},{"metadata":{"trusted":true},"cell_type":"code","source":"StarburstPlot(df_fut50, 'Franchising', 'YOY_Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.9 Correlation map"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Rank', 'Restaurant', 'Headquarters','Headquarters_Longitude', 'Headquarters_Latitude', 'Franchising']\n    \ncorr = CorMap(df_fut50, cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.10 Correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"Coll_Matrix()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.11 Finding State and Country data\nSeparating location info into State and Country."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fut50[[\"State\",\"Country\"]] = df_fut50.Headquarters.str.rsplit(', ', 2, expand=True).drop(0, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.12 Drop Country\nAll States are in US so drop Country."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fut50.drop('Country', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_fut50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.13 Removing unneeded columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_LocCat_fut50 = df_fut50.drop(columns = ['Rank','Restaurant', 'Headquarters', 'Headquarters_Latitude', 'Headquarters_Longitude'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_LocCat_fut50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.14 Counts of Location data\nCalifornia is most common location. Clearly location matters once again!"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(df_LocCat_fut50.State)\nplot = plt.setp(ax.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.15 Visualising Franchise data\nClear imbalance between Franchise or not. So it is an important feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(df_LocCat_fut50.Franchising)\nplot = plt.setp(ax.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.16 Setting up Regression model\nThis time YOY_Sales is the dependent variable since Rating is quantified by this."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_LocCat_fut50.drop('YOY_Sales', axis = 1)\n\ny = df_LocCat_fut50[['YOY_Sales']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\nohe_df = pd.get_dummies(X_train['State'], prefix = 'State')\nX_train = pd.concat([X_train, ohe_df], axis=1).drop(['State'], axis=1)\n\nohe_df = pd.get_dummies(X_test['State'], prefix = 'State')\nX_test = pd.concat([X_test, ohe_df], axis=1).drop(['State'], axis=1)\n\ncol_list = X_train.append(X_test).columns.tolist()\n\nX_train = X_train.reindex(columns = col_list).fillna(0)\nX_test = X_test.reindex(columns = col_list).fillna(0)\n  \nlog_cols = ['Sales', 'Units', 'YOY_Units', 'Unit_Volume']\n\nfor cols in log_cols:\n    X_train[cols] = np.log(X_train[cols])\n    X_test[cols] = np.log(X_test[cols])\n\nX_train.Franchising = X_train.Franchising.replace(['Yes', 'No'], [1,0])\nX_test.Franchising = X_test.Franchising.replace(['Yes', 'No'], [1,0])\n\ndf.replace(0, 5)\n    \ny_train = np.log(y_train)\ny_test = np.log(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.17 Running Random Forest regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestRegressor(verbose = 1)\nrf_model.fit(X_train, y_train)\ny_hat_rf = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.18 Running XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10)\n\nxgb_reg.fit(X_train,y_train)\n\ny_hat_xgb = xgb_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.19 MAE for Random Forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_test, y_hat_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.20 MAE for XGBoost model\nClearly this is MUCH worse than Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_test, y_hat_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.21 Joining YOY_Sales actuals onto forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(data=y_hat_rf, columns=[\"Random_Forest\"])\ndf2 = pd.DataFrame(data=y_hat_xgb, columns=[\"XGBoost\"])\n\ndf = pd.concat([y_test.reset_index(drop = True), df1, df2], axis=1).sort_values(by = 'YOY_Sales', ascending = False).reset_index(drop = True)\n\nfor cols in list(df):\n    df[cols] = np.exp(df[cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.22 Visualising forecasts vs actuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"Estimateplot(0,'YOY_Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C.23 Subsetting forecasts\nLower values are easier to forecast, and Random_forest clearly runs way better here."},{"metadata":{"trusted":true},"cell_type":"code","source":"Estimateplot(5,'YOY_Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# D. Comparing all data"},{"metadata":{},"cell_type":"markdown","source":"# D.1 Plotting all data on one graph\nEvery graph is plotted onto one graph. There are clearly some clusters in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = folium.Map(location=[50.70, -33.94], zoom_start=1, tiles='CartoDB positron')\n\nfor _, r in df_top250.iterrows():\n    tooltip = f\"{r['Restaurant']} (Rank: {str(r['Rank'])} for Top 250)\"\n    location = [r['Headquarters_Latitude'], r['Headquarters_Longitude']]\n    popup = r['Headquarters']\n    \n    cm = folium.Circle(location = location,\n                        tooltip = tooltip, popup = popup,\n                        color = 'blue')\n    \n    m.add_child(cm)\n    \nfor _, r in df_ind100.iterrows():\n    tooltip = f\"{r['Restaurant']} (Rank: {str(r['Rank'])} for Ind 100)\"\n    location = [r['Headquarters_Latitude'], r['Headquarters_Longitude']]\n    popup = r['Headquarters']\n    \n    cm = folium.Circle(location = location,\n                        tooltip = tooltip, popup = popup,\n                        color = 'red')\n    \n    m.add_child(cm)\n\nfor _, r in df_fut50.iterrows():\n    tooltip = f\"{r['Restaurant']} (Rank: {str(r['Rank'])} for Future 50)\"\n    location = [r['Headquarters_Latitude'], r['Headquarters_Longitude']]\n    popup = r['Headquarters']\n    \n    cm = folium.Circle(location = location,\n                        tooltip = tooltip, popup = popup,\n                        color = 'green')\n    \n    m.add_child(cm)\n\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# D.2 Stacking all datasets\nAll three dataframes with coordinates are stacked."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"stack_df_top250 = df_top250[['Headquarters','State','Restaurant','Headquarters_Latitude','Headquarters_Longitude']]\nstack_df_top250['Type'] = 'Top 250 Restaurants'\n\nstack_df_ind100 = df_ind100[['Headquarters','State','Restaurant','Headquarters_Latitude','Headquarters_Longitude']]\nstack_df_ind100['Type'] = 'Independent 100 Restaurants'\n\nstack_df_fut50 = df_fut50[['Headquarters','State','Restaurant','Headquarters_Latitude','Headquarters_Longitude']]\nstack_df_fut50['Type'] = 'Future 50 Restaurants'\n\nstack_df = pd.concat([stack_df_top250,stack_df_ind100, stack_df_fut50],ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# D.3 Data converted into Array for DBScan"},{"metadata":{"trusted":true},"cell_type":"code","source":"coords = stack_df[['Headquarters_Latitude', 'Headquarters_Longitude']].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# D.4 Calculating clusters using distances\nThe distance is set to 350 km, to get 14 clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cluster_map(distance):\n    kms_per_radian = 6371.0088\n    epsilon = distance / kms_per_radian\n    db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n    cluster_labels = db.labels_\n    \n    clustercount = set(list(cluster_labels))\n    \n    clusters = pd.DataFrame(cluster_labels, columns = ['Clusters'])\n    \n    print(\"Total cluster count: \" + str(len(clustercount)))\n    \n    return(clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = cluster_map(350)\n\nresult = pd.concat([stack_df, clusters], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_list = ['red', 'blue', 'green', 'purple', \n              'orange', 'darkred','lightred', \n              'beige', 'darkblue', 'darkgreen', \n              'cadetblue', 'darkpurple', 'white',\n              'pink', 'lightblue', 'lightgreen', \n              'gray', 'black', 'lightgray']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# D.5 Visualising cluster data\nAll clusters are placed on one graph.\nClearly there is a strong preference for the West Coast."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = folium.Map(location=[40.70, -93.94], zoom_start=3, tiles='CartoDB positron')\n\nfor _, r in result.iterrows():\n    fill_color = color_list[r['Clusters']]\n    \n    tooltip = f\"{r['Restaurant']}; List: {r['Type']}; Cluster: {r['Clusters']}\"\n    location = [r['Headquarters_Latitude'], r['Headquarters_Longitude']]\n    popup = r['Headquarters']\n    \n    cm = folium.Circle(location = location,\n                        tooltip = tooltip, popup = popup,\n                        color = fill_color)\n    \n    m.add_child(cm)\n    \nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bar chart for each type\nAll graphs are placed onto one graph. This shows that restaurants tend to cluster similarly for each listing.\n\nCluster 0, 1 and 2 are the primary clusters. Cluster 0 is the East coast, Cluster 1 is the West coast whilst 2 is on the south end of the East coast."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\n\nsns.countplot(data=result, x='Clusters', hue='Type', ax=ax)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}