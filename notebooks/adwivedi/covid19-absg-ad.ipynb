{"cells":[{"metadata":{},"cell_type":"markdown","source":"**In this code, I have taken only top five json of covid file to find the relationship among the adjectives used in different paper of COVID19 research. ABSG method is a novel visualization approach to find relationship of similarity of different papers.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pprint\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport random\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import weight_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make a word list of common adjectives in data-set\n\nwords_dict = {\"Uncurable\":-1,\"love\":1,\"good\":1,\"awesome\":1,\"nice\":1,\"good quality\":1,\"classic\":1,\"pretty\":1,\"seasoned\":1,\"lovely\":1,\"privileged\":1,\"attentive\":1,\"friendly\":1,\"modern\":1,\"exceptional\":1,\"enthusiastic\":1,\"famous\":1,\"prompt\":1,\"special\":1,\"unbelievable\":1,\"courteous\":1,\"delightful\":1,\"efficient\":1,\"inexpensive\":1,\"great\":1,\"pleasant\":1,\"fresh\":1,\"cool\":1,\"refresh\":1,\"positive\":1,\"beautiful\":1,\"wonderful\":1,\"perfect\":1,\"best\":1,\"amazing\":1,\"excellent\":1,\"impressive\":1,\"impressed\":1,\"pleased\":1,\"overwhelmed\":1,\"negative\":-1,\"mean\":-1,\"bad\":-1,\"sad\":-1,\"poor\":-1,\"frustrated\":-1,\"low\":-1,\"worse\":-1,\"worst\":-1,\"horrible\":-1,\"cheap\":-1,\"ridiculous\":-1,\"overpriced\":-1,\"costly\":-1,\"pneumatic\":-1,\"strange\":-1,\"unprofessional\":-1,\"nasty\":-1,\"late\":-1,\"low quality\":-1,\"bad quality\":-1,\"disappointed\":-1,\"disappointing\":-1,\"angry\":-1}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract common adjectives from the different data sets \n\ndef get_adjectives(text):\n    blob = TextBlob(text)\n    adjectives = list()\n    for word, tag in blob.tags:\n        if tag == 'JJ':\n            adjectives.append(word.lower())\n    return set(adjectives)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the unique attribute from is dataset and map them based on similarity and buid a bipartite sentiment graph\n\ndef get_unique_attributes(file):\n    result = {}\n    unique_attr = {}\n\n    with open(file,'r') as file:\n\n        for i, line in enumerate(file):\n\n            result[i] = set()\n            words = line.split()\n\n            for w, word  in enumerate (map (lambda word :  word.lower().replace(\".\",\"\"), words)):\n                if word in words_dict:\n                    if 'no' == words[w-1].lower() or 'not' == words[w-1].lower():\n                        if 'not ' + word in unique_attr:\n                            unique_attr['not ' + word] +=  -1* words_dict[word]\n                        else:\n                            unique_attr['not ' + word] =  -1* words_dict[word]\n                    else:\n                        if word in unique_attr:\n                            unique_attr[word]+= words_dict[word]\n                        else:\n                            unique_attr[word]= words_dict[word]\n    return unique_attr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PercentageMissin(Dataset):\n    #\"\"\"this function will return the percentage of missing values in a dataset \"\"\"\n    if isinstance(Dataset,pd.DataFrame):\n        adict={} #a dictionary conatin keys columns names and values percentage of missin value in the columns\n        for col in Dataset.columns:\n            adict[col]=(np.count_nonzero(Dataset[col].isnull())*100)/len(Dataset[col])\n\n        return pd.DataFrame(adict,index=['% of missing'],columns=adict.keys())\n    else:\n        raise TypeError(\"can only be used with panda dataframe\")\n\nif __name__ == \"__main__\":\n\n    file1 = \"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/25621281691205eb015383cbac839182b838514f.json\"\n    file2 = \"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/7db22f7f81977109d493a0edf8ed75562648e839.json\"\n    file3 = \"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/6c3e1a43f0e199876d4bd9ff787e1911fd5cfaa6.json\"\n    file4 = \"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/2ce201c2ba233a562ee605a9aa12d2719cfa2beb.json\"\n    file5 = \"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/b460e5b511b4e2c3233f9476cd4e0616d6f405ac.json\"\n\n    files = [file1,file2,file3,file4,file5]\n    fnames = []\n\n    file_results = OrderedDict()\n    all_unique_attributes = set()\n\n    for f in files:\n        unique_attributes = get_unique_attributes(f)\n        file_results[os.path.basename(f)] = unique_attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('##################################### File wise attributes - start #################')\npp = pprint.PrettyPrinter()\npp.pprint(file_results)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('##################################### File wise attributes - end #################')\n\nfor res in file_results.values():\n    all_unique_attributes.update(res)\nprint('')\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('##################################### Unique attributes amongst all reviews #################')\npp.pprint(all_unique_attributes)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"B = nx.Graph()\nB.add_nodes_from(file_results.keys(), bipartite=0)\nB.add_nodes_from(all_unique_attributes, bipartite=1)\n\nall_edges = []\nfor file, weight_dict in file_results.items():\n    for attribute, weight in weight_dict.items():\n        all_edges.append((file,attribute,weight))\n        B.add_weighted_edges_from(all_edges)\n\n    print(B.edges(data=True))\n\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = {node:[0, i] for i,node in enumerate(file_results.keys())}\npos.update({node:[1, i] for i,node in enumerate(all_unique_attributes)})\nnx.draw(B, pos, with_labels=False)\nfor p in pos:  # raise text positions\n    pos[p][1] += 0.25\n    nx.draw_networkx_labels(B, pos)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#The cardinality of words used in different papers are shown in above graph.\n#'efficient and positive, low and negative words are most popular among these five papers, however considering all dataset will bring a different picture.'****"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}