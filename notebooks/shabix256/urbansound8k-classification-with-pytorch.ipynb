{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\n\nimport torch\nimport torchaudio\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nfrom torch import nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-13T12:32:02.325169Z","iopub.execute_input":"2021-07-13T12:32:02.325502Z","iopub.status.idle":"2021-07-13T12:32:04.253427Z","shell.execute_reply.started":"2021-07-13T12:32:02.325418Z","shell.execute_reply":"2021-07-13T12:32:04.252146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n\n    def __init__(self, annotations_file, audio_dir,transformation,target_sample_rate,num_samples,device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        # signal -> (num_channels,samples) -> (2,16000) -> (1,16000)\n        signal = self._resample_if_necessary(signal,sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n    \n    def _cut_if_necessary(self, signal):\n        # signal -> Tensor -> (1, num_samples) -> (1,50000) -> (1,22050)\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n    \n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            # [1,1,1] -> [1,1,1,0,0]\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n            signal = resampler(signal)\n        return signal\n    \n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1: # (2,16000)\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:04.255509Z","iopub.execute_input":"2021-07-13T12:32:04.256112Z","iopub.status.idle":"2021-07-13T12:32:04.273141Z","shell.execute_reply.started":"2021-07-13T12:32:04.256064Z","shell.execute_reply":"2021-07-13T12:32:04.271839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANNOTATIONS_FILE = '../input/urbansound8k/UrbanSound8K.csv'\nAUDIO_DIR = \"../input/urbansound8k\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:04.275582Z","iopub.execute_input":"2021-07-13T12:32:04.27625Z","iopub.status.idle":"2021-07-13T12:32:04.292191Z","shell.execute_reply.started":"2021-07-13T12:32:04.276104Z","shell.execute_reply":"2021-07-13T12:32:04.290944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\nprint(f\"Using device {device}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:04.294611Z","iopub.execute_input":"2021-07-13T12:32:04.295207Z","iopub.status.idle":"2021-07-13T12:32:04.373335Z","shell.execute_reply.started":"2021-07-13T12:32:04.295131Z","shell.execute_reply":"2021-07-13T12:32:04.371945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mel_spectogram = torchaudio.transforms.MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_fft=1024, #frame size\n    hop_length=512,\n    n_mels=64\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:04.375701Z","iopub.execute_input":"2021-07-13T12:32:04.376634Z","iopub.status.idle":"2021-07-13T12:32:04.504811Z","shell.execute_reply.started":"2021-07-13T12:32:04.376584Z","shell.execute_reply":"2021-07-13T12:32:04.503684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR,mel_spectogram,SAMPLE_RATE,NUM_SAMPLES,device)\nprint(f\"There are {len(usd)} samples in the dataset.\")\nsignal, label = usd[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:44:14.689555Z","iopub.execute_input":"2021-07-13T12:44:14.689973Z","iopub.status.idle":"2021-07-13T12:44:14.725148Z","shell.execute_reply.started":"2021-07-13T12:44:14.689941Z","shell.execute_reply":"2021-07-13T12:44:14.72411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:44:19.713432Z","iopub.execute_input":"2021-07-13T12:44:19.713812Z","iopub.status.idle":"2021-07-13T12:44:19.722611Z","shell.execute_reply.started":"2021-07-13T12:44:19.71378Z","shell.execute_reply":"2021-07-13T12:44:19.721126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:11.873373Z","iopub.execute_input":"2021-07-13T12:32:11.874347Z","iopub.status.idle":"2021-07-13T12:32:22.216643Z","shell.execute_reply.started":"2021-07-13T12:32:11.874291Z","shell.execute_reply":"2021-07-13T12:32:22.21539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\n\nclass CNNNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 4 conv blocks / flatten / linear / softmax\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(128 * 5 * 4, 10)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:22.219839Z","iopub.execute_input":"2021-07-13T12:32:22.220165Z","iopub.status.idle":"2021-07-13T12:32:22.240935Z","shell.execute_reply.started":"2021-07-13T12:32:22.220132Z","shell.execute_reply":"2021-07-13T12:32:22.239897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = CNNNetwork()\nsummary(cnn.cuda(),(1,64,44))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:22.243914Z","iopub.execute_input":"2021-07-13T12:32:22.24429Z","iopub.status.idle":"2021-07-13T12:32:22.314711Z","shell.execute_reply.started":"2021-07-13T12:32:22.24426Z","shell.execute_reply":"2021-07-13T12:32:22.313629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n    return train_dataloader\n\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in data_loader:\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")\n\n\ndef train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:32:22.316244Z","iopub.execute_input":"2021-07-13T12:32:22.316699Z","iopub.status.idle":"2021-07-13T12:32:22.329267Z","shell.execute_reply.started":"2021-07-13T12:32:22.316658Z","shell.execute_reply":"2021-07-13T12:32:22.328157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\nprint(f\"Using device {device}\")\n\n\n\nBATCH_SIZE = 128\nEPOCHS = 10\nLEARNING_RATE = 0.001\n\nANNOTATIONS_FILE = '../input/urbansound8k/UrbanSound8K.csv'\nAUDIO_DIR = \"../input/urbansound8k\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050\n\nmel_spectogram = torchaudio.transforms.MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_fft=1024, #frame size\n    hop_length=512,\n    n_mels=64\n)\n\n\nusd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR,mel_spectogram,SAMPLE_RATE,NUM_SAMPLES,device)\n\ntrain_dataloader = create_data_loader(usd, BATCH_SIZE)\n\n#  construct model and assign it to device\ncnn = CNNNetwork().to(device)\nprint(cnn)\n\n# initialise loss funtion + optimiser\nloss_fn = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(cnn.parameters(),\n                             lr=LEARNING_RATE)\n\n# train model\ntrain(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n\n# save model\ntorch.save(cnn.state_dict(), \"feedforwardnet.pth\")\nprint(\"Trained feed forward net saved at feedforwardnet.pth\")","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:44:57.190983Z","iopub.execute_input":"2021-07-12T09:44:57.191342Z","iopub.status.idle":"2021-07-12T09:54:14.933331Z","shell.execute_reply.started":"2021-07-12T09:44:57.191312Z","shell.execute_reply":"2021-07-12T09:54:14.931306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_mapping = [\n    \"air_conditioner\",\n    \"car_horn\",\n    \"children_playing\",\n    \"dog_bark\",\n    \"drilling\",\n    \"engine_idling\",\n    \"gun_shot\",\n    \"jackhammer\",\n    \"siren\",\n    \"street_music\"\n]\n\n\ndef predict(model, input, target, class_mapping):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(input)\n        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n        predicted_index = predictions[0].argmax(0)\n        predicted = class_mapping[predicted_index]\n        expected = class_mapping[target]\n    return predicted, expected\n\n\ncnn = CNNNetwork()\nstate_dict = torch.load(\"../input/trained-cnn-model/cnnnet.pth\")\ncnn.load_state_dict(state_dict)\n\n# load urban sound dataset dataset\nmel_spectrogram = torchaudio.transforms.MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_fft=1024,\n    hop_length=512,\n    n_mels=64\n)\n\nusd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                        AUDIO_DIR,\n                        mel_spectrogram,\n                        SAMPLE_RATE,\n                        NUM_SAMPLES,\n                        \"cpu\")\n\n\n# get a sample from the urban sound dataset for inference\ninput, target = usd[0][0], usd[0][1] # [batch size, num_channels, fr, time]\ninput.unsqueeze_(0)\n\n# make an inference\npredicted, expected = predict(cnn, input, target,\n                              class_mapping)\nprint(f\"Predicted: '{predicted}', expected: '{expected}'\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T12:36:35.975096Z","iopub.execute_input":"2021-07-13T12:36:35.975477Z","iopub.status.idle":"2021-07-13T12:36:36.164353Z","shell.execute_reply.started":"2021-07-13T12:36:35.975445Z","shell.execute_reply":"2021-07-13T12:36:36.163289Z"},"trusted":true},"execution_count":null,"outputs":[]}]}