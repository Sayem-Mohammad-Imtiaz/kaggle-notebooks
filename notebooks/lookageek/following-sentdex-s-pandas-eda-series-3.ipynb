{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"#### This Kernel Follows Part 6 of the sentdex's [Data Analysis with Pandas](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfSfqQuee6K8opKtZsh7sA9)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('../input/diamonds.csv', index_col=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca0498179ecde8161697d1fa927b494a4d8f0b3c"},"cell_type":"markdown","source":"The task is to predict the price of a diamond based on all the other data about the diamond in the dataset. Since the prediction is a continuous value - like 326 dollars for Diamond \\#1, it is a Regression problem. Constructing a linear regressor as our model using scikit-learn library.\n\n\nCursory look at the columns tell that all of them are pretty important at determining the price of the diamond, hence we will use all the columns (except the price) as our features, and price is our target. \n\nSome of our features seem to be a value from a finite set of possible values - like \"cut\" and \"color\", our linear regressor can only take numerical values, we need to turn these into numericals, we can just assign a number of each of the possible values (a vocabulary) like so:"},{"metadata":{"trusted":true,"_uuid":"6656c5a727bd1bf846fbc87240a16d56391f14fd"},"cell_type":"code","source":"df['cut'].astype(\"category\").cat.codes[:200]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aa945e63eebd010768a1e2422d0ec1123be906f"},"cell_type":"markdown","source":"The above will take cut and assigns increasing values based on when it encounters a new possible value. \n\nBy doing this, we are losing the weight of each value in the cut's vocabulary - \"Premium\" cut is definitely more valuable than \"Good\" cut. So need to associate a number which reflects the semantic weight of the values \"cut\" can take.\n\nTo achieve this - creating a dictionary for each of the text based features "},{"metadata":{"trusted":true,"_uuid":"3ed03b721569e76683540b3ef11c8af81a16486d"},"cell_type":"code","source":"cut_class_dict = {\"Fair\": 1, \"Good\": 2, \"Very Good\": 3, \"Premium\": 4, \"Ideal\": 5}\ncolor_dict = {\"J\": 1,\"I\": 2,\"H\": 3,\"G\": 4,\"F\": 5,\"E\": 6,\"D\": 7}\nclarity_dict = {\"I3\": 1, \"I2\": 2, \"I1\": 3, \"SI2\": 4, \"SI1\": 5, \"VS2\": 6, \"VS1\": 7, \"VVS2\": 8, \"VVS1\": 9, \"IF\": 10, \"FL\": 11}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af92c0f86aad702eb3225ad2f152730b0fca389c"},"cell_type":"markdown","source":"The numerical values corresponding to each text value possibility for the dictionary come from the descriptive text in the dataset which talks about the relative values for the vocalbulary "},{"metadata":{"trusted":true,"_uuid":"d12578e765e7c466ecb12e2f60514fa09dd4785b"},"cell_type":"code","source":"# Mapping using these dictionaries in the dataframe\ndf['cut'] = df['cut'].map(cut_class_dict)\ndf['color'] = df['color'].map(color_dict)\ndf['clarity'] = df['clarity'].map(clarity_dict)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e295108666b2e95d065031dc3742048430a3502a"},"cell_type":"code","source":"import sklearn\nfrom sklearn import svm\n\n# Shuffle the dataframe using sk learn, you can use pandas reindex method with np.random too\ndf = sklearn.utils.shuffle(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eacaf4c1dc4a9f46e9a7895c31b4e91957f31b55"},"cell_type":"code","source":"# X is the feature set - a list of list of features\nX = df.drop('price', axis=1).values\n# y is the target - a list of prices\ny = df['price'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8f8d50eddb70ada105904538aea8b622a4600a5"},"cell_type":"markdown","source":"### Scaling the values in feature set to be between 0 and 1\n\nModels like the Linear Regressors are basically just doing linear algebra over and over again and the simpler you can make the data - by reducing the spread, the faster it can converge! We can employ scaling to all the columns of the data rows to be between 0 and 1 - there are many math formulas to do this\n* `scaled_value = value / mean`\n* `scaled_value_z_score = value - mean / standard_deviation`\n\nand many others\n\nHere we will use sklearn's preprocessing to do the magic for us, we are not as much worried about the specific range of values post scaling - it can be 0 to 1 or -1 to 1 or -3 to 3 - we are good if it is sufficiently small enough!\n"},{"metadata":{"trusted":true,"_uuid":"8cb35f29d70cf51d0bc8960db3bbe4a08c8c3f0c"},"cell_type":"code","source":"X = sklearn.preprocessing.scale(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d7ccffb2a0812603bbc247c5bbef08708dccaef"},"cell_type":"markdown","source":"### Train and test split of the data rows\n\nTo validate the model we will hold out some data rows - test data - and not use them to train the model, by evaluating the model's prediction on previously unseen data (during the training phase that is) we will know the correctness of the model - whether it can take in new data when deployed to production and provide good predictions on data that will come in tomorrow."},{"metadata":{"trusted":true,"_uuid":"8461b41e5c593cf98478cb10c2b4f12fe45e4afd"},"cell_type":"code","source":"# Video uses a manual method, but sklearn gives a nice method to do this declaratively\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a49df3696547699f826a0254d9a15fed667ea1d4"},"cell_type":"markdown","source":"### Defining and training the model using train data\n\nUsing Support Vector Regressor - a modification of SVM which, used for classification, modified using a kernel to act as a regressor.\n\nSklearn is a beauty when it comes to API to define and train models"},{"metadata":{"trusted":true,"_uuid":"d2c4ba8609c0c0d5944bcac0ab845c6457dd9c9b"},"cell_type":"code","source":"%%time\n# Support Vector Regression with Linear Kernel\nmodel = svm.SVR(kernel='linear')\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b85ca7f50f98a471823f866e573bc393db022bd"},"cell_type":"markdown","source":"Now let us put the trained model on a pedestal and ask it to predict for our test data and evaluate the score - higher the score = predictions matched well with the actual values in test data set"},{"metadata":{"trusted":true,"_uuid":"7440309be329eb1fe58d1d89694aea58c29c7c9b"},"cell_type":"code","source":"model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"360a4fce8e389c02d75f395875ce42f839e2e7ba"},"cell_type":"markdown","source":"Prodding further - printing out a few predicted and actual values for the test set "},{"metadata":{"trusted":true,"_uuid":"a1c434c82ea8abf79e9af0e75297adba9e6bc5bd"},"cell_type":"code","source":"for X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e2a5b4507c58ca6ff9ef38563c2b13ab92916e3"},"cell_type":"markdown","source":"For some cases it is predicting negative prices! Not so useful in those cases.\n\n#### Generally there are two approaches from here\n* Train a few different models, compare and contrast\n* Tweak the model's knobs to perform better\n\nWe will take the first route, train a few and evaluate"},{"metadata":{"trusted":true,"_uuid":"5bacfa9eca635d29b52eb3a4452006eb0aad7832"},"cell_type":"code","source":"%%time\n# Support Vector Regression with RBF Kernel\nmodel = svm.SVR(kernel='rbf')\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b6b06b26247a1498d12e9243a318b019e740d63"},"cell_type":"code","source":"print(f\"--- Score: {model.score(X_test, y_test)} ---\")\n\nfor X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bcc2cb4a26c1efda59c9a08a0d84c6742dec634"},"cell_type":"markdown","source":"RBF Kernel seem to have done really bad with score of 0.51, but hey, it does have the negative problem which the linear kernel had"},{"metadata":{"trusted":true,"_uuid":"dd69a671bcaff4237876180466053091eaab2448"},"cell_type":"code","source":"%%time\n# Using SGD Regressor\nmodel = sklearn.linear_model.SGDRegressor(max_iter=10_000)\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847ee9369fa24decd81d5701402c0e14f0849f13"},"cell_type":"code","source":"print(f\"--- Score: {model.score(X_test, y_test)} ---\")\n\nfor X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cc4be1b633233e4dc3160bb800977e86c1f741b"},"cell_type":"code","source":"%%time\n# Using Linear Regression\nmodel = sklearn.linear_model.LinearRegression()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a8199d5e1c59824489f769808e60c638cc52aa1"},"cell_type":"code","source":"print(f\"--- Score: {model.score(X_test, y_test)} ---\")\n\nfor X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29db19783b005099dfce3b713bd4dcc2d4b93d58"},"cell_type":"markdown","source":"SGD Regressor seem to have done quite well with score of 0.9, but there are some negative predictions here too!\n\nEnsembling these together will give a better score, this is what is done in production systems, ensembles would throw out negative predictions by constituent models and take prediction from another non-negative model - thus smoothing out the aberrations "},{"metadata":{"_uuid":"5203cbe32947ab0b9d659a968694889bd5fd517d"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"373710953f54f023dd61ae17b7ff77d66c66148d"},"cell_type":"markdown","source":"As some people in the video's comments section have mentioned - to use \"dummies\" instead of assigning our own scale values for text based columns - we are losing out on the scale of difference, how different is \"Premium\" from \"Fair\" - is price affected by the linear scale what we have assumed, or there is a different scale - an exponential different between \"Premium\" and \"Fair\". \n\nTo sidestep this entire debate, we can use a representation called [\"dummies\" in our dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html), referred to as one-hot encoding as well, where for each possible value of \"cut\" or \"color\" columns we create a new column in our dataframe and if the cut is \"Ideal\" then the value of column \"cut_Ideal\" is marked 1, all others are marked as 0. "},{"metadata":{"trusted":true,"_uuid":"87bf6a739c1abdaaf04c8cb2ffe66957edcf6bd1"},"cell_type":"code","source":"df = pd.read_csv('../input/diamonds.csv', index_col=0)\ndummies_df = pd.get_dummies(df)\ndummies_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e6f767af0775fd614562b8273b20d9acf017b25"},"cell_type":"code","source":"# X is the feature set - a list of list of features\nX = dummies_df.drop('price', axis=1)\n# y is the target - a list of prices\ny = dummies_df['price'].values\n\n# Scale our features\nX[['depth', 'carat', 'table', 'x', 'y', 'z']] = sklearn.preprocessing.scale(X[['depth', 'carat', 'table', 'x', 'y', 'z']])\nX = X.values\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70bef35e64b7227f9441780d98a7939920c78712"},"cell_type":"markdown","source":"#### Training all three models, and checking what improvement we can get out of it"},{"metadata":{"trusted":true,"_uuid":"a6cfd0795a6850db6ae1521946e6ba141e0f1647"},"cell_type":"code","source":"%%time\n# Support Vector Regression with Linear Kernel\nmodel = svm.SVR(kernel='linear')\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"476c7becc45497e6dbe42989309e6b5a5c24287e"},"cell_type":"code","source":"print(f\"--- Score: {model.score(X_test, y_test)} ---\")\n\nfor X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb0f9beb908bc4dc4ac44bf0cb9eeabbea4bc3b5"},"cell_type":"code","source":"%%time\n# Support Vector Regression with RBF Kernel\nmodel = svm.SVR(kernel='rbf')\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4455acaca85713a8d5d50eb68839be1bcca1f8ed"},"cell_type":"code","source":"print(f\"--- Score: {model.score(X_test, y_test)} ---\")\n\nfor X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e1c77891943e027930cbade219cc0351a1e72b3"},"cell_type":"code","source":"%%time\n# Using SGD Regressor\nmodel = sklearn.linear_model.SGDRegressor(max_iter=10_000)\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"904a38d8452bba525eebae99b390071e7918c920"},"cell_type":"code","source":"print(f\"--- Score: {model.score(X_test, y_test)} ---\")\n\nfor X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bae4867ea0006253af2fdbb86f8bf2f578012dec"},"cell_type":"markdown","source":"Seems to have no significant improvement, only a small increment, can fall back to LinearRegressor once."},{"metadata":{"trusted":true,"_uuid":"e8ea30b1ec67025f81ec33ec10ce4cbcbc43c532"},"cell_type":"code","source":"%%time\n# Using Linear Regression\nmodel = sklearn.linear_model.LinearRegression()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d835b573c1327e52afea679d9b5420e5e870f488"},"cell_type":"code","source":"print(f\"--- Score: {model.score(X_test, y_test)} ---\")\n\nfor X, y in list(zip(X_test, y_test))[:50]:\n    print(f\"Predicted: {model.predict([X])[0]}, Actual: {y}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d8709fe66eeec7e48162851b0378c7b3dd9dafd"},"cell_type":"markdown","source":"A simple linear regressor itself starts giving 0.91 score - by taking much much lesser time to train. Always tend towards simpler models - before jumping into fancy deep learning method - so apparent here when comparing support vector machines vs simple linear regression.\n\nRegarding buying some more accuracy beyond 91%, We will visit back, maybe should:\n* give some more training data, current split is 70-30\n* tune hyperparameters"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}