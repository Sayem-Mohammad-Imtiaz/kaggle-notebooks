{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Synopsis\n\nThis dataset, from the University Of Wisconsin, is the result of 569 digitalised images of breast mass tissue. The problem presented is a classification task with 2 outcomes, malignant and benign. The data consists of 30 features, each real valued.\n\n> \"Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\" [1](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n\n\n## Goal\n\nAccurately determine the diagnosis (M = malignant, B = benign) of potentially cancerous breast tissue cells based on their features.\n\n","metadata":{"_uuid":"75041d0c-6bcc-4a92-a328-37851946a11f","_cell_guid":"b1ce77e2-96ff-4e6e-8a23-ad1c0a51c410"}},{"cell_type":"markdown","source":"<h1 id='Initialisation'>\n1. Initialisation\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>","metadata":{}},{"cell_type":"code","source":"#Numpy, Pandas\nimport numpy as np\nimport pandas as pd\n\n!pip install seaborn --upgrade\n\n#Visualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = [10, 6]","metadata":{"_uuid":"64d0e63d-dd0c-471f-baf3-4e49d27e13d3","_cell_guid":"d112f317-9f3f-4db4-abc9-0ddfd55f4a34","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breast_cancer_data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\n\nbreast_cancer_data.head()","metadata":{"_uuid":"3b2d2d2a-7d08-4723-a2cd-f1e1f66faaa0","_cell_guid":"1e088d50-f79a-49a6-93ce-62262ffbad2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breast_cancer_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Preparing training/Test Sets'>\n1.1 Preparing training/Test Sets\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nThe dataset is split approximately 70%/30% into train and test sets respectively. Cross validation will be used to evaluate the models, so I did not include a validation set in the data split. These are also stratified with respect to the diagnosis. Due to the imbalance of the whole dataset it is important that this imbalance is incorporated into the models that are trained. The id column doesn't provide useful information for the learning algorithms so it is dropped, as well as the mysterious 'Unnamed: 32' column. The diagnosis column is redefined into [0,1] being the benign and malignant outcomes respectively. Here, I:\n\n* Drop the Id column as it has no objective value\n* Split data into train/validation/test sets, being careful to stratify the data (at every split) based on the diagnosis\n* Map target variable to numeric values for analysis","metadata":{"trusted":true}},{"cell_type":"code","source":"X=breast_cancer_data.drop('diagnosis', axis=1)\ny=breast_cancer_data['diagnosis'].copy()\n\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breast_cancer_data.drop('Unnamed: 32', axis=1, inplace=True)\nbreast_cancer_data.drop('id', axis=1, inplace=True)\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(breast_cancer_data,\n                               test_size=0.4, \n                               stratify = breast_cancer_data['diagnosis'],\n                               random_state=101)\n\nX=breast_cancer_data.drop('diagnosis', axis=1).copy()\ny=breast_cancer_data['diagnosis'].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.3, \n                                                    stratify = y, \n                                                    random_state=101)\n#train size: 70%\n#test size: 30%\n\ntrain['diagnosis'] = train['diagnosis'].map({'M':1, 'B':0}).copy()","metadata":{"_uuid":"d185c72e-e048-4c5f-a3f1-2a39cdf6fbb1","_cell_guid":"c984607e-5767-45df-b03f-1c600529f21e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Missing Data'>\n1.2 Missing Data\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nThere is one column with all of its entries as Nan; 'Unnamed: 32'. I will delete this column.","metadata":{"_uuid":"d803150c-d3d5-4d3b-8651-3bd7ac6402cd","_cell_guid":"a68dad6c-cf75-4806-a3bf-f3b7175e8332","trusted":true}},{"cell_type":"code","source":"breast_cancer_data.isnull().sum().any()","metadata":{"_uuid":"11fbda54-57af-499c-822b-330b647b1a6f","_cell_guid":"3ebd4337-9924-448c-9ddd-1cf3f2899227","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Training Data Analysis'>\n2. Training Data Analysis\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nGoals:\n\n* Look at the proportion of each diagnosis\n* Gather statistical insight about the training data (mainly the binomial distributions of every feature, to determine a scaler for the data)\n* See which features correlate the most with the diagnosis","metadata":{"_uuid":"bb9178d2-2904-450f-a608-e153246f2010","_cell_guid":"bb2c4b29-0a4d-4378-94a6-10b84038659e","trusted":true}},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"36a8176f-57a8-4200-987e-369fc02326f0","_cell_guid":"73e7f930-a8e7-4c30-8d6c-b64f1eaa3749","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_values = X_train.corrwith(train['diagnosis']).sort_values(ascending=False)\ncorr_values = pd.DataFrame(corr_values, columns = ['corr_w_diagnosis'])\ncorr_values = corr_values.reset_index().rename(columns={'index':'features'})\ncorr_values['features'] = corr_values['features'].astype(str)\ncorr_values","metadata":{"_uuid":"4d65f283-f0b5-48ab-9827-4fbe8f38c0cd","_cell_guid":"0a440653-76d6-4605-93e9-2ea69737322f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are many features which have a low correlation to the target variable. The high dimensionality of the data presents a multicollinearity problem which the above correlation values might not pick up on. In other words, how do each of the features affect each other resulting in each diagnosis? Could a cell's mean texture and mean fractal dimension conspire to highly influence the diagnosis? As I'm not an expert in this field, I calculate the **variable inflation factors** for each feature and decide which features to remove from the training and testing datasets. I could have also created a correlation matrix but I wanted a more robust method especially for a topic like this (predicting a cancer diagnosis is not a trivial thing in the real world!).","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif_corr = pd.DataFrame()\nvif_corr['features'] = X_train.columns\nvif_corr['vif_value'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif_corr.sort_values('vif_value', ascending = False)\nvif_corr['features'] = vif_corr['features'].astype(str)\nvif_corr = pd.merge(corr_values, vif_corr, on = 'features')\nvif_corr.sort_values(by = 'corr_w_diagnosis', ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the vif values of each feature are a similar match to the correlation values (but for example, concave points_worst has a much lower vif value, so it might not actually be that useful at all. ","metadata":{}},{"cell_type":"code","source":"vif_corr.sort_values(by = 'vif_value', ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I filter out the features with a low vif value and low correlation to the target (values less than 0.6 and 100 respectively. These values can be changed if needed).","metadata":{}},{"cell_type":"code","source":"features_to_drop = vif_corr.loc[(vif_corr['corr_w_diagnosis'] < 0.6) & (vif_corr['vif_value'] < 200)]\nfeatures_to_drop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(features_to_drop['features'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.pie(train, \n             values=train['diagnosis'].value_counts(), \n             names = ['Benign', 'Malignant'], \n             title='Proportion of diagnoses in training data', \n             width=800, \n             height=400\n            )\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['diagnosis'].map({1:'Malignant', 0:'Benign'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising all of the features vs the diagnosis, separated by mean, worst and standard-error.","metadata":{}},{"cell_type":"code","source":"#splitting feature columns into mean/worst and se \nfeature_list = list(train.columns)\nfeature_array = np.array_split(feature_list, 3)\n\nmean = list(feature_array[0])\nmean.remove('diagnosis')\n\nworst = list(feature_array[1])\n\nse = list(feature_array[2])\n\nfeature_lists = [mean, worst, se]\nfor feat in feature_lists:\n    fig, axs = plt.subplots(ncols = 1, nrows = 10, sharey = True, figsize = (12,40))\n    for i in range(0,10):\n        sns.scatterplot( x = feat[i], y = 'diagnosis', data = train, ax = axs[i] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look at the binomial distributions for every feature:","metadata":{"_uuid":"907127a2-1676-4994-8bcd-2842870c1ae2","_cell_guid":"d3fce078-efc1-403b-84bd-be80a4315d81","trusted":true}},{"cell_type":"code","source":"pd.options.plotting.backend = 'matplotlib'\nX_train.hist(bins=30, figsize=(20,15))\nplt.title('Distribution of features before PowerTransformer transform')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After testing StandardScaler, MinmaxScaler and PowerTransformer, I found that my trained models performed better when PowerTransformer was used to scale the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\nX_train_hist = X_train.copy()\n\n#PowerTransformer transform \nscaler = PowerTransformer()\nhist_data = pd.DataFrame(scaler.fit_transform(X_train))\n\nhist_data.hist(bins = 30, figsize = (20,15))\nplt.title('Distribution of features after PowerTransformer transform')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Transforming Data'>\n2.1. Observations\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\n* This is an imbalanced dataset so data should be stratified accordingly.\n* There is considerable overlap between features and diagnosis.\n* 3 of the most correlated features to the diagnosis are 'worst' features.\n* There are 4 features which are negatively correlated to the target.","metadata":{}},{"cell_type":"markdown","source":"<h1 id='Transforming Data'>\n3. Transforming Data\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nI made a custom transformer that takes in the X and y data and does the following:\n\n* Makes sure there are no missing rows\n* Drops 'diagnosis', 'id' and 'unnamed' columns\n* Drops the 'Unnamed: 32' column\n* Maps target variable to numeric values\n* Delete negatively correlated features","metadata":{"_uuid":"9352fd66-4cbc-46e9-85a7-473178b04ae4","_cell_guid":"2505f683-a30e-4ecd-8c87-860fd4bf4985","trusted":true}},{"cell_type":"code","source":"def custom_X_transformer(X):\n    try: \n        #Drop rows with missing values\n        X = X.replace([np.inf, -np.inf], np.nan)\n        X = X.dropna()\n    except:\n        pass\n    try:\n        #Dropping irrelevant columns if they haven't already been dropped\n        X = X.drop('diagnosis', axis=1)\n    except:\n        pass\n    try:\n        X = X.drop('Unnamed: 32', axis=1)\n    except:\n        pass\n    try:\n        #Dropping features with low correlation and vif values\n        X = X.drop(list(features_to_drop['features'].values), axis=1)\n    except:\n        pass\n    else:\n        pass\n    return X\n\ndef custom_y_transformer(y):\n    #Map diagnosis to numerical values\n    y = y.map({'M':1, 'B':0})\n    y = y.dropna()\n    \n    return y","metadata":{"_uuid":"a604a079-a8bb-4bb5-9569-a28b652586f2","_cell_guid":"8d55470c-26b2-44e9-b68f-f49fa7cd6b85","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tr = custom_X_transformer(X_train)\nX_test_tr = custom_X_transformer(X_test)\n\ny_train_tr = custom_y_transformer(y_train)\ny_test_tr = custom_y_transformer(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Model Training'>\n4. Model Training\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nSo, on to the models. In a real world scenario, we do not want to mis-diagnose someone who is later disgnosed with breast cancer. In the hyperparameter tuning section, I will attempt to minimize the number of false negatives and increase the **sensitivity** of the trained models below. In short, the models will predict a malignant outcome more often than the training data indicates. However, only <code>LogisticRegression()</code>, <code>RandomForestClassifier()</code> and <code>SVC()</code> algorithms support <code>class_weight</code> as a hyperparameter. The rest of the algorithms will be trained without class weight tuning.\n\nI have chosen a handfull of classifier models to train, keeping the list short to conserve running time. In particular, I chose two boosting algorithms to see if they would be an improvement over other stacked models or deep learning models.\n\n* Define LogisticRegression, RandomForestClassifier, SVC, KNeighborsClassifier AdaBoostClassifier and GradientBoostingClassifier pipelines\n* Scale the data using PowerTransformer\n* Review their cross validated accuracy scores with default hyperparameters\n* Tune hyperparameters of each model \n* Stack models with default hyperparameters\n* Define and train a multi-layer-perceptron model on the data\n* Evaluate the best of these models on the test set","metadata":{}},{"cell_type":"code","source":"np.seterr(divide = 'ignore')    #Using powerTransformer gave a 'Divide by zero' error, as a naive workaround I just removed the error and still got good results.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set the rate of positive diagnoses to be higher than benign diagnoses, to decrease the number of false-positives\n#Default weights would be ~ 0:3.7, 1:6.3\nclass_weights = {0:1, 1:30}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n#Boosting algorithms\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ndef get_models():\n    models = {\n        'LogisticRegression': LogisticRegression(class_weight = class_weights),\n        'RandomForestClassifier': RandomForestClassifier(class_weight = class_weights),\n        'SVC': SVC(class_weight = class_weights),\n        'KNeighborsClassifier': KNeighborsClassifier(),\n        'AdaBoostClassifier': AdaBoostClassifier(),\n        'GradientBoostingClassifier': GradientBoostingClassifier()\n    }\n    \n    return models, class_weights","metadata":{"_uuid":"04f0ca2e-c9fb-4f6a-aaff-62b88437958a","_cell_guid":"74771ce4-35cf-422e-aea5-89fe16c90a00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I decided not to use Principal Component Analysis in my pipelines as its use had a negligible effect on the final models.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.decomposition import PCA\n\ndef get_pipelines():\n    models = get_models()[0].values()\n    model_names = get_models()[0].keys()\n    pipe_dict = {}\n    for model, name in zip(models, model_names):\n        pipe_dict[name] = Pipeline([\n                                    ('scaler', PowerTransformer()),\n                                    ('classifier', model) \n                                    ])\n    return pipe_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Training Base Models'>\n4.1 Training Base Models\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\ndef train_base_models(X_train, y_train):\n    models = get_models()[0]    #Gets only the models from get_models, not the class_weights.\n    pipelines = list(get_pipelines().values())    \n    names = list(models.keys())\n    for pipe, name in zip(pipelines, names):\n        pipe.fit(X_train, y_train)\n        score = cross_val_score(pipe, X_train, y_train, cv = 3)    \n        \n        print( 'Accuracy scores for {}: {}'.format(name, score) )\n        print( 'Average accuracy score for {}: {}'.format(name, sum(score)/3) )\n        print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base_models(X_train_tr, y_train_tr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Hyperparameter Tuning Using BayesSearchCV'>\n4.2 Hyperparameter Tuning Using BayesSearchCV\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nAfter training the baseline models it's time to improve their accuracy with hyperperameter tuning. I did not use RandomSearchCV here as, while being less time consuming to run, the results were often poorer than those of the base models. GridSearch in this case simply would have taken too much time to run. Instead, I used BayesSearchCV as it is more time efficient and gave better accuracy for the tuned models. Here I use skopt but hyperopt is also a good alternative library. After playing around with the hyperparameters and training the models, I found \n\n","metadata":{}},{"cell_type":"code","source":"from skopt.space import Real, Integer, Categorical\nfrom skopt.utils import use_named_args\n\n#0: benign, 1:malignant\n#Making each algorithm predict a malignant diagnosis 10x more than a benign one. Reduces the number of false-negatives.\n\nlr_search = {\n    'classifier__C': Real(1e-2, 1e2),\n    'classifier__max_iter': Integer(50, 10000),\n    'classifier__solver': Categorical(['lbfgs', 'liblinear'])\n}\nrf_search = {\n    'classifier__n_estimators': Integer(50, 300),\n    'classifier__min_impurity_decrease': Real(1e-2, 1e-1),\n    'classifier__max_depth': Real(1e-1, 1e2),\n    'classifier__min_samples_split': Real(1e-6, 1),\n    'classifier__min_samples_leaf': Real(1e-3, 0.5)\n}\nsvc_search = {\n    'classifier__C': Real(1e-2, 1e3),\n    'classifier__kernel': Categorical(['linear', 'rbf']),\n    'classifier__gamma': Categorical(['scale', 'auto'])\n}\nknn_search = {\n    'classifier__n_neighbors': Integer(1, 50),\n    'classifier__leaf_size': Integer(1, 100),\n    'classifier__p': Categorical([1,2])\n}\nadaboost_search = {\n    'classifier__n_estimators': Integer(1, 200),\n    'classifier__learning_rate': Real(1e-2, 1e-1)\n}\ngradboost_search = {\n    'classifier__n_estimators': Integer(1, 200),\n    'classifier__loss': Categorical(['deviance', 'exponential']),\n    'classifier__learning_rate': Real(1e-3, 1e-1),\n    'classifier__min_samples_split': Integer(2, 10),\n    'classifier__min_samples_leaf': Real(1e-3, 0.5)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skopt import BayesSearchCV\n\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n@ignore_warnings(category = ConvergenceWarning)\ndef bayes_param_search(X_train, y_train):\n    models = get_models()[0]\n    pipelines = list(get_pipelines().values())\n    searches = [lr_search, rf_search, svc_search, knn_search, adaboost_search, gradboost_search]\n    best_params = []    #stores the best_params_ of every model pipeline\n    for i in range( 0, len(searches) ):\n        opt = BayesSearchCV(\n            pipelines[i],\n            searches[i],\n            cv=3,    \n            n_iter=100,    #number of settings that are tried, i found that any more than this was unnecessary.\n            random_state=2021\n        )\n        opt.fit(X_train, y_train)\n        best_params.append(opt.best_params_)\n\n        print( 'valid score: {}'.format(opt.best_score_) )\n        print( 'best params: {}'.format(str(opt.best_params_)) )\n        print('')\n        \n    return best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@ignore_warnings(category = ConvergenceWarning)\ndef train_opt_models(X_train, y_train, X_test, y_test):\n    opt_models = []\n    best_params_list = bayes_param_search(X_train, y_train)\n    models = get_models()[0]\n    pipelines = list(get_pipelines().values())   #retreives all of the pipelines which are included in the bayes search\n    names = list(models.keys())\n    for name, pipe, params in zip(names, pipelines, best_params_list):\n        pipe = pipe.set_params(**params)\n        pipe.fit(X_train, y_train)\n        \n        #scoring opt model        \n        train_score = cross_val_score(pipe, X_train, y_train, cv = 3)\n        test_score = pipe.score(X_test, y_test)\n\n        print( 'Accuracy scores for {}: {}'.format(str(name), train_score) )\n        print( 'Average accuracy score for {}: {}'.format(str(name), sum(train_score)/3) )\n        print( 'Testing accuracy score: {}'.format(test_score) )\n        print('')\n\n        #store model for later use if needed\n        opt_models.append(pipe)\n        \n    return opt_models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_opt_models(X_train_tr, y_train_tr, X_test_tr, y_test_tr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='Stacking Base Models'>\n4.3 Stacking Base Models\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nThis next function takes the pipelines from <code>get_pipelines()</code> and stacks the following baseline models:\n\n* AdaBoostClassifier\n* Random forest classifier\n\nand, \n\n* SVC\n* AdaBoostClassifier\n\nAfter testing different combinations of models, with class weights, without class weight, using different final estimators (i.e. the model used to combine all of the estimators), I quickly realised that trying all possible combinations of model stacking would be very cumbersome. So, in the end I chose some 'good' performing model stacks and trained them with a RandomForest model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import make_pipeline\n\n@ignore_warnings(category = ConvergenceWarning)\ndef train_model_stacker(X_train, y_train, X_test, y_test):\n    stacked_models = []\n    class_weights = get_models()[1]\n    stacker_names = ['estimator0', 'estimator1']\n    estimators0 = [\n        ('AdaBoostClassifier', AdaBoostClassifier()),\n        ('RandomForestClassifier', make_pipeline( PowerTransformer(), RandomForestClassifier(class_weight = class_weights) ))]\n    estimators1 = [\n        ('SVC', SVC(class_weight = class_weights)),\n        ('AdaBoostClassifier', make_pipeline( PowerTransformer(), AdaBoostClassifier() ))\n    ]\n    estimators_list = [estimators0, estimators1]\n    \n    for estimator, name in zip(estimators_list, stacker_names):\n        stacker = StackingClassifier(\n                                    estimators = estimator,\n                                    final_estimator =  AdaBoostClassifier()\n                                    )\n        stacker.fit(X_train_tr, y_train_tr)\n        \n        #train score\n        train_score = cross_val_score(stacker, X_train, y_train, cv = 3)\n        print( 'Training accuracy scores for {}: {}'.format(name, train_score) )\n        print( 'Average training accuracy score for {}: {}'.format(name, sum(train_score)/3) )\n        \n        #test score\n        test_score = stacker.score(X_test, y_test)\n        print( 'Testing accuracy scores for {}: {}'.format(name, test_score) )\n        print('')\n        \n        #store model for later use if needed\n        stacked_models.append(stacker)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model_stacker(X_train_tr, y_train_tr, X_test_tr, y_test_tr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id='MLP Model'>\n4.4 MLP Model\n<a class=\"anchor-link\" href='https://www.kaggle.com/mr11235/breast-cancer-wisconsin-classification'>¶</a>\n</h1>\n\nThe final model is a simple, deep learning classifier model. Again, I tweaked its imputs to improve performance:\n\nLayers:\n* <code>units</code> (number of neurons in each layer) are set to (12, 8, 1). Optimal units for the best accuracy took some tinkering with this model. \n* first layer <code>input_dim</code> set to dimension of training set, as is the norm, output node is the 'classifier' node (outputs 0 or 1)\n* <code>kernel_initializer</code> controls the weights for each layer. 'uniform' generates weights based on a normal distribution.\n\nfitting model:\n* <code>epochs</code> 'pass throughs' of training data through the model. after tinkering with this value, I found above 200 to have no impact.","metadata":{}},{"cell_type":"code","source":"#Set the rate of positive diagnoses to be higher than benign diagnoses, to decrease the number of false-positives\nclass_weight = {0:1 , 1:30}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\ndef train_MLP_model(X_train, y_train, X_test, y_test):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=2021)\n    \n    model = Sequential()\n    model.add(Dense(units = 12,\n                    input_dim = len(X_train.columns),\n                    kernel_initializer = 'random_uniform',\n                    activation ='relu'\n                   )\n             )\n    model.add(Dense(units = 8,\n                    kernel_initializer = 'random_uniform',\n                    activation ='relu'\n                   )\n             )\n    model.add(Dense(1,\n                    kernel_initializer = 'random_uniform',\n                    activation ='sigmoid'\n                   )\n             )\n    model.compile(\n        optimizer = 'adam', \n        loss = 'binary_crossentropy', \n        metrics = ['accuracy']\n    )\n    history = model.fit(\n                X_train,\n                y_train,\n                batch_size = 16,\n                epochs = 100,\n                verbose = 1,\n                class_weight = class_weight\n            )\n    \n    #train score\n    train_score = model.evaluate(X_valid, y_valid)\n    print( 'Training accuracy score : {}'.format(train_score[1]) )\n    \n    #test score\n    test_score = model.evaluate(X_test, y_test)\n    print( 'Test accuracy score : {}'.format(test_score[1]) )\n    \n    #use for final predictions\n    preds = model.predict(X_test)\n    \n    return (model, train_score, test_score, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scaling X data\ntransformer = PowerTransformer()\nscaled_X_train = pd.DataFrame(transformer.fit_transform(X_train_tr))\nscaled_X_test = pd.DataFrame(transformer.fit_transform(X_test_tr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing MLP model\nMLP_model = train_MLP_model(scaled_X_train, y_train_tr, scaled_X_test, y_test_tr)\nMLP_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [1 * (x[0]>=0.5) for x in MLP_model[3]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ntest_confusion_matrix = confusion_matrix(y_test_tr, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}