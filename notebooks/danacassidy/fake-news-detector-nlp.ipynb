{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# \"Fake\" vs. \"Real\" news: NLP","metadata":{}},{"cell_type":"markdown","source":"The topic of \"fake\" versus \"real\" news is one that's become more pressing as social media continues to evolve and information is more rapidly spread. The concept of news itself is not new and is a fundamental aspect of modern-day democracies. Journalists work to hold powerful entities and figures accountable and are supposed to be an ally of the people. Yet, distrust of the media is extremely common due to the nature of present-day society and officials constantly calling the media into question. Harmful conspiracies and propaganda aren't new but now have a platform to thrive and be spread among social sites. Companies and people are able to mask \"pink slime,\" or garbage-level information, as quality journalism. This is a problem, as most people aren't entirely media literate and can't tell the difference -- but a computer can.\n\n#### The purpose of this notebook is to use Natural Language Processing to train a model to tell the difference between \"fake\" and \"real\" news.","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents","metadata":{}},{"cell_type":"markdown","source":"#### 1. Importing libraries and data \n#### 2. Data cleaning\n#### 3. Feature Extraction\n#### 4. Training the model\n#### 5. Analyzing and exploring some more\n#### 6. Conclusion + takeaways\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing libraries and  data ","metadata":{}},{"cell_type":"markdown","source":"Let's import all the libraries we'll need to import and analyze the data.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's import our csv files of both \"fake\" and \"real\" news datasets. The dataset is from: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset?select=True.csv. Then, we'll take a look at the heads of our csv files.","metadata":{}},{"cell_type":"code","source":"fake = pd.read_csv('Fake.csv', index_col=0)\nreal = pd.read_csv('True.csv', index_col=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets both contain four columns which include: the article's title, the article's body text, the subject of the news, and the date the article was published. It's a fairly straightforward dataset, which is great to work with.","metadata":{}},{"cell_type":"markdown","source":"Let's check the info, describe, and value_counts aspects of the datasets.","metadata":{}},{"cell_type":"code","source":"fake.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real.describe()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data cleaning","metadata":{}},{"cell_type":"markdown","source":"I'm going to drop the \"Subject\" columns to make it easier for our model to figure out what is real and what is fake. It'll also be ideal for me to concatenate the two types of news, but I'll go and add more labels to identify everything more clearly.","metadata":{}},{"cell_type":"code","source":"real['label'] ='real'\nfake['label'] = 'fake'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_data = pd.concat([fake,real],axis=0)\nnews_data = news_data.sample(frac=1).reset_index(drop=True)\nnews_data.drop('subject',axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's split up our dataset into test vs. training!","metadata":{}},{"cell_type":"markdown","source":"First, I'll import train_test_split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll run the train_test_split and check the heads of our training data as well as the length of X_train","metadata":{}},{"cell_type":"code","source":"X = news_data['text']\ny = news_data['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Feature ExtractionÂ¶","metadata":{}},{"cell_type":"markdown","source":"I will be using tf-idf term weighting as the feature to extract from the texts.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_tfidf = TfidfVectorizer(stop_words='english',max_df=0.8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's fit the vectorizer and then transform X_train into a tf-idf matrix. \nThen, we will use that same vectorizer to transform the X_test","metadata":{}},{"cell_type":"code","source":"tfidf_train = my_tfidf.fit_transform(X_train)\ntfidf_test = my_tfidf.transform(X_test)\n\ntfidf_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Training the model","metadata":{}},{"cell_type":"markdown","source":"I will be using PassiveAggressiveClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pa_clf = PassiveAggressiveClassifier(max_iter=50)\npa_clf.fit(tfidf_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the same algorithm to the test dataset to see how well it performs.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom mlxtend.plotting import plot_confusion_matrix\n\n\ny_pred = pa_clf.predict(tfidf_test)\n\nconf_mat = confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(conf_mat,\n                      show_normed=True, colorbar=True,\n                      class_names=['Fake', 'Real'])\naccscore = accuracy_score(y_test, y_pred)\nf1score = f1_score(y_test,y_pred,pos_label='real')\n\nprint('The accuracy of prediction is {:.2f}%.\\n'.format(accscore*100))\nprint('The F1 score is {:.3f}.\\n'.format(f1score))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model does a great job at predicting if news is fake, with a 99.34% level of accuracy. The F1 score is also extremely high, as well, with a score of 0.993.","metadata":{}},{"cell_type":"markdown","source":"## 5. Analyzing and exploring some more","metadata":{}},{"cell_type":"markdown","source":"I'm going to explore: ","metadata":{}},{"cell_type":"markdown","source":" - What criteria the model learned to make it incredibly accurate","metadata":{}},{"cell_type":"markdown","source":"- If this model can be applied well to other articles that aren't in the data set or if this dataset had a particular characertistic that made it stand out","metadata":{}},{"cell_type":"markdown","source":"To begin with, let's see what the model's criteria was.","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.extmath import density\nfrom sklearn.pipeline import make_pipeline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Dimensionality (i.e., number of features): {:d}\".format(pa_clf.coef_.shape[1]))\nprint(\"Density (i.e., fraction of non-zero elements): {:.3f}\".format(density(pa_clf.coef_)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The algorithm found that less than half of the features were not useful in determining whether or not an article is real. But let's examine the other features:","metadata":{}},{"cell_type":"markdown","source":"Non-zero weight sorting:","metadata":{}},{"cell_type":"code","source":"weights_nonzero = pa_clf.coef_[pa_clf.coef_!=0]\nfeature_sorter_nonzero = np.argsort(weights_nonzero)\nweights_nonzero_sorted =weights_nonzero[feature_sorter_nonzero]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(9,3))\nsns.lineplot(data=weights_nonzero_sorted, ax=axs[0])\naxs[0].set_ylabel('Weight')\naxs[0].set_xlabel('Feature number \\n (Zero-weight omitted)')\n\naxs[1].hist(weights_nonzero_sorted,\n            orientation='horizontal', bins=500,)\naxs[1].set_xlabel('Count')\n\nfig.suptitle('Weight distribution in features with non-zero weights')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like even with the features that have non-zero weights, a lot of them have a value close to zero. This isn't shocking, as there were almost one-hundred thousand tokens, so most of them were probably useless for the task at hand.","metadata":{}},{"cell_type":"markdown","source":"But what tokens were  useful?","metadata":{}},{"cell_type":"markdown","source":"### Let's extract \"Indicator\" tokens","metadata":{}},{"cell_type":"code","source":"tokens = my_tfidf.get_feature_names()\ntokens_nonzero = np.array(tokens)[pa_clf.coef_[0]!=0]\ntokens_nonzero_sorted = np.array(tokens_nonzero)[feature_sorter_nonzero]\n\nnum_tokens = 10\nfake_indicator_tokens = tokens_nonzero_sorted[:num_tokens]\nreal_indicator_tokens = np.flip(tokens_nonzero_sorted[-num_tokens:])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_indicator = pd.DataFrame({\n    'Token': fake_indicator_tokens,\n    'Weight': weights_nonzero_sorted[:num_tokens]\n})\n\nreal_indicator = pd.DataFrame({\n    'Token': real_indicator_tokens,\n    'Weight': np.flip(weights_nonzero_sorted[-num_tokens:])\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The top {} tokens likely to appear in fake news were the following: \\n'.format(num_tokens))\ndisplay(fake_indicator)\n\nprint('\\n\\n...and the top {} tokens likely to appear in real news were the following: \\n'.format(num_tokens))\ndisplay(real_indicator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_contain_fake = fake.text.loc[[np.any([token in body for token in fake_indicator.Token])\n                                for body in fake.text.str.lower()]]\nreal_contain_real = real.text.loc[[np.any([token in body for token in real_indicator.Token])\n                                for body in real.text.str.lower()]]\n\nprint('Articles that contained any of the matching indicator tokens:\\n')\n\nprint('FAKE: {} out of {} ({:.2f}%)'\n      .format(len(fake_contain_fake), len(fake), len(fake_contain_fake)/len(fake) * 100))\nprint(fake_contain_fake)\n\nprint('\\nREAL: {} out of {} ({:.2f}%)'\n      .format(len(real_contain_real), len(real), len(real_contain_real)/len(real) * 100))\nprint(real_contain_real)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some noticable points:","metadata":{}},{"cell_type":"markdown","source":"- Fake news tens to use Getty Images, most likely because a lot of fake articles aren't necessarily done by actual journalists, which means they need to find photos elsewhere.\n\n- Weekdays are often included in real news, like \"Tuesday\",\"Wednesday\",etc. because AP style prefers articles to state the day it took place if it happened that week, or the actual date of the event if it was prior.\n\n- The categories went beyond politics, but many indicator terms seemed relevant to U.S. Politics. This includes terms like \"gop\", \"sen\", \"republican\", and more.\n\n- \"gop\" is often used more in fake news that real news, while \"republican\" is more often used in real news. This is likely because AP style tells journalists to refer to the party as \"Republicans.\"","metadata":{}},{"cell_type":"markdown","source":"### Other questions:","metadata":{}},{"cell_type":"markdown","source":"- Why are \"read\" and \"featured\" the top two fake-news indicator tokens? Is it because an author was trying to claim that the story is real because it's been read a lot and featured elsewhere?\n\n- The same question goes for \"nov\" and \"washington\", which perhaps infers that a lot of fake articles came around election time in November and discussed the month and the capitol a lot.\n\n- It is clear that Reuters is reputable, but a lot of articles begin with a \"City Name (Reuters)\" which the algorithim must have identified as real. I wonder if the algorithm could still tell if an article is real if this identifier was removed.","metadata":{}},{"cell_type":"markdown","source":"These are all speculations, but it would be interesting to see how these terms are actually used within the test. But that is beyond the scope of this project at hand.","metadata":{}},{"cell_type":"markdown","source":"## 6. Conclusion + takeaways","metadata":{}},{"cell_type":"markdown","source":"I used the TfidVectorizer and PassiveAggressiveClassifier algorithms to find \"fake news\" within the dataset. It was extremely accurate and able to identify the \"fake news\" at a consistently high rate with a high f1 score.","metadata":{}}]}