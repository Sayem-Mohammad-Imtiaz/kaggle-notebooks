{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom time import time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/column_2C_weka.csv')\n\ndata.info()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno\nmissingno.matrix(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(16, 10))\ncorr = data.corr()\ncorr_mtx = sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=False, ax=ax, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import mean\n\nn_row = 3\nn_col = 2\nfeature_variables = list(data.columns)[0:-1]\ntarget_variable = list(data.columns)[-1]\nf, axes = plt.subplots(n_row, n_col, figsize=(16, 12))\nk = 0\n\nfor i in list(range(n_row)):\n    for j in list(range(n_col)):\n        sns.barplot(x = feature_variables[k], y = target_variable, data = data, estimator = mean, color = 'lightblue', ax=axes[i, j])\n        k = k + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n\nscaler = MinMaxScaler()\nfeatures_raw = data.drop(columns=['class'])\ntarget_raw = data['class']\n\ntypes_aux = pd.DataFrame(features_raw.dtypes)\ntypes_aux.reset_index(level=0, inplace=True)\ntypes_aux.columns = ['Variable','Type']\nnumerical = list(types_aux[types_aux['Type'] == 'float64']['Variable'].values)\n\nfeatures_minmax_transform = pd.DataFrame(data = features_raw)\nfeatures_minmax_transform[numerical] = scaler.fit_transform(features_raw[numerical])\n\nfeatures_minmax_transform.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\n\nlb.fit(target_raw)\ntarget = lb.transform(target_raw)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport os\n\nX = features_minmax_transform\ny = target\n\nclf_a = MultinomialNB()\nclf_b = DecisionTreeClassifier(random_state = 0)\nclf_c = RandomForestClassifier(random_state = 0)\nclf_d = LogisticRegression(random_state = 0)\nclf_e = SGDClassifier(random_state = 0)\nclf_f = KNeighborsClassifier()\n\nlist_clf = [clf_a, clf_b, clf_c, clf_d, clf_e, clf_f]\n\nresults = []\nfor clf in list_clf:\n    start = time()\n    clf_name = clf.__class__.__name__\n    cv = 5\n    scoring = 'f1'\n\n    scores_f1 = cross_val_score(clf, X, y, cv=cv, scoring = scoring)\n    scores_ = cross_val_score(clf, X, y, cv=cv)\n    end = time()\n    train_time = end  - start\n    results.append([clf_name, np.mean(scores_f1), np.mean(scores_), train_time])\n\n\ndf_results = pd.DataFrame(np.array(results))\ndf_results.columns = ['Classifier', 'F1-Score', 'Accuracy', 'Train Time']\ndf_results.sort_values(by=['F1-Score'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nfor i in range(10, 300):\n    rfc = RandomForestClassifier(random_state = 0, n_estimators=i)\n    start = time()\n    cv = 5\n    scoring = 'f1'\n\n    scores_f1 = cross_val_score(rfc, X, y, cv=cv, scoring = scoring)\n    scores_ = cross_val_score(rfc, X, y, cv=cv)\n    end = time()\n    train_time = end  - start\n    results.append(['RandomForestClassifier', np.mean(scores_f1), np.mean(scores_), train_time, i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame(np.array(results))\ndf_results.columns = ['Classifier', 'F1-Score', 'Accuracy', 'Train Time', 'n_estimators']\nfig, ax1 = plt.subplots(figsize=(16, 10))\n\ncolor = 'tab:red'\nax1.set_xlabel('n_estimators')\nax1.set_ylabel('Accuracy', color=color)\nax1.plot(range(10,300), df_results['Accuracy'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = 'tab:blue'\nax2.set_ylabel('F1-Score', color=color)  # we already handled the x-label with ax1\nax2.plot(range(10,300), df_results['F1-Score'], color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results[\"Accuracy\"] = df_results.Accuracy.astype(float)\ndf_results.loc[df_results['Accuracy'].idxmax()]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}