{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\n* In this study, we will use logistic regression and KNN, SVM, Navie Bayes, Decision Tree algorithms for machine learning."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* First we need to see our datas and understand them, we will use colum_2C_weka data for it"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* we dont need to understand very well our datas for using them in logistic regression and KNN. These methods helps us to predict values which can give results as 0-1 or in our example Abnormal-Normal\n* for using logistic regression and KNN, we need to make class type: object to > int or float"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename({'pelvic_tilt numeric':'pelvic_tilt_numeric'}, axis = 1)\ndata = data.rename({'class':'classification'}, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"classification\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets visualize our datas first\nA = data[data.classification == \"Abnormal\"]\nN = data[data.classification == \"Normal\"]\nplt.scatter(A.pelvic_incidence, A.pelvic_tilt_numeric)\nplt.scatter(N.pelvic_incidence, N.pelvic_tilt_numeric)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"pelvic_tilt_numeric\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A.lumbar_lordosis_angle, A.sacral_slope)\nplt.scatter(N.lumbar_lordosis_angle, N.sacral_slope)\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"sacral_slope\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"classification\", data=data)\ndata.loc[:,'classification'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in these graphics, we see the differences with normal and abnormal datas in different features, we can make it expand and look for all features too, but we will see how we will use log regression and KNN methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"#we need to make our class values object to int \ndata.classification = [1 if each == \"Normal\" else 0 for each in data.classification]\ndata.classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#first we need to split our datas for train and test\ny = data.classification.values\nx_data = data.drop([\"classification\"], axis=1)\nx_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will normalize our x_data values because some values are small, some values are big, thats why we need to make it all between 0-1 for having a meaningful data\nx = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data))\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will split our datas because some datas we will use for train and some we will use for test\n#we use test_size = 0.3 because %30 datas will be use for test and random_state is for we will train our datas in similar way otherwise after train there doesnt have a close results\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets use log regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\nlr.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets use KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train, y_train)\nknn.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(x_train, y_train)\nknn.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=20)\nknn.fit(x_train, y_train)\nknn.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=100)\nknn.fit(x_train, y_train)\nknn.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we see when neighbour value is raising, score of KNN is changing, lets find best k value"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_list = []\nfor each in range(1,100):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\n    \nplt.plot(range(1,100), score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we see that KNN value = 21 around is best score we have"},{"metadata":{"trusted":true},"cell_type":"code","source":"neig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets use SVM algoritm\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\nsvm.fit(x_train, y_train)\nsvm.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets use SVM algoritm\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nnb.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets use decision tree algoritm\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ndt.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets use random forest algoritm\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\nrf.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100)\nrf.fit(x_train, y_train)\nrf.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=50)\nrf.fit(x_train, y_train)\nrf.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we see that changing n_estimators changing the score. N_estimators means how many decision trees we will use in our algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will use confusion matrix for finding which datas we predicted were true and false, for this we will use random forest classifier\nrf = RandomForestClassifier(n_estimators=50)\nrf.fit(x_train, y_train)\nrf.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets visualize confussion matrix\n\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, linewidths = 0.5, linecolor = \"red\", fmt = \".0f\", ax=ax)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we see that before as we equaled abnormal values = 0, normal values = 1. Now we see that our 0(abnormal values) 59 of them were predicted truely, 7 were predicted falsely and 1(normal values) were predicted falsely, 20 were predicted truely"},{"metadata":{},"cell_type":"markdown","source":"## CONCLUSION\n* As we can see for this data KNN score is : 0.82, Log regression score is : 0.74, SVM score is : 0.80, Naive Bayes : 0.817, Decision Tree : 0.75, Random Forest : 0.849. Random Forest score is better"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}