{"cells":[{"metadata":{"id":"Ok4SRQl6MyPG"},"cell_type":"markdown","source":"# Introducing Random Forest\nA Random Forest ðŸŒ²ðŸŒ²ðŸŒ² is actually just a bunch of Decision Trees ðŸŒ² bundled together (ohhhhh thatâ€™s why itâ€™s called a forest). In this notebook we will learn how to build Random Forest Model.\n\n![Random Forest](https://www.frontiersin.org/files/MyHome%20Article%20Library/284242/284242_Thumb_400.jpg)\n\n## Agenda\n*  About Dataset\n*  Loading Libraries\n*  Loading Data\n*  Understanding Data\n*  Separating Input Features and Ouput/Target Features\n*  Splitting Data into Train and Test Sets.\n*  Build Model\n*  Prediction\n*  Check Model Performance","execution_count":null},{"metadata":{"id":"wRF6qujhPKTS"},"cell_type":"markdown","source":"## About Dataset\nI hope all of you guys remembered the wine dataset on which we have done exploratory data analysis and also build logistic regression model for this dataset. Here we will take red wine data. Given different physiochemical tests, we want to predict the quality of wine in range 1 to 10.\n\nThe reason behind taking the same dataset is that we can easily notice the differences between Logistic Regression and Random Forest models.","execution_count":null},{"metadata":{"id":"sF4d3sdyU8oO"},"cell_type":"markdown","source":"## Loading Libraries\nAll Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n\nIn data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).","execution_count":null},{"metadata":{"id":"FwAkpNo1Mrrm","trusted":false},"cell_type":"code","source":"import numpy as np        # Fundamental package for linear algebra and multidimensional arrays\nimport pandas as pd       # Data analysis and manipultion tool","execution_count":null,"outputs":[]},{"metadata":{"id":"XY-9mk_iVFCb"},"cell_type":"markdown","source":"## Loading Data\nPandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data.","execution_count":null},{"metadata":{"id":"mam10VpCVCTr","trusted":false},"cell_type":"code","source":"# In read_csv() function, we have passed the location to where the files are located in the UCI website. The data is separated by ';'\n# so we used separator as ';' (sep = \";\")\nred_wine_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"id":"moAPkvs5VOsK"},"cell_type":"markdown","source":"## Understanding Data\nLet's see how our data looks.\n\nOne can explore my another notebook which is especially focused on understanding the data and getting insights from it (in short Exploratory Data Analysis) using the same dataset. It will help you correlate the works done in both the notebooks. \n\nLink to the notebook: https://www.kaggle.com/manishkc06/eda-an-introduction","execution_count":null},{"metadata":{"id":"EErcLHN6VL5V","outputId":"e50267d6-75c2-4bd9-e2d3-227486c6f9b0","trusted":false},"cell_type":"code","source":"# Red Wine\nred_wine_data.head() ","execution_count":null,"outputs":[]},{"metadata":{"id":"5UrCf3jiVSOH","outputId":"1d032d1d-494d-4dc6-d8b0-0058253e622e","trusted":false},"cell_type":"code","source":"red_wine_data.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"WVnbfl6bVYUy"},"cell_type":"markdown","source":"### Different attributes\n**Input variables (based on physicochemical tests):**\n\n*  fixed acidity\n*  volatile acidity\n*  citric acid\n*  residual sugar\n*  chlorides\n*  free sulfur dioxide\n*  total sulfur dioxide\n*  density\n*  pH\n*  sulphates\n*  alcohol\n\n**Output variable (based on sensory data):**\n\n*  quality (score between 0 and 10)","execution_count":null},{"metadata":{"id":"GLRJniSeVUz6","outputId":"5259a3d7-7fb5-41b3-c00f-a7b6c0fe96e0","trusted":false},"cell_type":"code","source":"# Basic statistical details about data\nred_wine_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"2HWRT4OsWEc2"},"cell_type":"markdown","source":"Let's see target variable 'quality'.","execution_count":null},{"metadata":{"id":"rWP_HIuTWBHG","outputId":"713c9c5b-135b-4453-dd6d-f18ba18322e2","trusted":false},"cell_type":"code","source":"red_wine_data.quality.value_counts().plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"id":"mPy8ieoUWN2i"},"cell_type":"markdown","source":"We can observe here more wines are of average quality than poor quality and good quality. This is what we had observed in our EDA notebook of wine data.","execution_count":null},{"metadata":{"id":"1svtm0XGWQyy"},"cell_type":"markdown","source":"We have already done the EDA part of this dataset in our earlier notebook. So we will not dive into EDA more here. Let's separate the independent and dependent variables.","execution_count":null},{"metadata":{"id":"0-SNJazYWTpq"},"cell_type":"markdown","source":"## Separating Input Features and Output Features\nBefore building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables. Like here in this data, we can see that whether a person will buy insurance or not is dependent on the age of that person\n\nBy convention input variables are represented with 'X' and output variables are represented with 'y'.","execution_count":null},{"metadata":{"id":"HbTWX-liWIi1","trusted":false},"cell_type":"code","source":"# Input/independent variables\nX = red_wine_data.drop('quality', axis = 1)   # her we are droping the quality feature as this is the target and 'X' is input features, the changes are not \n                                              # made inplace as we have not used 'inplace = True'\n\ny = red_wine_data.quality             # Output/Dependent variable","execution_count":null,"outputs":[]},{"metadata":{"id":"1-53TfcsWceq"},"cell_type":"markdown","source":"## Splitting the data into Train and Test Set\nWe want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n\nFor this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module.","execution_count":null},{"metadata":{"id":"h8w-mRH9WX0H","trusted":false},"cell_type":"code","source":"# import train_test_split\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"id":"luYFQCzeWgSm","trusted":false},"cell_type":"code","source":"# split the data\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42)\n\n# X_train: independent/input feature data for training the model\n# y_train: dependent/output feature data for training the model\n# X_test: independent/input feature data for testing the model; will be used to predict the output values\n# y_test: original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n \n# test_size = 0.30: 30% of the data will go for test set and 70% of the data will go for train set\n# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code","execution_count":null,"outputs":[]},{"metadata":{"id":"Z6z_C7YzWmZU"},"cell_type":"markdown","source":"## Building Model\nNow we are finally ready, and we can train the model.\n\nFirst, we need to import our model - Random Forest Classifier (again, using the sklearn library).\n\nThen we would feed the model both with the data (X_train) and the answers for that data (y_train)","execution_count":null},{"metadata":{"id":"__FN2Sf5Win9","trusted":false},"cell_type":"code","source":"# Importing RandomForestClassifier from sklearn.ensemble\n# We will be further discussing about why Random Forest is in ensemble module of sklearn library\nfrom sklearn.ensemble import RandomForestClassifier ","execution_count":null,"outputs":[]},{"metadata":{"id":"qxXkvEOxXFTI","trusted":false},"cell_type":"code","source":"rfc = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"id":"3boReK8zXJLJ","outputId":"3db109d7-d535-40f9-eba8-2cf00a5c99b8","trusted":false},"cell_type":"code","source":"rfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"nfqBh8Z5XSI8"},"cell_type":"markdown","source":"## Prediction\nNow Random Forest model (i.e. rfc) is trained using X_train and y_trian data. Let's predict the target value (i.e. quality of wine) for the X_test data. We use \"predict()\" method for prediction.","execution_count":null},{"metadata":{"id":"va_0XIeEXL4W","trusted":false},"cell_type":"code","source":"predictions = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"a7hemiKeXr-6"},"cell_type":"markdown","source":"We already have actual target values (i.e. y_test) for X_test. Let's compare y_test and the predicted value for X_test by our log_model.","execution_count":null},{"metadata":{"id":"wc1IvVhqXnsd","outputId":"4ec1ec15-7f9e-4631-adf6-614c4cab796c","trusted":false},"cell_type":"code","source":"y_test.values","execution_count":null,"outputs":[]},{"metadata":{"id":"pcMNtgLvXutN","outputId":"70a2f7b6-3327-4adc-c56f-80031970939a","trusted":false},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"id":"Nu7AQK0QX3Wm"},"cell_type":"markdown","source":"## Model Performance\nWe can also check how accurate our model is performing using the 'accuracy_score' class from 'sklearn.metrics'.","execution_count":null},{"metadata":{"id":"M9DL66-e5e20","outputId":"64c6393a-5054-4baa-aee1-f067e17549a0","trusted":false},"cell_type":"code","source":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{"id":"aInxzc5r5je4"},"cell_type":"markdown","source":"If you observe here, the class wise false positives (above the main diagonal) and the class wise false negatives (below the main diagonal) are almost symmetrical. So, the accuracy score is an important metric here.","execution_count":null},{"metadata":{"id":"DSTWZ5e9XzzD","trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"id":"ZrA4G3InX7yH","outputId":"cc3864a8-197c-4b02-d2d6-f1e0b4ee7560","trusted":false},"cell_type":"code","source":"accuracy_score(y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{"id":"0EHiAfIgX-1K"},"cell_type":"markdown","source":"You can observe that the accuracy is improved with Random Forest Model. Logistic Regression Model gave 54% of accuracy and Random Forest is giving 66.8% of accuracy on the same dataset.","execution_count":null},{"metadata":{"id":"yKppyoOpYTzi"},"cell_type":"markdown","source":"**Thanks for reading the Notebook!!!**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}