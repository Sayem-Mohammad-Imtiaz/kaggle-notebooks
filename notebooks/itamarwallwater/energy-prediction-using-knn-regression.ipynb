{"cells":[{"metadata":{"id":"zADo2v5OYPHq"},"cell_type":"markdown","source":"**Kaggle link**\n\n[Dataset on kaggle](https://www.kaggle.com/loveall/appliances-energy-prediction)\n\n**Context**\n\nExperimental data used to create regression models of appliances energy use in a low energy building.\n\n**Content**\n\nThe data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters).","execution_count":null},{"metadata":{"id":"yQMws2-HYLxu"},"cell_type":"markdown","source":"**T dew point**\n\nIf you are interested in a calculation that gives an approximation of dew point temperature if you know the observed temperature and relative humidity, the following formula was proposed in a 2005 article by Mark G. Lawrence in the Bulletin of the American Meteorological Society:\n\nTd = T - ((100 - RH)/5.)\n\nwhere Td is dew point temperature (in degrees Celsius), T is observed temperature (in degrees Celsius), and RH is relative humidity (in percent). Apparently this relationship is fairly accurate for relative humidity values above 50%.","execution_count":null},{"metadata":{"id":"_SDOIjWARofn"},"cell_type":"markdown","source":"# Import and Load data","execution_count":null},{"metadata":{"id":"Gf6xyjHiYSUX","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom datetime import datetime\nfrom enum import Enum\nfrom itertools import combinations, product\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, make_scorer\n\nimport os\n\nFIGURE_SIZE = (15, 12)\nSNS_FIGURE_SIZE = (20, 15)\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '../input/appliances-energy-prediction/KAG_energydata_complete.csv'\nloaded_df = pd.read_csv(file_path, parse_dates=['date'])\n\nfeatures_df = loaded_df.drop(['rv1', 'rv2','lights', 'Appliances'], axis = 1)\ntarget_column = loaded_df['Appliances'] + loaded_df['lights'] # Both represent energy use in Wh","execution_count":null,"outputs":[]},{"metadata":{"id":"ryeLYBNj8N0r"},"cell_type":"markdown","source":"# View data","execution_count":null},{"metadata":{"id":"aLbaQIeORctn"},"cell_type":"markdown","source":"First lets take a look at the shape of our dataframe, to see how many potential features we have and the size of our infomation (observations)","execution_count":null},{"metadata":{"id":"XgoU_ze9RKxe","outputId":"7f149ed1-2f0a-4a2a-d411-fe595b2405bb","trusted":true},"cell_type":"code","source":"features_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"pcQ3Xb6-SAi1"},"cell_type":"markdown","source":"So we have 19735 row, meaning that we have enough information for our regression\n\nBut we have 25 potential features! that could easily cause our model to be overfitted - we'll have to reduce this number","execution_count":null},{"metadata":{"id":"9-cgFV__Sulx"},"cell_type":"markdown","source":"### Missing values","execution_count":null},{"metadata":{"id":"V-FMB4dHS8K3"},"cell_type":"markdown","source":"Lets check for missing values in our dataframe","execution_count":null},{"metadata":{"id":"OMB6uotiTKDf","outputId":"d6b56716-31a9-4fbd-a0af-4bbe28698b51","trusted":true},"cell_type":"code","source":"loaded_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"TZSV7w5eTRe3"},"cell_type":"markdown","source":"No missing values!","execution_count":null},{"metadata":{"id":"EY14HCsgjfNw"},"cell_type":"markdown","source":"### Correlation map","execution_count":null},{"metadata":{"id":"9vSFKy2qTXlY"},"cell_type":"markdown","source":"Next we'll check for correlation between our features, as our current goal is reduce the number of features","execution_count":null},{"metadata":{"id":"H8xwQD-XjaoY","outputId":"edc7920b-f563-44be-e045-6cc5e9b7fcfb","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=SNS_FIGURE_SIZE)\nmask_matrix = np.triu(features_df.corr())\nsns.heatmap(features_df.corr(), annot = True, cmap= 'coolwarm', linewidths=3, linecolor='black', mask=mask_matrix)","execution_count":null,"outputs":[]},{"metadata":{"id":"lgXKRB8hTr1a"},"cell_type":"markdown","source":"If we set the \"correlation limit\" to 0.8, we get 4 potential groups of variables with high correlation and therefore redundent","execution_count":null},{"metadata":{"id":"y0V4k3HvlEJ3","trusted":true},"cell_type":"code","source":"correlated_column_group_1 = ['T1', 'T2', 'T3', 'T4', 'T5', 'T7', 'T8', 'T9']\ncorrelated_column_group_2 = ['RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_7']\ncorrelated_column_group_3 = ['T6', 'T_out'] # Tdewpoint could be added (correlation of 0.79)\ncorrelated_column_group_4 = ['RH_8', 'RH_9']","execution_count":null,"outputs":[]},{"metadata":{"id":"EZLyfj1fSg9L"},"cell_type":"markdown","source":"### More viewing options","execution_count":null},{"metadata":{"id":"ubFIoXob8R9E","outputId":"49d3e660-caf3-4763-f209-0dea3c0bfddf","trusted":true},"cell_type":"code","source":"features_df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"ENIGr9smQ8xa","outputId":"0b48a0cc-e20e-4a2f-dcc8-33f42b3aba75","trusted":true},"cell_type":"code","source":"features_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"ry3Pm49h8U9P","outputId":"94022b39-1817-49cf-b546-401c1b879d9f","trusted":true},"cell_type":"code","source":"features_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"jvWJLeUN8Xjv","outputId":"d01d3d12-0813-4f7d-96b5-d7c771040f90","trusted":true},"cell_type":"code","source":"features_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"cjWCNmjg8Z4E","outputId":"7b6763c2-daad-4f2b-a77c-6941ed4760b1","trusted":true},"cell_type":"code","source":"features_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"gpTVHmcvhSik"},"cell_type":"markdown","source":"# Feature enginnering","execution_count":null},{"metadata":{"id":"j6cyoqzlUiiD"},"cell_type":"markdown","source":"## Extracting time features","execution_count":null},{"metadata":{"id":"L9NEphveHyY2"},"cell_type":"markdown","source":"As we go about our exploration, we'll collect irrelevant features, meaning features that doesn't increase our information.\n\nwe'll add the column 'Date' since it can't be used in the final regression","execution_count":null},{"metadata":{"id":"X_1KEIYFHpPf","trusted":true},"cell_type":"code","source":"irrelevant_features = []\nirrelevant_features.append('date')","execution_count":null,"outputs":[]},{"metadata":{"id":"kzFscodSU2Ma"},"cell_type":"markdown","source":"In order to study our features further, we'll increase our resolution by expending the 'date' column into 3 columns:\n1. Day of week - could serve as a categorical feature if needed, since weekdays and weekends are likely to yield different energy consumption.\n2. Month - could serve as a categorical feature if needed, since different months of the year are also likely to yield different energy consumption.\n3. Hour - will be our lowest resolution for the exploration.\n\nThis feature extraction will be done using a custom function, that takes a dataframe with a column labeled 'date' and adds the needed feature we've mentioned","execution_count":null},{"metadata":{"id":"ubOQCkYM83Mz"},"cell_type":"markdown","source":"### time features extractor","execution_count":null},{"metadata":{"id":"O8iOutYhT4JB","trusted":true},"cell_type":"code","source":"def time_features_extractor (df, attr_list = ('year', 'month', 'day', 'hour', 'minute', 'second', 'dayofweek')):\n    for attr in attr_list:\n        df[attr] = df['date'].apply(lambda x: getattr(x, attr))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"JTlE0OZAPzVZ","outputId":"98d926ac-748b-49ce-88b4-36ae222ecb15","trusted":true},"cell_type":"code","source":"features_df = time_features_extractor(features_df, ('dayofweek', 'month', 'hour'))\nfeatures_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZcVEKABRY11u"},"cell_type":"markdown","source":"In order to transform the column we've created 'dayofweek' to a binary column between weekdays and weekends, we'll divide by 5 without resdiual, and add 'dayofweek' to our irrelavent features list","execution_count":null},{"metadata":{"id":"CgiB8eVRZ0Kn","trusted":true},"cell_type":"code","source":"features_df['weekday'] = features_df['dayofweek'].apply(lambda x: x//5)\nirrelevant_features.append('dayofweek')","execution_count":null,"outputs":[]},{"metadata":{"id":"wecRNPnWatBl"},"cell_type":"markdown","source":"### Creating group by objects","execution_count":null},{"metadata":{"id":"1exKYsImai8O"},"cell_type":"markdown","source":"Next, to ease up the rest of the exploration, we'll create two group by objects, one for month and one for day","execution_count":null},{"metadata":{"id":"65R-YFKFUAmZ","trusted":true},"cell_type":"code","source":"gb_month = features_df.groupby(['month'])\ngb_hour = features_df.groupby(['hour'])","execution_count":null,"outputs":[]},{"metadata":{"id":"8iXwCkQZS28L"},"cell_type":"markdown","source":"## Confirming and dealing with correlated features","execution_count":null},{"metadata":{"id":"SQ-1Ce28AIpE"},"cell_type":"markdown","source":"### correlated column group 1","execution_count":null},{"metadata":{"id":"OZ2uz6T2APhc","outputId":"7ded3276-6c23-458e-f466-c217562b9f8e","trusted":true},"cell_type":"code","source":"ccgroup1_gb_month = gb_month[correlated_column_group_1]\nccgroup1_gb_hour = gb_hour[correlated_column_group_1]\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=FIGURE_SIZE)\nccgroup1_gb_month.mean().plot(ax=ax1)\nccgroup1_gb_hour.mean().plot(ax=ax2)\n\nax1.set_ylabel('Temprature[C]')\nax2.set_ylabel('Temprature[C]')","execution_count":null,"outputs":[]},{"metadata":{"id":"KxOqUd13dDM4"},"cell_type":"markdown","source":"From this view, we can see the besides T2, most of those features shows a very similar trend and a small variety of values (around 1 degree), but T2 shows a different trend from the rest, and offers a wider variety of values(up to 3 degrees)\n\nwe'll first create a function to average the correlated features and then we'll add the irrelevant results to our irrelevant features list","execution_count":null},{"metadata":{"id":"x9uMoOmJenig","trusted":true},"cell_type":"code","source":"def features_average (df, features_to_average, new_feature_name):\n    df[new_feature_name] = np.mean(df[features_to_average], axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"j4vwFITSjtfh","outputId":"907c0c10-89ae-4a6b-bfdc-894a11d7e001","trusted":true},"cell_type":"code","source":"correlated_column_group_1.remove('T2')\nirrelevant_features = irrelevant_features + correlated_column_group_1\nfeatures_df = features_average(features_df,\n                               features_to_average = correlated_column_group_1,\n                               new_feature_name = 'T_avg_1_3_4_5_7_8_9')\nfeatures_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"Xp0CHmCCltzq"},"cell_type":"markdown","source":"### correlated column group 2","execution_count":null},{"metadata":{"id":"mSzsKuijlyjx","outputId":"d60d220d-3c24-4b2b-b6f9-790714419aaa","trusted":true},"cell_type":"code","source":"ccgroup2_gb_month = gb_month[correlated_column_group_2]\nccgroup2_gb_hour = gb_hour[correlated_column_group_2]\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=FIGURE_SIZE)\nccgroup2_gb_month.mean().plot(ax=ax1)\nccgroup2_gb_hour.mean().plot(ax=ax2)\n\nax1.set_ylabel('Humidity[%]')\nax2.set_ylabel('Humidity[%]')","execution_count":null,"outputs":[]},{"metadata":{"id":"2HFhPnfYmA_U"},"cell_type":"markdown","source":"Again, we can see that we have a group of features that acts similarly and one \"outlair\" ('RH_7')\n\nWe'll deal with it in the same manner","execution_count":null},{"metadata":{"id":"0CQyyldomfoB","outputId":"ce9d6d6b-26cd-43d7-8b75-023ac5cc57ad","trusted":true},"cell_type":"code","source":"correlated_column_group_2.remove('RH_7')\nirrelevant_features = irrelevant_features + correlated_column_group_2\nfeatures_df = features_average(features_df,\n                               features_to_average = correlated_column_group_2,\n                               new_feature_name = 'RH_avg_1_2_3_4')\nfeatures_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"xHfAExoXqn8q"},"cell_type":"markdown","source":"### correlated column group 3","execution_count":null},{"metadata":{"id":"AMlnirpDqwpM","outputId":"294cbd52-ad7e-4b82-dd88-f3ca7b5c3c5d","trusted":true},"cell_type":"code","source":"ccgroup3_gb_month = gb_month[correlated_column_group_3]\nccgroup3_gb_hour = gb_hour[correlated_column_group_3]\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=FIGURE_SIZE)\nccgroup3_gb_month.mean().plot(ax=ax1)\nccgroup3_gb_hour.mean().plot(ax=ax2)\n\nax1.set_ylabel('Temprature[C]')\nax2.set_ylabel('Temprature[C]')","execution_count":null,"outputs":[]},{"metadata":{"id":"d6k91Ns-rMl9"},"cell_type":"markdown","source":"These two features are almost identical - so we'll unite them","execution_count":null},{"metadata":{"id":"DpMtWBwVrWBU","outputId":"7cb6e671-5c4d-4e8d-eb53-cb947d1e5a99","trusted":true},"cell_type":"code","source":"irrelevant_features = irrelevant_features + correlated_column_group_3\nfeatures_df = features_average(features_df,\n                               features_to_average = correlated_column_group_3,\n                               new_feature_name = 'T_avg_6_out')\nfeatures_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"uRGYH6nRqpXe"},"cell_type":"markdown","source":"### correlated column group 4","execution_count":null},{"metadata":{"id":"NybKxd16rCRF","outputId":"11b945b1-3814-45ca-fca5-8277bbaa1e1a","trusted":true},"cell_type":"code","source":"ccgroup4_gb_month = gb_month[correlated_column_group_4]\nccgroup4_gb_hour = gb_hour[correlated_column_group_4]\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=FIGURE_SIZE)\nccgroup4_gb_month.mean().plot(ax=ax1)\nccgroup4_gb_hour.mean().plot(ax=ax2)\n\nax1.set_ylabel('Humidity[%]')\nax2.set_ylabel('Humidity[%]')","execution_count":null,"outputs":[]},{"metadata":{"id":"3D-cIQiLsRfM"},"cell_type":"markdown","source":"These two features are almost identical - so we'll unite them","execution_count":null},{"metadata":{"id":"T8-d3GspsIJ3","outputId":"ebe31624-e515-416e-9749-ac4089ead222","trusted":true},"cell_type":"code","source":"irrelevant_features = irrelevant_features + correlated_column_group_4\nfeatures_df = features_average(features_df,\n                               features_to_average = correlated_column_group_4,\n                               new_feature_name = 'RH_avg_8_9')\nfeatures_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"14EigDuhsbqa"},"cell_type":"markdown","source":"### Summary and final exclusion","execution_count":null},{"metadata":{"id":"mEGTUtO_sjZw"},"cell_type":"markdown","source":"Now that we've found our irrelevant features, we'll exclude them from df and check again how many features we have now","execution_count":null},{"metadata":{"id":"kZpeBD-6tTLv","outputId":"41de73e3-1e5a-4779-bb2a-4003f75c5377","trusted":true},"cell_type":"code","source":"irrelevant_features.append('hour')\n\nchosen_features_df = features_df.drop(irrelevant_features, axis=1)\nchosen_features_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"CrWPJwVnwd6H"},"cell_type":"markdown","source":"So we're down to 15 features! much better! \n\nLets take another look at the correlation map to see whats the current situation","execution_count":null},{"metadata":{"id":"k3NG0IMSLJpq","outputId":"26e50fab-bd59-4090-eec2-8fa258b3d6da","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=SNS_FIGURE_SIZE)\nmask_matrix = np.triu(chosen_features_df.corr())\nsns.heatmap(chosen_features_df.corr(), annot = True, cmap= 'coolwarm', linewidths=3, linecolor='black', mask=mask_matrix)","execution_count":null,"outputs":[]},{"metadata":{"id":"yy8fCeBuRH3l"},"cell_type":"markdown","source":"## Preprocessing","execution_count":null},{"metadata":{"id":"CYDhu5QfRMux"},"cell_type":"markdown","source":"Before we'll fit our selected features to the model, we'll preprocess(mostly normalization) our data.\n\nLets first divide the features into categories, so we can select different preprocessing for each group\n","execution_count":null},{"metadata":{"id":"DZAVBoqNiIIe","trusted":true},"cell_type":"code","source":"temperature_columns = [column_name for column_name in chosen_features_df.columns if column_name.startswith('T')]\nhumidity_columns = [column_name for column_name in chosen_features_df.columns if column_name.startswith('RH')]\nenvironment_columns = ['Windspeed', 'Press_mm_hg', 'Visibility']","execution_count":null,"outputs":[]},{"metadata":{"id":"CLYgjCMhR4oq"},"cell_type":"markdown","source":"For the temprature and humidity columns, we'll use a global min/max scaler. meaning that we'll take the global min/max from all temperature/humidity columns, and use those limits for our scaling the data between 0 to 1\n\nTo that purpose we'll bulit a custom transformer.","execution_count":null},{"metadata":{"id":"3SlWyv0XR1ln","trusted":true},"cell_type":"code","source":"class CustomMinMaxScaler (BaseEstimator, TransformerMixin):\n    def __init__(self, columns_to_scale):\n        self.columns_to_scale = columns_to_scale\n\n    def fit(self, X, y=None):\n        self.min_value_ = self._get_global_min(X[self.columns_to_scale])\n        self.max_value_ = self._get_global_max(X[self.columns_to_scale])\n        return self\n\n    def transform(self, X, *_):\n        norm_value = self.max_value_ - self.min_value_ \n        X[self.columns_to_scale] = X[self.columns_to_scale].apply(lambda x: (x-self.min_value_)/norm_value)\n        return X\n\n    def _get_global_max(self, part_X):\n        return max(np.max(part_X))\n\n    def _get_global_min(self, part_X):\n        return min(np.min(part_X))","execution_count":null,"outputs":[]},{"metadata":{"id":"J3s6c6XmWZb_"},"cell_type":"markdown","source":"Lets test them out","execution_count":null},{"metadata":{"id":"kl9cIt3pZJ7I","outputId":"5292a782-3dd1-468d-b77b-eb778defc333","trusted":true},"cell_type":"code","source":"scaled_features_df = chosen_features_df.copy()\n\ntemperature_scaler = CustomMinMaxScaler(columns_to_scale = temperature_columns)\ntemperature_scaler.fit_transform(scaled_features_df)\n\nhumidity_scaler = CustomMinMaxScaler(columns_to_scale = humidity_columns)\nhumidity_scaler.fit_transform(scaled_features_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"UAFZs-FeTgp0"},"cell_type":"markdown","source":"Great! for the rest of the columns, we'll scale them in the same range of our other columns, so we'll use the standard MinMaxScaler","execution_count":null},{"metadata":{"id":"qUA9EthJNmaE","trusted":true},"cell_type":"code","source":"min_max_scaler = MinMaxScaler()\nscaled_features_df['Press_mm_hg'] = min_max_scaler.fit_transform(np.array(chosen_features_df['Press_mm_hg']).reshape(-1,1))\nscaled_features_df['Windspeed'] = min_max_scaler.fit_transform(np.array(chosen_features_df['Windspeed']).reshape(-1,1))\nscaled_features_df['Visibility'] = min_max_scaler.fit_transform(np.array(chosen_features_df['Visibility']).reshape(-1,1)) ","execution_count":null,"outputs":[]},{"metadata":{"id":"CBYTV2opaYq0"},"cell_type":"markdown","source":"Good, so now we have our features chosen and scaled, we'll set the final groups and create a column transformer to orginize the results neatly.","execution_count":null},{"metadata":{"id":"ADnFisdxUUSE","trusted":true},"cell_type":"code","source":"class ColumnsDevision(Enum):\n    final_temperature_columns = [column_name for column_name in scaled_features_df.columns if column_name.startswith('T')]\n    final_humidity_columns = [column_name for column_name in scaled_features_df.columns if column_name.startswith('RH')]\n    final_environment_columns = ['Press_mm_hg', 'Windspeed', 'Visibility']\n\ncolumn_transformer = make_column_transformer(\n                        (CustomMinMaxScaler(columns_to_scale = ColumnsDevision.final_temperature_columns.value), ColumnsDevision.final_temperature_columns.value),\n                        (CustomMinMaxScaler(columns_to_scale = ColumnsDevision.final_temperature_columns.value), ColumnsDevision.final_humidity_columns.value),\n                        (MinMaxScaler(), ColumnsDevision.final_environment_columns.value),\n                        )","execution_count":null,"outputs":[]},{"metadata":{"id":"xRurt3nlgDod"},"cell_type":"markdown","source":"# Model selection and optimization","execution_count":null},{"metadata":{"id":"OKGPU35aeo7x"},"cell_type":"markdown","source":"## Inital model selection","execution_count":null},{"metadata":{"id":"hbEi6wnyM6sb"},"cell_type":"markdown","source":"First, we'll split the data into train/test sets, by ratio of 4:1 accordingly, or 80%-20% split","execution_count":null},{"metadata":{"id":"2R9gjMcgtZmk","trusted":true},"cell_type":"code","source":"regression_df = scaled_features_df.copy()\nX_train, X_test, y_train, y_test = train_test_split(regression_df, target_column, test_size=TEST_SIZE, random_state=RANDOM_STATE) # stratify=y","execution_count":null,"outputs":[]},{"metadata":{"id":"8HK5CA0cNQeL"},"cell_type":"markdown","source":"For initial test, we'll take the 3 models that we've learned so far and run them on our dataset, in order to choose on which model to foucs our hyper-parameter tuning ","execution_count":null},{"metadata":{"id":"t0xbeUhtQ2ft","outputId":"80a15b21-3dc9-42b0-e360-68aec6876619","trusted":true},"cell_type":"code","source":"models = [\n           ['Linear regression: ', LinearRegression()],\n           ['Desicion tree regressor: ', DecisionTreeRegressor()],\n           ['KNeighbors regressor: ',  KNeighborsRegressor()],\n         ]\n\nrmse_loss = lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred))\nmodel_data = []\nfor name,curr_model in models :\n    curr_model_data = {}\n    curr_model_data[\"Name\"] = name\n    curr_model.fit(X_train, y_train)\n    curr_model_data[\"Train_RMSE_Score\"] = round(rmse_loss(y_train, curr_model.predict(X_train)),2)\n    curr_model_data[\"Train_R^2_Score\"] = round(r2_score(y_train, curr_model.predict(X_train)),2)\n    curr_model_data[\"Test_RMSE_Score\"] = round(rmse_loss(y_test,curr_model.predict(X_test)),2)\n    curr_model_data[\"Test_R^2_Score\"] = round(r2_score(y_test,curr_model.predict(X_test)),2)\n    model_data.append(curr_model_data)\n\nmodel_data","execution_count":null,"outputs":[]},{"metadata":{"id":"mCqc_NhDNnmT"},"cell_type":"markdown","source":"Lets analyze the results:\n1. Linear regression - very poor results on both the train and test sets, makes sense since we have a few categorical veriables which tends to hinder this regression.\n2. Desicion tree regressor = seems to casue over fitting on the train set(R^2=1), and a poor score on the test set as a result.\n3. K nearest neighbors regressor - medium results on both the train/test sets, but since it's the best out of the three we'll focus on it.\n\nWe have our regressor, but the results are still far from being optimized, which means we should go back to feature enginnering in order to improve our results","execution_count":null},{"metadata":{"id":"FXdz1qZMNewj"},"cell_type":"markdown","source":"In order to asses the value of RMSE, we can look at the mean value of the target column and it's distribution","execution_count":null},{"metadata":{"id":"ZyBDVDhmNv2i","outputId":"0add577b-2f41-4d98-c26b-ae05254e92b6","trusted":true},"cell_type":"code","source":"target_column.hist()","execution_count":null,"outputs":[]},{"metadata":{"id":"Yx5kuL5tNdq0","outputId":"65ed2d37-1d91-4875-c36c-4255d4d2e0a7","trusted":true},"cell_type":"code","source":"target_column.mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"NvTwWGyAZg96"},"cell_type":"markdown","source":"## Feature selection","execution_count":null},{"metadata":{"id":"vnEhGXZv0VyY"},"cell_type":"markdown","source":"Now that we've chosen a model, we'll need to optimize our feature selection.\nWe have 15 features - the number of possible combinations for feature selection is around 30k.. so in order to choose the right combination, we'll construct a series of functions designed to get all possible combinations (using itertools) and evaluate each one of them in our model.\n\nThe basic algorithim is as follows:\n1. select a number of features (from 15 to 5)\n2. find all possible combinations for that number of features\n3. for each possible combination, run KNN regression\n4. calculate the RMSE and R^2 for both train and test datasets\n5. pick a different number of features and repeat steps 2-4 till all numbers between 5-15 were selected\n\nWe'll save the log and details in a dictionary call \"models_data\", and after we'll attempt to find the best 10 feature selections, by finding the minimum RMSE and the maximum R^2\n\nAfter we'll have all that information, we can select a feature list for further optimization\n\n**Note:** since we're doing a step-by-step optimization, we are at risk of missing the global optimal point and landing in local optimal point instead","execution_count":null},{"metadata":{"id":"fp0opD5hintu","trusted":true},"cell_type":"code","source":"class FeatureSelector ():\n    def get_model_data (self, reg_model, X_train, X_test, y_train, y_test):\n        model_data = {}\n        model_data['features'] = set(X_train.columns)\n        model_data['parameters'] = reg_model.get_params()\n        reg_model.fit(X_train, y_train)\n        y_hat_train = reg_model.predict(X_train)\n        y_hat_test = reg_model.predict(X_test)\n        model_data['Train_RMSE_score'] = round(rmse_loss(y_train, y_hat_train),2)\n        model_data['Train_R^2_score'] = round(r2_score(y_train, y_hat_train),2)\n        model_data['Test_RMSE_score'] = round(rmse_loss(y_test, y_hat_test),2)\n        model_data['Test_R^2_score'] = round(r2_score(y_test, y_hat_test),2)\n        return model_data\n\n    def build_and_test_model (self, reg_model, df, target_column):\n        X_train, X_test, y_train, y_test = train_test_split(df, target_column, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n        return self.get_model_data (reg_model, X_train, X_test, y_train, y_test)\n\n    def feature_iterator (self, df, target_column, reg_model, min_features_num, max_features_num):\n        models_data = {}\n        for features_num in range(min_features_num, max_features_num):\n            possible_features = set(combinations(df.columns, features_num))\n            for inner_index, selected_features in enumerate(possible_features):\n                model_name = 'reg_model_' + str(features_num) + '_' + str(inner_index)\n                models_data[model_name] = self.build_and_test_model (reg_model, df[list(selected_features)], target_column)\n        return models_data\n\n    def get_n_best_models(self, models_data, key_parameter, result_goal, n_results):\n        best_models_names = []\n        temp_models_data = models_data.copy()\n        for index in range(n_results):\n            if result_goal == 'max':\n                current_best_model_name = self.get_model_max_value(temp_models_data, key_parameter)\n            elif result_goal == 'min':\n                current_best_model_name = self.get_model_min_value(temp_models_data, key_parameter)\n            best_models_names.append(current_best_model_name)\n            del temp_models_data[current_best_model_name]\n        return best_models_names\n\n    def get_model_max_value (self, models_data, key_parameter):\n        max_value = 0\n        for name, model in models_data.items():\n            if model[key_parameter] > max_value:\n                max_value = model[key_parameter]\n                model_name = name\n        return model_name\n\n    def get_model_min_value (self, models_data, key_parameter):\n        min_value = 1000000\n        for name, model in models_data.items():\n            if model[key_parameter] < min_value:\n                min_value = model[key_parameter]\n                model_name = name\n        return model_name\n\n    def models_data_to_df (self, models_data, models_list, selected_keys):\n        temp_list = []    \n        for model_name in models_list:\n            temp_dict = {key: value for key, value in models_data[model_name].items() if key in selected_keys}\n            temp_list.append(temp_dict)\n        result_df = pd.DataFrame(temp_list, index=models_list)\n        return result_df\n\n    def set_parameters (self, reg_model, parameters_name, parameters_value):\n        for index, parameter_name in enumerate(parameters_name):\n            setattr(reg_model, parameter_name, parameters_value[index])\n        return reg_model\n    \n    def tune_models_parameters (self, df, target_column, reg_model, models_data, models_list, parameters_to_tune):\n        parameter_matrix = product(*parameters_to_tune.values())\n        new_models_data = models_data.copy()\n        for model_name in models_list:\n            selected_features = models_data[model_name].get('features')\n            for inner_index, parameters in enumerate(parameter_matrix):\n                reg_model = self.set_parameters(reg_model, parameters_to_tune.keys() ,parameters)\n                new_model_name = model_name + '_' + str(inner_index)\n                new_models_data[new_model_name] = self.build_and_test_model (reg_model, df[list(selected_features)], target_column)\n        return new_models_data\n\nfeature_selector = FeatureSelector()","execution_count":null,"outputs":[]},{"metadata":{"id":"70pHvX4ngcnD","trusted":true},"cell_type":"code","source":"# number of combination - 15 choose 5 + 15 choose 6 + 15 choose 7 + ..\nmodels_data = feature_selector.feature_iterator(scaled_features_df, target_column, KNeighborsRegressor(), 5, len(scaled_features_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"id":"jyxAtRu1Jjp2","outputId":"af05e0bd-21c5-4f4e-bcba-17fb2040d755","trusted":true},"cell_type":"code","source":"models_data","execution_count":null,"outputs":[]},{"metadata":{"id":"vDv6X4NG3_ft"},"cell_type":"markdown","source":"Now that we have the results, we can find the optimal combinations for RMSE/R^2","execution_count":null},{"metadata":{"id":"WRQ2p_geK0R2","outputId":"82f62908-6fe2-421d-fe36-05ae12d03ea7","trusted":true},"cell_type":"code","source":"selected_keys = ('Test_R^2_score', 'Test_RMSE_score', 'Train_R^2_score', 'Train_RMSE_score')\nselected_models = feature_selector.get_n_best_models(models_data, 'Test_R^2_score', 'max', 10)\nresult_df = feature_selector.models_data_to_df(models_data, selected_models, selected_keys)\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"id":"wnwIAsfSLW0l","trusted":true},"cell_type":"code","source":"features_opt1_df = scaled_features_df[models_data[selected_models[0]].get('features')]","execution_count":null,"outputs":[]},{"metadata":{"id":"9AMc8qGqOI0-","outputId":"73fd457f-4379-4fb7-9729-80e20b27c46c","trusted":true},"cell_type":"code","source":"selected_keys = ('Test_R^2_score', 'Test_RMSE_score', 'Train_R^2_score', 'Train_RMSE_score')\nselected_models = feature_selector.get_n_best_models(models_data, 'Test_RMSE_score', 'min', 10)\nresult_df = feature_selector.models_data_to_df(models_data, selected_models, selected_keys)\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"id":"CI0rpL6ULw6X","trusted":true},"cell_type":"code","source":"features_opt2_df = scaled_features_df[models_data[selected_models[0]].get('features')]","execution_count":null,"outputs":[]},{"metadata":{"id":"NV5JzoC1giHP"},"cell_type":"markdown","source":"## Hyperparameter finetuning using GridSearchCV","execution_count":null},{"metadata":{"id":"hdCNRI-mPjqo"},"cell_type":"markdown","source":"### Setting up and running the search","execution_count":null},{"metadata":{"id":"6_btkkf70y8w"},"cell_type":"markdown","source":"Our first attempt to finetune the models parameter will be using GridSearchCV.\nIn order to do so, we'll need to carefully define parameter ranges and scoring methods","execution_count":null},{"metadata":{"id":"jC7QwUI4_cRd","trusted":true},"cell_type":"code","source":"r2_scorer = make_scorer (r2_score)\nrmse_scorer = make_scorer (rmse_loss)\n\nleaf_size_range = [10, 20, 30, 40, 50, 60]\nn_neighbors_range = [3, 5, 7, 9, 11]\np_range = [1, 2]\n        \ntuned_params = {\n    'leaf_size': leaf_size_range,\n    'n_neighbors': n_neighbors_range,\n    'p' : p_range\n               }\n\nscoring = {'RMSE' : rmse_scorer,\n           'R^2' : r2_scorer}","execution_count":null,"outputs":[]},{"metadata":{"id":"88jir9hZ1Wdm"},"cell_type":"markdown","source":"Now that everything is set, we can define the grid search object and use it to optimize our model","execution_count":null},{"metadata":{"id":"7eDLyQlzGIT1","trusted":true},"cell_type":"code","source":"gs = GridSearchCV(estimator = KNeighborsRegressor(), param_grid = tuned_params, scoring= scoring, refit='RMSE', cv=5, return_train_score=True)\n\ngs.fit(features_opt1_df, target_column)\nfeatures_opt1_best_params = gs.best_params_\n\ngs.fit(features_opt2_df, target_column)\nfeatures_opt2_best_params = gs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"QzH_blIpPsWO"},"cell_type":"markdown","source":"### the results and final selection","execution_count":null},{"metadata":{"id":"r2x9Ed_AyGe9","outputId":"5c4abc96-4b7b-4b78-b9bb-a3d914e6e5e1","trusted":true},"cell_type":"code","source":"features_opt1_best_params","execution_count":null,"outputs":[]},{"metadata":{"id":"u5AugTujGZeX","outputId":"a37074fa-f859-4d94-f5bb-e6f798437054","trusted":true},"cell_type":"code","source":"features_opt2_best_params","execution_count":null,"outputs":[]},{"metadata":{"id":"u19ucjji0LW4","outputId":"bb737665-da7b-4257-c50e-bda4908cffdc","trusted":true},"cell_type":"code","source":"cv_results_df = pd.concat([pd.DataFrame(gs.cv_results_['params']),\n                           pd.DataFrame(gs.cv_results_['mean_test_RMSE'], columns=['RMSE']),\n                           pd.DataFrame(gs.cv_results_['mean_test_R^2'], columns=['R^2'])] ,axis=1)\ncv_results_df = cv_results_df.sort_values(['RMSE'], axis=0, ascending=False)\ncv_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"AtRr7jUU1qLN"},"cell_type":"markdown","source":"To see the actual results of each model, we'll now run each model-df couple with its parameters","execution_count":null},{"metadata":{"id":"bex_FjjuNnKW","outputId":"346397f5-c917-4a0e-dd9c-f45c456f64bb","trusted":true},"cell_type":"code","source":"opt_clf_1 = KNeighborsRegressor()\nfor parameter, value in features_opt1_best_params.items():\n    setattr(opt_clf_1, parameter, value)\nfeature_selector.build_and_test_model(opt_clf_1, features_opt1_df, target_column)","execution_count":null,"outputs":[]},{"metadata":{"id":"IXnFoevGN9RF","outputId":"1269c3ea-b380-4c8d-8583-82127c4dc5be","trusted":true},"cell_type":"code","source":"opt_clf_2 = KNeighborsRegressor()\nfor parameter, value in features_opt2_best_params.items():\n    setattr(opt_clf_2, parameter, value)\nfeature_selector.build_and_test_model(opt_clf_2, features_opt2_df, target_column)","execution_count":null,"outputs":[]},{"metadata":{"id":"VuyJqpREQeoK"},"cell_type":"markdown","source":"## Hyperparameter finetuning using FeatureSelector (custom)","execution_count":null},{"metadata":{"id":"xjOEP8wLLFxb"},"cell_type":"markdown","source":"Alternatively, we can use the FeatureSelector we've built here\n\nIn order to expend out search and lower the chance that we've landed on a local optimum point, we'll now search the \"top\" models with different features, and for each of these models tune the parameters for optimum results\n\nTo that purpose, we'll use our FeatureSelector","execution_count":null},{"metadata":{"id":"7IbSep2XQ7ud"},"cell_type":"markdown","source":"### Setting up and running the search","execution_count":null},{"metadata":{"id":"n_zFRN45u1h1","trusted":true},"cell_type":"code","source":"selected_models = feature_selector.get_n_best_models(models_data, 'Test_RMSE_score', 'min', 10) + feature_selector.get_n_best_models(models_data, 'Test_R^2_score', 'max', 10)","execution_count":null,"outputs":[]},{"metadata":{"id":"pXKJLuxUutvO","trusted":true},"cell_type":"code","source":"updated_models_data = feature_selector.tune_models_parameters (scaled_features_df, target_column, KNeighborsRegressor(), models_data, selected_models, tuned_params)","execution_count":null,"outputs":[]},{"metadata":{"id":"uItJUpVmQ_iY"},"cell_type":"markdown","source":"### the results and final selection","execution_count":null},{"metadata":{"id":"JLAro-qfxdYo","outputId":"4eb658ef-ea27-4c91-ee98-913659d8e57e","trusted":true},"cell_type":"code","source":"selected_keys = ('Test_R^2_score', 'Test_RMSE_score', 'Train_R^2_score', 'Train_RMSE_score')\nselected_models = feature_selector.get_n_best_models(updated_models_data, 'Test_RMSE_score', 'min', 10) + feature_selector.get_n_best_models(updated_models_data, 'Test_R^2_score', 'max', 10)\nresult_df = feature_selector.models_data_to_df(updated_models_data, selected_models, selected_keys)\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"id":"vPi0LbaTwknY","outputId":"5ddf4ad4-a760-48c5-9a72-e66339dee36c","trusted":true},"cell_type":"code","source":"features_opt3_df = scaled_features_df[updated_models_data[selected_models[0]].get('features')]\nfeatures_opt3_best_params = updated_models_data[selected_models[0]].get('parameters')\nopt_clf_3 = KNeighborsRegressor()\nfor parameter, value in features_opt3_best_params.items():\n    setattr(opt_clf_3, parameter, value)\nfeature_selector.build_and_test_model(opt_clf_3, features_opt2_df, target_column)","execution_count":null,"outputs":[]},{"metadata":{"id":"y_OEijLICbWf"},"cell_type":"markdown","source":"# Final regressor (for production level)","execution_count":null},{"metadata":{"id":"Vpoz2sR_fxLt","trusted":true},"cell_type":"code","source":"class ClassiferContsants(Enum):   \n    N_NEIGHBORS = 3\n    LEAF_SIZE = 10\n    P = 2","execution_count":null,"outputs":[]},{"metadata":{"id":"3xU7ew4xhaE4","trusted":true},"cell_type":"code","source":"class KnnRegressor (BaseEstimator, ClassifierMixin):  \n    def __init__(self,\n                 n_neighbors = ClassiferContsants.N_NEIGHBORS.value,\n                 leaf_size = ClassiferContsants.LEAF_SIZE.value,\n                 p = ClassiferContsants.P.value,\n                ):\n      self.n_neighbors = n_neighbors\n      self.leaf_size = leaf_size\n      self.p = p\n\n    def _pipeline_constructor(self):\n        self.pipeline_ = Pipeline(steps=[\n                                       ('classifier', KNeighborsRegressor(\n                                                                          n_neighbors = self.n_neighbors,\n                                                                          leaf_size = self.leaf_size,\n                                                                          p = self.p,\n                                                                                                 ),\n                                       )])\n\n    def fit(self, X, y=None):  \n        self._pipeline_constructor()\n        self.pipeline_.fit(X, y)\n        return self\n\n    def predict(self, X, y=None):\n        return self.pipeline_.predict(X)\n\n    def score(self, X, y=None):\n        rmse_score = lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred))\n        rmse, r2 = round(rmse_score(X, y), 2), round(r2_score(X, y), 2)\n        print(f\"RMSE = {rmse}\\t R^2 = {r2}\")\n        return rmse, r2","execution_count":null,"outputs":[]},{"metadata":{"id":"VUd8wnGInAxM","outputId":"56b062df-a899-456e-b512-1463cd5b092e","trusted":true},"cell_type":"code","source":"clf = KnnRegressor()\nprint('Model 1 results:')\nregression_df = features_opt1_df.copy()\nX_train, X_test, y_train, y_test = train_test_split(regression_df, target_column, test_size=TEST_SIZE, random_state=RANDOM_STATE)\nclf.fit(X_train, y_train)\nprint ('Train results')\nclf.score(y_train, clf.predict(X_train))\nprint ('Test_results')\nclf.score(y_test, clf.predict(X_test))\n\nprint('\\nModel 2 results:')\nregression_df = features_opt2_df.copy()\nX_train, X_test, y_train, y_test = train_test_split(regression_df, target_column, test_size=TEST_SIZE, random_state=RANDOM_STATE)\nclf.fit(X_train, y_train)\nprint ('Train results')\nclf.score(y_train, clf.predict(X_train))\nprint ('Test_results')\nclf.score(y_test, clf.predict(X_test))\n\nprint('\\nModel 3 results:')\nregression_df = features_opt3_df.copy()\nX_train, X_test, y_train, y_test = train_test_split(regression_df, target_column, test_size=TEST_SIZE, random_state=RANDOM_STATE)\nclf.fit(X_train, y_train)\nprint ('Train results')\nclf.score(y_train, clf.predict(X_train))\nprint ('Test_results')\nclf.score(y_test, clf.predict(X_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}