{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/glass/glass.csv\")\ndf.isna().sum(axis=0) #No missing values\ndf.hist()\ndescr_stats = df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"descr_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All variables are skewed - Al, Na and Si come closest to normal distribution. LDA may not be suitable for this.\n\nMixed results in terms of left-skewed and right-skewed, so uniformly transforming all variables will not yield suitable results.\n\nFor Ba, Fe, and K, only a small number of samples contain a high level of both these elements.\n\nBa: 75th percentile=0, max=3.15\nFe: 75th percentile=0.1, max=0.51\nK: 75th percentile=0.61, max=6.21 but lower percentiles are non zero though quite small.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only two types of glass (type-2 and type-1) make up over 2/3rds of the total samples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(column=\"Ba\",by=\"Type\",vert=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ba is not present in type-6 glass. In other types, the concentration ranges are overlapping and no definitive conclusion is possible.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(column=\"Fe\",by=\"Type\",vert=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(column=\"K\",by=\"Type\",vert=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fe and K also missing from Type 6, but more evenly distributed among the other types.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ba_fe_k = df.loc[(df[\"Ba\"]==0)&(df[\"Fe\"]==0)&(df[\"K\"]==0)]\nlen(ba_fe_k.loc[df[\"Type\"]==6]),len(ba_fe_k)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the entire data set, there are only 14 entries in which the concentration of all three of Ba, Fe and K is zero, and it includes all 9 samples of Type 6.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_table = df.corr()\ncorr_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Type of glass has been considered as continuous variable here. Type of glass showing strong -ve correlation with Mg content and +ve correlation in the 0.5-0.6 range with Al, Ba and Na content. Large gap between the above 4 and the 5th which is at abs value of 0.19.\n\nIgnoring the type of glass, there is a very strong correlation (>0.8) between calcium content and refractive index. As RI is a result of calcium content, it may be considered to exclude it from the independent variables to be used for model building\n\nBa and Fe are missing entirely from one type which may be significant. Mg content showing absolute correlation > 0.4 with Al, Ca and Ba, and Al content showing absolute correlation around 0.47-0.48 with Mg and Ba.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating separate DFs for dependent and independent variables\ntarget = df[\"Type\"]\nx_vars = df.drop(columns=\"Type\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing k-means clustering to see what kinds of clusters are formed and what kind of homogeneity is observed in the clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n#Number of clusters is same as number of types of glass\nkmeans_init = KMeans(init=\"random\",n_clusters=len(df[\"Type\"].value_counts()),n_init=10,random_state=42) \nkmeans_algo = kmeans_init.fit(df)\nkmeans_clust_nums = kmeans_algo.predict(df)\ncompare = pd.crosstab(target,kmeans_clust_nums) #Compare the cluster numbering and actual glass types\n'''\ncsum = np.cumsum(np.array(compare),axis=1)\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nfrom matplotlib import cm\nclrmap = plt.get_cmap('RdBu')\ncNorm  = colors.Normalize(vmin=0, vmax=len(compare.index))\nscalarMap = cm.ScalarMappable(norm=cNorm, cmap=clrmap)\nplt.figure()\nfor c in compare.columns:\n    for i in range(0,len(compare.index)):\n        if i==0:\n            plt.bar(c,compare[c][compare.index[i]],color=str(0.13*i))\n        else:\n            plt.bar(c,compare[c][compare.index[i]],color=str(0.13*i),bottom=compare[c][compare.index[i-1]]) #Why is one color getting repeated?\n        \n'''\ncompare","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observed that no type of glass is exclusive to just one cluster.\n\nType 5 most widely scattered (among 4 different clusters). Type 7 has most distinctive identification (23/29 samples of type 7 are in cluster 0 and 6/29 in cluster 5). Total 23/29 entries in cluster 0 are of Type 7, so only Type 7 glass is significantly correlated with a particular cluster.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_vars,target,train_size=0.8,random_state=64) #80% training data, 20% testing data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier()\nknn_model.fit(x_train,y_train)\nknn_pred = knn_model.predict(x_test) #Classified into 6 different types\nknn_compare = pd.crosstab(y_test,knn_pred)\nknn_dsum = 0\nfor c in knn_compare.columns:\n    knn_dsum = knn_dsum + knn_compare[c][c]\nknn_accuracy = knn_dsum/knn_compare.sum().sum() #56% accuracy\n\n#Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg_model = LogisticRegression()\nlogreg_model.fit(x_train,y_train) #Failed to converge\nlogreg_pred = logreg_model.predict(x_test)\nlogreg_compare = pd.crosstab(y_test,logreg_pred)\nlogreg_dsum = 0\nfor c in logreg_compare.columns:\n    logreg_dsum = logreg_dsum + logreg_compare[c][c]\nlogreg_accuracy = logreg_dsum/logreg_compare.sum().sum() #44% accuracy\n\n#Decision trees\nfrom sklearn import tree\ndtree_gini_model = tree.DecisionTreeClassifier(criterion=\"gini\")\ndtree_entropy_model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n\ndtree_gini_model.fit(x_train,y_train)\ndtree_gini_pred = dtree_gini_model.predict(x_test) #Classified into 6 different categories\ndtree_gini_compare = pd.crosstab(y_test,dtree_gini_pred)\ndtree_gini_dsum = 0\nfor c in dtree_gini_compare.columns:\n    dtree_gini_dsum = dtree_gini_dsum + dtree_gini_compare[c][c]\ndtree_gini_accuracy = dtree_gini_dsum/dtree_gini_compare.sum().sum() #53% accuracy\n\ndtree_entropy_model.fit(x_train,y_train)\ndtree_entropy_pred = dtree_entropy_model.predict(x_test) #Classified into 5 different categories\ndtree_entropy_compare = pd.crosstab(y_test,dtree_entropy_pred)\ndtree_entropy_dsum = 0\nfor c in dtree_entropy_compare.columns:\n    dtree_entropy_dsum = dtree_entropy_dsum + dtree_entropy_compare[c][c] #Correctly classified counts - diagonal elements only\ndtree_entropy_accuracy = dtree_entropy_dsum/dtree_entropy_compare.sum().sum() #74% accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_cmp1 = pd.DataFrame([[knn_accuracy],[logreg_accuracy],[dtree_gini_accuracy],[dtree_entropy_accuracy]])\nacc_cmp1.index = [\"KNN\",\"Logistic reg\",\"D_trees - Gini\",\"D_trees - Entropy\"]\nacc_cmp1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision trees entropy-based classifier found to show highest accuracy, with lowest shown by logistic regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_compare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_compare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_gini_compare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_entropy_compare","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression was unable to even detect the presence of 6 different types.\nExcept for logistic regression, a majority of samples of Type 1 were correctly identified.\nIn all cases except entropy, Type 7 was classified with 100% accuracy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now using k-fold validation to see the kind of results obtained and comparing to a single fixed test-train split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score\n\nkfold = RepeatedStratifiedKFold(n_splits=10,n_repeats=5,random_state=42) #Divide data set into 10 parts, and perform 5 iterations with different train/test splits\nknn_kfold = cross_val_score(knn_model,x_vars,target,scoring=\"accuracy\",cv=kfold) #Median accuracy around 68%, large variation 50%-86%\nlogreg_kfold = cross_val_score(logreg_model,x_vars,target,scoring=\"accuracy\",cv=kfold) #Median accuracy around 60%, large variation 45%-75%\ndtree_gini_kfold = cross_val_score(dtree_gini_model,x_vars,target,scoring=\"accuracy\",cv=kfold) #Median accuracy around 69%, large variation 45%-90%\ndtree_entropy_kfold = cross_val_score(dtree_entropy_model,x_vars,target,scoring=\"accuracy\",cv=kfold) #Median accuracy around 70%, large variation 52%-90%\n\nplt.figure()\nplt.title(\"With RI included\")\npd.concat([pd.DataFrame(knn_kfold),pd.DataFrame(logreg_kfold),pd.DataFrame(dtree_gini_kfold),pd.DataFrame(dtree_entropy_kfold)],axis=1).boxplot()\nplt.xticks((1,2,3,4),(\"KNN\",\"Logistic\\nregression\",\"Decision trees\\nGini\",\"Decision trees\\nEntropy\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Warning: Count of smallest class is less than number of items in split.\nThere are only 9 samples of Type 6, whereas each split will contain 10 samples. So there is at least one split which will not contain any sample of Type 6 glass.\n\nThe median accuracy for all methods is found to be mch higher after k-fold validation.\nIt appears that the test-train split was not at all covered in the k-fold combinations - the minimum accuracy observed in the k-fold work is more than the accuracy observed through the train-test split.\n\nNext step: As RI has been observed to be highly correlated with the Ca concentration, removing RI from the data to reduce multicollinearity and repeat the above procedure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_noRI = x_vars.drop(columns=\"RI\") #This will be required for k-fold cross validation\n#Not splitting into train and test again because some issues observed with random_state not giving consistent results\n#Subsetting the original test and train data to obtain new data\nx_train_noRI = x_train.drop(columns=\"RI\")\nx_test_noRI = x_test.drop(columns=\"RI\")\n\n#KNN\nknn_model = KNeighborsClassifier()\nknn_model.fit(x_train_noRI,y_train)\nknn_pred = knn_model.predict(x_test_noRI)\nknn_compare2 = (knn_compare,pd.crosstab(y_test,knn_pred))\nknn_dsum = 0\nfor c in knn_compare2[1].columns:\n    knn_dsum = knn_dsum + knn_compare2[1][c][c]\nknn_accuracy2 = (knn_accuracy,knn_dsum/knn_compare2[1].sum().sum())\n\n#Logistic regression\nlogreg_model = LogisticRegression()\nlogreg_model.fit(x_train_noRI,y_train)\nlogreg_pred = logreg_model.predict(x_test_noRI)\nlogreg_compare2 = (logreg_compare,pd.crosstab(y_test,logreg_pred))\nlogreg_dsum = 0\nfor c in logreg_compare2[1].columns:\n    logreg_dsum = logreg_dsum + logreg_compare2[1][c][c]\nlogreg_accuracy2 = (logreg_accuracy,logreg_dsum/logreg_compare2[1].sum().sum())\n\n#Decision trees\ndtree_gini_model = tree.DecisionTreeClassifier(criterion=\"gini\")\ndtree_entropy_model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n\ndtree_gini_model.fit(x_train_noRI,y_train)\ndtree_gini_pred = dtree_gini_model.predict(x_test_noRI)\ndtree_gini_compare2 = (dtree_gini_compare,pd.crosstab(y_test,dtree_gini_pred))\ndtree_gini_dsum = 0\nfor c in dtree_gini_compare2[1].columns:\n    dtree_gini_dsum = dtree_gini_dsum + dtree_gini_compare2[1][c][c]\ndtree_gini_accuracy2 = (dtree_gini_accuracy,dtree_gini_dsum/dtree_gini_compare2[1].sum().sum())\n\ndtree_entropy_model.fit(x_train_noRI,y_train)\ndtree_entropy_pred = dtree_entropy_model.predict(x_test_noRI)\ndtree_entropy_compare2 = (dtree_entropy_compare,pd.crosstab(y_test,dtree_entropy_pred))\ndtree_entropy_dsum = 0\nfor c in dtree_entropy_compare2[1].columns:\n    dtree_entropy_dsum = dtree_entropy_dsum + dtree_entropy_compare2[1][c][c]\ndtree_entropy_accuracy2 = (dtree_entropy_accuracy,dtree_entropy_dsum/dtree_entropy_compare2[1].sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_cmp2 = pd.DataFrame([[knn_accuracy2],[logreg_accuracy2],[dtree_gini_accuracy2],[dtree_entropy_accuracy2]])\nacc_cmp2.index = [\"KNN\",\"Logistic reg\",\"D_trees - Gini\",\"D_trees - Entropy\"]\nacc_cmp2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mixed results: No change in KNN accuracy, improvement in logistic regression and decision trees Gini-based, while accuracy of entropy-based decision tree decreased","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next: Try classification after removing Ba, Fe and K","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_3removed = x_noRI.drop(columns=[\"Ba\",\"Fe\",\"K\"]) #This will be required for k-fold cross validation\n#Not splitting into train and test again because some issues observed with random_state not giving consistent results\n#Subsetting the original test and train data to obtain new data\nx_train_3removed = x_train_noRI.drop(columns=[\"Ba\",\"Fe\",\"K\"])\nx_test_3removed = x_test_noRI.drop(columns=[\"Ba\",\"Fe\",\"K\"])\n\n#KNN\nknn_model = KNeighborsClassifier()\nknn_model.fit(x_train_3removed,y_train)\nknn_pred = knn_model.predict(x_test_3removed)\nknn_compare3 = (knn_compare2,pd.crosstab(y_test,knn_pred))\nknn_dsum = 0\nfor c in knn_compare3[1].columns:\n    knn_dsum = knn_dsum + knn_compare3[1][c][c]\nknn_accuracy3 = (knn_accuracy2[1],knn_dsum/knn_compare3[1].sum().sum())\n\n#Logistic regression\nlogreg_model = LogisticRegression()\nlogreg_model.fit(x_train_3removed,y_train)\nlogreg_pred = logreg_model.predict(x_test_3removed)\nlogreg_compare3 = (logreg_compare2,pd.crosstab(y_test,logreg_pred))\nlogreg_dsum = 0\nfor c in logreg_compare3[1].columns:\n    logreg_dsum = logreg_dsum + logreg_compare3[1][c][c]\nlogreg_accuracy3 = (logreg_accuracy2[1],logreg_dsum/logreg_compare3[1].sum().sum())\n\n#Decision trees\ndtree_gini_model = tree.DecisionTreeClassifier(criterion=\"gini\")\ndtree_entropy_model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n\ndtree_gini_model.fit(x_train_3removed,y_train)\ndtree_gini_pred = dtree_gini_model.predict(x_test_3removed)\ndtree_gini_compare3 = (dtree_gini_compare2,pd.crosstab(y_test,dtree_gini_pred))\ndtree_gini_dsum = 0\nfor c in dtree_gini_compare3[1].columns:\n    dtree_gini_dsum = dtree_gini_dsum + dtree_gini_compare3[1][c][c]\ndtree_gini_accuracy3 = (dtree_gini_accuracy2[1],dtree_gini_dsum/dtree_gini_compare3[1].sum().sum())\n\ndtree_entropy_model.fit(x_train_3removed,y_train)\ndtree_entropy_pred = dtree_entropy_model.predict(x_test_3removed)\ndtree_entropy_compare3 = (dtree_entropy_compare2,pd.crosstab(y_test,dtree_entropy_pred))\ndtree_entropy_dsum = 0\nfor c in dtree_entropy_compare3[1].columns:\n    dtree_entropy_dsum = dtree_entropy_dsum + dtree_entropy_compare3[1][c][c]\ndtree_entropy_accuracy3 = (dtree_entropy_accuracy2[1],dtree_entropy_dsum/dtree_entropy_compare3[1].sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_cmp3 = pd.DataFrame([[knn_accuracy3],[logreg_accuracy3],[dtree_gini_accuracy3],[dtree_entropy_accuracy3]])\nacc_cmp3.index = [\"KNN\",\"Logistic reg\",\"D_trees - Gini\",\"D_trees - Entropy\"]\nacc_cmp3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mixed results again - improvement in KNN, no change in logistic regression, Gini accuracy reduced, entropy accuracy increased","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_compare3[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_compare3[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_gini_compare3[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_entropy_compare3[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of samples of Type 6 in test data is only 2, which is not enough to conclusively make any statement regarding effectiveness.\n\nAccuracy of identifying Type 2 increased after removing the Ba, Fe and K columns.\n\nHowever the entropy decision tree was unable to recognise the presence of 6 types in the data.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}