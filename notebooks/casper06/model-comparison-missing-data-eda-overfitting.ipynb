{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport scipy.stats as stats\nfrom scipy.stats import chi2\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T10:15:57.47043Z","iopub.execute_input":"2021-06-12T10:15:57.470898Z","iopub.status.idle":"2021-06-12T10:15:58.55097Z","shell.execute_reply.started":"2021-06-12T10:15:57.470825Z","shell.execute_reply":"2021-06-12T10:15:58.549383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><strong>Content</strong></div>\n\n<div class=\"list-group\">\n    <a class=\"list-group-item list-group-item-action\" href=\"#mda\">Missing Data Analysis</a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#eda\">Exploratory Data Analysis</a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#sdo\">Small Dataset and Overfitting</a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#dpp\">Data Pre-Processing</a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#mtt\">Model Training & Testting</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\"><strong>The Data</strong></div>","metadata":{}},{"cell_type":"code","source":"#load the data\ndataDf = pd.read_csv('../input/Transformed Data Set - Sheet1.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:58.554593Z","iopub.execute_input":"2021-06-12T10:15:58.555141Z","iopub.status.idle":"2021-06-12T10:15:58.589341Z","shell.execute_reply.started":"2021-06-12T10:15:58.555046Z","shell.execute_reply":"2021-06-12T10:15:58.588263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataDf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:58.591024Z","iopub.execute_input":"2021-06-12T10:15:58.591339Z","iopub.status.idle":"2021-06-12T10:15:58.619223Z","shell.execute_reply.started":"2021-06-12T10:15:58.591284Z","shell.execute_reply":"2021-06-12T10:15:58.618268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataDf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:58.620898Z","iopub.execute_input":"2021-06-12T10:15:58.621211Z","iopub.status.idle":"2021-06-12T10:15:58.627908Z","shell.execute_reply.started":"2021-06-12T10:15:58.621156Z","shell.execute_reply":"2021-06-12T10:15:58.626814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataDf.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:58.631333Z","iopub.execute_input":"2021-06-12T10:15:58.631698Z","iopub.status.idle":"2021-06-12T10:15:58.645913Z","shell.execute_reply.started":"2021-06-12T10:15:58.631649Z","shell.execute_reply":"2021-06-12T10:15:58.644645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset is a very small dataset and it contains 66 samples and 5 variables including the target variable. We can see that all the variables are categorical. Furthermore, all of them are nominal which means there is no order in the data. \n\n<div class=\"alert alert-block alert-success\" id='mda'><strong>Missing Data Aanalysis</strong></div>","metadata":{}},{"cell_type":"code","source":"#plot missing data using msno.matrix\nmsno.matrix(dataDf)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:58.649822Z","iopub.execute_input":"2021-06-12T10:15:58.650676Z","iopub.status.idle":"2021-06-12T10:15:59.340995Z","shell.execute_reply.started":"2021-06-12T10:15:58.650579Z","shell.execute_reply":"2021-06-12T10:15:59.339837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are lucky :), there are no missing data available on the dataset. Therefore, it's time to start exploratory data analysis. \n\n<div class='alert alert-block alert-success' id='eda'><strong>Exploratory Data Analysis</strong></div>\n\nLet's study the target variable first.\n \n #### 1. Gender","metadata":{}},{"cell_type":"code","source":"#create data frame and store value counts and percentages\ndef valueCount(data, traget):\n    return pd.DataFrame({\n        'Count': data[traget].value_counts(),\n        'Precent(%)': data[traget].value_counts(normalize=True) * 100\n    })","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.34297Z","iopub.execute_input":"2021-06-12T10:15:59.34367Z","iopub.status.idle":"2021-06-12T10:15:59.350889Z","shell.execute_reply.started":"2021-06-12T10:15:59.343584Z","shell.execute_reply":"2021-06-12T10:15:59.349591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataDf['Gender'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.35243Z","iopub.execute_input":"2021-06-12T10:15:59.352807Z","iopub.status.idle":"2021-06-12T10:15:59.368072Z","shell.execute_reply.started":"2021-06-12T10:15:59.35274Z","shell.execute_reply":"2021-06-12T10:15:59.366922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valueCount(dataDf, 'Gender')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.370348Z","iopub.execute_input":"2021-06-12T10:15:59.370704Z","iopub.status.idle":"2021-06-12T10:15:59.489042Z","shell.execute_reply.started":"2021-06-12T10:15:59.370637Z","shell.execute_reply":"2021-06-12T10:15:59.488074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two values in Gender:\n* F = Female\n* M = Male\n \nMost importantly, data distribution is balanced(50% of male and 50% of female samples). What is balanced data distribution? Let me explain using this example. If in our data set we have male samples which are approximately the same as female samples, then we can say our dataset is balanced. On the other hand, if there is a very high difference between the male samples and female samples, then we can say our dataset is imbalanced.\n\n\"Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.\" (Brownlee, 2019).\n\nHandling imbalanced data is whole another story. Therefore, I'm not going to discuss it here. However, I would like to attach some articles here for your reference. \n\n* https://machinelearningmastery.com/what-is-imbalanced-classification/\n* https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/\n* https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28\n\nOk, Now we have some idea about the target variable. Let's study next variable,\n\n#### 2. Favorite Color ","metadata":{}},{"cell_type":"code","source":"dataDf['Favorite Color'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.49059Z","iopub.execute_input":"2021-06-12T10:15:59.490936Z","iopub.status.idle":"2021-06-12T10:15:59.499858Z","shell.execute_reply.started":"2021-06-12T10:15:59.490879Z","shell.execute_reply":"2021-06-12T10:15:59.498714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valueCount(dataDf, 'Favorite Color')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.501709Z","iopub.execute_input":"2021-06-12T10:15:59.502016Z","iopub.status.idle":"2021-06-12T10:15:59.521949Z","shell.execute_reply.started":"2021-06-12T10:15:59.501968Z","shell.execute_reply":"2021-06-12T10:15:59.520643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Favorite Color', hue='Gender', data=dataDf)\n\nplt.xticks(rotation =40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.524263Z","iopub.execute_input":"2021-06-12T10:15:59.524758Z","iopub.status.idle":"2021-06-12T10:15:59.726163Z","shell.execute_reply.started":"2021-06-12T10:15:59.52468Z","shell.execute_reply":"2021-06-12T10:15:59.725094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable \"Favorite Color\" contains three values: \n* Cool - 56%\n* Warm - 33.3%\n* Netural - 10.6% \n\nAccording to data, we can see that more males like Cool and Natural colors when compared to females. On the other hand, more females like Warm colors when compared to males.\n\n#### 3. Favorite Music Genre","metadata":{}},{"cell_type":"code","source":"dataDf['Favorite Music Genre'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.728237Z","iopub.execute_input":"2021-06-12T10:15:59.728634Z","iopub.status.idle":"2021-06-12T10:15:59.740092Z","shell.execute_reply.started":"2021-06-12T10:15:59.728562Z","shell.execute_reply":"2021-06-12T10:15:59.738898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valueCount(dataDf, 'Favorite Music Genre')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.742743Z","iopub.execute_input":"2021-06-12T10:15:59.7437Z","iopub.status.idle":"2021-06-12T10:15:59.765417Z","shell.execute_reply.started":"2021-06-12T10:15:59.743634Z","shell.execute_reply":"2021-06-12T10:15:59.764476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Favorite Music Genre', hue='Gender', data=dataDf)\n\nplt.xticks(rotation =40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:15:59.767276Z","iopub.execute_input":"2021-06-12T10:15:59.767942Z","iopub.status.idle":"2021-06-12T10:16:00.078158Z","shell.execute_reply.started":"2021-06-12T10:15:59.76788Z","shell.execute_reply":"2021-06-12T10:16:00.077427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable \"Favorite Music Genre\" contains seven values: \n\n* Rock - 28.7%\n* Pop - 25.7%\n* Hip hop - 12%\n* Electronic - 12%\n* R&B and soul - 9%\n* Folk/Traditional - 6%\n* Jazz/Blues - 6%\n\nThe highest number of participants like to listen to Rock music. Most of the male participants like to listen to Hip hop, Electronic, R&B, and soul and most of the female participants like to listen to Pop and Jazz. Surprisingly, a higher number of female participants have mentioned that their favorite music genre is Rock when compared to male participants. \n\n#### 4. Favorite Beverage  ","metadata":{}},{"cell_type":"code","source":"dataDf['Favorite Beverage'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:00.079637Z","iopub.execute_input":"2021-06-12T10:16:00.080414Z","iopub.status.idle":"2021-06-12T10:16:00.090375Z","shell.execute_reply.started":"2021-06-12T10:16:00.080343Z","shell.execute_reply":"2021-06-12T10:16:00.089077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valueCount(dataDf, 'Favorite Beverage')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:00.09299Z","iopub.execute_input":"2021-06-12T10:16:00.093917Z","iopub.status.idle":"2021-06-12T10:16:00.122429Z","shell.execute_reply.started":"2021-06-12T10:16:00.093837Z","shell.execute_reply":"2021-06-12T10:16:00.121218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Favorite Beverage', hue='Gender', data=dataDf)\n\nplt.xticks(rotation =40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:00.124766Z","iopub.execute_input":"2021-06-12T10:16:00.125651Z","iopub.status.idle":"2021-06-12T10:16:00.400708Z","shell.execute_reply.started":"2021-06-12T10:16:00.125563Z","shell.execute_reply":"2021-06-12T10:16:00.399327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable \"Favorite Beverage\" contains six values: \n\n* Doesn't drink - 21.2%\n* Beer - 19.6%\n* Other - 16.6%\n* Wine - 15.5%\n* Vodka - 13.6%\n* Folk/Traditional - 6%\n* Whiskey - 13.6%\n\nThe highest number of participants do not drink and most of them are male participants. When compared with male participants higher a number of female participants like to drink Wine, Whiskey, and Other beverages. On the other hand, male participants like to drink Vodka and Beer. \n\n#### 4. Favorite Soft Drink  ","metadata":{}},{"cell_type":"code","source":"dataDf['Favorite Soft Drink'].unique()\t","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:00.402885Z","iopub.execute_input":"2021-06-12T10:16:00.403287Z","iopub.status.idle":"2021-06-12T10:16:00.414337Z","shell.execute_reply.started":"2021-06-12T10:16:00.403217Z","shell.execute_reply":"2021-06-12T10:16:00.413207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valueCount(dataDf, 'Favorite Soft Drink')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:00.416272Z","iopub.execute_input":"2021-06-12T10:16:00.416977Z","iopub.status.idle":"2021-06-12T10:16:00.436912Z","shell.execute_reply.started":"2021-06-12T10:16:00.416892Z","shell.execute_reply":"2021-06-12T10:16:00.435643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Favorite Soft Drink', hue='Gender', data=dataDf)\n\nplt.xticks(rotation =40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:00.438897Z","iopub.execute_input":"2021-06-12T10:16:00.439579Z","iopub.status.idle":"2021-06-12T10:16:00.714297Z","shell.execute_reply.started":"2021-06-12T10:16:00.439517Z","shell.execute_reply":"2021-06-12T10:16:00.713103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable \"Favorite Soft Drink\" contains four values: \n\n* Coca Cola/Pepsi - 48.4%\n* Fanta - 21.2%\n* 7UP/Sprite - 19.6%\n* Other - 10.6%\n\nMajority of male and female participants mentioned that their favorite soft drink is Coca Cola and Pepsi. Almost 40% of participants like to drink 7UP, Sprite, and Fanta. Now we've studied all the individual variables, let's expand our analysis for bivariate analysis.\n\n#### 5. Favorite Music Genre and Favorite Color","metadata":{}},{"cell_type":"code","source":"ax = sns.catplot(x='Favorite Music Genre', col='Favorite Color', kind='count', data=dataDf)\n\nax.set_xticklabels(rotation=40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:00.716215Z","iopub.execute_input":"2021-06-12T10:16:00.716846Z","iopub.status.idle":"2021-06-12T10:16:01.636413Z","shell.execute_reply.started":"2021-06-12T10:16:00.716779Z","shell.execute_reply":"2021-06-12T10:16:01.635185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the above graph, we can see that:\n1. Jazz/Blues is the only favorite music genre of participants who like warm colors.\n2. Participants who like natural colors are only like Rock, HipHop, and POP music.\n3. Higher number of participants who like cool colors also like Rock, POP, Hip hop, and Electronic music.\n\n#### 6. Favorite Music Genre and Favorite Color","metadata":{}},{"cell_type":"code","source":"ax = sns.catplot(x='Favorite Beverage', col='Favorite Color', kind='count', data=dataDf)\n\nax.set_xticklabels(rotation=40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:01.638358Z","iopub.execute_input":"2021-06-12T10:16:01.639005Z","iopub.status.idle":"2021-06-12T10:16:02.609215Z","shell.execute_reply.started":"2021-06-12T10:16:01.638938Z","shell.execute_reply":"2021-06-12T10:16:02.608015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to above graph, we can see that:\n1. Participants who like natural colors do not like to drink Whiskey.\n2. Favorite color of most of the participants who do not drink and who like to drink Beer is cool.\n3. Almost the same number of participants whose favorite colors are Cool and Warm also like to drink Whiskey and Wine.\n\n#### 7. Favorite Soft Drink and Favorite Color\n","metadata":{}},{"cell_type":"code","source":"ax = sns.catplot(x='Favorite Soft Drink', col='Favorite Color', kind='count', data=dataDf)\n\nax.set_xticklabels(rotation=40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:02.61116Z","iopub.execute_input":"2021-06-12T10:16:02.611822Z","iopub.status.idle":"2021-06-12T10:16:03.482361Z","shell.execute_reply.started":"2021-06-12T10:16:02.611752Z","shell.execute_reply":"2021-06-12T10:16:03.481105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to above graph, we can see that:\n\n1. Favourite color of most of the participants who like to drink Coca Cola/Pepsi is cool.\n2. Participants whose favorite color is natural do not like to drink soft drinks much.\n\n#### 8. Favorite Beverage and Favorite Soft Drink","metadata":{}},{"cell_type":"code","source":"ax = sns.catplot(x='Favorite Beverage', col='Favorite Soft Drink', kind='count', data=dataDf)\n\nax.set_xticklabels(rotation=40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:03.48446Z","iopub.execute_input":"2021-06-12T10:16:03.485173Z","iopub.status.idle":"2021-06-12T10:16:04.655387Z","shell.execute_reply.started":"2021-06-12T10:16:03.485099Z","shell.execute_reply":"2021-06-12T10:16:04.654225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to above graph, we can see that:\n1. Generally, participants who like to drink Coca Cola/Pepsi like Vodka, Wine, and Beer.\n2. Most of the participants who like to drink Fanta also like to drink Whiskey.\n\n<div class='alert alert-block alert-success' id='sdo'><strong>Small Dataset and Overfitting</strong></div>\n\nThe goal of a machine learning model is to generalize patterns in training data so that you can correctly predict new data that has never been presented to the model. In other words, we need to find a good balance between bias and variance such that it minimizes the total error. \n\n![](https://miro.medium.com/max/700/1*9hPX9pAO3jqLrzt0IE3JzA.png)\n\nModels trained on a small dataset tend to overfit and produce inaccurate results. Therefore, we have to avoid overfitting but how do we do it? we can use the below techniques:\n\n* [Use simple models](https://hackernoon.com/7-effective-ways-to-deal-with-a-small-dataset-2gyl407s)\n* [Remove outliers from data](https://www.kaggle.com/rafjaa/dealing-with-very-small-datasets)\n* [Select relevant features](https://hackernoon.com/7-effective-ways-to-deal-with-a-small-dataset-2gyl407s)\n* [Ensembling Techniques](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)\n* [Cross validation](https://elitedatascience.com/overfitting-in-machine-learning)\n","metadata":{}},{"cell_type":"markdown","source":"<div class='alert alert-block alert-success' id='dpp'><strong>Data Pre-Processing</strong></div>\n\nLet's convert all categorical variables into dummy variables. I use One-Hot Encoding to convert all variables since all of them are nominal.","metadata":{}},{"cell_type":"code","source":"dataDf = pd.get_dummies(dataDf, columns=['Gender', 'Favorite Color', 'Favorite Music Genre', 'Favorite Beverage', 'Favorite Soft Drink'], prefix=['gender', 'color', 'music', 'beverage', 'drink'], drop_first=True)\n\n#gender 1 = male, 0 = female\ndataDf.rename(columns={'gender_M': 'gender'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:04.657334Z","iopub.execute_input":"2021-06-12T10:16:04.657987Z","iopub.status.idle":"2021-06-12T10:16:04.683834Z","shell.execute_reply.started":"2021-06-12T10:16:04.657919Z","shell.execute_reply":"2021-06-12T10:16:04.682167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-block alert-success' id='mtt'><strong>Model Training and Testing</strong></div>\n\nOk, the dataset is ready it's time for train models. Like I discussed earlier, to avoid overfitting I use simple machine learning models and cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:04.685782Z","iopub.execute_input":"2021-06-12T10:16:04.68642Z","iopub.status.idle":"2021-06-12T10:16:05.657974Z","shell.execute_reply.started":"2021-06-12T10:16:04.686352Z","shell.execute_reply":"2021-06-12T10:16:05.65725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataDf.drop('gender', axis=1)\nY = dataDf['gender']","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:16:05.659172Z","iopub.execute_input":"2021-06-12T10:16:05.659572Z","iopub.status.idle":"2021-06-12T10:16:05.665048Z","shell.execute_reply.started":"2021-06-12T10:16:05.659531Z","shell.execute_reply":"2021-06-12T10:16:05.664374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 10-fold cross-validation technique is used to evaluate each algorithm, importantly configured with the same random seed to ensure that the same splits to the training data are performed and that each algorithms is evaluated in precisely the same way.\n\n* https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/","metadata":{}},{"cell_type":"code","source":"# prepare models\nmodels = []\n\nmodels.append(('LR', LogisticRegression(random_state=10)))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('DT', DecisionTreeClassifier(random_state=10)))\nmodels.append(('RF', RandomForestClassifier(random_state=10)))\n\naccuracies = []\nnames = []\nstds = []\n\n#loop through all models\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=10)\n    validateResults = cross_val_score(model, X, Y, cv=kfold)\n    \n    accuracies.append(validateResults.mean())\n    stds.append(validateResults.std())\n    names.append(name)\n\n#store results in dataframe\nresultsDf = pd.DataFrame({\n    'model': names,\n    'accuracy': accuracies,\n    'std': stds,\n})","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:32:57.760058Z","iopub.execute_input":"2021-06-12T10:32:57.760386Z","iopub.status.idle":"2021-06-12T10:32:58.098723Z","shell.execute_reply.started":"2021-06-12T10:32:57.760334Z","shell.execute_reply":"2021-06-12T10:32:58.097647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's store model name, mean accuracy, and standard deviation in new datafram.","metadata":{}},{"cell_type":"code","source":"resultsDf.sort_values(by=['accuracy'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:33:10.271983Z","iopub.execute_input":"2021-06-12T10:33:10.272328Z","iopub.status.idle":"2021-06-12T10:33:10.285212Z","shell.execute_reply.started":"2021-06-12T10:33:10.272272Z","shell.execute_reply":"2021-06-12T10:33:10.284259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot mean accuracy and standard deviation\nresultsDf.plot('model', style='.-', figsize=(10, 5))\n\nplt.xticks(range(len(resultsDf['model'])), resultsDf['model'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T10:33:17.344061Z","iopub.execute_input":"2021-06-12T10:33:17.344438Z","iopub.status.idle":"2021-06-12T10:33:17.599288Z","shell.execute_reply.started":"2021-06-12T10:33:17.344379Z","shell.execute_reply":"2021-06-12T10:33:17.598049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to above results, we can see that non of any model is able to provide accuracy higher than 60%. Among them, the Naive Bayes model is able to provide the highest accuracy 58.8% with a standard deviation of 27%. The standard deviation shows us, how precise the estimates are. This means in our case that the accuracy of our model can differ +-27%. I think the reason for the low accuracy is the size of the dataset. Therefore, if we need to achieve better results, I believe that we have to provide enough data.\n\nHope you've enjoyed my work, if you like my work and need to share something with me leave a comment :)","metadata":{}}]}