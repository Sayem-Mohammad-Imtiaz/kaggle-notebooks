{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# baseline cnn model for mnist\nfrom numpy import mean\nfrom numpy import std\nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten,Dropout\nfrom keras.optimizers import SGD\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/sign-language-mnist/amer_sign2.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/sign-language-mnist/amer_sign3.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/sign-language-mnist/american_sign_language.PNG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\ndf_train = pd.read_csv('../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nd = df_train.columns[1:]\ntrainX = df_train[d].values\ntrainY =  df_train['label'].values\ntestX = df_test[d].values\ntestY =  df_test['label'].values\ntrainX = trainX.reshape(trainX.shape[0], 28, 28,1)\ntestX = testX.reshape(testX.shape[0], 28, 28,1)\ntrainY = to_categorical(trainY)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_pixels(train, test):\n\t# convert from integers to floats\n\ttrain_norm = train.astype('float32')\n\ttest_norm = test.astype('float32')\n\t# normalize to range 0-1\n\ttrain_norm = train_norm / 255.0\n\ttest_norm = test_norm / 255.0\n\t# return normalized images\n\treturn train_norm, test_norm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom matplotlib import pyplot\nfor i in range(9):\n\t# define subplot\n\tpyplot.subplot(330 + 1 + i)\n\t# plot raw pixel data\n\tpyplot.imshow(trainX[i].reshape(28,28), cmap=pyplot.get_cmap('gray'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model():\n    #create the neural network model\n    model = Sequential()\n    model.add(Conv2D(128, (4, 4), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(Conv2D(64, (4, 4), activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dropout(0.4))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(25, activation='softmax'))\n    # compile model\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate a model using k-fold cross-validation\ndef evaluate_model(dataX, dataY, n_folds=5):\n    scores, histories = list(), list()\n    # prepare cross validation\n    kfold = KFold(n_folds, shuffle=True, random_state=1)\n    # enumerate splits\n    for train_ix, test_ix in kfold.split(dataX):\n    # define model\n        model = define_model()\n        # select rows for train and test\n        train_X, train_Y, test_X, test_Y = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n        # fit model\n        history = model.fit(train_X, train_Y, epochs=10, batch_size=32, validation_data=(test_X, test_Y), verbose=0)\n        # evaluate model\n        _, acc = model.evaluate(test_X, test_Y, verbose=0)\n        print('> %.3f' % (acc * 100.0))\n        # stores scores\n        model.save('final_model.h5')\n \n        scores.append(acc)\n        histories.append(history)\n        return scores, histories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot diagnostic learning curves\ndef summarize_diagnostics(histories):\n    for i in range(len(histories)):\n        # plot loss\n        plt.subplot(2, 1, 1)\n        plt.title('Cross Entropy Loss')\n        plt.plot(histories[i].history['loss'], color='blue', label='train')\n        plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n        # plot accuracy\n        plt.subplot(2, 1, 2)\n        plt.title('Classification Accuracy')\n        plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n        plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_test_harness():\n    #load the dataset\n   \n    # prepare pixel data\n    trainX1, testX1 = prep_pixels(trainX, testX)\n    # evaluate model\n    scores, histories = evaluate_model(trainX1, trainY)\n    # learning curves\n    summarize_diagnostics(histories)\n    # summarize estimated performance\n    #summarize_performance(scores)\n\n# entry point, run the test harness\nrun_test_harness()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX = df_test[d].values\ntestY =  df_test['label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX = testX.reshape(testX.shape[0], 28, 28,1)\nfrom keras import models    \nmodel = models.load_model('final_model.h5')\n\npred_y = np.argmax((model.predict(testX)),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(testY, pred_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}