{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import seaborn\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.base import TransformerMixin\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\nn_colors = 256 \npalette = sns.diverging_palette(20, 220, n=n_colors) # Create the palette\ncolor_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n\ndef value_to_color(val):\n    val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n    ind = int(val_position * (n_colors - 1)) # target index in the color palette\n    return palette[ind]\n\ndef heatmap(x, y, size):\n    fig, ax = plt.subplots()\n    \n    # Mapping from column names to integer coordinates\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    size_scale = 500\n    ax.scatter(\n        x=x.map(x_to_num), # Use mapping for x\n        y=y.map(y_to_num), # Use mapping for y\n        c=corr['value'].apply(value_to_color), # Vector of square color values, mapped to color palette  \n        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n        marker='s' # Use square as scatterplot marker\n    )\n\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    \n    ax.grid(False, 'major')\n    ax.grid(True, 'minor')\n    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) \n    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n    \n    \ndata = pd.read_csv('../input/health-care-data-set-on-heart-attack-possibility/heart.csv')\ncolumns = ['cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach' , 'exang', 'oldpeak'] \ncorr = data[columns].corr()\ncorr = pd.melt(corr.reset_index(), id_vars='index') \ncorr.columns = ['x', 'y', 'value']\nheatmap(\n    x=corr['x'],\n    y=corr['y'],\n    size=corr['value'].abs()\n)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Performing SHAP Analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = (data['target'] == 1)  \nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\nprint(\"Cool\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show] \ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nmy_model.predict_proba(data_for_prediction_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap \n\nexplainer = shap.TreeExplainer(my_model)\n\nshap_values = explainer.shap_values(data_for_prediction)\nprint(\"^o^\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nk_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\nk_shap_values = k_explainer.shap_values(data_for_prediction)\nshap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###LIME Implementation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(train_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring ='accuracy'\n# Model 1 - Logistic Regression\nmodel_lr = LogisticRegression()\nmodel_lr.fit(train_X, train_y)\n\naccuracy_score(val_y, model_lr.predict(val_X))\n\n# Model 2 - RandomForest Classifier\nmy_model = RandomForestClassifier()\nmy_model.fit(train_X, train_y)\naccuracy_score(val_y, my_model.predict(val_X))\n\n# Model 3 - XGB Classifier\nmodel_xgb = XGBClassifier()\nmodel_xgb.fit(train_X, train_y)\naccuracy_score(val_y, model_xgb.predict(val_X))\n\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X, y)\n\nmy_model = RandomForestClassifier()\nmy_model.fit(X, y)\n\nmodel_xgb = XGBClassifier()\nmodel_xgb.fit(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lime\nimport lime.lime_tabular\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}