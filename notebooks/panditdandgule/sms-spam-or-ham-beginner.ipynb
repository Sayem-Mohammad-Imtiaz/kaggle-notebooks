{"cells":[{"metadata":{"_uuid":"3cac5ebe5effbc8c22ced3bd85a3d3e827104cf6"},"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQUXi3mkDlIZMmaGJzZVQnEEC535eNtp3WbO5HzZMxhCcUwucLo)"},{"metadata":{"_uuid":"83e92e96c1b2693175187f170e874f80811b915b"},"cell_type":"markdown","source":"# **SMS: Spam or Ham (Beginner)**"},{"metadata":{"_uuid":"14f89fbf45c9e39030ace6d215aa377f75437f69"},"cell_type":"markdown","source":"For my first kernel on Natural Language Processing (NLP), I chose the SMS Spam Collection Dataset.  \nIt contains  the text of 5572 SMS messages and a label, classifying the message as \"spam\" or \"ham\".\n\nIn this kernel I explore some common techniques of NLP like:\n\n* **Removing Punctuation and Stopwords**\n* **Tokenizer, Bag of words**  \n* **Term frequency inverse document frequency (TFIDF)**\n\nBased on these preprocessing, I train 6 different models that classify **unknown** messages as spam or ham. \n\n* **Naive Bayes Classifier**\n* **SVM Classifier**  \n* **KNN Classifier**\n* **SGD Classifier**\n* **Gradient Boosting Classifier**\n* **XGBoost Classifier**\n\nFor easier handling of the preprocessing steps (for train and test data) and the optimization of different  \nmodels for the same conditions, the classification is done with **Pipelines** including GridSearchCV.  \nFinally, for the model evaluation different **metrics** are examined:  \naccuracy, precision, recall, fscore, roc_auc"},{"metadata":{"_uuid":"501639d7c1474c946d7c02b6d592e841fee03ea5","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66e435d174d98055e192cdfe780c814a3be8d89d"},"cell_type":"markdown","source":"**The Notebook follows this outline:**  "},{"metadata":{"_uuid":"252a2abb0a3ce947720af21d20922de2187151ee"},"cell_type":"markdown","source":"**Part 0: Imports, define functions**  \n[import libraries](#Imports)  \ndefine [functions](#Functions) that are used often  \n\n[**Part 1: Exploratory Data Analysis**](#Part-1:-EDA)  \n**1.1 Get an overview of the dataset**  \nhead, describe and value counts  \n[Distribution of the target variable](#Distribution-of-the-target-variable)  \n[Add numerical label for spam](#Add-numerical-label-for-spam)  \n**1.2 length of message**  \n[Add feature: length of message](#Add-feature:-length-of-message)  \n**1.3 WordClouds**  \n[WordCloud: Ham messages](#WordCloud:-Ham-messages)  \n[WordCloud: Spam messages](#WordCloud:-Spam-messages)  \n\n[**Part 2: Preprocessing**](#Part-2:-Preprocessing)  \n[**2.1 Remove punctuation and stopwords**](#2.1-Remove-Punctuation-and-Stopwords)   \n[**2.2 Top 30 words in ham and spam messages**](#2.2-Top-30-words-in-ham-and-spam-messages)  \n**2.3 Bag of words with CountVectorizer**  \n[The Bag of Words representation](#The-Bag-of-Words-representation)  \n[Examples for spam and ham messages](#Examples-for-spam-and-ham-messages)  \n[Applying bow_transformer on all messages](#Applying-bow_transformer-on-all-messages)  \n**2.4 Term frequency inverse document frequency (TFIDF)**  \n[From occurrences to frequencies](#From-occurrences-to-frequencies)  \n[TfidfTransformer from sklearn](#TfidfTransformer-from-sklearn)  "},{"metadata":{},"cell_type":"markdown","source":"\n[**Part 3: Classifiers**](#Part-3:-Classifiers)  \n[**3.1 First test for Classification**](#3.1-First-test-for-Classification) with Naive Bayes Classifier  \n[**3.2 train test split**](#3.2-train-test-split)  \n**3.3 Pipelines for Classification of unknown messages**  \n[Multinomial Naive Bayes](#3.3.1-MultinomialNB)  (simple: Preprocessing and Classification)  \n[KNN Classifier](#3.3.2-KNN)  (GridSearchCV for model parameter)   \n[Support Vector Classifier](#3.3.3-SVC)  (GridSearchCV for Preprocessing)  \n[SGD Classifier](#3.3.4-SGD)  (GridSearchCV for Preprocessing and model parameter)  \n[GradientBoostingClassifier](#3.3.5-GradientBoostingClassifier)    (GridSearchCV for Preprocessing and model parameter)  \n[XGBoost Classifier](#3.3.6-XGBoost-Classifier)    (GridSearchCV for Preprocessing and model parameter)  \n**3.4 Comparison of results**  \n[confusion_matrix](#confusion_matrix) +++ [accuracy_score](#accuracy_score)       \n[precision_score](#precision_score) +++ [recall_score](#recall_score)  \n[f1_score](#f1_score) +++  [classification_report](#classification_report)    \n[roc_auc_score](#roc_auc_score)  \n**3.5 Optimize classifiers with scoring by precision**  \n3.5.1 [GridSearchCV pipelines version 2](#3.5.1-GridSearchCV-pipelines-version-2)  \n3.5.2 [Confusion matrices for scoring by precision](#3.5.2-Confusion-matrices-for-scoring-by-precision)  \n**3.6 Optimize classifiers with scoring by recall**  \n3.6.1 [GridSearchCV pipelines version 3](#3.6.1-GridSearchCV-pipelines-version-3)  \n3.6.2 [Confusion matrices for scoring by recall](#3.6.2-Confusion-matrices-for-scoring-by-recall)  \n**3.7 Optimize classifiers with scoring by roc_auc**  \n3.7.1 [GridSearchCV pipelines version 4](#3.7.1-GridSearchCV-pipelines-version-4)    \n3.7.2 [Confusion matrices for scoring by roc auc](#3.7.2-Confusion-matrices-for-scoring-by-roc-auc)  \n\n[**Part 4: NLTK**](#Part-4:-NLTK)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"823e81881f13f82f83817a252b0fa834446f4412"},"cell_type":"markdown","source":"**TODO : **  \n \n\n**include feature text length in model**   \n\n**NLTK**"},{"metadata":{"trusted":true,"_uuid":"ffaf535a4250a7fbaa1c68b27fb55d9a96cd4c0c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**References:**  \nParts of the EDA and preprocessing are based on the Capstone Project in Jose Portilla's Udemy course.  \nI can recommend this course for beginners in Python ML.  \n"},{"metadata":{"trusted":true,"_uuid":"d3b53cdae8daf6bbb12abac40690e83ff28db5b2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d31e3961805d4b1205440831bacd4796a3d5e281"},"cell_type":"markdown","source":"# **Part 0: Imports, define functions** "},{"metadata":{"_uuid":"7e07c8e4ca9941fa0b409bbc48c79f4b432c9832"},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nimport wordcloud\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d042a0fd2b1b124201c84b367e5a3c811f3c7278"},"cell_type":"markdown","source":"## Functions"},{"metadata":{"_uuid":"21fed8f50a1f88bf2d614a7c193606c685beae8d"},"cell_type":"markdown","source":"print Classification Report and Accuracy"},{"metadata":{"trusted":true,"_uuid":"6d1507ee9876985f963650eba7fe84f87eefb076"},"cell_type":"code","source":"def print_validation_report(y_true, y_pred):\n    print(\"Classification Report\")\n    print(classification_report(y_true, y_pred))\n    acc_sc = accuracy_score(y_true, y_pred)\n    print(\"Accuracy : \"+ str(acc_sc))\n    \n    return acc_sc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72615b9c2a0f9c82cb80879df5783abc35da6d50"},"cell_type":"markdown","source":"plot_confusion_matrix"},{"metadata":{"trusted":true,"_uuid":"a54d45761deee27294673d1aeadebcc1c3eed146"},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", cbar=False, ax=ax)\n    #  square=True,\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeb01c9e17e719ec7fc159a92c865efda18a8d54"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"082306c6380a6bc0a2d1aa16843ab4200f011ebe"},"cell_type":"markdown","source":"# **Part 1: EDA**"},{"metadata":{"trusted":true,"_uuid":"5654c4b08e286b2833bc03d056878325041c75c9"},"cell_type":"code","source":"data = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb63ad287008768565b8f80617bb763e2ac5423d"},"cell_type":"markdown","source":"Columns 2,3,4 contain no important data and can be deleted.  \nAlso, we rename column v1 as \"label\" and v2 as \"text\""},{"metadata":{"trusted":true,"_uuid":"f412489f7e767443b6e656850cca153393f0cca5"},"cell_type":"code","source":"data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36b87f12ddaaf8db2d41ffd8d78b6a92a6634ad5"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa53b10c9da3d3e7d09fb0d29ac09d3ff84c83f6"},"cell_type":"code","source":"data.groupby(\"label\").describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ce5027974bdd19359abdeeb32dac9c436dcd0f2"},"cell_type":"markdown","source":"### Distribution of the target variable"},{"metadata":{"_uuid":"599f248cd4b81880ad40911f0b8c72edb89f924d"},"cell_type":"markdown","source":"The dataset contains 4825 ham and 747 spam messages.  \nFor both classes, some messages appear more than once (common phrases, etc.)."},{"metadata":{"trusted":true,"_uuid":"6a4c90bd39e5bb963d845562e66854455b9118ca"},"cell_type":"code","source":"data.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4f8372b931251cb5984f82969771619fbac7f2b"},"cell_type":"code","source":"data.label.value_counts().plot.bar();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6653bc13fd51bc3b17bea2acd62996f952a88880"},"cell_type":"markdown","source":"### Add numerical label for spam   \nTarget must be numerical for ML classification models"},{"metadata":{"trusted":true,"_uuid":"f81edcf4efc48b1a93add45419bdd6d1177a7dda"},"cell_type":"code","source":"data['spam'] = data['label'].map( {'spam': 1, 'ham': 0} ).astype(int)\ndata.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efe33ff44f565f771a2276235239db858f3ba1ee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 length of message"},{"metadata":{"_uuid":"5815aab27e942c1f49e9be291121fd145d405ad0"},"cell_type":"markdown","source":"### Add feature: length of message"},{"metadata":{"trusted":true,"_uuid":"676699099d15dc5ea90e7ad15ffd6f62c1033854"},"cell_type":"code","source":"data['length'] = data['text'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58d7dfbc59752599edeec691c73f86656c2cc4f0"},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"285f6d0fa94ecd4b114a2567e2b608159ce6c025"},"cell_type":"code","source":"data.hist(column='length',by='label',bins=60,figsize=(12,4));\nplt.xlim(-40,950);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8aa167a101b039fcbd6c2194b14b9cc0acee6e23"},"cell_type":"markdown","source":"Looks like spam messages are generally longer than ham messages:  \nBulk of ham has length below 100, for spam it is above 100.  \nWe will check if this feature is useful for the classification task in Part 3.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ham  = data[data['spam'] == 0].copy()\ndata_spam = data[data['spam'] == 1].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3 WordClouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_wordcloud(data_spam_or_ham, title):\n    text = ' '.join(data_spam_or_ham['text'].astype(str).tolist())\n    stopwords = set(wordcloud.STOPWORDS)\n    \n    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',\n                    colormap='viridis', width=800, height=600).generate(text)\n    \n    plt.figure(figsize=(10,7), frameon=True)\n    plt.imshow(fig_wordcloud)  \n    plt.axis('off')\n    plt.title(title, fontsize=20 )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud: Ham messages"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(data_ham, \"Ham messages\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud: Spam messages"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(data_spam, \"Spam messages\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc9f69c3e24cd2aa2f2559825a7ad4710e361115"},"cell_type":"markdown","source":"# **Part 2: Preprocessing**"},{"metadata":{"trusted":true,"_uuid":"da7836b09691c2ba2a96f9af3bc8662a1fa0d788"},"cell_type":"markdown","source":"**Basic preprocessing for common NLP tasks includes converting text to lowercase and removing punctuation and stopwords.**  \n**Further steps, especially for text classification tasks, are:**  \n* Tokenization\n* Vectorization and \n* TF-IDF weighting  \n\n**Lets apply these approaches on the SMS messages.**"},{"metadata":{"_uuid":"9a89e7cb53fb78632611598e27fcaadb4d00f127"},"cell_type":"markdown","source":"## 2.1 Remove Punctuation and Stopwords"},{"metadata":{"_uuid":"b0f0e77fc5ba947aa4585c259938064d464edfbe"},"cell_type":"markdown","source":"### Punctuation\n**We use the punctuation list from the string library:**"},{"metadata":{"trusted":true,"_uuid":"76654fd5e10f4c8a46d9d8c9dc5238e0a860d9af"},"cell_type":"code","source":"import string\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb9c3c22e0534e2630eed529b3989eac65a302b3"},"cell_type":"markdown","source":"### Stopwords  \nfrom sklearn documentation:  https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words  \nStop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text,   \nand which may be removed to avoid them being construed as signal for prediction.  \nSometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.  \n\nDue to the known issues in the ’english’ stop word list of sklearn, we use the stopwords from NLTK:"},{"metadata":{"_uuid":"b7fde6b90bb4cabe8eba93d07c95a91967062060"},"cell_type":"markdown","source":"**NLTK**"},{"metadata":{"trusted":true,"_uuid":"2b71df4ef3915bc73648a5380b29019a461305e7"},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords.words(\"english\")[100:110]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1815902b628a2ea8c7a65637767a5ca5ee161b1b"},"cell_type":"markdown","source":"**With the above lists for punctuation characters and stop words, we define a function to remove these from the text**  \n**This function also converts all text to lowercase**"},{"metadata":{"trusted":true,"_uuid":"29f3ba753b6fcd26816110fe86a6995f715ad178"},"cell_type":"code","source":"def remove_punctuation_and_stopwords(sms):\n    \n    sms_no_punctuation = [ch for ch in sms if ch not in string.punctuation]\n    sms_no_punctuation = \"\".join(sms_no_punctuation).split()\n    \n    sms_no_punctuation_no_stopwords = \\\n        [word.lower() for word in sms_no_punctuation if word.lower() not in stopwords.words(\"english\")]\n        \n    return sms_no_punctuation_no_stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe8cec520d8684c1cd481ea26505999576df8339"},"cell_type":"code","source":"data['text'].apply(remove_punctuation_and_stopwords).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Top 30 words in ham and spam messages"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1 Collections: Counter"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ham.loc[:, 'text'] = data_ham['text'].apply(remove_punctuation_and_stopwords)\nwords_data_ham = data_ham['text'].tolist()\ndata_spam.loc[:, 'text'] = data_spam['text'].apply(remove_punctuation_and_stopwords)\nwords_data_spam = data_spam['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_ham_words = []\nfor sublist in words_data_ham:\n    for item in sublist:\n        list_ham_words.append(item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_spam_words = []\nfor sublist in words_data_spam:\n    for item in sublist:\n        list_spam_words.append(item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_ham  = Counter(list_ham_words)\nc_spam = Counter(list_spam_words)\ndf_hamwords_top30  = pd.DataFrame(c_ham.most_common(30),  columns=['word', 'count'])\ndf_spamwords_top30 = pd.DataFrame(c_spam.most_common(30), columns=['word', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_hamwords_top30, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_spamwords_top30, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2 NLTK: FreqDist"},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist_ham  = nltk.FreqDist(list_ham_words)\nfdist_spam = nltk.FreqDist(list_spam_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hamwords_top30_nltk  = pd.DataFrame(fdist_ham.most_common(30),  columns=['word', 'count'])\ndf_spamwords_top30_nltk = pd.DataFrame(fdist_spam.most_common(30), columns=['word', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_hamwords_top30_nltk, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_spamwords_top30_nltk, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22567bbef309b303857f55083a088c8f38069a50"},"cell_type":"markdown","source":"## 2.2 Bag of words with CountVectorizer"},{"metadata":{"_uuid":"b4f59da6ef1965d82e67a5bcc4784382d393f9f0"},"cell_type":"markdown","source":"### The Bag of Words representation"},{"metadata":{"_uuid":"0d3879613c6ea148a9e7b832e9e84e832bf37073"},"cell_type":"markdown","source":"https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n\nText Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.  \nIn order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n\n**Tokenization**  \ntokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.  \n**Vectorization**  \ncounting the occurrences of tokens in each document.  \n**TF-IDF**  \nnormalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.  \n\n\n**Bag of Words**  \nIn this scheme, features and samples are defined as follows:\neach individual token occurrence frequency (normalized or not) is treated as a feature.  \nthe vector of all the token frequencies for a given document is considered a multivariate sample.  \nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.  \nWe call vectorization the general process of turning a collection of text documents into numerical feature vectors.   \nThis specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or “Bag of n-grams” representation.  \nDocuments are described by word occurrences while completely ignoring the relative position information of the words in the document."},{"metadata":{"_uuid":"9c2d71c7f0ce91b0c88c8def0d19f514a9f73fc0"},"cell_type":"markdown","source":"For futher details and example implementations see:  \nhttps://en.wikipedia.org/wiki/Bag-of-words_model  \nhttps://en.wikipedia.org/wiki/Document-term_matrix  \n\nAn Introduction to Bag-of-Words in NLP  \nhttps://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428"},{"metadata":{"_uuid":"d3ff0bbb14d8550b32f36fc04bddc8cf081d9087"},"cell_type":"markdown","source":"In this kernel we apply the CountVectorizer from sklearn as BOW model.  \nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html    \nAs tokenizer we use the remove_punctuation_and_stopwords function defined above"},{"metadata":{"trusted":true,"_uuid":"eda2cf09b93bc33d893d4133515ddef9f3dbf6d3"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer(analyzer = remove_punctuation_and_stopwords).fit(data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92ca036b1a95efd0c8fbd336b747a4868a960fcb"},"cell_type":"code","source":"print(len(bow_transformer.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"069f3f06649b4851e2188aade8bf16611c2ff98b"},"cell_type":"markdown","source":"In all sms messages bow_transformer counted 9431 different words."},{"metadata":{"trusted":true,"_uuid":"b3a4998219d1f29059a895c04c18c888f715ad17"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e4f6a7394403f076dd6eef9a8927e47746370a1"},"cell_type":"markdown","source":"### Examples for spam and ham messages"},{"metadata":{"_uuid":"bf3023555a79d34bf050cbbb428198eb91c7119f"},"cell_type":"markdown","source":"Lets look at some vectorization examples for spam and ham messages"},{"metadata":{"trusted":true,"_uuid":"aed6b2ad047f291b51847d3dbe9d7fb48edea4fd"},"cell_type":"code","source":"sample_spam = data['text'][8]\nbow_sample_spam = bow_transformer.transform([sample_spam])\nprint(sample_spam)\nprint(bow_sample_spam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, cols = bow_sample_spam.nonzero()\nfor col in cols: \n    print(bow_transformer.get_feature_names()[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8780104e357122de05157333eff275d46526c62"},"cell_type":"code","source":"print(np.shape(bow_sample_spam))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f6d3995c2a0b364fed178d399ae36a0cf6133d2"},"cell_type":"code","source":"sample_ham = data['text'][4]\nbow_sample_ham = bow_transformer.transform([sample_ham])\nprint(sample_ham)\nprint(bow_sample_ham)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, cols = bow_sample_ham.nonzero()\nfor col in cols: \n    print(bow_transformer.get_feature_names()[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f78df8555292ca47c4000ea29214044a8a6f193e"},"cell_type":"markdown","source":"### Applying bow_transformer on all messages"},{"metadata":{"trusted":true,"_uuid":"7afebe912b171c75db39ff6dd2b2f326bb2ab33e"},"cell_type":"code","source":"bow_data = bow_transformer.transform(data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c346737685259d8f2b70a9e5f49b77bbd81892a9"},"cell_type":"code","source":"bow_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1a8ddbe80a38a8e3b233c769657f4adbc95dcce"},"cell_type":"code","source":"bow_data.nnz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f54d816940a9447989ad3add6b2849953a9a3e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c050910c4505978b6abf83e638b9efb6b46fc959"},"cell_type":"markdown","source":"**Sparsity: percentage of none zero entries**  \nhttps://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n**Sparsity**  \nAs most documents will typically use a very small subset of the words used in the corpus,  \nthe resulting matrix will have many feature values that are zeros (typically more than 99% of them).  \nFor instance a collection of 10,000 short text documents (such as emails) will use a vocabulary  \nwith a size in the order of 100,000 unique words in total while each document will use 100 to   \n1000 unique words individually.  \nIn order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector,  \nimplementations will typically use a sparse representation such as available in the scipy.sparse package.\n"},{"metadata":{"trusted":true,"_uuid":"cb69e61eb5066af722d1d4389dd2a8afaaa80f5b"},"cell_type":"code","source":"bow_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9386c74ce10b6c8c5b1c7d21413d3c5019e00606"},"cell_type":"code","source":"bow_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"147aecb10addaee63be626fa299555654a0449b2"},"cell_type":"code","source":"bow_data.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0a67e9663c17edf16718b8ecc4d40be2f15345f"},"cell_type":"code","source":"bow_data.nnz","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"946ea9e4f865aba3e6d28da6b326c6e0b7b3dd0d"},"cell_type":"markdown","source":"number of none zero entries divided by matrix size  "},{"metadata":{"trusted":true,"_uuid":"de2aee91877b45651de69cdf576ee79a5bd6c04a"},"cell_type":"code","source":"print( bow_data.nnz / (bow_data.shape[0] * bow_data.shape[1]) *100 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c58e4914d2bfc63fe69c94e1e02a8db14378cb6b"},"cell_type":"markdown","source":"Around 10% of the matrix are non zeros (=ones)"},{"metadata":{"trusted":true,"_uuid":"9e6af25790a2dc1d236703d4692eaef1a6e4ccfb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34171014a4d1bbbb89f635fa0f28561140162eef"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8968ae791a11a649e589b95236f8515d12d0dba1"},"cell_type":"markdown","source":"## 2.3 Term frequency inverse document frequency - TFIDF"},{"metadata":{"_uuid":"cecafb740fb49406220b1c354ce9a429e822e1e7"},"cell_type":"markdown","source":"### From occurrences to frequencies  \nhttps://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#from-occurrences-to-frequencies\n\nOccurrence count is a good start but there is an issue: longer documents will have higher average count values  \nthan shorter documents, even though they might talk about the same topics.  \nTo avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document  \nby the total number of words in the document: these new features are called **tf for Term Frequencies**.  \nAnother refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are  \ntherefore less informative than those that occur only in a smaller portion of the corpus.  \nThis downscaling is called **tf–idf for “Term Frequency times Inverse Document Frequency”**."},{"metadata":{"_uuid":"e9d280f9fcc8a25e59e1dca52d5de26dc3ba3de1"},"cell_type":"markdown","source":"For futher details and example implementations see:  \nhttps://en.wikipedia.org/wiki/Tf%E2%80%93idf"},{"metadata":{"_uuid":"c3f25088775f0caa568fef7164a0475d27f5302a"},"cell_type":"markdown","source":"https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments"},{"metadata":{"trusted":true,"_uuid":"574b03a52794e4124d47e653f44320e29f73a169"},"cell_type":"markdown","source":"### TfidfTransformer from sklearn\nBoth tf and tf–idf can be computed as follows using TfidfTransformer:   \nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"},{"metadata":{"trusted":true,"_uuid":"bc362886dcc6db5f68e03031d87dece50b0b500f"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(bow_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba16200dd77c59a8e50f1270920c50c1c386a01d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f80ca5b3bbdf2a5f89e5046fd5c7097c5a9c1d8f"},"cell_type":"code","source":"tfidf_sample_ham = tfidf_transformer.transform(bow_sample_ham)\nprint(tfidf_sample_ham)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c50ea5d0fad1e579741b3d879d4201ad059d3c1a"},"cell_type":"code","source":"tfidf_sample_spam = tfidf_transformer.transform(bow_sample_spam)\nprint(tfidf_sample_spam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb6c826546bba11a45d88a6e65bb18a231ba0eb1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71d9302f4d0895922b5d7197338bf482410279e2"},"cell_type":"code","source":"data_tfidf = tfidf_transformer.transform(bow_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46472347f3522d5669bf6dd48fd759570b079378"},"cell_type":"code","source":"data_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(data_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train test split"},{"metadata":{},"cell_type":"markdown","source":"for TFIDF matrix only"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_tfidf_train, data_tfidf_test, label_train, label_test = \\\n    train_test_split(data_tfidf, data[\"spam\"], test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tfidf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c060c8eb5dcc435b48c5b767a89a17c962893295"},"cell_type":"code","source":"data_tfidf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for TFIDF matrix and feature \"length\""},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import  hstack\nX2 = hstack((data_tfidf ,np.array(data['length'])[:,None])).A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2_train, X2_test, y2_train, y2_test = \\\n    train_test_split(X2, data[\"spam\"], test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f206a0ea10a5997e7509bccd25e2d72e045f56ae"},"cell_type":"markdown","source":"# Part 3: Classifiers"},{"metadata":{"trusted":true,"_uuid":"347dcda838abc7c4b452cee43076d430606a52e4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c4181849789efae863fee973649f7827f2bd20e"},"cell_type":"markdown","source":"## 3.1 First test for Classification  "},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Classifier"},{"metadata":{},"cell_type":"markdown","source":"sparse matrix to matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tfidf_train = data_tfidf_train.A\ndata_tfidf_test = data_tfidf_test.A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MNB Model using only TFIDF matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_detect_model = MultinomialNB().fit(data_tfidf_train, label_train)\npred_test_MNB = spam_detect_model.predict(data_tfidf_test)\nacc_MNB = accuracy_score(label_test, pred_test_MNB)\nprint(acc_MNB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cbc3816802cc4cfe056dd7d6c906955b84526fd"},"cell_type":"markdown","source":"Our first classifier seems to work well, it has an accuracy of 96.5 % for the test set.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\ndata_tfidf_train_sc = scaler.fit_transform(data_tfidf_train)\ndata_tfidf_test_sc  = scaler.transform(data_tfidf_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MNB Model using only TFIDF matrix, scaled"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_detect_model_minmax = MultinomialNB().fit(data_tfidf_train_sc, label_train)\npred_test_MNB = spam_detect_model_minmax.predict(data_tfidf_test_sc)\nacc_MNB = accuracy_score(label_test, pred_test_MNB)\nprint(acc_MNB)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying the min max scaler on the TFIDF matrix improves the performance of the MNB classifier:  \nIt now has an accuracy of 98.2 % for the test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MNB model with TFIDF matrix and feature \"length\", unscaled"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_detect_model_2 = MultinomialNB().fit(X2_train, y2_train)\npred_test_MNB_2 = spam_detect_model_2.predict(X2_test)\nacc_MNB_2 = accuracy_score(y2_test, pred_test_MNB_2)\nprint(acc_MNB_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting MNB with the unscaled features TFIDF + length of message decreases performance.  \nLets now check the fit with the scaled features."},{"metadata":{"trusted":true},"cell_type":"code","source":"X2_tfidf_train = X2_train[:,0:9431]\nX2_tfidf_test  = X2_test[:,0:9431]\nX2_length_train = X2_train[:,9431]\nX2_length_test  = X2_test[:,9431]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX2_tfidf_train = scaler.fit_transform(X2_tfidf_train)\nX2_tfidf_test  = scaler.transform(X2_tfidf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX2_length_train = scaler.fit_transform(X2_length_train.reshape(-1, 1))\nX2_length_test  = scaler.transform(X2_length_test.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2_train = np.hstack((X2_tfidf_train, X2_length_train))\nX2_test  = np.hstack((X2_tfidf_test,  X2_length_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MNB model with TFIDF matrix and feature \"length\", scaled"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_detect_model_3 = MultinomialNB().fit(X2_train, y2_train)\npred_test_MNB_3 = spam_detect_model_3.predict(X2_test)\nacc_MNB_3 = accuracy_score(y2_test, pred_test_MNB_3)\nprint(acc_MNB_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We studied the same classifier, Multinomial Naive Bayes, with different set of features and found that the results vary regarding the accuracy of the predictions.  \nIn the following we study a different classifier, again with different set of features.\nAlso we study what this accuracy actually means and also if this metric is the optimal one we should apply for this task."},{"metadata":{},"cell_type":"markdown","source":"KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters_KNN = {'n_neighbors': (10,15,17), }\n\ngrid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\n\ngrid_KNN.fit(data_tfidf_train, label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01480b009d97fd3167f36e02123f8db5abf19bb4"},"cell_type":"code","source":"print(grid_KNN.best_params_)\nprint(grid_KNN.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters_KNN = {'n_neighbors': (6,8,10), }\ngrid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\ngrid_KNN.fit(data_tfidf_train_sc, label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_KNN.best_params_)\nprint(grid_KNN.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1627a592628703b79643715903239d9bac9e8321"},"cell_type":"markdown","source":"## 3.2 train test split"},{"metadata":{},"cell_type":"markdown","source":"for text data"},{"metadata":{"trusted":true,"_uuid":"44e528fc9bbd846cbd235a7d9f6425d88fb5114e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nsms_train, sms_test, label_train, label_test = \\\n    train_test_split(data[\"text\"], data[\"spam\"], test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77e34b5a7b77cf0a0c1a0e111f6fd3df4a7d6410"},"cell_type":"code","source":"sms_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10423816907b50e971d57f2e5ca231f810d4c4ac","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0f7c4b43b754d92c49907e2460f4316181469b6"},"cell_type":"markdown","source":"## 3.3 Classification Pipelines"},{"metadata":{"_uuid":"e9de6be8ac49c14d38d68a16b01a0d987de546c3"},"cell_type":"markdown","source":"After splitting the data into a train and test set we now use a pipeline to apply the   \n**CountVectorizer** and the **TfidfTransformer** on both sets.  \nWe also add a classifier to the pipeline, so we can combine all necessary steps in one object:  \n* Preprecocessing  \n* Crossvalidation (GridsearchCV)\n* Fitting  \n* Predicting\n* Evaluating (test score)"},{"metadata":{"trusted":true,"_uuid":"0a46deb491be34f6996ba25d7413b81fd22d7359"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96adf620c8154e6c879193e03c6682e90a51a15c"},"cell_type":"markdown","source":"### 3.3.1 MultinomialNB"},{"metadata":{},"cell_type":"markdown","source":"**simple Pipeline. no optimization**"},{"metadata":{"trusted":true,"_uuid":"73c6e99b7980f41cfd7d44f0e999eb15d4561f23"},"cell_type":"code","source":"pipe_MNB = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_MNB' , MultinomialNB()),\n                    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test score**"},{"metadata":{"trusted":true,"_uuid":"35e991c44093fe5f5bb72f65398c36f28c06402b"},"cell_type":"code","source":"pipe_MNB.fit(X=sms_train, y=label_train)\npred_test_MNB = pipe_MNB.predict(sms_test)\nacc_MNB = accuracy_score(label_test, pred_test_MNB)\nprint(acc_MNB)\nprint(pipe_MNB.score(sms_test, label_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two steps  \n**CountVectorizer** and **TfidfTransformer**  \ncan also be performed in one step with  \n**TfidfVectorizer**  \nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  \nConvert a collection of raw documents to a matrix of TF-IDF features  \nEquivalent to CountVectorizer followed by TfidfTransformer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_MNB_tfidfvec = Pipeline([ ('tfidf_vec' , TfidfVectorizer(analyzer = remove_punctuation_and_stopwords)),\n                               ('clf_MNB'   , MultinomialNB()),\n                            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_MNB_tfidfvec.fit(X=sms_train, y=label_train)\npred_test_MNB_tfidfvec = pipe_MNB_tfidfvec.predict(sms_test)\nacc_MNB_tfidfvec = accuracy_score(label_test, pred_test_MNB_tfidfvec)\nprint(acc_MNB_tfidfvec)\nprint(pipe_MNB_tfidfvec.score(sms_test, label_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Yes, results are identical**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.2 KNN  \nPipeline with GridSearchCV  \noptimize best model parameter: n_neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_KNN = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_KNN' , KNeighborsClassifier() )\n                    ])\n\nparameters_KNN = {'clf_KNN__n_neighbors': (8,15,20), }\n\ngrid_KNN = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\n\ngrid_KNN.fit(X=sms_train, y=label_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**best_params_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**cross validation score: best_score_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_grid_KNN = grid_KNN.predict(sms_test)\nacc_KNN = accuracy_score(label_test, pred_test_grid_KNN)\nprint(acc_KNN)\nprint(grid_KNN.score(sms_test, label_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa3ebc8288ee6bfe9c391e4db9a20802e5fc1f8e"},"cell_type":"markdown","source":"### 3.3.3 SVC  \nPipeline with GridSearchCV  \nsearch best preprocessing: apply TfidfTransformer (yes/no)"},{"metadata":{"trusted":true,"_uuid":"c9528ec0130d4354a93f75459f0d223ee05a5a99"},"cell_type":"code","source":"pipe_SVC = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_SVC' , SVC(gamma='auto', C=1000)),\n                    ])\n\n\nparameters_SVC = dict(tfidf=[None, TfidfTransformer()],\n                      clf_SVC__C=[500, 1000,1500]\n                      )\n#parameters = {'tfidf__use_idf': (True, False),    }\n\ngrid_SVC = GridSearchCV(pipe_SVC, parameters_SVC, \n                        cv=5, n_jobs=-1, verbose=1)\n\ngrid_SVC.fit(X=sms_train, y=label_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**best_params_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SVC.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**cross validation score: best_score_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SVC.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_grid_SVC = grid_SVC.predict(sms_test)\nacc_SVC = accuracy_score(label_test, pred_test_grid_SVC)\nprint(acc_SVC)\nprint(grid_SVC.score(sms_test, label_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.4 SGD  \nPipeline with GridSearch  \nsearch best preprocessing: use_idf (yes/no)  \nand best model parameters (alpha, penalty)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_SGD = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_SGD' , SGDClassifier(random_state=5)),\n                    ])\n\nparameters_SGD = {\n    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    'tfidf__use_idf': (True, False),\n    #'tfidf__norm': ('l1', 'l2'),\n    #'clf_SGD__max_iter': (5,10),\n    'clf_SGD__alpha': (1e-05, 1e-04),\n}\n\ngrid_SGD = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                               n_jobs=-1, verbose=1)\n\ngrid_SGD.fit(X=sms_train, y=label_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**best_params_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SGD.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**cross validation score: best_score_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SGD.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_grid_SGD = grid_SGD.predict(sms_test)\nacc_SGD = accuracy_score(label_test, pred_test_grid_SGD)\nprint(acc_SGD)\nprint(grid_SGD.score(sms_test, label_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.5 GradientBoostingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_GBC = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                      ('tfidf'   , TfidfTransformer() ),\n                      ('clf_GBC' , GradientBoostingClassifier(random_state=5) ),\n                    ])\n\nparameters_GBC = { 'tfidf__use_idf': (True, False), \n                   'clf_GBC__learning_rate': (0.1, 0.2),\n                   #'clf_GBC__min_samples_split': (3,5), \n                 }\n\ngrid_GBC = GridSearchCV(pipe_GBC, parameters_GBC, \n                        cv=5, n_jobs=-1, verbose=1)\n\ngrid_GBC.fit(X=sms_train, y=label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GBC.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GBC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_grid_GBC = grid_GBC.predict(sms_test)\nacc_GBC = accuracy_score(label_test, pred_test_grid_GBC)\nprint(acc_GBC)\nprint(grid_GBC.score(sms_test, label_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.6 XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\n# Set params['eval_metric'] = ...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_XGB = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                      ('tfidf'   , TfidfTransformer() ),\n                      ('clf_XGB' , xgb.XGBClassifier(random_state=5) ),\n                    ])\n\nparameters_XGB = { 'tfidf__use_idf': (True, False), \n                   'clf_XGB__eta': (0.01, 0.02),\n                   'clf_XGB__max_depth': (5,6), \n                 }\n\ngrid_XGB = GridSearchCV(pipe_XGB, parameters_XGB, \n                        cv=5, n_jobs=-1, verbose=1)\n\ngrid_XGB.fit(X=sms_train, y=label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_XGB.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_XGB.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_grid_XGB = grid_XGB.predict(sms_test)\nacc_XGB = accuracy_score(label_test, pred_test_grid_XGB)\nprint(acc_XGB)\nprint(grid_XGB.score(sms_test, label_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff0832c9bdd34f6ee98c672c80f94d45877fd63c"},"cell_type":"markdown","source":"## 3.4 Comparison of results"},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019  \n\nhttps://scikit-learn.org/stable/modules/model_evaluation.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a binary classification task, there are 4 possible results:\n\n\nTN: True negatives  (ham mails labeled as ham)  \nFP: False positives (ham mails labeled as spam)  \nFN: False negatives (spam mails labeled as ham)  \nTP: True positives  (spam mails labeled as spam)  "},{"metadata":{},"cell_type":"markdown","source":"### confusion_matrix"},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix gives an overview of the classification results:  \nThe diagonal elements represent the number of points for which the predicted label is equal to the true label,  \nwhile off-diagonal elements are those that are mislabeled by the classifier.  \nThe higher the diagonal values of the confusion matrix the better, indicating many correct predictions.  \nThe rows of a confusion matrix correspond to the true (actual) classes and the columns correspond to the predicted classes.  \nSo, all together the confusion matrix for a binary classifier consists of 4 values:"},{"metadata":{},"cell_type":"markdown","source":"TN FP  \nFN TP\n"},{"metadata":{},"cell_type":"markdown","source":"**Using seaborn heat map for nice plot of confusion matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", square=True, cbar=False)\n    #  \n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dictionary of predictions**"},{"metadata":{"trusted":true,"_uuid":"62513e57732c87ae4a8d2ad765bdbe967a9a2ffb"},"cell_type":"code","source":"list_clf = [\"MNB\", \"KNN\", \"SVC\", \"SGD\", \"GBC\", \"XGB\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN, \n             pred_test_grid_SVC, pred_test_grid_SGD,\n             pred_test_grid_GBC, pred_test_grid_XGB]\n\ndict_pred = dict(zip(list_clf, list_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_all_confusion_matrices(y_true, dict_all_pred, str_title):\n    \n    list_classifiers = list(dict_all_pred.keys())\n    plt.figure(figsize=(10,7.5))\n    plt.suptitle(str_title, fontsize=20, fontweight='bold')\n    n=231\n\n    for clf in list_classifiers : \n        plt.subplot(n)\n        plot_confusion_matrix(y_true, dict_all_pred[clf])\n        plt.title(clf, fontweight='bold')\n        n+=1\n\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.9)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_all_confusion_matrices(label_test, dict_pred, \"Pipelines v1, scoring=accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### accuracy_score"},{"metadata":{},"cell_type":"markdown","source":"classification accuracy = correct predictions / total predictions = (TP + TN) / (TP + TN + FP + FN)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_acc = {}\nfor clf in list_clf :\n    dict_acc[clf] = accuracy_score(label_test, dict_pred[clf])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \" , dict_acc[clf])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the accuracy_score with the confusion matrices, one finds that  \naccuracy score may not be the best parameter to choose the best classifier.  \nSGD, a model with high accuracy_score, incorrectly classifies 6 ham mails as spam,  \nwhich is usually not wanted for a spam classifier (important mails might get lost).  \nMNB has less accuracy than SGD, but it classifies all ham mails correctly.  \nSVC also classifies all ham mails correctly but compared to MNB it classifies   \nmuch more spam mails correctly.  \nApart from accuracy there are further scoring methods to evaluate a classifier.  \nLets look at the other classifier scores in more detail:  \nprecision, recall, fscore, support, roc_auc"},{"metadata":{},"cell_type":"markdown","source":"### precision_score"},{"metadata":{},"cell_type":"markdown","source":"The precision is the ratio TP / (TP + FP) where TP is the number of true positives and TP the number of false positives.  \nThe precision is intuitively the ability of the classifier not to label as positive a sample that is negative.  \nPrecision = 1 for FP = 0 and precision goes up when FP goes down."},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", precision_score(label_test, dict_pred[clf]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By definition the precision is calculated for the negative class (label = 0, ham mails).  \nThis is also the default when calling precision score without any further parameters.  \nBut we can also examine the precision for the individual labels (ham,spam = 0,1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", precision_score(label_test, dict_pred[clf], average=None, labels=[0,1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Precision for classifying ham mails is 1.0 for the MNB and SVC classifier.  \nSGD has the best precision for classifying ham mails."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### recall_score"},{"metadata":{},"cell_type":"markdown","source":"The recall is the ratio TP / (TP + FN) where TP is the number of true positives and FN the number of false negatives.  \nThe recall is intuitively the ability of the classifier to find all the positive samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", recall_score(label_test, dict_pred[clf]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall is defined regarding the positive class (label=1, spam mails).  \nAgain, if we call the recall score method with the labels parameter, we get  \nthe recall for ham and spam messages:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", recall_score(label_test, dict_pred[clf], average=None, labels=[0,1] ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### f1_score"},{"metadata":{},"cell_type":"markdown","source":"The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall,  \nwhere an F-beta score reaches its best value at 1 and worst score at 0.  \nThe F-beta score weights recall more than precision by a factor of beta.  \nbeta == 1.0 means recall and precision are equally important."},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", f1_score(label_test, dict_pred[clf]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", f1_score(label_test, dict_pred[clf], average=None, labels=[0,1] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### classification_report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(label_test, pred_test_MNB))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The support is the number of occurrences of each class in y_true."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### precision_recall_fscore_support"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"4f0ebc9406c2e41b585ab2ce572f6cc5d0a351fc"},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", precision_recall_fscore_support(label_test, dict_pred[clf], average=None, labels=[0,1] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### roc_auc_score"},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in list_clf :\n    print(clf, \" \", roc_auc_score(label_test, dict_pred[clf] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics\nsklearn.metrics.SCORERS.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e4855f548722b8d2e45d17856870bed7f8abd76"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5 Optimize classifiers with scoring by precision"},{"metadata":{},"cell_type":"markdown","source":"We perform GridSearchCV again, using the same parameter grids and pipelines like before.  \nFor all classifier pipelines, we only change the scoring method from \"accuracy\" to \"precision\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'precision'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5.1 GridSearchCV pipelines version 2"},{"metadata":{},"cell_type":"markdown","source":"MNB"},{"metadata":{},"cell_type":"markdown","source":"The precision for MNB was already 1.0 so it can not be improved."},{"metadata":{},"cell_type":"markdown","source":"KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN_2 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_KNN_2.fit(X=sms_train, y=label_train)\npred_test_grid_KNN_2 = grid_KNN_2.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN_2.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SVC_2 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SVC_2.fit(X=sms_train, y=label_train)\npred_test_grid_SVC_2 = grid_SVC_2.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SGD_2 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SGD_2.fit(X=sms_train, y=label_train)\npred_test_grid_SGD_2 = grid_SGD_2.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GBC"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GBC_2 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_GBC_2.fit(X=sms_train, y=label_train)\npred_test_grid_GBC_2 = grid_GBC_2.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_XGB_2 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_XGB_2.fit(X=sms_train, y=label_train)\npred_test_grid_XGB_2 = grid_XGB_2.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5.2 Confusion matrices for scoring by precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_clf = [\"MNB\", \"KNN_2\", \"SVC_2\", \"SGD_2\", \"GBC_2\", \"XGB_2\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN_2, \n             pred_test_grid_SVC_2, pred_test_grid_SGD_2,\n             pred_test_grid_GBC_2, pred_test_grid_XGB_2]\n\ndict_pred_2 = dict(zip(list_clf, list_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_all_confusion_matrices(label_test, dict_pred_2, \"Pipelines v2, scoring=precision\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6 Optimize classifiers with scoring by recall"},{"metadata":{},"cell_type":"markdown","source":"For spam detection optimizing tbe classifiers by precision seems most reasonable.  \nBut for other tasks it may be advantageous to have a classifier with maximum recall.  \nFor example, in Credit Card Fraud detections, you want to find all fraud samples.  \nFor all classifier pipelines, we perform GridSearchCV again, using the same parameter grids  \nand only changing the scoring method to \"recall\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'recall'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6.1 GridSearchCV pipelines version 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MNB"},{"metadata":{},"cell_type":"markdown","source":"TODO:  \nparamgrid for MNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN_3 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_KNN_3.fit(X=sms_train, y=label_train)\npred_test_grid_KNN_3 = grid_KNN_3.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN_3.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SVC_3 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SVC_3.fit(X=sms_train, y=label_train)\npred_test_grid_SVC_3 = grid_SVC_3.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SGD_3 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SGD_3.fit(X=sms_train, y=label_train)\npred_test_grid_SGD_3 = grid_SGD_3.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GBC"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GBC_3 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_GBC_3.fit(X=sms_train, y=label_train)\npred_test_grid_GBC_3 = grid_GBC_3.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_XGB_3 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_XGB_3.fit(X=sms_train, y=label_train)\npred_test_grid_XGB_3 = grid_XGB_3.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6.2 Confusion matrices for scoring by recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_clf = [\"MNB\", \"KNN_3\", \"SVC_3\", \"SGD_3\", \"GBC_3\", \"XGB_3\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN_3, \n             pred_test_grid_SVC_3, pred_test_grid_SGD_3,\n             pred_test_grid_GBC_3, pred_test_grid_XGB_3]\n\ndict_pred_3 = dict(zip(list_clf, list_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_all_confusion_matrices(label_test, dict_pred_3, \"Pipelines v3, scoring=recall\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.7 Optimize classifiers with scoring by roc_auc_score"},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'roc_auc'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.7.1 GridSearchCV pipelines version 4"},{"metadata":{},"cell_type":"markdown","source":"MNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN_4 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_KNN_4.fit(X=sms_train, y=label_train)\npred_test_grid_KNN_4 = grid_KNN_4.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN_4.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thr = roc_curve(label_test, grid_KNN_4.predict_proba(sms_test)[:,1])\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc_knn4 = auc(fpr, tpr) * 100\nplt.legend([\"AUC {0:.3f}\".format(auc_knn4)]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SVC_4 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SVC_4.fit(X=sms_train, y=label_train)\npred_test_grid_SVC_4 = grid_SVC_4.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_SGD_4 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SGD_4.fit(X=sms_train, y=label_train)\npred_test_grid_SGD_4 = grid_SGD_4.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GBC"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GBC_4 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_GBC_4.fit(X=sms_train, y=label_train)\npred_test_grid_GBC_4 = grid_GBC_4.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thr = roc_curve(label_test, grid_GBC_4.predict_proba(sms_test)[:,1])\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc_gbc4 = auc(fpr, tpr) * 100\nplt.legend([\"AUC {0:.3f}\".format(auc_gbc4)]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_XGB_4 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_XGB_4.fit(X=sms_train, y=label_train)\npred_test_grid_XGB_4 = grid_XGB_4.predict(sms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.7.2 Confusion matrices for scoring by roc auc"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_clf = [\"MNB\", \"KNN_4\", \"SVC_4\", \"SGD_4\", \"GBC_4\", \"XGB_4\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN_4, \n             pred_test_grid_SVC_4, pred_test_grid_SGD_4,\n             pred_test_grid_GBC_4, pred_test_grid_XGB_4]\n\ndict_pred_4 = dict(zip(list_clf, list_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_all_confusion_matrices(label_test, dict_pred_4, \"Pipelines v4, scoring=roc auc\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 4: NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['text'][7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sent_tokenize(data['text'][7]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_tokenize(data['text'][7]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopWords = set(stopwords.words('english'))\nwords = word_tokenize(data['text'][7])\nwordsFiltered = []\n\nfor w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)\n\nprint(wordsFiltered)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}