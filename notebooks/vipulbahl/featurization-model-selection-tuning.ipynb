{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Pre-steps 1: Import the necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import zscore\nimport math as math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n%matplotlib inline\n# sns.set(color_codes=True)\nsns.set(style=\"darkgrid\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pre-Step2: Load the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#step 2.1: Read the dataset\nrdata=pd.read_csv('/kaggle/input/yeh-concret-data/Concrete_Data_Yeh.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deliverable -1 (Exploratory data quality report)"},{"metadata":{},"cell_type":"markdown","source":"#### 1.a Univariate analysis\nUnivariate analysis â€“ data types and description of the independent attributes which should include (name, meaning, range of values observed, central values (mean and median), standard deviation and quartiles, analysis of the body of distributions / tails, missing values, outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# step 2.1: browse through the first few columns\nrdata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\n\n##### Dataset:\n- The dataset is composed of the actual concrete compressive strength (MPa) for a given mixture under a specific age (days). \n- The observations have been determined through a laboratory.\n- The Data is in raw form and is not scaled and has 8 quantitative input variables, and 1 quantitative output variable\n\n\n##### The main aim of the project is to create a Model of strength of high performance concrete"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2.2: Understand the shape of the data\nshape_data=rdata.shape\nprint('The shape of the dataframe is',shape_data,'which means there are',shape_data[0],'rows of observations and',shape_data[1],'attributes of age, ingredients and concrete compression strength')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2.3: Identify Duplicate records in the data \n# It is very important to check and remove data duplicates. \n# Else our model may break or report overly optimistic / pessimistic performance results\ndupes=rdata.duplicated()\nprint(' The number of duplicates in the dataset are:',sum(dupes), '\\n')\ndupes_record=pd.DataFrame(rdata[dupes])\nprint(' The duplicate observations are:') \ndupes_record","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# step 2.3.1: Remove duplicates from the data\nt1data=rdata.copy()\nt1data=t1data.drop_duplicates(keep=\"first\")\n\ndupes1=t1data.duplicated()\nprint(' The number of duplicates in the new dataset are:',sum(dupes1), '\\n',\n      'Clearly evident that now there are no duplicates in the dataset.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1data.columns=['cement','slag','ash','water','superplastic','coarseagg','fineagg','age','strength']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shape_t1data=t1data.shape\nprint('The shape of the new dataframe is',shape_t1data,'which means there are',shape_t1data[0],'rows of observations and',shape_data[1],'attributes of age, ingredients and concrete compression strength')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2.4: Lets analyze the data types\nt1data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Refering the summary of the dataframe as above; In the dataset, all the columns appear to be of numerical data with data type Integer and float. There are no null values.<br>\n\nThe dataset contains the following variables:\n\nindependet variables are as below:\n\n- Cement : measured in kg in a m3 mixture\n- Blast : measured in kg in a m3 mixture\n- Fly ash : measured in kg in a m3 mixture\n- Water : measured in kg in a m3 mixture\n- Superplasticizer : measured in kg in a m3 mixture\n- Coarse Aggregate : measured in kg in a m3 mixture\n- Fine Aggregate : measured in kg in a m3 mixture\n- Age : day (1~365)\n\nThe dependent variable is :<br>\n\n- Strength: Concrete compressive strength measured in MPa<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA 1: lets evaluate statistical details of the dataset. \ncname=t1data.columns\ndata_desc=t1data.describe().T\ndata_desc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the describe summary above: <br>\n1. Cement: \n    - The range of cement is [102,540] with a median of 265. The mean is more that median which means that there could be slight skewness on the right.\n    - There might be few outliars on the right; we will plot a box plot to confirm the same. <br><br>\n\n2. slag: \n    - The range of slag is [0,359.4] with a median of 20. The mean is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n3. ash: \n    - The range of ash is [0,200.1] with a median of 0. The mean is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n4. water: \n    - The range of water is [121.8,247] with a median of 185.7. The mean is slightly less than median which means that there could be skewness on the left.\n    - There doesnt appear to be any outliars. We will identify by plotting a box plot.<br><br>\n    \n5. Superplastic: \n    - The range of superplastic is [0,32.2] with a median of 6.1. The mean is almost equal to median which means that there might not be any skewness.\n    - There doesnt appear to be any outliars. We will identify by plotting a box plot.<br><br>\n    \n6. Coarseagg: \n    - The range of coarseagg is [801, 1145] with a median of 968. The mean is slightly more than median which means that there might be skewness on the right.\n    - There doesnt appear to be any outliars. We will identify by plotting a box plot.<br><br>\n    \n7. fineagg: \n    - The range of coarseagg is [594,992.6] with a median of 780. The mean is slightly less than median which means that there might be skewness on the left.\n    - There doesnt appear to be any outliars. We will identify by plotting a box plot.<br><br>\n    \n8. age: \n    - The range of age is [1,365] with a median of 28. The mean is more than median which means that there might be skewness on the right.\n    - There might be outliars. We will identify by plotting a box plot.<br><br>\n    \n9. strength: \n    - The range of age is [2.33,82.6] with a median of 33.8. The mean is more than median which means that there might be skewness on the right.\n    - There might be outliars. We will identify by plotting a box plot.<br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Attributes in the Group\nAtr1g1='cement'\nAtr2g1='slag'\nAtr3g1='ash'\nAtr4g1='water'\nAtr5g1='superplastic'\nAtr6g1='coarseagg'\nAtr7g1='fineagg'\nAtr8g1='age'\nAtr9g1='strength'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA 1: Outliar Detection leveraging Box Plot\ndata=t1data\nfig, ax = plt.subplots(1,9,figsize=(38,16)) \nsns.boxplot(x=Atr1g1,data=data,ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g1,data=data,ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g1,data=data,ax=ax[2],orient='v')\nsns.boxplot(x=Atr4g1,data=data,ax=ax[3],orient='v')\nsns.boxplot(x=Atr5g1,data=data,ax=ax[4],orient='v')\nsns.boxplot(x=Atr6g1,data=data,ax=ax[5],orient='v')\nsns.boxplot(x=Atr7g1,data=data,ax=ax[6],orient='v')\nsns.boxplot(x=Atr8g1,data=data,ax=ax[7],orient='v')\nsns.boxplot(x=Atr9g1,data=data,ax=ax[8],orient='v')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n- Attributes slag, water, superplastic, fineagg, age and strength have outliars.\n- we will work on the outliars in the next section"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=t1data\n#EDA 2: Skewness check\nAtr1g1_skew=round(stats.skew(data[Atr1g1]),4)\nAtr2g1_skew=round(stats.skew(data[Atr2g1]),4)\nAtr3g1_skew=round(stats.skew(data[Atr3g1]),4)\nAtr4g1_skew=round(stats.skew(data[Atr4g1]),4)\nAtr5g1_skew=round(stats.skew(data[Atr5g1]),4)\nAtr6g1_skew=round(stats.skew(data[Atr6g1]),4)\nAtr7g1_skew=round(stats.skew(data[Atr7g1]),4)\nAtr8g1_skew=round(stats.skew(data[Atr8g1]),4)\nAtr9g1_skew=round(stats.skew(data[Atr9g1]),4)\n\nprint(' The skewness of',Atr1g1,'is', Atr1g1_skew)\nprint(' The skewness of',Atr2g1,'is', Atr2g1_skew)\nprint(' The skewness of',Atr3g1,'is', Atr3g1_skew)\nprint(' The skewness of',Atr4g1,'is', Atr4g1_skew)\nprint(' The skewness of',Atr5g1,'is', Atr5g1_skew)\nprint(' The skewness of',Atr6g1,'is', Atr6g1_skew)\nprint(' The skewness of',Atr7g1,'is', Atr7g1_skew)\nprint(' The skewness of',Atr8g1,'is', Atr8g1_skew)\nprint(' The skewness of',Atr9g1,'is', Atr9g1_skew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Attribute: Age has high skewness.<br>\n- Atributes: cement, slag, superplastic have slight skewness.<br>\n- Attributes: ash, water, coarseagg, fineagg, strength have negligible skewness.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"##EDA 3: Spread\ndata=t1data\nfig, ax = plt.subplots(1,9,figsize=(16,8)) \nsns.distplot(data[Atr1g1],ax=ax[0]) \nsns.distplot(data[Atr2g1],ax=ax[1]) \nsns.distplot(data[Atr3g1],ax=ax[2])\nsns.distplot(data[Atr4g1],ax=ax[3])\nsns.distplot(data[Atr5g1],ax=ax[4])\nsns.distplot(data[Atr6g1],ax=ax[5])\nsns.distplot(data[Atr7g1],ax=ax[6])\nsns.distplot(data[Atr8g1],ax=ax[7])\nsns.distplot(data[Atr9g1],ax=ax[8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Understanding the distribution of different attributes.\n- cement: There is tail on the right hand side and it is nearly normally distributed.\n- slag: Slag seems to have bi-modal distribution meaning it has 2 Gaussian. There also appear to be a tail on the right. This might mean that their might be extreme value or possibility of outliars\n- ash: Ash seems to have bi-modal distribution meaning it has 2 Gaussian. There also appear to be a tail on the right. This might mean that their might be extreme value or possibility of outliars\n- water: Water seems to have 3 Gaussian. There also appear to be a tail on the left. This might mean that their might be extreme value or possibility of outliars on the left.\n- superplastic: Superplastic seems to have bi-modal distribution meaning it has 2 Gaussian. There also appear to be a tail on the right. This might mean that their might be extreme value or possibility of outliars.\n- coarseagg: Coarseagg seems to have bi-modal distribution meaning it has 2 Gaussian. Coarseagg doesnt seem to have any tail and hence it appears that this attribute might not have any outliars.\n- fineagg: Fineagg seems to have bi-modal distribution meaning it has 2 Gaussian. There also appear to be a slight tail on the right. This might mean that their might be extreme value or possibility of outliars\n- age: The attribute age seems to have 5 Gaussian. There also appear to be a tail on the right. This might mean that their might be extreme value or possibility of outliars.\n- strength: strength has a near normal distribution with a slight tail on the right; indicating possibility of extreme values or outlairs."},{"metadata":{},"cell_type":"markdown","source":"#### 1.b Multivariate analysis \nBi-variate analysis between the predictor variables and between the predictor variables and target column.Comment on your findings in terms of their relationship and degree of relation if any. Presence of leverage points. Visualize the analysis using boxplots and pair plots,histograms or density curves. Select the most appropriate attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2.6: Lets visually understand if there is any correlation between the independent variables. \nusecols =[i for i in t1data.columns if i != 'strength']\nsns.pairplot(t1data,diag_kind='kde');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n- On the diagonals we have changed the plot to a density plot. The default is histogram.\n- The different plots signifies how the different attributes interact with each other; how they depend on each other\n- It is very important to solve the interdependence, since all the algorithms assume that all attributes are independent of each other. So if there is interdependence between the attributes then our model will perform sub-optimally in production.\n- It appears that in the dataset there are 2 Gaussians or 2 clusters hidden in the data set as density plot of multiple attributes has bi-modal distribution.\n- We will do cluster analysis to understand any hidden patterns or hidden clusters in the data set. Hence, to begin with we will consider that the dataset has 3-4 clusters (as seen from the pair plot; there are multiple attributes with atleast 2 Gaussians)\n\n\nFrom the scatter plots between different attributes; it appears that there isn't any significant correlation between attributes. We will calculate correlation to ascertain the same.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2.7: lets evaluate correlation between different attributes.\n# The dependent attribute strength has been ignored from the correlation heatmap. \n#The reason for the same will be explained in the next section.\ncorr=t1data.corr()\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr,annot=True,linewidth=0.05,ax=ax, fmt= '.2f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As observed while analysing pairplot; there doesnt seem to be very high correlation between independent attributes. However, the following attributes appear to have some correlation\n- There appear to correlation between Superplastic and water\n- There seem to be some correlation between water and fineagg\n- There seem to be some correlation between cement and strength\n- There seem to be some correlation between ash and superplastic\n\nScaling: There are 2 scales in the independent attributes (kg/m3 and days). We will scale the data in the next sections"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Analyzing Dependent variable (Strength) vs Independent variable (cement, age and water)\nfig, ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(y=\"strength\", x=\"cement\", hue=\"water\", size=\"age\", data=t1data, ax=ax, sizes=(50, 300),\n                palette='RdYlGn', alpha=0.9)\nax.set_title(\"Strength vs Cement, Age, Water\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"observation:\n- Strength correlates positively with Cement\n- Strength correlates positively with Age, though less than Cement\n- Older Cement tends to require more Water, as shown by the larger green data points\n- Strength correlates negatively with Water\n- High Strength with a low Age requires more Cement"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Analyzing Dependent variable (Strength) vs Independent variable (FineAgg, Ash, Superplastic)\nfig, ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(y=\"strength\", x=\"fineagg\", hue=\"ash\", size=\"superplastic\", data=t1data, ax=ax, sizes=(50, 300),\n                palette='RdYlBu', alpha=0.9)\nax.set_title(\"Strength vs FineAgg, Ash, Superplastic\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1)) # Moved outside the chart so it doesn't cover any data\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"observation:\n- strength doesnt have any clear correlation with ash\n- strength correlates positively with superplastic"},{"metadata":{},"cell_type":"markdown","source":"#### 1.c Outliar Addressal\nPick one strategy to address the presence outliers and missing values and perform necessary imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# before proceeding further, lets first create a copy of the data-set.\n# we will use simple imputer and strategy as median for addressing outliars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t2data = t1data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA 2: Outliar Detection leveraging Box Plot\ndata=t2data\nfig, ax = plt.subplots(1,9,figsize=(38,16)) \nsns.boxplot(x=Atr1g1,data=data,ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g1,data=data,ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g1,data=data,ax=ax[2],orient='v')\nsns.boxplot(x=Atr4g1,data=data,ax=ax[3],orient='v')\nsns.boxplot(x=Atr5g1,data=data,ax=ax[4],orient='v')\nsns.boxplot(x=Atr6g1,data=data,ax=ax[5],orient='v')\nsns.boxplot(x=Atr7g1,data=data,ax=ax[6],orient='v')\nsns.boxplot(x=Atr8g1,data=data,ax=ax[7],orient='v')\nsns.boxplot(x=Atr9g1,data=data,ax=ax[8],orient='v')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outliar_detection(col):\n    Q1=t2data[col].quantile(0.25)\n    Q3=t2data[col].quantile(0.75)\n    IQR=Q3-Q1\n    Lower_Whisker = Q1-1.5*IQR\n    Upper_Whisker = Q3+1.5*IQR\n    t2data[col][t2data[col]> Upper_Whisker] = np.nan\n    t2data[col][t2data[col]< Lower_Whisker] = np.nan\n    return t2data[col][t2data[col].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in usecols:\n    outliar_detection(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t2data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing the missing values with median\ncolumns=t2data.columns\nimp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n\nimp_median.fit_transform(t2data)\n# imp_median.fit(t2data)\nt2data=pd.DataFrame(imp_median.transform(t2data))\nt2data.columns=columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=t2data\nfig, ax = plt.subplots(1,9,figsize=(38,16)) \nsns.boxplot(x=Atr1g1,data=data,ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g1,data=data,ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g1,data=data,ax=ax[2],orient='v')\nsns.boxplot(x=Atr4g1,data=data,ax=ax[3],orient='v')\nsns.boxplot(x=Atr5g1,data=data,ax=ax[4],orient='v')\nsns.boxplot(x=Atr6g1,data=data,ax=ax[5],orient='v')\nsns.boxplot(x=Atr7g1,data=data,ax=ax[6],orient='v')\nsns.boxplot(x=Atr8g1,data=data,ax=ax[7],orient='v')\nsns.boxplot(x=Atr9g1,data=data,ax=ax[8],orient='v')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we remove outliers and replace with median, the distribution shape changes, the standard deviation becomes tighter creating new outliers. The new outliers would be much closer to the centre than original outliers so we accept them without modifying them"},{"metadata":{},"cell_type":"markdown","source":"### Deliverable -2 (Feature Engineering techniques)"},{"metadata":{},"cell_type":"markdown","source":"#### 2.a Identify opportunities (if any) to create a composite feature, drop a feature etc."},{"metadata":{},"cell_type":"markdown","source":"##### 2.a.i Creation of a composite feature."},{"metadata":{},"cell_type":"markdown","source":"Feature Engineering: \n- A key component for testing strength and durability of concrete mix is the water to cement ratio.\n- A lower ratio leads to higher strength and durability.\n- Since this attribute doesnt exist in the dataset; hence we will compute and add this attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"t2data['w/c ratio']=t2data['water']/t2data['cement']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t2data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 2.a.ii Deletion of features.\n - We will leverage PCA for dimensionality reduction.\n - The approach we are going to follow is; we will build 2 sets of models; \n     - One without deletion of any attributes and \n     - Second - reducing attributes by leveraging PCA\n     - They we will decide whether we will recommend the model with dimensionality reduction or without dimensionality reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is always better to make a copy of the data before applying any transformation on data\nt3data=t2data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t3data_scaled=t3data.apply(zscore)\nX_scaled=t3data_scaled.drop('strength',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covMatrix = np.cov(X_scaled,rowvar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choosing PCA components to be 8 and fitting it on the scaled data. \n#The count of 8 has been selected randomly to check the variance explained by 8 components; \n#We will finalize the components basis the count of components required to explain 95% variance\npca = PCA(n_components=8)\npca.fit(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computing the eigen Values\nprint(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets compute the eigen Vectors\nprint(pca.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(list(range(1,9)),pca.explained_variance_ratio_,alpha=0.5, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.step(list(range(1,9)),np.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cumulating explained variance ratio to identify how many principal components are required to explain 95% of the variance\ncum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n# print(\"Cumulative Variance Explained\", cum_var_exp)\npd.DataFrame(cum_var_exp,columns=['Cumul Variance Explanation'],index=['1','2','3','4','5','6','7','8'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6 components explains over 95% of the variance. Hence we will take 6 components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca6 = PCA(n_components=6)\npca6.fit(X_scaled)\nprint(pca6.components_)\nprint(pca6.explained_variance_ratio_)\nXpca6 = pca6.transform(X_scaled)\nY = t3data_scaled['strength']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pca, X_test_pca, y_train_pca, y_test_pca=train_test_split(Xpca6,Y,test_size=0.30,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check split of data\n\nprint(\"{0:0.2f}% data is in training set\".format((len(X_train_pca)/len(t3data.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(X_test_pca)/len(t3data.index)) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 2.b Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help"},{"metadata":{},"cell_type":"markdown","source":"we will begin our model building considering linear regression; basis the performance of the algorithm, we will try other model. We will also try polynomial regression algorithm with different degree of freedom. The model building and analysing the best model will be done in step 3.\n\nWe will train the following regression algorithms:\n1. Linear Regression\n2. SVR\n3. Ridge Regression\n4. Lasso Regression\n5. Polynomial Regression\n5. Decision Tree\n6. Random Forest\n7. Bagging\n8. Ada Boost\n9. Gradient Boost"},{"metadata":{},"cell_type":"markdown","source":"##### 2.c Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and present your findings in terms of the independent attributes and their suitability to predict strength"},{"metadata":{},"cell_type":"markdown","source":"Referring the pair plot in section 1:\n- It appears that in the dataset there are 3 to 4 Gaussians since for multiple variables (slag, ash, superplastic) there are 2 or more gaussians or 2 clusters hidden in the data set as density plot of multiple attributes has bi-modal distribution.\n- We will do cluster analysis to understand any hidden patterns or hidden clusters in the data set. Hence, to begin with we will consider that the dataset has 2-6 clusters (as seen from the pair plot; there are atleast 2 Gaussians). Then we will finalize the cluster leveraging elbow plots."},{"metadata":{},"cell_type":"markdown","source":"We will use K-means clustering. We dont know how many clusters to look for; we got a hint that the number of clusters are likely to be in the range of 2 to 6. So lets explore the range of 2 to 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"kdata=t2data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# expect 3 to four clusters from the pair plot visual inspection hence restricting from 2 to 5\n\ncluster_range = range( 2, 6 )\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(kdata)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elbow plot to ascertain the number of clusters\n\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The elbow plot confirms our visual analysis that there are likely 3 good clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kdata_z = kdata.apply(zscore)\n\ncluster = KMeans( n_clusters = 3, random_state = 1 )\ncluster.fit(kdata_z)\n\nprediction=cluster.predict(kdata_z)\nkdata_z[\"GROUP\"] = prediction     # Creating a new column \"GROUP\" which will hold the cluster id of each record\n\nkdata_z_copy = kdata_z.copy(deep = True)  # Creating a mirror copy for later re-use instead of building repeatedly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids = cluster.cluster_centers_\ncentroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroid_df = pd.DataFrame(centroids, columns = list(kdata) )\ncentroid_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kdata_z.boxplot(by = 'GROUP',figsize = (40,18), layout = (2,15));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### we notice that there are outliars; However, we had resolved outliards earlier and replaced with median. When we solve for outliars, the distribution shape changes, the standard deviation becomes tighter creating new outliers. The new outliers would be much closer to the centre than original outliers so we accept them without modifying them. Hence, we will ignore these outliars"},{"metadata":{},"cell_type":"markdown","source":"Lets analyse the variable at cluster level.\nAt cluster level, we want to understand how strength is impact by different attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"### strength Vs cement\n\nvar = 'cement'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The more horizontal the line is, the more weak the independent variable is in predicting the target variable\n\nObservation:\n   - For cluster 2 (orange line) and cluster 3 (green line), there seem to be some positive relationship between strength and cement.\n   - cluster 1 represented by blue line appear to be straight line which means that for cluster 1, strength is weakly predicted by cement. \n   - So cement may not be a good predictor for all the 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# strength Vs water\n\nvar = 'water'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - For group 1 (blue) and group 2 (orange), there seem to be some negative relationship between strength and water\n   - group 3 represented by green line appear have a positive relationship\n   - So 2 clusters seem to have negative relationship while 1 cluster seem to positive relationship. Hence, water may not be a good predictor for all the 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# strength Vs fineagg\n\nvar = 'fineagg'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - group 3 (green line) seems to represent good relationship between strength and fineagg\n   - For group 1 (blue) and group 2 (orange), there appear to be straight line which means that for group 1 and 2, strength is weakly predicted by fineagg. \n   - So fineagg may not be a good predictor for all the 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# strength Vs slag\n\nvar = 'slag'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - For all 3 clusters, there appear to be straight line which means that for group 1,2 and 3 strength is weakly predicted by slag. \n   - So slag may not be a good predictor for all the 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# strength Vs ash\n\nvar = 'ash'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - group 3 (green line) seems to represent slight postive relationship between strength and ash\n   - For group 2 (orange), there appear to be straight line which means that for group 1 and 2, strength is weakly predicted by ash.\n   - For group 1 (blue line), there appear to be slight negative relationship between strengh and ash\n   - So ash may not be a good predictor for all the 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# strength Vs Superplasticizer\n\nvar = 'superplastic'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - For cluster 2 (orange line) and cluster 3 (green line), there appears to be slight relationship between superplastic and strength.\n   - For cluster 1 (blue), there appear to be straight line which means that for group 1, strength is weakly predicted by superplastic.\n   - So superplastic may not be a good predictor for all the 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# strength Vs Coarse Aggregate\n\nvar = 'coarseagg'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - For all 3 clusters, there appear to be straight line which means that for group 1,2 and 3 strength is weakly predicted by coarseagg. \n   - So coarseagg may not be a good predictor for all the 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# strength Vs age\n\nvar = 'age'\n\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'strength',data=kdata_z,hue='GROUP')\nplot.set(ylim = (-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - For all 3 clusters, there appear to be some relationship between strength and age. Hence, age can be considered as an attribute which can predict strength for all 3 clusters."},{"metadata":{},"cell_type":"markdown","source":"### 3. Deliverable -3 (create the model )"},{"metadata":{},"cell_type":"markdown","source":"#### 3.a Obtain feature importance for the individual features and present your findings"},{"metadata":{},"cell_type":"markdown","source":"The feature importance was seen in section 2.a when we performed Principal component analysis. There is another way to identify feature importance which is through decision tree regressor. We will identify feature importance while building model using decision tree regressor."},{"metadata":{},"cell_type":"markdown","source":"In this step, We will build multiple Algorithms and then basis the performance, we will decide the Algorithm that gives the best performance in section 4. However, Before we proceed further, lets scale the data and then divide the data into train and test"},{"metadata":{},"cell_type":"markdown","source":"From the dataset it is quite evident that for independent variables, there are 2 scales e.g: kg/m3, days. Now, Machine learning algorithms dont recognize the unit of data; Hence, it won't be prudent to compare Kg/m3 with age. Higher ranging numbers in one of the attributes will have superiority. \n10 kg/m3 and 10 days means different but machine learning algorithm understand both to be the same.<br>\n\nScales impacts\n1. gradient descent based algorithms like Linear Regression, Logistics Regression\n2. Distance based algorithms like KNN, K-means and SVM\n\nScales dont impact:\n1. Tree based algorithms like Decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets build our regression model\n# before proceeding further we'll scale the data so that we can analyse them further\n# Linear models are not impacted by scaling; however when we use regularization models like ridge and lasso; they are impacted by scaling. \n# Hence, to be on safe side lets scaling the data; since we might use regulaization of the data.\n\nt2data_scaled=t2data.apply(zscore)\nX=t2data_scaled.drop('strength',axis=1)\ny = t2data_scaled['strength']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# splitting the data into train and test\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.30,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check split of data\nprint(\"{0:0.2f}% data is in training set\".format((len(X_train)/len(t2data.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(X_test)/len(t2data.index)) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will train the following regression algorithms:\n1. Linear Regression\n2. SVR\n3. Ridge Regression\n4. Lasso Regression\n5. Polynomial Regression\n5. Decision Tree\n6. Random Forest\n7. Bagging\n8. Ada Boost\n9. Gradient Boost"},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 1: Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model on train data\nregression_model = LinearRegression()\nregression_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us explore the coefficients for each of the independent attributes\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[idx]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intercept = regression_model.intercept_\n\nprint(\"The intercept for our model is {}\".format(regression_model.intercept_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_model.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_LR= regression_model.score(X_test, y_test)\nscore_LR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_model_pca = LinearRegression()\nregression_model_pca.fit(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_model_pca.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intercept_pca = regression_model_pca.intercept_\n\nprint(\"The intercept for our model is {}\".format(regression_model_pca.intercept_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_LR_pca = regression_model_pca.predict(X_test_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_model_pca.score(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_LR_PCA = regression_model_pca.score(X_test_pca, y_test_pca)\nscore_LR_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 2: SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = svm.SVR()\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_SVR = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_SVR = clf.score(X_test, y_test)\nscore_SVR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_pca = svm.SVR() \nclf_pca.fit(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_pca.score(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_SVR_PCA=clf_pca.score(X_test_pca, y_test_pca)\nscore_SVR_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 3: Ridge Regression: Regularised Linear Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.fit(X_train,y_train)\nprint(\"Ridge model:\",ridge.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_ridge = ridge.score(X_test,y_test)\nscore_ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_pca = Ridge(alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_pca.fit(X_train_pca,y_train_pca)\nprint(\"Ridge model:\",ridge.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_pca.score(X_train_pca,y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_ridge_PCA = ridge_pca.score(X_test_pca,y_test_pca)\nscore_ridge_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 4: Lasso Regression - Regularised Linear Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso=Lasso(alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.fit(X_train,y_train)\nprint(\"Lasso Model\",lasso.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lasso = lasso.score(X_test,y_test)\nscore_lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_pca=Lasso(alpha=0.1)\nlasso_pca.fit(X_train_pca,y_train_pca)\nprint(\"Lasso Model\",lasso.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_pca.score(X_train_pca,y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lasso_PCA=lasso_pca.score(X_test_pca, y_test_pca)\nscore_lasso_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 5: Polynomial Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(degree=2, interaction_only=True)\nX_train3 = poly.fit_transform(X_train)\nX_test3 = poly.fit_transform(X_test)\n\npoly_clf = linear_model.LinearRegression()\n\npoly_clf.fit(X_train3, y_train)\n\ny_pred = poly_clf.predict(X_test3)\n\n#print(y_pred)\n\n#In sample (training) R^2 will always improve with the number of variables!\nprint(poly_clf.score(X_train3, y_train))\nscore_LR_poly = poly_clf.score(X_test3, y_test)\nscore_LR_poly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_pca = PolynomialFeatures(degree=4, interaction_only=True)\nX_train_poly = poly_pca.fit_transform(X_train_pca)\nX_test_poly = poly_pca.fit_transform(X_test_pca)\n\npoly_clf_pca = linear_model.LinearRegression()\n\npoly_clf_pca.fit(X_train_poly, y_train_pca)\n\ny_pred = poly_clf_pca.predict(X_test_poly)\n\n#print(y_pred)\n\n#In sample (training) R^2 will always improve with the number of variables!\nprint(poly_clf_pca.score(X_train_poly, y_train_pca))\nscore_LR_poly_PCA = poly_clf_pca.score(X_test_poly, y_test_pca)\nscore_LR_poly_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regressor Model 6: Decision Tree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes. We will also compute feature importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = DecisionTreeRegressor(random_state=1,max_depth=5)\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = regressor.feature_importances_\nfeature_names=X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = {'Features' : feature_names,'Feature Importance' : feature_importances\n          }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_Importance_df = pd.DataFrame(summary)\nprint('The feature importance is:','\\n')\nFeature_Importance_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_DTR = regressor.predict(X_test)\nscore_DTR= regressor.score(X_test, y_test)\nscore_DTR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_pca = DecisionTreeRegressor(random_state=1,max_depth=5)\nregressor_pca.fit(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_dtr_pca = regressor_pca.predict(X_test_pca)\nscore_DTR_PCA = regressor_pca.score(X_test_pca, y_test_pca)\nscore_DTR_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 7: Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf = RandomForestRegressor() \n# n_estimators = 50,random_state=1,max_features=3\nmodel_rf = model_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_rf = model_rf.predict(X_test)\nscore_RF = model_rf.score(X_test, y_test)\nscore_RF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf_pca = RandomForestRegressor() \n# n_estimators = 50,random_state=1,max_features=3\nmodel_rf_pca = model_rf_pca.fit(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_rf_pca = model_rf_pca.predict(X_test_pca)\nscore_RF_PCA = model_rf_pca.score(X_test_pca, y_test_pca)\nscore_RF_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 8: Bagging Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bgcl = BaggingRegressor()\n#n_estimators=50,random_state=1\nbgcl = bgcl.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_bag = bgcl.predict(X_test)\nscore_bag = bgcl.score(X_test , y_test)\nscore_bag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bgcl_pca = BaggingRegressor()\n#n_estimators=50,random_state=1\nbgcl_pca = bgcl_pca.fit(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_bag_pca = bgcl_pca.predict(X_test_pca)\nscore_bag_PCA = bgcl_pca.score(X_test_pca , y_test_pca)\nscore_bag_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 9: Ada Boost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AdaBC = AdaBoostRegressor()\n# n_estimators=50, random_state=1\n#abcl = AdaBoostClassifier( n_estimators=50,random_state=1)\nAdaBC = AdaBC.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_ada = AdaBC.predict(X_test)\nscore_AdaBC = AdaBC.score(X_test , y_test)\nscore_AdaBC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AdaBC_pca = AdaBoostRegressor()\n# n_estimators=50, random_state=1\n#abcl = AdaBoostClassifier( n_estimators=50,random_state=1)\nAdaBC_pca = AdaBC_pca.fit(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_ada_pca = AdaBC_pca.predict(X_test_pca)\nscore_AdaBC_PCA = AdaBC_pca.score(X_test_pca , y_test_pca)\nscore_AdaBC_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regression Model 10: Gradient Boost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building the model with all the attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GraBR = GradientBoostingRegressor()\n# n_estimators=50, random_state=1\n#abcl = AdaBoostClassifier( n_estimators=50,random_state=1)\nGraBR_fit = GraBR.fit(X_train, y_train)\ny_predict_GraBR = GraBR.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing the model on train data\nscore_GraBR_train = GraBR.score(X_train , y_train)\nscore_GraBR_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing the model on the test data\nscore_GraBR = GraBR.score(X_test , y_test)\nscore_GraBR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Building the model with reduced dimensionality (PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GraBR_pca = GradientBoostingRegressor()\n# n_estimators=50, random_state=1\n#abcl = AdaBoostClassifier( n_estimators=50,random_state=1)\nGraBR_pca = GraBR_pca.fit(X_train_pca, y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_GraBR_pca = GraBR_pca.predict(X_test_pca)\nscore_GraBR_PCA = GraBR_pca.score(X_test_pca , y_test_pca)\nscore_GraBR_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Deliverable - 4 (Tuning the model)"},{"metadata":{},"cell_type":"markdown","source":"#### 4.a Identifying Algorithms suitable for this project"},{"metadata":{},"cell_type":"markdown","source":"We will consolidate all the models built in deliverable 3 into a table for quick reference and then make our observations<br>\nTo do the same, we will create summary of all the models and then call them inside a data frame"},{"metadata":{},"cell_type":"markdown","source":"#### Step 4.a.i: Summarise all the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = {'Score': [score_LR, score_lasso,score_ridge, score_LR_poly, score_SVR, score_DTR,score_RF,score_bag,score_AdaBC, score_GraBR],\n\n                    'Score for models trained with 6 Principal Components': [score_LR_PCA,score_lasso_PCA,score_ridge_PCA,score_LR_poly_PCA, score_SVR_PCA, score_DTR_PCA, score_RF_PCA, score_bag_PCA, score_AdaBC_PCA, score_GraBR_PCA]\n\n                     }\n\nmodels=['Linear Regression','Lasso','Ridge','Polynomial Regression','SVR', 'Decision Tree Regressor','Random Forest','Bagging','Ada Boost','Gradient Boost']\nsum_df = pd.DataFrame(summary,models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n- The table in step 4.a.i captures scores of all the models trained by us. <br>\n- As learnt from the case study document, our main objective is to identify a model that predicts the strength of high performance concrete. The model trained through Gradient Boost Algorithm seems to give the best results. <br>\n- We also computed scores for models trained with 6 principal components to check the impact on performance with reduced dimensionality.\n- We noticed a significant dip in score for Gradient boost (from 89.7 to 82) when we reduced dimensionality to 6 (from 9). \n- The score of model with all dimensions outweighs the benefits provided by reducing dimensions, since there is a significant drop in score to 0.82. Hence, we will go-ahead with Gradient Boost model considering all the dimensions"},{"metadata":{},"cell_type":"markdown","source":"We performed an exhuastive EDA; reduced dimensionality; did scaling, built multiple models in our endeavours to identify the model which gives the best results. As seen above Gradient Boost Algorithm gives the best results and hence we selected the same. In the next steps we will perform hyper parameter tuning to identify the parameters which can further enhance performance of the Gradient Boost Algorithm."},{"metadata":{},"cell_type":"markdown","source":"#### 4.b Techniques employed to squeeze that extra performance out of the model without making it overfit or underfit"},{"metadata":{},"cell_type":"markdown","source":"We will the technique of grid search to get that extra performance from the model. Since the best performing model is Gradient Boost; hence the technique will be applied on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator=GradientBoostingRegressor()\nsearch_grid={'n_estimators':[100,200,300,400,500,600],'learning_rate':[.001,0.01,.1],'max_depth':[1,2,3,4,5],'subsample':[.5,.75,1],'random_state':[1]}\nsearch=GridSearchCV(estimator=estimator,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=1,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.fit(X_train,y_train)\nsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating the Gradient Boosting Regressor with the best parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GraBR = GradientBoostingRegressor(learning_rate= 0.1,max_depth= 3,n_estimators= 600,random_state= 1,subsample= 1)\n\nGraBR_fit = GraBR.fit(X_train, y_train)\ny_predict_GraBR = GraBR.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Testng on the train data\n\nscore_GraBR = GraBR.score(X_train , y_train)\nscore_GraBR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Testing on the test data\nscore_GraBR = GraBR.score(X_test , y_test)\nscore_GraBR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n- Grid search help us getting the optimal parameters which will help in getting the best parameters for the selected model.\n- By selecting the recommended parametes for learning rate, max depth, n_estimators, subsample; we notice that the performance of our model has increased from 0.897 to 0.925;"},{"metadata":{},"cell_type":"markdown","source":"#### 4.c Model performance range at 95% confidence level"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(GraBR, X, y, cv=10)\nCV_score_acc_GraBR = scores.mean()\nCV_score_std_GraBR = scores.std()\n\nprint(scores)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (CV_score_acc_GraBR*100.0, CV_score_std_GraBR*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n- As we know that Cross validation is a technique to evaluate and validate a model and estimates its performance in unseen data;\n- From the calculation above it is quite clear that the accuracy of the Gradient Boost model in the production environment is expected to be 92.677% (+-) standard deviation. \n- So if we have to say it with 95% confidence level then the model accuracy in the production environment is expected to be in the range of 92.677% (+-) 2 * standard deviation i.e. [88.719, 96.637] which is quite good and acceptable"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}