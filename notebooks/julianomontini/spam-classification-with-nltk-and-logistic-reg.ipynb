{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spam classification with NLTK and Logistic Regression\nThis is a simple spam classification that I built, also it is my first kernel, so any suggestions are much appreciated! :)"},{"metadata":{},"cell_type":"markdown","source":"First things first, I built this section to act like hyperparameters, and they are: <br>\n_MOST_COMMON_COUNT controls the maximum words returned by the Counter some sections below, also is used to determine the shape of the features matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"_MOST_COMMON_COUNT = 500","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this implementation I used NLTK(https://www.nltk.org) which has a lot of great utilities, like tokenizers, stemmers and collections of stop words, I also used numpy ( for matrix manipulation ), pandas ( for reading the csv and a little of pre-processing ) and sklearn ( for train-test splitting, confusion matrix and logistic regression )."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nimport unicodedata\nfrom collections import Counter\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we load the csv, select the first two columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv', encoding='Windows-1252')\ndf = df.loc[:, ['v1', 'v2']]\ndf = df.dropna()\ndf.head()","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"     v1                                                 v2\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"This function takes a raw text input, then removes any unwanted characters, filters stopwords and stems the sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")\ndef normalize_text(text):\n    tokens = wordpunct_tokenize(text)\n    tokens = [\n        unicodedata.normalize('NFD', stemmer.stem(t) )\n        for t in tokens \n        if t not in stop_words\n    ]\n    sentence = ' '.join(tokens)\n    return sentence","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Updating the table column using the function defined above\ndf.loc[:, 'v2'] = df.loc[:, 'v2'].apply(normalize_text)","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all messages are normalized, reducing the total amount of distinct words that we have in our messages... And now we are ready to identify which words are more frequent"},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = Counter()\nfor line in df.loc[:, 'v2']:\n    counter.update(wordpunct_tokenize(line))","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we assign each common word to a index, that will be used to create the feature matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common = [w[0] for w in counter.most_common(_MOST_COMMON_COUNT)]\nword_map = {str(word): index for (word, index) in zip(most_common, range(len(most_common)))}","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function accepts a text as input, creates an array with the size of the variable _MOST_COMMON_COUNT for each word that is common, the array index corresponding to that word is incremented by one"},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_to_freq(text):\n    features = np.zeros(_MOST_COMMON_COUNT)\n    for word in wordpunct_tokenize(text):\n        if str(word) in word_map:\n            word_index = word_map[str(word)]\n            features[word_index] = features[word_index] + 1\n    return features.tolist()","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declares the features and labels matrixes\nX = np.array([transform_to_freq(line) for line in df.loc[:, 'v2']])\ny = (df.loc[:, 'v1'].values == 'spam').reshape(-1, 1)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splits the dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fits the model with the train split and generates predictions based on test data\nregressor = LogisticRegression(solver='lbfgs').fit(X_train, y_train.ravel())\npredictions = regressor.predict(X_test)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix, used to extract metrics below\ncm = confusion_matrix(y_test, predictions)\ncm","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"array([[962,   3],\n       [ 18, 132]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn = cm[0][0] # True Negatives\nfn = cm[1][0] # False Negatives\ntp = cm[1][1] # True Positives\nfp = cm[0][1] # False Positives\n\nrecall = tp / (tp + fn)\nprecision = tp / (tp + fp)\naccuracy = (tp + tn) / (tn + fn + tp + fp)\nf1_score = 2 * ((precision * recall) / (precision + recall))\nf1_score","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"0.9263157894736842"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}