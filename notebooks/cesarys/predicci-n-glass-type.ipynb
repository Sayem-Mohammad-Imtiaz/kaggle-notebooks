{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Funciones","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics as metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\ndef metricas(y_train,y_pred_train,y_test,y_pred_test):\n    valores=y.value_counts().index.to_list()\n    \n    # Matriz de confusion: Train\n    cm_train=metrics.confusion_matrix(y_train,y_pred_train,labels=valores)\n    df_cm=pd.DataFrame(cm_train,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusión: Train')\n    plt.xlabel('Predicción')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    # Matriz de confusion: Test\n    cm_test=metrics.confusion_matrix(y_test,y_pred_test,labels=valores)\n    df_cm=pd.DataFrame(cm_test,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusión: Test')\n    plt.xlabel('Predicción')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    accuracy_train=metrics.accuracy_score(y_train,y_pred_train)\n    accuracy_test=metrics.accuracy_score(y_test,y_pred_test)\n    precision_train=metrics.precision_score(y_train,y_pred_train,average='micro')\n    precision_test=metrics.precision_score(y_test,y_pred_test,average='micro')\n    recall_train=metrics.recall_score(y_train,y_pred_train,average='micro')\n    recall_test=metrics.recall_score(y_test,y_pred_test,average='micro')\n    f_score=f1_score(y_test,y_pred_test,average='micro')\n    \n    train = (accuracy_train*100, precision_train*100, recall_train*100)\n    test = (accuracy_test*100, precision_test*100, recall_test*100)\n\n    ind = np.arange(3)  # the x locations for the groups\n    ind_n = np.arange(4)  # the x locations for the groups\n    width = 0.3       # the width of the bars\n    \n    fig = plt.figure(figsize = (8,5))\n    ax = fig.add_subplot(111)\n    \n    rects1 = ax.bar(ind, train, width, color='r')\n    rects2 = ax.bar(ind+width, test, width, color='g')\n    rects3 = ax.bar(3, f_score*100, width, color='b')\n    \n    ax.set_ylabel('Scores')\n    ax.set_xticks(ind_n + width/2)\n    ax.set_xticklabels( ('Accuracy', 'Precisión', 'Recall', 'F1 Score') )\n    ax.legend( (rects1[0], rects2[0]), ('Train', 'Test') )\n    \n    def autolabel(rects):\n        for rect in rects:\n            h = rect.get_height()\n            ax.text(rect.get_x()+rect.get_width()/2., 1.00*h, '%.3f'%round(h,3),\n                    ha='center', va='bottom')\n\n    autolabel(rects1)\n    autolabel(rects2)\n    autolabel(rects3)\n    plt.title('Puntajes')\n    plt.ylim(0,120)\n    plt.show()\n    \n    return ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cargando Datos","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/glass/glass.csv')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. AED","metadata":{}},{"cell_type":"markdown","source":"### Tipos de datos de todas las características\n\nTodas las variables son numéricas","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Veremos los valores únicos de cada variables","metadata":{}},{"cell_type":"markdown","source":"Analizando los valores únicos podemos observar que las variables todas son de tipo numéricas continuas, además de que el target esta dividido en 7 tipos de glass.","metadata":{}},{"cell_type":"code","source":"for i in df.columns:\n    print(\"*\"*20)\n    print(i)\n    print(df[i].sort_values().unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribución de datos del target","metadata":{}},{"cell_type":"markdown","source":"Podemos observar solo hay 6 tipos de glass y que el cuarto tipo de glass no se encuentra.","metadata":{}},{"cell_type":"code","source":"datos_x = df.Type.value_counts().index.to_list()\ndatos_y = df.Type.value_counts().to_list()\nsuma = df.Type.value_counts().sum()\nprint(df.Type.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_color=['grey','black','orange','green','blue','red','red']\ngraph = plt.bar(datos_x, datos_y, color=list_color)\nplt.title('Distribución de datos del Target')\n\ni = 0\n\nfor p in graph:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    plt.text(x+width/2,\n             y+height*1.01,\n             str(height)+\" -> \"+str(round(height/suma,2))+'%',\n             ha='center',\n             weight='bold')\n    i+=1\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Resumen estadístico de cada característica","metadata":{}},{"cell_type":"code","source":"df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Análisis gráfico multivariado","metadata":{}},{"cell_type":"code","source":"filas=len(df.columns.to_list())\nc=1\nfig=plt.figure(figsize=(25,7*filas))\n    \nfor i,j in enumerate(df.columns.to_list()):\n    plt.subplot(filas,2, c)\n    sns.distplot(df[j])\n    c = c + 1\n    \n    plt.subplot(filas,2, c)\n    ax1=sns.boxplot(x=df[j],palette=\"Blues\",linewidth=1)\n    c = c + 1\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.columns:\n    plt.figure(figsize=(10,4))\n    sns.boxplot(x='Type', y=i, data=df)\n    plt.title(\"Type VS \"+i)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(df.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aqui observamos que al hacer un gráfico de las 3 variables más correlacionadas al target, se forma una especie de alejamiento de los glass de tipo 7, un alejamiento en glass de tipo 6 y 5, y mucha junta de datos los datos en los glass de tipo 1, 2 y 3.","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_3d(df, x='Mg', y='Al', z='Ba',\n              color='Type')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Limpieza de Datos","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Completitud de Datos","metadata":{}},{"cell_type":"markdown","source":"No encontramos datos nulos","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 3.1. Creación de Características","metadata":{}},{"cell_type":"code","source":"df_fi = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(df_fi.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_fi['nuevo'] = df_fi['Mg']*df_fi['Al']+df_fi['Ba']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Feature Extraction (PCA)","metadata":{}},{"cell_type":"markdown","source":"Las pocas características, que tienen poca correlación y las pocas variables altamente correlacionadas. Nos conviene realizar un Feature Extraction con PCA.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(df.drop('Type',axis=1))\npca.explained_variance_ratio_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(pca.components_)):\n    print('% Var. explicada ('+str(i+1)+' componentes): ', np.cumsum(pca.explained_variance_ratio_)[i]*100)\n    \nplt.bar(range(1,len(pca.components_)+1),pca.explained_variance_ratio_, alpha=.2,color='red')\nplt.plot(range(1,len(pca.components_)+1),np.cumsum(pca.explained_variance_ratio_),alpha=4,color='blue')\nplt.title(\"Varianza explicada y pareto\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos la importancia de las variables para las nuevas variables generadas por el PCA.","metadata":{}},{"cell_type":"code","source":"pcaFin = PCA(n_components=3)\npcaFin.fit(df.drop('Type',axis=1))\npd.DataFrame(pcaFin.components_,columns=df.drop('Type',axis=1).columns.to_list())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pcaFin.transform(df.drop('Type',axis=1))\nX = pd.DataFrame(X)\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca = X.copy()\ndf_pca['Target'] = df['Type']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En un pairplot de las 3 variables generadas por el PCA, podemos observar un mejor alejamiento de los tipos de glass. Esto favorece al algoritmo, para que pueda realizar una mejor predicción del modelo.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df_pca, hue='Target', palette='hls')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar que con las 3 variables de PCA, los diferentes tipos de glass se distancian aún más, lo que favorece a la predicción del algoritmo.","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_3d(df_pca, x=0, y=1, z=2,\n              color='Target')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Entrenamiento y Validación","metadata":{}},{"cell_type":"code","source":"X = df_pca.drop('Target', axis=1)\ny = df_pca.Target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1. Partición Muestral","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. Algoritmos de Machine Learning","metadata":{}},{"cell_type":"markdown","source":"## GridSearchCV with RF","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators':[300,400,500,600,700,800],\n              'max_depth':[5,6,7,8,9,10] ,\n              'n_jobs':[-1],\n              'max_features':[1,2]}\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(),\n             param_grid=parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_param= grid_search.best_params_\nbest_param","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train=best_model.predict(X_train)\ny_pred_test= best_model.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Regresión Logística","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(multi_class='auto')\nlr.fit(X_train,y_train)\ny_pred_train=lr.predict(X_train)\ny_pred_test= lr.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Árboles de Decisión","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree_model = tree.fit(X_train,y_train)\ny_pred_train = tree_model.predict(X_train)\ny_pred_test = tree_model.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bosques Aleatorios","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\"\"\"\nrf = RandomForestClassifier(n_estimators=500,     # Numero de arboles\n                            max_features=3,       # Numero de variables por arbol\n                            min_samples_leaf=15,  # Numero de obs por nodo hoja\n                            min_samples_split=40) # Numero de obs por nodo hoja\n\"\"\"\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\ny_pred_train = tree_model.predict(X_train)\ny_pred_test = tree_model.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nAdaBoost=AdaBoostClassifier(learning_rate=0.001, n_estimators=250)\nAdaBoost.fit(X_train, y_train)\ny_pred_train = AdaBoost.predict(X_train)\ny_pred_test = AdaBoost.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost\nxgb = xgboost.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_pred_train = xgb.predict(X_train)\ny_pred_test = xgb.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier()\nclassifier.fit(X_train, y_train)\ny_pred_train = classifier.predict(X_train)\ny_pred_test = classifier.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GBM","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbm = GradientBoostingClassifier(learning_rate=0.01, n_estimators=1500,max_depth=4, \n                                    min_samples_split=2, min_samples_leaf=1, subsample=1,\n                                    max_features='sqrt', random_state=10)\ngbm.fit(X_train,y_train)\ny_pred_train=gbm.predict(X_train)\ny_pred_test= gbm.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}