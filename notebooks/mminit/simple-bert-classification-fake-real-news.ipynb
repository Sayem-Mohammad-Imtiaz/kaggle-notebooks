{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Basics\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport gc\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Utils\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport time\n\n#Â Check files\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check dataset","metadata":{}},{"cell_type":"code","source":"df_real = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\ndf_fake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\n\ndata_real = df_real.text.values.tolist()\nlabels_real = [0 for el in data_real]\nprint(f'Real samples: {len(data_real)}')\n\n\ndata_fake = df_fake.text.values.tolist()\nlabels_fake = [1 for el in data_fake]\n\nprint(f'Fake samples: {len(data_fake)}')\n\ndata = data_real.copy()\nlabels = labels_real.copy()\n\ndata.extend(data_fake)\nlabels.extend(labels_fake)\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25)\n\nprint(f'train samples: {len(X_train)}')\nprint(f'test samples: {len(X_test)}')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T09:45:01.748808Z","iopub.execute_input":"2021-09-10T09:45:01.749076Z","iopub.status.idle":"2021-09-10T09:45:04.432819Z","shell.execute_reply.started":"2021-09-10T09:45:01.749042Z","shell.execute_reply":"2021-09-10T09:45:04.432109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize and build DataLoader objects","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Check where the PAD token is\nprint('Special tokens indices')\nprint(f'{tokenizer.pad_token}: {tokenizer.convert_tokens_to_ids(tokenizer.pad_token)}')\nprint(f'{tokenizer.unk_token}: {tokenizer.convert_tokens_to_ids(tokenizer.unk_token)}')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T09:45:17.0236Z","iopub.execute_input":"2021-09-10T09:45:17.02388Z","iopub.status.idle":"2021-09-10T09:45:17.029402Z","shell.execute_reply.started":"2021-09-10T09:45:17.023832Z","shell.execute_reply":"2021-09-10T09:45:17.028593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build Data loaders for training/testing\ndef collate_batch(batch):\n    label_list, text_list = [], []\n    for (_labels, _text) in batch: \n        processed_text = torch.tensor(tokenizer.encode(_text, add_special_tokens=True))\n        text_list.append(processed_text)\n        label_list.append(_labels)\n    return pad_sequence(text_list, padding_value=0.)[:512], torch.tensor(label_list)\n\ntrain_loader = DataLoader(list(zip(y_train, X_train)), batch_size=32, shuffle=True, collate_fn=collate_batch)\ntest_loader = DataLoader(list(zip(y_test, X_test)), batch_size=32, shuffle=True, collate_fn=collate_batch)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T09:45:17.030771Z","iopub.execute_input":"2021-09-10T09:45:17.031054Z","iopub.status.idle":"2021-09-10T09:45:17.049516Z","shell.execute_reply.started":"2021-09-10T09:45:17.031019Z","shell.execute_reply":"2021-09-10T09:45:17.048836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model and set training layers","metadata":{}},{"cell_type":"code","source":"# Load model. This is just one of the simple ones\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n\n# Freeze the model\nfor param in model.parameters(): \n    param.requires_grad = False\n\n# Let last layer to train\nmodel.classifier = nn.Linear(in_features=768, out_features=2, bias=True)\n\n# Send to gpu\ndevice = torch.device('cuda')\n_ = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T09:45:47.331208Z","iopub.execute_input":"2021-09-10T09:45:47.331707Z","iopub.status.idle":"2021-09-10T09:45:47.337655Z","shell.execute_reply.started":"2021-09-10T09:45:47.331668Z","shell.execute_reply":"2021-09-10T09:45:47.336826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"learning_rate = 2e-3\noptimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\nepochs = 3\ncriterion = nn.CrossEntropyLoss()\n\ntrain_loss = []\ntest_loss = []\ntrain_accuracy = []\ntest_accuracy = []\n\ntest_max = 20\n\nfor epoch in range(epochs): \n    \n    correct_predictions = 0\n    train_size = 0\n    running_loss = 0\n    step = 0\n    \n    for x, y in tqdm(train_loader, desc=f'Epoch {epoch} train', leave=False):\n        \n        model.train() \n        \n        x = x.to(device)\n        y = y.to(device)\n        \n        \n        optimizer.zero_grad()\n\n        scores = model(x.T)[0]\n        loss = criterion(scores, y)\n        \n        loss.backward()\n        optimizer.step()\n        \n        model.eval()\n        with torch.no_grad():\n            _, predictions = scores.max(1)\n            correct_predictions +=(predictions == y).sum()\n            train_size += predictions.size(0)\n            \n        running_loss += loss.item()\n        step += 1\n        \n        # These two lines, allegedly, make it more memory efficient\n        del x, y, scores, loss \n        gc.collect()\n         \n    # Save train loss and accuracy\n    train_loss.append(running_loss/step)\n    train_accuracy.append(correct_predictions/train_size)\n        \n    ## Eval\n    # Just for some batches, to see it is not increasing, to save a bit of time.\n    # We get the accuracy and loss of the full test dataset below.\n    model.eval()\n    with torch.no_grad():\n        correct_predictions_test = 0\n        test_size = 0\n        running_test_loss = 0\n        step = 0\n        test_it = 0\n        \n        for x, y in tqdm(test_loader, desc=f'Epoch {epoch} test', leave=False): \n            \n            x= x.to(device)\n            scores = model(x.T)[0]\n            \n            del x\n            gc.collect()\n            \n            y = y.to(device)\n            loss = criterion(scores, y)\n            _, predictions = scores.max(1)\n            correct_predictions_test +=(predictions == y).sum()\n            test_size += predictions.size(0)\n            running_test_loss += loss.item()\n            step += 1\n            test_it += 1\n            if test_it > test_max:\n                break\n            \n        \n        # Save test loss and accuracy\n        test_loss.append(running_test_loss/step)\n        test_accuracy.append(correct_predictions_test/test_size)\n        \n        \n        # Print\n        print(f'Epoch {epoch}')\n        print(f'\\n   Train loss: {train_loss[-1]:.4f}      Train accuracy: {train_accuracy[-1]:.4f}')\n        print(f'\\n   Test loss: {test_loss[-1]:.4f}      Test accuracy: {test_accuracy[-1]:.4f}\\n\\n')\n        time.sleep(2)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:44:26.981904Z","iopub.execute_input":"2021-09-10T10:44:26.982355Z","iopub.status.idle":"2021-09-10T11:49:08.419101Z","shell.execute_reply.started":"2021-09-10T10:44:26.982318Z","shell.execute_reply":"2021-09-10T11:49:08.418236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\nax1.plot(range(len(train_loss)), train_loss, label='Train loss')\nax1.plot(range(len(test_loss)), test_loss, label='Test loss')\nax1.legend()\n\nax2.plot(range(len(train_accuracy)), train_accuracy, label='Train accuracy')\nax2.plot(range(len(test_accuracy)), test_accuracy, label='Test accuracy')\nax2.legend();","metadata":{"execution":{"iopub.status.busy":"2021-09-10T11:52:25.690233Z","iopub.execute_input":"2021-09-10T11:52:25.690519Z","iopub.status.idle":"2021-09-10T11:52:26.085095Z","shell.execute_reply.started":"2021-09-10T11:52:25.69049Z","shell.execute_reply":"2021-09-10T11:52:26.084412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy and loss of the full test set\nmodel.eval()\nwith torch.no_grad():\n    correct_predictions_test = 0\n    test_size = 0\n    running_test_loss = 0\n    step = 0\n    test_it = 0\n\n    for x, y in tqdm(test_loader, desc=f'Test', leave=False): \n\n        x= x.to(device)\n        scores = model(x.T)[0]\n\n        del x\n        gc.collect()\n\n        y = y.to(device)\n        loss = criterion(scores, y)\n        _, predictions = scores.max(1)\n        correct_predictions_test +=(predictions == y).sum()\n        test_size += predictions.size(0)\n        running_test_loss += loss.item()\n        step += 1\n        test_it += 1","metadata":{"execution":{"iopub.status.busy":"2021-09-10T11:53:51.071402Z","iopub.execute_input":"2021-09-10T11:53:51.071682Z","iopub.status.idle":"2021-09-10T11:59:30.810811Z","shell.execute_reply.started":"2021-09-10T11:53:51.071652Z","shell.execute_reply":"2021-09-10T11:59:30.809998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Full test loss: {running_test_loss/step:.4f}\\nFull test accuracy: {correct_predictions_test/test_size:.4f}')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T12:01:39.298796Z","iopub.execute_input":"2021-09-10T12:01:39.299376Z","iopub.status.idle":"2021-09-10T12:01:39.304369Z","shell.execute_reply.started":"2021-09-10T12:01:39.299336Z","shell.execute_reply":"2021-09-10T12:01:39.303309Z"},"trusted":true},"execution_count":null,"outputs":[]}]}