{"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","version":"3.6.3","file_extension":".py","nbconvert_exporter":"python"}},"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.model_selection import train_test_split\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nnp.set_printoptions(threshold=np.inf)\nimport operator\nimport nltk\n# Any results you write to the current directory are saved as output.","execution_count":null,"metadata":{"_cell_guid":"852b8924-de04-4aa2-bd92-a3fb842b0b73","_uuid":"ebc493f6ea786dddc4766268f70218afbe1cc70c"},"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv', encoding='latin-1')\ndf.head()","execution_count":null,"metadata":{"_cell_guid":"d30a3474-bd98-417b-adda-29c35a9869e9","_uuid":"94033cfe4290df93e7a7811ebf071261724da1a9"},"outputs":[]},{"cell_type":"code","source":"df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)","execution_count":null,"metadata":{"_cell_guid":"af697b84-0899-4280-ab26-5ad5d98ea2f2","_uuid":"585f966500c667b0353f59f707ce61f26f0e675f","collapsed":true},"outputs":[]},{"cell_type":"code","source":"df = df.rename(columns={'v1': 'class', 'v2':'text'})\ndf.head()","execution_count":null,"metadata":{"_cell_guid":"6c228170-6f01-46b0-b2a7-a12276abff42","_uuid":"15901b28a73fd4a26dabc2d4fa45273455e8eb55"},"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import WhitespaceTokenizer\ntokeniser = WhitespaceTokenizer()\n\n\ndef tokenize(sentence):\n    return tokeniser.tokenize(sentence)\n","execution_count":null,"metadata":{"_cell_guid":"8d8d24ef-29e5-4605-a965-f9d51281dcbf","_uuid":"5b040f9ec3b31c337d7f1f7c3e233896658b845f","collapsed":true},"outputs":[]},{"cell_type":"code","source":"num_top_words = 1000\n\nall_words = {}\n\ndef build_words(string_in):\n    for w in tokenize(string_in):\n        all_words[w] = all_words.get(w, 0) + 1\n\nfor x in df['text']:\n    build_words(x)\n\nsorted_words = sorted(all_words.items(), key=operator.itemgetter(1), reverse=True)\nsorted_words = list(map(lambda x: x[0], sorted_words))\nsorted_words = sorted_words[:num_top_words]\n\nwords_by_emails = []\n\ndef count_words_per_email(text):\n    row = np.zeros(len(sorted_words))  # Add the label column\n    for word in tokenize(text):\n        try:\n            row[sorted_words.index(word)] = row[sorted_words.index(word)] + 1\n        except ValueError:\n            pass\n    return row\n\nX_rows = []\nfor _row in df['text']:\n    X_rows.append(count_words_per_email(_row))\nX_rows = np.array(X_rows)\n\nprint(X_rows.shape)    ","execution_count":null,"metadata":{"_cell_guid":"b0d0eadc-f273-488d-947a-3006a1db4423","_uuid":"78db8280325d7c6014999a4045224ac41a851d7e"},"outputs":[]},{"cell_type":"code","source":"_labels = df['class'].map(lambda x: 1.0 if x == 'spam' else 0.0).values\ny_labels = tf.one_hot(_labels, depth=2)","execution_count":null,"metadata":{"_cell_guid":"e43292f4-6ead-473f-b950-b702ec20a247","_uuid":"f9210d4179834538cd948152fbe517f0c81ef5a3","collapsed":true},"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.003\nn_h1 = 512\nn_h2 = 256\n\nn_class = 2\nbatch_size = 64\nepochs = 5\nX = tf.placeholder(\"float\", [None, X_rows.shape[1]])\nY = tf.placeholder('float', [None, n_class])\n\nweights = {\n    'w0': tf.get_variable(\"w0\", shape=[X_rows.shape[1], n_h1], initializer=tf.contrib.layers.xavier_initializer()),\n    'w1': tf.get_variable(\"w1\", shape=[n_h1, n_h2], initializer=tf.contrib.layers.xavier_initializer()),\n    'w_out': tf.get_variable(\"w_out\", shape=[n_h2, n_class], initializer=tf.contrib.layers.xavier_initializer())\n}\n\nbias = {\n    'b0': tf.Variable(tf.random_normal([n_h1])),\n    'b1': tf.Variable(tf.random_normal([n_h2])),\n    'b_out': tf.Variable(tf.random_normal([n_class])),\n}\n\nkeep_prob_1 = tf.placeholder(tf.float32)\nkeep_prob_2 = tf.placeholder(tf.float32)","execution_count":null,"metadata":{"_cell_guid":"4c52438b-1ad9-43ab-8f5c-a580346ee6c7","_uuid":"a1da864e94e644b5f7ced12cc6fe800670ca6427"},"outputs":[]},{"cell_type":"code","source":"def neural_net(x):\n    layer_1 = tf.nn.leaky_relu(tf.add(tf.matmul(x, weights['w0']), bias['b0']))\n    layer_1_drop = tf.nn.dropout(layer_1, keep_prob_1)\n    layer_2 = tf.nn.leaky_relu(tf.add(tf.matmul(layer_1_drop, weights['w1']), bias['b1']))\n    layer_2_drop = tf.nn.dropout(layer_2, keep_prob_2)\n    out = tf.add(tf.matmul(layer_2_drop, weights['w_out']), bias['b_out'])\n    return out\n","execution_count":null,"metadata":{"_cell_guid":"631086d2-b576-4360-ba7d-82f0b9f1c64f","_uuid":"fc45d8598176b7cf27d4b486ed36b9ee93edbfe4","collapsed":true},"outputs":[]},{"cell_type":"code","source":"logits = neural_net(X)","execution_count":null,"metadata":{"_cell_guid":"6ebf43ea-3ffe-451a-bc01-6badc6923849","_uuid":"f606c8b5aad1440d4001c03bf0f44ac337b0e3ff"},"outputs":[]},{"cell_type":"code","source":"loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))","execution_count":null,"metadata":{"_cell_guid":"0e60a006-d78e-4358-ae6a-012fcd0e95fb","_uuid":"5104f20392d813bdf94055439b6589b007f0d591","collapsed":true},"outputs":[]},{"cell_type":"code","source":"optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)","execution_count":null,"metadata":{"_cell_guid":"426e23cf-66e8-42f7-a038-9f4f30f41fe4","_uuid":"eab630b6294915df3d39d8385d25ccbd9339bc4a","collapsed":true},"outputs":[]},{"cell_type":"code","source":"train_op = optimizer.minimize(loss_op)","execution_count":null,"metadata":{"_cell_guid":"067174b6-7ef0-4e98-8033-97dfb49dff92","_uuid":"e4dd3bb7ed31a8224ae89ccc0db56e47193040e7","collapsed":true},"outputs":[]},{"cell_type":"code","source":"correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":null,"metadata":{"_cell_guid":"be4bb4de-8b7e-4c44-9601-b23f0d29dd8e","_uuid":"c848cf7ca7bfd1884a0aeca0de8341ef4921481b","collapsed":true},"outputs":[]},{"cell_type":"code","source":"init = tf.global_variables_initializer()","execution_count":null,"metadata":{"_cell_guid":"111d3540-370d-404e-b112-15cbc3f2e082","_uuid":"7432ba201656b4376a9b06a7d817d0630ab0ec77","collapsed":true},"outputs":[]},{"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run(init)\n    labels = sess.run(y_labels)\n    X_train, X_test, y_train, y_test = train_test_split(X_rows, labels, test_size=0.1)\n    num_batches = int(len(X_train) / batch_size)\n    for _e in range(epochs):\n        for step in range(num_batches):\n            feed_dict={\n                 X: X_train[step * batch_size: (step + 1) * batch_size],\n                 Y: y_train[step * batch_size: (step + 1) * batch_size],\n                 keep_prob_1: 0.8,\n                 keep_prob_2: 0.8\n            }\n            sess.run(train_op, feed_dict=feed_dict)\n            if step == 0 or step % 10 == 0:\n                loss, acc = sess.run([loss_op, accuracy], feed_dict=feed_dict)\n                print('Step: {0}, Loss={1}, Training Accuracy={2}'.format(\n                    step, loss, acc\n                ))\n    # Test\n    num_batches = int(len(X_test) / batch_size)\n    total_acc = 0\n    for step in range(num_batches):\n        feed_dict={\n             X: X_test[step * batch_size: (step + 1) * batch_size],\n             Y: y_test[step * batch_size: (step + 1) * batch_size],\n             keep_prob_1: 1.0,\n             keep_prob_2: 1.0\n        }\n        acc = sess.run(accuracy, feed_dict=feed_dict)\n        total_acc = total_acc + acc\n        print('Average Test Accuracy = {0}'.format(total_acc/(step + 1)))\n        \n\n        ","execution_count":null,"metadata":{"_cell_guid":"248bfe91-ae97-4e1e-a6c4-16236b94562d","_uuid":"2704775c6843baf8a23b0e46449704cfb22c94be"},"outputs":[]},{"cell_type":"code","source":"","execution_count":null,"metadata":{"_cell_guid":"5869e2a5-4a6f-4537-b56f-e0bbd0591cd1","_uuid":"296b7808052b09adc80c5fdf1e16385908f0f73a","collapsed":true},"outputs":[]}],"nbformat":4}