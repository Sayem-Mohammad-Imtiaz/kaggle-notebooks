{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Pattern Analysis of Stack Overflow 2018\nIn this notebook, I'll try to use different unsupervised learning approaches to find any hidden patterns in the Stack Overflow 2018 survey data. Hopefully they will be able to uncover something interesting which would otherwise go unnoticed. Let's see how far we can go :)\n\n<img src='http://i66.tinypic.com/261yxjn.png' width='100%' />\n\n## Table of Contents:\n* [1. Preprocessing the data](#preprocess)\n* [2. Principal Component Analysis](#pca)\n    * [2.1. Scree Plot](#pca_scree)\n    * [2.1. PCA Biplots](#pca_biplot)\n* [3. Clustering](#clustering)\n    * [3.1. HDBSCAN and TSNE](#hdbscan_tsne)\n    * [3.2. KMeans and TSNE](#kmeans_tsne)\n    * [3.3. Playing with TSNE perplexity](#perplexity)\n    * [3.3. Cluster Correlations](#cluster_correlation)  "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport hdbscan\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\n\nmatplotlib.style.use('ggplot')\nmatplotlib.rcParams['figure.figsize'] = (20.0, 5.0)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"b9ee7184473e4c1ef7828ec2820d7f259ccd2664"},"cell_type":"markdown","source":"# 1. Preprocessing the data <a class=\"anchor\" id=\"preprocess\"></a>\nIn this dataset we have some columns where people have been able to select multiple values; this has to be extracted and put on a format which can be interpreted by the unsupervised models. My data processing is essentially the same as I've used in [this notebook](https://www.kaggle.com/nanomathias/predicting-r-vs-python), the basic point being to create one-hot-encoding columns for all categorical data in the dataset."},{"metadata":{"trusted":true,"_uuid":"6cb27f17e6ad3a22ee114bafc2b2c1ca42a1c97a","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Read in the survey results, shuffle results\nprint(f\">> Loading data\")\ndf = pd.read_csv('../input/survey_results_public.csv', low_memory=False).sample(frac=1)\n\n# Columns with multiple choice options\nMULTIPLE_CHOICE = [\n    'CommunicationTools','EducationTypes','SelfTaughtTypes','HackathonReasons', \n    'DatabaseWorkedWith','DatabaseDesireNextYear','PlatformWorkedWith',\n    'PlatformDesireNextYear','Methodology','VersionControl',\n    'AdBlockerReasons','AdsActions','ErgonomicDevices','Gender',\n    'SexualOrientation','RaceEthnicity', 'LanguageWorkedWith',\n    'IDE', 'FrameworkWorkedWith', 'FrameworkDesireNextYear',\n    'LanguageDesireNextYear', 'DevType',\n]\n\n# Columns which we are not interested in\nDROP_COLUMNS = [\n    'Salary', 'SalaryType', 'Respondent', 'CurrencySymbol'\n]\n\n# Drop too easy columns\nprint(f\">> Deleting uninteresting or redundant columns: {DROP_COLUMNS}\")\ndf.drop(DROP_COLUMNS, axis=1, inplace=True)\n\n# Go through all object columns\nfor c in MULTIPLE_CHOICE:\n    \n    # Check if there are multiple entries in this column\n    temp = df[c].str.split(';', expand=True)\n\n    # Get all the possible values in this column\n    new_columns = pd.unique(temp.values.ravel())\n    for new_c in new_columns:\n        if new_c and new_c is not np.nan:\n\n            # Create new column for each unique column\n            idx = df[c].str.contains(new_c, regex=False).fillna(False)\n            df.loc[idx, f\"{c}_{new_c}\"] = 1\n\n    # Info to the user\n    print(f\">> Multiple entries in {c}. Added {len(new_columns)} one-hot-encoding columns\")\n\n    # Drop the original column\n    df.drop(c, axis=1, inplace=True)\n        \n# For all the remaining categorical columns, create dummy columns\ndf = pd.get_dummies(df)\n\n# Fill in missing values\ndf.dropna(axis=1, how='all', inplace=True)\ndummy_columns = [c for c in df.columns if len(df[c].unique()) == 2]\nnon_dummy = [c for c in df.columns if c not in dummy_columns]\ndf[dummy_columns] = df[dummy_columns].fillna(0)\ndf[non_dummy] = df[non_dummy].fillna(df[non_dummy].median())\nprint(f\">> Filled NaNs in {len(dummy_columns)} OHE columns with 0\")\nprint(f\">> Filled NaNs in {len(non_dummy)} non-OHE columns with median values\")\n\n# Create correlation matrix\ncorr_matrix = df.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\nprint(f\">> Dropping the following columns due to high correlations: {to_drop}\")\ndf = df.drop(to_drop, axis=1)\n\n# Perform scaling on all non-dummy columns. Create X and y\nnondummy_columns = [c for c in df.columns if df[c].max() > 1]\nX = deepcopy(df)\nX.loc[:, nondummy_columns] = scale(df[nondummy_columns])\nX.drop('ConvertedSalary', axis=1, inplace=True)\nprint(f\">> Shape of final dataframe X: {X.shape}\")","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"ca39a37c5e2ed3f9c971fe64a5d9783d061f1a20"},"cell_type":"markdown","source":"# 2. Principal Component Analysis<a class=\"anchor\" id=\"pca\"></a>\nLet us start with one of the classics, namely, principal component analysis (PCA). Right now we have more than 800 columns (dimensions) in our dataset, and with PCA we essentially try to find a new orthogonal vector space which describes most of the variation in the data. See the following image for an illustration:\n\n<img src='https://static1.squarespace.com/static/5a316dfecf81e0076f50dae2/t/5ac35d702b6a284b3fde6131/1522753187751/PCA.png?format=500w'  />\n<center>Image taken from [osgdigitallabs](https://www.osgdigitallabs.com/blogs/2018/4/3/dimensionality-reduction)</center>\n\n"},{"metadata":{"trusted":true,"_uuid":"27b9932f12f8ca43fbadd368f738055d8cf1c940","collapsed":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Create a PCA object, specifying how many components we wish to keep\npca = PCA(n_components=50)\n\n# Run PCA on scaled numeric dataframe, and retrieve the projected data\npca_trafo = pca.fit_transform(X)\n\n# The transformed data is in a numpy matrix. This may be inconvenient if we want to further\n# process the data, and have a more visual impression of what each column is etc. We therefore\n# put transformed/projected data into new dataframe, where we specify column names and index\npca_df = pd.DataFrame(\n    pca_trafo,\n    index=X.index,\n    columns=[\"PC\" + str(i + 1) for i in range(pca_trafo.shape[1])]\n)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"9c5bc34004a507699bafcbf4d181ae5aaaab430a"},"cell_type":"markdown","source":"## 2.1. Scree plot<a class=\"anchor\" id=\"pca_scree\"></a>\nNow we've reduced our 800-dimensional space into 50 \"principal components\", and the first thing we want to check is how much of the dataset is retained within these 50 components - sometimes we can get lucky that more than 30% of our data is even contained in the first 2 components, which means that we would be able to plot those two components in a biplot, and see 30% of the variation in the dataset just from a standard 2D plot. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"3fd6cd776e1610a0b179a92c807b7dcb0eaf5f37","collapsed":true},"cell_type":"code","source":"# Plot the explained variance# Plot t \nplt.plot(\n    pca.explained_variance_ratio_, \"--o\", linewidth=2,\n    label=\"Explained variance ratio\"\n)\n\n# Plot the cumulative explained variance\nplt.plot(\n    pca.explained_variance_ratio_.cumsum(), \"--o\", linewidth=2,\n    label=\"Cumulative explained variance ratio\"\n)\n\n# Show legend\nplt.ylim([-0.1, 0.7])\nplt.legend(loc=\"best\", frameon=True)\nplt.show()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"b1749d8116a69756ba252f9bab59bdf7c8102cc2"},"cell_type":"markdown","source":"In this case we are not so lucky - with 50 components we still only explain about 55% of the dataset, and with only 2 components we include a little less than 20% of the total variation in the dataset. Still, 10% in only two variables is still significantly better than the about ~2% included in the original features. \n\n## 2.2. PCA Biplots<a class=\"anchor\" id=\"pca_biplot\"></a>\nEven though it's a quite small part of the variation in the dataset, let us try to plot the PCA biplots for the first few components to see if we can see any pattern. Since it might be interesting, I'll also color different dev types in different colors, just to see if any pattern emerges."},{"metadata":{"trusted":true,"_uuid":"43882c07d5bdb8dcf199dfac6079859c276c5579","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"_, axes = plt.subplots(2, 2, figsize=(15, 10))\n\ndev_types = [c for c in X.columns if 'DevType' in c]\ncolors = sns.color_palette('hls', len(dev_types)).as_hex()\n\nfor i, dev in enumerate(dev_types):   \n    idx = (df[dev] == 1)\n    pca_df.loc[idx].plot(kind=\"scatter\", x=\"PC1\", y=\"PC2\", ax=axes[0][0], c=colors[i], alpha=0.1)\n    pca_df.loc[idx].plot(kind=\"scatter\", x=\"PC2\", y=\"PC3\", ax=axes[0][1], c=colors[i], alpha=0.1, label=dev)\n    pca_df.loc[idx].plot(kind=\"scatter\", x=\"PC3\", y=\"PC4\", ax=axes[1][0], c=colors[i], alpha=0.1)\n    pca_df.loc[idx].plot(kind=\"scatter\", x=\"PC4\", y=\"PC5\", ax=axes[1][1], c=colors[i], alpha=0.1)\n    \naxes[0][1].legend(loc='best', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"530bffc87b86963c8338d538528f38d725e85710"},"cell_type":"markdown","source":"Seems like there are no very clear patterns in first few components of the PCA, and they do not seem to clearly separate the dataset when it comes to developer types at least."},{"metadata":{"_uuid":"6da34ccdfe7599687d15e2b897564bd1bdd55ce9"},"cell_type":"markdown","source":"# 3. Clustering<a class=\"anchor\" id=\"clustering\"></a>\nNow we'll play with clustering of the dataset using different approaches\n\n## 3.1. HDBSCAN & T-SNE<a class=\"anchor\" id=\"hdbscan_tsne\"></a>\nFirst let's try HDBSCAN for clustering algorithm and for dimensionality reduction I'll use TSNE. The reasons for chosing HDBSCAN of other typical clustering algorithms (K-Means, etc.) are nicely summarized [here](http://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html). Let's see if any patterns emerge by this approach.\n\nDue to the size of the dataset, I've chosen to take a smaller sample for this visualization; otherwise it took too long to comfortably run during a cup of coffee."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cdbd0a4dd86546426e5426e7731260abec3e7310","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Create a sample dataset\nsample = X.sample(30000)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c46748867b34d9a46c0d66881b9609c7ee9fbddb","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"%%time \nprint(\">> Clustering using HDBSCAN\")\nclusterer = hdbscan.HDBSCAN(min_cluster_size=500)\nclusterer.fit(sample)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe7a2bd7616d1ebc566a21f6cb6a18c62ca4ac2","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"%%time\nprint(\">> Dimensionality reduction using TSNE\")\nprojection = TSNE(init='pca', random_state=42).fit_transform(sample)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1abed4347128563c1700918433b5e791895ef697","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def get_cluster_colors(clusterer, palette='Paired'):\n    \"\"\"Create cluster colors based on labels and probability assignments\"\"\"\n    n_clusters = len(np.unique(clusterer.labels_))\n    color_palette = sns.color_palette(palette, n_clusters)\n    cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n    if hasattr(clusterer, 'probabilities_'):\n        cluster_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n    return cluster_colors\n\n# Create the plot on the TSNE projection with HDBSCAN colors\n_, ax = plt.subplots(1, figsize=(20, 10))\nax.scatter(\n    *projection.T, \n    s=50, linewidth=0, \n    c=get_cluster_colors(clusterer), \n    alpha=0.25\n)\nplt.show()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"39f6bf036d23373067274be81829117d63b66454"},"cell_type":"markdown","source":"Interestingly, TSNE creates a very clear pattern with lots of clusters, yet the HDBSCAN clusterer does not seem to clearly agree that all these groups of people are actually belonging to a specific cluster. It is important to remember that TSNE **does not** preserve distances or density from the original dataset, rather it tries to preserve nearest neighbors, and as a result can create \"fake\" patterns, which do not neccesarily have any easily intepretable meaning. Some of the identified clusters by HDBSCAN, however, do align with observed groups from the TSNE. Most of the clusters, however, fall into one big group in the TSNE, and are not that clearly separable in the 2D TSNE embedding. \n\nWe'll inspect the clusters more closely in a later section."},{"metadata":{"_uuid":"8017b238136d7c1a755e837808fc0e8733bc5bdf"},"cell_type":"markdown","source":"## 3.2. KMeans & T-SNE<a class=\"anchor\" id=\"kmeans_tsne\"></a>\nFor KMeans we need to determine how many clusters we need. We'll keep working with the subset of data, so that processing won't take too long."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"85463bcc837489888c30c85abf17c3359fecba51","collapsed":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(random_state=42)\nskplt.cluster.plot_elbow_curve(kmeans, sample, cluster_ranges=[1, 5, 10, 50, 100, 200])\nplt.show()","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"f3bf278d018c1e552f140882828554d5fd8f47b3"},"cell_type":"markdown","source":"It seems like there's a kink around 6 clusters, so let's go with that for now. As before we'll plot the colors of the KMeans clusters onto the TSNE plot."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"54b7b1493b5bc4cb2e5103d5da90395f4e8eb643","collapsed":true},"cell_type":"code","source":"# Create the plot on the TSNE projection with HDBSCAN colors\n_, ax = plt.subplots(1, figsize=(20, 10))\nkmeans = KMeans(n_clusters=6).fit(sample)\nax.scatter(\n    *projection.T, \n    s=50, linewidth=0, \n    c=get_cluster_colors(kmeans, 'hls'), \n    alpha=0.25\n)\nplt.show()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"c080814b7e0c0bb94b2a11ff8526277c46d8a650"},"cell_type":"markdown","source":"Beyond looking slighly psychedelic the KMeans clusters seem paint a pretty clear picture together with TSNE - seems like a lot of the clusers identifier by KMeans are co-located at the groups of points generated by TSNE. Now that we have cluster identifications with two clustering methods, HDBSCAN and KMeans, we should play a bit with the TSNE settings to see how they can influence the plots."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6c98b11fcf4532c1ddac5231791cb02934fa2514"},"cell_type":"markdown","source":"## 3.3. Playing with TSNE perplexity<a class=\"anchor\" id=\"perplexity\"></a>\nThe perplexity setting of TSNE can quite often lead to significant variations in the output result, so it could be interesting to see how different values affect our results."},{"metadata":{"trusted":true,"_uuid":"74e537b7553cf46bc07735e7c339f20fe7308e6a","collapsed":true},"cell_type":"code","source":"perplexities = [5, 30, 50, 100]\n\n_, axes = plt.subplots(2, 4, figsize=(20, 10))\nfor i, perplexity in tqdm(enumerate(perplexities)):\n    \n    # Create projection\n    projection = TSNE(init='pca', perplexity=perplexity).fit_transform(sample)\n    \n    # Plot for HDBSCAN clusters\n    axes[0, i].set_title(\"Perplexity=%d\" % perplexity)\n    axes[0, i].scatter(\n        *projection.T, \n        s=50, linewidth=0, \n        c=get_cluster_colors(clusterer), \n        alpha=0.25\n    )\n    \n    # Plot for KMeans clusters\n    axes[1, i].scatter(\n        *projection.T, \n        s=50, linewidth=0, \n        c=get_cluster_colors(kmeans, 'hls'), \n        alpha=0.25\n    )\n\nplt.show()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"968b7f0d6bfde7ac6211e677761daa2ec0a29ecf"},"cell_type":"markdown","source":"This looks very interesting; on the first row we have the HDBSCAN clusters, and the second row are the KMeans clusters. We can clearly see how changing the perplexity changes how and how well the classes / clusters are separated in the TSNE embedding.\n\n## Cluster Correlations<a class=\"anchor\" id=\"cluster_correlation\"></a>\nHaving played enough with the funny TSNE visualizations, let's try investigate what is actually inside the clusters identified by HDBSCAN. We'll do that by going through each cluster, and finding the features that are highly correlated within those clusters."},{"metadata":{"trusted":true,"_uuid":"6ea396d8cbd3f0593fe02aadabab2b7d507071a9","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Get the data for each cluster (not noise, aka -1)\nunique_clusters = [c for c in np.unique(clusterer.labels_) if c > -1]\n        \n# Create a figure for holding the correlation plots\ncols = 2\nrows = np.ceil(len(unique_clusters) / cols).astype(int)\n_, axes = plt.subplots(rows, cols, figsize=(20, 10*rows))\nif rows > 1:\n    axes = [x for l in axes for x in l]\n\n# Calculate sample means\nsample_mean = sample.median()\n\n# Go through clusters identified by HDBSCAN\nfor i, label in enumerate(unique_clusters):\n    \n    # Get index of this cluster\n    idx = clusterer.labels_ == label\n    \n    # Identify feature where the median differs significantly\n    median_diff = (sample.median() - sample[idx].median()).abs().sort_values(ascending=False)\n    \n    # Create boxplot of these features for all vs cluster\n    top = median_diff.index[0:20]\n    temp_concat = pd.concat([sample.loc[:, top], sample.loc[idx, top]], axis=0).reset_index(drop=True)\n    temp_concat['Cluster'] = 'Cluster {}'.format(i+1)\n    temp_concat.loc[0:len(sample),'Cluster'] = 'All respondees'\n    temp_long = pd.melt(temp_concat, id_vars='Cluster')\n    \n    sns.boxplot(x='variable', y='value', hue='Cluster', data=temp_long, ax=axes[i])\n    for tick in axes[i].get_xticklabels():\n        tick.set_rotation(90)\n    axes[i].set_title(f'Cluster #{i+1} - {idx.sum()} respondees')    \n\n# Tight layout    \nplt.tight_layout()\nplt.show()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"63df6d35d1bbf5558da1e33f7bbc877bd8ddefb0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}