{"cells":[{"metadata":{},"cell_type":"markdown","source":"Red wine quality dataset is a very simple dataset to work with, all the features are numerical and there is no missing data! Can it be more convenient! Honestly such datasets are a bit boring for me, however as a wine lower, I am very excited to work on it! :D\n\nThis is a classification problem (can be also regression but I prefer to do classification!)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First lets lave look at the data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nprint(df.info())\nfeatures = df.drop(columns='quality').columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there is no missing data and all the features are of numerical type.\nThis is a small dataset and all features are numerical, so SVM can be an option, it works well on small datasets, KNN maybe ... the dimension of data is big so it will be slow, logistic regression, Naive bayes and decision tree also might be a good option for this problem! But, I need still more information to make a decision. Lets look at the statistic summary of dataset:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(df.describe())\nplt.figure(figsize=(20,14))\nsns.boxplot(data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features have different variances, features with higher variances are dominant in distance dependant-classification methods like KNN, so to use KNN features should be scaled first, maybe that is a good excuse for not using KNN! Hahaha... \nWe need still more information, lets see how is the distribution of wines in different classes:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(df.quality, bins=20, kde=False)\nplt.ylabel('The number of wines')\nplt.xlabel('Wine quality')\nplt.title('The number of wines per quality')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, it is an unbalanced dataset... As a rule of thumb, decision tree-based models peforms best, so I will definietly try that, KNN is valnurable; \nAccuracy is not a good measure for performance evaluation of an unbalanced dataset, AUC is iu sed for binary classification problems, so my choice is logloss which is most common accuracy measure for unbalanced datasets! One decision is made, it is a progress!\nNow I want to see the correlation between different features and with quality of wine! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ncorr_mat = df.corr(method='pearson')\nsns.heatmap(corr_mat, annot=True) \nplt.xticks(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I love heatmap, it is beautiful and gives you a lot of information! It turns out that there is a quite remarkable correlation between wine quality and its alcohol content which is not surprising at all! \nDensity and citric acid have high correlation?!! I am wondering why...\nIf I am going to to feature reduction, between citric acid and fixed acidity I will keep only one of them, but now I don't think it is necessary, lets keep all the features. \nNow it is time to build a ML model. Based on the observations above, I apply SVM, logistic regression, naive bayes and decision tree models to the data!\nFirst, without hyperparameter tuning, I want to see a rough comparison between their performance. I keep a subset of data as validation dataset out to use it later as unseen data by model!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split-out validation df\narray = df.values\nX = array[:,0:11]\ny = array[:,11]\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)\n\nmodels = []\nmodels.append(('Decision Tree', DecisionTreeClassifier()))\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto', probability=True))) \nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='neg_log_loss')\n    results.append(cv_results)\n    names.append(name)\n\nplt.figure(figsize=(12,10))\nplt.boxplot(results, labels=names)\nplt.title('Performance comparison of different algorithms')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently Logistic Regression and SVM performs better than other algorithms, so I narrow down my choices to these two. I compare the performance of these two on validation data set and then make my final decision:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('SVM', SVC(gamma='auto', probability=True))) \nresults = []\nnames = []\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    predictions = model.predict_proba(X_validation)\n    score=log_loss(Y_validation, predictions)\n    results.append(cv_results)\n    names.append(name)\n    print('logloss score of {} on unseen data is: '.format(name) + str(score))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So SVM is out! Now I should optimize hyperparameters of Linear Regression model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LogisticRegression(solver='liblinear', multi_class='ovr') ### Note: default solver is 'lbfgs' which only supports L2 (Ridge);  solver 'liblinear' supports both 'L1' and 'L2' ; Usually Ridge is better than Lasso, Lasso is good for feature reduction\nparameters = {'C': np.logspace(-5, 8, 15, 10, 12)}\n\n# Create grid search using 5-fold cross validation\nlr_cv = GridSearchCV(model, parameters, cv=5, verbose=0)\nbest_model = lr_cv.fit(X_train, Y_train)\n\n# View best parameters\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\n# Evaluate predictions\npredictions = lr_cv.predict_proba(X_validation)\nprint('logloss score on unseen data is: ' + str(log_loss(Y_validation, predictions)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I got only a tiny improvement by hyperparameter tuning... Lets try XGBoost classifier, I usually try it, it is fast and powerful!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(max_depth=35, random_state=42, n_estimators=1500, learning_rate=0.005, booster='gbtree', objective='multi:softprob', min_child_weight=0.1, n_jobs=10)\nxgb.fit(X_train, Y_train)\ny_pred=xgb.predict_proba(X_validation)\n\nprint(\"logloss score XGBoost: {}\".format(log_loss(Y_validation, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the performance of XGBoost is better than logistic regression, but not so much! I would like to see which features have had the most influence in this result:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nfeat_imp = pd.Series(xgb.feature_importances_, index=features).sort_values(ascending=True)\nfeat_imp.plot(kind='barh', title='Feature Importances XGBoost')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}