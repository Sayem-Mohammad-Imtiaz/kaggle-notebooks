{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tensorflow import keras\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf = pd.read_csv('../input/car-price-prediction/CarPrice_Assignment.csv', index_col = 'car_ID')\ny = df.price\nX = df\nX.drop(['price'], axis = 1, inplace = True)\nX_train, X_valid, y_train, y_valid = train_test_split(df, y, train_size = 0.8, test_size = 0.2)\nprint(f\"Shape of X_train = {X_train.shape}\\nShape of y_train = {y_train.shape}\\nShape if X_valid : {X_valid.shape}\\nShape of y_valid = {y_valid.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\ncat_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['object']]\n\nsi = SimpleImputer(strategy = 'constant')\nohe = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n\npp_pipeline = ColumnTransformer(\ntransformers = [\n    ('num', si, num_cols),\n    ('cat', ohe, cat_cols)\n])\n\npp_X_train = pp_pipeline.fit_transform(X_train)\npp_X_valid = pp_pipeline.transform(X_valid)\nprint(pp_X_train.shape)\nprint(pp_X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepocessing done here. Creating a DNN using keras\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(pp_X_train.shape[-1], input_shape = (pp_X_train.shape[-1],)))\nmodel.add(Dense(45, activation = 'relu'))\nmodel.add(Dense(1, activation  = 'linear'))\n\nmodel.compile(optimizer = 'adam', loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\ntrained_model = model.fit(pp_X_train, y_train, epochs = epochs, validation_data = (pp_X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import accuracy_score\n\n#Plot the decrease of MAE with each iteration\nprint(f\"Training MAE : {trained_model.history['loss'][-1]}\")\nprint(f\"Validation MAE : {trained_model.history['val_loss'][-1]}\")\nimport matplotlib.pyplot as plt\nplt.clf()\nfig = plt.figure()\nfig.suptitle('Graph of training loss and validation loss')\nplt.plot(range(epochs), trained_model.history['loss'], 'b', range(epochs), trained_model.history['val_loss'], 'r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Autotuned model with Keras Tuner\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import kerastuner as kt\nimport IPython\nimport tensorflow as tf\ndef model_builder(hp):\n    model = Sequential()\n    model.add(Dense(pp_X_train.shape[-1], input_shape = (pp_X_train.shape[-1],)))\n    hp_units = hp.Int('units', min_value = 10, max_value = 180, step = 45)\n    model.add(Dense(units = hp_units, activation = 'relu'))\n    model.add(Dense(units = hp_units, activation = 'relu'))\n    # Give the last layer\n    model.add(Dense(1, activation = 'linear'))\n    \n    #Create HP for learning rate\n    hp_learn_rate = hp.Choice('learning rate', values = [1e-4, 1e-3, 1e-2, 1e-1])\n    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learn_rate), \n                  loss = keras.losses.MeanAbsoluteError(reduction=\"auto\", name=\"mean_absolute_error\"),\n                  metrics = [keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None)])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use custom function to build auto-tuned MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner = kt.RandomSearch(model_builder,\n                        objective = 'val_loss',\n                        max_trials=5,\n                        executions_per_trial=4,\n                        directory = 'output',\n                        project_name = 'MLPRegressor')\n# Code Taken from intro to tensorflow core\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n  def on_train_end(*args, **kwargs):\n    IPython.display.clear_output(wait = True)\n    \ntuner.search(pp_X_train, y_train, epochs = 10, validation_data = (pp_X_valid, y_valid), callbacks = [ClearTrainingOutput()])\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 3)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"at_model = tuner.hypermodel.build(best_hps)\nprint(at_model.summary())\nepochs = 100\nat_trained_model = at_model.fit(pp_X_train, y_train, epochs = epochs, validation_data = (pp_X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the decrease of MAE with each iteration\nprint(f\"Training MAE : {at_trained_model.history['loss'][-1]}\")\nprint(f\"Validation MAE : {at_trained_model.history['val_loss'][-1]}\")\nimport matplotlib.pyplot as plt\nplt.clf()\nfig = plt.figure()\nfig.suptitle('Graph of training loss and validation loss')\nplt.plot(range(epochs), at_trained_model.history['loss'], 'b', range(epochs), at_trained_model.history['val_loss'], 'r')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}