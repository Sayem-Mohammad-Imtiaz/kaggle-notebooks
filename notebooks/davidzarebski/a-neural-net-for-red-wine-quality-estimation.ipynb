{"nbformat_minor":1,"metadata":{"language_info":{"nbconvert_exporter":"python","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","name":"python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"cells":[{"source":"# A neural NET for Red Wine Quality Estimation\n\nfun fact: wine testing is a common thought experiment used by philosophers who do not believe in strong AI (see the papers of Frank Jackson from the 80', as an example). Let's try, though**","cell_type":"markdown","metadata":{"_uuid":"234c7e2a954d79c624e0e26f38fbe7ccfb5cb327","_cell_guid":"17610759-c82a-495e-a152-fe9d8956ddea"}},{"source":"import pandas as pd \nimport numpy as np\n\nfrom keras import layers, optimizers, regularizers\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\nfrom keras.models import Sequential\n\nfrom keras.utils import plot_model\n#from kt_utils import *\nimport keras.backend as K\n\nimport seaborn as sns\n\nfrom sklearn import preprocessing, model_selection \n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"## Visualisation\nlet's first import the data","cell_type":"markdown","metadata":{}},{"source":"data = pd.read_csv(\"../input/winequality-red.csv\")\ndata[\"quality\"] =data[\"quality\"].astype(object)\ndata.tail(5)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"g = sns.pairplot(data, vars=[\"fixed acidity\", \"volatile acidity\",\"citric acid\"], hue=\"quality\")\nplt.show(g)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"h = sns.pairplot(data, vars=[\"residual sugar\", \"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\"], hue=\"quality\")\nplt.show(h)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"i = sns.pairplot(data, vars=[\"density\",\"pH\",\"sulphates\",\"alcohol\"], hue=\"quality\")\nplt.show(i)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"Looks like we could expect some patterns. Acidity does not affect the quality appreciation but sugar and sulfur do.\nHowever, there is something weird with the qualities. It does not seem that we have a nice distribution amongst the 9 scores.","cell_type":"markdown","metadata":{}},{"source":"j = sns.countplot(x=\"quality\", data=data)\nplt.show(j)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"Even worst than I feared. Not only we don't have samples of all the score but the central categories are over-represend (Not even gaussian styled). Thus, **we cannot expect miracles from modelisation**. Let's try, though.\n\n## Modelisation\n\nSince a quality score is a discrete value, the problem could be considered as a **multi-class classification problem**. The last layer of our network will thus involve a **softMax regression**.","cell_type":"markdown","metadata":{}},{"source":"data[\"quality\"] =data[\"quality\"].astype(int)\ndata = pd.get_dummies(data, columns=[\"quality\"])\ndata.head(5)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"X = data.iloc[:,0:11].values # first columns\nY = data.iloc[:,12:].values # last columns\n\nX = preprocessing.normalize(X, axis = 0)\n\nX_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size=0.2)\n\nprint(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"A simple sequential NN with **dropout **to prevent **overfitting**","cell_type":"markdown","metadata":{}},{"source":"winemod1 = Sequential()\n# layer 1\nwinemod1.add(Dense(30, input_dim=11, activation='relu', name='fc0',kernel_regularizer=regularizers.l2(0.01)))\nwinemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n#layer 2\nwinemod1.add(Dense(50, name='fc1',bias_initializer='zeros'))\nwinemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\nwinemod1.add(Activation('tanh'))\nwinemod1.add(Dropout(0.5))\n#layer 3\nwinemod1.add(Dense(100, name='fc2',bias_initializer='zeros'))\nwinemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\nwinemod1.add(Activation('relu'))\nwinemod1.add(Dropout(0.5))\n#layer 4\nwinemod1.add(Dense(5, name='fc3',bias_initializer='zeros'))\nwinemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\nwinemod1.add(Activation('softmax'))","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"winemod1.summary()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"We had to tune slightly the Adam optimizer","cell_type":"markdown","metadata":{}},{"source":"Adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nwinemod1.compile(optimizer = Adam, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"winemod1.fit(x = X_train, y = Y_train, epochs = 200,verbose=1, batch_size = 64,validation_data=(X_test, Y_test))","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"preds = winemod1.evaluate(x = X_test, y = Y_test)\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"This is the best I could obtain without overfitting. The algorithm might perform better with a better distribution among the different scores. An other explanation might be that wine testing is strongly subjective.","cell_type":"markdown","metadata":{}}],"nbformat":4}