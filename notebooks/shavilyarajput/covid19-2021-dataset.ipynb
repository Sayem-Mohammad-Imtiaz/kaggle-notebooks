{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-09T16:29:03.264692Z","iopub.execute_input":"2021-09-09T16:29:03.26518Z","iopub.status.idle":"2021-09-09T16:29:03.275172Z","shell.execute_reply.started":"2021-09-09T16:29:03.265147Z","shell.execute_reply":"2021-09-09T16:29:03.274075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading dataset \ndf = pd.read_csv('/kaggle/input/latest-covid19-india-statewise-data/Latest Covid-19 India Status.csv')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:03.276791Z","iopub.execute_input":"2021-09-09T16:29:03.27748Z","iopub.status.idle":"2021-09-09T16:29:03.299656Z","shell.execute_reply.started":"2021-09-09T16:29:03.277434Z","shell.execute_reply":"2021-09-09T16:29:03.298532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting first 5 rows \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:03.301755Z","iopub.execute_input":"2021-09-09T16:29:03.302171Z","iopub.status.idle":"2021-09-09T16:29:03.315836Z","shell.execute_reply.started":"2021-09-09T16:29:03.302126Z","shell.execute_reply":"2021-09-09T16:29:03.314888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking no of states & UT .\nprint('There are {} no of states & union territories info . '.format(len(df['State/UTs'])))\nprint('And they are: \\n {}'.format(df['State/UTs'].unique()))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:03.317606Z","iopub.execute_input":"2021-09-09T16:29:03.317904Z","iopub.status.idle":"2021-09-09T16:29:03.330486Z","shell.execute_reply.started":"2021-09-09T16:29:03.317872Z","shell.execute_reply":"2021-09-09T16:29:03.329558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking which states or UT has the highest active cases .\nplt.figure(figsize = (10,9))\nplt.plot(df['Active'],df['State/UTs'])\nplt.xlabel('No of Active Cases')\nplt.ylabel('States or Union Territories')","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:03.331789Z","iopub.execute_input":"2021-09-09T16:29:03.332088Z","iopub.status.idle":"2021-09-09T16:29:03.675128Z","shell.execute_reply.started":"2021-09-09T16:29:03.332051Z","shell.execute_reply":"2021-09-09T16:29:03.674482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>So,we can clearly figure out Ladakh,Kerala,Karnataka has the highest active cases.</h3>","metadata":{}},{"cell_type":"code","source":"# Checking which states or UT has the highest discharged cases .\nplt.figure(figsize = (10,9))\nplt.plot(df['Discharged'],df['State/UTs'])\nplt.xlabel('Discharged Cases')\nplt.ylabel('States or Union Territories')","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:03.676034Z","iopub.execute_input":"2021-09-09T16:29:03.676383Z","iopub.status.idle":"2021-09-09T16:29:04.01214Z","shell.execute_reply.started":"2021-09-09T16:29:03.676356Z","shell.execute_reply":"2021-09-09T16:29:04.011387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Manipur,Maharashtra,Madhya Pradesh has the highest discharge numbers,\nfollowed by Ladakh , Kerala , Karnataka and etc </h3> ","metadata":{}},{"cell_type":"code","source":"# Checking which states or UT has the highest death cases .\nplt.figure(figsize = (10,9))\nplt.plot(df['Deaths'],df['State/UTs'])\nplt.xlabel('Deaths Cases')\nplt.ylabel('States or Union Territories')","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:04.013127Z","iopub.execute_input":"2021-09-09T16:29:04.013514Z","iopub.status.idle":"2021-09-09T16:29:04.357224Z","shell.execute_reply.started":"2021-09-09T16:29:04.013485Z","shell.execute_reply":"2021-09-09T16:29:04.356535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>We cannot analyse Deaths by No of discharge cases because we can see No of discharge cases in Maharashtra was the most but still Maharashtra has the highest death cases <.h3>","metadata":{}},{"cell_type":"code","source":"ratio_df = df.iloc[:,5:]\nratio_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:04.35864Z","iopub.execute_input":"2021-09-09T16:29:04.358997Z","iopub.status.idle":"2021-09-09T16:29:04.371884Z","shell.execute_reply.started":"2021-09-09T16:29:04.358968Z","shell.execute_reply":"2021-09-09T16:29:04.370953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,9))\nplt.plot(ratio_df,df['State/UTs'] , label = ('Active','Discharge','Death'))\nplt.xlabel(\"Ratio's of Active,Discharge and Death cases.\")\nplt.ylabel('States or Union Territories')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:29:04.3733Z","iopub.execute_input":"2021-09-09T16:29:04.373603Z","iopub.status.idle":"2021-09-09T16:29:04.694962Z","shell.execute_reply.started":"2021-09-09T16:29:04.373574Z","shell.execute_reply":"2021-09-09T16:29:04.694057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>As we can see death ratios are very minimal campared to active ratio's</h3>","metadata":{}},{"cell_type":"markdown","source":"<h3> So let's try to predict No of deaths </h3>","metadata":{}},{"cell_type":"code","source":"#We have got str values in States/UTs columns\ns = (df.dtypes == 'object')\nobject_cols = s[s].index\nprint(\"Categorical Variable :{} \".format(object_cols))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:39:14.184463Z","iopub.execute_input":"2021-09-09T16:39:14.184996Z","iopub.status.idle":"2021-09-09T16:39:14.192536Z","shell.execute_reply.started":"2021-09-09T16:39:14.184952Z","shell.execute_reply":"2021-09-09T16:39:14.191537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#so lets convert them into int values.\nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\ndf[object_cols] = oe.fit_transform(df[object_cols])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:40:43.624635Z","iopub.execute_input":"2021-09-09T16:40:43.624958Z","iopub.status.idle":"2021-09-09T16:40:43.632414Z","shell.execute_reply.started":"2021-09-09T16:40:43.624929Z","shell.execute_reply":"2021-09-09T16:40:43.631512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So lets check the values which are assigned by Ordinal Encoder to object column.\n\ndf[object_cols]","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:42:15.994357Z","iopub.execute_input":"2021-09-09T16:42:15.994956Z","iopub.status.idle":"2021-09-09T16:42:16.00754Z","shell.execute_reply.started":"2021-09-09T16:42:15.994908Z","shell.execute_reply":"2021-09-09T16:42:16.00653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's one again check our dataframe .\ndf.sample(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:43:05.694343Z","iopub.execute_input":"2021-09-09T16:43:05.694884Z","iopub.status.idle":"2021-09-09T16:43:05.708778Z","shell.execute_reply.started":"2021-09-09T16:43:05.694835Z","shell.execute_reply":"2021-09-09T16:43:05.707816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Divide features and labels for train_test_split\ndf_features = df.iloc[:,:]\ndf_label = df['Deaths']","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:43:53.413521Z","iopub.execute_input":"2021-09-09T16:43:53.413833Z","iopub.status.idle":"2021-09-09T16:43:53.418185Z","shell.execute_reply.started":"2021-09-09T16:43:53.413801Z","shell.execute_reply":"2021-09-09T16:43:53.416969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since We have label as Deaths so we need to drop it from the features.\ndf_features = df_features.drop('Deaths',axis =1 )\ndf_features.sample(1)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:43:58.218502Z","iopub.execute_input":"2021-09-09T16:43:58.218821Z","iopub.status.idle":"2021-09-09T16:43:58.23258Z","shell.execute_reply.started":"2021-09-09T16:43:58.218793Z","shell.execute_reply":"2021-09-09T16:43:58.231597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We don't have our features in same scale i.e some values are close to 100 and some are close to 1\n# So in order to get the best model we should scale our features.\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndf_features_cols = df_features.columns \n# In scaling , standard scaler will remove the columns from features.\n# So we need to add those columns back to the scaled features\ndf_features[df_features_cols] = sc.fit_transform(df_features[df_features_cols])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:44:02.533369Z","iopub.execute_input":"2021-09-09T16:44:02.533744Z","iopub.status.idle":"2021-09-09T16:44:02.54649Z","shell.execute_reply.started":"2021-09-09T16:44:02.533711Z","shell.execute_reply":"2021-09-09T16:44:02.545373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check our scaled features \ndf_features.sample(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:44:41.77889Z","iopub.execute_input":"2021-09-09T16:44:41.779224Z","iopub.status.idle":"2021-09-09T16:44:41.792238Z","shell.execute_reply.started":"2021-09-09T16:44:41.779195Z","shell.execute_reply":"2021-09-09T16:44:41.791331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to split our dataframe into train_test . \n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(df_features,df_label,test_size = 0.2 )\nprint('Shape of x_train: ',x_train.shape)\nprint('Shape of y_train:',y_train.shape)\nprint('Shape of x_test:',x_test.shape)\nprint('Shape of y_test:',y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:58:08.044806Z","iopub.execute_input":"2021-09-09T16:58:08.045147Z","iopub.status.idle":"2021-09-09T16:58:08.053323Z","shell.execute_reply.started":"2021-09-09T16:58:08.045107Z","shell.execute_reply":"2021-09-09T16:58:08.052139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's build ML model out of this data now. \n\nfrom sklearn.linear_model import LinearRegression\nlr_model = LinearRegression()\nlr_model.fit(x_train,y_train)\nlr_model.score(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:58:11.509181Z","iopub.execute_input":"2021-09-09T16:58:11.509563Z","iopub.status.idle":"2021-09-09T16:58:11.748645Z","shell.execute_reply.started":"2021-09-09T16:58:11.509526Z","shell.execute_reply":"2021-09-09T16:58:11.747593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to do predictions on x_test \ny_pred = lr_model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:58:40.879638Z","iopub.execute_input":"2021-09-09T16:58:40.879957Z","iopub.status.idle":"2021-09-09T16:58:40.885371Z","shell.execute_reply.started":"2021-09-09T16:58:40.879929Z","shell.execute_reply":"2021-09-09T16:58:40.884674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error,r2_score\nprint(\"Mean Squared Error: {}\".format(mean_squared_error(y_test,y_pred)))\nprint(\"R2 Score: {}\".format(r2_score(y_test,y_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:02:43.988963Z","iopub.execute_input":"2021-09-09T17:02:43.989551Z","iopub.status.idle":"2021-09-09T17:02:43.995815Z","shell.execute_reply.started":"2021-09-09T17:02:43.989517Z","shell.execute_reply":"2021-09-09T17:02:43.99506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> The less the value of Mean Squared Error the better the model is , Whereas for R2 score the closer the value gets to 100 % the better the model.</h3>","metadata":{}}]}