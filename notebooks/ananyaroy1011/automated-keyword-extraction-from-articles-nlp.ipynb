{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Background**\nIn research & news articles, keywords form an important component since they provide a concise representation of the article’s content. Keywords also play a crucial role in locating the article from information retrieval systems, bibliographic databases and for search engine optimization. Keywords also help to categorize the article into the relevant subject or discipline.\nConventional approaches of extracting keywords involve manual assignment of keywords based on the article content and the authors’ judgment. This involves a lot of time & effort and also may not be accurate in terms of selecting the appropriate keywords. With the emergence of Natural Language Processing (NLP), keyword extraction has evolved into being effective as well as efficient.\nAnd in this article, we will combine the two — we’ll be applying NLP on a collection of articles (more on this below) to extract keywords.","metadata":{}},{"cell_type":"markdown","source":"# About the dataset\nIn this article, we will be extracting keywords from a dataset that contains about 3,800 abstracts. The original dataset is from Kaggle — NIPS Paper. Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. This dataset includes the title and abstracts for all NIPS papers to date (ranging from the first 1987 conference to the current 2016 conference).\nThe original dataset also contains the article text. However, since the focus is on understanding the concept of keyword extraction and using the full article text could be computationally intensive, only abstracts have been used for NLP modelling. The same code block can be used on the full article text to get a better and enhanced keyword extraction.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T04:10:45.5806Z","iopub.execute_input":"2021-06-05T04:10:45.581142Z","iopub.status.idle":"2021-06-05T04:10:45.599566Z","shell.execute_reply.started":"2021-06-05T04:10:45.581043Z","shell.execute_reply":"2021-06-05T04:10:45.598433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing the dataset**\nThe dataset used for this article is a subset of the papers.csv dataset provided in the NIPS paper datasets on Kaggle. Only those rows that contain an abstract have been used. The title and abstract have been concatenated after which the file is saved as a tab separated *.txt file.","metadata":{}},{"cell_type":"code","source":"import pandas\n# load the dataset\ndataset = pandas.read_csv('/kaggle/input/nips-papers/papers.csv')\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:45.653552Z","iopub.execute_input":"2021-06-05T04:10:45.653873Z","iopub.status.idle":"2021-06-05T04:10:49.368282Z","shell.execute_reply.started":"2021-06-05T04:10:45.653842Z","shell.execute_reply":"2021-06-05T04:10:49.367389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preliminary text exploration**\nBefore we proceed with any text pre-processing, let's quickly explore the dataset in terms of word counts, most common and most uncommon words.","metadata":{}},{"cell_type":"markdown","source":"# **Fetch word count for each abstract**","metadata":{}},{"cell_type":"code","source":"#Fetch wordcount for each abstract\ndataset['word_count'] = dataset['title'].apply(lambda x: len(str(x).split(\" \")))\ndataset[['title','word_count']].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:49.369831Z","iopub.execute_input":"2021-06-05T04:10:49.370223Z","iopub.status.idle":"2021-06-05T04:10:49.390628Z","shell.execute_reply.started":"2021-06-05T04:10:49.370183Z","shell.execute_reply":"2021-06-05T04:10:49.389802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average word count is about 156 words per abstract. The word count ranges from a minimum of 27 to a maximum of 325. The word count is important to give us an indication of the size of the dataset that we are handling as well as the variation in word counts across the rows.","metadata":{}},{"cell_type":"code","source":"##Descriptive statistics of word counts\ndataset.word_count.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:49.392553Z","iopub.execute_input":"2021-06-05T04:10:49.392846Z","iopub.status.idle":"2021-06-05T04:10:49.412449Z","shell.execute_reply.started":"2021-06-05T04:10:49.392812Z","shell.execute_reply":"2021-06-05T04:10:49.411391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identify common words\nfreq = pandas.Series(' '.join(dataset['title']).split()).value_counts()[:20]\nfreq","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:49.413957Z","iopub.execute_input":"2021-06-05T04:10:49.414228Z","iopub.status.idle":"2021-06-05T04:10:49.446204Z","shell.execute_reply.started":"2021-06-05T04:10:49.414203Z","shell.execute_reply":"2021-06-05T04:10:49.445179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identify uncommon words\nfreq1 =  pandas.Series(' '.join(dataset \n         ['title']).split()).value_counts()[-20:]\nfreq1","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:49.447517Z","iopub.execute_input":"2021-06-05T04:10:49.447949Z","iopub.status.idle":"2021-06-05T04:10:49.475516Z","shell.execute_reply.started":"2021-06-05T04:10:49.447918Z","shell.execute_reply":"2021-06-05T04:10:49.474838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nstem = PorterStemmer()\nword = \"inversely\"\nprint(\"stemming:\",stem.stem(word))\nprint(\"lemmatization:\", lem.lemmatize(word, \"v\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:49.476601Z","iopub.execute_input":"2021-06-05T04:10:49.477039Z","iopub.status.idle":"2021-06-05T04:10:52.565462Z","shell.execute_reply.started":"2021-06-05T04:10:49.47701Z","shell.execute_reply":"2021-06-05T04:10:52.564494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Libraries for text preprocessing\nimport re\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\n#nltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:52.566712Z","iopub.execute_input":"2021-06-05T04:10:52.566994Z","iopub.status.idle":"2021-06-05T04:10:52.571976Z","shell.execute_reply.started":"2021-06-05T04:10:52.566966Z","shell.execute_reply":"2021-06-05T04:10:52.570654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Creating a list of stop words and adding custom stopwords\nstop_words = set(stopwords.words(\"english\"))\n##Creating a list of custom stopwords\nnew_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\"]\nstop_words = stop_words.union(new_words)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:52.574776Z","iopub.execute_input":"2021-06-05T04:10:52.575192Z","iopub.status.idle":"2021-06-05T04:10:52.60503Z","shell.execute_reply.started":"2021-06-05T04:10:52.575149Z","shell.execute_reply":"2021-06-05T04:10:52.604121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = []\nfor i in range(0, 3847):\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', dataset['title'][i])\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:52.608601Z","iopub.execute_input":"2021-06-05T04:10:52.608884Z","iopub.status.idle":"2021-06-05T04:10:52.809112Z","shell.execute_reply.started":"2021-06-05T04:10:52.608857Z","shell.execute_reply":"2021-06-05T04:10:52.808388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#View corpus item\ncorpus[222]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:52.810156Z","iopub.execute_input":"2021-06-05T04:10:52.810565Z","iopub.status.idle":"2021-06-05T04:10:52.81544Z","shell.execute_reply.started":"2021-06-05T04:10:52.810524Z","shell.execute_reply":"2021-06-05T04:10:52.814615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Word cloud\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:52.816563Z","iopub.execute_input":"2021-06-05T04:10:52.816862Z","iopub.status.idle":"2021-06-05T04:10:54.725906Z","shell.execute_reply.started":"2021-06-05T04:10:52.816835Z","shell.execute_reply":"2021-06-05T04:10:54.724941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:54.72704Z","iopub.execute_input":"2021-06-05T04:10:54.72734Z","iopub.status.idle":"2021-06-05T04:10:54.876142Z","shell.execute_reply.started":"2021-06-05T04:10:54.727304Z","shell.execute_reply":"2021-06-05T04:10:54.875255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(cv.vocabulary_.keys())[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:54.877305Z","iopub.execute_input":"2021-06-05T04:10:54.877582Z","iopub.status.idle":"2021-06-05T04:10:54.884168Z","shell.execute_reply.started":"2021-06-05T04:10:54.877552Z","shell.execute_reply":"2021-06-05T04:10:54.883283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequently occuring words\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(corpus, n=20)\ntop_df = pandas.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\n#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:54.885306Z","iopub.execute_input":"2021-06-05T04:10:54.88562Z","iopub.status.idle":"2021-06-05T04:10:55.353986Z","shell.execute_reply.started":"2021-06-05T04:10:54.885593Z","shell.execute_reply":"2021-06-05T04:10:55.353012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequently occuring Bi-grams\ndef get_top_n2_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2),  \n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop2_words = get_top_n2_words(corpus, n=20)\ntop2_df = pandas.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)\n#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:55.355802Z","iopub.execute_input":"2021-06-05T04:10:55.356203Z","iopub.status.idle":"2021-06-05T04:10:55.782673Z","shell.execute_reply.started":"2021-06-05T04:10:55.356163Z","shell.execute_reply":"2021-06-05T04:10:55.781719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequently occuring Tri-grams\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), \n           max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pandas.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)\n#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:10:55.783935Z","iopub.execute_input":"2021-06-05T04:10:55.784233Z","iopub.status.idle":"2021-06-05T04:10:56.229498Z","shell.execute_reply.started":"2021-06-05T04:10:55.784206Z","shell.execute_reply":"2021-06-05T04:10:56.228517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)\n# get feature names\nfeature_names=cv.get_feature_names()\n \n# fetch document for which keywords needs to be extracted\ndoc=corpus[532]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:12:29.270012Z","iopub.execute_input":"2021-06-05T04:12:29.270366Z","iopub.status.idle":"2021-06-05T04:12:29.294644Z","shell.execute_reply.started":"2021-06-05T04:12:29.270336Z","shell.execute_reply":"2021-06-05T04:12:29.293502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function for sorting tf_idf in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,5)\n \n# now print the results\nprint(\"\\nAbstract:\")\nprint(doc)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:12:34.367216Z","iopub.execute_input":"2021-06-05T04:12:34.367567Z","iopub.status.idle":"2021-06-05T04:12:34.377632Z","shell.execute_reply.started":"2021-06-05T04:12:34.367533Z","shell.execute_reply":"2021-06-05T04:12:34.376674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ideally for the IDF calculation to be effective, it should be based on a large corpora and a good representative of the text for which the keywords need to be extracted. In our example, if we use the full article text instead of the abstracts, the IDF extraction would be much more effective. However, considering the size of the dataset, I have limited the corpora to just the abstracts for the purpose of demonstration.\nThis is a fairly simple approach to understand fundamental concepts of NLP and to provide a good hands-on practice with some python codes on a real-life use case. The same approach can be used to extract keywords from news feeds & social media feeds.","metadata":{}}]}