{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Práctica 1: Análisis exploratorio de datos, preprocesamiento y validación de modelos de clasificación\\*\n\n### Minería de Datos: Curso académico 2020-2021\n\n### Compañeros:Juan Ibañez y Raul Milla"},{"metadata":{},"cell_type":"markdown","source":"# 1.Preliminares"},{"metadata":{},"cell_type":"markdown","source":"Iniciamos las distintas bibliotecas y script necesarios para "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Third party\nfrom matplotlib import pyplot as mpl\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer, Normalizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nimport plotly.figure_factory as ff\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as panda\nfrom imblearn.over_sampling import SMOTE\n# Local application\nimport miner_a_de_datos_an_lisis_exploratorio_utilidad as utils\nseed = 27912","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Analizamos la base de datos\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Esta base de datos nos cuenta basicamente si una persona tiene un cancer benigno o maligno(este atributo sera el objetivo de nuestro sistema)por otro lado tenemos otros atributos que nos indican la media,el error y la desviacion de algunos datos sobre las celulas del paciente.\nLo primero que haremos sera dividir los diferentes **casos** entre test y training,mas tarde estos casos se dividiran por sus **variables**(separaremos el objetivo de los demas).Antes de esto tendriamos que eliminar una **variable**  en todos los casos,la ultima que es un null y no aporta nada al estudio.Luego tendriamos que buscar aquellas **variables** que tampoco aportan mucho al estudio para ello buscaremos su correlacion con las demas ,pero antes lo normalizaremso dividiendo cada **variable** entre el valor maximo de todos los casos\n"},{"metadata":{},"cell_type":"markdown","source":"Agregamos los datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepathC = \"../input/breast-cancer-wisconsin-data/data.csv\"\n\nindexC = \"id\"\ntargetC = \"diagnosis\"\n\n\ndataC = utils.load_data(filepathC, indexC, targetC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez cargado hacemos una prueba para comprobar que efectivamente se ha cargado la BBDD,haciendo una tirada de las 5 primeras"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que añade una tabla que sobra,por lo que eliminamos"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC = dataC.drop(dataC.columns[31], axis='columns')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Volvemos a comprobar que esta vez la tabla este bien y que ya no aparece la ultima columna"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determinados que son los atributos y cuales los target"},{"metadata":{"trusted":true},"cell_type":"code","source":"(X, y) = utils.divide_dataset(dataC, target=targetC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez que comprobamos que se realiza correctamente determinamos que cantidad de datos usamos para el entrenamiento(un 70%).Tambien definimos las X e Y de test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hacemos una comprobacion de que se ha cargado"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.sample(5, random_state=seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Procedemos a juntar los atributos con los target"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC_train = utils.join_dataset(X_train, y_train)\ndataC_test = utils.join_dataset(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Analisis de los datos de CW"},{"metadata":{},"cell_type":"markdown","source":"Comprobamos cuantos **casos** hay "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hay 398 casos y 31 variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC_train.info(memory_usage=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Asi vemos que no hay ningun dato dañado,es decir que todos los casos tienes sus variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_histogram(dataC_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Al haber tantas **variables** no podemos verlo claramente y no solo eso al no estar normalizado tenemos **variables** que van a lo mejor de 0 a 100 y otros que van hasta mas de 4000 por lo que para ver todos los datos juntos debemos de normalizarlo entre el maximo de cada uno para que todo vaya de 0 a 1(esto lo haremos mas adelante)\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_barplot(dataC_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_pairplot(dataC_train, target=targetC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos ver no podemos ver casi nada,por lo que tenemos que restringir los datos.\nTambien ya podemos eliminar algunos datos como por ejemplo perimetro y area ya que son dependientes del radio,pero en vez de eso haremos una tabla de coorelacion\n"},{"metadata":{},"cell_type":"markdown","source":"Ahora para poder constrastar datos debemos de separar diagnosis de los demas"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC_trainC,dataC_trainO=(dataC_train.drop(\"diagnosis\",axis=1),dataC_train[\"diagnosis\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para buscar que otro atributos no nos son utiles,dividimos cada columna por su maximo"},{"metadata":{"trusted":true},"cell_type":"code","source":"columnas=[]\nfor i in dataC.drop(['diagnosis'],axis='columns'):\n    maximo=max(dataC[i])\n    dataC[i]=dataC[i]/maximo\n    columnas.append(i)\nfor i in dataC_train.drop(['diagnosis'],axis='columns'):\n    maximo=max(dataC_train[i])\n    dataC_train[i]=dataC_train[i]/maximo\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora tenemos que observar los datos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=mpl.subplots(figsize=(31,31))\ncorrMatrix = dataC_trainC.corr()\n\nsns.heatmap(corrMatrix,annot=True,linewidths=.5,fmt=\".1f\",ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A partir de esto podemos ver que hay algunos datos que por asi decirlo sobran ya que tienen mucha correlacion,asi que lo que haremos sera una funcion para que localice aquellos que tienen una correlacion de 0.9 o mas y luego eliminarlo"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorias=dataC_trainC.columns\nx=0\naux=x\neliminado=[]\nfor i in categorias:\n    aux=x+1\n    no_encontrado=True\n    while (aux<len(categorias) and no_encontrado and not(i in eliminado)  ):\n        #print(eliminado)\n        if(corrMatrix[i][categorias[aux]]>0.8):\n            #print([corrMatrix[i][categorias[aux]]])\n            no_encontrado=False\n            eliminado.append(i)\n            \n        aux=aux+1\n    x=x+1\nfor i in eliminado:\n    dataC_trainC=dataC_trainC.drop(i,axis=1)\n    dataC_train=dataC_train.drop(i,axis=1)\nprint(eliminado)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=mpl.subplots(figsize=(len(dataC_trainC.columns),len(dataC_trainC.columns)))\ncorrMatrix = dataC_trainC.corr()\n\nsns.heatmap(corrMatrix,annot=True,linewidths=.5,fmt=\".1f\",ax=ax)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora podemos ver que no hay ninguna correlacion mayor que 0.9"},{"metadata":{},"cell_type":"raw","source":"Ahora veremos de nuevo los datos y se veran mejor "},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_barplot(dataC_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_pairplot(dataC_train, target=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_histogram(dataC_trainC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por ultimo debemos de separar los datos del test como ya hicimos anteriormente"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC_testC,dataC_testO=(dataC_test.drop(\"diagnosis\",axis=1),dataC_test[\"diagnosis\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# 4.Prepocesamiento de datos"},{"metadata":{},"cell_type":"markdown","source":"Ahora tenemos que hacer varias tareas:\n* Limpieza de datos\n* Integracion de datos\n* Transformacion de datos\n* Reduccion de datos\nLas dos ultimas ya se han realizado con anterioridad,y sobre la limpieza ya hemos visto que no es necesario ya que no hay datos anomalos o nuloss "},{"metadata":{},"cell_type":"markdown","source":"# Discretizacion"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features = SelectKBest(chi2, 4)\nl1_normalizer = Normalizer(\"l1\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.Algoritmos de classificacion y evaluacion"},{"metadata":{},"cell_type":"markdown","source":"Empezamos Algoritmo Zero-R"},{"metadata":{"trusted":true},"cell_type":"code","source":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")\nutils.evaluate(zero_r_model,\n               dataC_trainC, dataC_testC,\n               dataC_trainO, dataC_testO)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como era de esperar no nos da mucha confianza ya que basicamente coge el valor que mas se repite en el train y se lo aplica a todos los test"},{"metadata":{},"cell_type":"markdown","source":"Ahora probaremos con el modelo tree_model"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier(random_state=seed)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para este caso debemos de reducir el test ya que tiene mas atributos que el train"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataC_testC_red=dataC_testC\nfor i in eliminado:\n    dataC_testC_red=dataC_testC_red.drop(i,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(tree_model,\n               dataC_trainC, dataC_testC_red,\n               dataC_trainO, dataC_testO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizer = KBinsDiscretizer(n_bins=2, strategy=\"uniform\")\ndiscretize_tree_model = make_pipeline(discretizer, tree_model)\nutils.evaluate(discretize_tree_model,\n               dataC_trainC, dataC_testC_red,\n               dataC_trainO, dataC_testO)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tras hacer las diferentes pruebas vemos que el mejor algoritmo que tenemos es el modelo de arbol pero sin descretizar con un error de menos de un 8 por cierto y vemos que el peor obviamente ha sido el de Zero ya que ese nos da un error muy alto(casi el 40%).Por otro lado no podria decir cual es mejor y cual se recomieda usar por que por un lado el algortimo de arbo descretizado solo da un benigno mal asi que dependiendo de que es mas valioso si o bien que se equivoque en falsos positivos o en positivos falsos o en los dos."},{"metadata":{},"cell_type":"markdown","source":"# Práctica 1: Análisis exploratorio de datos, preprocesamiento y validación de modelos de clasificación\\*\n\n### Minería de Datos: Curso académico 2020-2021\n\n### Pima diabetes"},{"metadata":{},"cell_type":"markdown","source":"# 1. Preliminares"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 27912","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Acceso y almacenamiento de datos"},{"metadata":{},"cell_type":"markdown","source":"El conjunto de datos que vamos a emplear es Pima diabetes. Tiene dos posibles resultados.\n\n0 Cuando el paciente no tiene diabetes\n1 Cuando el paciente tiene diabetes\nQue conforman los valores de la variable a predecir, la variable objetivo (Outcome). Se tienene en cuenta las siguientes variables predictoras:\n\nAge: Edad del paciente.\nPregnancies: Número de embarazos de la paciente.\nGlucose: Concentración de glucosa en plasma sanguíneo.\nBloodPressure: Presión diastólica arterial (mm Hg).\nSkinThickness: Grosor de la piel en el triceps (mm).\ninsulin: Cantidad de insulina en sangre (mu Insulina/ml).\nBMI: Índice de Masa Corporal (kg/m^2).\nDiabetesPedigreeFunction: Función para el historial de diabetes en la familia del paciente."},{"metadata":{},"cell_type":"markdown","source":"Cargamos el data-set"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/pima-indians-diabetes-database/diabetes.csv\"\n\nindex = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En el caso de Pima Diabetes no disponemos de variable índice, asi que index lo ponemos a \"none\"\n Usamos la función head para obtener las n primeras instancias del conjunto de datos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pero para conseguir una muestra menos sesgada hacemos una aleatoria, porque los data-set suelen estar ordenanos en función de sus variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creamos conjunto de datos separado dos subconjuntos, uno con las variables predictoras (X) y otro con la variable objetivo (y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"(X, y) = utils.divide_dataset(data, target=\"Outcome\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos que los conjuntos de las variables predictoras y el de la variable objetivo se han sepeparado correctamente"},{"metadata":{},"cell_type":"markdown","source":"X.sample(5, random_state=seed)"},{"metadata":{},"cell_type":"markdown","source":"Comprobamos también con la variable objetivo:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para hacer un proceso de Holdout creamos la muestra de Test y la muestra de training con la siguiente proporción:\n\n* Muestra training (70%)\n* Muestra test (típicamente, 30%)\n\n\nPara realizar un *holdout* podemos utilizar el método `train_test_split` de `scikit-learn`:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comenzamos con las variables predictoras del conjunto de datos de training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hacemos la muestra con el conjunto de training"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Análisis exploratorio de datos"},{"metadata":{},"cell_type":"markdown","source":"Ahora para ver las propiedades de nuestro conjunto de datos y poder sacar conclusiones de cara al preprocesamiento usaremos gráficos y estadisticos."},{"metadata":{},"cell_type":"markdown","source":"### Descripción del conjunto de datos"},{"metadata":{},"cell_type":"markdown","source":"Con shape podemos saber la cantidad de casos que hay en nuestra base de datos frente a la cantidad que variables que disponemos, 768 pacientes estudiados frente a las nueve variables que disponemos, que son las diferentes pruebas que se han hecho a estos pacientes."},{"metadata":{},"cell_type":"markdown","source":"Usamos el metodo info para saber de que tipo son las variables"},{"metadata":{},"cell_type":"markdown","source":"data.info(memory_usage=False)"},{"metadata":{},"cell_type":"markdown","source":"En nuestro caso todas las variables predictoras son varibales continuas, y solo la variable obejetivo es una variable discreta,"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.cat.categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nuestra variable objetivo tiene dos posibles valores, 0 y 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Visualización de las variables"},{"metadata":{},"cell_type":"markdown","source":"Vamos a comenzar visualizando las variables numéricas del conjunto de datos usando histogramas para las variables numéricas y diagramas de barras para las variables categóricas"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_histogram(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar varias variables tiene tienen datos ruidosos, todos los que tienen una cantidad 0, alejados de la mayoría de valores de esas variables, este sería el caso de`Glucose` , `BloodPressure`, `SkinThickness`,`insulin` y `BMI`. Tambein podemos ver que `SkinThickness`, `Age` y `Insulin` tienen una disposición central. No encuentro valores anomalos en este analisis multivariado."},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_barplot(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En el caso de nuestra única variable categórica podemos ver que la muestra no esta balanceada, de nuestros 768 pacientes la mayoría dio negativo en tener diabetes como nos dice este analisis univariado."},{"metadata":{},"cell_type":"markdown","source":"A continuacion creamos dos matrices para analizar la correlación entre pares de variables, en el primer caso el numero de cada hueco representa la coincidencia o solopamiento entre variables , que en caso de ser demasiado alta nos podria indicar la redundancia de ciertas variables predictoras"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=mpl.subplots(figsize=(31,31))\ncorrMatrix = X_train.corr()\n\nsns.heatmap(corrMatrix,annot=True,linewidths=.5,fmt=\".1f\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viendo el resultado ninguna de las variables predictoras es redundante"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_pairplot(data, target=\"Outcome\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En esta caso a diferencia de la de iris no podemos averiguar claramente el poder discriminatorio de cada variable predictora pues hay muchos datos ruidosos."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include=\"number\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include=\"category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Preprocesamiento de datos"},{"metadata":{},"cell_type":"markdown","source":"Lo primero que haremos para tratar los datos crudos sera una imputación de valores perdidos,hay varias variables , 'Glucose','BloodPressure','Insulin','BMI','SkinThickness', que por el contexto no deberían tener valores a 0, así que lo que haremos será sustituir esos valores por la mediana de cada uno , separando también segun la variable objetivo."},{"metadata":{},"cell_type":"markdown","source":"Calculamos la mediana de esas variables para usarlas a modo de marcador"},{"metadata":{"trusted":true},"cell_type":"code","source":"def medina(variable):\n    sumt1=[]\n    sumt2=[]\n    n1=0\n    n2=0\n    t=0\n    for i in data[variable]:\n        j=data['Outcome'][t]\n        t=t+1\n        if (j==1 and i>0):\n            n1=n1+1\n            sumt1.append(i)\n        elif (j ==0 and i>0):\n            n2=n2+1\n            sumt2.append(i)\n    return (np.median(sumt1),np.median(sumt2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medina('Glucose')\nmedina('BloodPressure')\nmedina('BMI')\nmedina('SkinThickness')\nmedina('Insulin')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con las medianas calculadas pasamos a remplazar los 0 por NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"col=['Glucose','BloodPressure','Insulin','BMI','SkinThickness']\nfor i in col:\n    data[i].replace(0, np.nan, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora remplazamos los NaN por las medianas calculadas"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data['Outcome'] == 0 ) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1 ) & (data['Insulin'].isnull()), 'Insulin'] = 169.5\ndata.loc[(data['Outcome'] == 0 ) & (data['Glucose'].isnull()), 'Glucose'] = 107\ndata.loc[(data['Outcome'] == 1 ) & (data['Glucose'].isnull()), 'Glucose'] = 140\ndata.loc[(data['Outcome'] == 0 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndata.loc[(data['Outcome'] == 1 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32\ndata.loc[(data['Outcome'] == 0 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndata.loc[(data['Outcome'] == 1 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.5\ndata.loc[(data['Outcome'] == 0 ) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1 ) & (data['BMI'].isnull()), 'BMI'] = 34.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_histogram(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos que los valores con 0 de las variables han sido eliminados"},{"metadata":{},"cell_type":"markdown","source":"Para tratar los datos anomalos usaremos StandarScaler que \"escala\" la propiedad restando por la media y diviendo por la desviación estándar, con esto conseguiremos que los datos anomalos sean menos sesgados que le resto en comparacion con el resto."},{"metadata":{"trusted":true},"cell_type":"code","source":"yOut = data.Outcome\nXOut = data.drop('Outcome', axis = 1)\ncolumns = XOut.columns\nscaler = StandardScaler()\nXOut = scaler.fit_transform(XOut)\ndataOut_x = panda.DataFrame(XOut, columns = columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creamos nuevos subconjuntos sin los datos anomalos"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dataOut_x, yOut,\n                                                                    stratify=yOut, random_state = seed, train_size = train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y por último discretizaremos,teniendo en cuenta lo visto en el diagrama de puntos probaremos con las tres discretizaciones , las tres con dos bins ya que nuestra variable objetivo tiene dos valores"},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizerQuantile = KBinsDiscretizer(n_bins=2, strategy='quantile')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizerUniform = KBinsDiscretizer(n_bins=2, strategy=\"uniform\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizerKmeans = KBinsDiscretizer(n_bins=2, strategy=\"kmeans\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Algoritmos de clasificación"},{"metadata":{},"cell_type":"markdown","source":"Genreamos modelos de ZeroR y de CART. Para ello usamos los siguientes clasificadores de la librería  scikit-learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El algoritmo zero_r no nos dara mucha precisión pero nos servira como baseline, como referencia para el resto. Para que salga la clase más frecuente del subconjunto train usamos el hiperparámetro strategy=\"most_frequent\""},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier(random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tambien creamos una pipeline para cada una de las discretizaciones y usando el algoritmo CART"},{"metadata":{"trusted":true},"cell_type":"code","source":"discretize_tree_model_Quantile = make_pipeline(discretizerQuantile, tree_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discretize_tree_model_Uniform = make_pipeline(discretizerUniform, tree_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discretize_tree_model_Kmeans = make_pipeline(discretizerKmeans, tree_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Evaluación de modelos"},{"metadata":{},"cell_type":"markdown","source":"* ### ZERO-R"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(zero_r_model,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El zero-r nos da la precisión más baja de los algoritmos testeados."},{"metadata":{},"cell_type":"markdown","source":"* ### CART sin discretización."},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(tree_model,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### CART con discretizacion de igual frecuencia (Quantile)"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(discretize_tree_model_Quantile,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### CART con discretizacion de igual anchura (Uniform)"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(discretize_tree_model_Uniform,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### CART con discretizacion basada en k-medias (Kmeans)"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(discretize_tree_model_Kmeans,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"omo podemos ver ninguna de las tres discretizaciones resultan en una mejora del algoritmo CART, todo lo contrario pues la precisión del algortimo es sensible superior sin ninguna discretización, aun así podemos observar que la que da mejor resultado de las tres es la discretización de igual anchura."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}