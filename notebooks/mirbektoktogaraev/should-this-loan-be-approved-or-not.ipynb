{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Summary**\n\nI am a Junior Data Analyst and this is my very first kaggle notebook.\nThe main goal of this analysis is to get an understanding of the dataset.\nI would be very grateful for comments on EDA and data preprocessing parts.\nFor prediction I tried xgboost without any tunning. The last part will be updated.\n\n**Data**\n\nThe original [dataset](https://www.kaggle.com/mirbektoktogaraev/should-this-loan-be-approved-or-denied) from the U.S. Small Business Administration (SBA)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport os\nimport math\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install uszipcode\nimport uszipcode\nfrom uszipcode import SearchEngine\nsearch = SearchEngine()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read csv file\ncredit = pd.read_csv('../input/should-this-loan-be-approved-or-denied/SBAnational.csv', header ='infer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Quick overview of our Dataset and variables\ncredit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting duplicates if any\ncredit = credit.drop_duplicates(keep = 'first')\n#Shape of the data: 899164 rows and 27 columns\ncredit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of columns:\nprint(credit.dtypes.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Summary of data frame (count, mean, standart deviation, min, quartiles, max)\ncredit.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NA observation. A lot of NAs :(\ncredit.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Our target is to predict \"MIS_Value\". According to the document, \"MIS_Status\" has 2 variables: \n#Loan status charged off = CHGOFF, Paid in full = PIF\nCounter(credit.MIS_Status).keys() # We have: \"PIF\", CHGOFF, nan in this column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(credit.MIS_Status).values() # \"PIF\": 739609, CHGOFF: 157558, nan:1997\n#Quite imbalanced. We will try to not lose our rows with CHGOFF values while we are cleaning the dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"                                                    #####FIRST Dealing with NAs.##### \n                                                          #Name, City, and State\n# 4 columns belong to Borrower information: Name, City, State, and Zip.\n#Name is unique and useless and we can drop an entire column.\ncredit = credit.drop(axis =1, columns = ['Name'])\n#Next are City and State. As you can see, there are no NAs in Zip, so we can easily impute City and State uzing Zip values\n#Creating conditions for a loop\ncond = (credit.City.isnull()|credit.State.isnull())\nmissing_rows = credit[cond].index\n#I will go through loop and impute City and State using zearch.by_zipcode function.\nfor i in missing_rows:\n    zipcode = search.by_zipcode(credit.iloc[i,3]) # 3 corresponds to Zip code\n    credit.iloc[i,1] = zipcode.major_city # 1 -> City\n    credit.iloc[i,2] = zipcode.state # 2 -> State\n\n#Check how NAs were imputed. We still have 4 NAs. I have looked through them. One zipcode = 0 \n#and the other 3 are not in the list of search.by_zipcode function. I think we can delete these rows.\ncredit.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next features with a lot of NAs are Bank and BankState. \n#I would like to check if State of borrower = BankState. \n#If so we can just delete one of them.\nequal = 0\nnon_equal = 0\nfor i in credit.index:\n    if credit.State[i] == credit.BankState[i]:\n        equal = equal + 1\n    else:\n        non_equal = non_equal +1\nprint(equal, non_equal)\n# Sad. The assumption is incorrect, 473949 cases coincide, while 425215 cases do not.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since our goal is to define a default case, I will check if there are default cases in the rows where the \n#Bank Name and Bank State are unknown. \ncond_1 = credit.Bank.isnull()|credit.BankState.isnull()\nmissing_rows_1 = credit[cond_1].index\nyes = 0\nno = 0\nfor i in missing_rows_1:\n    if credit.MIS_Status[i] == 'CHGOFF':\n        yes = yes+1\n    else:\n        no = no +1\nprint(yes,no)\n#Not bad, 72 cases against 1494. I'm still not sure if I need the columns with the State of the Bank and Bank name.\n#We can delete 1494 rows with not default cases, since the data is imbalanced. We'll see.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature - 'ChgOffDate' corresponds to the date when a loan was declared to be in default. \n#I made little investigation about ChgOffDate feature: we have 739609 paid cases and 736465 NAs \n#seems like if credit is paid then there is an NA in this featue. default date = default case.\n#So, we will just drop this feature.\ncredit = credit.drop(axis =1, columns = ['ChgOffDate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, I want to clean my dependent variable - MIS_Status drop NAs and change dtype from object to integer.\n#1 - grant a loan (low risk of default), 0 - do not grant a loan (high risk of default)\n#And, of course, drop 4 rows with NAs in City and State columns \ncredit = credit.dropna(axis =0, subset=['City','State','MIS_Status'])\n\nloan_status = {'P I F': 1,'CHGOFF': 0} \ncredit.MIS_Status = [loan_status[item] for item in credit.MIS_Status] \n\nCounter(credit.MIS_Status).keys() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(credit.MIS_Status).values() # count values 739607 = \"1\" against 157556 = \"0\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#According to the document we have, 5 columns are with currency values. I want to change all of them to the float.\n#This will help us see the corealation of currency to our target value.\ncurrency = [19,20,22,23,24] #To convert to float.\nfor i in currency:\n    credit[credit.columns[i]] = credit[credit.columns[i]].replace('[\\$,]', '', regex=True).astype(float) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looks good.\ncredit.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Next feature is New Exist. According to the document\n#1 = Existing business, 2 = New business \n#And at this moment we have 134 NAs\nCounter(credit.NewExist).keys() # unique values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"Counter(credit.NewExist).values() # count values \n#In fact, we have more than 136 NAs. \n#252559 = New business\n#643443 = Existing business\n#1027 = \"0\" whatever it means\n#134 = nan as we can see from previous output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have a feature 'RetainedJob' - From the document, it shows number of jobs retained. \n#I can assume that if loan retains some jobs it is an existing business.\n# I will create a condition and iterate through loop to \n#assign new value \"1\" which is Existing business to those rows where Retained Job is >= 1\ncond_2 = credit[(credit['NewExist'] == 0) & (credit['RetainedJob'] >=1)].index\nfor i in cond_2:\n    credit.loc[i,['NewExist']] = 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next we will do the same thing with another condition: isnull and Retained Job >=1\ncond_3 = credit[(credit.NewExist.isnull()) & (credit['RetainedJob'] >=1)].index\nfor i in cond_3:\n    credit.loc[i,['NewExist']] = 1    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will check if there are default cases in these rows.\ncredit[(credit['NewExist'] == 0) & (credit['MIS_Status'] == 0)] #60 rows\ncredit[(credit.NewExist.isnull()) & (credit['MIS_Status'] == 0)] #1 row\n#Our goal is to impute 61 rows in NewExist and we can delete others.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Ok, we continue our investigation.\n#Interesting column - \"FranchiseCode\":\n#Nofranchise = 0 or 1\n#Franchise code = other numbers\n#My assumption is if Franchise code != 0 and != 1, maybe it is a New Business, not the existing one.\n#Someone gets a franchise and opens a Starbucks in the city.\nCounter(credit.FranchiseCode).keys() #51732 Franchise Loans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"Counter(credit.FranchiseCode).values() # 845431 non Franchise Loans ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check our assumption, if Franchise Code is with digits = New business\ncond_4 = credit[(credit['FranchiseCode'] != 0) & (credit['FranchiseCode'] != 1)] #Lets store our Franchise cases\nCounter(cond_4.NewExist).values()\n# We have:\n# 27940 rows - New Business\n# 23725 rows - Exisitng Business\n# 67 rows - NA in Existing Business\n# Our assumption is not correct!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counter(credit.NewExist).keys() # unique values\n#Counter(credit.NewExist).values() # count values \n\n\n# I think I will stop my investigation on this stage and will drop NAs in NewExist feature.\n#Before it was 134 NAs + 1027 with zero value, after imputation we have 19 rows = NA and 874 = \"0\". In total, 893.\n#To check this you can run code above this cell\n\n#First, I will assign NA to 0 values and drop all NAs in this feature.\ncond_5 = credit[(credit['NewExist'] == 0)].index\nfor i in cond_5:\n    credit.loc[i,['NewExist']] = np.nan #11 corresponds to NewExist column\n    \ncredit = credit.dropna(axis =0, subset=['NewExist'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Much better.\ncredit.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I decided to drop Bank Name and Bank State on this stage. My idea was to use state as a predictor, because \n#different states have different economic environments.\n#According to documentation, State of Borrower is a right Feature to use for this goal.\n#Also, Bank Name (> 5000 names) as a Borrowers Name is a unique value, so we can delete it, too.\n#Columns \"Disbursment Date\", \"DisbursementGross\", \"BalanceGross\" and \"ChgOffPrinGr\" \n#contain information that is important after default is declared, so we can't use these columns for predicting\n#default risks. I delete them.\ncredit = credit.drop(axis =1, columns = ['Bank', 'BankState', 'DisbursementDate', 'DisbursementGross', \n                                         'BalanceGross','ChgOffPrinGr'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we have only 2 features left: LowDoc and RevLineCr,\n#We will start from - LowDoc. Loan Program: Y = Yes, N = No\nCounter(credit.LowDoc).keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(credit.LowDoc).values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I made some research about \"LowDoc program\". So, these are less than 150 000$ short-term loans. \n#To get this loan you need less documents\n#And it was a very popular loan program in the USA in 2000-2007. And seems like it's a strong predictor\n#I will select rows with this condition and check values in LowDoc feauture\n#But first I should convert ApprovalDate from object to DateTime format\nfrom datetime import date\ncredit['ApprovalDate'] = credit['ApprovalDate'].astype(str)\ncredit['ApprovalDate'] = pd.to_datetime(credit['ApprovalDate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cond_6 = credit[(credit['LowDoc'] != \"Y\") & (credit['LowDoc'] != \"N\")]\ncond_6\n#5997 rows with NAs and other different values. (1404)\n#Seems like it's a very important value, we will not drop rows with NAs. \n#We will try to impute LowDoc value using other features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(cond_6.LowDoc).values() #dict_values([757, 1, 2578, 603, 74, 494, 1490])\nCounter(cond_6.LowDoc).keys() #dict_keys(['C', '1', nan, 'S', 'R', 'A', '0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see how many rows in cond_6 are default cases\nCounter(cond_6.MIS_Status).values()\n#4420 rows - not default\n#1577 - default cases. \n#Would be nice to impute these rows with values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's examine LowDoc loans and Not LowDoc loans and try to find any patterns\nlow_doc = credit[credit['LowDoc'] == \"Y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_doc['GrAppv'].describe() # 75% of loans are =< $100 000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_doc['Term'].describe() #75% or loans <= 93 months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(low_doc.MIS_Status).keys()\nCounter(low_doc.MIS_Status).values() #MIS_Status 1 = 100153, 0 = 9893 (1 is 10 times more than 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check if we have some rows in cond_6 with these conditions\ncond_7 = cond_6[(cond_6['GrAppv'] <= 100000) & (cond_6['Term'] <= 93) & (cond_6['MIS_Status'] == 1)]\ncond_7\n#Ok, we can assign 1 to 1565 rows in LowDoc program. It's better to then just delete the rows.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will iterate a loop to assign value 'Y' to these rows in column 15 (LowDoc)\nfor i in cond_7.index:\n    credit.loc[i,['LowDoc']] = 'Y'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Examine results 4432 rows. I will drop other values\ncond_8 = credit[(credit['LowDoc'] != \"Y\") & (credit['LowDoc'] != \"N\")]\ncond_8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will assign nan to all values not equal to \"Y\" and \"N\" and than drop NAs\nfor i in cond_8.index:\n    credit.loc[i,'LowDoc'] = np.nan\n\ncredit = credit.dropna(axis =0, subset=['LowDoc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next is RevLineCr. According to the documentation, revolving line of credit: Y = Yes, N = No\nCounter(credit.RevLineCr).keys()\nCounter(credit.RevLineCr).values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Again we will select rows not equal to Y and N\ncond_9 = credit[(credit['RevLineCr'] != \"Y\") & (credit['RevLineCr'] != \"N\")] #277188 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(cond_9.MIS_Status).values() # 1:231537 0:44978 rows.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will select all rows with RevLine Yes and RevLine No and try to find any patterns to impute NAs\nRevLine_yes = credit[credit['RevLineCr'] == \"Y\"]\nRevLine_no = credit[credit['RevLineCr'] == \"N\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RevLine_yes.Term.describe() #highest term 312","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RevLine_no.Term.describe() #highest term 527","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of emp\nRevLine_no.NoEmp.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RevLine_yes.NoEmp.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I can't find any clear pattern and there are a lot of NAs in this row, so I decided to delete this column on this stage.\n#Also, I found out that column #0 is unique, too. It's an ID number of Identifier – Primary key. We will drop it, too.\n#We don't need columns with dates anymore: ApprovalDate, ApprovalFY\n#SBA_Appv - a guaranteed ammount from the US government. Useless, too. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit = credit.drop(axis =1, columns = ['Zip','LoanNr_ChkDgt','ApprovalDate', 'ApprovalFY','RevLineCr','SBA_Appv'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Counter(credit.City).keys() # 2 = 125348 1 = 292878 \n#We can try to create a feature according to cities (Small, Big, Medium), but maybe later for improving the model. \n#On this stage I think we can drop this column, too many unique values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(credit.State).keys() \n#My idea is to create new features using States and NAICS. For each State and for each Sector of business I will give points.\n#Lower default rate - higher points.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function extracts first 2 digits from variable, if variables = 0, returns 0\ndef first_two(d):\n    if d <= 0:\n        return 0\n    return (d // 10 ** (int(math.log(d, 10)) - 1))\n#Function returns points according to the given rate. Lower rate, higher points.\ndef point_def(rate):\n    if rate <= 12:\n        return 5\n    elif  12 < rate <= 17: \n        return 4\n    elif 17 < rate <= 21:\n        return 3\n    elif 21 < rate <= 25:\n        return 2\n    elif rate > 25:\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function returns points according to the default rate of each sector.\ndef apply_score(i):\n    sector_default = {21 : 8, 11 : 9, 55 : 10, 62:10, 22:14, 92:15,54:19, 42:19,31:19,32:16,33:14,81:20,71:21,72:22,44:22,45:23,23:23,56:24,61:24,51:25,48:27,49:23,52:28,53:29}\n    if i > 0:\n        defrate = None\n        if i in sector_default:\n            defrate = sector_default[i]\n            return point_def(defrate)\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Once the functions are ready I will create a new column - \"Sector_Points\" and apply functions to get points\ncredit['Sector_Points'] = credit.NAICS.apply(first_two).apply(apply_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, we will give points to each State.\n#I indicated only the highest and the lowest default rates in the library and assigned def rate = 18 (mean) to the states left\n#Function for the state scores \ndef apply_score_state(i):\n    state_default = {'MT':8, 'ND': 8, 'WY':8, 'SD':8, 'VT':8, 'ME':10,'NH':10, 'NM':10, 'AK':10, 'WA':13,'AD':13, 'MN':13, \n                     'WI':13, 'IA':13,'NE':13, 'KS':13, 'MA':13,'CT':13,'RI':13,'PA':13, 'NV':23, 'IL':23, 'MI':23, 'KY':23,\n                     'GA':23, 'FL':28}\n    temp_defrate = None\n    average_def_rate = 18\n    if i in state_default:\n        temp_defrate = state_default[i]\n        return point_def(temp_defrate)\n    return point_def(average_def_rate)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function and create new feature\ncredit['State_Points'] = credit.State.apply(apply_score_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I think on this stage we can drop columns:\"City\",\"State\" and \"NAICS\"\n#Because when I dummify these columns, especially State, 50 more features will be created.\ncredit = credit.drop(axis =1, columns = ['City', 'State', 'NAICS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, I want to change some Columns this way:\n#Term to Years 12 = 1 etc\n#CreateJob to IscreateJob (1,0)\n#Retained Job to IsRetainedJob (1,0)\n#FranchiseCode to IsFranchise (1,0)\n\n#For this I will create a simple function, which I can apply to several columns and create new features.\ndef yes_no(i):\n    if i > 0:\n        return 1\n    return 0\n\ncredit.Term = credit.Term//12\ncredit['IscreateJob'] = credit.CreateJob.apply(yes_no)\ncredit['IsRetained'] = credit.RetainedJob.apply(yes_no)\ncredit['IsFranchise'] = credit.FranchiseCode.apply(yes_no)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will recode LowDoc Yes and No to 1 and 0.\ndef lowdoc(i):\n    if i == \"Y\":\n        return 1\n    return 0\ncredit.LowDoc = credit.LowDoc.apply(lowdoc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This column is totally OK :)\nCounter(credit.UrbanRural).keys() #1 = Urban, 2 =Rural, 0 = Undefined\nCounter(credit.UrbanRural).values() #1 = Urban, 2 =Rural, 0 = Undefined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we don't need FrancshiseCode column I will drop it\ncredit = credit.drop(axis =1, columns = ['FranchiseCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check our dataframe\ncredit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As we can see, there is skewness.\nplt.figure(figsize=(15, 8))\nsns.distplot(credit.GrAppv, color=\"g\", kde=False)\nplt.ylabel('Density')\nplt.title('Distribution of Approved ammount')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fix skewness of GrAppv using log\ncredit['GrAppv'] = np.log(credit['GrAppv']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"                                                    #### SIMPLE MODEL ####\n#Split data into train and test sets + label target value\nfrom sklearn.model_selection import train_test_split\ny = credit.MIS_Status\nX = credit.drop(['MIS_Status'], axis=1)\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will apply Simple Imputer and Standart Scaler from sklearn package\nfrom sklearn.impute import SimpleImputer \nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n\n#Scaling features with Standart Scaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_sc =scaler.fit_transform(train_X)\ntest_sc = scaler.transform(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will train xgboost without any tunning and check results.\nimport xgboost as xgb\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n#Train the XGboost Model for Classification\n#Model with default parameters\nmodel = xgb.XGBClassifier()\ntrain_model = model.fit(train_sc, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction\nfrom sklearn.metrics import classification_report\npred = train_model.predict(test_X)\nprint('Model XGboost Report %r' % (classification_report(test_y, pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's use accuracy score\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy for model: %.2f\" % (accuracy_score(test_y, pred) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}