{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"real = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\n\nfake.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, I'm going to make some wordclouds to take a look at the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nfake_words = \"\"\nstopwords = set(STOPWORDS)\nstopwords.add(\"wa\")\nstopwords.add(\"thi\")\nfor text in fake.text.values:\n    text = str(text)\n    words = text.split()\n    fake_words += \" \".join([(i.lower() + \" \") for i in words])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fake news word cloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_cloud = WordCloud(width = 500, height = 500, background_color = 'white', stopwords = stopwords, min_font_size = 10)\nfake_cloud.generate(fake_words)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(fake_cloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title(\"Fake news\")\n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Real news word cloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real_words = \"\"\nfor text in real.text.values:\n    text = str(text)\n    words = text.split()\n    real_words += \" \".join([(i.lower() + \" \") for i in words])\nreal_cloud = WordCloud(width = 500, height = 500, background_color = 'white', stopwords = stopwords, min_font_size = 10)\nreal_cloud.generate(real_words)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(real_cloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title(\"Real news\")\n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge real & fake data, clean it, and split it into train/test/validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real[\"fake?\"] = np.zeros(len(real))\nfake[\"fake?\"] = np.ones(len(fake))\nfake.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\nnews = real.append(fake)\nnews = shuffle(news)\nnews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ndef clean_text(text):\n    words = str(text).split()\n    words = [i.lower() + \" \" for i in words]\n    words = \" \".join(words)\n    words = words.translate(words.maketrans('', '', string.punctuation))\n    return words\n\nnews['text'] = news['text'].apply(clean_text)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(news)\ntrain, validation = train_test_split(train, test_size = 0.2)\nprint(len(train), len(validation), len(test) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenize & pad text to create input data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 10000\ntrunc_type = \"post\"\npad_type = \"post\"\noov_tok = \"<OOV>\"\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train.text)\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sequences = tokenizer.texts_to_sequences(np.array(train.text))\ntraining_padded = pad_sequences(training_sequences,truncating=trunc_type, padding=pad_type)\n\nmax_length = len(training_padded[0])\n\nvalidation_sequences = tokenizer.texts_to_sequences(np.array(validation.text))\nvalidation_padded = pad_sequences(validation_sequences, padding=pad_type, truncating=trunc_type, maxlen = max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = np.copy(training_padded)\nvalidate_x = np.copy(validation_padded)\ntrain_y = train['fake?'].values\nvalidate_y = validation['fake?'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_x), len(train_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use KerasTuner for Hyperparameter tuning for our Sequential Tensorflow model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U keras-tuner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kerastuner.tuners import RandomSearch\n\ndef build_model(hp):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, hp.Int('units', min_value = 5, max_value = 200, step = 25), input_length=max_length),\n        tf.keras.layers.Conv1D(16, 5, activation='relu'),\n        tf.keras.layers.GlobalMaxPooling1D(),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy',\n                      optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate',\n                      values=[1e-2, 1e-3, 1e-4])), \n                      metrics=['accuracy'])\n    return model\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3)\n\n# history = model.fit(train_x, train_y, epochs = 30, validation_data = (validate_x, validate_y),\n#                    callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.search(train_x, train_y, epochs = 3,verbose = 2,validation_data = (validate_x, validate_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=2)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.results_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = tf.keras.Sequential([\n#     tf.keras.layers.Embedding(vocab_size, 16, input_length=max_length),\n#     tf.keras.layers.Conv1D(16, 5, activation='relu'),\n#     tf.keras.layers.GlobalMaxPooling1D(),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n# model.compile(loss='binary_crossentropy',\n#                   optimizer=tf.keras.optimizers.Adam(), \n#                   metrics=['accuracy'])\n# history = model.fit(train_x, train_y, verbose = 2, epochs = 3, validation_data = (validate_x, validate_y),\n#                    callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])\nmodel = tuner.get_best_models()[0]\nhistory = model.fit(train_x, train_y, verbose = 2, epochs = 3, validation_data = (validate_x, validate_y),\n                   callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classify test set and calculate accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(np.array(test.text))\ntest_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen = max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.round(model.predict(test_padded))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = np.sum(1 if i==j else 0 for i,j in zip(preds, test[\"fake?\"].values)) / len(test)\nprint(\"Accuracy: \", acc )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}