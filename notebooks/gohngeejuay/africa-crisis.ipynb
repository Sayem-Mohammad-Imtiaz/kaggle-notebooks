{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Goal for this kernel: Handle preprocessing steps,learning Naive Bayes classifier and its variants, nearest neighbour, support vector machines and random forest. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"crises = pd.read_csv(\"../input/africa-economic-banking-and-systemic-crisis-data/african_crises.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crises.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns: \n**case** A number which denotes a specific country\n\n**cc3** A three letter country code\n\n**country** The name of the country\n\n**year** The year of the observation\n\n**systemic_crisis** \"0\" means that no systemic crisis occurred in the year and \"1\" means that a systemic crisis occurred in the year.\n\n**exch_usd** The exchange rate of the country vis-a-vis the USD\n\n**domestic_debt_in_default** \"0\" means that no sovereign domestic debt default occurred in the year and \"1\" means that a sovereign domestic debt default occurred in the year\n\n**sovereign_external_debt_default** \"0\" means that no sovereign external debt default occurred in the year and \"1\" means that a sovereign external debt default occurred in the year\n\n**gdp_weighted_default** The total debt in default vis-a-vis the GDP\n\n**inflation_annual_cpi** The annual CPI Inflation rate\n\n**independence** \"0\" means \"no independence\" and \"1\" means \"independence\"\n\n**currency_crises** \"0\" means that no currency crisis occurred in the year and \"1\" means that a currency crisis occurred in the year\n\n**inflation_crises** \"0\" means that no inflation crisis occurred in the year and \"1\" means that an inflation crisis occurred in the year\n\n**banking_crisis** \"no_crisis\" means that no banking crisis occurred in the year and \"crisis\" means that a banking crisis occurred in the year"},{"metadata":{"trusted":true},"cell_type":"code","source":"crises.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crises.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kumar,D.(2018,Dec 25).Introduction to Data Preprocessing in Machine Learning. Retrieved from: https://towardsdatascience.com/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d\n\nData processing Steps:\n* Handling Null Values\n* Standardization\n* Handling Categorical Variables\n* One-Hot Encoding\n* Multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for null values\ncrises[crises.isna().any(axis = 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for null values\ncrises.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking correlation between factors to systemic_crisis\ncrises.drop(['case'],axis = 1).corr()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crises.iloc[:,[3,5,6,7,8,9,10,11,12]].apply(lambda x : x.corr(crises['systemic_crisis']))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 3 highest correlation with systemic_crisis are sovereign_external_debt_default(0.249850),exch_usd(0.202687),year(0.197450).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separating the features and target\nx = crises.drop(['case','cc3','country','systemic_crisis'],axis = 1)    #drop irrelevant columns\ny = crises['systemic_crisis']#crises.loc[crises['systemic_crisis']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explanatory variables: All variables excluding case,cc3,country,systemic_crisis columns\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dependent variable: systemic_crisis\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert banking_crisis column into encoding using one hot encoding\n#Two different ways of creating dummy variables\n#x = pd.get_dummies(x)  #Create two column for banking_crisis(one for crisis, another for no_crisis)\nx = pd.get_dummies(x,drop_first = True) #Only one column for banking_crisis(0 = crisis, 1 = no_crisis)\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)\n\ndrop_first = Whether to get k-1 dummies out of k categorical levels by removing the first level. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_transformed = scaler.fit_transform(x)    #result as a numpy array\nx = pd.DataFrame(x_transformed, index = x.index, columns = x.columns )    #keep as a dataframe\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference:https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n\nStandardScaler : It transforms the data in such a manner that it has mean as 0 and standard deviation as 1. In short, it standardizes the data. Standardization is useful for data which has negative values. It arranges the data in normal distribution. It is more useful in classification than regression."},{"metadata":{},"cell_type":"markdown","source":"Reference:https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn\n\n1.Fit(): Method calculates the parameters μ and σ and saves them as internal objects.\n\n2.Transform(): Method using these calculated parameters apply the transformation to a particular dataset.\n\n3.Fit_transform(): joins the fit() and transform() method for transformation of dataset."},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x,y,test_size = 0.25,random_state = 0)\nlogReg = LogisticRegression()\nlogReg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred = logReg.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy = \" + str(metrics.accuracy_score(y_test,y_pred)))    #sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logReg.coef_)\nprint(x.columns)\nfor i in range(len(logReg.coef_[0])):    \n    print(\"Coeeficient of \" + x.columns[i] + \" : \" + str(logReg.coef_[0][i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n#Convert log-odds into odds\nodds = []\nfor i in range(len(logReg.coef_[0])):\n    odds.append(math.exp(logReg.coef_[0][i]))\n#print(odds)\n\n#Convert odds into probability\nprobs = []\nfor i in range(len(odds)):\n    probs.append(odds[i]/(1+odds[i]))\n#print(probs)\nfor i in range(len(probs)):    \n    print(\"Coeeficient of \" + x.columns[i] + \" : \" + str(probs[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other methods: naive bayes, nearest neighbour, svm, decision trees, boosted trees, random forest, neural network"},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes**\nhttps://scikit-learn.org/stable/modules/naive_bayes.html\n\n*GaussianNB* \n\n*MultinomialNB* \n\n*ComplementNB*\n\n*BernoulliNB*\n\n*CategoricalNB*\n\nReferences: \n\nImplementing 3 Naive Bayes classifiers in scikit-learn. (2018, May 7). Retrieved from: https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/\n\nDas,A.(2018,Sep 25). A comprehensive Naive Bayes Tutorial using scikit-learn. Retrieved from:https://medium.com/@awantikdas/a-comprehensive-naive-bayes-tutorial-using-scikit-learn-f6b71ae84431\n\nNavlani,A. (2018,Dec 4). Naive Bayes Classification using Scikit-learn.Retrieved from:https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn\nNaive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example, a loan applicant is desirable or not depending on his/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive. This assumption is called class conditional independence.\n\n<img src = \"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836882/image_3_ijznzs.png\">\n* P(h): the probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h. (Marginal prob)\n* P(D): the probability of the data (regardless of the hypothesis). This is known as the prior probability. (Marginal prob of other cond)\n* P(h|D): the probability of hypothesis h given the data D. This is known as posterior probability. (Get using the formula above)\n* P(D|h): the probability of data d given that the hypothesis h was true. This is known as posterior probability. (Conditional prob)\n\nMultiple features\n<img src = \"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836884/image_5_uhsgzr.png\">\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GaussianNB\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\ny_pred = gnb.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy of GaussianNB = \" + str(metrics.accuracy_score(y_test,y_pred)))    #sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BernoulliNB\nfrom sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB()\nbnb.fit(X_train,y_train)\ny_pred = bnb.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy BernoulliNB = \" + str(metrics.accuracy_score(y_test,y_pred)))    #sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MultinomialNB\n# from sklearn.naive_bayes import MultinomialNB\n# mnb = MultinomialNB()\n# mnb.fit(X_train,y_train)\n# y_pred = mnb.predict(X_test)\n# cm = confusion_matrix(y_test,y_pred)\n# print(cm)\n# print(\"Accuracy MultinomialNB = \" + str(metrics.accuracy_score(y_test,y_pred)))    #sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n\n#ValueError: Input X must be non-negative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ComplementNB\n# from sklearn.naive_bayes import ComplementNB\n# cnb = ComplementNB()\n# cnb.fit(X_train,y_train)\n# y_pred = cnb.predict(X_test)\n# cm = confusion_matrix(y_test,y_pred)\n# print(cm)\n# print(\"Accuracy ComplementNB = \" + str(metrics.accuracy_score(y_test,y_pred)))    #sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n\n#ValueError: Input X must be non-negative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CategoricalNB\n# from sklearn.naive_bayes import CategoricalNB\n# cnb = CategoricalNB()\n# cnb.fit(X_train,y_train)\n# y_pred = cnb.predict(X_test)\n# cm = confusion_matrix(y_test,y_pred)\n# print(cm)\n# print(\"Accuracy CategoricalNB = \" + str(metrics.accuracy_score(y_test,y_pred)))    #sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n\n#ImportError: cannot import name 'CategoricalNB'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nearest neighbour"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy knn 5 neighbor : \" + str(metrics.accuracy_score(y_test,y_pred)))    #sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_result = []\nfor i in range(5,30,5):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    accuracy_result.append(metrics.accuracy_score(y_test,y_pred))\nprint(accuracy_result)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot([5,10,15,20,25],accuracy_result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support vector machine"},{"metadata":{},"cell_type":"markdown","source":"Reference: https://scikit-learn.org/stable/modules/svm.html\nThe advantages of support vector machines are:\n* Effective in high dimensional spaces.\n* Still effective in cases where number of dimensions is greater than the number of samples.\n* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels. \n\nThe disadvantages of support vector machines include:\n* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n\nSupport Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data\n\nSetting C: C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation.\n\nLinearSVC and LinearSVR are less sensitive to C when it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, larger C values will take more time to train, sometimes up to 10 times longer, as shown by Fan et al. (2008)"},{"metadata":{},"cell_type":"markdown","source":"Reference: \n\nNavlani,A.(2019,Dec 28).Support Vector Machines with Scikit-learn.Retrieved from:https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n\n\nGenerally, Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n\n<img src = \"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index3_souoaz.png\">\n**Support Vectors**\nSupport vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n\n**Hyperplane**\nA hyperplane is a decision plane which separates between a set of objects having different class memberships.\n\n**Margin**\nA margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.\n\nThe main objective is to segregate the given dataset in the best possible way. The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane.\n\nSVM uses a kernel trick to transform the input space to a higher dimensional if some problems cannot be solved in a linear hyperplane. Now can separate the points using linear separation. The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps you to build a more accurate classifier.\n\n**Parameters:****\n* *Kernel:* The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n* *Regularization: *Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane. (The strength of the regularization is inversely proportional to C)\n* *Gamma:* A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting. In other words, you can say a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma considers all the data points in the calculation of the separation line."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n#linear kernel\nlinsvm = svm.SVC(kernel = \"linear\")\nlinsvm.fit(X_train,y_train)\ny_pred = linsvm.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy linear kernel svm : \" + str(metrics.accuracy_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Different kernels: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n#rbf kernel\nrbfsvm = svm.SVC(kernel = \"rbf\")\nrbfsvm.fit(X_train,y_train)\ny_pred = rbfsvm.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy rbf kernel svm : \" + str(metrics.accuracy_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Different kernels: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n#poly kernel\npolysvm = svm.SVC(kernel = \"poly\")\npolysvm.fit(X_train,y_train)\ny_pred = polysvm.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy poly kernel svm : \" + str(metrics.accuracy_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sigmoid kernel\nsigsvm = svm.SVC(kernel = \"sigmoid\")\nsigsvm.fit(X_train,y_train)\ny_pred = sigsvm.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy sigmoid kernel svm : \" + str(metrics.accuracy_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #precomputed kernel\n# presvm = svm.SVC(kernel = \"precomputed\")\n# presvm.fit(X_train,y_train)\n# y_pred = presvm.predict(X_test)\n# cm = confusion_matrix(y_test,y_pred)\n# print(cm)\n# print(\"Accuracy precomputed kernel svm : \" + str(metrics.accuracy_score(y_test,y_pred)))\n\n#ValueError: X.shape[0] should be equal to X.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regularization parameter: C.Must be positive float.\ncValues = [0.01,0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.5,3.0,4.0,5.0,6.0]\nresults = []\nfor i in range(len(cValues)):\n    #poly kernel\n    polysvm = svm.SVC(kernel = \"poly\", C = cValues[i])\n    polysvm.fit(X_train,y_train)\n    y_pred = polysvm.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    print(\"Accuracy poly kernel svm with C value = \" + str(cValues[i]) + \" : \"+ str(metrics.accuracy_score(y_test,y_pred)))\n    results.append(metrics.accuracy_score(y_test,y_pred))\nplt.plot(cValues,results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest\n\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\nNavlani,A.(2018,May 16). Understanding Random Forests Classifiers in Python. Retrieved from:https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n\nA forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy Random Forest : \" + str(metrics.accuracy_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(clf.feature_importances_)):    \n    print(\"Coeeficient of \" + x.columns[i] + \" : \" + str(clf.feature_importances_[i]))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}