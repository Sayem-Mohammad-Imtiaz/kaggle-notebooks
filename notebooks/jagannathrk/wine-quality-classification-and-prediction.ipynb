{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting stated : load nessesory libraries.\nimport pandas as pd # For working with data\nimport numpy as np # For stats and faster array calculations\nimport seaborn as sns # For visualization\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\") # Loading the csv file into dataframe\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df1.copy() # Making a copy of data always helps.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All the featues are numericals.\n### 1.This problem can be persued as Regression problem as well as classification problem. 2.All the numerical features helps as we don't need to spend much time in data preprocessing,converting categorical data to numerical"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum() # Do we have any nyll values or not","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe() # Getting an overview of data, how the feature are spreaded.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the 50%,75% and maximum value, It seems that some of the features need outlier treatment.[residual sugar,chlorides,free sulfur dioxide,total sulfur dioxide]\n#### Also we don't have any null values. This will save time from missing-value treatment."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['residual sugar'].plot(kind='box')# take a look at destribution","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['residual sugar'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### most of the data lies below 5. Values greater than 5 can be treated as outliers. "},{"metadata":{"trusted":false},"cell_type":"code","source":"temp = df[df['residual sugar']<5]['residual sugar']\ntemp = temp.mean() #we can take mode() but the distribution is a skewed normal distribution so, won't make much difference.\n\nboolean = df['residual sugar']>5\n\ndf.loc[boolean,'residual sugar'] = df.loc[boolean,'residual sugar'].apply(lambda x : temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['residual sugar']) #after outlier treatment","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We'll apply the above technique to all the features having outliers problem."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['chlorides'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['chlorides'].plot(kind='box')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"temp = df[df['chlorides']<0.2]['chlorides']\ntemp = temp.mean()\n\nboolean = df['chlorides']>0.2\n\ndf.loc[boolean,'chlorides'] = df.loc[boolean,'chlorides'].apply(lambda x : temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['chlorides'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['free sulfur dioxide'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['free sulfur dioxide'].plot(kind='box')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"temp = df[df['free sulfur dioxide']<40]['free sulfur dioxide']\ntemp = temp.mean()\n\nboolean = df['free sulfur dioxide']>40\n\ndf.loc[boolean,'free sulfur dioxide'] = df.loc[boolean,'free sulfur dioxide'].apply(lambda x : temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['free sulfur dioxide'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['sulphates'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"temp = df[df['sulphates']<1.25]['sulphates']\ntemp = temp.mean()\n\nboolean = df['sulphates']>1.25\n\ndf.loc[boolean,'sulphates'] = df.loc[boolean,'sulphates'].apply(lambda x : temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['sulphates'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['total sulfur dioxide'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"temp = df[df['total sulfur dioxide']<150]['total sulfur dioxide']\ntemp = temp.mean()\n\nboolean = df['total sulfur dioxide']>150\n\ndf.loc[boolean,'total sulfur dioxide'] = df.loc[boolean,'total sulfur dioxide'].apply(lambda x : temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['total sulfur dioxide'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our data and target values were in the same dataframe, now that the data preprocessing is done we can move forward to split our data into Data(X) and target(Y).\n\n#### Also, looking at the data, we need to do some featurescaling as well, as the feature have different range of values"},{"metadata":{"trusted":false},"cell_type":"code","source":"x = df.drop('quality',axis=1)\ny = df.quality\ny = np.array(y)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Loading ML regression models.\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\nfrom sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\n\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_validate # cross validation is used to measure how godd a ML model performes on data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cross_validate(lr,x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cross_validate(dtr,x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cross_validate(rfr,x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### No ML model has performed as per our expectation.\n#### Let's see if we can achive better results with RandomForestRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 0,stratify=y)\n\nscore_list =[]\nfor i in range(2,100,5):\n    rfr = RandomForestRegressor(n_estimators = i)\n    rfr.fit(x_train,y_train)\n    pred = rfr.predict(x_test)\n    score_list.append(mean_absolute_error(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nplt.plot(list(range(2,100,5)),score_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### After increasing n_estimators(trees in random forest) in RFR model, we get mean_absolute_error around 0.38-0.40 Which means accuracy around 60-62%. This can be considered as a good regression model for this data."},{"metadata":{"trusted":false},"cell_type":"code","source":"rfr = RandomForestRegressor(n_estimators=35)\nrfr.fit(x_train,y_train)\npred = rfr.predict(x_test)\nmean_absolute_error(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now We'll Move to the Classificaion problem.\n### For this, we can describe wine qualities as good, bad or normal."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['quality']) # To get an idea about the distibution of wine quality data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Wine having quality score <4.5 can be considered as poor and >6.5 as good quality wine."},{"metadata":{"trusted":false},"cell_type":"code","source":"### Generating Categorical Column.\n\ndef applier(x):\n    if x<4.5:\n        return 'bad'\n    elif 4.5<x<6.5:\n        return 'normal'\n    else :\n        return 'good'\n\ndf['type'] = df['quality'].apply(lambda x : applier(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### We have to again specify our data(x) and target(x) for models. \n\nx = df.drop(columns=['quality','type'])\ny = df[['type']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### We need to do go through the feature scaling process again, as we have new unprocessed data.\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nscaler = MinMaxScaler()\nscaler1 = StandardScaler()\nx = scaler1.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### importing ML classification models.\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### We'll implement the cross validation process from skretch.\n\nlr_pred = []\ndtc_pred = []\nrfc_pred = []\ngnb_pred = []\n\nfrom sklearn.model_selection import KFold\nkf = KFold(5)\n\nkf.get_n_splits(x)\nfor train_index, test_index in kf.split(x):\n    X_train, X_test = x[train_index], x[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n\n    lr.fit(X_train,y_train)\n    dtc.fit(X_train,y_train)\n    rfc.fit(X_train,y_train)\n    gnb.fit(X_train,y_train)\n    \n    lr_pred.append(lr.score(X_test,y_test))\n    dtc_pred.append(dtc.score(X_test,y_test))\n    rfc_pred.append(rfc.score(X_test,y_test))\n    gnb_pred.append(gnb.score(X_test,y_test))\nprint(lr_pred,dtc_pred,rfc_pred,gnb_pred,sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As we can see, LogisticRegression and RandomForest performs better than DecisionTree or NaiveBayes\n### This is because, DecisionTree useually overfits the training data and NaiveBayes expects features to be naive(unrelated to each other) bacause it works on bayes theorem which is only applied on a set of independent events."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}