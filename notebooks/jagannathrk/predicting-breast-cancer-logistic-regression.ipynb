{"cells":[{"metadata":{"_cell_guid":"a73d4a50-ce59-42b1-945d-f99658b4c155","_uuid":"4cd2e240de9fb92725cd6931ac174d5c0ea197c6"},"cell_type":"markdown","source":"# Predicting Breast Cancer - Logistic Regression\n\n###### Mike M. Lee\n###### 10th Oct 2017\n\n# 0. Introduction\n---\n\nHello Kagglers! \n\nWelcome to my first kernel on Kaggle. In this notebook, I explore the Breast Cancer dataset and develop a `Logistic Regression` model to try classifying suspected cells to Benign or Malignant. This notebook was inspired by [Mehgan Risdal's kernel](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) on the Titanic data, and [Pedro Marcelino's kernel](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) on the Housing Prices data.\n\nThe contents of this notebook will follow the outline below:\n1. **The Data** - *Exploratory Data Analysis*\n2. **The Variables** - *Feature Selection*\n3. **The Model** - *Building a Logistic Regression Model*\n4. **The Prediction** - *Making Predictions with the Model*\n\nThroughout the notebook, I will try to aid your understanding with some visualizations where necessary. I hope you enjoy reading through this notebook,  and please leave comments below if you have any questions or feedbacks. I am a total beginner to the field of Data Science, so any feedback is welcome since it helps me realize my mistakes and also allows me to pick up new insights. \n\nLet's dive right into the data now!\n\n\n\n\n# 1. The Data\n---\n*Extracted from the [UCI ML repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)*\n\n### Attribute Information:\n\n* **id** \n* **diagnosis**: M = malignant, B = benign\n\n*Columns 3 to 32* \n\nTen real-valued features are computed for each cell nucleus: \n\n* **radius**: distances from center to points on the perimeter \n* **texture**: standard deviation of gray-scale values\n* **perimeter** \n* **area** \n* **smoothness**: local variation in radius lengths \n* **compactness**: perimeter^2 / area - 1.0 \n* **concavity**: severity of concave portions of the contour\n* **concave points**: number of concave portions of the contour\n* **symmetry** \n* **fractal dimension**: \"coastline approximation\" - 1\n\nThe mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.  For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n---"},{"metadata":{"_uuid":"578c081220dce5fb90e3622f1ff3adefcef70971","_cell_guid":"695ad59e-69fa-4a0d-a59f-204856d2c71e","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# import dependencies\n# data cleaning and manipulation \nimport pandas as pd\nimport numpy as np\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning\nfrom sklearn.preprocessing import StandardScaler\n\nimport sklearn.linear_model as skl_lm\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score\nfrom sklearn.model_selection import train_test_split\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n# initialize some package settings\nsns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15f3de69cbef86cf97c2bf2c97ebe9faadf007ee","_cell_guid":"bd98c087-a305-4d9b-b581-c6245fdc8f25","_kg_hide-output":false,"scrolled":true,"trusted":true},"cell_type":"code","source":"# read in the data and check the first 5 rows\ndf = pd.read_csv('../input/data.csv', index_col=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"175cd422-5cba-42de-95db-61746caffd4f","_uuid":"08b46e9495442b7ea346ce9862e3b0f4ea60e8be"},"cell_type":"markdown","source":"The last column, **Unnamed:32**, seems like it has a whole bunch of missing values. Let's quickly check for any missing values for other columns as well."},{"metadata":{"_cell_guid":"b9a68601-5b5e-4e77-920d-f98e61b8505c","_uuid":"acea2d29a01413419f5add3309727781e10da197","scrolled":true,"trusted":true},"cell_type":"code","source":"# general summary of the dataframe\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70cdb909-0a62-4114-99b9-d177b46c18fa","_uuid":"283eb644ecf5581cb6c99baae6fd29c5e0444fd0"},"cell_type":"markdown","source":"It looks like our data does not contain any missing values, except for our suspect column **Unnamed: 32**, which is full of missing values. Let's go ahead and remove this column entirely. After that, let's check for the data type of each column."},{"metadata":{"_uuid":"ac927ab4bbe80cc713a78109c725dbd67f81d342","_cell_guid":"4ba0851a-2cec-40fd-a0db-839f21b9fa62","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# remove the 'Unnamed: 32' column\ndf = df.drop('Unnamed: 32', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"80228e6c-863f-4205-b9c5-824f60cfc422","_uuid":"9989c1a33c5b687d1d33ea6d7f4246f752278de7","scrolled":true,"trusted":true},"cell_type":"code","source":"# check the data type of each column\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"724cdad7-4ffe-467b-a90a-2a7b6aa7ba02","_uuid":"3db99e254faa194ec6c51d09671bb8a14b520e95"},"cell_type":"markdown","source":"Our response variable, **diagnosis**, is categorical and has two classes,  'B' (Benign) and 'M' (Malignant). All explanatory variables are numerical, so we can skip data type conversion.\n\nLet's now take a closer look at our response variable, since it is the main focus of our analysis. We begin by checking out the distribution of its classes."},{"metadata":{"_cell_guid":"6a8e4ba0-2e31-4c13-8fa5-8b80aff8bb28","_uuid":"58b11df84464de83ff281f440dd0cfca16535cea","scrolled":false,"trusted":true},"cell_type":"code","source":"# visualize distribution of classes \nplt.figure(figsize=(8, 4))\nsns.countplot(df['diagnosis'], palette='RdBu')\n\n# count number of obvs in each class\nbenign, malignant = df['diagnosis'].value_counts()\nprint('Number of cells labeled Benign: ', benign)\nprint('Number of cells labeled Malignant : ', malignant)\nprint('')\nprint('% of cells labeled Benign', round(benign / len(df) * 100, 2), '%')\nprint('% of cells labeled Malignant', round(malignant / len(df) * 100, 2), '%')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"95817caf-80c5-4256-ae93-1bc49a950b37","_uuid":"2845c759a3614827bcd07417a57a601ebb6b32d8"},"cell_type":"markdown","source":"Out of the 569 observations, 357 (or 62.7%) have been labeled malignant, while the rest 212 (or 37.3%) have been labeled benign. Later when we develop a predictive model and test it on unseen data, we should expect to see a similar proportion of labels.\n\nAlthough our dataset has 30 columns excluding the **id** and the **diagnosis** columns, they are all in fact very closely related since they all contain information on the same 10 key attributes but only differ in terms of their perspectives (i.e., the mean, standard errors, and the mean of the three largest values denoted as \"worst\"). \n\nIn this sense, we could attempt to dig out some quick insights by analyzing the data in only one of the three perspectives. For instance, we could choose to check out the relationship between the 10 key attributes and the **diagnosis** variable by only choosing the \"mean\" columns.\n\nLet's quickly scan for any interesting patterns between our 10 \"mean\" columns and the response variable by generating a scatter plot matrix as shown below:"},{"metadata":{"_cell_guid":"bf6a891e-fd10-482e-a046-c8dbba28b668","_uuid":"51dbbec5a09319c0dc1211745c1c9d5ebd725531","scrolled":false,"trusted":true},"cell_type":"code","source":"# generate a scatter plot matrix with the \"mean\" columns\ncols = ['diagnosis',\n        'radius_mean', \n        'texture_mean', \n        'perimeter_mean', \n        'area_mean', \n        'smoothness_mean', \n        'compactness_mean', \n        'concavity_mean',\n        'concave points_mean', \n        'symmetry_mean', \n        'fractal_dimension_mean']\n\nsns.pairplot(data=df[cols], hue='diagnosis', palette='RdBu')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20291b33-424f-4cf6-824a-dee11eb2c8dd","_uuid":"9db39d34a3586db7b8b936b5c0de924d811f7319"},"cell_type":"markdown","source":"There are some interesting patterns visible. For instance, the almost perfectly linear patterns between the **radius**, **perimeter** and **area** attributes are hinting at the presence of multicollinearity between these variables. Another set of variables that possibly imply multicollinearity are the **concavity**, **concave_points** and **compactness**. \n\nIn the coming up section, we will generate a matrix similar to the one above, but this time displaying the correlations between the variables instead of a scatter plot. Let's find out if our hypothesis about the multicollinearity has any statistical support. "},{"metadata":{"_cell_guid":"52ec8505-c1b2-41c6-aa2c-93f6ed64f425","_uuid":"a14872dd172d43864cce34a4cc67ae5141289e64"},"cell_type":"markdown","source":"# 2. The Variables\n---\nAs said earlier, let's take a look at the correlations between our variables. This time however, we will create a correlation matrix with all variables (i.e., the \"mean\" columns, the \"standard errors\" columns, as well as the \"worst\" columns)."},{"metadata":{"_cell_guid":"73c26323-0332-4872-be87-2cffb9b1fdea","_uuid":"d1b40364190bcacae69a2cf84c3f096665b0f845","scrolled":false,"trusted":true},"cell_type":"code","source":"# Generate and visualize the correlation matrix\ncorr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0f69800-9d90-4f58-a884-d3d8230a06c4","collapsed":true,"_uuid":"e3500469a85aa91872a1d3e9395a3036ffa569cb"},"cell_type":"markdown","source":"Looking at the matrix, we can immediately verify the presence of multicollinearity between some of our variables. For instance, the **radius_mean** column has a correlation of 1 and 0.99 with **perimeter_mean** and **area_mean** columns, respectively. This is probably because the three columns essentially contain the same information, which is the physical size of the observation (the cell). Therefore we should only pick one of the three columns when we go into further analysis. \n\nAnother place where multicollienartiy is apparent is between the \"mean\" columns and the \"worst\" column. For instance, the **radius_mean** column has a correlation of 0.97 with the **radius_worst** column. In fact, each of the 10 key attributes display very high (from 0.7 up to 0.97) correlations between its \"mean\" and \"worst\" columns. This is somewhat inevitable, because the \"worst\" columns are essentially just a subset of the \"mean\" columns; the \"worst\" columns are also the \"mean\" of some values (the three largest values among all observations). Therefore, I think we should discard the \"worst\" columns from our analysis and only focus on the \"mean\" columns. \n\nIn short, we will drop all \"worst\" columns from our dataset, then pick only one of the three attributes that describe the size of cells. But which one should be pick?\n\nLet's quickly go back to 6th grade and review some geometry. If we think of a cell as roughly taking a form of a circle, then the formula for its radius is, well, its radius,  *r*. The formulae for its perimeter and area are then **\\\\(2\\pi r\\\\) ** and **\\\\(\\pi r^2\\\\) **, respectively. As we can see, a cell's **radius** is the basic building block of its size. Therefore, I think it is reasonable to choose **radius** as our attribute to represent the size of a cell. \n\nSimilarly, it seems like there is multicollinearity between the attributes **compactness**, **concavity**, and **concave points**. Just like what we did with the size attributes, we should pick only one of these three attributes that contain information on the shape of the cell. I think **compactness** is an attribute name that is straightforward, so I will remove the other two attributes. \n\nWe will now go head and drop all unnecessary columns. "},{"metadata":{"_cell_guid":"156ff174-a585-4eb6-9fbd-0c5861a6f439","_uuid":"bac27000a23c99672fda30b7296c09fd3d67408e","trusted":true},"cell_type":"code","source":"# first, drop all \"worst\" columns\ncols = ['radius_worst', \n        'texture_worst', \n        'perimeter_worst', \n        'area_worst', \n        'smoothness_worst', \n        'compactness_worst', \n        'concavity_worst',\n        'concave points_worst', \n        'symmetry_worst', \n        'fractal_dimension_worst']\ndf = df.drop(cols, axis=1)\n\n# then, drop all columns related to the \"perimeter\" and \"area\" attributes\ncols = ['perimeter_mean',\n        'perimeter_se', \n        'area_mean', \n        'area_se']\ndf = df.drop(cols, axis=1)\n\n# lastly, drop all columns related to the \"concavity\" and \"concave points\" attributes\ncols = ['concavity_mean',\n        'concavity_se', \n        'concave points_mean', \n        'concave points_se']\ndf = df.drop(cols, axis=1)\n\n# verify remaining columns\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e54f22be-5cda-4509-8515-f670b1805ff7","collapsed":true,"_uuid":"b45fb7fcf19dd29a5121d78d3ea51c0589987e1e"},"cell_type":"markdown","source":"Are we all set now?\n\nLet's take a look at the correlation matrix once again, this time created with our trimmed-down set of variables."},{"metadata":{"_cell_guid":"a100b412-a601-4ef0-af1b-1a014328ec68","_uuid":"95321bcfa9b6194fd49c3b5b9d9213e92d7796ce","trusted":true},"cell_type":"code","source":"# Draw the heatmap again, with the new correlation matrix\ncorr = df.corr().round(2)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3bce675-cd9c-430b-a0ca-a4ce202520dc","collapsed":true,"_uuid":"e4b91c2bd3fc2ba46acd748611f90bc9f74c8dfc"},"cell_type":"markdown","source":"Looks great! Now let's move on to our model."},{"metadata":{"_cell_guid":"ea6a87e7-5192-4f3d-87ec-566e74b95c52","_uuid":"fbc5ac65c21f0b0a7983bddfcc29c8ff7d4a325d"},"cell_type":"markdown","source":"# 3. The Model\n___\n\nIt's finally time to develop our model! We will start by first splitting our dataset into two parts; one as a training set for the model, and the other as a test set to validate the predictions that the model will make. If we omit this step, the model will be trained and tested on the same dataset, and it will underestimate the true error rate, a phenomenon known as **overfitting**. It is like writing an exam after taking a look at the questions and answers beforehand. We want to make sure that our model truly has predictive power and is able to accurately label unseen data. We will set the test size to 0.3; i.e., 70% of the data will be assigned to the training set, and the remaining 30% will be used as a test set. In order to obtain consistent results, we will set the random state parameter to a value of 40."},{"metadata":{"_cell_guid":"81c5e78d-c9e4-4909-9470-88fbbccafa32","_uuid":"28b92554ae77ea3283734acdc12e6944bae8d9ef","trusted":true},"cell_type":"code","source":"# Split the data into training and testing sets\nX = df\ny = df['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9450c803-b233-4cfb-ab2b-1a8a0275d636","_uuid":"858b842d2a95ba099241c04c9f6c22d1f95b9814"},"cell_type":"markdown","source":"Now that we have split our data into appropriate sets, let's write down the formula to be used for the `logistic regression`."},{"metadata":{"_cell_guid":"d5b377ee-4462-4bd7-9d51-9a6d4512bc66","_uuid":"4af1f9baaee26a15b3db2259ebf3a6611431f92d","scrolled":true,"trusted":true},"cell_type":"code","source":"# Create a string for the formula\ncols = df.columns.drop('diagnosis')\nformula = 'diagnosis ~ ' + ' + '.join(cols)\nprint(formula, '\\n')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7bf632a2-5111-40d3-a6f7-34adfd3f30e3","_uuid":"a3094a7abf18202fd0eba832fa6213f3582f0497"},"cell_type":"markdown","source":"The formula includes all of the variables that were finally selected at the end of the previous section. We will now run the `logistic regression` with this formula and take a look at the results."},{"metadata":{"_cell_guid":"dbe16604-b13a-4c7c-bdf2-2e3ad0977f65","_uuid":"90b8731f67bb6a8155cb44c97f17c53554f92336","scrolled":false,"trusted":true},"cell_type":"code","source":"# Run the model and report the results\nmodel = smf.glm(formula=formula, data=X_train, family=sm.families.Binomial())\nlogistic_fit = model.fit()\n\nprint(logistic_fit.summary())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a634825-9bc6-4656-8f64-409ba722815d","_uuid":"403131448aee247b990deb89e860664ce860a260"},"cell_type":"markdown","source":"Great! In the next section, we will feed in the test data to this model to yield predictions of labels. Then, we will evaluate how accurately the model have predicted the data. "},{"metadata":{"_cell_guid":"46f7728a-079e-4925-bc48-84378713abdb","_uuid":"dfe1d209f9b1b14458d114294245583853b37e3e"},"cell_type":"markdown","source":"# 4. The Prediction\n___\n\nIn the previous section, we have successfully developed a logistic regression model. This model can take some unlabeled data and effectively assign each observation a probability ranging from 0 to 1. This is the key feature of a logistic regression model. However, for us to evaluate whether the predictions are accurate, the predictions must be encoded so that each instance can be compared directly with the labels in the test data. In other words, instead of numbers between 0 or 1, the predictions should show \"M\" or \"B\", denoting malignant and benign respectively. In our model, a probability of 1 corresponds to the \"Benign\" class, whereas a probability of 0 corresponds to the \"Malignant\" class. Therefore, we can apply a threshhold value of 0.5 to our predictions, assigning all values closer to 0 a label of \"M\" and assigniing all values closer to 1 a label of \"B\". \n\nIf this is confusiing, let's go through this step-by-step."},{"metadata":{"_cell_guid":"2b12d89a-417b-4e9e-8488-b4d72f3f2f58","_uuid":"ad5ac6457c9f3389d3ef0545597d21e1dcf92475","trusted":true},"cell_type":"code","source":"# predict the test data and show the first 5 predictions\npredictions = logistic_fit.predict(X_test)\npredictions[1:6]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08edda96-9f39-4edc-943e-76dc7d9d6bd3","_uuid":"837a08b171d11107ad1b5681906352692fa335d7","scrolled":true,"trusted":true},"cell_type":"code","source":"# Note how the values are numerical. \n# Convert these probabilities into nominal values and check the first 5 predictions again.\npredictions_nominal = [ \"M\" if x < 0.5 else \"B\" for x in predictions]\npredictions_nominal[1:6]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edda11cc-1a74-4bef-b69e-4bbbd6ebce71","_uuid":"385a0e9890d2f22b6c6d5fe7456e20c4a5ee9e6f"},"cell_type":"markdown","source":"We can confirm that probabilities closer to 0 have been labeled as \"M\", while the ones closer to 1 have been labeled as \"B\". Now we are able to evaluate the accuracy of our predictions by checking out the classification report and the confusion matrix."},{"metadata":{"_cell_guid":"7d1d4482-8f8a-44a5-b82b-6a6154201ec8","_uuid":"6c9368e32cb571f6443283107b5936966aee4860","scrolled":false,"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predictions_nominal, digits=3))\n\ncfm = confusion_matrix(y_test, predictions_nominal)\n\ntrue_negative = cfm[0][0]\nfalse_positive = cfm[0][1]\nfalse_negative = cfm[1][0]\ntrue_positive = cfm[1][1]\n\nprint('Confusion Matrix: \\n', cfm, '\\n')\n\nprint('True Negative:', true_negative)\nprint('False Positive:', false_positive)\nprint('False Negative:', false_negative)\nprint('True Positive:', true_positive)\nprint('Correct Predictions', \n      round((true_negative + true_positive) / len(predictions_nominal) * 100, 1), '%')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7d77a9d-1d67-411d-9bd3-dec7e7db7eeb","_uuid":"510fac1f8a87e69999a5b7dcc56ca15368caa922"},"cell_type":"markdown","source":"Our model have accurately labeled 96.5% of the test data. This is just the beginning however. We could try to increase the accuracy even higher by using a different algorithm other than the `logistic regression`, or try our model with different set of variables. There are defintely many more things that could be done to modify our model, but I will conclude this report here for now.\n\nThank you so much for reading through this report. As I mentioned in the beginning, this is my first ever Kernel on Kaggle. I hope you enjoyed it, and please leave comments below for feedbacks and suggestions!"}],"metadata":{"language_info":{"version":"3.6.3","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":1}