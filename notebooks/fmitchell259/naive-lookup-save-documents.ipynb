{"cells":[{"metadata":{},"cell_type":"markdown","source":"**<h2><ins>Naive Lookup With Document Save</ins></h2><br>**\n\nTo begin answering the questions in task 2 I wanted to get an understanding of how many documents covered the required topics. To quickly gain that understanding I constructed a naive document look up, simply using (relatively) standard libraries to find documents based on a keyword. \n\nOnce found, the index of these documents within the corona dataframe are saved to a list. The progam then simply navigates to the relevant directory and copies the file to a newly created folder (names with date and time). \n\nI plan to use this simple lookup table to assign naive labels to the data (`smoking`, `neonates`, `high_risk`, `transmission_dynamics` and `co_infection`). Once labelled I will then use unsupervised learning (K-Means clustering) to cluster the data (using a set of vector features engineered using the doc2vec model) and compare the contents of each cluster with my manually assigned labels. Thsi clustered data will be further used to engineer features for a Convolutional Neural Net.  \n\nUsing this approach I aim to provide a `proof of concept` for using vectorised text to assign clustered features, with a view to using a CNN to classify unseen documents as containing information surrounding the outlined tasks. \n\n-------------------------------------------\n"},{"metadata":{},"cell_type":"markdown","source":"**<ins>Import Libraries</ins><br>**\n\nAs mentioned above, nothing facy here, just good old pandas, numpy and the standard library (with a dash of glob for good measure).\n\n--------------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport time\nimport shutil\nimport os\nimport stat, sys ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<ins>Custom Functions</ins><br>**\n\nBelow I created a series of custom functions for finding documents, creating a folder and copying the json files. \n\n----------------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_doc_index(subject, w2v_model):\n    \n    documents = np.where(corona_data['text_body'].str.contains(subject) ,corona_data['text_body'].index, 0)\n    index = np.where(documents != 0)\n    return index\n\n\n\ndef print_document_title(list_of_index):\n    \n    for index in list_of_index:\n        for i in index:\n            print('-----------\\n')\n            report = corona_data.iloc[i]\n            print(report['title'])\n            print(f\"Paper Index {i}\")\n            print('-------------\\n')\n    \n    \n\n\n\ndef print_document_body(list_of_index):\n    \n    for index in list_of_index:\n        for i in index:\n            print('-----------\\n')\n            report = corona_data.iloc[i]\n            print(report['text_body'])\n            print(f\"Paper Index {i}\")\n            print('-------------\\n')\n\n\n\ndef create_document_directory():\n    \n    current_directory = os.getcwd()\n    current_time = time.strftime(\"%Y-%m-%d @ %H:%M:%S\")\n    final_directory = os.path.join(current_directory, r'Saved Documents: ' + current_time)\n    if not os.path.exists(final_directory):\n       os.makedirs(final_directory)\n    folder_name = f\"Saved Documents: {current_time}\"\n    \n    return folder_name\n\n\ndef save_documents(list_of_index, corona_data, new_image_folder):\n    \n    dir_ = \"../input/CORD-19-research-challenge/2020-03-13\"\n    output_dir = \"../output/kaggle/working\"\n    \n    for i in list_of_index:\n        for ind in i:\n            paper = corona_data.iloc[ind]\n            filename = paper['doc_id']\n            source = paper['source']\n            ext = \".json\"\n            \n            source_folder = f\"/{source}/{source}/\"\n            output_dir = f\"/{new_image_folder}\"\n            \n            save_doc = shutil.copy2(os.path.join(dir_ + source_folder, filename + ext),\n                                    os.path.join(output_dir, filename + ext))\n            \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<ins>Get The Data</ins><br>**\n\nCan't do anything without it! This data is loaded from the .csv file created in a previous kernel. \n\n---------------------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_data = pd.read_csv(\"../input/kaggle-covid19/kaggle_covid-19_open_csv_format.csv\")\ncorona_data = corona_data.drop(columns=['abstract'])\ncorona_data = corona_data.fillna(\"Unknown\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<ins>A Place To Hold All Those Delightful Documents</ins><br>**\n\nWhat better place than a python dictionary. \n\nUsing the `return_doc_index` function I can query the corpus with keywords and return the document index in a list. \n\n----------------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_folder = {\"risk\": return_doc_index(\"risk\", corona_data),\n\n              \"preg\": return_doc_index(\"pregnant\", corona_data),\n\n               \"smoking\": return_doc_index(\"smoking\", corona_data),\n\n               \"co_infection\": return_doc_index(\"co infection\", corona_data),\n\n                \"neonates\": return_doc_index(\"neonates\", corona_data),\n\n               \"transmission\": return_doc_index(\"transmission dynamics\", corona_data),\n\n                \"high_risk\": return_doc_index(\"high-risk patient\", corona_data)\n             }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<ins>Take A Peek Inside The Folder</ins><br>**\n\nFor sanity let's check that it actually found some documents. \n\n--------------------------"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(f\"Number of Documents that Mention Risk: {len(doc_folder['risk'][0])}\")\n\nprint(f\"Number of Documents that Mention Pregnancy: {len(doc_folder['preg'][0])}\")\n\nprint(f\"Number of Documents that Mention Smoking: {len(doc_folder['smoking'][0])}\")\n\nprint(f\"Number of Documents that Mention Neonates: {len(doc_folder['neonates'][0])}\")\n\nprint(f\"Number of Documents that Mention Transmission Dynamics: {len(doc_folder['transmission'][0])}\")\n\nprint(f\"Number of Documents that Mention High Risk Patients: {len(doc_folder['high_risk'][0])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<ins>Save Files And We're Done!</ins><br>**\n\nWith the index of any relevant documents saved I use the `create_document_folder` and `save_documents` function to...well....create a folder and save the documents. \n\n---------------------------------------\n\n<i>Please note that I have hashed out the final line of code as it accesses the original dataset of json files, please ensure you are running this script from a root directory that contains the project subfolders.</i>\n\n------------------------------------------\n\n<i>Also note that if running in a local environment you will need to change the `directory path` inside the `save_documents` function.</i>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_image_folder = create_document_directory()\n\n# save_docs = save_documents(doc_folder['high_risk'], corona_data, new_image_folder)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n-----------------------------------------"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}