{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df.diagnosis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create and Prepare the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('id',axis =1,inplace = True)\ndf.drop('Unnamed: 32',axis =1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##to see how many null values are there\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['diagnosis'],color='g')\nplt.title('Plot Diagnosis (M=1 ,B=0)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate a scatter plot matrix with the \"mean\" columns\ncols = ['diagnosis',\n        'radius_mean', \n        'texture_mean', \n        'perimeter_mean', \n        'area_mean', \n        'smoothness_mean', \n        'compactness_mean', \n        'concavity_mean',\n        'concave points_mean', \n        'symmetry_mean', \n        'fractal_dimension_mean']\n\nsns.pairplot(data=df[cols], hue='diagnosis', palette='rocket')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here radius mean,perimeter mean ,area mean are highly correlated with other ,thus leading to problem known as multicollinearity\n\n\nalmost perfectly linear patterns between the radius, perimeter and area attributes are hinting at the presence of multicollinearity between these variables. (they are highly linearly related) Another set of variables that possibly imply multicollinearity are the concavity, concave_points and compactness.\n\nMulticollinearity is a problem as it undermines the significance of independent varibales and we fix it by removing the highly correlated predictors from the model.\n\nUse Partial Least Squares Regression (PLS) or Principal Components Analysis, regression methods that cut the number of predictors to a smaller set of uncorrelated components.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate and visualize the correlation matrix\n\ncorr = df.corr().round(2) ##round-off to 2\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" we can verify the presence of multicollinearity between some of the variables. For instance, the radius_mean column has a correlation of 1 and 0.99 with perimeter_mean and area_mean columns, respectively.\n\nThis is because the three columns essentially contain the same information, which is the physical size of the observation (the cell). Therefore we should only pick ONE of the three columns when we go into further analysis.\n\n\n\nAnother place where multicollienartiy is apparent is between the \"mean\" columns and the \"worst\" column.\n\nFor instance, the radius_mean column has a correlation of 0.97 with the radius_worst column.\nalso there is multicollinearity between the attributes compactness, concavity, and concave points.\nSo we can choose just ONE out of these, I am going for Compactness."},{"metadata":{"trusted":true},"cell_type":"code","source":"# first, drop all \"worst\" columns\ncols = ['radius_worst', \n        'texture_worst', \n        'perimeter_worst', \n        'area_worst', \n        'smoothness_worst', \n        'compactness_worst', \n        'concavity_worst',\n        'concave points_worst', \n        'symmetry_worst', \n        'fractal_dimension_worst']\ndf = df.drop(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# then, drop all columns related to the \"perimeter\" and \"area\" attributes\ncols = ['perimeter_mean',\n        'perimeter_se', \n        'area_mean', \n        'area_se']\ndf = df.drop(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lastly, drop all columns related to the \"concavity\" and \"concave points\" attributes\ncols = ['concavity_mean',\n        'concavity_se', \n        'concave points_mean', \n        'concave points_se']\ndf = df.drop(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# verify remaining columns\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw the heatmap again, with the new correlation matrix\ncorr = df.corr().round(2)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building a Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=df, hue='diagnosis', palette='rocket')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['diagnosis'],axis=1)\ny = df['diagnosis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test,y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=40)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"\nStandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. ... StandardScaler makes the mean of the distribution 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It transforms the data in such a manner that it has mean as 0 and standard deviation as 1. \nIn short, it standardizes the data. Standardization is useful for data which has negative values. \nIt arranges the data in a standard normal distribution."},{"metadata":{},"cell_type":"markdown","source":"### Models and finding out the Best one\n"},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr =LogisticRegression()\n\nmodel1 = lr.fit(X_train,y_train)\nprediction_model1 = model1.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm =confusion_matrix(y_test,prediction_model1)\ncm##True Positive,etc thing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm,annot=True)\nplt.savefig('confusion_matrix_loR.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP=cm[0][0]\nTN=cm[1][1]\nFN=cm[1][0]\nFP=cm[0][1]\nprint('Testing Accuracy:',(TP+TN)/(TP+TN+FN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,prediction_model1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,prediction_model1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\nmodel2 = dtc.fit(X_train,y_train)\nprediction_model2 =model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(y_test,prediction_model2)\ncm2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,prediction_model2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,prediction_model2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel3 = rfc.fit(X_train,y_train)\nprediction_model3 = model3.predict(X_test)\ncm3 = confusion_matrix(y_test,prediction_model3)\ncm3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,prediction_model3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,prediction_model3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNN  \n"},{"metadata":{},"cell_type":"markdown","source":"#### SVM   "},{"metadata":{},"cell_type":"markdown","source":"#### Naives Bayes  "},{"metadata":{},"cell_type":"markdown","source":"Techniques to create multiple model at same time"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models =[]\nmodels.append(('KNN',KNeighborsClassifier()))\nmodels.append(('SVC',SVC()))\nmodels.append(('Naive Bayes',GaussianNB()))\n\n\nmodels.append(('LogisticRegression',LogisticRegression()))\nmodels.append(('DecisionTreeClassifier',DecisionTreeClassifier()))\nmodels.append(('RandomForestClassifier',RandomForestClassifier()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KFOLD is a model validation technique, where it's not using your pre-trained model. Rather it just use the hyper-parameter and trained a new model with k-1 data set and test the same model on the kth set. K different models are just used for validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model\n\nresults =[]\nnames=[]\n\nfor name , model in models:\n    kfold =KFold(n_splits = 10 ,random_state = 40)\n    cv_results =cross_val_score(model, X_train, y_train, cv=kfold,scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    \n    msg= '%s:, %f, (%f)' % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here SVC is giving the best result so using it to make predictions on test Data"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# make predictions on test datasets\n\nSVM = SVC()\nmodel4 = SVM.fit(X_train,y_train)\nprediction_model4 = model4.predict(X_test)\n\nprint(confusion_matrix(y_test, prediction_model4))\nprint(accuracy_score(y_test, prediction_model4))\nprint(classification_report(y_test, prediction_model4))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We are getting the best accuracy with SVM which is 96.4% , the model is predicting with 96% accuracy on our test data"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}