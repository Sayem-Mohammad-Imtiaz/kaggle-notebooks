{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Dataset Information","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"    RI: refractive index\n    Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n    Mg: Magnesium\n    Al: Aluminum\n    Si: Silicon\n    K: Potassium\n    Ca: Calcium\n    Ba: Barium\n    Fe: Iron\n    \n    Type of glass: \n        1 building_windows_float_processed\n        2 building_windows_non_float_processed\n        3 vehicle_windows_float_processed\n        4 vehicle_windows_non_float_processed (none in this database)\n        5 containers\n        6 tableware\n        7 headlamps","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\npd.set_option('display.max_columns', None)\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"glass = pd.read_csv(\"../input/glass/glass.csv\")\nglass.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features in data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the data columns\ncols = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']\ntarget = [\"Type\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset Info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"glass.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statistical Summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"glass.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Countplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(glass[\"Type\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking distribution of the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(3,3, figsize=(16, 12))\nax = ax.flatten()\ni = 0\nfor col in cols:\n    skew = glass[col].skew()\n    sns.distplot(glass[col], ax = ax[i], fit= stats.norm, kde=False, label='Skew = %.3f' %(skew))\n    ax[i].legend(loc='best')\n    i += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    None of the features are normally distributed and some have outliers\n\n    Note: Outlier treatment maybe done to check impact on classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Univariate Box Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"glass.iloc[:,:-1].boxplot(figsize=(12,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n    - Silicon is the main component of Glass making more than 70% of composition\n    - Combined Silicon, Sodium and Calcium make up around 90%\n    - Iron is the least important component\n\nAbove box plot confirms the outliers\n\n    I prefer to use models without outlier treatment, in many cases it can improve the model performance.\n    But it also leads to change of information which might alter real/practical situations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Box plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(3,3, figsize=(16, 12))\nax = ax.flatten()\ni = 0\nfor col in cols:\n    sns.boxplot(\"Type\", col, ax = ax[i], data=glass)\n    ax[i].legend([col], loc='best')\n    i += 1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences\n    - Refractive index lies between 1.51 and 1.54\n    - Type 6 and 7 have higher Na %\n    - Type 1,2 and 3 have higher Mg %\n    - Type 5 and 7 have higher Al %\n    - Si % is similar in all types\n    - Type 6 has no K composition\n    - Type 5 and 6 have higher Ca composition\n    - Ba is mostly used in Type 7\n    - Fe is used in Type 1,2 and 3","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Scatter Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.plotting.scatter_matrix(glass.iloc[:,:-1], c=glass.iloc[:,-1], figsize=(20, 20), marker='o')\nplt.legend(glass[\"Type\"].unique())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pairplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(glass, hue='Type', diag_kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\ncorr = glass.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, fmt= '.2f', cmap='YlGnBu', mask=mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    K and Ca have no correlation with Type, which means for some type it maybe high for some low causing cancelling effect","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering - Based on the mean of K and Ca in classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"glass.groupby(\"Type\")[\"Ca\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glass.groupby(\"Type\")[\"K\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glass[\"Ca_morethan9\"] = np.where(glass[\"Ca\"]>9, 1, 0)\nglass[\"K_morethandot7\"] = np.where(glass[\"K\"]>0.7, 1, 0)\nglass[\"K_lessthandot4\"] = np.where(glass[\"K\"]<0.4, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols.append(\"Ca_morethan9\")\ncols.append(\"K_morethandot7\")\ncols.append(\"K_lessthandot4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statistical Importance Check for Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.stats as sms\n\nfor col in cols:\n    data = sm.formula.ols(col+\"~ Type\", data=glass).fit()\n    pval = sms.anova.anova_lm(data)[\"PR(>F)\"][0]\n    print(f\"Pval for {col}: {pval}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K and Ca are not siginificant, but the new variables we have created are significant. KUDOS!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing & Evaluation Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier,\\\n                            BaggingClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into train and test\ndef split_data(X, Y, seed=1, train_size=0.7):\n    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, train_size=train_size, random_state = seed, stratify=Y)\n    xtrain, xtest = preprocess(xtrain, xtest)\n    return (xtrain, xtest, ytrain, ytest)\n\n# preprocess the data for training\ndef preprocess(x1, x2=None):\n    sc = StandardScaler()\n    x1 = pd.DataFrame(sc.fit_transform(x1), columns=x1.columns)\n    if x2 is not None:\n        x2 = pd.DataFrame(sc.transform(x2), columns=x2.columns)\n        return (x1,x2)\n    return x1\n\n# for model evaluation and training\ndef eval_model(model, X, Y, seed=1):\n    xtrain, xtest, ytrain, ytest = split_data(X, Y)\n    model.fit(xtrain, ytrain)\n    \n    trainpred = model.predict(xtrain)\n    trainpred_prob = model.predict_proba(xtrain)\n    testpred = model.predict(xtest)\n    testpred_prob = model.predict_proba(xtest)\n    \n    print(\"Train ROC AUC : %.4f\"%roc_auc_score(ytrain, trainpred_prob, multi_class='ovr'))\n    print(\"\\nTrain classification report\\n\",classification_report(ytrain, trainpred))\n    \n    ### make a bar chart for displaying the wrong classification of one class coming in which other class\n    \n    print(\"\\nTest ROC AUC : %.4f\"%roc_auc_score(ytest, testpred_prob, multi_class='ovr'))\n    print(\"\\nTest classification report\\n\",classification_report(ytest, testpred))\n    \ndef plot_importance(columns, importance):\n    plt.bar(columns, importance)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating the X and Y data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = glass.drop([\"Type\"], axis=1)\nX_sc = preprocess(X)\nY = glass[\"Type\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating array of models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_logr = LogisticRegression(random_state=seed,n_jobs=-1)\nmodel_nb = GaussianNB()\nmodel_dt = DecisionTreeClassifier(random_state=seed)\nmodel_dt_bag = BaggingClassifier(model_dt, random_state=seed, n_jobs=-1)\nmodel_ada = AdaBoostClassifier(random_state=seed)\nmodel_gbc = GradientBoostingClassifier(random_state=seed)\nmodel_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)\nmodel_xgb = XGBClassifier(random_state=seed)\nmodel_lgbm = LGBMClassifier(random_state=seed, n_jobs=-1)\nmodel_knn = KNeighborsClassifier(n_jobs=-1)\n\nmodels = []\nmodels.append(('LR',model_logr))\nmodels.append(('NB',model_nb))\nmodels.append(('DT',model_dt))\nmodels.append(('Bag',model_dt_bag))\nmodels.append(('Ada',model_ada))\nmodels.append(('GBC',model_gbc))\nmodels.append(('RF',model_rf))\nmodels.append(('XGB',model_xgb))\nmodels.append(('LGBM',model_lgbm))\nmodels.append(('KNN',model_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running the algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\nresults = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison of Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Only Significant Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = glass.drop([\"Type\",\"K\",\"Ca\"], axis=1)\nX_sc = preprocess(X)\nY = glass[\"Type\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<H3>Random Forest Model has best performance, so we can work further on it and tune to improve performance\nModel Tuning can be performed using RandomGridSearchCV or Bayesian Optimization which I will add further.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Application of PCA\n<b>PCA is a statistical method which can help identify the pattern in data and also help in dimensionality reduction. However, there is a misconception that PCA reduces the set of variables but this is not the case.\nPCA transforms the variables into a new coordinate system with variables accounting for the maximum variance in the data.</b>\n<p><b> Also remember that PCA requires standardized data as the variable scale can heavily impact the transformation and end up with total garbage</b></p>\n\n<H3>A small tip, eigenvalues >0.7 indicate a strong variable importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nX = glass.drop([\"Type\"], axis=1)\nX_std = preprocess(X)\npca = PCA(n_components=None)\n# None means that we are selecting all the principal components. Once again, WE ARE NOT DROPPING ANY VARIABLES.\npca.fit(X_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the eigenvalues\npca.explained_variance_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the % of variance explained\nvar_exp = pca.explained_variance_ratio_\ncum_var = np.cumsum(pca.explained_variance_ratio_)\nprint(\"Cummulative variance:\\n\", cum_var)\nplt.plot(range(1, len(var_exp)+1), cum_var, color='r', marker='^', label=\"Cummulative Variance\")\nplt.bar(range(1, len(var_exp)+1), var_exp, color='r', label=\"Individual Variance\")\nplt.legend(loc='best')\nplt.title(\"PCA components vs Variance Explained\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First two components represent almost 50% variance in the data.\n\nThe first 8 components represent more than 95% variance in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca8 = PCA(n_components = 8)\nX_pca = pd.DataFrame(pca8.fit_transform(X_std))\nX_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_pca, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca9 = PCA(n_components = 10)\nX_pca = pd.DataFrame(pca9.fit_transform(X_std))\nX_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_pca, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca9 = PCA(n_components = 0.99)\nX_pca = pd.DataFrame(pca9.fit_transform(X_std))\nX_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_pca, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thank You for viewing my Kernel, if you like it or have any suggestions you are welcome. Also please upvote the Kernel.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}