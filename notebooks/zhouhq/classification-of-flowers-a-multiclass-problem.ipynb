{"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","file_extension":".py","name":"python"}},"cells":[{"source":"***Hongqiang Zhou***\n\nIn the given data set, we have 210 color images of flowers, each is labeled as one from ten classes. In this notebook, a neural network model is built and trained on this data set. Considering the complexity of the problem, the present data set may be small for model training and regularization. Nevertheless, I find it has a lot of fun to practice machine learning skills on these beautiful images, and hope it will become a good start for a brain storm.\n\nThis notebook includes following sections:\n        1. Building a neural network with TensorFlow\n        2. Data manipulation\n        3. Model applications\n            3.1 Softmax regression\n            3.2 Three-layer neural network\n        4. A brief conclusion\n\nAcknowledgement: I highly appreciate Olga Belitskaya for her wonderful job and commitment to this community.  \n\n\n**1. Building a neural network with TensorFlow**","cell_type":"markdown","metadata":{"_uuid":"2b6bbd30919cef25701da04f54ac099dbcdebc6e","_cell_guid":"392e4a40-2a7f-4103-b7cb-d87ddd4827cc"}},{"source":"import numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom itertools import cycle\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5, 4) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'","outputs":[],"cell_type":"code","metadata":{"_uuid":"9769470b1c9d1e4de28b33f0fd7a8ab5bf999832","_cell_guid":"1306a8b7-469d-4033-964b-0439332badb0","collapsed":true},"execution_count":1},{"source":"The neural network model is built with functions in below cell. This model includes a few key techniques, i.e., gradient descent, mini-batch, and L2-regularization. The TensorFlow library saves us a lot of effort in coding the tedious procedures.","cell_type":"markdown","metadata":{"_uuid":"3a3e7f3f162b5679a430a35c8e049834167c375e","_cell_guid":"68812676-f59a-49c3-b593-fa9624591eae"}},{"source":"def create_placeholders(n_x, n_y):\n    X = tf.placeholder(dtype = tf.float32, shape = (n_x, None), name = 'X')\n    Y = tf.placeholder(dtype = tf.float32, shape = (n_y, None), name = 'Y')\n    \n    return X, Y\n\ndef initialize_parameters(layers_dims):\n    num_layers = len(layers_dims) - 1\n    parameters = {}\n    for l in range(1, num_layers + 1):\n        parameters['W' + str(l)] = tf.get_variable('W' + str(l), [layers_dims[l], layers_dims[l - 1]],\\\n                        initializer = tf.contrib.layers.xavier_initializer(seed = next(seeds)))\n        parameters['b' + str(l)] = tf.get_variable('b' + str(l), [layers_dims[l], 1], \\\n                                                  initializer = tf.zeros_initializer())\n    return parameters   \n\ndef forward_propagation(X, parameters):\n    L = len(parameters) // 2\n    A = X\n    for l in range(1, L):\n        Z = tf.add(tf.matmul(parameters['W' + str(l)], A), parameters['b' + str(l)])\n        A = tf.nn.relu(Z)\n    ZL = tf.add(tf.matmul(parameters['W' + str(L)], A), parameters['b' + str(L)])\n    \n    return ZL\n\ndef compute_l2_regularization_cost(parameters, l2):\n    L = len(parameters) // 2\n    cost = 0.0\n    for l in range(1, L + 1):\n        cost += tf.reduce_sum(tf.nn.l2_loss(parameters['W' + str(l)]))\n    l2_regularization = cost * l2\n    \n    return l2_regularization\n\ndef compute_cross_entropy_cost(ZL, Y):\n    logits = tf.transpose(ZL)\n    labels = tf.transpose(Y)\n    cross_entropy_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n                                            logits = logits, labels = labels))\n    \n    return cross_entropy_cost\n\ndef random_mini_batches(X, Y, minibatch_size = 64):\n    m = X.shape[1]\n    minibatches = []\n    \n    np.random.seed(next(seeds))\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n    \n    num_complete_minibatches = m // minibatch_size\n    for k in range(0, num_complete_minibatches):\n        minibatch_X = shuffled_X[:, k * minibatch_size : (k + 1) * minibatch_size]\n        minibatch_Y = shuffled_Y[:, k * minibatch_size : (k + 1) * minibatch_size]\n        minibatch = (minibatch_X, minibatch_Y)\n        minibatches.append(minibatch)\n        \n    if m % minibatch_size != 0:\n        minibatch_X = shuffled_X[:, num_complete_minibatches * minibatch_size : ]\n        minibatch_Y = shuffled_Y[:, num_complete_minibatches * minibatch_size : ]\n        minibatch = (minibatch_X, minibatch_Y)\n        minibatches.append(minibatch)\n    \n    return minibatches\n                     \ndef model(X_train, Y_train, layers_dims, l2 = 1e-6, learning_rate = 0.0001, num_epochs = 100, \\\n         minibatch_size = 64, print_cost_interval = None):\n    \n    ops.reset_default_graph()\n    (n_x, m) = X_train.shape\n    n_y = Y_train.shape[0]\n    costs =[]\n    \n    X, Y = create_placeholders(n_x, n_y)\n    parameters = initialize_parameters(layers_dims)\n    ZL = forward_propagation(X, parameters)\n    cross_entropy_cost = compute_cross_entropy_cost(ZL, Y)\n    l2_regularization_cost = compute_l2_regularization_cost(parameters, l2)\n    cost = cross_entropy_cost + l2_regularization_cost\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n    init = tf.global_variables_initializer()\n    \n    with tf.Session() as sess:\n        sess.run(init)\n        \n        for epoch in range(num_epochs):\n            epoch_cost = 0.0\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n            num_minibatches = len(minibatches)\n            \n            for minibatch in minibatches:\n                (minibatch_X, minibatch_Y) = minibatch\n                _, minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, \\\n                                                                             Y: minibatch_Y})\n                epoch_cost = epoch_cost + minibatch_cost / num_minibatches \n                \n            costs.append(epoch_cost)\n            \n            if print_cost_interval is not None and epoch % print_cost_interval == 0:\n                print('Cost after epoch {}: {}'.format(epoch, np.float(epoch_cost)))\n        else:\n            if print_cost_interval is not None:\n                print('Cost after epoch {}: {}'.format(epoch, np.float(epoch_cost)))\n            \n        parameters = sess.run(parameters)\n    \n    return parameters, costs","outputs":[],"cell_type":"code","metadata":{"_uuid":"80b03d5594ddfbd91d7463a0b9b5141c18d68468","_cell_guid":"56fb32a2-5f66-4629-ad8c-69dd7bbe5ccf","collapsed":true},"execution_count":2},{"source":"def predict(parameters, X):\n    nx = X.shape[0]\n    params = {}\n    L = len(parameters) // 2\n    for l in range(1, L+1):\n        params['W' + str(l)] = tf.convert_to_tensor(parameters['W' + str(l)])\n        params['b' + str(l)] = tf.convert_to_tensor(parameters['b' + str(l)])\n    \n    x = tf.placeholder(dtype = tf.float32, shape = (nx, None))\n    z = forward_propagation(x, params) \n    preds = tf.argmax(z)\n    \n    with tf.Session() as sess:\n        preds = sess.run(preds, feed_dict = {x: X})\n        \n    return preds","outputs":[],"cell_type":"code","metadata":{"_uuid":"fd2202a51a1c2ead60c4e8df6c3decf5711f0b2d","_cell_guid":"dd2e032d-3034-4b84-b4e1-733d590af144","collapsed":true},"execution_count":3},{"source":"**2. Data manipulation**\n\nEach image in the data set has 128 by 128 pixels, respectively in horizontal and vertical dimensions, and three layers representing green, blue, and red colors.","cell_type":"markdown","metadata":{"_uuid":"9a2a90c9286603cf73d3868d496fd77b25f2bd35","_cell_guid":"39e75713-a0ca-4bb7-9d05-ce09081c47b1"}},{"source":"images = []\nfor i in range(1, 211):\n    fname = '../input/flower_images/' + str(i).zfill(4) + '.png'\n    image = np.array(ndimage.imread(fname, flatten = False))\n    image = scipy.misc.imresize(image, size = (128, 128))\n    images.append(image)\n    \nimages = np.asarray(images)\n\nlabels = pd.read_csv('../input/flower_images/flower_labels.csv')\nlabels = labels['label']\nlabels = np.asarray(labels)","outputs":[],"cell_type":"code","metadata":{"_uuid":"19d65980798143cd2bf75c0d46a22f43ee7439b6","_cell_guid":"ff71c67d-8c23-432c-ba2d-59b24f26ffca","collapsed":true},"execution_count":4},{"source":"To make future works easier, we can wrap all images into a single HDF5 file through procedures in below cell.","cell_type":"markdown","metadata":{"_uuid":"1aa4d619de219c9ace0ca2da8d5b4a8f3a553365","_cell_guid":"580b8aa1-786b-46a5-a8ea-bfd1db529f32"}},{"source":"\"\"\"\nwith h5py.File('../input/FlowerColorImages.h5', 'w') as f:\n    f.create_dataset('images', data = images)\n    f.create_dataset('labels', data = labels)\n\nwith h5py.File('../input/FlowerColorImages.h5', 'r') as f:\n    images = f['images'].value\n    labels = f['labels'].value    \n\"\"\"","outputs":[],"cell_type":"code","metadata":{"_uuid":"dac413882ba7f09fbf8832a32247d32ba5012841","_cell_guid":"aaf35705-e947-4af5-87c7-3cc2916b56b7"},"execution_count":5},{"source":"Here I play a trick to double the size of the given data set, i.e., to flip all images in horizontal dimension. ","cell_type":"markdown","metadata":{"_uuid":"2b1a683569e26410d40721308638ac131e11f435","_cell_guid":"bd597968-9cac-4e50-854e-2b6e02425ae9"}},{"source":"images_flip = np.zeros(images.shape)\nlabels_flip = np.zeros(labels.shape)\nfor i in range(len(images)):\n    images_flip[i, :, :, :] = images[i, :, np.arange(127, -1, -1), :]\n    labels_flip[i] = labels[i]\nimages_expanded = np.concatenate([images, images_flip], axis = 0)\nlabels_expanded = np.concatenate([labels, labels_flip]).astype(int)","outputs":[],"cell_type":"code","metadata":{"_uuid":"91fd00e977fa3f9571d0a5cd49d136562f57dcb4","_cell_guid":"8236405a-1db2-4b65-9f0b-576adcbc425c","collapsed":true},"execution_count":6},{"source":"X = images_expanded[:, :, :, :3].reshape(420, -1) / 255.\nY = np.eye(10)[labels_expanded.reshape(-1)]\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.3, random_state = 1)","outputs":[],"cell_type":"code","metadata":{"_uuid":"fb5711794de61eb7f229ddd1375c91f990534d11","_cell_guid":"cfd48da3-a8b4-45f7-a832-01ec40ee2cb0","collapsed":true},"execution_count":7},{"source":"**3. Model applications**\n\nCross-validation is a good approach to regularize the hyper-parameters. This can be done with the below cell. I have conducted some preliminary regularization on my PC and obtained relatively good parameters. It is recommended to do a fine-tune on a more powerful machine, for example, a GPU server which can be rented in Amazon or Microsoft's cloud system. Hopefully, this can make significant imporvement on model performance. \n\nThis cell has been commented off to prevent readers from accidently starting a terrifyingly long computation.","cell_type":"markdown","metadata":{"_uuid":"ed2f4e991546f151ceb45d94997fe0d36bf6b347","_cell_guid":"df2b19b3-343a-431b-8002-63a99129f525"}},{"source":"\"\"\"\ndef k_fold_cross_validation(X, Y, k, layer_dims, l2):\n    fold_size = X.shape[0] // k\n    np.random.seed(next(seeds))\n    permutation = list(np.random.permutation(X.shape[0]))\n    shuffled_X = X[permutation, :]\n    shuffled_Y = Y[permutation, :]\n\n    accuracy = 0.0\n    for i in range(k):\n        val_X = shuffled_X[i * fold_size : (i + 1) * fold_size, :]\n        val_Y = shuffled_Y[i * fold_size : (i + 1) * fold_size, :]\n        \n        train_X = np.concatenate([shuffled_X[0 : i * fold_size, :],  \\\n                                  shuffled_X[(i + 1) * fold_size : -1, :]], axis = 0)\n        train_Y = np.concatenate([shuffled_Y[0 : i * fold_size, :], \\\n                                 shuffled_Y[(i + 1) * fold_size : -1, :]])\n        \n        parameters, _ = model(train_X.T, train_Y.T, layer_dims, l2, learning_rate = 1e-4, \\\n                                   num_epochs = 600, minibatch_size = 32, print_cost_interval = None)\n        \n        preds = predict(parameters, val_X.T)\n        accuracy += np.sum(preds == np.argmax(val_Y, axis = 1))\n           \n    accuracy = accuracy / float(k * fold_size)\n    \n    return accuracy \n\"\"\"","outputs":[],"cell_type":"code","metadata":{"_uuid":"a2aab1f70a71bb06aa82c4c20e9c659237f1f521","_cell_guid":"d20ffb25-7aaa-4e68-9d16-cb2fa8de1678"},"execution_count":8},{"source":"\"\"\"\nnp.random.seed(1)\nseeds = np.random.randint(0, 10000, 10000)\nseeds = cycle(seeds)\naccuracies = []\n\nfor l2 in 10 ** np.linspace(-5, 1, 7):\n    accuracy = k_fold_cross_validation(train_X, train_Y, 5, [49152, 64, 30, 10], l2, next(seeds))\n    accuracies.append(accuracy)\n    print('l2 = {0}, accuracy = {1}'.format(l2, accuracy))\n    \npd.DataFrame({'l2': 10 ** np.linspace(-3, 1, 5), 'accuracy': accuracies}).to_csv( \\\n        'cv_cache.csv', header = True, index = False)    \n\"\"\"","outputs":[],"cell_type":"code","metadata":{"_uuid":"824e70f90ff1c9073c0b7e7bfe37934a6cf9e547","_cell_guid":"179562ba-dbc1-49e5-aae2-51574eb081fb"},"execution_count":9},{"source":"***3.1 Softmax regression***\n\nWe first test the neural network without hidden layers. In this case, it is equivalent to a softmax regression model. ","cell_type":"markdown","metadata":{"_uuid":"921c1bf86fd1c9497ede81c1d1e55de628867929","_cell_guid":"8536e898-7ce0-4459-b10f-2f7054147a68"}},{"source":"np.random.seed(1)\nseeds = np.random.randint(0, 10000, 10000)\nseeds = cycle(seeds)\n\nparameters, costs = model(train_X.T, train_Y.T, [49152, 10], l2 = 0.001, learning_rate = 0.0001, \\\n                          num_epochs = 400, minibatch_size = 32, print_cost_interval = 100)","outputs":[],"cell_type":"code","metadata":{"_uuid":"2136aacba7c0e0e03cda8fba802f6a69b0d92558","_cell_guid":"0a75949f-c22a-4cd4-8599-fd352d0e5dd0"},"execution_count":10},{"source":"In below cell, we plot the cost function with epochs. The computation stabilizes well before iterations are terminated.","cell_type":"markdown","metadata":{"_uuid":"5b9599598ffa8ab5ddbfb550c5ca7c7011c2d1e2","_cell_guid":"63a0875a-b994-4321-a404-8426f7402ed9"}},{"source":"fig, ax = plt.subplots()\nplt.plot(costs)\nplt.xlabel('epoch')\nplt.ylabel('cost')\nplt.title('Cost minimization')","outputs":[],"cell_type":"code","metadata":{"_uuid":"3cd5f215db3b7c38bc61ad5d1a0b37b526ef5ca8","_cell_guid":"365b20c0-d0b4-4286-9e72-880bcc18ddf4"},"execution_count":11},{"source":"In below cell, we compute the test error. The model accuracy is not exciting. ","cell_type":"markdown","metadata":{"_uuid":"54ca26fb4e470fa3fe4907a890f34a5c7b96ec5d","_cell_guid":"00aed3d1-bd54-44f6-a887-79449d0b0367"}},{"source":"preds = predict(parameters, test_X.T)\naccuracy = np.sum(preds == np.argmax(test_Y, axis = 1)) / float(test_Y.shape[0])\nprint ('Model accuracy = {}'.format(accuracy))","outputs":[],"cell_type":"code","metadata":{"_uuid":"d7c52390a2a61070925b15cf9bf2e57c99c338e8","_cell_guid":"d315e7fe-e914-439a-9c34-e8501878b97d"},"execution_count":12},{"source":"***3.2 Three-layer neural network***\n\nWe then build a three-layer neural network, and see if this will help improve the model accuracy.","cell_type":"markdown","metadata":{"_uuid":"c1dc5bb87aec0d2ae73ffd4078cf5272d562a15b","_cell_guid":"e3620894-54ba-439a-9eea-df8082bc29ed","collapsed":true}},{"source":"parameters, costs = model(train_X.T, train_Y.T, [49152, 64, 30, 10], l2 = 0.1, learning_rate = 0.0001, \\\n                          num_epochs = 600, minibatch_size = 32, print_cost_interval = 100)","outputs":[],"cell_type":"code","metadata":{"_uuid":"f6bc593a5a120b3cba093984cdfb44aa68d0bae5","_cell_guid":"22092d4c-e08a-417a-9877-07277b8c2b53"},"execution_count":13},{"source":"fig, ax = plt.subplots()\nplt.plot(costs)\nplt.xlabel('epoch')\nplt.ylabel('cost')\nplt.title('Cost minimization')","outputs":[],"cell_type":"code","metadata":{"_uuid":"d00099c777d465d7a2fa57cc0235f7c58b03ae57","_cell_guid":"7618412b-081b-4cff-a2d1-0152089f27f2"},"execution_count":14},{"source":"preds = predict(parameters, test_X.T)\naccuracy = np.sum(preds == np.argmax(test_Y, axis = 1)) / float(test_Y.shape[0])\nprint ('Model accuracy = {}'.format(accuracy))   ","outputs":[],"cell_type":"code","metadata":{"_uuid":"b15405f353b00f2c3d48ef41a1922e1e560258e7","_cell_guid":"8507ce60-9754-40ef-bafa-81d0d7e05403"},"execution_count":15},{"source":"It improves model accuracy a little bit. There could be further improvement if we test a lot more combination of hyper-parameters. Another way to improve the model is to train it on a larger data set.","cell_type":"markdown","metadata":{"_uuid":"5d558a40ede7ef5b4e519ab59e153e2d447feb22","_cell_guid":"fee363db-856d-45f0-9ce3-ca77f299d1cf","collapsed":true}},{"source":"**4. A brief conclusion**\n\nIn this notebook, we develop a neural network model and apply it on a data set of 210 color images of flowers. Our experiments reveal that a multi-layer neural network is capable of better prediction accuracy compared with a softmax regression model trained on the same training data set. \n\nI believe there is stil a big margin for the present model to improve if large data set is available. The limited size of present data set also induces significant variance. Test error can be significantly affected by the random state in training and test data. ","cell_type":"markdown","metadata":{"_uuid":"77b09a0fa95376a9d9c5cb4cdf0562851d370245","_cell_guid":"3e8ff77d-33b8-4d38-840b-c9163372ae87","collapsed":true}},{"source":"*Copyright reserved to Hongqiang Zhou (hongqiang.zhou@hotmail.com)*\n\n*Last updated on 31 Oct. 2017*","cell_type":"markdown","metadata":{"_uuid":"c1470118c927c6a3766934ad2923fc9ddf2a54f1","_cell_guid":"cba3c4da-c4ec-42c2-8ed5-d0a9a221b5d1","collapsed":true}},{"source":"","outputs":[],"cell_type":"code","metadata":{"_uuid":"17953e6dad2ab8d302f3b9b206513a99ea05c15a","_cell_guid":"35adcbbd-174c-4ba7-9fa8-54c5f9b1bd7d","collapsed":true},"execution_count":null}],"nbformat":4}