{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport imblearn\nimport time\nimport sklearn.metrics as m\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n#Load Data\ndf=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n\ndf.info()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = []\nfor i in df.columns:\n    if len(df[i].unique())==1:\n        drop_cols.append(i)\nprint(\"Total columns with only 1 unique value:\", len(drop_cols))\ndf.drop(drop_cols, 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[' Flow Packets/s']] = df[[' Flow Packets/s']].apply(pd.to_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset on train and test\nfrom sklearn.model_selection import train_test_split\ntrain, test=train_test_split(df,test_size=0.3, random_state=10)\n\n#Exploratory Analysis\n# Descriptive statistics\ntrain.describe()\ntest.describe()\n\n# Packet Attack Distribution\ntrain[' Label'].value_counts()\ntest[' Label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scalling numerical attributes\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = train.select_dtypes(include=['float64','int64']).columns\nsc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))\nsc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))\n\n# turn the result back to a dataframe\nsc_traindf = pd.DataFrame(sc_train, columns = cols)\nsc_testdf = pd.DataFrame(sc_test, columns = cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing one hot encoder from sklearn \nfrom sklearn.preprocessing import OneHotEncoder \n\n# creating one hot encoder object \nonehotencoder = OneHotEncoder() \n\ntrainDep = train[' Label'].values.reshape(-1,1)\ntrainDep = onehotencoder.fit_transform(trainDep).toarray()\ntestDep = test[' Label'].values.reshape(-1,1)\ntestDep = onehotencoder.fit_transform(testDep).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X=sc_traindf\ntrain_y=trainDep[:,0]\n\ntest_X=sc_testdf\ntest_y=testDep[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Selection\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier();\n\n# fit random forest classifier on the training set\nrfc.fit(train_X, train_y);\n\n# extract important features\nscore = np.round(rfc.feature_importances_,3)\nimportances = pd.DataFrame({'feature':train_X.columns,'importance':score})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\n\n# plot importances\nplt.rcParams['figure.figsize'] = (11, 4)\nimportances.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Recursive feature elimination\nfrom sklearn.feature_selection import RFE\nimport itertools\n\nrfc = RandomForestClassifier()\n\n# create the RFE model and select 10 attributes\nrfe = RFE(rfc, n_features_to_select=20)\nrfe = rfe.fit(train_X, train_y)\n\n# summarize the selection of the attributes\nfeature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), train_X.columns)]\nselected_features = [v for i, v in feature_map if i==True]\n\nselected_features\n\na = [i[0] for i in feature_map]\ntrain_X = train_X.iloc[:,a]\ntest_X = test_X.iloc[:,a]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset Partition\nX_train,X_test,Y_train,Y_test = train_test_split(train_X,train_y,train_size=0.70, random_state=2)\n\n#Fitting Models\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Train KNeighborsClassifier Model\nKNN_Classifier = KNeighborsClassifier(n_jobs=-1)\nKNN_Classifier.fit(X_train, Y_train); \n\n# Train LogisticRegression Model\nLGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\nLGR_Classifier.fit(X_train, Y_train);\n\n# Train Gaussian Naive Baye Model\nBNB_Classifier = BernoulliNB()\nBNB_Classifier.fit(X_train, Y_train)\n\n# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTC_Classifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate Models\nfrom sklearn import metrics\n\nmodels = []\nmodels.append(('Naive Baye Classifier', BNB_Classifier))\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('KNeighborsClassifier', KNN_Classifier))\nmodels.append(('LogisticRegression', LGR_Classifier))\n\nfor i, v in models:\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, v.predict(X_train))\n    confusion_matrix = metrics.confusion_matrix(Y_train, v.predict(X_train))\n    classification = metrics.classification_report(Y_train, v.predict(X_train))\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validate Models\nfor i, v in models:\n    accuracy = metrics.accuracy_score(Y_test, v.predict(X_test))\n    confusion_matrix = metrics.confusion_matrix(Y_test, v.predict(X_test))\n    classification = metrics.classification_report(Y_test, v.predict(X_test))\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PREDICTING FOR TEST DATA\npred_knn = KNN_Classifier.predict(test_X)\npred_NB = BNB_Classifier.predict(test_X)\npred_log = LGR_Classifier.predict(test_X)\npred_dt = DTC_Classifier.predict(test_X)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}