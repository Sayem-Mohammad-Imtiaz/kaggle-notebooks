{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ce projet a pour but de créer un modèle prédisant si un futur client d'une compagnie va y rester fidèle.\n## On se donne pour objectif une précision de 70% et un recall de 60%.","metadata":{}},{"cell_type":"code","source":"#Import de modules\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Chargement des données","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Description du Dataset","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"L'absence de valeurs nulles va grandement nous faciliter la tâche.","metadata":{}},{"cell_type":"code","source":"#Répartition des types de données\ndf.dtypes.value_counts().plot.pie()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Examen de la colonne Target","metadata":{}},{"cell_type":"markdown","source":"On sait que \"Exited\" = 0 signifie qu'un client reste au côté de l'entreprise et donc que \"Exited\" = 1 signifie qu'un client quitte l'entreprise.","metadata":{}},{"cell_type":"code","source":"df['Exited'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Il y a plus de clients restant dans l'entreprise que de clients la quittant.","metadata":{}},{"cell_type":"code","source":"#Exprimé en probabilité\ndf['Exited'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Exited'].value_counts(normalize=True).plot.pie()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les scores ne sont pas équilibrés, il faudra utiliser une mesure d'évalutation comme : le score F1, la sensibilité, la précision.","metadata":{}},{"cell_type":"markdown","source":"## Analyse des variables quantitatives","metadata":{}},{"cell_type":"code","source":"#Histogrammes des float\nfor col in df.select_dtypes('float') :\n    plt.figure()\n    sns.histplot(df[col])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_continues = [col for col in df.select_dtypes('int')][2:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Histogrammes des int\nfor col in int_continues :\n    plt.figure()\n    sns.histplot(df[col])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque quelques outliers pour les variables \"Balance\" et \"CreditScore\".","metadata":{}},{"cell_type":"markdown","source":"## Analyse de nos variables qualitatives","metadata":{}},{"cell_type":"code","source":"#Liste des différentes valeurs que peuvent prendre les variables qualitatives\nfor col in df.select_dtypes('object') :\n    print(f'{col :<10}, {df[col].unique()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"liste_object = [col for col in df.select_dtypes('object')]\nobject_clear = liste_object[1:]\n\nfor col in object_clear :\n    plt.figure()\n    df[col].value_counts().plot.pie()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Répartition sur 3 pays avec 50% de clients Français.","metadata":{}},{"cell_type":"code","source":"int_cat = [col for col in df.select_dtypes('int')][4:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in int_cat :\n    print(f'{col :<15}, {df[col].unique()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in int_cat :\n    plt.figure()\n    df[col].value_counts().plot.pie()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyse de la relation Features / Target","metadata":{}},{"cell_type":"markdown","source":"## Features qualitatives vs target","metadata":{}},{"cell_type":"code","source":"#Répartition des pays et du genre sur la target\nfor col in object_clear :\n    plt.figure()\n    sns.heatmap(pd.crosstab(df['Exited'], df[col]), annot = True, fmt = 'd')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in object_clear :\n    sns.catplot(data=df, x=\"Exited\", hue=col, kind = 'count')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in int_cat :\n    sns.catplot(data=df, x=\"Exited\", hue=col, kind = 'count')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les membres possédant un seul produit, ou étant inactifs sont plus à même de quitter l'entreprise.","metadata":{}},{"cell_type":"markdown","source":"## Features quantitatives vs target","metadata":{}},{"cell_type":"markdown","source":"#### Création de sous-ensemble Exited / non-Exited","metadata":{}},{"cell_type":"code","source":"exited_df = df[df['Exited'] == 1]\nnon_exited_df = df[df['Exited'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_columns = df[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']].columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in continuous_columns :\n    sns.histplot(data= exited_df[col], label = 'Exited', kde = True, color = 'red')\n    sns.histplot(data= non_exited_df[col], label = 'Non-Exited', kde = True, color = 'orange')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les facteurs décisifs semblent être l'âge et le score de crédit.","metadata":{}},{"cell_type":"markdown","source":"# Développement de modèles","metadata":{}},{"cell_type":"markdown","source":"## Features Engineering","metadata":{}},{"cell_type":"markdown","source":"On a remarqué que les variables \"CreditScore\" et \"Balance\" présentent des outliers. Nous allons les supprimer.","metadata":{}},{"cell_type":"code","source":"#On crée une copie propre de notre dataframe\ndfml = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fonction supprimant les outliers et retirant quelques colonnes inutiles\ndef features_engineering(dfml) : \n    dfml = dfml[dfml['CreditScore'] < 850]\n    dfml = dfml[dfml['Balance'] > 0]\n    dfml.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1, inplace = True)\n    return dfml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfml = features_engineering(dfml)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notre dataframe a été correctement formaté.","metadata":{}},{"cell_type":"markdown","source":"## Création du train set et du test set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On crée un test_set de 20% de la taille de notre dataframe initial\ntrain_set, test_set = train_test_split(dfml, test_size = 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set['Exited'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set['Exited'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les deux classes de notre modèle sont correctements réparties sur le train et le test set","metadata":{}},{"cell_type":"code","source":"#Séparation des features et des targets\nX_train = train_set.drop('Exited', axis = 1)\ny_train = train_set['Exited']\nX_test = test_set.drop('Exited', axis = 1)\ny_test = test_set['Exited']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing","metadata":{}},{"cell_type":"code","source":"#importation des différents outils sklearn\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, PolynomialFeatures, OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On sépare les features quantitatives et qualitatives\nnumerical_features = make_column_selector(dtype_include = np.number)\ncategorical_features = make_column_selector(dtype_exclude = np.number)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On normalise les features quantitatives, on encode les features qualitatives\nnumerical_pipeline = make_pipeline(RobustScaler(), SelectKBest(f_classif, k = 5))\ncategorical_pipeline = make_pipeline(OneHotEncoder())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On re-combine le tout dans notre preprocesseur\npreprocessor = make_column_transformer((numerical_pipeline, numerical_features), (categorical_pipeline, categorical_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modellisation","metadata":{}},{"cell_type":"code","source":"#liste des différents modèles que l'on va essayer\nknc = make_pipeline(preprocessor, KNeighborsClassifier())\nlinear_svc = make_pipeline(preprocessor, LinearSVC())\npoly_svc = make_pipeline(preprocessor, SVC(kernel = \"poly\"))\nrbf_svc = make_pipeline(preprocessor, SVC(kernel = \"rbf\"))\nsigmoid_svc = make_pipeline(preprocessor, SVC(kernel = \"sigmoid\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On les places dans un dictionnaire pour pouvoir itérer dessus\ndict_of_models = {'KNC' : knc, 'Linear SVC' : linear_svc, 'Poly SVC' : poly_svc, 'RBF SVC' : rbf_svc, 'Sigmoïd SVC' : sigmoid_svc}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Procédure d'évaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import learning_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fonction d'évaluation, renvoyant matrice de confusion, rapport de  classification et tracé des courbes d'apprentissage entre validation set et \n#train set\ndef evaluation(model) : \n    \n    model.fit(X_train, y_train)\n    ypred = model.predict(X_test)\n    \n    print(confusion_matrix(y_test, ypred))\n    print(classification_report(y_test, ypred))\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train, cv = 4, scoring = 'f1', train_sizes = np.linspace(0.1, 1, 10))\n    \n    plt.figure(figsize = (12, 8))\n    plt.plot(N, train_score.mean(axis = 1), label = 'train score')\n    plt.plot(N, val_score.mean(axis = 1), label = 'validation score')\n    plt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On applique la fonction à nos différents modèles\nfor name, model in dict_of_models.items() :\n    print(name)\n    evaluation(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On retient les modèles de Polynomial et RBF SVC qui semblent procurer les meilleurs scores sans faire d'over ou d'under fitting.","metadata":{}},{"cell_type":"markdown","source":"## Optimisation du RBF SVC","metadata":{}},{"cell_type":"code","source":"#On va effectuer une optimisation par grille de recherche\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Après recherche, on obtient les hyperparamètres suivants :\nhyper_params = {'svc__gamma' : ['auto'], \n                'svc__C' : [225]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On entraine notre meilleur modèle\ngrid = GridSearchCV(rbf_svc, hyper_params, scoring = \"f1\", cv = 4)\n\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\n\ny_pred = grid.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation(grid.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Précédemment, nous obtenions : \n \n Précision : 0.85,       Recall : 0.36,       Score f1 : 0.50\n \n Après optimisiation, on parvient à améliorer le recall :\n \n Précision : 0.85,       Recall : 0.47,       Score f1 : 0.61\n ","metadata":{}},{"cell_type":"markdown","source":"## Precision Recall Curve","metadata":{}},{"cell_type":"markdown","source":"Nous allons désormais essayer de créer une situation de compromis, afin de gagner un peu de recall au détriment de la précision.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision, recall, threshold = precision_recall_curve(y_test, grid.best_estimator_.decision_function(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On affiche la précision et le recall pour différents seuils\nplt.plot(threshold, precision[:-1], label = 'precision')\nplt.plot(threshold, recall[:-1], label = 'recall')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#On crée notre modèle final de prédiction\ndef model_final(model, X, threshold = 0):\n    return model.decision_function(X) > threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Après quelques essais, on place le seuil sur -0.65 pour un compromis optimal\ny_pred = model_final(grid.best_estimator_, X_test, threshold = -0.65)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = 'd')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La heatmap nous permet de visualiser les prédictions de notre modèle.","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### On obtient une précision de 0.72 et un recall de 0.60.\n\n### Notre objectif est atteint !","metadata":{}}]}