{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Automatically Tagging Stack Overflow Questions\n\n## Documentation"},{"metadata":{},"cell_type":"markdown","source":"## Read Data\n\nUse pandas to read csv files and print head"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# Read CSV files to get questions and tags\ndf_questions = pd.read_csv(\"../input/Questions.csv\", encoding=\"ISO-8859-1\")\ndf_tags = pd.read_csv(\"../input/Tags.csv\", encoding=\"ISO-8859-1\", dtype={'Tag': str})\n\n\ndf_questions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tags.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process tags\nProcess them tags into something nice to query"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group tags by id and join them\ndf_tags['Tag'] = df_tags['Tag'].astype(str)\ngrouped_tags = df_tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\ngrouped_tags.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset index for making simpler dataframe\ngrouped_tags.reset_index()\ngrouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags.values})\ngrouped_tags_final.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unnecessary columns\ndf_questions.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)\n\n# Merge questions and tags into one dataframe\ndf = df_questions.merge(grouped_tags_final, on='Id')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\n# Filter out questions with a score lower than 5\nnew_df = df[df['Score']>5]\n\n# Split tags in order to get a list of tags\nnew_df['Tags'] = new_df['Tags'].apply(lambda x: x.split())\nall_tags = [item for sublist in new_df['Tags'].values for item in sublist]\n\nflat_list = [item for sublist in new_df['Tags'].values for item in sublist]\n\nkeywords = nltk.FreqDist(flat_list)\nkeywords = nltk.FreqDist(keywords)\n\n# Get most frequent tags\nfrequencies_words = keywords.most_common(25)\ntags_features = [word[0] for word in frequencies_words]\n# Drop unnecessary columns at this point\nnew_df.drop(columns=['Id', 'Score'], inplace=True)\nprint(tags_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_common(tags):\n    \"\"\"Function to check if tag is in most common tag list\"\"\"\n    tags_filtered = []\n    for i in range(0, len(tags)):\n        if tags[i] in tags_features:\n            tags_filtered.append(tags[i])\n    return tags_filtered\n\n# Change Tags column into None for questions that don't have a most common tag\nnew_df['Tags'] = new_df['Tags'].apply(lambda x: most_common(x))\nnew_df['Tags'] = new_df['Tags'].apply(lambda x: x if len(x)>0 else None)\n\n# Drop rows that contain None in Tags column\nnew_df.dropna(subset=['Tags'], inplace=True)\nnew_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess Data\n- Remove special characters from title and body\n- Remove stop words\n- Remove HTML tags\n- Convert characters to lowercase\n- Lemmatize the words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport lxml\nimport re\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Filter out HTML\nnew_df['Body'] = new_df['Body'].apply(lambda x: BeautifulSoup(x, \"lxml\").get_text()) \n\ntoken = ToktokTokenizer()\nlemma = WordNetLemmatizer()\nstop_words = set(stopwords.words(\"english\"))\n\ndef strip_list_noempty(mylist):\n    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n    return [item for item in newlist if item != '']\n\ndef removeStopWords(text):\n    words = token.tokenize(text)\n    filtered = [w for w in words if not w in stop_words]\n    return ' '.join(map(str, filtered))\n\ndef removePunctuation(text):\n    punct = '!\"$%&\\'()*,./:;<=>?@[\\\\]^_`{|}~'\n    words=token.tokenize(text)\n    punctuation_filtered = []\n    regex = re.compile('[%s]' % re.escape(punct))\n    remove_punctuation = str.maketrans(' ', ' ', punct)\n    for w in words:\n        if w in tags_features:\n            punctuation_filtered.append(w)\n        else:\n            punctuation_filtered.append(regex.sub('', w))\n  \n    filtered_list = strip_list_noempty(punctuation_filtered)\n        \n    return ' '.join(map(str, filtered_list))\n\ndef lemmatizeWords(text):\n    words=token.tokenize(text)\n    listLemma=[]\n    for w in words:\n        x=lemma.lemmatize(w, pos=\"v\")\n        listLemma.append(x.lower())\n    return ' '.join(map(str, listLemma))\n\n\n# Remove stopwords, punctuation and lemmatize for text in body\nnew_df['Body'] = new_df['Body'].apply(lambda x: removeStopWords(x))\nnew_df['Body'] = new_df['Body'].apply(lambda x: removePunctuation(x))\nnew_df['Body'] = new_df['Body'].apply(lambda x: lemmatizeWords(x))\n\n# Remove stopwords, punctuation and lemmatize for title. Also weight title 3 times\nnew_df['Title'] = new_df['Title'].apply(lambda x: str(x)) \nnew_df['Title'] = new_df['Title'].apply(lambda x: removePunctuation(x)) \nnew_df['Title'] = new_df['Title'].apply(lambda x: removeStopWords(x)) \nnew_df['Title'] = new_df['Title'].apply(lambda x: lemmatizeWords(x)) \nnew_df['Title'] = new_df['Title'].apply(lambda x: ' '.join(x.split()*3))\nnew_df['Title']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classifier implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\n\n# Define X, y\nX1 = new_df['Body']\nX2 = new_df['Title']\ny = new_df['Tags']\nprint(len(X1), len(X2), len(y))\n\n# Define multilabel binarizer\nmultilabel_binarizer = MultiLabelBinarizer()\ny_bin = multilabel_binarizer.fit_transform(y)\n\n\nvectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0005,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       ngram_range = (1, 3),\n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\",\n                                       max_features=35000)\n\nvectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       ngram_range = (1, 3),\n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\",\n                                       max_features=35000)\n\nX1_tfidf = vectorizer_X1.fit_transform(X1)\nX2_tfidf = vectorizer_X2.fit_transform(X2)\n\n# Stack X1 and X2 into X_tfidf\nX_tfidf = hstack([X1_tfidf,X2_tfidf])\n\n# Split training and test data    \nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Label Powerset\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.metrics import f1_score\nfrom skmultilearn.problem_transform import LabelPowerset\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import jaccard_similarity_score\nfrom sklearn import model_selection\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\nsvc = LinearSVC()\nsgd = SGDClassifier(n_jobs=-1)\n\ndef print_score(y_pred, clf):\n    print(\"Clf: \", clf.__class__.__name__)\n    print(\"Accuracy score: {}\".format(accuracy_score(y_test, y_pred)))\n    print(\"Recall score: {}\".format(recall_score(y_true=y_test, y_pred=y_pred, average='weighted')))\n    print(\"Precision score: {}\".format(precision_score(y_true=y_test, y_pred=y_pred, average='weighted')))\n    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n    print(\"F1 score: {}\".format(f1_score(y_pred, y_test, average='weighted')))\n    print(\"---\")    \n\nclf = LabelPowerset(svc)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint_score(y_pred, clf)\n\nkfold = KFold(n_splits=5)\nX_sparse = X_tfidf.tocsr()\n\nscores = []\n\nfor train_indices, test_indices in kfold.split(X_sparse, y_bin):\n    clf.fit(X_sparse[train_indices], y_bin[train_indices])\n    print(clf.score(X_sparse[test_indices], y_bin[test_indices]))\n    scores.append(clf.score(X_sparse[test_indices], y_bin[test_indices]))\n\nprint(sum(scores)/len(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Classifier Chains\nfrom sklearn.multioutput import ClassifierChain\nimport numpy as np\n\n\nchains = [ClassifierChain(svc, order='random', random_state=i)\n          for i in range(10)]\n\nfor chain in chains:\n    chain.fit(X_train, y_train)\n\nY_pred_chains = np.array([chain.predict(X_test) for chain in\n                          chains])\n\nY_pred_ensemble = Y_pred_chains.mean(axis=0)\nensemble_accuracy_score = accuracy_score(y_test, Y_pred_ensemble >= .5)\nensemble_recall_score = recall_score(y_test, Y_pred_ensemble >= .5, average='weighted')\nensemble_precision_score = precision_score(y_test, Y_pred_ensemble >= .5, average='weighted')\nensemble_f1_score = f1_score(y_pred, Y_pred_ensemble >= .5, average='weighted')\nhamm = hamming_loss(Y_pred_ensemble >= .5, y_test)*100\nprint(ensemble_accuracy_score, ensemble_recall_score, ensemble_precision_score, ensemble_f1_score, hamm)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Binary Relevance\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\n\n# initialize binary relevance multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = BinaryRelevance(svc)\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\nprint_score(predictions, classifier)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}