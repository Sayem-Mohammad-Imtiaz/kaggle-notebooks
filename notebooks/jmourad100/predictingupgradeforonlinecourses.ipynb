{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\n\nimport numpy as np\nimport pylab as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/coursesdata/\"\nstudentInfo = pd.read_csv(PATH + 'studentInfo.csv')\ncourses = pd.read_csv(PATH + 'courses.csv')\nassessments = pd.read_csv(PATH + 'assessments.csv')\nstudentAssessment = pd.read_csv(PATH + 'studentAssessment.csv')\nstudentReview = pd.read_csv(PATH + 'studentReview.csv')\n\nstudentInfo.head()\n# courses.head()\n# assessments.head()\n# studentAssessment.head()\n# studentReview.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging Tables\nresult = pd.merge(studentInfo, courses, left_on=('course','run'), right_on=('course','run'),how='left', sort=False);\nresult = pd.merge(result, assessments, left_on=('course','run'), right_on=('course','run'),how='left', sort=False);\nresult = pd.merge(result, studentAssessment, left_on=('student_id','assessment_id'), right_on=('student_id','assessment_id'),how='left', sort=False);\nresult = pd.merge(result, studentReview, left_on=('student_id','course'), right_on=('student_id','course'),how='left', sort=False);\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reorder Columns\nresult = result[['student_id','course', 'run',  'gender', 'region', 'highest_education_level', 'age_range', 'completed', \n                 'date_enrolled', 'date_unenrolled', 'course_length', 'assessment_id','assessment_type', 'date', 'weight',\n                 'date_submitted', 'score', 'student_review','upgraded']]\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping by (student, Course and run), so we can predict for each (user, couurse, run) the upgraded value\n\nresult.groupby(['student_id', 'course','run']).agg({\n    'gender': lambda x: x[0],\n    'region': lambda x: x[0],\n    'highest_education_level': lambda x: x[0],\n    'age_range': lambda x: x[0],\n    'completed': lambda x: x[0],\n    'date_enrolled': lambda x: x[0],\n    'date_unenrolled': lambda x: x[0],\n    'course_length': lambda x: x[0],\n    'assessment_id': 'count',\n    'assessment_type': lambda x: x[0],\n    'date': lambda x: x[0],\n    'weight': lambda x: x[0],\n    'date_submitted': lambda x: x[0],\n    'score': lambda x: x[0],\n    'student_review': lambda x: x[0],\n    'upgraded' :lambda x: x[0]\n})\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new User_Course_Run identifier\nresult['ID'] = result['student_id'].map(str) + '_' + result['course'] + '_' + result['run']\n\n# Making User_Course_Run the first in the dataframe, and removing [student_id, course, run]\nresult['student_id'] = result['ID']\nresult.rename(columns={'student_id': 'Student_course_Run_id'}, inplace=True)\nresult.drop(['course', 'run', 'ID'], axis=1, inplace=True)\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turning non numeric values into numbers using labelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Lebel encoding Target column\nleup = LabelEncoder()\nleup.fit(result.upgraded)\nresult.upgraded=leup.transform(result.upgraded)\n\ncat_cols = ['gender','region','highest_education_level','age_range','completed','date_enrolled','assessment_type']\nfor col in cat_cols:\n    if col in result.columns:\n        le = LabelEncoder()\n        le.fit(list(result[col].astype(str).values))\n        result[col] = le.transform(list(result[col].astype(str).values))\n        \nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Target (Most studnets don't upgrade)\nimport seaborn as sns\n\nsns.countplot(x='upgraded', data=result);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running this command, we can see that some columns have missing values\nresult.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the distribution of each column\nresult.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Destribution of columns ['date_enrolled', 'course_length', 'date', 'weight', 'score']\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,6))\nboxplot = result.boxplot(column=['date_enrolled', 'course_length', 'date', 'weight', 'score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missig values (We will use the mean to impute the missing values)\nresult.score = result.score.fillna(result.score.mean())\n\nresult['date_submitted'] = result['date_submitted'].fillna(result['date_submitted'].mean())\nresult['date_unenrolled'] = result['date_unenrolled'].fillna(result['date_unenrolled'].mean())\nresult['date'] = result['date'].fillna(result['date'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Baseline Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the baseline model, we will use just the numeric columns.\n# In order to not lose the review effect, We will create a \"student_review_len\"\n# column before removing the \"student_review\" column.\n\ndef add_review_features(df):\n    df['student_review'] = df['student_review'].apply(lambda x:str(x))\n    df['student_review_len'] = df['student_review'].apply(len)\n    df['student_review_n_capitals'] = df['student_review'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['student_review_n_words'] = df['student_review'].str.count('\\S+')\n    return df\n\nresult = add_review_features(result)\n# Removing unique identifiers + studnt review\ndata = result.drop(['assessment_id','student_review'],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into 80% training and 20% test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nX = data.drop(['Student_course_Run_id', 'upgraded'],axis=1)\ny = data.upgraded\n\n# Standardize features by removing the mean and deviding by variance\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\n\nskf = StratifiedKFold(n_splits=5)\nskf.get_n_splits(X, y)\n\n# Accuracies and F-Scores across k folds\naccs, fsc = [], []\n\nprint(skf)\nStratifiedKFold(n_splits=5, random_state=10, shuffle=False)\nfor train_index, test_index in skf.split(X, y):\n    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create Model\n    clf =  RandomForestClassifier(n_estimators=10, random_state=10)\n    # Train Decision Tree Classifer\n    clf = clf.fit(X_train,y_train)\n    # Predict the response for test dataset\n    y_pred = clf.predict(X_test)\n    \n    # Evaluate performance\n    print(\"Fold Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n    print(\"Fold F1-Score:\",metrics.f1_score(y_test, y_pred), end='\\n\\n')\n    accs.append(metrics.accuracy_score(y_test, y_pred))\n    fsc.append(metrics.f1_score(y_test, y_pred))\n    \nprint(\"Overall Accuracy: {:0.2f} +/- {:0.2f}\".format(np.mean(accs), np.std(accs)))\nprint(\"Overall F1-Score: {:0.2f} +/- {:0.2f}\".format(np.mean(fsc), np.std(fsc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCrouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we will plot WordCrouds for the two classes (upgrade) and (Not upgrade)\n# We can see that words like \"Great\" are indicators for the decision of the student\n\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\nn_posts = 1000\ndata = result\nrev_Up = ' '.join(data[data['upgraded'] == 0]['student_review'].str.lower().values[:n_posts])\nrev_Nup = ' '.join(data[data['upgraded'] == 1]['student_review'].str.lower().values[:n_posts])\n\nwordcloud_S = WordCloud(max_words=20, scale = 2, stopwords=stop, contour_width=3, contour_color='steelblue').generate(rev_Up)\nwordcloud_I = WordCloud(max_words=20, scale = 2, stopwords=stop, contour_width=3, contour_color='steelblue').generate(rev_Nup)\n\nfig, ax = plt.subplots(1,2, figsize=(22, 6))\nax[0].imshow(wordcloud_S)\nax[0].set_title('Top words studnet review (Not upgrade)',fontsize = 20)\nax[0].axis(\"off\")\n\nax[1].imshow(wordcloud_I)\nax[1].set_title('Top words studnet review (upgrade)',fontsize = 20)\nax[1].axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Creating model using student Review (Bert Large)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\nimport os, re, pickle\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# pytorch bert imports\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\n# keras imports\nfrom keras.utils import np_utils\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import CuDNNLSTM, LSTM, Activation, Dense, Dropout, Input, Embedding, concatenate, Bidirectional\nfrom keras.layers import SpatialDropout1D, Dropout, add, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_PRETRAINED_DIR = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/'\nBERT_VOCAB_DIR = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt'\nMAX_LENGTH = 50 # Because review_len_mean is near 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nlp_preprocessing(text):\n    filter_char = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n    text = text.lower()\n    text = text.replace(filter_char,'')\n    text = text.replace('[^a-zA-Z0-9 ]', '')\n    return text\n\nresult[\"student_review\"] = result[\"student_review\"].apply(nlp_preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising BERT tokenizer\ntokenizer = BertTokenizer(vocab_file=BERT_VOCAB_DIR)\ndef tokenization(row):\n    row = tokenizer.tokenize(row)\n    row = tokenizer.convert_tokens_to_ids(row)\n    return row\n\nresult[\"student_review\"] = result[\"student_review\"].apply(tokenization)\n\n# Cheking some review after tokenization\nresult[\"student_review\"].sample(20).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def string_ids(doc):\n    doc = [str(i) for i in doc]\n    return ' '.join(doc)\n\nresult[\"student_review\"] = result[\"student_review\"].apply(string_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_text = np.zeros((result.shape[0],MAX_LENGTH),dtype=np.int)\nX_num  = X_scaled #Numerical features\nfor i,ids in enumerate(list(result['student_review'])):\n    input_ids = [int(i) for i in ids.split()[:MAX_LENGTH]]\n    inp_len = len(input_ids)\n    X_text[i,:inp_len] = np.array(input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bert_embed_matrix():\n    bert = BertModel.from_pretrained(BERT_PRETRAINED_DIR)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat\n\nembedding_matrix = get_bert_embed_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_UNITS = 128\nHIDDEN_UNITS = 4 * LSTM_UNITS\nN_NUMERICAL  = X_scaled.shape[-1]\n\ndef build_model(embedding_matrix):\n    \n    words = Input(shape=(MAX_LENGTH,))\n    numerics = Input(shape=(N_NUMERICAL,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.5)(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n    \n    hidden = concatenate([hidden, numerics])\n    out = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[words, numerics], outputs=out)\n    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(lr = 0.001))\n\n    return model\n\n# Checking Model Architecture\nbuild_model(embedding_matrix).summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_idx, val_idx = train_test_split(list(range(len(X_text))) ,test_size=0.2, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 5\n\n# Model Training and prediction phase\nmodel = build_model(embedding_matrix)\n\nmodel.fit(\n    [X_text[tr_idx], X_num[tr_idx]], y[tr_idx],\n    validation_data = ([X_text[val_idx], X_num[val_idx]], y[val_idx]),\n    batch_size = 500,\n    epochs = EPOCHS,\n    verbose = 1,\n    callbacks=[LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** epoch))]\n)\nmodel_val_preds = model.predict([X_text[val_idx], X_num[val_idx]], batch_size=1000).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n# Convert predictions to int, so we can compute metrics\ny_val = (np.array(model_val_preds) > 0.5).astype(np.int)\n\nprint(\"Accuracy: {:0.2f}\".format(accuracy_score(y[val_idx], y_val)))\nprint(\"F1-Score: {:0.2f}\".format(f1_score(y[val_idx], y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Conclusion:\nWe can see that by adding student reviews embeddings, we were able to get some inprovement in F1-score\nIn This case we have an umbalenced dataset, so even if we have got higher accuracy for both the first model (without reviews)\nand the second model (with reviews embeddings), the F1-score is the more important metric to consider in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}