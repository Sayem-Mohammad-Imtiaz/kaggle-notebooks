{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import requests # send web-requests (to fetch the html files from websites)\nfrom bs4 import BeautifulSoup # beautiful-soup: navigate through the html\nimport numpy as np\nimport pandas as pd\nimport os, re, pprint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Configuration\nurl = \"http://www.covidmaroc.ma/pages/Accueil.aspx\"\n\nheaders = {\n  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n}\n\nData = {\n  \"Date\"      : \"\",\n  \"Confirmed\" : \"\",\n  \"Deaths\"    : \"\",\n  \"Recovered\" : \"\",\n  \"Excluded\"  : \"\",\n  \n  \"Beni Mellal-Khenifra\"     : \"\",\n  \"Casablanca-Settat\"        : \"\",\n  \"Draa-Tafilalet\"           : \"\",\n  \"Dakhla-Oued Ed-Dahab\"     : \"\",\n  \"Fes-Meknes\"               : \"\", \n  \"Guelmim-Oued Noun\"        : \"\",\n  \"Laayoune-Sakia El Hamra\"  : \"\",\n  \"Marrakesh-Safi\"           : \"\",\n  \"Oriental\"                 : \"\",\n  \"Rabat-Sale-Kenitra\"       : \"\",\n  \"Souss-Massa\"              : \"\",\n  \"Tanger-Tetouan-Al Hoceima\": \"\",\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Helper functions\ndef getDate(text):\n    tag = text.find_all('p')[0]\n    \n    # Remove chars other than numbers and -\n    tag = re.sub(r'[^0-9\\-]', '', str(tag))\n    return re.search(\"([0-9]{2}\\-[0-9]{2}\\-[0-9]{4})\", tag).group(0)\n\ndef getRecovered_Deaths(text):\n    tag = text.find_all('p')[1]\n    tag_Recov = tag.find_all('span')[0].get_text()\n    tag_death = tag.find_all('span')[1].get_text()\n    \n    # Remove unicode characters\n    tag_death = tag_death.encode('ascii', 'ignore').decode(\"utf-8\")\n    tag_Recov = tag_Recov.encode('ascii', 'ignore').decode(\"utf-8\")\n    return (int(tag_Recov), int(tag_death))\n\ndef getConfirmed_Excluded(text):\n    tagConf = text.find_all('p')[3].get_text()\n    tagExcl = text.find_all('p')[4].get_text()\n    \n    # Remove unicode characters\n    tagConf = tagConf.encode('ascii', 'ignore').decode(\"utf-8\")\n    tagExcl = tagExcl.encode('ascii', 'ignore').decode(\"utf-8\")\n    return (int(tagConf), int(tagExcl))\n\ndef percent_to_actuale_number(percentage):\n    return np.ceil(Data['Confirmed']*percentage/100)\n\ndef get_region_cases(text):\n    headers = text.find_all('h2')\n    \n    vals = []\n    for i in range(1, len(headers)):\n        try:\n            val = headers[i].get_text()\n            # Remove unicode characters\n            val = val.encode('ascii', 'ignore').decode(\"utf-8\")\n            val = val.split(' ')[0]\n            \n            # Select values that start with numbers\n            if not val[0].isdigit():\n                continue\n            \n            val = float(val.replace('%','').replace(',','.'))\n            vals.append(percent_to_actuale_number(val))\n        except:\n            pass\n    return vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Main function\ndef get_covid_Data(url):\n\n    try:\n        result = requests.get(url, headers=headers)\n    except HTTPError as e:\n        print(\"error in opening url\")\n        return None\n  \n    try:\n        bsObj = BeautifulSoup(result.content.decode(\"utf-8\"), \"html.parser\")\n\n        table1 =  bsObj.find_all('table')[0] # Grab the first table\n#         table2 =  bsObj.find_all('table')[1] # Grab the second table\n\n        Data[\"Date\"] = getDate(table1)\n        Data[\"Recovered\"], Data[\"Deaths\"] = getRecovered_Deaths(table1)\n        Data[\"Confirmed\"], Data[\"Excluded\"] = getConfirmed_Excluded(table1)\n\n#         CasesByRegion = get_region_cases(table2)\n#         Data[\"Beni Mellal-Khenifra\"]      = CasesByRegion[0]\n#         Data[\"Casablanca-Settat\"]         = CasesByRegion[1]\n#         Data[\"Draa-Tafilalet\" ]           = CasesByRegion[2]\n#         Data[\"Dakhla-Oued Ed-Dahab\"]      = CasesByRegion[3]\n#         Data[\"Fes-Meknes\"]                = CasesByRegion[4]\n#         Data[\"Guelmim-Oued Noun\"]         = CasesByRegion[5]\n#         Data[\"Laayoune-Sakia El Hamra\"]   = CasesByRegion[6]\n#         Data[\"Marrakesh-Safi\"]            = CasesByRegion[7]\n#         Data[\"Oriental\"]                  = CasesByRegion[8]\n#         Data[\"Rabat-Sale-Kenitra\"]        = CasesByRegion[9]\n#         Data[\"Souss-Massa\"]               = CasesByRegion[10]\n#         Data[\"Tanger-Tetouan-Al Hoceima\"] = CasesByRegion[11]\n\n        pprint.pprint(Data)\n\n    except AttributeError as e:\n        print(\"error in reading html\")\n        return None\n\n\nget_covid_Data(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(Data.items(), columns=['__', '__']).T\n# Replace Header with first row\ndf.columns = df.iloc[0]\ndf = df[1:]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Next step is to run this code every day and add add records to the Full Dataset\n## Now detailed data is shared every day in PDF File. So Next step is to Create another script to scrape data from pdf files.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}