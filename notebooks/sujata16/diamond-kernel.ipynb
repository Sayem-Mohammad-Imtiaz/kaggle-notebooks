{"cells":[{"metadata":{"_uuid":"383b40ec-6bc4-416b-b9f9-e9ac4e36d567","_cell_guid":"f39bda4d-b094-4938-bea4-73dab739c234","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n #       print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ndf  = pd.read_csv('/kaggle/input/diamonds/diamonds.csv')\nprint(list(df.columns))\nprint(df.head(3))\nplt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu')\n#sns.pairplot(df[df.columns], height=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature engineering -- manual \nimport collections\ndiamond_cut = {'Fair':0,\n               'Good':1,\n               'Very Good':2, \n               'Premium':3,\n               'Ideal':4}\n\ndiamond_color = {'J':0,\n                 'I':1, \n                 'H':2,\n                 'G':3,\n                 'F':4,\n                 'E':5,\n                 'D':6}\n\ndiamond_clarity = {'I1':0,\n                   'SI2':1,\n                   'SI1':2,\n                   'VS2':3,\n                   'VS1':4,\n                   'VVS2':5,\n                   'VVS1':6,\n                   'IF':7}\n#\ndf.cut  = df.cut.map(diamond_cut)\ndf.color = df.color.map(diamond_color)\ndf.clarity = df.clarity.map(diamond_clarity)\nprint(df.head())\nplt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Highly correlated columns are -- \n1. Price\n2. carot\n3. length (x)\n4. breath (y)\n5. height (z)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the distribution of various columns now. \n# Knowing distribution helps in the modeling\nplt.figure(figsize=(10,7))\nsns.distplot(df.carat)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.distplot(df.price)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.distplot(df.x)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.distplot(df.y)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.distplot(df.z)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are not treating the non-Gaussian properties of these columns. This can be done later."},{"metadata":{},"cell_type":"markdown","source":"Let's evaluate some of the common models on the Diamond data. \nBefire fitting the models, we must standardize or normalize the input data set otherwise non-normalizing can lead to the bad model fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge \nfrom sklearn.model_selection import train_test_split\n#\nX = df.drop(['price'],1)\ny = df['price']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n#\n# scaling the entire data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\nprint(X_train)\n#\nprint(\"Linear Regression\")\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(lr.score(X_test, y_test))\n# \nprint(\"Ridge Regression\")\nrr = Ridge()\nrr.fit(X_train, y_train)\nprint(rr.score(X_test, y_test))\n#\nprint(\"Ridge Regression normalized\")\nrr_norm = Ridge(normalize=True)\nrr_norm.fit(X_train, y_train)\nprint(rr_norm.score(X_test, y_test))\n#\nprint(\"testing the gradient boosting model\")\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor()\ngb.fit(X_train, y_train)\nprint(gb.score(X_test, y_test))","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}