{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport random\nimport os\nfrom os import path\nfrom PIL import Image\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Set Plot Theme\nsns.set_palette([\n    \"#30a2da\",\n    \"#fc4f30\",\n    \"#e5ae38\",\n    \"#6d904f\",\n    \"#8b8b8b\",\n])\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\n\n# Modeling\nimport statsmodels.api as sm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.util import ngrams\nfrom collections import Counter\nfrom gensim.models import word2vec\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense , Dropout, Bidirectional,SpatialDropout1D,Flatten\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\",index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(df.isnull(),cmap='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataframe Dimension: {} Rows, {} Columns\".format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"Title\", \"Division Name\",\"Department Name\",\"Class Name\"]].describe(include=[\"O\"]).T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Recommended IND'])\nplt.title(\"Count of recommended vs non recommended items\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continous Distributions\nfig = plt.figure(figsize=(20,14))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.distplot(df[\"Positive Feedback Count\"])\nax1 = plt.title(\"Positive Feedback Count Distribution\")\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.distplot(df['Age'])\nax2 = plt.title(\"Age distribution\")\n\nax3 = plt.subplot2grid((2,2),(1,0),colspan=2)\nax3 = sns.distplot(np.log10((df[\"Positive Feedback Count\"][df[\"Positive Feedback Count\"].notnull()]+1)))\nax3 = plt.title(\"Log Positive Feedback count\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentage_accumulation(series, percentage):\n    return (series.sort_values(ascending=False)\n            [:round(series.shape[0]*(percentage/100))]\n     .sum()/series\n     .sum()*100)\n\n# Gini Coefficient- Inequality Score\n# Source: https://planspace.org/2013/06/21/how-to-calculate-gini-coefficient-from-raw-data-in-python/\ndef gini(list_of_values):\n    sorted_list = sorted(list_of_values)\n    height, area = 0, 0\n    for value in sorted_list:\n        height += value\n        area += height - value / 2.\n    fair_area = height * len(list_of_values) / 2.\n    return (fair_area - area) / fair_area\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inequality = []\nfor x in list(range(100)):\n    inequality.append(percentage_accumulation(df[\"Positive Feedback Count\"], x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(inequality)\nplt.title(\"Percentage of Positive Feedback by Percentage of Reviews\")\nplt.xlabel(\"Review Percentile starting with Feedback\")\nplt.ylabel(\"Percent of Positive Feedback Received\")\nplt.axvline(x=20, c = \"r\")\nplt.axvline(x=53, c = \"g\")\nplt.axhline(y=78, c = \"y\")\nplt.axhline(y=100, c = \"b\", alpha=.3)\nplt.show()\nprint(\"{}% of Positive Feedback belongs to the top 20% of Reviews\".format(\n    round(percentage_accumulation(df[\"Positive Feedback Count\"], 20))))\n\n# Gini\nprint(\"\\nGini Coefficient: {}\".format(round(gini(df[\"Positive Feedback Count\"]),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Division Name'],df['Department Name']),cmap='Purples',annot=True, linewidths=.5,fmt='g',\n                cbar_kws={'label': 'Count'})\nax1 = plt.title('Division Name Count by Department Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Division Name'],df['Department Name'],normalize=True).mul(100).round(0),cmap='Purples',annot=True,fmt='g',cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Division Name Count by Department Name - Crosstab\\nHeatmap Overall Percentage Distribution\")\nplt.tight_layout(pad=0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"], normalize='columns').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"Purples\",cbar_kws={'label': 'Percentage %'})\nax1 = plt.title('Division Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Columns')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"], normalize='index').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"Purples\",cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Division Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Index\")\nplt.tight_layout(pad=0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'],df['Department Name']),cmap='inferno_r',annot=True, linewidths=.5,fmt='g',\n                cbar_kws={'label': 'Count'})\nax1 = plt.title('Class Name Count by Department Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'],df['Department Name'],normalize=True).mul(100).round(0),cmap='inferno_r',annot=True,fmt='g',cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Department Name - Crosstab\\nHeatmap Overall Percentage Distribution\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"], normalize='columns').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"nipy_spectral_r\",cbar_kws={'label': 'Percentage %'})\nax1 = plt.title('Class Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Columns')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"], normalize='index').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"nipy_spectral_r\",cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Index\")\nplt.tight_layout(pad=0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'],df['Division Name']),cmap='cubehelix_r',annot=True, linewidths=.5,fmt='g',\n                cbar_kws={'label': 'Count'})\nax1 = plt.title('Class Name Count by Division Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'],df['Division Name'],normalize=True).mul(100).round(0),cmap='cubehelix_r',annot=True,fmt='g',cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Division Name - Crosstab\\nHeatmap Overall Percentage Distribution\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"], normalize='columns').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"ocean_r\",cbar_kws={'label': 'Percentage %'})\nax1 = plt.title('Class Name Count by Division Name - Crosstab\\nHeatmap % Distribution by Columns')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"], normalize='index').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"ocean_r\",cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Division Name - Crosstab\\nHeatmap % Distribution by Index\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minmaxscaler(df):\n    return (df-df.min())/(df.max()-df.min())\ndef zscorenomalize(df):\n    return (df - df.mean())/df.std()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11,4)})\npd.isnull(df).sum().plot(kind='bar',color='royalblue')\nplt.ylabel('count')\nplt.title('Missing values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(16,6)})\nplt.hist(df['Age'],bins=50,color='royalblue')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.title(\"Age Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(14,6)})\nsns.boxplot(x='Rating',y='Age',data=df)\nplt.title('Rating Distribution per Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Rating'])\nplt.title(\"Rating count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,14))\nax1 = plt.subplot2grid((2,2),(0,0))\n# ax1 = plt.xticks(rotation=90)\nax1 = sns.countplot(df['Division Name'])\nax1 = plt.title(\"Review in each division\")\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = plt.xticks(rotation=90)\nax2 = sns.countplot(df['Department Name'])\nax2 = plt.title(\"Review in each department\")\n\nax3 = plt.subplot2grid((2,2),(1,0),colspan=2)\nax3 = plt.xticks(rotation=90)\nax3 = sns.countplot(df['Class Name'])\nax3 = plt.title(\"Reviews in each Class\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommended = df[df['Recommended IND']==1]\nnot_recommended = df[df['Recommended IND']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommended","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 14))\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = sns.countplot(recommended['Division Name'],color='red',alpha=1,label=\"recommended\")\nax1 = sns.countplot(not_recommended['Division Name'],color='black',alpha=1,label=\"non-recommended\")\nax1 = plt.title(\"Recommended Items in each Division\")\nax1 = plt.legend(loc='best')\n\nax2 = plt.subplot2grid((2, 2), (0, 1))\nax2 = sns.countplot(recommended['Department Name'],color='blue',alpha=1,label=\"recommended\")\nax2 = sns.countplot(not_recommended['Department Name'],color='black',alpha=1,label=\"non-recommended\")\nax2 = plt.title(\"Recommended Items in each Department\")\nax2 = plt.legend(loc='best')\n\nax3 = plt.subplot2grid((2, 2), (1, 0),colspan=2)\nax3 = sns.countplot(recommended['Class Name'],color='orange',alpha=1,label=\"recommended\")\nax3 = sns.countplot(not_recommended['Class Name'],color='black',alpha=1,label=\"non-recommended\")\nax3 = plt.title(\"Recommended Items for each class\")\nax3 = plt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UNSTACKED and Percentage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentstandardize_barplot(x,y,hue, data, ax=None, order= None):\n    sns.barplot(x= x, y=y, hue=hue, ax=ax, order=order,\n    data=(data[[x, hue]]\n     .reset_index(drop=True)\n     .groupby([x])[hue]\n     .value_counts(normalize=True)\n     .rename('Percentage').mul(100)\n     .reset_index()\n     .sort_values(hue)))\n    plt.title(\"Percentage Frequency of {} by {}\".format(hue,x))\n    plt.ylabel(\"Percentage %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hue = \"Recommended IND\"\nfig = plt.figure(figsize=(20, 14))\n\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = percentstandardize_barplot(x=\"Department Name\",y=\"Percentage\",hue=hue,data=df)\nax1 = plt.title(\"Recommended Items in each Department\")\nax1 = plt.legend(loc='best')\n\nax2 = plt.subplot2grid((2, 2), (0, 1))\nax2 = percentstandardize_barplot(x=\"Division Name\",y=\"Percentage\", hue=hue,data=df)\nax2 = plt.title(\"Recommended Items in each Division\")\nax2 = plt.legend(loc='best')\n\nax3 = plt.subplot2grid((2, 2), (1, 0),colspan=2)\nax3 = percentstandardize_barplot(x=\"Class Name\",y=\"Percentage\", hue=hue,data=df)\nax3 = plt.title(\"Recommended Items for each class\")\nax3 = plt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xvar = [\"Department Name\",\"Division Name\",\"Class Name\"]\nhue = \"Rating\"\nfig = plt.figure(figsize=(20, 14))\n\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = percentstandardize_barplot(x=xvar[0],y=\"Percentage\", hue=hue,data=df)\nax1 = plt.title(\"Percentage Frequency of {}\\nby {}\".format(hue, xvar[0]))\nax1 = plt.ylabel(\"Percentage %\")\n\nax2 = plt.subplot2grid((2, 2), (0,1))\nax2 = percentstandardize_barplot(x=xvar[1],y=\"Percentage\", hue=\"Rating\",data=df)\nax2 = plt.title(\"Percentage Frequency of {}\\nby {}\".format(hue, xvar[1]))\nax2 = plt.ylabel(\"Percentage %\")\n\nax3 = plt.subplot2grid((2, 2), (1,0),colspan=2)\nax2 = plt.xticks(rotation=45)\nax3 = percentstandardize_barplot(x=xvar[2],y=\"Percentage\", hue=\"Rating\",data=df)\nax3 = plt.title(\"Percentage Frequency of {}\\nby {}\".format(hue, xvar[2]))\nax3 = plt.ylabel(\"Percentage %\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hue = \"Rating\"\nfig = plt.figure(figsize=(20, 14))\n\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = sns.countplot(x=\"Rating\", hue=\"Recommended IND\",data=df)\nax1 = plt.title(\"Occurrence of {}\\nby {}\".format(hue, \"Recommended IND\"))\nax1 = plt.ylabel(\"Count\")\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = percentstandardize_barplot(x=\"Rating\",y=\"Percentage\", hue=\"Recommended IND\",data=df)\nax2 = plt.title(\"Percentage Normalized Occurrence of {}\\nby {}\".format(hue, \"Recommended IND\"))\nax1 = plt.ylabel(\"% Percentage by Rating\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,8))\nplt.xticks(rotation=90)\nplt.xlabel('Clothing ID')\nplt.ylabel(\"Popularity\")\nplt.title(\"ID of Top 50 Clothing Items\")\ndf['Clothing ID'].value_counts()[:30].plot(kind='bar',color='royalblue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.jointplot(x= df[\"Positive Feedback Count\"], y=df[\"Age\"], kind='reg', color='royalblue')\ng.fig.suptitle(\"Scatter Plot for Age and Positive Feedback Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,14))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.boxplot(x=\"Division Name\",y='Rating',data=df)\nax1 = plt.title('Rating Distribution per Division')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.boxplot(x=\"Department Name\",y='Rating',data=df)\nax2 = plt.title('Rating Distribution per Department')\n\nax3 = plt.subplot2grid((2,2),(1,0),colspan=2)\nax3 = plt.xticks(rotation=45)\nax3 = sns.boxplot(x=\"Class Name\",y='Rating',data=df)\nax3 = plt.title('Rating Distribution per Class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DEEP LEARNING ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Review Text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[['Review Text','Rating']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(inplace=True)\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['Review Text']\ny = pd.get_dummies(data['Rating']).values\n# y = data['Rating']\nmessages = X.copy()\nmessages = list(messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voc_size = len(X)\n\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(messages)):\n    review = re.sub('[^a-zA-Z]', ' ', messages[i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_repr=[one_hot(words,voc_size)for words in corpus] \nsent_length=40\nembedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=sent_length)\nprint(embedded_docs[0:5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(embedded_docs)\ny = np.array(y)\nprint(X.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM=100\nmodel = Sequential()\nmodel.add(Embedding(voc_size, EMBEDDING_DIM, input_length=sent_length))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"\\n\")\nprint(\"-------------MODEL SUMMARY--------------\")\nprint(\"\\n\")\nmodel.summary()\nepochs = 100\nbatch_size = 64\nfrom tensorflow.keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',restore_best_weights=True)\nprint(\"\\n\")\nprint(\"-------------STARTING TRAINING--------------\")\nprint(\"\\n\")\nhistory = model.fit(X_train,y_train,validation_split=0.2,epochs=epochs,batch_size=batch_size,callbacks=[es])\nprint(\"\\n\")\nprint(\"-------------TRAINING COMPLETED--------------\")\nprint(\"\\n\")\naccr = model.evaluate(X_test,y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MORE MODELS WILL BE ADDED","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# BERT ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,RandomSampler,TensorDataset,SequentialSampler\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\nSEED = 10\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = data['Review Text'].values\nMAX_LEN = 256\n# importing bert tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\ninput_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\nlabels = data['Rating'].values\n\nprint(\"Actual sentence before tokenization: \",sentences[1])\nprint(\"Encoded Input from dataset: \",input_ids[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_mask=[]\nattention_mask = [[float(i>0) for i in seq] for seq in input_ids]\nprint(attention_mask[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(input_ids,labels,random_state=41,test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_masks, test_masks,_,_  = train_test_split(attention_mask,input_ids,random_state=41,test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = torch.tensor(X_train)\nX_test = torch.tensor(X_test)\ny_train = torch.tensor(y_train)\ny_test = torch.tensor(y_test)\ntrain_masks = torch.tensor(train_masks)\ntest_masks = torch.tensor(test_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape,y_test.shape, train_masks.shape,test_masks.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=32\ntrain_data = TensorDataset(X_train,train_masks,y_train)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data , sampler=train_sampler, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = TensorDataset(X_test,test_masks,y_test)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6).to(device)\n# Parameters:\nlr = 2e-5\nadam_epsilon = 1e-8\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader)*epochs\noptimizer = AdamW(model.parameters(),lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\nscheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Store our loss and accuracy for plotting\ntrain_loss_set = []\nlearning_rate = []\n\n# Gradients gets accumulated by default\nmodel.zero_grad()\n\n# tnrange is a tqdm wrapper around the normal python range\nfor _ in tnrange(1,epochs+1,desc='Epoch'):\n  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n  # Calculate total loss for this epoch\n  batch_loss = 0\n\n  for step, batch in enumerate(train_dataloader):\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n\n    # Forward pass\n    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    loss = outputs[0]\n    \n    # Backward pass\n    loss.backward()\n    \n    # Clip the norm of the gradients to 1.0\n    # Gradient clipping is not in AdamW anymore\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    \n    # Update learning rate schedule\n    scheduler.step()\n\n    # Clear the previous accumulated gradients\n    optimizer.zero_grad()\n    \n    # Update tracking variables\n    batch_loss += loss.item()\n\n  # Calculate the average loss over the training data.\n  avg_train_loss = batch_loss / len(train_dataloader)\n\n  #store the current learning rate\n  for param_group in optimizer.param_groups:\n    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n    learning_rate.append(param_group['lr'])\n    \n  train_loss_set.append(avg_train_loss)\n  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n    \n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables \n  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n\n  # Evaluate data for one epoch\n  for batch in test_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    \n    # Move logits and labels to CPU\n    logits = logits[0].to('cpu').numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    labels_flat = label_ids.flatten()\n    \n    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n    \n    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n    \n    eval_accuracy += tmp_eval_accuracy\n    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n    nb_eval_steps += 1\n\n  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metrics['Actual_class'].unique() , df_metrics['Predicted_class'].unique() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Review Text','Rating']].drop_duplicates(keep='first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## emotion labels\nlabel2int = {\n  \"bad\": 2,\n  \"neutral\": 3,\n  \"good\": 4,\n    \"excellent\":5\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model\n# model_save_folder = 'model/'\n# tokenizer_save_folder = 'tokenizer/'\n\n# path_model = F'/kaggle/working/{model_save_folder}'\n# path_tokenizer = F'/kaggle/working/{tokenizer_save_folder}'\n\n# #create the dir\n\n# !mkdir -p {path_model}\n# !mkdir -p {path_tokenizer}\n\n# ## Now let's save our model and tokenizer to a directory\n# model.save_pretrained(path_model)\n# tokenizer.save_pretrained(path_tokenizer)\n\n# model_save_name = 'fineTuneModel.pt'\n# path = path_model = F'/kaggle/working/{model_save_folder}/{model_save_name}'\n# torch.save(model.state_dict(),path);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# we get a validation accuracy of almost 70 percent using BERT \n\n# more models will be added ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}