{"cells":[{"metadata":{},"cell_type":"markdown","source":"## *Last edit by DLao - 2020/09 updated with full data*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<br>\n<br>\n\n\n![](https://cdn.statically.io/img/thakoni.com/f=auto%2Cq=30/wp-content/uploads/2020/06/1591106722_Lucifer-Season-5-Release-Date-Cast-Netflix-And-Everything-You.jpg)\n# Netflix Analytics - Movie Recommendation through Correlations / CF\n<br>\n\nI love Netflix! Everyone does?\n\nThis project aims to build a movie recommendation mechanism within Netflix. The dataset I used here come directly from Netflix. It consists of 4 text data files, each file contains over 20M rows, i.e. over 4K movies and 400K customers. All together **over 17K movies** and **500K+ customers**! \n\n<br>\nOne of the major challenges is to get all these data loaded into the Kernel for analysis, I have encountered many times of Kernel running out of memory and tried many different ways of how to do it more efficiently. Welcome any suggestions!!!\n\nThis kernel will be consistently be updated! Welcome any suggestions! Let's get started!\n\n<br>\nFeel free to fork and upvote if this notebook is helpful to you in some ways!\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Model\nfrom keras.layers import Input, Reshape, Dot\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Concatenate, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data\n### As the raw data format is not readable as csv file, we need some pre-process steps to convert it into csv format and then import to pandas dataframe later"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame to store all imported data\nif not os.path.isfile('data.csv'):\n    data = open('data.csv', mode='w')\n\nfiles = ['../input/netflix-prize-data/combined_data_1.txt',\n         '../input/netflix-prize-data/combined_data_2.txt',\n         '../input/netflix-prize-data/combined_data_3.txt',\n         '../input/netflix-prize-data/combined_data_4.txt']\n\n# Remove the line with movie_id: and add a new column of movie_id\n# Combine all data files into a csv file\nfor file in files:\n  print(\"Opening file: {}\".format(file))\n  with open(file) as f:\n    for line in f:\n        line = line.strip()\n        if line.endswith(':'):\n            movie_id = line.replace(':', '')\n        else:\n            data.write(movie_id + ',' + line)\n            data.write('\\n')\ndata.close()\n\n# Read all data into a pd dataframe\ndf = pd.read_csv('data.csv', names=['movie_id', 'user_id','rating','date'])\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-process data\n### From dataframe df, let's take only a smaller dataset of 2000 top rated movies and 100000 top users (who gave the most rates) and save into new df: lite_rating_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"lite_rating_df = pd.DataFrame()\n\ngroup = df.groupby('user_id')['rating'].count()\ntop_users = group.sort_values(ascending=False)[:10000]\n\ngroup = df.groupby('movie_id')['rating'].count()\ntop_movies = group.sort_values(ascending=False)[:2000]\n\nlite_rating_df = df.join(top_users, rsuffix='_r', how='inner', on='user_id')\nlite_rating_df = lite_rating_df.join(top_movies, rsuffix='_r', how='inner', on='movie_id')\n\n# Re-name the users and movies for uniform name from 0..2000 and 10000\nuser_enc = LabelEncoder()\nlite_rating_df['user'] = user_enc.fit_transform(lite_rating_df['user_id'].values)\nmovie_enc = LabelEncoder()\nlite_rating_df['movie'] = movie_enc.fit_transform(lite_rating_df['movie_id'].values)\n\nn_movies = lite_rating_df['movie'].nunique()\nn_users = lite_rating_df['user'].nunique()\n\n# print(n_movies, n_users)\nlite_rating_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = lite_rating_df[['user', 'movie']].values\ny = lite_rating_df['rating'].values\n\n# Split train and test data (for test model performance at last)\nX_training, X_test, y_training, y_test = train_test_split(X, y, test_size=0.1)\n\n# Split train and validation data (to monitor model performance in training)\nX_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1)\n\n# Set the embedding dimension d of Matrix factorization\ne_dimension = 50\n\nX_train_array = [X_train[:, 0], X_train[:, 1]]\nX_val_array = [X_val[:, 0], X_val[:, 1]]\nX_test_array = [X_test[:, 0], X_test[:, 1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build and train deep learning model\n### The embeddings is used to represent each user and each movie in the data. The dot product of user embedding matrix (size: n_users x e_dimension) and movie embedding matrix (size: n_movies x e_dimension) is a good approximation of the rating from user for movie. The model's goal is to minimize the distqace between this dot product and the ratings (training target)"},{"metadata":{"trusted":true},"cell_type":"code","source":"user = Input(shape=(1,))\nu = Embedding(n_users, e_dimension, embeddings_initializer='he_normal',\n              embeddings_regularizer=l2(1e-6))(user)\nu = Reshape((e_dimension,))(u)\nmovie = Input(shape=(1,))\nm = Embedding(n_movies, e_dimension, embeddings_initializer='he_normal',\n              embeddings_regularizer=l2(1e-6))(movie)\nm = Reshape((e_dimension,))(m)\n\nx = Dot(axes=1)([u, m])\n# Build last deep learning layers \nx = Dense(128, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(1)(x)\n\nmodel = Model(inputs=[user, movie], outputs=x)\nmodel.compile(loss='mean_squared_error', \n              optimizer=Adam(lr=0.001), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()]\n              )\n\n# Set up for early stop if the validation loss stop improving for more than 1 epoch\ncallbacks_list = [keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                patience=1,\n                                                ),\n                  # Saves the weights after every epoch\n                  keras.callbacks.ModelCheckpoint(  \n                      filepath='Model_1',\n                      monitor='val_loss',\n                      save_best_only=True,\n                      )]\n\n# Print model info summary\nmodel.summary()  \n\nhistory = model.fit(x=X_train_array, y=y_train, batch_size=64, epochs=20,\n                    verbose=1, \n                    callbacks=callbacks_list,\n                    validation_data=(X_val_array, y_val)\n                    )\n\n# Save the model (we should make a good habit of always saving our models after training)\nmodel.save(\"Model_1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the training and validation loss\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'ro', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = tf.keras.metrics.RootMeanSquaredError()\nm.update_state(model.predict(X_test_array), y_test)\nm.result().numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n### In the test result, we can see that our model's RMSE is 0.7731, which is quite good and seemingly so much improved from the Cinematch's performance (0.9514) or the prize winner team ”BellKor’s Pragmatic Chaos” (0.8567) but that is not really true. As our model have not trained in the original massive dataset with more sparse matrix and testing in the qualifying dataset (Netflix's test data) is not possible since the competition closed, any comparation would hardly be correct.\n\n### Therefore, in this notebook, my main purpose is to show a deep learning approach to the challenge which is simple and effective to apply with a decent accuracy. Any comment about further improvement or correction would be very welcome."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}