{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Forecasting Onset of Diabetes Mellitus\n\nThis project focuses on predicting the whether or not a patient has diabetes. The data is cleaned, analyzed, and used to develop a predictive model."},{"metadata":{},"cell_type":"markdown","source":"## Columns \n\nPregnancies: Number of times pregnant <br><br>\nGlucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test <br><br>\nBloodPressure: Diastolic blood pressure (mm Hg) <br> <br>\nSkinThickness: Triceps skin fold thickness (mm) <br><br>\nInsulin: 2-Hour serum insulin (mu U/ml) <br><br>\nBMI: Body mass index (weight in kg/(height in m)^2)<br><br>\nDiabetes Pedigree Function: Diabetes pedigree function<br><br>\nAge: Age (years)<br><br>\nOutcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0"},{"metadata":{},"cell_type":"markdown","source":"## Import Tools "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n%matplotlib inline \nimport seaborn as sns\nimport itertools \nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"import os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) \"\"\"\ndf = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning <brk>\n    \nExplore the data to look for any inconsistencies. <br>\n\nSome good procedures when going through data for the first time are:\n1. Check number of rows & columns --> df.shape\n2. Check for null values \n3. Check data types of each column \n4. Note any imbalances in data, such as one target outcome having significantly more data records than others"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns = {'BloodPressure':'BP', 'DiabetesPedigreeFunction':\"DPF\"}) #Rename column titles to make them shorter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes** <brk>\n    \n    1. Do the minimum values of 0 make sense for all the different features ??  \n    2. There are no null value data points in the original data \n    3. All the data is in the correct data type format (int or float). \n    \n    The zero values in certain columns (Glucose, BP, Skin Thickness, Insuline, BMI) will be replaced with 'NaN' until further\n    analysis is conducted on the data. \n    \n    It is really important to handle these zero values as they affect the statistics of the data. When they are replaced by \n    NaN, Python automatically disregards them when calculating metrics such as mean, median, percentile, etc. "},{"metadata":{"trusted":true},"cell_type":"code","source":"zeroCols = ['Glucose', 'BP', 'SkinThickness', 'Insulin','BMI'] # Columns with incorrect Zero values \ndf2 = df.copy() # create a copy of the original dataframe \ndf2[zeroCols] = df2[zeroCols].replace(0,np.NaN) #Replace 0s with NaNs\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcomes = df2[\"Outcome\"].value_counts()\nprint(outcomes)\n\n# 0 = No Diabetes\n# 1 = Diabetes ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** There is an big imbalance between the number of records for both outcomes. This must be taken into consideration when training the model to predict the onset of diabetes. "},{"metadata":{},"cell_type":"markdown","source":"**Descriptive Stats Comparison** <br>\nComparison of descriptive stats between the original data and the new data with 0s replaced with NaNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() # Original data loaded into a dataframe ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.describe() # Data with 0s replaced with NaNs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** After replacing the 0s, 'SkinThickness' and 'Insulin' data has been significantly reduced. Almost 50% of the Insulin values are Null/missing and 30% of SkinThickness values are Null/missing."},{"metadata":{"trusted":true},"cell_type":"code","source":"#df2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_values = (df2.isna().sum()/len(df2))*100\nnull_values.drop(labels = ['Pregnancies','DPF','Age','Outcome'], inplace=True)\nprint(\"Column Name\" + \"     \" + \"% of Null Values\\n\")\nprint(null_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is possible that the **presence/lack** of Skin Thickness and Insulin data is related to a person having diabetes. To check, the ratio between the number of data points for each outcome was noted. If the ratio for all the features is within the same range, then it could be assumed that there might not be a relationship as mentioned previously. However, if ratio is significantly skewed one way (ex: records of diabetes patients have significantly more data points of insulin collected), it could be an indicator of a relationship between the missing values and a patient having diabetes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dp = df2.groupby('Outcome').count() # Grouping the number of data points for both outcomes \n#print(dp)\noutcome_0 = dp.loc[0,:] # Number of data points related to Outcome = 0\noutcome_1 = dp.loc[1,:] # Number of data points related to Outcome = 1\nprint(\"Column Name\" + \"     \" + \"Outcome 1 to Outcome 2 Data Points Ratio\\n\") \nprint(outcome_1/outcome_0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the ratios displayed above, the ratio for the number of data points is around 0.5 (plus-min 0.04). It would be safe to conclude that the ratio between the number of data points for Skin Thickness and Insulin for both outcomes are within the same range as other features. "},{"metadata":{},"cell_type":"markdown","source":"## Simple EDA\nExplore the data to get a better understanding of different trends, correlations, patterns, etc. "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"hist = df2.hist(figsize = (20,20))\n# Disregard the outcome histogram ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Based on the skewness of the appropriate features, the 0 values will be replaced. **"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Replace 0 values in BMI, BP (Blood Pressure), Glucose, Insulin, and Skin Thickness \n\ndf2['Glucose'].fillna(df2['Glucose'].median(), inplace = True)\ndf2['BMI'].fillna(df2['BMI'].median(), inplace = True)\ndf2['BP'].fillna(df2['BP'].mean(), inplace = True)\ndf2['Insulin'].fillna(df2['Insulin'].median(), inplace = True)\ndf2['SkinThickness'].fillna(df2['SkinThickness'].mean(), inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"histZR = df2.hist(figsize = (20,20)) # Histogram of data with zeros replaced ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The zeros have been replaced with mean/median values**  <br>\nIf the NaN records were removed, half of the records in the dataset would have to be removed as the Insulin data by itself had 48% NaN values. "},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis <br>\nThe data will now be explored more in-depth "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Heat Map\nhmap = sns.heatmap(df2.corr(), cmap = \"BrBG\", annot=True)\n#plt.savefig(r'C:\\Users\\Shakti\\Desktop\\Data Science\\Projects\\Pima-Indians-Diabetes-Project\\Data Visualizations\\heatmap.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes:** <br>\n1. There is minimal correlation between Skin Thickness & Insulin to the Outcomes. \n2. Relatively, Glucose (0.49) and BMI (0.31) have the highest correlation with the Outcome "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Pair Plot \npplot=sns.pairplot(df2, hue = 'Outcome', palette=\"husl\")\nplt.show()\n#plt.savefig(r'C:\\Users\\Shakti\\Desktop\\Data Science\\Projects\\Pima-Indians-Diabetes-Project\\Data Visualizations\\pairplot.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** From the pair plot, it is hard to find any features which clearly distinguish between the outcomes. <br>\n\n**Next:** Lets split the data for both outcomes and compare their descriptive stats. "},{"metadata":{"trusted":true},"cell_type":"code","source":"out0 = df2[df2['Outcome']==0]\nout1 = df2[df2['Outcome']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out0.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From a first glance, it seems that the data for Outcome 1 has higher descritive stats values. However, there is also a **high standard deviation** and  the distrbution for all the features are not 'normal'. "},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation (Machine Learning)\n\nData needs to prepared to be used in machine learning models. If there is categorical data, it would need to be encoded. Numerical data would need to be scaled. \n\nSplitting the data into training and testing sets is also very important.It is never good to train your model on some data, and then test it on that same data. There are different approaches to help improve generalization in a model, but it is always important to test the model on data it has never seen before."},{"metadata":{},"cell_type":"markdown","source":"### Approach <br>\n\n**Imbalanced Data** <br>\nApproximately 35% of the data has an outcome of 1, and 65% of the data has an outcome of 0. The imbalance is not extreme but learning to address such problems is still important when developming machine learning models. In this approach an ensemble learning method will be used with **SVM, Logistic Regression, Random Forests, and KNN** \n\n\n**AUROC** will be used to gage a models' ability to correctly classify the data. The models will also be \"penalized\" or optimized to handle the imbalanced data as best as possible. For more information, look into ROC curves.\n\n**Train/Validation/Test Split**:The data will be split into three groups. <br>\nTraining data: data used to train the model <br>\nValidation: data used to tune the hyperparameters <br> \nTest: data used for final evaluation of the model <br>\n--> Ratio: 70/15/15"},{"metadata":{},"cell_type":"markdown","source":"### Import Algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling \nSome algorithms require the features to be **scaled/standardized/normalized**. There are different ways to accomplish this, but the method may vary based on the spread of the data. We will use Sklearn's **Robust Scaler** which uses the inerquartile range to scale the values.\n\nRequired: KNN, Logistic Regression, SVM <br>\nNot Required: Random Forests "},{"metadata":{},"cell_type":"markdown","source":"**Data Prep** <br>\nThe data will be split into training/validation/test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = (df2.iloc[:,:8]).values # feature values \ntarget = (df2.loc[:,'Outcome']).values # target values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train - Validation/Test Split --> 70/30 \ntestSize = 0.3\ntrainSize = 0.7\nvalidSize = 0.5\nrs = 42 # random state \n\nx_train, x, y_train, y = train_test_split(features,target,train_size = trainSize, random_state=rs)\nx_val, x_test, y_val, y_test = train_test_split(x,y,train_size=validSize, random_state = rs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"# of Training Data:{len(x_train)}\\n# of Validation Data: {len(x_val)}\\n# of Test Data:{len(x_test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Scaling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = RobustScaler()\nxTrain_scaled = scaler.fit_transform(x_train)\nxVal_scaled = scaler.fit_transform(x_val)\nxTest_scaled= scaler.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning Models\n\n**Evaluation Metrics**<br>\nThese metrics give us a better understanding of how our model performs. Ideally, AUC of ROC should be as close to 1 as possible. Sensititivy is a measure of the proportion of actual positives that are classified (e.g.the percentage of sick people who are correctly identified as having the condition). Sensitivity is a measure of actual negatives that are correctly classified (e.g. the percentage of healthy people who are correctly identified as not having the condition). Both of those metrics are used to measure the AUC. <br><br>\nAccuracy alone is not a good enough measure. If there were 150 (Class 1) and 10 (Class 2) data points. Even if the model misclassifies all of the Class 2 data points, the accuracy would still be 150/160 (93.75%). In cases which imbalances in data it is vital to use other metrics to measure the performance of the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def modeleval(yTrue, yPredict, print_metrics,modelname):\n    # Area Under ROC Curve\n    auc = roc_auc_score(yTrue,yPredict)\n\n    # Confusion Matrix Evaluation \n    cm = confusion_matrix(yTrue,yPredict)\n\n    # True negative, Flase positive, false negative, true positive\n    tn, fp, fn, tp = confusion_matrix(yTrue,yPredict).ravel() \n\n    # True Positive Rate (Sensitivity)\n    tpr = tp/(tp+fn)\n\n    # True Negative Rate (Specificity)\n    tnr = tn/(tn+fp)\n\n    # Accuracy \n    acc = accuracy_score(yTrue,yPredict)\n    \n    # Model Metrics\n    mm = {\n        'AUC':auc,\n        'Confusion Matrix':cm,\n        'TN':tn,\n        'FP':fp,\n        'FN':fn,\n        'TP':tp,\n        'TPR':tpr,\n        'TNR':tnr,\n        'Accuracy':acc\n    }\n    \n    if print_metrics:\n        print(f\"Sensitivity:{mm['TPR']}\\n\\n\\\nSpecificity:{mm['TNR']}\\n\\n\\\nAUC of ROC:{mm['AUC']}\\n\\n\\\nAccuracy:{mm['Accuracy']}\\n\\n\")\n        \n        x = pd.crosstab(yTrue, yPredict, rownames=['True'], colnames=['Predicted'], margins=True)\n        print(f\"{x}\\n\")\n        plot_confusion_matrix(yTrue, yPredict,classes=np.array(['No Diabetes','Diabetes']),\n                      title='Confusion matrix: ' + modelname)\n        plt.show()\n    \n    return mm  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotroc(yvt,yvp,modelname): # y_validation_truth & y_validation_prediction\n    f, t, thresh = roc_curve(yvt, yvp)\n    roc_auc = auc(f, t)\n    plt.title('Receiver Operating Characteristic: ' + modelname)\n    plt.plot(f, t, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Confusion Matrix\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    \"\"\"if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm) \"\"\"\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n# This function was obtained from the Scikit-learn documentation for plotting the confusion matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. SVM "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize SVM Classifier \nsvm_ = SVC(kernel='rbf',class_weight = 'balanced',random_state = 1)\n\n#Train the model with the training data \nsvm_.fit(xTrain_scaled,y_train)\n\n#Validate the model \ny_valPredict = svm_.predict(xVal_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluate the SVM Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"svmModel = modeleval(y_val,y_valPredict,True,'SVM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotroc(y_val,y_valPredict,\"SVM\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:** <br> Using the validation set, the different kernels for SVM were tested (Linear, RBF, Sigmoid, and Poly) to gage the model performance. It was determed that the RBF (Radial Basis Function) provided the best results based on the metrics displayed above. "},{"metadata":{},"cell_type":"markdown","source":"### 2. KNN (k-Nearest Neighbour)"},{"metadata":{"trusted":true},"cell_type":"code","source":"knnAUC = [] # AUCROC values \nvalScores = [] # Validation accuracy scores \nkvalues = [] # K values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range (1,21):            \n    knn = KNeighborsClassifier(i)\n    knn.fit(xTrain_scaled,y_train)\n\n    #Predict \n    knnPred = knn.predict(xVal_scaled)\n    \n    #Evaluation \n    knnModel = modeleval(y_val,knnPred,False,'k-NN')\n    \n    knnAUC.append(knnModel['AUC'])\n    valScores.append(knnModel['Accuracy'])\n    kvalues.append(i)\n    \nknnPerformance = pd.DataFrame({'AUC':knnAUC,'Accuracy':valScores})","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot KNN Performance \nfigknn = plt.figure(figsize=(8,8))\nknnP = plt.subplot(111)\nknnP.plot(kvalues,knnAUC, label = 'AUC', marker = 'o')\nknnP.plot( kvalues,valScores,label = 'Accuracy', ls = '-')\nplt.xlabel('K-Values')\nplt.ylabel(\"AUCROC & Accuracy\")\nplt.title('K-NN Model Performance')\nplt.xticks(np.arange(1,21,1))\nknnP.legend()\nplt.show()\n#figknn.savefig(r'C:\\Users\\Shakti\\Desktop\\Data Science\\Projects\\Pima-Indians-Diabetes-Project\\Data Visualizations\\knnModelTuning.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be noted from the graph above, **K=11** provides the highest AUCROC and Accuracy for the K-NN Model. With \n**AUCROC = 71.40%** and **Accuracy = 75.65%** "},{"metadata":{},"cell_type":"markdown","source":"**Rebuild Model** <br>\nRebuild the model with the optimal parameters. --> K=10"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(11)\nknn.fit(xTrain_scaled,y_train)\n\n#Predict \nknnPred = knn.predict(xVal_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Evaluation \nknnModel = modeleval(y_val,knnPred,True, 'k-NN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotroc(y_val,knnPred, \"KNN\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"regularization = ['l1', 'l2'] # Regularization methods \ncost = [0.001, 0.01, 0.1, 1, 10,100] \naucLogreg = np.zeros((len(cost),len(regularization)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = 0\ncol = 0\n\nfor i in regularization:\n    for c in cost:\n        logreg = LogisticRegression(C = c,class_weight = 'balanced',penalty = i)\n        logreg.fit(xTrain_scaled,y_train)\n        lrPred=logreg.predict(xVal_scaled)\n        lrModel = modeleval(y_val,lrPred,False,'Logistic Regression')\n        \n        aucLogreg[row,col] = lrModel['AUC']\n        row += 1\n    row=0\n    col +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot KNN Performance \nfiglr = plt.figure(figsize=(8,8))\nlrP = plt.subplot(111)\nfor i in range(2):\n    lrP.plot(cost, aucLogreg[:,i], label = regularization[i], marker = 'o')\n#lrP.plot( kvalues,valScores,label = 'Accuracy', ls = '-')\nplt.xlabel('C-Values')\nplt.ylabel(\"AUCROC\")\nplt.title('Logistic Regression Model Performance: Regularization')\nplt.xscale('log')\nplt.xticks(cost)\nlrP.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(C=0.01,class_weight = 'balanced',penalty = 'l2')\n\n#Train\nlogreg.fit(xTrain_scaled,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict\nlrPred=logreg.predict(xVal_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate\nlrModel = modeleval(y_val,lrPred,True,'Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotroc(y_val,lrPred, \"Logistic Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Regularization** <br>\nRegularization is a method is used to reduce the risk of overfitting a model. In our model, the 'l2' (Ridge Regression) regularization is used, as it can be seen that it helps obtain a higher AUROC. C is a control variable which helps control the Lambda operator in the regularization function. It is the inverse of the if regularization strength (a.k.a Lambda) <br>\n\nThe reason C=0.01 was chosen over 0.001 (gives highest AUROC in the graph) was because a C=0.01 provides a better trade off between Specificity and Sensitivity."},{"metadata":{},"cell_type":"markdown","source":"### 4. Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\nrf.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict\nrfPred=rf.predict(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate\nrfModel = modeleval(y_val,rfPred,True,'Random Forests')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotroc(y_val,rfPred, \"Random Forests\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The results obtains so far for the Random Forest model are based on default hyperparamters in the model. The AUCROC is only 0.61 and the accuracy is 67.83%. To improve the model, we will have to optimize the hyperparamters.**"},{"metadata":{},"cell_type":"markdown","source":"**GridSearchCV** <br>\n\nGridSearchCV is a tool that is used to help with hyperparameter tuning to help pick the optimal hyperparameters. It can be applied over different typse of models, but in this case, we will only be applying it to the Randon Forest model since this model has a lot of hyperparameters which can affect the model's performance. It is an important tool which significantly help improve the model's design. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# The parameter grid outlines which paramters you want to optimize and the corresponsind hyperparameter values you want to test.\nparam_grid = { \n    'n_estimators': [10,50,100,200,300,500,600],\n    'max_features': ['auto', 'sqrt', 'log2'],\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the grid search model\n\nrf_gs = GridSearchCV(estimator = RandomForestClassifier(random_state = 5),\n                    param_grid = param_grid, cv=5)\n\n#Train the model\nrf_gs.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtain the best paramters determined by the grid search\nprint(f\"The best paramters for the Random Forest model are: \\n {rf_gs.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**-->** Rebuild the model with the optimal parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_custom = RandomForestClassifier(max_features= 'auto',n_estimators=200,random_state = 5)\nrf_optimal = RandomForestClassifier(max_features= 'log2',n_estimators=100,random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\nrf_custom.fit(x_train,y_train)\nrf_optimal.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict\nrf_customPred=rf_custom.predict(x_val)\nrf_optimalPred=rf_optimal.predict(x_val)\n\n#Evaluate\nprint(\"----------------------------------Metrics for the CUSTOM Random Forests Model----------------------------------\\n\")\nrf_customModel = modeleval(y_val,rf_customPred,True,'Random Forests (Custom)')\nplotroc(y_val,rf_customPred, \"Random Forests (Custom)\")\nprint(\"----------------------------------Metrics for the OPTIMAL Random Forests Model----------------------------------\\n\")\nrf_optimalModel = modeleval(y_val,rf_optimalPred,True,'Random Forests (Optimal)')\nplotroc(y_val,rf_optimalPred, \"Random Forests (Optimal)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **Custome RF Model** was made using paramters tested without GridSearch. It provided a beter a AUCROC comapred to the the **Optimal RF Model** created using GridSearch. It is important to note that in this case, the sensitivity, specificity, and AUC are primarily being used to asses the model performance. GridSearch also asses other metrics to evaluate the model performance which can be seen found through the documentation. In this case, we will use the custom rf model. "},{"metadata":{},"cell_type":"markdown","source":"## Model Testing\n\nThe models will be tested on the **test data** to observe their performance on unseen data. This will help gage which model performs best on new data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"theModels = []\ntheModels.append(('SVM',svm_))\ntheModels.append(('k-NN', knn))\ntheModels.append(('Logistic Regresion', logreg))\ntheModels.append(('Random Forests',rf_custom))\n#model_names = ['SVM','k-NN','Logistic Regression','Random Forests']\n#models = [svm_,knn,logreg,rf_customModel]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iterate over the models\ntheAcc = []\ntheAUC = []\n# xTest_scaled= scaler.fit_transform(x_test)\nfor name,model in theModels:\n    if name == 'Random Forests':\n        modelPred = model.predict(x_test)\n    else:\n        modelPred = model.predict(xTest_scaled)\n    \n    modelMetrics = modeleval(y_test,modelPred,False,name)\n    \n    #msg = \"{name}: {modelMetrics['Accuracy']}\".format(name,)\n    msg = name + \"\\n --> Accuracy: {:.2f} %\\n --> AUC: {:.2f}\\n\".format(modelMetrics['Accuracy']*100,modelMetrics['AUC'])\n    print(msg)\n    \n    theAcc.append(modelMetrics['Accuracy'])\n    theAUC.append(modelMetrics['AUC'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pos = np.arange(len(theModels))\nmname = []\nfor i in range(len(theModels)):\n    mname.append(theModels[i][0])\n    \nfigModel = plt.figure(figsize=(8,8))\nplt.bar(y_pos + 0.00,theAUC,align='center',alpha = 0.65,width = 0.25,label = 'AUCROC')\nplt.plot(y_pos,theAcc,color = 'r',label = 'Accuracy')\n#plt.bar(y_pos + 0.25,theAUC,align='center',alpha = 0.65,width = 0.25,label = 'AUCROC')\nplt.xticks(y_pos,mname)\nplt.title('Model Accuracy and AUCROC')\nplt.ylabel('Accuracy/AUC')\nplt.xlabel('Model')\nplt.legend()\nplt.show()\n#figModel.savefig(r'C:\\Users\\Shakti\\Desktop\\Data Science\\Projects\\Pima-Indians-Diabetes-Project\\Data Visualizations\\ModelTesting.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knnTestPred = knn.predict(xTest_scaled)\nplot_confusion_matrix(y_test, knnTestPred,classes=np.array(['No Diabetes','Diabetes']),\n                      title='Confusion matrix: ' + 'k-NN Model')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, t, thresh = roc_curve(y_test, knnTestPred)\nroc_auc = auc(f, t)\nfigKnnTest = plt.figure(figsize=(8,8))\nplt.title('Receiver Operating Characteristic: ' + 'k-NN Model')\nplt.plot(f, t, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nFrom the information above, we can not that in this project, k-NN was provided the best accuracy and AUCROC. Random Forests was also another model which performed relatively well. It was surpring to see the SVM model perform so poorly after getting an AUC of almost 76% on the validation set. However, it is an important display of how models may perform on unseen data. <br>\n\n### Improvements\n\nThis project covered a lot of different concepts which are helpful in building machine learning models, such as data manipulation, data exploration, data scaling/normalization, and model evaluation. However, there are always ways to improve. For example, an ensemble voting method can be used to use multiple models to determine the final outcome. When working with medical data, it is also important to consider an optimal value for tradeoff between false negatives and false positives. Feature engineering techniques can also help enhane model performance. For instance, it can be noted that between some features, certain ranges of values consisted of more points relating to non-diabetic/diabetic patients. That information can be used to derive additional feature can help the model make more accuracy predictions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}