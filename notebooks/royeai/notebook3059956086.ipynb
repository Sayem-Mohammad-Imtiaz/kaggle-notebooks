{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n\n\n\n\n################################################################\ndef multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1):\n#Build the model\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n    s = inputs\n\n    #Contraction path\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n    c1 = Dropout(0.1)(c1)\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n    \n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n    c2 = Dropout(0.1)(c2)\n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n     \n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n    c3 = Dropout(0.2)(c3)\n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n     \n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n     \n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = Dropout(0.3)(c5)\n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n    \n    #Expansive path \n    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n     \n    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n     \n    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n    c8 = Dropout(0.1)(c8)\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n     \n    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n    c9 = Dropout(0.1)(c9)\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n     \n    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n     \n    model = Model(inputs=[inputs], outputs=[outputs])\n    \n    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    #model.summary()\n    \n    return model\n ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T20:39:28.206509Z","iopub.execute_input":"2021-06-30T20:39:28.206911Z","iopub.status.idle":"2021-06-30T20:39:32.940313Z","shell.execute_reply.started":"2021-06-30T20:39:28.206832Z","shell.execute_reply":"2021-06-30T20:39:32.939437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import normalize\nimport os\nimport glob\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T20:39:32.941681Z","iopub.execute_input":"2021-06-30T20:39:32.942025Z","iopub.status.idle":"2021-06-30T20:39:33.074808Z","shell.execute_reply.started":"2021-06-30T20:39:32.941988Z","shell.execute_reply":"2021-06-30T20:39:33.074085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n#Resizing images, if needed\nSIZE_X = 256 \nSIZE_Y = 256\nn_classes=20 #Number of classes for segmentation\n\n#Capture training image info as a list\ntrain_images = []\n\n# for directory_path in glob.glob(\"../input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/\"):\nfor img_path in sorted(glob.glob(os.path.join(\"../input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/\", \"*.jpg\"))):\n    img = cv2.imread(img_path, 1)       \n    img = cv2.resize(img, (SIZE_Y, SIZE_X))\n    train_images.append(img)\n       \n#Convert list to array for machine learning processing        \ntrain_images = np.array(train_images)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T20:39:42.88425Z","iopub.execute_input":"2021-06-30T20:39:42.884573Z","iopub.status.idle":"2021-06-30T20:42:57.963766Z","shell.execute_reply.started":"2021-06-30T20:39:42.884542Z","shell.execute_reply":"2021-06-30T20:42:57.962898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T20:43:12.43261Z","iopub.execute_input":"2021-06-30T20:43:12.432959Z","iopub.status.idle":"2021-06-30T20:43:12.573784Z","shell.execute_reply.started":"2021-06-30T20:43:12.432928Z","shell.execute_reply":"2021-06-30T20:43:12.572874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_images[0])\nplt.show()\nprint(train_images.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T20:43:17.979566Z","iopub.execute_input":"2021-06-30T20:43:17.979943Z","iopub.status.idle":"2021-06-30T20:43:18.145815Z","shell.execute_reply.started":"2021-06-30T20:43:17.979911Z","shell.execute_reply":"2021-06-30T20:43:18.144995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Capture mask/label info as a list\ntrain_masks = [] \n# for directory_path in glob.glob(\"128_patches/masks/\"):\nfor mask_path in sorted(glob.glob(os.path.join(\"../input/semantic-drone-dataset/RGB_color_image_masks/RGB_color_image_masks/\", \"*.png\"))):\n    mask = cv2.imread(mask_path, 0)       \n    mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n    train_masks.append(mask)\n        \n#Convert list to array for machine learning processing          \ntrain_masks = np.array(train_masks)\nplt.imshow(train_masks[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:03:10.284798Z","iopub.execute_input":"2021-06-30T22:03:10.285141Z","iopub.status.idle":"2021-06-30T22:05:02.185326Z","shell.execute_reply.started":"2021-06-30T22:03:10.285111Z","shell.execute_reply":"2021-06-30T22:05:02.184525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mimg = cv2.imread('../input/semantic-drone-dataset/RGB_color_image_masks/RGB_color_image_masks/001.png')\nprint(mimg.shape)\nplt.imshow(mimg)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:26:42.578102Z","iopub.execute_input":"2021-06-30T22:26:42.578432Z","iopub.status.idle":"2021-06-30T22:26:44.663319Z","shell.execute_reply.started":"2021-06-30T22:26:42.578401Z","shell.execute_reply":"2021-06-30T22:26:44.662502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(mimg.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:27:21.483827Z","iopub.execute_input":"2021-06-30T22:27:21.484187Z","iopub.status.idle":"2021-06-30T22:27:23.043252Z","shell.execute_reply.started":"2021-06-30T22:27:21.484155Z","shell.execute_reply":"2021-06-30T22:27:23.042432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(mimg)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:07:56.536167Z","iopub.execute_input":"2021-06-30T22:07:56.536495Z","iopub.status.idle":"2021-06-30T22:07:57.94115Z","shell.execute_reply.started":"2021-06-30T22:07:56.536462Z","shell.execute_reply":"2021-06-30T22:07:57.940206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:05:02.186834Z","iopub.execute_input":"2021-06-30T22:05:02.187171Z","iopub.status.idle":"2021-06-30T22:05:02.192482Z","shell.execute_reply.started":"2021-06-30T22:05:02.187136Z","shell.execute_reply":"2021-06-30T22:05:02.191567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes=20","metadata":{"execution":{"iopub.status.busy":"2021-06-30T21:03:27.622703Z","iopub.execute_input":"2021-06-30T21:03:27.623042Z","iopub.status.idle":"2021-06-30T21:03:27.626471Z","shell.execute_reply.started":"2021-06-30T21:03:27.623013Z","shell.execute_reply":"2021-06-30T21:03:27.62566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################\n#Encode labels... but multi dim array so need to flatten, encode and reshape\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nn, h, w = train_masks.shape\nprint(\"train_masks:\",train_masks.shape)\ntrain_masks_reshaped = train_masks.reshape(-1,1)\nprint(\"train_masks_reshaped:\",train_masks_reshaped.shape)\ntrain_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\ntrain_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n\nprint(\"train_masks_encoded_original_shape.unique:\",np.unique(train_masks_encoded_original_shape))\nprint(\"classes codes:\",labelencoder.classes_)\n\n#################################################\n# print(\"train_images.shape:\",train_images.shape)\n# train_images = np.expand_dims(traifn_images, axis=3)\ntrain_images = normalize(train_images, axis=1)\nprint(\"train_images.shape:\",train_images.shape)\n\ntrain_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n\n#Create a subset of data for quick testing\n#Picking 10% for testing and remaining for training\nfrom sklearn.model_selection import train_test_split\nX1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\nprint(\"X1.shape:\",X1.shape)\n#Further split training data t a smaller subset for quick testing of models\nX_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n\nprint(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \ny_train.reshape(-1,1)\nfrom keras.utils import to_categorical\n\ntrain_masks_cat = to_categorical(y_train.reshape(-1,1), num_classes=n_classes)\ny_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n\n\ntest_masks_cat = to_categorical(y_test, num_classes=n_classes)\ny_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:24:31.198914Z","iopub.execute_input":"2021-06-30T22:24:31.199241Z","iopub.status.idle":"2021-06-30T22:24:35.60664Z","shell.execute_reply.started":"2021-06-30T22:24:31.199212Z","shell.execute_reply":"2021-06-30T22:24:35.605757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(train_masks_reshaped_encoded),\n                                                 train_masks_reshaped_encoded)\nprint(\"Class weights are...:\", class_weights)\n\n\nIMG_HEIGHT = X_train.shape[1]\nIMG_WIDTH  = X_train.shape[2]\nIMG_CHANNELS = X_train.shape[3]\nprint(\"X_train.shape:\",X_train.shape)\ndef get_model():\n    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n\nmodel = get_model()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n#If starting with pre-trained weights. \n#model.load_weights('???.hdf5')\n\nhistory = model.fit(X_train, y_train_cat, \n                    batch_size = 16, \n                    verbose=1, \n                    epochs=50, \n                    validation_data=(X_test, y_test_cat), \n                    #class_weight=class_weights,\n                    shuffle=False)\n                    \n\n\nmodel.save('test.hdf5')\n#model.save('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')\n############################################################","metadata":{"execution":{"iopub.status.busy":"2021-06-30T20:46:50.273252Z","iopub.execute_input":"2021-06-30T20:46:50.273569Z","iopub.status.idle":"2021-06-30T20:54:41.906162Z","shell.execute_reply.started":"2021-06-30T20:46:50.273537Z","shell.execute_reply":"2021-06-30T20:54:41.905136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model\n\t# evaluate model\n_, acc = model.evaluate(X_test, y_test_cat)\nprint(\"Accuracy is = \", (acc * 100.0), \"%\")\n\n\n###\n#plot the training and validation accuracy and loss at each epoch\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'y', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n##################################\n#model = get_model()\nmodel.load_weights('test.hdf5')  \n#model.load_weights('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')  \n\n#IOU\ny_pred=model.predict(X_test)\ny_pred_argmax=np.argmax(y_pred, axis=3)\n\n##################################################\n\n#Using built in keras function\nfrom keras.metrics import MeanIoU\n# n_classes = 4\nIOU_keras = MeanIoU(num_classes=n_classes)  \nIOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\nprint(\"Mean IoU =\", IOU_keras.result().numpy())\n\n\n#To calculate I0U for each class...\nvalues = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n\nclass1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\nclass2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\nclass3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\nclass4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n\nprint(\"IoU for class1 is: \", class1_IoU)\nprint(\"IoU for class2 is: \", class2_IoU)\nprint(\"IoU for class3 is: \", class3_IoU)\nprint(\"IoU for class4 is: \", class4_IoU)\n\nplt.imshow(train_images[0, :,:,0], cmap='gray')\nplt.show()\nplt.imshow(train_masks[0], cmap='gray')\nplt.show()\n#######################################################################","metadata":{"execution":{"iopub.status.busy":"2021-06-30T21:35:57.192109Z","iopub.execute_input":"2021-06-30T21:35:57.192444Z","iopub.status.idle":"2021-06-30T21:36:00.242346Z","shell.execute_reply.started":"2021-06-30T21:35:57.192413Z","shell.execute_reply":"2021-06-30T21:36:00.241588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Predict on a few images\n#model = get_model()\n#model.load_weights('???.hdf5')  \nimport random\ntest_img_number = random.randint(0, len(X_test))\ntest_img = X_test[test_img_number]\nground_truth=y_test[test_img_number]\ntest_img_input=np.expand_dims(test_img, 0)\nprediction = (model.predict(test_img_input))\npredicted_img=np.argmax(prediction, axis=3)[0,:,:]\n\n\nplt.figure(figsize=(12, 8))\nplt.subplot(231)\nplt.title('Testing Image')\nplt.imshow(test_img[:,:,0], cmap='gray')\nplt.subplot(232)\nplt.title('Testing Label')\nplt.imshow(ground_truth[:,:,0], cmap='jet')\nplt.subplot(233)\nplt.title('Prediction on test image')\nplt.imshow(predicted_img, cmap='jet')\nplt.show()\n\n#####################################################################\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T21:44:38.70551Z","iopub.execute_input":"2021-06-30T21:44:38.705881Z","iopub.status.idle":"2021-06-30T21:44:39.113905Z","shell.execute_reply.started":"2021-06-30T21:44:38.705849Z","shell.execute_reply":"2021-06-30T21:44:39.113132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}