{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hard-drive-test-data/harddrive.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n\nI'll start by removing all models that does not have any failure. It helps to reduce the bias and unbalancement of the dataset.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## serial number with failures\nfail_hds = df[df['failure'] == 1]['serial_number'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['serial_number'].isin(fail_hds)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sorting values\n\nI Need to sort the values first by their serial number, and then by the date. This will guarantee I'll have the database grouped by each model of HDD's cycle of \"life\".","metadata":{}},{"cell_type":"code","source":"# sorting by serial number and then by date, to get the failure as last value (end of cycle)\ndf = df.sort_values(['serial_number','date'])\n\ndf.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dropping NAs\n\nI'll remove all columns that have more than 5% of NA values. This will clean a lot the dataset.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,8))\ndf.isna().sum().plot(kind='bar')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_notna = df[df.columns[~(df.isna().sum().values/len(df) > 0.05)]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_notna.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_notna.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiclassifier Approach:\n\nEvery cycle of life of one HDD has a failure at the end. It means that we'll have a very imbalanced dataset. Let's consider n executions for all HDDs, and N HDDs. So, we'll have $N$ failures and $n - N$ good executions.\n\nI'll tackle this using the Multiclassifier Approach: there will be k models in k databases, each of one I'll add m failures to the executions right before the N execution (fail).","metadata":{}},{"cell_type":"markdown","source":"## Finding the best m values\n\nFor this, we need to see the differences between each model of HDD start time of use and failure","metadata":{}},{"cell_type":"code","source":"fails = df[df['failure'] == 1].index.values\n\nrev_fails = fails[::-1]\nshift_fails = np.roll(fails[::-1], shift=-1)\nshift_fails[-1] = 0\nfails_dif = fails[::-1] - shift_fails","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts, bins = np.histogram(fails_dif)\nplt.figure(figsize=(12,8))\nplt.hist(bins[:-1], bins, weights=counts)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea is to select all HDDs that have more than 10 days of interval between start and fail.","metadata":{}},{"cell_type":"code","source":"fails_dif_ordered = fails_dif[::-1]\n\nmodels = [features for (features, i) in zip(df[df['failure'] == 1].serial_number.values, range(0, len(df[df['failure'] == 1].index.values))) if fails_dif[i]>10]\n#np.where(features.append()fails_dif_ordered > 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = df[df.serial_number.isin(models)]\nfinal_df.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are good to create k databases to train the model. I'll create 4 databases, with the values: [1, 3, 5, 7], for the number of failures in each cycle. After all, we we'll have almost 50% of database of failures, creating the balanced one.\n\nThis approach is good because it's not very recommended creating synthetic data of failure values. With this approach, it's possible to see when the failure is starting to happen.","metadata":{}},{"cell_type":"code","source":"m = [3, 5, 7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code below will create 3 failures cases, each one for each m value. The case of 1 fail value is already done.","metadata":{}},{"cell_type":"code","source":"failures = []\nfails = final_df[final_df['failure'] == 1].index.values\nfor i in m:\n    failure = list(range(0, len(final_df), 1)) \n    f = 0\n    while f < len(failure):\n        if f+i-1 in fails:\n            \n            for values in range(f,f+i,1):\n                failure[values] = 1\n                \n            f=f+i\n        else:\n            failure[f] = 0\n            f=f+1\n        \n    failures.append(failure)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs = []\nfor i in range(0, 4, 1):\n    if i == 0:\n        dfs.append(final_df)\n    else:\n        aux_df = final_df.copy()\n        aux_df['failure'] = failures[i-1]\n        dfs.append(aux_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs[1].tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have 4 databases, and we need only take out date, serial number and model to start the training.","metadata":{}},{"cell_type":"code","source":"for df in dfs:\n    df.drop(['date', 'serial_number', 'model'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs[2].tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, the 4 databases are ready to train.","metadata":{}},{"cell_type":"markdown","source":"# Building the Model\n\nI'll use the Gradient Boosting, trained in each dataset. First, I'll train a GridSearch to find the optimum parameters for each dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\naccs = [] #accuracies\nrecs = [] #recalls\nprecs = [] #precisions\nf1s = [] #f1 scores\nrocs = [] #roc auc scores\n\nfor df in dfs:\n    \n    X = df.drop('failure', axis=1)\n    y = df.failure\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    \n    \n    gbrt = GradientBoostingClassifier(max_features='sqrt', random_state=0)\n    learning_rates = [0.1, 0.05, 0.01]\n    n_estimators = [16, 32, 64]\n    max_depths = [5, 10, 15]\n    \n    params = {'learning_rate': learning_rates, 'n_estimators':n_estimators, 'max_depth':max_depths}\n    clf = GridSearchCV(gbrt, params)\n    clf.fit(X_train, y_train)\n    models.append(clf)\n    \n    final_pred = clf.predict(X_test)\n    accs.append(accuracy_score(y_test, final_pred))\n    recs.append(recall_score(y_test, final_pred))\n    precs.append(precision_score(y_test, final_pred))\n    f1s.append(f1_score(y_test, final_pred))\n    rocs.append(roc_auc_score(y_test, final_pred))\n    print(clf.best_params_)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating\n\nOff course, in the first model (k=1), we will see an almost perfect accuracy but a zero recall. This means that everytime the model guesses that won't happen a failure. But, despite that, the recall and AUC-ROC values improve everytime k grows, but accuracy does not fall a lot. This is very good.","metadata":{}},{"cell_type":"code","source":"x = ['k=1', 'k=3', 'k=5', 'k=7']\n\nplt.figure(figsize=(12,8))\nplt.bar(x, accs)\nplt.title('Accuracy values of each model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.bar(x, recs)\nplt.title('Recall values of each model')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.bar(x, precs)\nplt.title('Precision values of each model')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.bar(x, f1s)\nplt.title('F1-Score values of each model')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.bar(x, rocs)\nplt.title('AUC-ROC values of each model')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This approch can be used in real life, eitheir in HDD or in big factories that use a lot of automated machines, with constant failures during the production.\n\nThe idea of taking this approach is from the paper of Gian Antonio Susto, Andrea Schirru, Simone Pampuri, Sean McLoone and Alessandro Beghi, \"Machine Learning for Predictive Maintenance:a Multiple Classiﬁer Approach\". ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}