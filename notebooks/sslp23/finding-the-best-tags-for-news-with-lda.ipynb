{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Run in terminal or command prompt\n# python3 -m spacy download en\nimport numpy as np\nimport pandas as pd\nimport re, nltk, spacy, gensim\n# Sklearn\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom pprint import pprint\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom unidecode import unidecode\n!pip install wordcloud\nfrom wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run in terminal or command prompt\n# python3 -m spacy download en\nimport numpy as np\nimport pandas as pd\nimport re, nltk, spacy, gensim\n# Sklearn\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom pprint import pprint\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom unidecode import unidecode\n!pip install wordcloud\nfrom wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the best tags for news with LDA\n\nThe idea here is to find the best tags for this news database applying a LDA model to the description column. LDA is an generative statistical model for dimensionality reduction of text variables, grouping them in topics. \n\nFirst, I'll apply the LDA, see which is the topic probability for each document, then I'll calculate the importance of each word from the document and see if it's relevant enough to be a tag.\n\nThe reasons why I chose description column instead of content is that, first it's faster to train, also because it seems that content column is the same of description, but with the names of the newspaper, location, and other useless things for this model."},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/internet-articles-data-with-users-engagement/articles_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Unnamed: 0', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model = df.dropna(subset=['description'])[['title', 'description']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\nFirst, I'll follow these steps to clean the data and make it ready to apply the model:\n\n- Split dataset\n- Remove special characters\n- Tokenize data\n- Lemmatize data\n\nI'll split the data just to have some news to test. So I'll use only 0.1 of test size."},{"metadata":{"trusted":true},"cell_type":"code","source":"msk = np.random.rand(len(df_model)) < 0.9\n\ndf_train = df_model[msk]\ndf_test = df_model[~msk]\n\ndf_train.reset_index(inplace=True, drop=True)\ndf_test.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to list\ndata = df_train.description.values.tolist()\n# Remove new line characters\ndata = [re.sub(r'\\s+', ' ', sent) for sent in data]\n# Remove distracting single quotes\ndata = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n# Remove special characters\ndata = [re.sub('[^A-Za-z0-9]+', ' ', sent) for sent in data]\n# Remove accentuation\ndata = [unidecode(text) for text in data]\npprint(data[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizing the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\nprint(data_words[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lemmatizing the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n    return texts_out\n\n# Initialize spacy ‘en’ model, keeping only tagger component (for efficiency)\nnlp = spacy.load('en', disable=['parser', 'ner'])\n# Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'VERB']) #select noun and verb\nprint(data_lemmatized[:2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorizing and Creating the Model\nI'll create the vector with the bag of words, and then apply TF-IDF to measure the importance of each word to the document. After that, I'll create the LDA Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word',       \n                             min_df=10,# minimum reqd occurences of a word \n                             stop_words='english',             # remove stop words\n                             lowercase=True,                   # convert all words to lowercase\n                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n                             # max_features=50000,             # max number of uniq words    \n)\ndata_vectorized = vectorizer.fit_transform(data_lemmatized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF IDF\ntfidf = TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf.fit(data_vectorized)\n\ndata_tfidf = tfidf.transform(data_vectorized)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, I'll build the LDA model: I won't do grid search, due to computational cost, but I'll test to find the best number of topics. After that, I'll print the model to check if lda_model contains the best one from the for loop."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build LDA Model\n\nlw_perp = np.inf\nlda_model = 0\nperps = []\nn_topics = range(10,50,5)\nn_topics = list(range(10,55,5))\n\nfor topic in n_topics:\n    lda = LatentDirichletAllocation(n_components=topic,               # Number of topics\n                                          max_iter=10,               # Max learning iterations\n                                          learning_method='online',   \n                                          random_state=100,          # Random state\n                                          batch_size=128,            # n docs in each learning iter\n                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n                                          n_jobs = -1,               # Use all available CPUs\n                                         )\n    lda.fit(data_tfidf)\n    \n    perp = lda.perplexity(data_tfidf)\n    perps.append(perp)\n    \n    if perp <= lw_perp:\n        lw_perp = perp\n        lda_model = lda\n    \nlda_output = lda_model.fit_transform(data_tfidf)\n\nplt.plot(n_topics, perps)\nplt.xlabel('Number of Topics')\nplt.ylabel('Perplexity')\nplt.title('Perplexity for each number of topics')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lda_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, we have now the LDA model with 10 components (topics).\n\nNow, I want to see two things: First, which words are the most important for each topic, that I'll show using wordclouds, and in which topic each document is being placed."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n    keywords = np.array(vectorizer.get_feature_names())\n    topic_keywords = []\n    for topic_weights in lda_model.components_:\n        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n        topic_keywords.append(keywords.take(top_keyword_locs))\n    return topic_keywords\n\ntopic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=15)\ndf_topic_keywords = pd.DataFrame(topic_keywords)\ndf_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\ndf_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\ndf_topic_keywords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"df_topic_keywords stores all the topics and its most important words. Let's see that in wordclouds:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for topic in df_topic_keywords.index.values:\n    summ = df_topic_keywords.loc[topic, :].values\n    all_summary = \" \".join(s for s in summ)\n    stopwords = set(STOPWORDS)\n    wordcloud = WordCloud(stopwords=stopwords,\n                      background_color='black', width=1600,                            \n                      height=800).generate(all_summary)\n    \n    fig, ax = plt.subplots(figsize=(10,4))            \n    ax.imshow(wordcloud, interpolation='bilinear')       \n    ax.set_axis_off()\n    plt.title(topic)\n    plt.imshow(wordcloud)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I'll build a dataframe containing each title of news (document) with its respective topic:"},{"metadata":{"trusted":true},"cell_type":"code","source":"topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n\ndocnames = df_train['title']\n\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\ndf_document_topic\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the tags\n\nThe function that I'll be created has the following logic:\n- 1: Do the data preprocessing\n- 2: Apply the already trained vectorizer and tfidf models\n- 3: Find which tags are best, this consists of: calculation of the score -> topic probability of the document * importance of each word for the topic. The words with score higher than 0.001 (you can choose the threshold), are tags."},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_tags(data, vect=vectorizer, tfidf=tfidf, lda_model=lda_model, threshold=0.01):\n    global sent_to_words\n    global lemmatization\n    \n    # Remove new line characters\n    data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n    # Remove distracting single quotes\n    data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n    # Remove special characters\n    data = [re.sub('[^A-Za-z0-9]+', ' ', sent) for sent in data]\n    # Remove accentuation\n    data = [unidecode(text) for text in data]\n\n    \n    \n    \n    data_words = list(sent_to_words(data))\n    data = lemmatization(data_words)\n    \n    \n    ## TOPIC MODELLING\n    data = vect.transform(data)\n    data_ready = tfidf.transform(data)\n    \n    \n    \n    topic_probability_scores = lda_model.transform(data_ready)\n    \n    topics = lda_model.n_components\n    \n    lda_components = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis] # normalization\n    \n    tags = []\n    \n    \n    for topic in range(topics):\n        topic_score = topic_probability_scores[0][topic]\n        \n        for word in data_words[0]:\n            try:\n                word_score = lda_components[topic][vectorizer.get_feature_names().index(word)]\n            except:\n                word_score = 0\n            score = topic_score*word_score\n            if score >= threshold:\n                tags.append(word)\n    \n    return list(set(tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = [df_test['description'].values[1]]\nprint('text:',test)\nprint('recommended tags:', find_tags(test,threshold=0.001))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = [df_test['description'].values[5]]\nprint('text:',test)\nprint('recommended tags:', find_tags(test,threshold=0.0001))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = [df_test['description'].values[15]]\nprint('text:',test)\nprint('recommended tags:', find_tags(test,threshold=0.001))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recommended tags are usually good ones. I though in another methods, to exclude the dependency of the text (because this model will always recommend words that are in text), but I think that, as we have articles, that are usually long texts, this model would work well in production."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}