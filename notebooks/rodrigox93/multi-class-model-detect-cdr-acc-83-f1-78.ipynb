{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom scipy.io import arff\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport numpy as np\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"oasis_longitudinal = '/kaggle/input/mri-and-alzheimers/oasis_longitudinal.csv'\noasis_longitudinal = pd.read_csv (oasis_longitudinal)\noasis_longitudinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_longitudinal.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_cross_sectional = '/kaggle/input/mri-and-alzheimers/oasis_cross-sectional.csv'\noasis_cross_sectional = pd.read_csv (oasis_cross_sectional)\noasis_cross_sectional.rename(columns={'Educ': 'EDUC'}, inplace=True)\noasis_cross_sectional.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_cross_sectional.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete Data without CDR "},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_cross_sectional['CDR'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_cross_sectional.dropna(subset=['CDR'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_cross_sectional.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove unnecessary columns from the 2 datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_longitudinal.drop(['Subject ID'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['MRI ID'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['Visit'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['Group'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['Hand'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['MR Delay'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oasis_cross_sectional.drop(['ID'], axis = 1, inplace = True, errors = 'ignore')\noasis_cross_sectional.drop(['Delay'], axis = 1, inplace = True, errors = 'ignore')\noasis_cross_sectional.drop(['Hand'], axis = 1, inplace = True, errors = 'ignore')\noasis_cross_sectional.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Join the two datasets into one"},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [oasis_longitudinal, oasis_cross_sectional]\ndataset_final = pd.concat(frames)\ndataset_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_final.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre Processing"},{"metadata":{},"cell_type":"markdown","source":"### Review null data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_na = (dataset_final.isnull().sum() / len(dataset_final)) * 100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Lost proportion (%)' :round(data_na,2)})\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputation of missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute  import SimpleImputer\n# We perform it with the most frequent value \nimputer = SimpleImputer ( missing_values = np.nan,strategy='most_frequent')\n\nimputer.fit(dataset_final[['SES']])\ndataset_final[['SES']] = imputer.fit_transform(dataset_final[['SES']])\n\n# We perform it with the median\nimputer = SimpleImputer ( missing_values = np.nan,strategy='median')\n\nimputer.fit(dataset_final[['MMSE']])\ndataset_final[['MMSE']] = imputer.fit_transform(dataset_final[['MMSE']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1= M, 0 = F\ndataset_final['M/F'] = dataset_final['M/F'].replace(['M', 'F'], [1,0])  \ndataset_final.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(dataset_final['CDR'].values)\nle.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_final['CDR'] = le.transform(dataset_final['CDR'].values) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue later.\nprint('Label 0 :', round(dataset_final['CDR'].value_counts()[0]/len(dataset_final) * 100,2), '% of the dataset')\nprint('Label 0.5 :', round(dataset_final['CDR'].value_counts()[1]/len(dataset_final) * 100,2), '% of the dataset')\nprint('Label 1 :', round(dataset_final['CDR'].value_counts()[2]/len(dataset_final) * 100,2), '% of the dataset')\nprint('Label 2 :', round(dataset_final['CDR'].value_counts()[3]/len(dataset_final) * 100,2), '% of the dataset')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove label with label 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_final = dataset_final.drop(dataset_final[dataset_final['CDR']==3].index)\ndataset_final.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset_final.drop([\"CDR\"],axis=1)\ny = dataset_final[\"CDR\"].values # 0,0.5=1,1=2,2=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We divide our data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{0:0.2f}% Train\".format((len(X_train)/len(dataset_final.index)) * 100))\nprint(\"{0:0.2f}% Test\".format((len(X_test)/len(dataset_final.index)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_train))\nprint(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"FOLDS =10\n\nparametros_gb = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.005,0.5, 0.075, 0.1, 0.15, 0.2,0.3,0.8,0.9],\n    \"min_samples_split\": [0.01, 0.025, 0.005,0.4,0.5, 0.075, 0.1, 0.15, 0.2,0.3,0.8,0.9],\n    \"min_samples_leaf\": [1,2,3,5,8,10,15,20,40,50,55,60,65,70,80,85,90,100],\n    \"max_depth\":[3,5,8,10,15,20,25,30,40,50],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":range(1,100)\n    }\n\nmodel_gb= GradientBoostingClassifier()\n\n\ngb_random = RandomizedSearchCV(estimator = model_gb, param_distributions = parametros_gb, n_iter = 100, cv = FOLDS, \n                               verbose=2, random_state=42,n_jobs = -1, scoring='f1_macro')\ngb_random.fit(X_train, y_train)\n\ngb_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gb = GradientBoostingClassifier(subsample = 1.0,n_estimators= 23,\n                 min_samples_split = 0.025,\n                 min_samples_leaf = 3,\n                 max_features = 'log2',\n                 max_depth =10,\n                 loss = 'deviance',\n                 learning_rate = 0.8,\n                 criterion= 'mae')\nmodel_gb.fit(X_train,y_train)\n\nPredicted_gb= model_gb.predict(X_test)\nPredicted_gb_tr= model_gb.predict(X)\n\n\nacc = accuracy_score(Predicted_gb, y_test)\nacc_tr = accuracy_score(Predicted_gb_tr, y)\n\ntest_score = cross_val_score(model_gb, X_train, y_train, cv=FOLDS, scoring='accuracy').mean()\ntest_f1 = cross_val_score(model_gb, X_train, y_train, cv=FOLDS, scoring='f1_macro').mean()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Test\",acc)\nprint(\"Accuracy Training\",acc_tr)\nprint(\"Accuracy Cross_validate\",test_score)\nprint(\"F1 Cross_validate\",test_f1)\nprint(\"F1 Macro:\",f1_score(y_test, Predicted_gb, average='macro'))\nprint(\"F1 Micro:\",f1_score(y_test, Predicted_gb, average='micro'))  \nprint(\"F1 Weighted:\",f1_score(y_test, Predicted_gb, average='weighted'))\n\n#print(f1_score(y, Predicted_gb_tr, average='macro'))\n#print(f1_score(y, Predicted_gb_tr, average='micro'))  \n#print(f1_score(y, Predicted_gb_tr, average='weighted'))\n\nprint(\"\\nMatrix of confusion\")\nPredicted_gb= model_gb.predict(X)\nconfusion_matrix(y, Predicted_gb)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from joblib import dump, load\ndump(model_gb, 'model_gb.joblib') ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}