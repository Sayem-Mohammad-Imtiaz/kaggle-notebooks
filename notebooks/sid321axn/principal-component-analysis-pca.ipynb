{"cells":[{"metadata":{"trusted":true,"_uuid":"7681ac807e1d0f8f1fa03c3d8ab1a5f65a83f09e"},"cell_type":"markdown","source":"# About Dataset\n\nThe dataset contains the following features:\n1. age(in years)\n2. sex: (1 = male; 0 = female)\n3. cp: chest pain type\n4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n5. chol: serum cholestoral in mg/dl\n6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n7. restecg: resting electrocardiographic results\n8. thalach: maximum heart rate achieved\n9. exang: exercise induced angina (1 = yes; 0 = no)\n10. oldpeak: ST depression induced by exercise relative to rest\n11. slope: the slope of the peak exercise ST segment\n12. ca: number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target: 1 or 0 "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE \nfrom sklearn.decomposition import PCA\nimport umap\n%matplotlib inline\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Reading the dataset"},{"metadata":{"trusted":true,"_uuid":"c35ee76322702038ead0c6a9b3a978180ff792b9"},"cell_type":"code","source":"df = pd.read_csv('../input/heart.csv')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0618d942fec15a23adf22b7f2e18b16696c54bd0"},"cell_type":"code","source":"#Checking missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0604ad116092a41b64019e30a5778e12153dec5f"},"cell_type":"code","source":"feat=df.drop(['target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cd356369f5391ce4e9306ee6261e16dd1c69e6c"},"cell_type":"code","source":"target=df['target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dd52862276289b6703a83ba3ebb390460e6aae2"},"cell_type":"markdown","source":"# Correlation"},{"metadata":{"trusted":true,"_uuid":"e316faa27d9912d1bbf5538d7a13f6ffd864b7b0"},"cell_type":"code","source":"X=df.drop(['target'],axis=1)\nX.corrwith(df['target']).plot.bar(\n        figsize = (20, 10), title = \"Correlation with Target\", fontsize = 20,\n        rot = 90, grid = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ce0ba0553626a84e217d3f71205695569b31c27"},"cell_type":"markdown","source":"# PCA\n\nPCA is a technique which helps us in extracting a new set of variables from an existing large set of variables. These newly extracted variables are called Principal Components. You can refer to this article to learn more about PCA. For your quick reference, below are some of the key points you should know about PCA before proceeding further:\n\nA principal component is a linear combination of the original variables\nPrincipal components are extracted in such a way that the first principal component explains maximum variance in the dataset\nSecond principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component\nThird principal component tries to explain the variance which is not explained by the first two principal components and so on\n\nAs the dataset is small having less features we will use only 2 components or dimensions to see how much much variance it is covering\n"},{"metadata":{"trusted":true,"_uuid":"dbb3ef900e717b93d8776a9bcfdf074a77cd101e"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(feat.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d90d50587b6873471b932f28fbaf8e3f61817db5"},"cell_type":"code","source":"plt.plot(range(2), pca.explained_variance_ratio_)\nplt.plot(range(2), np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"Component-wise and Cumulative Explained Variance\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59dba5a1a46db6098fdac3d14858f9d9966c6a44"},"cell_type":"markdown","source":"In the above graph, the blue line represents component-wise explained variance while the orange line represents the cumulative explained variance. We are able to explain around **90%** variance in the dataset using just two components. Let us now try to visualize each of these decomposed components:"},{"metadata":{"trusted":true,"_uuid":"66331379321eda97bf46b428d1a09fa701fb6798"},"cell_type":"code","source":"def pca_results(good_data, pca):\n\t'''\n\tCreate a DataFrame of the PCA results\n\tIncludes dimension feature weights and explained variance\n\tVisualizes the PCA results\n\t'''\n\n\t# Dimension indexing\n\tdimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n\t# PCA components\n\tcomponents = pd.DataFrame(np.round(pca.components_, 4), columns = list(good_data.keys()))\n\tcomponents.index = dimensions\n\n\t# PCA explained variance\n\tratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n\tvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n\tvariance_ratios.index = dimensions\n\n\t# Create a bar plot visualization\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Plot the feature weights as a function of the components\n\tcomponents.plot(ax = ax, kind = 'bar');\n\tax.set_ylabel(\"Feature Weights\")\n\tax.set_xticklabels(dimensions, rotation=0)\n\n\n\t# Display the explained variance ratios\n\tfor i, ev in enumerate(pca.explained_variance_ratio_):\n\t\tax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n\n\t# Return a concatenated DataFrame\n\treturn pd.concat([variance_ratios, components], axis = 1)\n\npca_results = pca_results(feat, pca)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8efe5651c84eff77e175c4d4ae004e2f277abc7"},"cell_type":"markdown","source":"# Inference\n\nFirst 2 Principal components:\n\n**1st PC**: 74.76%\n**2nd PC**: 15.04% \n\nTotal: **89.8%**\n\n1. **first dimension** :  From the above plot it is noticed that the weight is large and positive for chol, while being slightly positive for sex and cp which means that customers who score highly in this component will have very little dimpact on heart disease whereas people having higher cholestrol have greater chances of heart disease.\n\n2. **Second Dimension**: From the above plot it is noticed that the weight is large and negative for **thalach** and slightly negative for cp,chol and slope, which means that patients who score high in this component will have very less chances of heart disease. Whereas age and trestbps are moderatively positive."},{"metadata":{"trusted":true,"_uuid":"01f88c4f8c70b830271c176e508fcd0a6d39ed96"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}