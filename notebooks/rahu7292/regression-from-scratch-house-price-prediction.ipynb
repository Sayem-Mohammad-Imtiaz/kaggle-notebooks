{"cells":[{"metadata":{},"cell_type":"markdown","source":"# REGRESSION From Scratch With BOSTON HOUSE PRICE PREDICTION"},{"metadata":{},"cell_type":"markdown","source":"<img src='https://drive.google.com/uc?id=1rxqFy4bsnYH325VpZwjZVeKfgKa1b4oS' width=1000 >"},{"metadata":{},"cell_type":"markdown","source":"### In this Notebook we will Learn:-\n* Basic EDA.\n* Aplly Scaling on Feature matrix.\n* Dimensionality Reduction (PCA) .\n* K-Cross validation to check accuracy.\n* Multi-linear Regression\n* Polynomial Regression\n* Support Vector Regressor (SVR)\n* Decision Tress Regressor \n* Random Forest Regressor"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, download_plotlyjs, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint()\nprint(\"The files in the dataset are:-\")\nfrom subprocess import check_output\nprint(check_output(['ls','../input']).decode('utf'))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Importing the dataset.\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv('../input/housing.csv', delim_whitespace=True, names=names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In this dataset MEDV is our Target, we have to predict the values of MEDV on the basis of all other variables.\n* MEDV: Median value of owner-occupied homes in $1000s \n* Our task is to predict the value of MEDV."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no null values in the dataset and all values are in their proper format.\n* Dataset is in its proper format, so we will go to regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr().iplot(kind='heatmap', )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation:-\n* These are correlation matrix between all variables.\n* The valriables which are highly correlated with MEDV, we need to select only those variable to make prediction.\n* But this thing we will do with the help of dimensionalty reduction algorithm (PCA). "},{"metadata":{},"cell_type":"markdown","source":"# REGRESSION:-"},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing = \n                     * In this we will follow 4 steps, MCSS.\n                     * M = dealing with Missing data\n                     * C = Dealing with the categorical dataset.\n                     * S = Splitting of dataset.\n                     * S = Scaling of the dataset.\n* As there is no missing values in the dataset.\n* There is no categorical values in the dataset.\n* As dataset is very small there is no need to split the dataset.\n* Before applying the dimensionalty reduction algorithm (PCA), we will follow 1 step i.e. Scaling of dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing of Useful libraries from sklearn library.\nfrom sklearn.preprocessing import StandardScaler   # For Scaling the dataset\nfrom sklearn.model_selection import train_test_split    # For Splitting the dataset\nfrom sklearn.linear_model import LinearRegression      # For Linear regression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us Create Feature matrix and Target Vector.\nx_train = df.iloc[:,:-1].values\ny_train = df.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaling of Feature matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_X=StandardScaler()\nx_train=sc_X.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dimensionalty Reduction by PCA.\n* We are doing this to reduce the number of dimensions/features in the dataset.\n* The features which have less effect on the prediction , we will remove those features.\n* It also boosts the process.\n* It saves time.\n* Here we will use Principal Component Analysis (PCA) with 'rbf' kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=None)\nx_train = pca.fit_transform(x_train)\n\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The sum of initial 5 values is \\t {0.47+0.11+0.09+0.06+0.06} , which is very good.\" )\nprint(\"So we will choose 5 number of features and reduce our training feature matrix to 5 features/columns. \")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=5)\nx_train = pca.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def all_models():    \n    # Multi-linear regression Model. \n    regressor_multi = LinearRegression()\n    regressor_multi.fit(x_train,y_train)\n    # Let us check the accuray\n    accuracy = cross_val_score(estimator=regressor_multi, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Multi-linear Regressor Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # Polynomial Regression\n    from sklearn.preprocessing import PolynomialFeatures\n    poly_reg=PolynomialFeatures(degree=4) #These 3 steps are to convert X matrix into X polynomial\n    x_poly=poly_reg.fit_transform(x_train) #matrix. \n    regressor_poly=LinearRegression()\n    regressor_poly.fit(x_poly,y_train)\n    # Let us check the accuray\n    accuracy = cross_val_score(estimator=regressor_poly, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Polynomial Regression Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # Random Forest Model\n    regressor_random = RandomForestRegressor(n_estimators=100,)\n    regressor_random.fit(x_train,y_train)\n    # Let us check the accuray\n    accuracy = cross_val_score(estimator=regressor_random, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Random Forest Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # SVR \n    regressor_svr = SVR(kernel='rbf')\n    regressor_svr.fit(x_train, y_train)\n    # Let us check the accuracy\n    accuracy = cross_val_score(estimator=regressor_svr, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the SVR Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # Decision Tress Model\n    regressor_deci = DecisionTreeRegressor()\n    regressor_deci.fit(x_train, y_train)\n    # Let us check the accuracy\n    accuracy = cross_val_score(estimator=regressor_deci, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Decision Tree Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    \n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us run all models together. If we have large dataset then we will not run all models together.\n# Then we will run one model at a time, otherwise your processor will struck down.\nall_models()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation:-\n* The best model is Multi-linear Regression.\n* In multi-linear Regressio, we are getting the accuracy of 31% and deviation of 54%.\n* The accuracy we  are getting is not that much good due many factors like less quantity of dataset, data not collected properly, something  wrong at the time of web scraping."},{"metadata":{},"cell_type":"markdown","source":"# IF THIS KERNEL IS HELPFUL, THEN PLEASE UPVOTE.\n<img src='https://drive.google.com/uc?id=17o_bxPmndgdL9Y6PPdcIXOBsJ0jizDNG' width=500 >"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}