{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import the dataset as a dataframe\ndf=pd.read_csv('/kaggle/input/glass/glass.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By above description we can see that the data is not scaled we will be using scaler "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(df['Type'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imbalanced data for types 3,5,6 and 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will generate synthetic samples using smote\nsampling = {1: 76, 2: 76, 3: 76,5:76,6:76,7:76}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #Visualize the Data\ndf.hist(bins=50, figsize=(15,15))\nplt.figure()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()\n#no NaN value in columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kernel density plot for analysis\ndef plot_skew_kurt(column_name):\n    from scipy import stats\n    from scipy.stats import skew,norm\n    sns.distplot(df[column_name],fit=norm);\n    plt.ylabel =('Frequency')\n    plt.title = (column_name+' Distribution');\n    #Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(df[column_name]);\n    #QQ plot\n    fig = plt.figure()\n    res = stats.probplot(df[column_name], plot=plt)\n    plt.show()\n    print(column_name+\" skewness: %f\" % df[column_name].skew())\n    print(column_name +\" kurtosis: %f\" % df [column_name].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_features=df.columns[:-1].tolist()\ndef plot_skew(columns_features):\n    for item in columns_features:\n        plot_skew_kurt(item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_skew(columns_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.pairplot(df,hue='Type')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef outlier_detec(feature):\n    data_mean, data_std,data_median = np.mean(df[feature]), np.std(df[feature]),np.median(df[feature])\n    cut_off = data_std * 3\n    #alculating the cut-off for identifying outliers as more than 3 standard deviations from the mean\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    # identify outliers\n    print(\"data_mean, data_std,data_median\",data_mean, data_std,data_median)\n    outliers = [x for x in df[feature] if x < lower or x > upper]\n    return outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for item in df.columns:\n    print(item,\"--->\",outlier_detec(item))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for log transforming the skewed data.\ndef log_transform(feature):\n    df[feature] = np.log1p(df[feature])\n    plot_skew_kurt(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming the data\nfor item in columns_features:\n    log_transform(item)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it can be seen that even after log-transformation the skewness is still present in some cases. \n*for now I will work on this only*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking correlation between features\ncorr = df[columns_features].corr()\nplt.figure(figsize=(11,11))\nsns.heatmap(corr, annot=True,\n           xticklabels= columns_features, yticklabels= columns_features, alpha = 0.7,   cmap= 'coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import (train_test_split,\n                                     cross_val_score, GridSearchCV)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n#to scale the data\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = df[columns_features] \ny_data = df.Type\nfrom collections import Counter\n#since out target data is imbalanced,  approach is to either oversample or undersample\n# since the data is very less< i will be doing oversampling\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.datasets import make_imbalance\nros = RandomOverSampler(sampling_strategy=sampling)\nX_data, y_data = ros.fit_resample(X_data, y_data)\nX_data = sc.fit_transform(X_data)\nX, X_test, y, y_test = train_test_split(X_data, y_data, test_size = 0.15 , random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using this code from some other developers. I forgot from where here I had taken this. *sorry* \ndef plot_pie(y):\n    target_stats = Counter(y)\n    labels = list(target_stats.keys())\n    sizes = list(target_stats.values())\n    explode = tuple([0.1] * len(target_stats))\n    def make_autopct(values):\n        def my_autopct(pct):\n            total = sum(values)\n            val = int(round(pct * total / 100.0))\n            return '{p:.2f}%  ({v:d})'.format(p=pct, v=val)\n        return my_autopct\n\n    fig, ax = plt.subplots()\n    ax.pie(sizes, explode=explode, labels=labels, shadow=True,\n           autopct=make_autopct(sizes))\n    ax.axis('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Information of the glass data set after making it '\n      'balanced by cleaning sampling: \\n sampling_strategy={} \\n y: {}'\n      .format(sampling, Counter(y_data)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pie(y_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"class type is balanced now"},{"metadata":{"trusted":true},"cell_type":"code","source":"#intializing the classifiers\nrfc=RandomForestClassifier(random_state=42)\n# gbc=GradientBoostingClassifier(random_state=42)\nlogreg=LogisticRegression(random_state=42)\nsvc=SVC(random_state=42)\nknn=KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.fit(X,y)\n#to find the important feature for the predictions\nrfc_features=rfc.feature_importances_\nrfc_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating param grids for GridSearch to find best possible hyperparameters\nimport numpy as np\nparam_grid_rfc = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,7,8],\n    'criterion' :['gini', 'entropy']\n}\n# param_grid_gbc = {\n#     \"learning_rate\": [0.01, 0.05, 0.15, 0.2],\n#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n#     \"n_estimators\":[200,500]\n# }\nparam_grid_logreg = {\n    \"C\":np.logspace(-3,3,7),\n    \"solver\":['lbfgs', 'liblinear', 'sag']\n}\nparam_grid_svc = {\n    'kernel': ['rbf'], \n    'gamma': [1e-2, 1e-3, 1e-4, 1e-5,'auto'],\n    'C': np.logspace(-3,3,7),\n    'decision_function_shape':('ovo','ovr'),\n    'shrinking':(True,False)\n}\nparam_grid_knn = {\n    'n_neighbors': [4,7,9,11], \n    'weights': ['uniform','distance'],\n    'metric':['euclidean','manhattan']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n#cross validation with hyperparameter tuning\ndef grid_searchCV(estimators,param_grids):\n    return GridSearchCV(estimator=estimators, param_grid=param_grids, cv= 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fetch CV for all models\ncv_rfc = grid_searchCV(rfc,param_grid_rfc)\n# cv_gbc = grid_searchCV(gbc,param_grid_gbc)\ncv_logreg = grid_searchCV(logreg,param_grid_logreg)\ncv_svc = grid_searchCV(svc,param_grid_svc)\ncv_knn=grid_searchCV(knn,param_grid_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting on whole training-data since there is a separate test-data available\ncv_rfc.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_logreg.fit(X, y)\ncv_svc.fit(X, y)\ncv_knn.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Tuned RFC Parameters: {}\".format(cv_rfc.best_params_)) \nprint(\"Best RFC Parameters score is {}\".format(cv_rfc.best_score_))\n\n# print(\"Tuned GBC Parameters: {}\".format(cv_gbc.best_params_)) \n# print(\"Best GBC Parameters score is {}\".format(cv_gbc.best_score_))\n\nprint(\"Tuned LogReg Parameters: {}\".format(cv_logreg.best_params_)) \nprint(\"Best LogReg Parameters score is {}\".format(cv_logreg.best_score_))\n\nprint(\"Tuned SVC Parameters: {}\".format(cv_svc.best_params_)) \nprint(\"Best SVC Parameters score is {}\".format(cv_svc.best_score_))\n\nprint(\"Tuned Knn Parameters: {}\".format(cv_knn.best_params_)) \nprint(\"Best knn Parameters score is {}\".format(cv_knn.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report,roc_curve,roc_auc_score\n#creating function for create classification report, model accuracy score and confusion matrix\ndef model_performance(ypred,y_test,model_name,model,X_test):\n    cnf=confusion_matrix(ypred,y_test)\n    print(\"confusion Matrix: \",cnf)\n    score=model.score(X_test,y_test)\n    print(\"accuracy-Score of \",model_name,\": \",score)\n    print(\"classification_report of \",model_name,\": \",classification_report(y_test, ypred))\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred_rfc=cv_rfc.predict(X_test)\n# ypred_gbc=cv_gbc.predict(X_test)\nypred_logreg=cv_logreg.predict(X_test)\nypred_svc=cv_svc.predict(X_test)\nypred_knn=cv_knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification Report of the models\nrfc_score=model_performance(ypred_rfc,y_test,'Random-Forest',cv_rfc,X_test)\n# gbc_score=model_performance(ypred_gbc,y_test,'Gradient-Boost',cv_gbc,X_test)\nlogreg_score=model_performance(ypred_logreg,y_test,'Logistic-Regression',cv_logreg,X_test)\nsvc_score=model_performance(ypred_svc,y_test,'Support Vector C',cv_svc,X_test)\nknn_score=model_performance(ypred_knn,y_test,'K-nearest neighbors',cv_knn,X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results= pd.DataFrame({'Models':['Logistic', 'RandomForrest','SVC','KNN'],\n                       'Score':[str('%.2f' % (logreg_score*100))+' %',str('%.2f' % (rfc_score*100))+' %'\n                                ,str('%.2f' % (svc_score*100))+' %' ,\n                                str('%.2f' % (knn_score*100))+' %' ]})\nresults.index=np.arange(1,len(results)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#by scores we can see that RFC outperformed other models and giving ~94 % of accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"prediction precision rate:\",round(cv_rfc.score(X_test,y_test),2)*100)\nresult=cv_rfc.predict(X_test)\nprint(\"predicted:\",result)\nmyarray = np.asarray(y_test.tolist())\nprint(\"original type:\",myarray)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}