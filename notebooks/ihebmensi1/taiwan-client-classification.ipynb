{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Thaiwan"},{"metadata":{},"cell_type":"markdown","source":"## 1) Data Cleaning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1- importation des bibliothéques pour la manipulation et la visualisation des données\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"help(pd.read_excel)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2- Importation de la base Bank of thaiwan\n\ntai=pd.read_csv(\"../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\",index_col=0)\ntai.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage de premier 5 client\ntai.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher le nombre de chaque type de clients\ntai[\"default.payment.next.month\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Affichage de dimension du Dataframe\ntai.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#la suppression des données manquants\n\ntai5=tai.dropna()\ndata_propre=tai.dropna()\ntai5.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## feature engineering "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regroupement des variables bill,paid et pay\nbill = ['BILL_AMT1', 'BILL_AMT2',\n       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\npaid = ['PAY_AMT1',\n       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\npay = ['PAY_0', 'PAY_2',\n       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher le nombre de chaque valeur dans Feature \"EDUCATION\"\ntai5.EDUCATION.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher le nombre de chaque valeur dans Feature \"EDUCATION\"\ntai5.MARRIAGE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remplacer les variable non attendues dans les deux colonnes \"EDUCATION\" et \"MARIAGE\"    \nfil = (tai5.EDUCATION == 5) | (tai5.EDUCATION == 6) | (tai5.EDUCATION == 0)\ntai5.loc[fil, 'EDUCATION'] = 4\n\nfil1 = (tai5.MARRIAGE== 0)\ntai5.loc[fil1, 'MARRIAGE'] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tai5.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Affichage des colonnes de dataframe tai\ntai.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Faire centrer et réduire les valeurs des données tai5\ncol_to_norm = ['LIMIT_BAL','BILL_AMT1', 'BILL_AMT2',\n       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\ntai5[col_to_norm] = tai5[col_to_norm].apply(lambda x : (x-np.mean(x))/np.std(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tai5.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tai5.PAY_0.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyse descriptive des données"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Réaliser une analyse descriptive\ndata_propre.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tester la variation entre la moyenne et la médiane\nabs(   (data_propre.mean() - data_propre.median()) / (data_propre.mean()) )*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualiser les points abérants\ndata_propre.boxplot(figsize=(18,8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# limiter 3 chiffres aprés la vigule\npd.options.display.float_format = '{:,.3f}'.format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage de la corrélation entre les variables\ndata_propre.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualiser la corrélation enutilisant heatmap sans les variables encodées\nplt.figure(figsize=(25,15))\nsns.heatmap(data_propre.corr() , annot=True)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data Seslection "},{"metadata":{},"cell_type":"markdown","source":"#### 1) Méthode de KBest "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appliquer une fonction qui permet de la décomposition des données\n# en train et test avec une division de 0.2\ndef get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n# Suppression de variable cible \"default payment next month\"\nfeature_cols = tai5.columns.drop('default.payment.next.month')\ntrain, valid, _ = get_data_splits(tai5)\n\n# Appliquer la méthode \"SelectKBest\" en gardant que les 8 colonnes qui expliquent l'information le mieux\n\nselector = SelectKBest(f_classif, k=8)\n\nX_new = selector.fit_transform(train[feature_cols], train['default.payment.next.month'])\nX_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Afficher dans un dataframe les meilleurs colonnes  , les autres colonnes ont des valeurs de 0\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train.index, \n                                 columns=feature_cols)\nselected_features.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Garder que les colonnes qui ont des valeurs non nuls, on obtient alors que les meilleurs colonnes \nselected_columns = selected_features.columns[selected_features.var() != 0]\n\n\ntrain[selected_columns].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Méthode de L1regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe \"warnings\" qui permet d'ignorer les erreurs de gravité \"warnings\"\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n# On fait la division des données\ntrain, valid, _ = get_data_splits(tai5)\n\nX1, y1 = train[train.columns.drop(\"default.payment.next.month\")], train['default.payment.next.month']\n\n# On choisit lasso-régression pour filtrer les colonnes\nlogistic = LogisticRegression(C=1, penalty=\"l1\", random_state=7).fit(X1, y1)\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(X1)\nX_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage des colonnes séléctionnés dans un dataframe\nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X1.index,\n                                 columns=X1.columns)\n#Suppression des donneés qui ont des valeurs nuls\nselected_columns = selected_features.columns[selected_features.var()!=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Découpage des données en variables explicatives et variables expliquées\nX = tai5.iloc[:,:23]  \ny = tai5.iloc[:,23]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe StandardScaler pour l'échantionnage\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appliquer la méthode pour le dataframe x des variables explicatives\nss.fit(X.values)\nmatriceTCL = ss.transform(X.values)\n#Afficher les données aprés remise a l'echellle avec standarscaler\ndataTCL =pd.DataFrame(matriceTCL , columns=X.columns)\ndataTCL.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher la forme des données de chaque feature en utilisant \"hist\"\npd.DataFrame(matriceTCL).hist(figsize=(15,13));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### la méthode Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe ExtraTreesClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Diviser les données pour l'aprentissage et le test afin d'avoir une meilleur résultat\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Faire l'apprentissage des données en utilisant le model ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X_train,y_train)\n#Afficher \"feature_importances\" pour chaque colonne\nprint(model.feature_importances_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualiser \"feature_importance\" dans un graphe\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(10).plot(kind='barh',color=['pink','black', 'coral', 'blue','red', 'green', 'cyan','y','orange','lime'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Méthode de PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe de PCA\nfrom sklearn.decomposition import PCA\n#initialiser la méthode PCA\npca = PCA(0.95)\n\n# fit and transform les données explicatives seulement\ndatapca = pca.fit(dataTCL.iloc[:,1:])\n# Afficher les pourcentages de variance expliquée de chaque feature\nval = pd.Series(datapca.explained_variance_ratio_)\nval.plot(kind='bar', title=\"graphes des valeurs propres\")\nplt.show()\n#  Création d'un dataframe qui contient les valeurs de chaque composants \n# Initiation pour afficher la cercle de corrélation\ncoef = np.transpose(pca.components_)\ncols = ['PC-'+str(x) for x in range(len(val))]\npc_infos = pd.DataFrame(coef, columns=cols, index=dataTCL.iloc[:,1:].columns)\npca.n_components_\ndatapca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage de la cercle de corrélation\n# Réalisation d'un graphe qui contient un cercle\nplt.Circle((0,0),radius=10, color='g', fill=False)\ncircle1=plt.Circle((0,0),radius=1, color='g', fill=False)\n# Ajouter les axes et donner la limite pour chaque axe\nfig, axes= plt.subplots(figsize=(10,10))\naxes.set_xlim(-1,1)\naxes.set_ylim(-1,1)\nfig.gca().add_artist(circle1)\nplt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\nplt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\naxes.add_artist(circle1)\n# Affichage de chaque variable explicative dans la cercle\nfor idx in range(len(pc_infos[\"PC-0\"])):\n    x = pc_infos[\"PC-0\"][idx]\n    y = pc_infos[\"PC-1\"][idx]\n    plt.plot([0.0,x],[0.0,y],'k-')\n    plt.plot(x, y, 'rx')\n    plt.annotate(pc_infos.index[idx], xy=(x,y))\nplt.xlim((-1,1))\nplt.ylim((-1,1))\nplt.title(\"Circle of Correlations\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Aprés la visualisation de la  corrélation entre les Features et l'étape Feature Selection, nous avons conclus que les features qui expriment bien la variable cible sont :\n####  'LIMIT_BAL','PAY_0', 'PAY_2','PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'"},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"tai5.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clos=['LIMIT_BAL', 'PAY_0', 'PAY_2',\n       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\ntai5[clos].head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = tai5.iloc[:,23] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Diviser les données pour l'apprentissage et le test\nfrom sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(tai5[clos].values,y.values , test_size = 0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nerror = []\n\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(i)\n    knn_model = knn.fit(X_train1, y_train1)\n    pred_i = knn_model.predict(X_test1)\n    error.append(np.mean(pred_i != y_test1))\nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Taux Erreur pour les differentes valeurs de k')\nplt.xlabel('K ')\nplt.ylabel('Erreur')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n# Appliquer la meilleur valeur de k=2\nknn1 = KNeighborsClassifier(28)\n# Faire l'étape d'apprentissage  \nknn_model1 = knn.fit(X_train1, y_train1)\n# réaliser la prédiction de X_test1\ny_pred_knn1 =knn_model1.predict(X_test1)\n# Afficher l'accuracy de prédiction\nknn_score=knn_model1.score(X_test1,y_test1)\nknn_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe accuracy_score\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test1, y_pred_knn1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la matrice de confusion\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test1, y_pred_knn1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test1, y_pred_knn1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1- DecisionTreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe \"DecisionTreeClassifier\"\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n# Faire l'étape d'apprentissage  \ndtc_model = DecisionTreeClassifier().fit(X_train1, y_train1)\n# réaliser la prédiction de X_test1\ny_pred_dtc = dtc_model.predict(X_test1)\n# Afficher l'accuracy de prédiction\ndtc_score=dtc_model.score(X_test1,y_test1)\ndtc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe accuracy_score\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test1, y_pred_dtc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test1, y_pred_dtc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2- RandomForestClassifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe \"RandomForestClassifier\"\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(max_depth=30,\n min_samples_leaf= 1,\n min_samples_split= 2,\n n_estimators= 100)\n# Faire l'étape d'apprentissage  \nrfc_model = rfc.fit(X_train1, y_train1)\n# réaliser la prédiction de X_test1\ny_pred_rfc = rfc_model.predict(X_test1)\n# Afficher l'accuracy de prédiction\nrfc_score=rfc_model.score(X_test1,y_test1)\nrfc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe accuracy_score\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test1, y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la matrice de confusion\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test1, y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test1, y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe \"LogisticRegression\"\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n# Faire l'étape d'apprentissage  \nlr_model = lr.fit(X_train1, y_train1)\n# réaliser la prédiction de X_test1\ny_pred_lr = lr_model.predict(X_test1)\n# Afficher l'accuracy de prédiction\nlr_score=lr_model.score(X_test1,y_test1)\nlr_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la matrice de confusion\nprint(confusion_matrix(y_test1, y_pred_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nprint(classification_report(y_test1, y_pred_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tester la prédiction d'un nouveau donnée\ndf = np.array([20000,2,2,1,24,2,2,-1,-1,-1,-1,3919,3102,689,0,0,0,0,689,0,0,0,0]).reshape(1,23)\ndf2 = np.array([20000,2,2,2,4,6,1]).reshape(1,7)\nlr_model.predict(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe \"GaussianNB\"\nfrom sklearn.naive_bayes import GaussianNB\nmodel_NB= GaussianNB()\n# Faire l'étape d'apprentissage  \nmodel_naive=model_NB.fit(X_train1,y_train1)\n# réaliser la prédiction de X_test1\ny_pred_nb =model_naive.predict(X_test1)\n# Afficher l'accuracy de prédiction\nnb_score=model_naive.score(X_test1,y_test1)\nnb_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la matrice de confusion\nprint(confusion_matrix(y_test1, y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nprint(classification_report(y_test1, y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3) Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe \"SVC\"\nfrom sklearn.svm import SVC\nmodel_svm= SVC(gamma='auto',C= 20, kernel='rbf')\n# Faire l'étape d'apprentissage \nmodel_svm1=model_svm.fit(X_train1,y_train1)\n# réaliser la prédiction de X_test1\ny_pred_svm = model_svm.predict(X_test1)\n# Afficher l'accuracy de prédiction\nsvm_score=model_svm1.score(X_test1,y_test1)\nsvm_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la matrice de confusion\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test1, y_pred_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nprint(classification_report(y_test1, y_pred_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tester la prédiction d'un nouveau donnée\ndf = np.array([210000,1,1,2,29,-2,-2,-2,-2,-2,-2,0,0,0,0,0,0,0,0,0,0,0,0]).reshape(1,23)\ndf2 = np.array([20000,2,2,2,4,6,1]).reshape(1,7)\n\n\nmodel_svm1.predict(df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4) XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe \"XGBClassifier\"\nfrom xgboost import XGBClassifier\nmodel_xgboost= XGBClassifier()\n# Faire l'étape d'apprentissage \nmodel_xgboost.fit(X_train1,y_train1)\n# réaliser la prédiction de X_test1\ny_pred_xgb = model_xgboost.predict(X_test1)\n# Afficher l'accuracy de prédiction\nmodel_xgboost.score(X_test1,y_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_score=model_xgboost.score(X_test1,y_test1)\nxgb_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la matrice de confusion\nprint(confusion_matrix(y_test1, y_pred_xgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nprint(classification_report(y_test1, y_pred_xgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tester la prédiction d'un nouveau donnée\ndf = np.array([20000,2,2,1,24,2,2,-1,-1,-1,-1,3919,3102,689,0,0,0,0,689,0,0,0,0]).reshape(1,23)\ndf2 = np.array([20000,2,2,2,4,6,1]).reshape(1,7)\nmodel_xgboost.predict(df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ADABOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe \"AdaBoostClassifier\"\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nmodel_ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n                         algorithm=\"SAMME\",\n                         n_estimators=500, learning_rate=0.8)\n# Faire l'étape d'apprentissage \nmodel_ada.fit(X_train1, y_train1)\n# réaliser la prédiction de X_test1\ny_pred_ada = model_ada.predict(X_test1)\nmodel_ada.score(X_test1,y_test1)\n# Afficher l'accuracy de prédiction\nada_score=model_ada.score(X_test1,y_test1)\nada_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la matrice de confusion\nprint(confusion_matrix(y_test1, y_pred_ada))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher \"classification_report\"\nprint(classification_report(y_test1, y_pred_ada))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tester la prédiction d'un nouveau donnée\ndf = np.array([20000,2,2,1,24,2,2,-1,-1,-1,-1,3919,3102,689,0,0,0,0,689,0,0,0,0]).reshape(1,23)\ndf2 = np.array([20000,2,2,2,4,6,1]).reshape(1,7)\nmodel_ada.predict(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Courbe ROC de différents modéle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer les méthodes \"roc_curve, auc\"\n%matplotlib inline\n\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création d'un tuple qui contient false positive rate, trus positive rate \nfpr1, tpr1, threshold1 = roc_curve(y_test1, y_pred_dtc) \nroc_auc1 = auc(fpr1, tpr1)\nfpr2, tpr2, threshold2 = roc_curve(y_test1, y_pred_rfc) \nroc_auc2 = auc(fpr2, tpr2)\nfpr3, tpr3, threshold3 = roc_curve(y_test1, y_pred_knn1)\nroc_auc3 = auc(fpr3, tpr3)\nfpr4, tpr4, threshold4 = roc_curve(y_test1, y_pred_svm)\nroc_auc4 = auc(fpr4, tpr4)\nfpr5, tpr5, threshold5 = roc_curve(y_test1, y_pred_xgb)\nroc_auc5 = auc(fpr5, tpr5)\nfpr6, tpr6, threshold6 = roc_curve(y_test1, y_pred_ada)\nroc_auc6= auc(fpr6, tpr6)\nfpr7, tpr7, threshold7 = roc_curve(y_test1, y_pred_lr)\nroc_auc7= auc(fpr7, tpr7)\nfpr8, tpr8, threshold8 = roc_curve(y_test1, y_pred_nb)\nroc_auc8= auc(fpr8, tpr8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10)) \nplt.plot(fpr1, tpr1, color='navy', lw=2, label='CART ROC curve (area = %0.2f)'% roc_auc1)\nplt.plot(fpr2, tpr2, color='green', lw=2, label='Random Forest ROC curve (area = %0.2f)'% roc_auc2)\nplt.plot(fpr3, tpr3, color='yellow', lw=2, label='kNN ROC curve (area = %0.2f)'% roc_auc3)\nplt.plot(fpr4, tpr4, color='orange', lw=2, label='SVM ROC curve (area = %0.2f)'% roc_auc4)\nplt.plot(fpr5, tpr5, color='purple', lw=2, label='XGBOOST ROC curve (area = %0.2f)'% roc_auc5)\nplt.plot(fpr6, tpr6, color='black', lw=2, label='ADABOOST ROC curve (area = %0.2f)'% roc_auc6)\nplt.plot(fpr7, tpr7, color='lime', lw=2, label='LogisticR ROC curve (area = %0.2f)'% roc_auc7)\nplt.plot(fpr8, tpr8, color='cyan', lw=2, label='NaiveB ROC curve (area = %0.2f)'% roc_auc8)\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--') \nplt.xlim([0.0, 1.0]) \nplt.ylim([0.0, 1.05]) \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.title('Classifiers ROC curves') \nplt.legend(loc = \"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création de fichier pickle  contient les models traités au-dessus\nimport pickle\nwith open('taiwan_final.pkl','wb') as file:\n    pickle.dump([knn_model1,dtc_model,rfc_model,lr_model,model_naive,model_svm1,model_xgboost,model_ada],file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création de fichier pickle contient false positive rate, true positive rate et roc_auc\nimport pickle\nwith open('taiwan_final_roc.pkl','wb') as file:\n    pickle.dump([fpr1, tpr1,roc_auc1,fpr2, tpr2,roc_auc2,fpr3, tpr3,roc_auc3,fpr4, tpr4,roc_auc4,fpr5, tpr5,roc_auc5,fpr6, tpr6,roc_auc6,fpr7, tpr7,roc_auc7,fpr8, tpr8,roc_auc8],file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création de fichier pickle contient le score de chaque modéle traité\nimport pickle\nwith open('taiwan_final_score.pkl','wb') as file:\n    pickle.dump([knn_score,dtc_score,rfc_score,lr_score,nb_score,svm_score,xgb_score,ada_score],file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ouvrir un fichier pickle\nwith open('taiwan_final.pkl','rb') as f:\n    ma= pickle.load(f)\nma[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Les modéles non Supervisé de clustering "},{"metadata":{},"cell_type":"markdown","source":"### Kmeans"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe kmeans\nfrom sklearn.cluster import KMeans\n#Utilise la méthode elbow pour connaitre la meilleur valeur de k\nsse = []\nk_rng = range(1,10)\nfor k in k_rng:\n    km = KMeans(n_clusters=k)\n    km.fit(X)\n    sse.append(km.inertia_)\n    print (km.inertia_)\nplt.xlabel('K')\nplt.ylabel('Sum of squared error')\nplt.plot(k_rng,sse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la méthode Kmeans pour k=2\nModel1 = KMeans(n_clusters=2)\nModel1.fit(data_propre.iloc[:,1:])\nModel1.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#faire identifier la classe de chaque valeur en utilisant crosstab\npd.crosstab(y,Model1.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CAH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importer la classe CAH\nfrom scipy.cluster.hierarchy import dendrogram, linkage,fcluster,set_link_color_palette\n# Faire la liaison entre tous les données en utilisant la fonction linkage()\nZ= linkage(X,method='ward',metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"CHA\") \n#Afficher la liaison obtenu avec dendogramme\ndendrogram(Z,labels=X.index,orientation='left',color_threshold=0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choisir un threshhold qui permet de diviser les données en deux classes\nplt.title('CAH avec matérialisation des 2 classes') \ndendrogram(Z,labels=X.index,orientation='left',color_threshold=1500\n          ) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer la ségmentation en utilisant fcluster\nclusters = fcluster(Z,criterion='distance', t=1500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#faire identifier la classe de chaque valeur en utilisant crosstab\npd.crosstab(y,clusters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DBSCAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importer la classe DBSCAN,metric\nfrom sklearn.cluster import DBSCAN \nfrom sklearn import metrics \nfrom sklearn.datasets.samples_generator import make_blobs \n\n  \n# Appliquer DBSCAN \ndb = DBSCAN(eps=0.3, min_samples=10).fit(X) \ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool) \ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_ \n  \n# Identifier le nombre de clusters   \nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) \n  \nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliquer DBSCAN au données échantilooné\nDBSCANModel = DBSCAN(metric='euclidean',eps=0.25,min_samples=10,algorithm='auto').fit(X.values)\nDBSCANModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}