{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Defining the problem\n\nOur goal in this notebook is to classify clients who are and are not willing to subscribe to a term deposit based on the given dataset (target column 'y')\n\nHere is the plan:\n1. Data Engineering: check data correctness, fill unknown data cells, mofidy and convert data properties for calculation\n2. Exploratory Data Analysis: analyzing data to filter out some main patterns or characteristics\n3. Training Models\n4. Evaluation","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Engineering\n\nImporting libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/banking-dataset-marketing-targets/train.csv\", sep =\";\")\ndata.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two things can be seen here:\n\n1. Dataset contains two datatypes: int64 (numerical) and object which is expressed as a string, knowing datatype helps us with using correct operations on correct data columns later on\n2. There is no null value (every columns has 45211 non-null values)","metadata":{}},{"cell_type":"markdown","source":"Check if there are any outliers (for example age value > 100).","metadata":{}},{"cell_type":"code","source":"data.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But seems like for numerical values according to min and max value all of them are reasonable values.\n\nIt can be concluded that numerical columns are complete because there is no null values and no outliers.\n\nWe will now check validity of string values","metadata":{}},{"cell_type":"code","source":"stringdata = data.select_dtypes(include=\"object\")\nfor column in stringdata:\n    print(stringdata[column].value_counts())\n    print (\"-\" * 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns job, education, contact and poutcome have \"unknown\" values, which have to be filled.\n\nJob column: Ignore the 288 rows with unknown job because firstly there seems to be no reasonable values to be replaced (\"blue collar\" and \"management\" frequencies are only about 25%), secondly this is a small amount of data comparing to our dataset size;\n\nPoutcome column: Ignore this property since most of data are unknown (36,959 out of 45,211);\n\nEducation and Contact columns will be filled with the reasonable most frequent values","metadata":{}},{"cell_type":"code","source":"data = data[data['job'] != 'unknown']\n\ndata.drop('poutcome', axis = 1, inplace = True)\n\ndata['education'].replace(\"unknown\", data['education'].mode()[0], inplace = True)\ndata['contact'].replace(\"unknown\", data['contact'].mode()[0], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two versions of data are created:\n1. data_visual is for exploratory data analysis\n2. data_calc is for calculations ","metadata":{}},{"cell_type":"code","source":"data_visual = data.copy(deep = True)\ndata_visual['y'].replace(\"no\", 0, inplace = True)\ndata_visual['y'].replace(\"yes\", 1, inplace = True)\ndata_visual","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intdata = data.select_dtypes(include=\"int64\")\nfor column in intdata:\n    data[column + \"_bin\"] = pd.cut(data[column], 8)\n    data.drop(column, axis = 1, inplace = True)\n    \nlabel = LabelEncoder()\ndata_calc = pd.DataFrame()\nfor column in data:\n    data_calc[column] = label.fit_transform(data[column])\n    \ndata_calc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis\n\nIn this part we will anaylize data by going through several graphs","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (17, 5))\nsns.distplot(data_visual.loc[data_visual.y == 0, 'age'], label = \"Not Subscribed\", hist = False)\nsns.distplot(data_visual.loc[data_visual.y == 1, 'age'], label = \"Subscribed\", hist = False)\nplt.title(\"Age Distribution by Subscription\")\n\nplt.figure(figsize = (17, 5))\nsns.distplot(data_visual.loc[data_visual.y == 0, 'duration'], label = \"Not Subscribed\", hist = False)\nsns.distplot(data_visual.loc[data_visual.y == 1, 'duration'], label = \"Subscribed\", hist = False)\nplt.title(\"Duration of Last Time Contact Distribution by Subscription\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be concluded that 60 year and older clients, young people around 20 year old and clients whose last time contact was longer than 500 seconds tends to agree to subscribe term deposit","metadata":{}},{"cell_type":"code","source":"plt.figure( figsize = (20, 5))\nsns.barplot(data = data_visual, x = 'job', y = 'y')\nplt.xlabel(\"Job\", fontsize = 14)\nplt.ylabel(\"Probability\", fontsize = 14)\nplt.title(\"Subscribe Probability by Job\", fontsize = 14)\n\nplt.figure( figsize = (20, 5))\nplt.subplot(121)\nsns.barplot(data = data_visual, x = 'marital', y = 'y')\nplt.xlabel(\"Marital Situation\", fontsize = 14)\nplt.ylabel(\"Probability\", fontsize = 14)\nplt.title(\"Subscribe Probability by Marital Situation\", fontsize = 14)\n\nplt.subplot(122)\nsns.barplot(data = data_visual, x = 'education', y = 'y')\nplt.xlabel(\"Education\", fontsize = 14)\nplt.ylabel(\"Probability\", fontsize = 14)\nplt.title(\"Subscribe Probability by Education\", fontsize = 14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be concluded that, groups of students and retired people, single and people with higher education tend to subscribe the term deposit ","metadata":{}},{"cell_type":"code","source":"plt.figure( figsize = (20, 8))\nsns.violinplot(x = 'job', y = 'age', hue = 'y', data = data_visual, split = True)\nplt.xlabel(\"Job\", fontsize = 14)\nplt.ylabel(\"Age\", fontsize = 14)\nplt.title(\"Age Distribution by Job, Divided by Subscription (0) for Not Subscribed, (1) for Subscribed\", fontsize = 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In every job older people, especially Housmaid tend to accept term deposit subscription","metadata":{}},{"cell_type":"code","source":"plt.figure( figsize = (20, 8))\n\nplt.subplot(121)\nsns.boxenplot(x = 'housing', y = 'age', hue = 'y', data = data_visual)\nplt.xlabel(\"Housing\", fontsize = 14)\nplt.ylabel(\"Age\", fontsize = 14)\nplt.title(\"Age Distribution by Housing Loan\", fontsize = 15)\n\nplt.subplot(122)\nsns.boxenplot(x = 'loan', y = 'age', hue = 'y', data = data_visual)\nplt.xlabel(\"Loan\", fontsize = 14)\nplt.ylabel(\"Age\", fontsize = 14)\nplt.title(\"Age Distribution by Personal Loan\", fontsize = 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color = sns.diverging_palette(250, 6, as_cmap = True)\n\nplt.figure(figsize = (14, 10))\nsns.heatmap(data_visual.corr(), cmap = color, annot = True)\nplt.title(\"Features Correlation\", fontsize = 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this Heatmap we can conclude a lot of things, for example:\n1. \"duration\" is the most positive correlated feature to Target y\n2. \"campaign\" is the most negative correlated feature to Target y\n3. \"previous\" and \"pdays\" are positive correlated to each other\n","metadata":{}},{"cell_type":"markdown","source":"# 3. Training Models\nThe plan is, we devide our dataset into train data and test data, and use different machine learning algorithms to train models","metadata":{}},{"cell_type":"code","source":"trainx, testx, trainy, testy = model_selection.train_test_split(data_calc.loc[:, data_calc.columns != 'y'], data_calc['y'], random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MLA = [\n       ensemble.AdaBoostClassifier(),\n       ensemble.BaggingClassifier(),\n       ensemble.GradientBoostingClassifier(),\n       ensemble.RandomForestClassifier(),\n       linear_model.LogisticRegressionCV(),  \n       linear_model.SGDClassifier(),\n       naive_bayes.GaussianNB(),\n       neighbors.KNeighborsClassifier(),\n       tree.DecisionTreeClassifier(),\n       tree.ExtraTreeClassifier(),\n]\n\nname = []\ntestscore = []\nfor alg in MLA:\n    name.append(alg.__class__.__name__)\n    alg.fit(trainx, trainy)\n    testscore.append(alg.score(testx, testy))\n    \ncomparison = pd.DataFrame({\"name\": name, \"testscore\": testscore})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluation\n\nNow we test our models on test datas and sort them","metadata":{}},{"cell_type":"code","source":"comparison = comparison.sort_values(by = \"testscore\", ascending = False)\ncomparison","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gradient Boosting Classifier is the best performed model with exactibility 88,79%","metadata":{}}]}