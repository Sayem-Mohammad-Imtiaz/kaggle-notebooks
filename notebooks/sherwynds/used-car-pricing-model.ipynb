{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ðŸš˜ Used Car Pricing Model\n\n**Goal:** The goal of this project is to estimate the resale price of a used Ford Focus using regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import and Read Data\n\nThe first step is to import necessary libraries and read the raw data into a Pandas DataFrame.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('../input/used-car-dataset-ford-and-mercedes/focus.csv')\nprint('Raw Data')\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 6 input features: `year`, `transmission`, `mileage`, `fuelType`, and `engineSize`. The `model` column can be dropped because it is constant across each row. The `price` column is the output ($y$) variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Convert Categorical Data to Dummy Values\n\nBefore running regression, the *categorical (nominal) data* must first be converted to *dummy values*. Dummy values indicate the absence or presence of a feature with a `0` or `1` value. For nominal data with $k$ possible values, we create $k-1$ dummy variables. The `drop_first=True` parameter drops the first of the $k$ possible values, leading to $k-1$ features. The two nominal features that must be converted are `transmission` and `fuelType`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"transmission = pd.get_dummies(data['transmission'], drop_first=True)\nfuelType = pd.get_dummies(data['fuelType'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Organize the DataFrame\n\nThe following step joins dummy variables, drops unnecessary features, and rearranges features in the DataFrame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns=['model', 'transmission', 'fuelType'])\ndata = data.join(transmission)\ndata = data.join(fuelType)\ndata.rename(columns={'year': 'year', 'price': 'price', 'mileage': 'mileage', 'engineSize': 'engine_size', 'Manual': 'manual', 'Semi-Auto': 'semi_auto', 'Petrol': 'petrol'}, inplace=True)\ndata = data[['year', 'mileage', 'engine_size', 'manual', 'semi_auto', 'petrol', 'price']]\nprint('Organized Data')\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split into Train/Test Data\n\nThe data must be split into a *training set* to build the model and a *test set* to evaluate its effectiveness. Here, 75% of the data is used to train and 25% is used to test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = data[['year', 'mileage', 'engine_size', 'manual', 'semi_auto', 'petrol']]\ny = data['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\nX_train = X_train.to_numpy()\nX_test = X_test.to_numpy()\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()\n\nprint('X_train\\n' + str(X_train[:4,:]) + '\\n')\nprint('y_train\\n' + str(y_train[:4]) + '\\n')\nprint('X_test\\n' + str(X_test[:4,:]) + '\\n')\nprint('y_test\\n' + str(y_test[:4]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 1: Simple Polynomial Regression\n\nThe first model will use just one numeric variable: `mileage`, to predict the output variable: `price`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualize the Shape of the Data\n\nIn order to develop an intuition around which function will fit the data best, it is helpful to visualize the relationship between $x$ (`mileage`) and $y$ (`price`) in the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select all rows from the second (mileage) column of the numpy arrays\nX_train_mileage = X_train[:,1]\nX_test_mileage = X_test[:,1]\n\nprint('X_train_mileage: ' + str(X_train_mileage))\nprint('y_train: ' + str(y_train))\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\noutput_notebook()\n\ntrain_plot = figure(title='Training Data', x_axis_label='Mileage', y_axis_label='Price')\ntrain_plot.circle(x=X_train_mileage, y=y_train, color='blue')\nshow(train_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is either cubic or close to the function $\\frac{1}{\\log(x)}$. Each curve is then plotted to determine the optimal model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cubic Model\n\nThe first model is a cubic one. That means it takes the form $y = ax^3 + bx^2 + cx + d$ for constants $a, b, c, d$, where $x$ is the `mileage` variable and $y$ is the predicted price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nFirst, the single feature $x$, is transformed into multiple featuers $x^0, x^1, x^2, x^3$.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\ncubic = PolynomialFeatures(degree=3)\nX_train_mileage_cubic = cubic.fit_transform(X_train_mileage[:,None])\nX_test_mileage_cubic = cubic.fit_transform(X_test_mileage[:,None])\n\nprint('X_train_mileage_cubic \\n' + str(X_train_mileage_cubic[:4,:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, linear regression is run to fit the model and generate predictions for the train and test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\ncubic_model = linear_model.LinearRegression()\ncubic_model.fit(X_train_mileage_cubic, y_train)\n\ntrain_predictions_cubic = cubic_model.predict(X_train_mileage_cubic)\ntest_predictions_cubic = cubic_model.predict(X_test_mileage_cubic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, predicted results are plotted against the train and test data and assigned an $R^2$ value (measure of how close the data fit the model).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.layouts import row\n\ntrain_plot_cubic = figure(title='Train Data', x_axis_label='Mileage', y_axis_label='Price')\ntest_plot_cubic = figure(title='Test Data', x_axis_label='Mileage', y_axis_label='Price')\n\ntrain_plot_cubic.circle(x=X_train_mileage, y=y_train, color='blue', legend_label='Actual')\ntrain_plot_cubic.circle(x=X_train_mileage, y=train_predictions_cubic, color='red', legend_label='Predicted')\n\ntest_plot_cubic.circle(x=X_test_mileage, y=y_test, color='blue', legend_label='Actual')\ntest_plot_cubic.circle(x=X_test_mileage, y=test_predictions_cubic, color='red', legend_label='Predicted')\n\nshow(row(train_plot_cubic,test_plot_cubic))\n\nprint('Train Data R-squared: ' + str(cubic_model.score(X_train_mileage_cubic,y_train)))\nprint('Test Data R-squred: ' + str(cubic_model.score(X_test_mileage_cubic,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reciprocal Logarithmic Model\n\nThe second approach is a model similar in shape to $\\frac{1}{\\log(x)}$.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, the data is transformed to match the reciprocal $\\log$ function $\\frac{1}{\\log(x)}$.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_mileage_rl = X_train_mileage[:,None]\nX_test_mileage_rl = X_test_mileage[:,None]\n\ndef addInvLogFeatures(numeric):\n    log_feats = numeric.copy()\n    valid = (log_feats != 1) & (log_feats > 0)\n    log_feats[valid] = np.log(log_feats[valid]) / np.log(10)\n    log_feats[log_feats <= 0] = 1e-10\n    inv_log_feats = 1 / log_feats\n    return np.hstack([numeric, inv_log_feats, numeric * inv_log_feats])\n\nX_train_mileage_rl = addInvLogFeatures(X_train_mileage_rl)\nX_test_mileage_rl = addInvLogFeatures(X_test_mileage_rl)\n\nprint('X_train_mileage_rl \\n' + str(X_train_mileage_rl[:4,:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, linear regression is run to fit the model and generate predictions for the train and test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rl_model = linear_model.LinearRegression()\nrl_model.fit(X_train_mileage_rl, y_train)\n\ntrain_predictions_rl = rl_model.predict(X_train_mileage_rl)\ntest_predictions_rl = rl_model.predict(X_test_mileage_rl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And again, predicted results are plotted against the train and test data and assigned an $R^2$ value (measure of how close the data fit the model).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_plot_rl = figure(title='Train Data', x_axis_label='Mileage', y_axis_label='Price')\ntest_plot_rl = figure(title='Test Data', x_axis_label='Mileage', y_axis_label='Price')\n\ntrain_plot_rl.circle(x=X_train_mileage, y=y_train, color='blue', legend_label='Actual')\ntrain_plot_rl.circle(x=X_train_mileage, y=train_predictions_rl, color='red', legend_label='Predicted')\n\ntest_plot_rl.circle(x=X_test_mileage, y=y_test, color='blue', legend_label='Actual')\ntest_plot_rl.circle(x=X_test_mileage, y=test_predictions_rl, color='red', legend_label='Predicted')\n\nshow(row(train_plot_rl,test_plot_rl))\n\nprint('Train Data R-squared: ' + str(rl_model.score(X_train_mileage_rl,y_train)))\nprint('Test Data R-squred: ' + str(rl_model.score(X_test_mileage_rl,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Principle Component Analysis (PCA)\n\n*Principal Component Analysis* flattens multiple numeric features into fewer features (known as components) that preserve the most important information from the original data. This allows for easy visualization of the data to get an intuition of which function would be best applied.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nX_train_numeric = X_train[:,:3]\nX_test_numeric = X_test[:,:3]\n\nscaler = StandardScaler()\nX_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\nX_test_numeric_scaled = scaler.fit_transform(X_test_numeric)\n\npca = PCA(n_components=1)\nX_train_pca = pca.fit_transform(X_train_numeric_scaled)\n\ntrain_plot_pca = figure(title='Principal Component Analysis', x_axis_label='Principal Component', y_axis_label='price')\ntrain_plot_pca.circle(x=X_train_pca[:,0], y=y_train, color='blue')\nshow(train_plot_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot suggests that both cubic and reciprocal logarithmic functions might be good fits for the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model 2: Multiple Polynomial Regression\n\nThe final model will use all numeric variables: `year`, `mileage`, `engine_size`, and dummy variables `manual`, `semi_auto`, `petrol`, to predict the output variable: `price`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cubic Model\nThe first model is a cubic one. That means it takes the form $y = ax^3 + bx^2 + cx + d$ for constants $a, b, c, d$. This time however, every variable is used as input, and not just `mileage`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, each numeric feature is transformed into polynomial features of degree 3.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cubic = cubic.fit_transform(X_train_numeric)\nX_test_cubic = cubic.fit_transform(X_test_numeric)\n# print(\"X_train_cubic \\n\" + str(X_train_cubic[:4,:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before running regression to fit the model, the features are scaled to make the orders of magnitude roughly the same. They are then combined with the ordinal features from before.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cubic_scaled = scaler.fit_transform(X_train_cubic)\nX_test_cubic_scaled = scaler.fit_transform(X_test_cubic)\n\nX_train_nominal = X_train[:,3:]\nX_test_nominal = X_test[:,3:]\n\nX_train_cubic_full = np.hstack([X_train_cubic_scaled, X_train_nominal])\nX_test_cubic_full = np.hstack([X_test_cubic_scaled, X_test_nominal])\n\n# print(\"X_train_cubic_full \\n\" + str(X_train_cubic_full[:4,:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, the cubic model is fit using `RidgeCV` - an advanced form of linear regression that *regularizes* data to prevent overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cubic_model = linear_model.RidgeCV()\ncubic_model.fit(X_train_cubic_full, y_train)\n\ntrain_predictions_cubic = cubic_model.predict(X_train_cubic_full)\ntest_predictions_cubic = cubic_model.predict(X_test_cubic_full)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Closeness of fit is assessed using $R^2$ once again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Data R-squared: ' + str(cubic_model.score(X_train_cubic_full,y_train)))\nprint('Test Data R-squred: ' + str(cubic_model.score(X_test_cubic_full,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of plotting the predictions, the following code compares the predicted price to the actual price, and calculates, on average, how far off the prediction was (for the test data).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({\"Predicted\": test_predictions_cubic, \"Actual\": y_test})\ndf['% Difference'] = (abs(df['Predicted']-df['Actual'])/df['Actual'])*100\n\nprint(\"Percentage Difference between Predicted and Actual Values (Cubic Model)\")\nprint(df.head())\nprint(\"\\nMean % Difference between Predicted and Actual Values: \" + str(df['% Difference'].mean()) +\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reciprocal Logarithmic Model\n\nAnother approach is a model similar in shape to $\\frac{1}{\\log(x)}$, with all the features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, the numeric features are transformed to match the reciprocal $\\log$ function $\\frac{1}{\\log(x)}$.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rl = addInvLogFeatures(X_train_numeric)\nX_test_rl = addInvLogFeatures(X_test_numeric)\n# print('X_train_rl \\n' + str(X_train_rl[:4,:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before running regression to fit the model, the features are scaled to make the orders of magnitude roughly the same. They are then combined with the ordinal features from before.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rl_scaled = scaler.fit_transform(X_train_rl)\nX_test_rl_scaled = scaler.fit_transform(X_test_rl)\n\nX_train_rl_full = np.hstack([X_train_rl_scaled, X_train_nominal])\nX_test_rl_full = np.hstack([X_test_rl_scaled, X_test_nominal])\n\n# print(\"X_train_rl_full \\n\" + str(X_train_rl_full[:4,:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the reciprocal $\\log$ model is fit using `RidgeCV` - a *regularized* version of regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rl_model = linear_model.RidgeCV()\nrl_model.fit(X_train_rl_full, y_train)\n\ntrain_predictions_rl = rl_model.predict(X_train_rl_full)\ntest_predictions_rl = rl_model.predict(X_test_rl_full)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, closeness of fit is measured with $R^2$.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Data R-squared: ' + str(rl_model.score(X_train_rl_full,y_train)))\nprint('Test Data R-squred: ' + str(rl_model.score(X_test_rl_full,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of plotting the predictions, the following code compares the predicted price to the actual price, and calculates, on average, how far off the prediction was (for the test data).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({\"Predicted\": test_predictions_rl, \"Actual\": y_test})\ndf['% Difference'] = (abs(df['Predicted']-df['Actual'])/df['Actual'])*100\n\nprint(\"Percentage Difference between Predicted and Actual Values (Reciprocal Log Model)\")\nprint(df.head())\nprint(\"\\nMean % Difference between Predicted and Actual Values: \" + str(df['% Difference'].mean()) +\"%\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}