{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n**The Sign Language Dataset is used here, An approach using Autoencoders will be implemented**\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets%2F3258%2F5337%2Famer_sign2.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1596903587&Signature=IOtoOmYCiLXST2%2FX%2BrOp13lJNdRF%2FyEKXh8JDDUmIP%2FpM%2BpBzOs4SPAwqBdyoVDwIePM6UmiZzf6fhCRgOKYv2DZpkqTtyxRLRhS3saS3rEi%2BpnJH2Y%2F%2Bo6sfLZeV7yjiHhazWNlpq4UVxEHh11zLeHISfR93xWcba2dNRYoillLROPWpFs5fu8N1W6m9TvLfuO3dBkrMJRD%2Fj8j%2BLvduoCDmBAnDCSVadjdBpKVsrBRCsFctC5XDt79YmsGKxAX8lXQBN%2BLKZwZ0%2FlpP%2F%2BXSuEpqMp4cGartmwGBYLLVPfTJ0s6Pe9BHCp1EYmUJUOFZsRFd3Cy5yDLDmXqhLogMA%3D%3D)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2 as cv2\nfrom sklearn.model_selection import train_test_split\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for missing values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So no missing data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So our dataset contains all int values let's check our label column**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['label'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So our label contains Categorical variables data so we will have to binarize it later**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let us check the label column data frequency**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train_df['label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_set = np.unique(np.array(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.set(style=\"darkgrid\")\nsns.countplot(y=labels, data=train_df, palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['label'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/sign-language-mnist/amer_sign2.png')\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/sign-language-mnist/american_sign_language.PNG')\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's Display the images in training data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"images = train_df.values\nimages = np.array([np.reshape (i, (28,28)) for i in images])\nimages = np.array([i.flatten() for i in images])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(images[0].reshape(28, 28))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LabelBinarizer\n\n**Binarize labels in a one-vs-all fashion**\n\n**Several regression and binary classification algorithms are available in scikit-learn. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.**\n\n**At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.**\n\n**Here the values are in categorical(nominal) so we are using LabelBinarizer**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's see how the data looks like now**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Developement","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=140)\n\nprint('Training Data shape : ',x_train.shape,  y_train.shape)\nprint('Testing Data shape : ',x_test.shape,  y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=256\nEPOCHS = 50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**reshaping x_train and x_test**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test = x_train.astype(np.float32), x_test.astype(np.float32)\n# Flatten images to 1-D vector of 784 features (28*28).\nx_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n# Normalize images value from [0, 255] to [0, 1].\n\nx_train = x_train/255.\nx_test = x_test/255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x_train[0].reshape(28,28)) #Since image size is 784 so (28,28)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's start with AutoEncoders!!\n\n**An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation for a set of data, typically for dimensionality reduction**\n\n![](https://camo.githubusercontent.com/5017c87b396b2745f13a289f913b037ea8352d50/687474703a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d3171546b5178373661424a4e68736b334954725542456644517a4451747a4d6330)\n\n**Here I am sharing some links that can give you heads-up about AutoEncoders**\n1. https://www.youtube.com/watch?v=H1AllrJ-_30\n2. https://www.youtube.com/watch?v=7mRfwaGGAPg","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_img = tf.keras.layers.Input(shape=(784,), name = \"input\")\n\n# this is the encoded representation of the input\nencoded = Dense(1024, activation='relu', name=\"emb_0\")(input_img)\nencoded = Dense(512, activation='relu', name=\"emb_1\")(encoded)\nencoded = Dense(256, activation='relu', name=\"emb_2\")(encoded)\nencoded = Dense(128, activation='relu', name=\"emb_3\")(encoded)\nencoded = Dense(64, activation='relu', name=\"emb_4\")(encoded)\nencoded = Dense(16, activation='relu', name=\"emb_5\")(encoded)\nlatent_vector = Dense(2, activation='relu', name=\"latent_vector\")(encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**More deeper the layers, better the performance**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the loss reconstruction of the input\ndecoded = Dense(16, activation='relu', name=\"dec_1\")(latent_vector)\ndecoded = Dense(64, activation='relu', name=\"dec_3\")(decoded)\ndecoded = Dense(128, activation='relu', name=\"dec_4\")(decoded)\ndecoded = Dense(256, activation='relu', name=\"dec_5\")(decoded)\ndecoded = Dense(512, activation='relu', name=\"dec_6\")(decoded)\ndecoded = Dense(1024, activation='relu', name=\"dec_7\")(decoded)\n\noutput_layer = Dense(784, activation = 'sigmoid', name=\"output\")(decoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder = tf.keras.models.Model(input_img, output_layer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's see the model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's create a separate Encoder Model as well**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = tf.keras.models.Model(input_img, latent_vector)\nencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autoencoder Model ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**One exception while training autoencoders is that it only trains itself on x_train and x_test, as they had been reshaped**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.compile(optimizer='adam', loss='mse')\nauto_history = autoencoder.fit(x_train, x_train, epochs=EPOCHS, batch_size=batch_size,validation_data=(x_test, x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded_imgs = autoencoder.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's compare the original images and new generated images from AutoEncoder**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 10 \nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}