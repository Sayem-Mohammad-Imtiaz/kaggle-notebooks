{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Install to have latest seaborn version to use histplot\n!pip install seaborn==0.11.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords,wordnet\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lemmatization\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functions Defined"},{"metadata":{},"cell_type":"markdown","source":"## 1) Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_data(path,file):\n    data = pd.read_csv(path+file)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = '/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/'\ntrain_file = 'Train.csv'\ntrain_data = load_data(train_path,train_file)\n\nvalid_path = '/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/'\nvalid_file = 'Valid.csv'\nvalid_data = load_data(valid_path,valid_file)\n\ntest_path = '/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/'\ntest_file = 'Test.csv'\ntest_data = load_data(test_path,test_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistics and details\nprint('Columns:',train_data.columns)\nprint('Shape:', train_data.shape)\nprint('Stats:',train_data.describe(include='object'))\nprint('Class Distribution:',train_data['label'].value_counts())\nprint('Info:',train_data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class distribution\nplt.figure(figsize=(12,6))\nsns.countplot(x='label',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get word count\ntrain_data['word_count']=train_data['text'].str.lower().str.len()\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.kdeplot(train_data['word_count'],shade=True,color='r').set_title('Kernel Distribution of Number of words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.histplot(train_data['word_count'],color='r')\nprint(sns.__version__)\nsns.histplot(data=train_data, x=\"word_count\").set_title('Word Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_wordcnt =  train_data['word_count'][train_data['label']==0]\npos_plot = sns.kdeplot(positive_wordcnt,color='b',shade=True)\nnegative_wordcnt = train_data['word_count'][train_data['label']==1]\nneg_plot = sns.kdeplot(negative_wordcnt,color='r',shade=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords\ndef preprocess_text(data):\n    stop = stopwords.words('english')\n    punct = '''!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~'''\n    #print(stop)\n    #Make lower\n    data['text'] = data['text'].str.lower()\n    #Remove stopwords\n    data['text'] = data['text'].apply(lambda x:' '.join([words for words in x.split() if words not in stop]))\n    #Remove punctuations\n    data['text'] = data['text'].str.translate(str.maketrans('', '', punct))\n    data['word_count'] = data['text'].str.split().str.len()\n    return data\ntrain_data = preprocess_text(train_data)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_wordcnt =  train_data['word_count'][train_data['label']==0]\npos_plot = sns.kdeplot(positive_wordcnt,color='b',shade=True)\nnegative_wordcnt = train_data['word_count'][train_data['label']==1]\nneg_plot = sns.kdeplot(negative_wordcnt,color='r',shade=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos_tag(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag,wordnet.NOUN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.head())\nprint(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize(data):\n    lemmatizer=WordNetLemmatizer()\n    data['text'] = data['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w,pos_tag(w)) for w in word_tokenize(x)]))\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Normalization;Lemmatization; No change as no pos_tag is there;hence add pos_tag\ntrain_data = lemmatize(train_data)\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_count = max(train_data['word_count'])\nprint(max_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view one lemmatized record\ntrain_data['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plotting word cloud\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(40,30))\n    plt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get postive and negative sentiment records for wordcloud\npositive = train_data['text'][train_data['label']==1]\nnegative = train_data['text'][train_data['label']==0]\nstop = set(stopwords.words('english'))\nstop.update([\"br\", \"href\",\"film\",\"movie\",\"one\"])\nprint(stop)\n#negative\n## Wordcloud\npos_wordcloud = WordCloud(stopwords=stop).generate(' '.join(positive))\nneg_wordcloud = WordCloud(stopwords=stop).generate(' '.join(negative))\n# pos_wordcloud = WordCloud(stopwords=stop,width=800,height=800,min_font_size=10).generate(' '.join(positive))\n# neg_wordcloud = WordCloud(stopwords=stop,width=800,height=800,min_font_size=10).generate(' '.join(negative))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cloud(pos_wordcloud)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cloud(neg_wordcloud)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove word_count column\ntrain_data = train_data[['text','label']]\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) Do pre-processing for test and valid dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_data = preprocess_text(valid_data)\nvalid_data = lemmatize(valid_data)\n\ntest_data = preprocess_text(test_data)\ntest_data = lemmatize(test_data)\ntest_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid_data = valid_data[['text']]\ny_valid_data = valid_data[['label']]\n\nx_test_data = test_data[['text']]\ny_test_data = test_data[['label']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) Building model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word technique\n#Create feature vectors using Bag of Words-TfIdf\ntfidf_converter = TfidfVectorizer(max_features=1000,min_df=5,max_df=0.7)\nx = tfidf_converter.fit_transform(train_data['text']).toarray()\ny = train_data['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bi-grams\n# Word technique\n#Create feature vectors using Bag of Words-TfIdf\nntfidf_converter = TfidfVectorizer(max_features=1000,min_df=5,max_df=0.7,ngram_range=(2,2))\nnx = ntfidf_converter.fit_transform(train_data['text']).toarray()\nny = train_data['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x.shape)\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid=np.array(y_valid_data).reshape(-1,1)\nprint(y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose model\n#Naive Bayes Algorithm p(sent|word) = p(sent)p(word|sent)/p(word)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\ngnb = GaussianNB()\ngnb.fit(x,y)\n\n\nngram_gnb = GaussianNB()\nngram_gnb.fit(nx,ny)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save model\nimport pickle\ngnb_model = pickle.dumps(gnb)\n\nngram_gnb_model = pickle.dumps(ngram_gnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use tfidf to transform test and valid data\nx_val_data = tfidf_converter.transform(x_valid_data['text']).toarray()\nx_tst_data = tfidf_converter.transform(x_test_data['text']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ngram\n# Use tfidf to transform test and valid data\nnx_valid_data = ntfidf_converter.transform(x_valid_data['text']).toarray()\nnx_test_data = ntfidf_converter.transform(x_test_data['text']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the model\ngnb_model = pickle.loads(gnb_model)\n\nngram_gnb_model = pickle.loads(ngram_gnb_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict valid\nypred_valid = gnb_model.predict(x_val_data)\n#print(ypred_valid)\nprint('Valid Accuracy:',accuracy_score(y_valid_data,ypred_valid))\n\n#Predict Test\nypred_test = gnb_model.predict(x_tst_data)\nprint('Test Accuracy:',accuracy_score(y_test_data,ypred_test))\nprint(confusion_matrix(y_test_data,ypred_test))\nprint('Classification Report:',classification_report(y_test_data,ypred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ngram\n# Predict valid\nnypred_valid = ngram_gnb_model.predict(nx_valid_data)\n#print(ypred_valid)\nprint('Valid Accuracy:',accuracy_score(y_valid_data,nypred_valid))\n\n#Predict Test\nnypred_test = ngram_gnb_model.predict(nx_test_data)\nprint('Test Accuracy:',accuracy_score(y_test_data,nypred_test))\nprint(confusion_matrix(y_test_data,ypred_test))\nprint('Classification Report:',classification_report(y_test_data,nypred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random text prediction\ntext = ['that is too bad.but i can assure it can be made better','not sure if the climax really did well','as expected']\ntext = tfidf_converter.transform(text).toarray()\nprint(gnb_model.predict(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random text prediction\ntext = ['that is not bad.but i can assure it can be made better','not sure if the climax really did well','as expected']\ntext = ntfidf_converter.transform(text).toarray()\nprint(ngram_gnb_model.predict(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(max_depth=4, random_state=0)\nrf.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model=pickle.dumps(rf)\n# Use tfidf to transform test and valid data\nx_rf_valid_data = tfidf_converter.transform(x_valid_data['text']).toarray()\nx_rf_test_data = tfidf_converter.transform(x_test_data['text']).toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the model\nrf_model = pickle.loads(rf_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict valid\nypred_valid = rf_model.predict(x_rf_valid_data)\n#print(ypred_valid)\nprint('RF Valid Accuracy:',accuracy_score(y_valid_data,ypred_valid))\n\n#Predict Test\nypred_test = rf_model.predict(x_rf_test_data)\nprint('RF Test Accuracy:',accuracy_score(y_test_data,ypred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random text prediction\ntext = ['that is too bad.but i can assure it can be made better','not sure if the climax really diid well']\ntext = tfidf_converter.transform(text).toarray()\nprint(rf_model.predict(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boosting\nimport sklearn\nprint(sklearn.__version__)\nfrom sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\nada.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use tfidf to transform test and valid data\nx_val_data = tfidf_converter.transform(x_valid_data['text']).toarray()\nx_tst_data = tfidf_converter.transform(x_test_data['text']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save model\nada_model = pickle.dumps(ada)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict valid and test data\nada_model = pickle.loads(ada_model)\ny_ada_val_pred = ada_model.predict(x_val_data)\nprint(\"Valid data Accuracy is :\",accuracy_score(y_valid_data,y_ada_val_pred))\n\n#Predict Test\ny_ada_tst_pred = ada_model.predict(x_tst_data)\nprint('Test Accuracy:',accuracy_score(y_test_data,y_ada_tst_pred))\nprint(confusion_matrix(y_test_data,y_ada_tst_pred))\nprint('Classification Report:',classification_report(y_test_data,y_ada_tst_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Text Prediction\ntext = ['not good','worth watching','bad','grew up listening to this.awful']\ntext = tfidf_converter.transform(text).toarray()\nprint(ada_model.predict(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random text prediction\ntext = ['that is too bad.but i can assure it can be made better','not sure if the climax really did well','as expected']\ntext = tfidf_converter.transform(text).toarray()\nprint(ada_model.predict(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}