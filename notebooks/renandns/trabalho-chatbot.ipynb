{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n<h2 align=\"center\"> Aula 03 - Técnicas de Pré-Processamento de Texto e Similaridade</h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>\n\n# Integrantes:\n### Leandro Freitas \t    - RA: 080162\n### Paulo Érico de Freitas\t- RA: 183545\n### Renan Souza\t\t        - RA: 070487 "},{"metadata":{},"cell_type":"markdown","source":"<p><b>Exercício 3:</b>\n\nEscreva um chatbot que, dado uma pergunta em Inglês, encontre uma pergunta mais parecida no corpus de perguntas e respostas disponível no Kaggle (https://www.kaggle.com/rtatman/questionanswer-dataset#S08_question_answer_pairs.txt) e exiba a resposta."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#file = open('../input/S08_question_answer_pairs.txt', 'r')\nfile = open('../input/questionanswer-dataset/S08_question_answer_pairs.txt', 'r')\narray_arquivo = []\nfor line in file:      \n    array_arquivo.append(line.strip().split('\\t'))\nfile = open('../input/questionanswer-dataset/S09_question_answer_pairs.txt', 'r')\n#array_arquivo = []\nfor line in file:      \n    array_arquivo.append(line.strip().split('\\t'))\nfile = open('../input/questionanswer-dataset/S10_question_answer_pairs.txt', 'r', encoding = \"ISO-8859-1\")\n#array_arquivo = []\nfor line in file:      \n    array_arquivo.append(line.strip().split('\\t'))\n#import pandas as pd\n#import numpy as np\n#import string\n#df_S08 = pd.read_csv('../input/S08_question_answer_pairs.txt', sep=\"\\t\", header=0)\n#df_S09 = pd.read_csv('../input/S09_question_answer_pairs.txt', sep=\"\\t\", header=0)\n#df_S10 = pd.read_csv('../input/S10_question_answer_pairs.txt', sep=\"\\t\", header=0, encoding = \"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texto = ''\nfor a in array_arquivo:\n    texto += a[1] + ' '","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array_arquivo[1][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sents_token = sent_tokenize(texto)\nprint(sents_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sents_array = np.array(sents_token)\nprint(sents_array[10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.DataFrame(sents_array)\nlen(a)\na.dropna()\nlen(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[print(a) for a in sents_array]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\nstopwords_list = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_tokenizer(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n\ntfs = tfidf_vectorizer.fit_transform(sents_array)\n\nprint(tfs.shape)\nprint(tfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print([k for k in tfidf_vectorizer.vocabulary_.keys()][:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd_transformer = TruncatedSVD(n_components=1000)\n\nsvd_transformer.fit(tfs)\n\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1][:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_variance = 0.0\nk = 0\nfor var in sorted(svd_transformer.explained_variance_ratio_)[::-1]:\n    cummulative_variance += var\n    if cummulative_variance >= 0.5:\n        break\n    else:\n        k += 1\n        \nprint(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd_transformer = TruncatedSVD(n_components=k)\nsvd_data = svd_transformer.fit_transform(tfs)\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(sents_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def retornar_pergunta(index):    \n    return array_arquivo[index + 1][1]\ndef retornar_resposta(index):    \n    if array_arquivo[index + 1][2] == 'NULL':\n        return 'We didn t find an answer, could you ask another question?'\n    else:\n        return array_arquivo[index + 1][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Escreva uma pergunta \n#query_vect = tfidf_vectorizer.transform([\"When did Jão graduate from William and Mary?\"])\n# ou Escolha uma pergunta dentre as existentes\nquery_vect = tfidf_vectorizer.transform([sents_array[3000]])\n#789\n#777\n\nresult_cosine = cosine_similarity(query_vect, tfidf_matrix)\n\nprint(np.amax(result_cosine))\nprint(np.argmax(result_cosine))\nprint(retornar_pergunta(np.argmax(result_cosine)))\nprint(retornar_resposta(np.argmax(result_cosine)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}