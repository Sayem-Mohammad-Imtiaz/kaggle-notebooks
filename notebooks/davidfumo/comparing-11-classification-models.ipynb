{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3e96c68f-e214-3e94-3f54-d9f527d609e8"},"source":"# Problem statement\n\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled\nmushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide\nto North American Mushrooms (1981). Each species is identified as definitely edible, definitely\npoisonous, or of unknown edibility and not recommended. This latter class was combined\nwith the poisonous one.\n\n-  **What types of machine learning models perform best on this dataset?** \n-  **Which features are most indicative of a poisonous mushroom?**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d43be08-9ff8-c828-8153-9525cf272605"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.cross_validation import train_test_split\nimport matplotlib.pyplot as plt\n% matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69f2b03b-6d63-dfdf-0981-ba069b60197e"},"outputs":[],"source":"df = pd.read_csv('../input/mushrooms.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f50f7c14-405a-e521-d598-15524b440a76"},"outputs":[],"source":"df.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"823371f8-5012-a188-58f0-82e413258d7b"},"outputs":[],"source":"df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27a18d05-7226-15df-48f6-fdaa38bf3f0d"},"outputs":[],"source":"# This dataset is ready for exploration, no data cleaning required \ndf.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f2af4e8-2a00-5950-1c66-9e760ef92114"},"source":"# Class Distribuition\n\nImbalanced data typically refers to a problem with classification problems where the classes\nare not represented equally. Imagine a case where 99% of the data belongs to one class, this can cause\nthe classification model to ignore the remaining class and indeed it would get very good accuracy.\nBut a small difference often does not matter and this is the case in mushrooms dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af96bced-ed00-745d-e77b-f18e0be6ff4b"},"outputs":[],"source":"# Class Distribuition\nsns.countplot(x=\"class\", data=df, palette=\"Greens_d\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d7c3469-313a-d48a-1b4d-3b9132521f8d"},"outputs":[],"source":"class_dist = df['class'].value_counts()\n\nprint(class_dist)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1dc70396-83ed-beea-f552-628614cb72a4"},"outputs":[],"source":"prob_e = class_dist[0]/(class_dist[0]+class_dist[1])\nprob_p = 1 - prob_e\nprint(prob_e)\nprint(prob_p)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e1a2c2cb-9ea9-0c56-1a50-a5d25bf8a8d5"},"source":"# Feature Transformation\n\nNow we have to convert all categorical variables using LabelEncoder from the awesome sklearn lib.  \nex. The class p will be maped to 1 and 0 as e"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1108b091-81fa-0f4d-e65e-a7d96d5a6920"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in df.columns:\n    df[col] = labelencoder.fit_transform(df[col])\n \ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d267b025-53ae-8a1c-dad8-09299a147a2a"},"source":"# Pearson Correlation Heatmap\n\nPerson Correlation helps us represent the statistical relationships between features, this is a simple way to get an intuition of \nthe contribution of each feature to the target variable. This correlation matrix can be easily plotted using Seaborn Heatmap. In Heatmap strong relationships are emphasized with sharp colors."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0454fa0e-8752-f2df-3c56-d709f58a6724"},"outputs":[],"source":"colormap = plt.cm.viridis\nplt.figure(figsize=(15,15))\nplt.title('Pearson Correlation of Features', size=15)\n\nsns.heatmap(df.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"005ba353-2e76-4762-1ba4-1cec31ef1faf"},"outputs":[],"source":"# sns.pairplot(df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9704e144-eea8-e729-03f6-cd8dcb67a047"},"source":"# Classification Models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1629eb68-884c-329b-c704-f7664d8c773a"},"outputs":[],"source":"X = df.drop('class', axis=1)\ny = df['class']\nRS = 123\n\n# Split dataframe into training and test/validation set \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RS)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d353ef0-33ce-16cf-9e55-3b2a6caa1b3b"},"outputs":[],"source":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nimport xgboost\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9973673-998e-b6dd-0e10-30b5bb04c7c2"},"outputs":[],"source":"# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict_proba(X_test)\n    ll = log_loss(y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7b40ffd-dedf-9405-7358-55815291fde1"},"source":"### Multiple classifiers\n\nLet's evaluate multiple classifiers at once. After this picking a single model and improving \nparameter."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a60d6359-afba-adf4-a97f-c3b190d54b45"},"outputs":[],"source":"sns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')\nplt.show()\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n\nplt.xlabel('Log Loss')\nplt.title('Classifier Log Loss')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c86f9d4-903b-7da2-9612-5f4f8158a5f9"},"source":"# Conclusion 1: Classification Model\nClearly Tree based models are wining here, even the most simple one (DecisionTreeClassifier), If I had to pick a classifier \ni would pick Decision Tree Classifier as it is the simplest one from (Decision Tree, random forest and Boosted Trees) and would run well on production environments.\nLet me know if you have different opinions, feel free to share your thoughts or ask any question. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74a93347-efe3-2076-f0b2-c67aebc3baba"},"outputs":[],"source":"# Inspect the learned Decision Trees\n# One of the major advantage of Decision Trees is the fact that they can easily be interpreted.  \nclf = DecisionTreeClassifier()\n\n# Fit with all the training set\nclf.fit(X, y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8607408-954a-5bc4-dd22-29b9684e4783"},"outputs":[],"source":"importances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = X.columns\n\nprint(\"Feature ranking:\")\nfor f in range(X.shape[1]):\n    print(\"%s : (%f)\" % (feature_names[f] , importances[indices[f]]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5d89d46-c953-32f2-67af-a6ad8d01cc42"},"outputs":[],"source":"f, ax = plt.subplots(figsize=(15, 15))\nplt.title(\"Feature ranking\", fontsize = 12)\nplt.bar(range(X.shape[1]), importances[indices],\n    color=\"b\", \n    align=\"center\")\nplt.xticks(range(X.shape[1]), feature_names)\nplt.xlim([-1, X.shape[1]])\nplt.ylabel(\"importance\", fontsize = 18)\nplt.xlabel(\"index of the feature\", fontsize = 18)"},{"cell_type":"markdown","metadata":{"_cell_guid":"23e41598-fe3c-694f-5895-df74bfa28a65"},"source":"# Conclusion 2: Feature Importance\nWow, There are a lot of features with no meaning to predict our target variable.\ncap-shape, cap-surface, cap-color, bruises, odor, gill-attachment, gill-spacing, gill-size are the most significant features."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}