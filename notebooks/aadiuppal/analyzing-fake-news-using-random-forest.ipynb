{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bfe6192d-b248-5c35-858c-d55e4f88c12f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('ggplot')\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nfake_news = pd.read_csv(\"../input/fake.csv\")\nfake_news.head(10)\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01b1ddc5-0e03-9e90-6ae4-9e0c3bb76e7a"},"outputs":[],"source":"fake_news.type.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42896f36-c4dc-83c5-cd9b-6ef080266584"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(fake_news, test_size = 0.2)\n#print(len(train),len(test))\n\ntrain_one = train[train[\"language\"]==\"english\"]\ntest_one = test[test[\"language\"]==\"english\"]\n#train_one.columns.values\n#print(len(test_one),len(train_one))\n#import nltk\nfrom nltk.corpus import stopwords \ntrain.columns.values\n#Text_col = train[\"text\"]\n#Author_col = train[\"author\"]\n#Site_col = train[\"site_url\"]\n#Title_col = train[\"title\"]\n#Thread_col = train[\"thread_title\"]\ntrain.head()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcdf2e3e-277a-675c-fdec-f5d0afebced0"},"outputs":[],"source":"import re\ndef refineWords(s):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", s) \n    words = letters_only.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    meaningful_words = [w for w in words if not w in stops]\n    #print( \" \".join( meaningful_words ))\n    return( \" \".join( meaningful_words ))\ntrain_one[\"text\"].fillna(\" \",inplace=True)    \ntrain_one[\"text\"] = train_one[\"text\"].apply(refineWords)\ntrain_one[\"author\"].fillna(\" \",inplace=True)    \ntrain_one[\"author\"] = train_one[\"author\"].apply(refineWords)\ntrain_one[\"site_url\"].fillna(\" \",inplace=True)    \ntrain_one[\"site_url\"] = train_one[\"site_url\"].apply(refineWords)\ntrain_one[\"title\"].fillna(\" \",inplace=True)    \ntrain_one[\"title\"] = train_one[\"title\"].apply(refineWords)\ntrain_one[\"thread_title\"].fillna(\" \",inplace=True)    \ntrain_one[\"thread_title\"] = train_one[\"thread_title\"].apply(refineWords)\ntrain_two = train_one.copy()\ntrain_one.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bfa9cb2-c2bd-07d7-0c35-87774b53fe87"},"outputs":[],"source":"train_one = train_two.copy()\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n#print(train_one[\"title\"].head())\n#temp  = (vectorizer.fit_transform(train_one[\"text\"]))\n#train_one[\"text\"] = temp.to_array()\ntrain_one[\"text\"] = vectorizer.fit_transform(train_one[\"text\"]).toarray()\ntrain_one[\"author\"] = vectorizer.fit_transform(train_one[\"author\"]).toarray()\ntrain_one[\"site_url\"] = vectorizer.fit_transform(train_one[\"site_url\"]).toarray()\ntrain_one[\"title\"] = vectorizer.fit_transform(train_one[\"title\"]).toarray()\ntrain_one[\"thread_title\"] = vectorizer.fit_transform(train_one[\"thread_title\"]).toarray()\ntrain_one.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa7db11b-8cb7-aedc-c4b2-e33808e98310"},"outputs":[],"source":"#print((train_one[\"text\"][11543]).shape)\n#print(type(temp))\nprint(train_one.describe())\ndist = np.sum(train_one, axis=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e6a203b-51d8-9721-b8c2-f37f5976bd8d"},"outputs":[],"source":"train_one[\"domain_rank\"].fillna(train_one.domain_rank.median(axis=0),inplace=True)\ntest_one[\"domain_rank\"].fillna(test_one.domain_rank.median(axis=0),inplace=True)  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfb54248-84d0-7ace-e21a-c5708bcac8b7"},"outputs":[],"source":"train_one[\"isSpam\"] = np.sign(train_one[\"spam_score\"]-0.5)\n#print(train_one[\"isSpam\"])\nfrom sklearn.ensemble import RandomForestClassifier\n#forest = RandomForestClassifier(n_estimators = 100)\nforest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\nfeatures_forest = train_one[[\"text\", \"author\", \"site_url\", \"title\", \"thread_title\",\"domain_rank\"]].values\nmy_forest = forest.fit(features_forest, train_one[\"isSpam\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9363fe9-aa46-3f29-0af3-15b324458cef"},"outputs":[],"source":"target = train_one[\"isSpam\"].values\nprint(my_forest.score(features_forest, target))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"964bad51-1d28-0047-ba1b-47d7ae170277"},"outputs":[],"source":"test_one[\"text\"].fillna(\" \",inplace=True)    \ntest_one[\"text\"] = test_one[\"text\"].apply(refineWords)\ntest_one[\"author\"].fillna(\" \",inplace=True)    \ntest_one[\"author\"] = test_one[\"author\"].apply(refineWords)\ntest_one[\"site_url\"].fillna(\" \",inplace=True)    \ntest_one[\"site_url\"] = test_one[\"site_url\"].apply(refineWords)\ntest_one[\"title\"].fillna(\" \",inplace=True)    \ntest_one[\"title\"] = test_one[\"title\"].apply(refineWords)\ntest_one[\"thread_title\"].fillna(\" \",inplace=True)    \ntest_one[\"thread_title\"] = test_one[\"thread_title\"].apply(refineWords)\ntest_two = test_one.copy()\n\n\n\ntest_one[\"text\"] = vectorizer.fit_transform(test_one[\"text\"]).toarray()\ntest_one[\"author\"] = vectorizer.fit_transform(test_one[\"author\"]).toarray()\ntest_one[\"site_url\"] = vectorizer.fit_transform(test_one[\"site_url\"]).toarray()\ntest_one[\"title\"] = vectorizer.fit_transform(test_one[\"title\"]).toarray()\ntest_one[\"thread_title\"] = vectorizer.fit_transform(test_one[\"thread_title\"]).toarray()\ntest_one[\"isSpam\"] = np.sign(test_one[\"spam_score\"]-0.5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddd758c9-ac5f-da68-2b95-8111f18ea70b"},"outputs":[],"source":"test_features = test_one[[\"text\", \"author\", \"site_url\", \"title\", \"thread_title\",\"domain_rank\"]].values\nmy_prediction = my_forest.predict(test_features)\nprint(len(my_prediction),len(test_one[\"isSpam\"]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b458ae1-8923-5a1c-de9c-44cb989f1d0c"},"outputs":[],"source":"count = 0\npred = my_prediction.tolist()\ntest_spam = test_one[\"isSpam\"].tolist()\nfor i in range(len(pred)):\n    if pred[i] == test_spam[i]:\n        count += 1\nprint(count,float(count)/len(my_prediction))\n#print(my_prediction)\n#print(test_spam)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}