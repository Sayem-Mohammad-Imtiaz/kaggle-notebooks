{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\n\ntemp = copy.deepcopy(df)\nfor i in range(len(df)):\n    if df.age.iloc[i] < 30:\n        temp.age.iloc[i] = 30\n    elif df.age.iloc[i] >= 65:\n        temp.age.iloc[i] = 70    \n    elif df.age.iloc[i] >= 60:\n        temp.age.iloc[i] = 60    \n    elif df.age.iloc[i] >= 55:\n        temp.age.iloc[i] = 60    \n    elif df.age.iloc[i] >= 50:\n        temp.age.iloc[i] = 50 \n    elif df.age.iloc[i] >= 45:\n        temp.age.iloc[i] = 50    \n    elif df.age.iloc[i] >= 40:\n        temp.age.iloc[i] = 40\n    elif df.age.iloc[i] >= 35:\n        temp.age.iloc[i] = 40\n    elif df.age.iloc[i] >= 30:\n        temp.age.iloc[i] = 30\ntemp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.subplot(2, 2, 1)\nplt.violinplot([df.age])\nplt.title('age')\nplt.subplot(2, 2, 2)\nplt.violinplot([df.sex])\nplt.title('sex')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = df.output.value_counts()\n\nplt.bar(tmp.index, tmp.values)\nplt.xticks([0, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df.age)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df.trtbps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df.cp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.heatmap(df.corr(method='pearson'), cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df.iloc[:, 0:-1].values\ny = df.iloc[:, -1].values\nx,y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)\n\nstandardScaler = StandardScaler()\nX_train =standardScaler.fit_transform(X_train)\nX_test = standardScaler.transform(X_test)\n\n# robustScaler = RobustScaler()\n# X_train =robustScaler.fit_transform(X_train)\n# X_test = robustScaler.transform(X_test)\n\n# maxAbsScaler = MaxAbsScaler()\n# X_train = maxAbsScaler.fit_transform(X_train)\n# X_test = maxAbsScaler.transform(X_test)\n\n# minMaxScaler = MinMaxScaler()\n# X_train = minMaxScaler.fit_transform(X_train)\n# X_test = minMaxScaler.transform(X_test)\n\nprint('Shape for training data', X_train.shape, y_train.shape)\nprint('Shape for testing data', X_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\npredicted=model.predict(X_test)\nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, predicted)*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# train\nxgb = XGBClassifier(n_estimators = 3, learning_rate = 0.7, max_depth = 7)\nxgb.fit(X_train, y_train)\n# prediction\nw_pred = xgb.predict(X_test)\nprint(\"The accuracy of XGB is : \", accuracy_score(y_test, w_pred)*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\n  \npredicted = model.predict(X_test)\nprint(\"The accuracy of SVM is : \", accuracy_score(y_test, predicted)*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators = 100, random_state = 0)  \nmodel.fit(X_train, y_train)  \npredicted = model.predict(X_test)\nprint(\"The accuracy of Random Forest is : \", accuracy_score(y_test, predicted.round())*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier(n_neighbors = 2)\nclassifier.fit(X_train, y_train)\npredicted = classifier.predict(X_test)\nprint(\"The accuracy of Random Forest is : \", accuracy_score(y_test, predicted.round())*100, \"%\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X_train, y_train)\npredicted = LR.predict(X_test)\nprint(\"The accuracy of LR is : \", accuracy_score(y_test, predicted.round())*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 40), random_state=0)\nNN.fit(X_train, y_train)\npredicted = NN.predict(X_test)\nprint(\"The accuracy of Neural Network is : \", accuracy_score(y_test, predicted.round())*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\npredicted = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"The accuracy of GaussianNB is : \", accuracy_score(y_test, predicted.round())*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras import optimizers\nimport tensorflow as tf\nimport random as python_random\n\nnp.random.seed(42)\npython_random.seed(42)\ntf.random.set_seed(42)\n\nmodel = Sequential()\nmodel.add(Dense(16, input_shape = (X_train.shape), activation = 'gelu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer = optimizers.RMSprop(lr=0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])    \nmodel.summary()\nmodel.fit(X_train, y_train, batch_size = 128, epochs = 120, verbose = 0)\nresults = model.evaluate(X_test, y_test)\npre = model.predict(X_test)\nprint(\"The accuracy of Keras is : \", accuracy_score(y_test, pre.round())*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}