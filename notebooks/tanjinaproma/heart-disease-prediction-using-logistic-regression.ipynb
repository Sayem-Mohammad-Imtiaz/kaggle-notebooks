{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Disease Prediction using Logistic Regression\nThis notebook is created for classifying binary classification of heart disease i.e. whether the patient has the 10 year risk of coronary heart disease or not. Since logistic regression is used to model binary dependent variable, i used it to estimate the probabilities of the problem.\n# Dataset\nhttps://www.kaggle.com/dileep070/heart-disease-prediction-using-logistic-regression\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"Data = pd.read_csv(\"/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv\")\nData.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we are dropping the Education feature, since it is not relevant with weither the patient will get affected with CHD or not.\n\nAfter that we are dropping the rows containing null values since they dont contain all the feature information regarding the specific patient.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.drop(['education'], axis=1, inplace=True)\nData.isnull().sum()\n\nData.dropna(axis = 0, inplace = True)\nprint(Data.shape[0])\nData.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Here, we are creating the independent variable X and dependent variable Y from the dataframe. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = Data.TenYearCHD.values\nX = Data.drop(['TenYearCHD'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Now that the data is ready , we have to only bring them in standardized form using Scikit learn StandardScaler Class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sc= StandardScaler()\nX = sc.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nX_train = X_train.T\nX_test = X_test.T\nY_train = Y_train.T\nY_test = Y_test.T\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n**Mathematical expression of the algorithm**:\n\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b $$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n\nThe cost is computed by taking the summation of all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At first, we initialize the weights and bias as vectors of zeros.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"###Initialize the weights and bias\ndef initialize_W_b_with_zeros(num_features):\n    w = np.zeros(shape = (num_features,1))\n    b = 0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we defined sigmoif function.\n\n$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Sigmoid Function\n\ndef sigmoid(z):\n    s = 1/(1+ np.exp(-z))\n    \n    return s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Propagation\nFor forward propagation we compute\n\n$A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n\nthen compute the cost function:\n\n$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nThen, for backward propagation, we compute the gradients dw and db by taking the derivatives.\n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Forward and Backward propagation function \ndef propagate(w,b, X,Y):\n    \n    m = X.shape[1]\n    z = np.dot(w.T, X) + b\n    A = sigmoid(z)\n    \n    loss =  - (Y * np.log(A) + (1-Y) * np.log( 1-A) )\n    cost=  np.sum(loss)/m\n    \n    dw = (1 / m) * np.dot(X, (A-Y).T)\n    db = (1 / m) * np.sum(A-Y)\n    \n    gradient= {\"dw\": dw,\n             \"db\": db}\n    \n    return gradient, cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Update the weights and biases\n\nHere, we update the weights and biases after every iteration of the propagation. we record the updated weights for the next iteration and also try to minimize the cost at each iteration.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w,b, X,Y, num_iterations, learning_rate):\n    \n    costs = []\n    \n    for i in range( num_iterations ):\n        gradient, cost = propagate(w,b, X,Y)\n        \n        dw = gradient['dw']\n        db = gradient['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 20 == 0:\n            costs.append(cost)\n            \n    parameters = {\"w\": w,\n                 \"b\": b}\n    \n    gradient= {\"dw\": dw,\n             \"db\": db}\n    \n    return parameters, gradient, costs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\n\nAfter creating the logistic regression model, we use test set for predicting the output.\n\n$\\hat{Y} = A = \\sigma(w^T X + b)$\n\nWe take an numpy array of m columns for m data samples to predict the output $\\hat{Y}$ ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict( w,b,X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid( np.dot(w.T , X) + b)\n    \n    for i in range(A.shape[1]):\n        if A[:,i] > 0.5 :\n              Y_prediction[:,i] = 1 \n      \n    return Y_prediction\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression Model\nIt is the final function integrating all the functions. We take the number of features present in the dependent variable which is X_Train in this case. the number of features is 14 for this dataset. So we initialize weights and biases of that diemnsion. \n\nThe model uses training set to learn the optimized weights and biases which then we use for test set. finally, we calculate the test accuracy.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def Logistic_Regression_model(X_train, X_test, Y_train, Y_test,num_iterations, learning_rate ):\n    num_features = X_train.shape[0]\n    w,b = initialize_W_b_with_zeros(num_features)\n    parameters, gradient, costs = update(w,b, X_train,Y_train, num_iterations, learning_rate)\n    \n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    Y_Test_Predict = predict(w,b, X_test)\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_Test_Predict - Y_test)) * 100))\n\n    \n    Dictionary = {\"Prediction \": Y_Test_Predict,\n                \"Weight\": w,\n                \"Bias\" :b,\n                \"Cost Function\" : costs}\n    \n    return Dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dictionary = Logistic_Regression_model(X_train, X_test, Y_train, Y_test, num_iterations = 1000, learning_rate = 0.10 )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test accuracy: 84.4 %","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot learning curve (with costs)\nimport matplotlib.pyplot as plt\ncosts = np.squeeze(Dictionary['Cost Function'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations')\nplt.title(\"Cost Reduction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For ensuring the accuracy of our model, the Logistic Regression classifier of Sklearn is given.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train.T,Y_train.T)\nprint(\"test accuracy {}\".format(lr.score(X_test.T,Y_test.T)))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}