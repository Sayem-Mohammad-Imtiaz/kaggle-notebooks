{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Project Scope**\n\nHaving a well-defined structure before performing a task helps in efficient execution of the task. This is true even in cases of building a machine learning model. Once you have built a model on a dataset, you can easily break down the steps and define a structured Machine learning pipeline.\n\nThis notebook coveres the process of building an end-to-end Machine Learning pipeline and implementing it on  BigMart sales prediction dataset.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"The dataset contains information about the stores, products and historical sales. We will predict the sales of the products in the stores.\n\nWe will start by building a prototype machine learning pipeline that will help us define the actual machine learning pipeline.","metadata":{}},{"cell_type":"code","source":"#Importing libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:07.523784Z","iopub.execute_input":"2021-05-30T20:23:07.524735Z","iopub.status.idle":"2021-05-30T20:23:07.531072Z","shell.execute_reply.started":"2021-05-30T20:23:07.524622Z","shell.execute_reply":"2021-05-30T20:23:07.530261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration and Preprocessing","metadata":{}},{"cell_type":"code","source":"#loading train data\ntrain = pd.read_csv(\"../input/big-mart-sales-prediction/Train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:07.53256Z","iopub.execute_input":"2021-05-30T20:23:07.53301Z","iopub.status.idle":"2021-05-30T20:23:07.604461Z","shell.execute_reply.started":"2021-05-30T20:23:07.53298Z","shell.execute_reply":"2021-05-30T20:23:07.603503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for missing values\ntrain.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:07.605812Z","iopub.execute_input":"2021-05-30T20:23:07.606127Z","iopub.status.idle":"2021-05-30T20:23:07.631747Z","shell.execute_reply.started":"2021-05-30T20:23:07.606098Z","shell.execute_reply":"2021-05-30T20:23:07.630677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only Item_Weight and Outlet_Size have missing values.\n\nItem_Weight is a continuous variable. We can use either mean or median to impute the missing values, but here we will use mean.\n\nOutlet_Size is a categorical variable so will use mode to impute the missing values in the column.","metadata":{}},{"cell_type":"code","source":"#impute missing values in Item_Weight using mean\ntrain.Item_Weight.fillna(train.Item_Weight.mean(), inplace=True)\ntrain.Item_Weight.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:07.633009Z","iopub.execute_input":"2021-05-30T20:23:07.633292Z","iopub.status.idle":"2021-05-30T20:23:07.643835Z","shell.execute_reply.started":"2021-05-30T20:23:07.633265Z","shell.execute_reply":"2021-05-30T20:23:07.64273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#impute missing values in Outlet_Size using mode\ntrain.Outlet_Size.fillna(train.Outlet_Size.mode()[0], inplace=True)\ntrain.Outlet_Size.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:07.647484Z","iopub.execute_input":"2021-05-30T20:23:07.647789Z","iopub.status.idle":"2021-05-30T20:23:07.660656Z","shell.execute_reply.started":"2021-05-30T20:23:07.647762Z","shell.execute_reply":"2021-05-30T20:23:07.659566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Machine learning models cannot work with categorical(string) data. We will convert the categorical variables into numeric types.","metadata":{}},{"cell_type":"code","source":"#checking categorical variables in the data\ntrain.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:07.663781Z","iopub.execute_input":"2021-05-30T20:23:07.664142Z","iopub.status.idle":"2021-05-30T20:23:07.676347Z","shell.execute_reply.started":"2021-05-30T20:23:07.664111Z","shell.execute_reply":"2021-05-30T20:23:07.675436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data has the following categorical variables\n\n* Item_Identifier\n* Item_Fat_Content\n* Item_Type\n* Outlet_Identifier\n* Outlet_Size\n* Outlet_Type\n* Outlet_Location_Type\n\nWe will use the categorical_encorders library to convert these variables into binary variables. We will not convert Item_Identifier.","metadata":{}},{"cell_type":"code","source":"import category_encoders as ce\n\n#create an object of OneHotEncorder\nOHE = ce.OneHotEncoder(cols=['Item_Fat_Content',\n                            'Item_Type',\n                            'Outlet_Identifier',\n                            'Outlet_Size',\n                            'Outlet_Location_Type',\n                            'Outlet_Type'],use_cat_names=True)\n\n#encode the variables\ntrain = OHE.fit_transform(train)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:07.67779Z","iopub.execute_input":"2021-05-30T20:23:07.678262Z","iopub.status.idle":"2021-05-30T20:23:09.050747Z","shell.execute_reply.started":"2021-05-30T20:23:07.67823Z","shell.execute_reply":"2021-05-30T20:23:09.04964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:09.052263Z","iopub.execute_input":"2021-05-30T20:23:09.052641Z","iopub.status.idle":"2021-05-30T20:23:09.083417Z","shell.execute_reply.started":"2021-05-30T20:23:09.052608Z","shell.execute_reply":"2021-05-30T20:23:09.082552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have taken care of our categorical variables, we move on to the continous variables.\nWe will nnormalize the data in such a way that the range of all variables is almost similar.\nWe will use the StandardScaler function to do this.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n#create an object of the StandardScaler\nscaler = StandardScaler()\n\n#fit with the Item_MRP\nscaler.fit(np.array(train.Item_MRP).reshape(-1,1))\n\n#transform the data\ntrain.Item_MRP = scaler.transform(np.array(train.Item_MRP).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:09.084679Z","iopub.execute_input":"2021-05-30T20:23:09.084984Z","iopub.status.idle":"2021-05-30T20:23:09.12117Z","shell.execute_reply.started":"2021-05-30T20:23:09.084955Z","shell.execute_reply":"2021-05-30T20:23:09.120162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the Model\nWe will use the Linear Regression and the Random Forest Regressor to predict the sales. We will create a validation set using the train_test_split() function.\n\ntest_size = 0.25 such that the validation set holds 25% of the data points while the train set has 75%.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n#seperate the independent and target variable\ntrain_X = train.drop(columns=['Item_Identifier', 'Item_Outlet_Sales'])\ntrain_Y = train['Item_Outlet_Sales']\n\n#split the data\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_X, train_Y, test_size=0.25) \n\n#shape of train test splits\ntrain_x.shape, valid_x.shape, train_y.shape, valid_y.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:09.12226Z","iopub.execute_input":"2021-05-30T20:23:09.122576Z","iopub.status.idle":"2021-05-30T20:23:09.399837Z","shell.execute_reply.started":"2021-05-30T20:23:09.122548Z","shell.execute_reply":"2021-05-30T20:23:09.398706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have split our data, we will train a linear regression model on this data and check its performance on the validation set. We will use RMSE as an evaluation metric.","metadata":{}},{"cell_type":"code","source":"#LinearRegression\nLR = LinearRegression()\n\n#fit the model\nLR.fit(train_x, train_y)\n\n#predict the target on train and validation data\ntrain_pred = LR.predict(train_x)\nvalid_pred = LR.predict(valid_x)\n\n# RMSE on train and validation data\nprint('RMSE on train data: ', mean_squared_error(train_y, train_pred)**(0.5))\nprint('RMSe on validation data: ', mean_squared_error(valid_y, valid_pred)**(0.5))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:09.401377Z","iopub.execute_input":"2021-05-30T20:23:09.401706Z","iopub.status.idle":"2021-05-30T20:23:09.467251Z","shell.execute_reply.started":"2021-05-30T20:23:09.401676Z","shell.execute_reply":"2021-05-30T20:23:09.465852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will train a random forest regressor and see if we can get an improvement on the train and validation errors.","metadata":{}},{"cell_type":"code","source":"#RandomForestRegressor\nRFR = RandomForestRegressor(max_depth=10)\n\n#fitting the model\nRFR.fit(train_x, train_y)\n\n#predict the target on train and validation data\ntrain_pred = RFR.predict(train_x)\nvalid_pred = RFR.predict(valid_x)\n\n#RMSE on train and test data\nprint('RMSE on train data :', mean_squared_error(train_y, train_pred)**(0.5))\nprint('RMSE on validation data :', mean_squared_error(valid_y, valid_pred)**(0.5))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:09.469133Z","iopub.execute_input":"2021-05-30T20:23:09.46962Z","iopub.status.idle":"2021-05-30T20:23:11.745903Z","shell.execute_reply.started":"2021-05-30T20:23:09.469576Z","shell.execute_reply":"2021-05-30T20:23:11.744685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a significant improvement on the RMSE values. The random forest algorithm gives us 'feature importance for all the variables in the data.\n\nWe have 45 features and not all of these features may be useful in forecasting. We will select the top 7 features which had a major contribution in forecasting sales values.\n\nIf the model performance is similar in both cases (by using 45 features and by using 7 features), then we should only use the top 7 features, in order to keep the model simple and efficient.\n\nThe goal is to have a less complex model without compromising on the overall model performance.","metadata":{}},{"cell_type":"code","source":"#plot the 7 most important features\nplt.figure(figsize=(10,8))\nfeat_importances = pd.Series(RFR.feature_importances_, index = train_x.columns)\nfeat_importances.nlargest(7).plot(kind='barh');","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:11.747439Z","iopub.execute_input":"2021-05-30T20:23:11.747863Z","iopub.status.idle":"2021-05-30T20:23:11.988393Z","shell.execute_reply.started":"2021-05-30T20:23:11.747813Z","shell.execute_reply":"2021-05-30T20:23:11.987296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training data with top 7 features\ntrain_x_7 = train_x[['Item_MRP',\n                      'Outlet_Type_Grocery Store',\n                      'Item_Visibility',\n                      'Outlet_Identifier_OUT027',\n                      'Outlet_Type_Supermarket Type3',\n                      'Item_Weight',\n                      'Outlet_Establishment_Year']]\n\n#validation data with top 7 important features\nvalid_x_7 = valid_x[['Item_MRP',\n                      'Outlet_Type_Grocery Store',\n                      'Item_Visibility',\n                      'Outlet_Identifier_OUT027',\n                      'Outlet_Type_Supermarket Type3',\n                      'Item_Weight',\n                      'Outlet_Establishment_Year']]\n\n#create an object of the RandomForestRegressor Model\nRFR_with_7 = RandomForestRegressor(max_depth=10, random_state=2)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:11.989714Z","iopub.execute_input":"2021-05-30T20:23:11.990007Z","iopub.status.idle":"2021-05-30T20:23:11.998096Z","shell.execute_reply.started":"2021-05-30T20:23:11.989978Z","shell.execute_reply":"2021-05-30T20:23:11.996974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fit the model\nRFR_with_7.fit(train_x_7, train_y)\n\n#predict the target on the training and validation data\npred_train_with_7 = RFR_with_7.predict(train_x_7)\npred_valid_with_7 = RFR_with_7.predict(valid_x_7)\n\n#RMSE on train and validation data\nprint('RMSE on train data: ', mean_squared_error(train_y, pred_train_with_7)**(0.5))\nprint('RMSE on validation data: ', mean_squared_error(valid_y, pred_valid_with_7)**(0.5))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:11.999569Z","iopub.execute_input":"2021-05-30T20:23:12.000051Z","iopub.status.idle":"2021-05-30T20:23:13.372311Z","shell.execute_reply.started":"2021-05-30T20:23:12.000018Z","shell.execute_reply":"2021-05-30T20:23:13.371119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using only 7 features has given us almost the same perfomance as the previous model where we were using 45 features. Now we will identify the final set of features that we need and the preprocessing steps for each of them.","metadata":{}},{"cell_type":"markdown","source":"# Identifying features to build the Machine Learning pipeline\nWe must list down the final set of features and necessary preprocessing steps for each of them, to be used in the ML pipeline. Since the RandomForestRegressor model with 7 features gave us almost the same performance as the previous model with 45 features, we will only use these features for our ML pipeline.","metadata":{}},{"cell_type":"markdown","source":"# Selected features and preprocessing steps\n* **Item_MRP:** It holds the price of the products. During the preprocessing step we used a standard scaler to scale these values.\n* **Outlet_Type_Grocery Store:** A binary column which indcates if the outlet type is a grocery store or not. To use this information in the model building process, we will add a binary feature in the existing data that contains 1 (if outlet type is a grocery store) and 0 (if the outlet type is something else).\n* **Item_Visibility:** Denotes visibility of products in the store. Since this variable had a small value range and no missing values, we did not apply any preprocessing steps on this variable.\n* **Outlet_Type_Supermarket Type3:** Another binary column indicating if the outlet type is a 'supermarket_type_3' or not. To capture this information we will create a binary feature that stores 1 (if outlet type is supermarket_type_3) and 0 (if not).\n* **Outlet_Identifier_OUT027:8** This feature specifies whether the outlet identifier is 'OUT027' or not. Similar to the  previous example, we will create a seperate column that carries 1 (if outlet identifier is OUT027) or 0 (if otherwise).\n* **Outlet_Establishment_Year:** This describes the year of establishment of the stores. Since we did not perform any transformation on values in this column, we will not preprocess it in the pipeline.\n* **Item_Weight:** During preprocessing we observed that this column had missing values. These missing values were imputed using the average of the column. This has to be taken into account while building the pipeline.\n\nWe will drop the other columns since we will not use them to train the model.\n","metadata":{}},{"cell_type":"markdown","source":"# Pipeline Design\nWe have built a prototype to understand the preprocessing requirement for our data. It is now time to form a pipeline design based on our learning from the prototype. We will define the pipeline in 3 stages:\n\n1. Create the required binary features\n2. Perform required data preprocessing and transformations:\n*  Drop the columns that are not required\n*  Missing value imputation (Item_Weight) by average\n*  Scale the Item_MRP\n3. Random Forest Regressor","metadata":{}},{"cell_type":"markdown","source":"# 1. Create the required binary features\nWe will create a custom transformer that will add 3 new binary columns to the existing data.\n\n* Outlet_Type: Grocery Store\n* Outlet_Type: Supermarket Type3\n* Outlet_Identifier_OUT027","metadata":{}},{"cell_type":"markdown","source":"# 2. Data Preprocessing and transformations\nWe will use a column transformer to do the required transformations. It will contain 3 steps:\n\n* Drop the columns that are not required for model training\n* Impute missing values in the column Item_Weight using the average\n* Scale the column Item_MRP using StandardScaler()","metadata":{}},{"cell_type":"markdown","source":"# 3. Use the model to predict the target on the cleaned data\nThis will be the final step in the pipeline. In the last two steps we preprocessed the data and made it ready for the model building process. We will use this data and build a machine learning model to predict the Item Outlet Sales.","metadata":{}},{"cell_type":"markdown","source":"# Building the pipeline\nWe will read the data set and seperate the independent and target variable from the training dataset.","metadata":{}},{"cell_type":"code","source":"#importing required libraries\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nimport category_encoders as ce \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:13.373757Z","iopub.execute_input":"2021-05-30T20:23:13.374066Z","iopub.status.idle":"2021-05-30T20:23:13.392561Z","shell.execute_reply.started":"2021-05-30T20:23:13.374036Z","shell.execute_reply":"2021-05-30T20:23:13.391447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read training dataset\ntrain = pd.read_csv(\"../input/big-mart-sales-prediction/Train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:13.394038Z","iopub.execute_input":"2021-05-30T20:23:13.394461Z","iopub.status.idle":"2021-05-30T20:23:13.419534Z","shell.execute_reply.started":"2021-05-30T20:23:13.394419Z","shell.execute_reply":"2021-05-30T20:23:13.418592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#seperate the independent and target variables\ntrain_x = train.drop(columns=['Item_Outlet_Sales'])\ntrain_y = train['Item_Outlet_Sales']","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:13.420811Z","iopub.execute_input":"2021-05-30T20:23:13.421331Z","iopub.status.idle":"2021-05-30T20:23:13.427196Z","shell.execute_reply.started":"2021-05-30T20:23:13.421299Z","shell.execute_reply":"2021-05-30T20:23:13.42633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to create 3 new binary columns using a custom transformer. Here are the steps we need to follow to create a custom transformer.\n\n* Define a class OutletTypeEncoder\n* Add the parameter BaseEstimator while defining the class\n* The class must contain fit and transform methods\n* In the transform method, we will define all the 3 columns that we want after the first stage in our ML pipeline.","metadata":{}},{"cell_type":"code","source":"# import the BaseEstimator\nfrom sklearn.base import BaseEstimator\n\n# define the class OutletTypeEncoder\n# This will be our custom transformer that will create 3 new binary columns\n# custom transformer must have methods fit and transform\n\nclass OutletTypeEncoder(BaseEstimator):\n\n    def __init__(self):\n        pass\n\n    def fit(self, documents, y=None):\n        return self\n\n    def transform(self, x_dataset):\n        x_dataset['outlet_grocery_store'] = (x_dataset['Outlet_Type'] == 'Grocery Store')*1\n        x_dataset['outlet_supermarket_3'] = (x_dataset['Outlet_Type'] == 'Supermarket Type3')*1\n        x_dataset['outlet_identifier_OUT027'] = (x_dataset['Outlet_Identifier'] == 'OUT027')*1\n        \n        return x_dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:13.428629Z","iopub.execute_input":"2021-05-30T20:23:13.428913Z","iopub.status.idle":"2021-05-30T20:23:13.440016Z","shell.execute_reply.started":"2021-05-30T20:23:13.428886Z","shell.execute_reply":"2021-05-30T20:23:13.438861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we will define the pre-processing steps required before the model building process.\n\n* Drop the columns – Item_Identifier, Outlet_Identifier, Item_Fat_Content, Item_Type, Outlet_Identifier, Outlet_Size, Outlet_Location_Type and Outlet_Establishment_Year\n* Impute missing values in column Item_Weight with mean\n* Scale the column Item_MRP using StandardScaler().\nThis will be the second step in our machine learning pipeline. After this step, the data will be ready to be used by the model to make predictions.","metadata":{}},{"cell_type":"code","source":"# Drop the columns - \n# Impute the missing values in column Item_Weight by mean\n# Scale the data in the column Item_MRP\npre_process = ColumnTransformer(remainder='passthrough',\n                                transformers=[('drop_columns', 'drop', ['Item_Identifier',\n                                                                        'Outlet_Identifier',\n                                                                        'Item_Fat_Content',\n                                                                        'Item_Type',\n                                                                        'Outlet_Identifier',\n                                                                        'Outlet_Size',\n                                                                        'Outlet_Location_Type',\n                                                                        'Outlet_Type'\n                                                                       ]),\n                                              ('impute_item_weight', SimpleImputer(strategy='mean'), ['Item_Weight']),\n                                              ('scale_data', StandardScaler(),['Item_MRP'])])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:13.44104Z","iopub.execute_input":"2021-05-30T20:23:13.44132Z","iopub.status.idle":"2021-05-30T20:23:13.451366Z","shell.execute_reply.started":"2021-05-30T20:23:13.441294Z","shell.execute_reply":"2021-05-30T20:23:13.450403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict the target\nThis will be the final block of the machine learning pipeline. We will specify 3 steps – create binary columns, preprocess the data, train a model.\n\nWhen we use the fit() function with a pipeline object, all three steps are executed. Post the model training process, we use the predict() function that uses the trained model to generate the predictions.","metadata":{}},{"cell_type":"code","source":"# Define the Pipeline\n\"\"\"\nStep1: get the oultet binary columns\nStep2: pre processing\nStep3: Train a Random Forest Model\n\"\"\"\nmodel_pipeline = Pipeline(steps=[('get_outlet_binary_columns', OutletTypeEncoder()), \n                                 ('pre_processing',pre_process),\n                                 ('random_forest', RandomForestRegressor(max_depth=10,random_state=2))\n                                 ])\n# fit the pipeline with the training data\nmodel_pipeline.fit(train_x,train_y)\n\n# predict target values on the training data\nmodel_pipeline.predict(train_x)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:13.452498Z","iopub.execute_input":"2021-05-30T20:23:13.452912Z","iopub.status.idle":"2021-05-30T20:23:15.236753Z","shell.execute_reply.started":"2021-05-30T20:23:13.452882Z","shell.execute_reply":"2021-05-30T20:23:15.235723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will read the test data set and we call predict function only on the pipeline object to make predictions on the test data.","metadata":{}},{"cell_type":"code","source":"# read the test data\ntest_data = pd.read_csv(\"../input/big-mart-sales-prediction/Test.csv\")\n\n# predict target variables on the test data \ny_sub = model_pipeline.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:15.238042Z","iopub.execute_input":"2021-05-30T20:23:15.238488Z","iopub.status.idle":"2021-05-30T20:23:15.365152Z","shell.execute_reply.started":"2021-05-30T20:23:15.238457Z","shell.execute_reply":"2021-05-30T20:23:15.36397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:15.366151Z","iopub.execute_input":"2021-05-30T20:23:15.366443Z","iopub.status.idle":"2021-05-30T20:23:15.37237Z","shell.execute_reply.started":"2021-05-30T20:23:15.366415Z","shell.execute_reply":"2021-05-30T20:23:15.370999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/big-mart-sales-prediction/Submission.csv\")\nsub[\"Item_Outlet_Sales\"] = y_sub\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:23:15.373615Z","iopub.execute_input":"2021-05-30T20:23:15.37391Z","iopub.status.idle":"2021-05-30T20:23:15.410181Z","shell.execute_reply.started":"2021-05-30T20:23:15.373883Z","shell.execute_reply":"2021-05-30T20:23:15.409092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-05-30T20:23:15.411626Z","iopub.execute_input":"2021-05-30T20:23:15.411931Z","iopub.status.idle":"2021-05-30T20:23:15.443936Z","shell.execute_reply.started":"2021-05-30T20:23:15.411902Z","shell.execute_reply":"2021-05-30T20:23:15.442688Z"},"trusted":true},"execution_count":null,"outputs":[]}]}