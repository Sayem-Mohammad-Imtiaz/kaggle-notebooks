{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't forget to turn on internet\n!pip install keras==2.1.2\n!pip install tensorflow==1.14.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\ntensorflow.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras\ntensorflow.keras.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nkeras.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ncwd = os.getcwd()\ncwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras import activations\nfrom keras import utils\nfrom keras.datasets import cifar10\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# the squashing function.\n# we use 0.5 in stead of 1 in hinton's paper.\n# if 1, the norm of vector will be zoomed out.\n# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n# and be zoomed out while original norm is greater than 0.5.\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n    return scale * x\n\n\n# define our own softmax function instead of K.softmax\n# because K.softmax can not specify axis.\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex / K.sum(ex, axis=axis, keepdims=True)\n\n\n# define the margin loss like hinge loss\ndef margin_loss(y_true, y_pred):\n    lamb, margin = 0.5, 0.1\n    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n\n\nclass Capsule(Layer):\n    \"\"\"A Capsule Implement with Pure Keras\n    There are two vesions of Capsule.\n    One is like dense layer (for the fixed-shape input),\n    and the other is like timedistributed dense (for various length input).\n\n    The input shape of Capsule must be (batch_size,\n                                        input_num_capsule,\n                                        input_dim_capsule\n                                       )\n    and the output shape is (batch_size,\n                             num_capsule,\n                             dim_capsule\n                            )\n\n    Capsule Implement is from https://github.com/bojone/Capsule/\n    Capsule Paper: https://arxiv.org/abs/1710.09829\n    \"\"\"\n\n    def __init__(self,\n                 num_capsule,\n                 dim_capsule,\n                 routings=3,\n                 share_weights=True,\n                 activation='squash',\n                 **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n        if activation == 'squash':\n            self.activation = squash\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(1, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(input_num_capsule, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n\n    def call(self, inputs):\n        \"\"\"Following the routing algorithm from Hinton's paper,\n        but replace b = b + <u,v> with b = <u,v>.\n\n        This change can improve the feature representation of Capsule.\n\n        However, you can replace\n            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n        with\n            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n        to realize a standard routing.\n        \"\"\"\n\n        if self.share_weights:\n            hat_inputs = K.conv1d(inputs, self.kernel)\n        else:\n            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        hat_inputs = K.reshape(hat_inputs,\n                               (batch_size, input_num_capsule,\n                                self.num_capsule, self.dim_capsule))\n        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n\n        b = K.zeros_like(hat_inputs[:, :, :, 0])\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(o, hat_inputs, [2, 3])\n                if K.backend() == 'theano':\n                    o = K.sum(o, axis=1)\n\n        return o\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg=mpimg.imread('/kaggle/input/sign-language-mnist/american_sign_language.PNG')\nimgplot = plt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_height = 28\nimg_width = 28\n\nmnistdata = pd.read_csv(\"/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\")\nlabeltrain_mnistdata = mnistdata.iloc[:,:1].to_numpy()\ntrain_mnistdata = mnistdata.iloc[:,1:].to_numpy()\ntrain_mnistdata = np.reshape(train_mnistdata,(train_mnistdata.shape[0], img_width,img_height, 1))\n\ntest_mnistdata = pd.read_csv(\"/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\")\nlabeltest_mnistdata = mnistdata.iloc[:,:1].to_numpy()\ntest_mnistdata = mnistdata.iloc[:,1:].to_numpy()\ntest_mnistdata = np.reshape(train_mnistdata,(train_mnistdata.shape[0], img_width,img_height, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nnum_classes = len(np.unique(labeltrain_mnistdata)) + 1\nepochs = 5\ndshape = 1\nrouting = 3\n\nx_train = train_mnistdata.astype('float32')\nx_test = test_mnistdata.astype('float32')\nx_train /= 255\nx_test /= 255\ny_train = utils.to_categorical(labeltrain_mnistdata.flatten(), num_classes)\ny_test = utils.to_categorical(labeltest_mnistdata.flatten(), num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A common Conv2D model\ninput_image = Input(shape=(None, None, dshape))\nx = Conv2D(64, (3, 3), activation='relu')(input_image)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = AveragePooling2D((2, 2))(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\n\n\n\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\nthen connect a Capsule layer.\n\nthe output of final model is the lengths of 10 Capsule, whose dim=16.\n\nthe length of Capsule is the proba,\nso the problem becomes a 10 two-classification problem.\n\"\"\"\n\nx = Reshape((-1, 128))(x)\ncapsule = Capsule(num_classes, 16, routing, True)(x)\noutput = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\nmodel = Model(inputs=input_image, outputs=output)\n\n# we use a margin loss\nmodel.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n# tf.keras.utils.plot_model(model, show_shapes=True)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Without Lambda\n# A common Conv2D model\ninput_image = Input(shape=(None, None, dshape))\nx = Conv2D(64, (3, 3), activation='relu')(input_image)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = AveragePooling2D((2, 2))(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\n\n\n\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\nthen connect a Capsule layer.\n\nthe output of final model is the lengths of 10 Capsule, whose dim=16.\n\nthe length of Capsule is the proba,\nso the problem becomes a 10 two-classification problem.\n\"\"\"\n\nx = Reshape((-1, 128))(x)\ncapsule = Capsule(num_classes, 16, routing, True)(x)\n# output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\ncapsule = Flatten()(capsule)\ncapsule = Dropout(.5)(capsule)\noutput = Dense(num_classes, activation='sigmoid')(capsule)\nmodel2 = Model(inputs=input_image, outputs=output)\n\n# we use a margin loss\nmodel2.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n# tf.keras.utils.plot_model(model, show_shapes=True)\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can compare the performance with or without data augmentation\ndata_augmentation = True\nmc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nif ModelTrain is None:\n    ModelTrain = model2\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    ModelTrain.fit(\n        x_train,\n        y_train,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=(x_test, y_test),\n        shuffle=True, callbacks=[mc,es])\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by dataset std\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n        width_shift_range=0.1,  # randomly shift images horizontally\n        height_shift_range=0.1,  # randomly shift images vertically\n        shear_range=0.,  # set range for random shear\n        zoom_range=0.,  # set range for random zoom\n        channel_shift_range=0.,  # set range for random channel shifts\n        # set mode for filling points outside the input boundaries\n        fill_mode='nearest',\n        cval=0.,  # value used for fill_mode = \"constant\"\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False,  # randomly flip images\n        # set rescaling factor (applied before any other transformation)\n        rescale=None,\n        # set function that will be applied on each input\n        preprocessing_function=None,\n        # image data format, either \"channels_first\" or \"channels_last\"\n        data_format=None\n        )\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    ModelTrain.fit_generator(\n        datagen.flow(x_train, y_train, batch_size=batch_size),\n        epochs=epochs,\n        validation_data=(x_test, y_test),\n        workers=4, callbacks=[mc,es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = 10\nto = 15\nfor x in range(start, to):\n    print(\"Predict:\" + str(np.argmax(ModelTrain.predict([x_test[x:x+1]]))))\n    print(\"Result:\" + str(np.argmax(y_test[x:x+1])))\n    a = x_test[x:x+1][0]\n    lum_img = a[:, :, 0]\n    plt.imshow(lum_img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ModelTrain.save(\"model_notlambda.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cwd","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}