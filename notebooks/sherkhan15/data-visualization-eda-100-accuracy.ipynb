{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nimport keras\nfrom keras.utils import to_categorical\nimport seaborn as sns\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import models\nfrom keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/forest-cover-type-dataset/covtype.csv',index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# We can see all columns have diffrent data types , float64(47), int64(7).","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* No Misssing values are presents"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SHAPE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\n\n# We can see that there are 154340 instances having 55 attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical description\n\npd.set_option('display.max_columns', None)\nprint(df.describe())\n\n# Learning :\n# No attribute is missing as count is 581012 for all attributes. Hence, all rows can be used\n# Negative value(s) present in Vertical_Distance_To_Hydrology. Hence, some tests such as chi-sq cant be used.\n# Wilderness_Area and Soil_Type are one hot encoded. Hence, they could be converted back for some analysis\n# Scales are not the same for all. Hence, rescaling and standardization may be necessary for some algos","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SKEWNESS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skewness of the distribution\n\nprint(df.skew())\n\n# Values close to 0 show less skew\n# Several attributes in Soil_Type show a large skew. Hence, some algos may benefit if skew is corrected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CLASS DISTRIBUTION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of instances belonging to each class\n\ndf.groupby('Cover_Type').size()\n\n\n# We see that all classes not have an equal presence. So, class re-balancing is necessary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=df['Cover_Type']\nsns.countplot(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = df.groupby('Cover_Type')\ng = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))\ng.head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explorartory Data Analysis**"},{"metadata":{},"cell_type":"markdown","source":"DATA INTERGRATION\n\n1. Correlation\n \n* Correlation tells relation between two attributes.\n* Correlation requires continous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\n\n\n\n#sets the number of features considered\nsize = 10 \n\n#create a dataframe with only 'size' features\ndata=df.iloc[:,:size] \n\n#get the names of all the columns\ncols=data.columns \n\n# Calculates pearson co-efficient for all combinations\ndata_corr = data.corr()\n\n# Set the threshold to select only only highly correlated attributes\nthreshold = 0.5\n\n# List of pairs along with correlation above threshold\ncorr_list = []\n\n#Search for the highly correlated pairs\nfor i in range(0,size): #for 'size' features\n    for j in range(i+1,size): #avoid repetition\n        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n            corr_list.append([data_corr.iloc[i,j],i,j]) #store correlation and columns index\n\n#Sort to show higher ones first            \ns_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n\n#Print correlations and column names\nfor v,i,j in s_corr_list:\n    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n\n# Strong correlation is observed between the following pairs\n# This represents an opportunity to reduce the feature set through transformations such as PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) Scatter Plot(pairlot)\n\nThe plots show to which class does a point belong to. The class distribution overlaps in the plots.\n\nHillshade patterns give a nice ellipsoid patterns with each other\n\nAspect and Hillshades attributes form a sigmoid pattern\n\nHorizontal and vertical distance to hydrology give an almost linear pattern."},{"metadata":{"trusted":true},"cell_type":"code","source":"for v,i,j in s_corr_list:\n    sns.pairplot(df, hue=\"Cover_Type\", height=6, x_vars=cols[i],y_vars=cols[j] )\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA VISUALIZATION"},{"metadata":{},"cell_type":"markdown","source":"\n\n*   HEAT MAP\n*   BOX PLOT\n*   PAIR PLOT\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = df.columns\ncol_list = [col for col in col_list if not col[0:4]=='Soil']\nfig, ax = plt.subplots(figsize=(10,10))  \nsns.heatmap(df[col_list].corr(),square=True,linewidths=1)\nplt.title('Correlation of Variables')\n\nplt.figure(figsize=(10,10))\nsns.boxplot(y='Slope',x='Cover_Type', data= df )\nplt.title('slope vs Cover_Type')\n\n\nsns.pairplot( df, hue='Cover_Type',vars=['Aspect','Slope','Hillshade_9am','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Fire_Points'],diag_kind=\"kde\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LM PLOT**\n\n*  Horizontal_Distance_To_Hydrology & Vertical_Distance_To_Hydrology with Soil_Type2\n*  Horizontal_Distance_To_Hydrology & Vertical_Distance_To_Hydrologywith Wilderness_Area1\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.lmplot(x='Horizontal_Distance_To_Hydrology', y='Vertical_Distance_To_Hydrology', data=df, hue='Soil_Type2',fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='Horizontal_Distance_To_Hydrology', y='Vertical_Distance_To_Hydrology', data=df, hue='Wilderness_Area1',fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Violin Plot** - a combination of box and density plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#names of all the attributes \ncols = df.columns\n\n#number of attributes (exclude target)\nsize = len(cols)-1\n\n#x-axis has target attribute to distinguish between classes\nx = cols[size]\n\n#y-axis shows values of an attribute\ny = cols[0:size]\n\n#Plot violin for all attributes\nfor i in range(0,size):\n    sns.violinplot(data=df,x=x,y=y[i])  \n    plt.show()\n\n#Elevation is has a separate distribution for most classes. Highly correlated with the target and hence an important attribute\n#Aspect contains a couple of normal distribution for several classes\n#Horizontal distance to road and hydrology have similar distribution\n#Hillshade 9am and 12pm display left skew\n#Hillshade 3pm is normal\n#Lots of 0s in vertical distance to hydrology\n#Wilderness_Area3 gives no class distinction. As values are not present, others gives some scope to distinguish\n#Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning \n\n*  Remove unnecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removal list initialize\nrem = []\n\n#Add constant columns as they don't help in prediction process\nfor c in df.columns:\n    if df[c].std() == 0: #standard deviation is zero\n        rem.append(c)\n\n#drop the columns        \ndf.drop(rem,axis=1,inplace=True)\n\nprint(rem)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Normalizing DataSet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nx = df[df.columns[:55]]\ny = df.Cover_Type\nx_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.7, random_state =  90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Select numerical columns which needs to be normalized*"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_norm = x_train[x_train.columns[0:10]]\ntest_norm = x_test[x_test.columns[0:10]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n*Normalize Training Data*\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scale = preprocessing.StandardScaler().fit(train_norm)\nx_train_norm = std_scale.transform(train_norm)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting numpy array to dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_norm_col = pd.DataFrame(x_train_norm, index=train_norm.index, columns=train_norm.columns) \nx_train.update(training_norm_col)\nprint (x_train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Normalize Testing Data by using mean and SD of training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_norm = std_scale.transform(test_norm)\ntesting_norm_col = pd.DataFrame(x_test_norm, index=test_norm.index, columns=test_norm.columns) \nx_test.update(testing_norm_col)\nprint (x_train.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As y variable is multi class categorical variable, hence using softmax as activation function and sparse-categorical cross entropy as loss function."},{"metadata":{},"cell_type":"markdown","source":"> ****Validating Data Through Relu Function****"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n keras.layers.Dense(64, activation=tf.nn.relu,                  \n input_shape=(x_train.shape[1],)),\n keras.layers.Dense(64, activation=tf.nn.relu),\n keras.layers.Dense(8, activation=  'softmax')\n ])\n\nmodel.compile(optimizer=tf.optimizers.Adam(),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nhistory2 = model.fit(\n x_train, y_train,\n epochs=5, batch_size = 60,\n validation_data = (x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize Training History**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\nhistory = model.fit(x_train, y_train, epochs=5,validation_split=0.7, shuffle=True)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}