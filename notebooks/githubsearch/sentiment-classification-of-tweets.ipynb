{"cells":[{"metadata":{"_cell_guid":"76e1b511-ae03-df3d-0792-3be5e6a63153","_uuid":"90b9ea85b7a87baeb03a41bcadb4f570ca4f24f1"},"cell_type":"markdown","source":"# Tweets Airlines Sentiments"},{"metadata":{"_cell_guid":"5071cfba-64de-5e2b-40e9-b353681690ac","_uuid":"9e84e7168cf9f1fb6705a250f2c30dfe74e6d1da","trusted":true,"collapsed":true},"cell_type":"code","source":"# Before we begin, we supress deprecation warnings resulting from nltk on Kaggle\nimport warnings\nimport gensim\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"5d288755-58cd-636a-dba8-561c23ff33fc","_uuid":"b625943056eb5a01f0dd79eb5375fd85c83be3d8"},"cell_type":"markdown","source":"# Sentiment classification on tweets about airlines\n\nThis notebook describes an attempt to classify tweets by sentiment. It describes the initial data exploration, as well as implementation of a classifier."},{"metadata":{"_cell_guid":"3d966459-d055-b849-d586-536c60bb951e","_uuid":"28afb74d2b4e0500cf7d4352361b5e4336734a8a"},"cell_type":"markdown","source":"## What is in the dataset?\n\nIt's always good to start by exploring the data that we have available. To do this we load the raw csv file using [Pandas][1] and check what the columns are.\n\n  [1]: http://pandas.pydata.org/"},{"metadata":{"_cell_guid":"02dd092c-c757-4f78-3eee-322f3ea2dfb3","_uuid":"55b3e678b880cd347a1067c19ac6138a5dfaa790","trusted":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd\ntweets = pd.read_csv(\"../input/Tweets.csv\")\nlist(tweets.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42a73f2f-cf7a-3f68-bd4d-551ef451fe33","_uuid":"f6e1c1c1f5566ce0449c236fa8c6184d53607e96"},"cell_type":"markdown","source":"We want to be able to determine the sentiment of a tweet without any other information but the tweet text itself, hence the 'text' column is our focus. Using the text we are going to try and predict 'airline_sentiment'.\n\nFirst we take a look at what a typical record looks like."},{"metadata":{"_cell_guid":"80ebba57-ca6f-2ec0-ab83-bc244490acab","_uuid":"ab5337b9c892457d9cd1660a75393115868db1a6","trusted":false,"collapsed":true},"cell_type":"code","source":"tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc5faad2-80ef-97f3-caaa-e2bf5e4a3332","_uuid":"4f346b461d14ccab1119f933ffe7d547774cd50e"},"cell_type":"markdown","source":"Now lets take a look at what sentiments have been found."},{"metadata":{"_cell_guid":"db4886e4-2fdc-e0a3-f1b3-12ddf8f16f8f","_uuid":"de66ad9198919ed8ea21b30855287ce78561650e","trusted":false,"collapsed":true},"cell_type":"code","source":"sentiment_counts = tweets.airline_sentiment.value_counts()\nnumber_of_tweets = tweets.tweet_id.count()\nprint(sentiment_counts)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e14bd936-6a24-d436-b6fc-0ee7d91b57ab","_uuid":"7eedf39b0105801bafddfdb4ee2ece95eb2b9cc2","trusted":false,"collapsed":true},"cell_type":"code","source":"dff = tweets.groupby([\"airline\", \"airline_sentiment\" ]).count()[\"name\"]\ndff","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1cd20ee6-34c1-1944-cf20-e45d15e098d3","_uuid":"6a2ce4a05d0f68b2fccd01169efa5511a30c00fd","trusted":false,"collapsed":true},"cell_type":"code","source":"df_companySentiment = dff.to_frame().reset_index()\ndf_companySentiment.columns = [\"airline\", \"airline_sentiment\", \"count\"]\ndf_companySentiment\n\n#df2 = dff.pivot('airline', 'airline_sentiment')\n#df2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b83eb84a-7e8f-2f37-3413-a1da3f38efad","_uuid":"5d76f8ec9cee54aba37bf1a4dd3dd7c3ff029d00","trusted":false,"collapsed":true},"cell_type":"code","source":"df2 = df_companySentiment\ndf2.index = df2['airline']\ndel df2['airline']\ndf2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1bb4d4f2-f198-d40d-0add-e1695a023fa1","_uuid":"d0625ada5a99de09a4091e5ab5f2c76175c48465","trusted":false,"collapsed":true},"cell_type":"code","source":"dff","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af011865-b7bc-4b69-f5cd-6252388719c4","_uuid":"5a7632e9440d9d04c78405c11d26bb4ed244a0e4","trusted":false,"collapsed":true},"cell_type":"code","source":"df3 = dff.pivot('airline', 'airline_sentiment')\ndf3","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22d40bfc-8c9e-361f-0f09-e96efcc359f9","_uuid":"adf4419e2d040055be370abc5ab2bc5effbac6d2","trusted":false,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.style\n%matplotlib inline\nimport matplotlib.style\nfrom matplotlib.pyplot import subplots\nmatplotlib.style.use('ggplot')\n\nfig, ax = subplots()\nmy_colors =['darksalmon', 'papayawhip', 'cornflowerblue']\ndf2.plot(kind='bar', stacked=True, ax=ax, color=my_colors, figsize=(12, 7), width=0.8)\nax.legend([\"Negative\", \"Neutral\", \"Positive\"])\nplt.title(\"Tweets Sentiments Analysis Airlines, 2017\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a274430-49f1-3bbe-10b6-0d7d0d18d0d8","_uuid":"01c54068856df1ddd356b3a8678c8c04f2b9e6d6"},"cell_type":"markdown","source":"It turns out that our dataset is unbalanced with significantly more negative than positive tweets. We will focus on the issue of identifying negative tweets, and hence treat neutral and positive as one class. It's good to keep in mind that, while a terrible classifier, if we always guessed a tweet was negative we'd be right 62.7% of the time (9178 of 14640). That clearly wouldn't be a very useful classifier, but worth to remember."},{"metadata":{"_cell_guid":"4bec5f35-a4fb-4469-348d-9aff1e058921","_uuid":"8d8649f1082512fcac1bde0cd4d189ac8a80b464"},"cell_type":"markdown","source":"# What characterizes text of different sentiments?\n\nWhile we still haven't decided what classification method to use, it's useful to get an idea of how the different texts look. This might be an \"old school\" approach in the age of deep learning, but lets indulge ourselves nevertheless. \n\nTo explore the data we apply some crude preprocessing. We will tokenize and lemmatize using [Python NLTK][1], and transform to lower case. As words mostly matter in context we'll look at bi-grams instead of just individual tokens.\n\nAs a way to simplify later inspection of results we will store all processing of data together with it's original form. This means we will extend the Pandas dataframe into which we imported the raw data with new columns as we go along.\n\n### Preprocessing\nNote that we remove the first two tokens as they always contain \"@ airline_name\". We begin by defining our normalization function.\n\n\n  [1]: http://www.nltk.org/"},{"metadata":{"_cell_guid":"733ade22-d351-8e80-d455-c780c0997c0e","_uuid":"946fc057b34bfc30c037451c87a60c7c737abc8c","trusted":false,"collapsed":true},"cell_type":"code","source":"import re, nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef normalizer(tweet):\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    tokens = nltk.word_tokenize(only_letters)[2:]\n    lower_case = [l.lower() for l in tokens]\n    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e1268e1-6303-2b30-efe2-fc5060f38cf6","_uuid":"75db96e6da84f02191829bfd4692ba295063dd02","trusted":false,"collapsed":true},"cell_type":"code","source":"normalizer(\"I recently wrote some texts.\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23f8feff-27e7-aadb-07cb-640b5c62a20b","_uuid":"6d75ecf7fafeb2036dc081743026fe19cffa26bb","trusted":false,"collapsed":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\ntweets['normalized_tweet'] = tweets.text.apply(normalizer)\ntweets[['text','normalized_tweet']].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba599ef6-8420-bfb6-4d58-522ecf36dacd","_uuid":"c6f062abb8555dc45384dc621f8cb8c14eb33f65","trusted":false,"collapsed":true},"cell_type":"code","source":"from nltk import ngrams\ndef ngrams(input_list):\n    #onegrams = input_list\n    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n    return bigrams+trigrams\ntweets['grams'] = tweets.normalized_tweet.apply(ngrams)\ntweets[['grams']].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdaed14a-cb84-040c-3f8f-203b269aab76","_uuid":"fd20944c3a8beb04abe9e4c1e39a540931a946a0"},"cell_type":"markdown","source":"And now some counting."},{"metadata":{"_cell_guid":"5901f465-8b2d-ec62-d502-ba94a74b550f","_uuid":"1ec78b40a56e937a58834b17686542a5d75c124d","trusted":false,"collapsed":true},"cell_type":"code","source":"import collections\ndef count_words(input):\n    cnt = collections.Counter()\n    for row in input:\n        for word in row:\n            cnt[word] += 1\n    return cnt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a393517-0a81-2e03-3b79-0157ef7f527d","_uuid":"26c96a2a22edd453e019bcdb40699217a62a7ccb","trusted":false,"collapsed":true},"cell_type":"code","source":"tweets[(tweets.airline_sentiment == 'negative')][['grams']].apply(count_words)['grams'].most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bb397b6a-ecc5-0b71-02f0-6bad9847ec81","_uuid":"86d8e881e819eff74e2ea02c5e3904aff93a845a"},"cell_type":"markdown","source":"We can already tell there's a pattern here. Sentences like \"cancelled flight\", \"late flight\", \"booking problems\",  \"delayed flight\" stand out clearly. Lets check the positive tweets."},{"metadata":{"_cell_guid":"9b0b4dab-3fa3-cbb1-abf4-5c0e87b1e5f6","_uuid":"36206481870e500bd269a46659cedc10a8948024","trusted":false,"collapsed":true},"cell_type":"code","source":"tweets[(tweets.airline_sentiment == 'positive')][['grams']].apply(count_words)['grams'].most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdbc38dbd17375dd94ad8150f15d9f5744423bf1"},"cell_type":"markdown","source":"### Some useful functions may use"},{"metadata":{"trusted":true,"_uuid":"25ba4f150513f0a36a1db74591ae4385edcc6f03"},"cell_type":"code","source":"# some references:\n\nclass Voc:\n    def __init__(self):\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = [\"PAD\", \"UNK\"] # might be changed\n        self.n_words = 10000 + 2 # might be changed\n\n    def unicodeToAscii(self, s):\n        return ''.join(\n            c for c in unicodedata.normalize('NFD', s)\n            if unicodedata.category(c) != 'Mn'\n        )\n    def remove_punctuation(self, sentence):\n        sentence = self.unicodeToAscii(sentence)\n        sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n        sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n        sentence = re.sub(r\"\\s+\", r\" \", sentence).strip()\n        return sentence\n\n    def fit(self, train_df, train_df_no_label, USE_Word2Vec=True):\n        print(\"Voc fitting...\")\n        \n        # tokenize\n        tokens = []\n        sentences = []\n        \n        for sequence in train_df[\"seq\"]:\n            token = sequence.strip(\" \").split(\" \")\n            tokens += token\n            sentences.append(token)\n\n\n        for sequence in train_df_no_label[\"seq\"]:\n            token = sequence.strip(\" \").split(\" \")\n            tokens += token\n            sentences.append(token)\n\n        # Using Word2Vec\n        if USE_Word2Vec:\n            dim = 100\n            print(\"Word2Vec fitting\")\n            model = Word2Vec(sentences, size=dim, window=5, min_count=20, workers=20, iter=20)\n            print(\"Word2Vec fitting finished....\")\n            # gensim index2word \n            self.index2word += model.wv.index2word\n            self.n_words = len(self.index2word)\n            # build up numpy embedding matrix\n            embedding_matrix = [None] * len(self.index2word) # init to vocab length\n            embedding_matrix[0] = np.random.normal(size=(dim,))\n            embedding_matrix[1] = np.random.normal(size=(dim,))\n            # plug in embedding\n            for i in range(2, len(self.index2word)):\n                embedding_matrix[i] = model.wv[self.index2word[i]]\n                self.word2index[self.index2word[i]] = i\n            \n            # \n            self.embedding_matrix = np.array(embedding_matrix)\n            return\n        else:\n            # Counter\n            counter = Counter(tokens)\n            voc_list = counter.most_common(10000)\n\n            for i, (voc, freq) in enumerate(voc_list):\n                self.word2index[voc] = i+2\n                self.index2word[i+2] = voc\n                self.word2count[voc] = freq\n\ndef print_to_csv(y_, filename):\n    d = {\"id\":[i for i in range(len(y_))],\"label\":list(map(lambda x: str(x), y_))}\n    df = pd.DataFrame(data=d)\n    df.to_csv(filename, index=False)\n\n\nclass BOW():\n    def __init__(self):\n        self.vectorizer = CountVectorizer(max_features=10000)\n\n    def unicodeToAscii(self, s):\n        return ''.join(\n            c for c in unicodedata.normalize('NFD', s)\n            if unicodedata.category(c) != 'Mn'\n        )\n    def remove_punctuation(self, sentence):\n        sentence = self.unicodeToAscii(sentence)\n        sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n        sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n        sentence = re.sub(r\"\\s+\", r\" \", sentence).strip()\n        return sentence\n\n    def fit(self, train_df, train_df_no_label):\n        # prepare copus\n    \n        corpus = list(map(lambda x: self.remove_punctuation(x), train_df['seq']))\n        corpus += list(map(lambda x: self.remove_punctuation(x), train_df_no_label['seq']))\n        print(\"BOW fitting\")\n        self.vectorizer.fit(corpus)\n        self.dim = len(self.vectorizer.get_feature_names())\n        print(\"BOW fitting done\")\n        return self\n\n    def batch_generator(self, df, batch_size, shuffle=True, training=True):\n         # (B, Dimension)\n        N = df.shape[0]\n        df_matrix = df.as_matrix()\n        \n        if shuffle == True:\n            random_permutation = np.random.permutation(N)\n            \n            # shuffle\n            X = df_matrix[random_permutation, 1]\n            y = df_matrix[random_permutation, 0].astype(int) # 0 is label's index\n        else:\n            X = df_matrix[:, 1]\n            y = df_matrix[:, 0].astype(int)\n        #\n        quotient = X.shape[0] // batch_size\n        remainder = X.shape[0] - batch_size * quotient\n\n        for i in range(quotient):\n            batch = {}\n            batch_X = self.vectorizer.transform(X[i*batch_size:(i+1)*batch_size]).toarray()\n            batch['X'] = Variable(torch.from_numpy(batch_X)).float()\n            if training:\n                batch_y = y[i*batch_size:(i+1)*batch_size]\n                batch['y'] = Variable(torch.from_numpy(batch_y))\n            else:\n                batch['y'] = None\n            batch['lengths'] = None\n            yield batch\n            \n        if remainder > 0: \n            batch = {}\n            batch_X = self.vectorizer.transform(X[-remainder:]).toarray()\n            batch['X'] = Variable(torch.from_numpy(batch_X)).float()\n            if training:\n                batch_y = y[-remainder:]\n                batch['y'] = Variable(torch.from_numpy(batch_y))\n            else:\n                batch['y'] = None\n            batch['lengths'] = None\n            yield batch\n\nclass Preprocess:\n    '''\n        Preprocess raw data\n    '''\n    def __init__(self):\n        self.regex_remove_punc = re.compile('[%s]' % re.escape(string.punctuation))\n        pass\n    def unicodeToAscii(self, s):\n        return ''.join(\n            c for c in unicodedata.normalize('NFD', s)\n            if unicodedata.category(c) != 'Mn'\n        )\n\n    def normalizeString(self, sentence):\n        sentence = self.unicodeToAscii(sentence.strip())\n        #sentence = self.unicodeToAscii(sentence.lower().strip())\n        # remove punctuation\n        if False:\n            sentence = self.regex_remove_punc.sub('', sentence)\n        sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n        sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n        sentence = re.sub(r\"\\s+\", r\" \", sentence).strip()\n        return sentence\n\n    def remove_punctuation(self, sentence):\n        sentence = self.regex_remove_punc.sub('', sentence)\n        return sentence\n\n    def read_txt(self, train_filename, test_filename, train_filename_no_label):\n        train_df = None\n        test_df = None\n        train_df_no_label = None\n        \n        if train_filename is not None:\n            train_df = pd.read_csv(train_filename, header=None, names=[\"label\", \"seq\"], sep=\"\\+\\+\\+\\$\\+\\+\\+\",\n                                  engine=\"python\")\n            # remove puncuation\n            #train_df[\"seq\"] = train_df[\"seq\"].apply(lambda seq: self.normalizeString(seq))\n            \n        \n        if test_filename is not None:\n            with open(test_filename, \"r\") as f:\n                reader = csv.reader(f, delimiter=\",\")\n                rows = [[row[0], \",\".join(row[1:])] for row in reader]\n                test_df = pd.DataFrame(rows[1:], columns=rows[0]) # first row is column name\n            # remove puncuation\n            #test_df[\"text\"] = test_df[\"text\"].apply(lambda seq: self.normalizeString(seq))\n        if train_filename_no_label is not None:\n            train_df_no_label = pd.read_csv(train_filename_no_label, sep=\"\\n\", header=None, names=[\"seq\"])\n            train_df_no_label.insert(loc=0, column=\"nan\", value=0)\n            # remove puncuation\n            #train_df_no_label[\"seq\"] = train_df_no_label[\"seq\"].apply(lambda seq: self.normalizeString(seq))\n        \n        return train_df, test_df, train_df_no_label\n\nclass Sample_Encode:\n    '''\n        Transform \n    '''\n    def __init__(self, voc):\n        self.voc = voc\n\n    def sentence_to_index(self, sentence):\n        encoded = list(map(lambda token: self.voc.word2index[token] if token in self.voc.word2index \\\n            else UNK_token, sentence))\n        return encoded\n\n    def pad_batch(self, index_batch):\n        '''\n            Return padded list with size (B, Max_length)\n        '''\n        return list(itertools.zip_longest(*index_batch, fillvalue=PAD_token))\n\n    def batch_to_Variable(self, sentence_batch, training=True):\n        '''\n            Input: a numpy of sentence\n            ex. [\"i am a\", \"jim l o \"]\n\n            Output: a torch Variable and sentence lengths\n        '''\n        # split sentence\n        sentence_batch = sentence_batch.tolist()\n        \n        # apply\n        for training_sample in sentence_batch:\n            # split training sentence\n            training_sample[1] = training_sample[1].strip(\" \").split(\" \")\n\n        # encode batch\n        index_label_batch = [(training_sample[0], self.sentence_to_index(training_sample[1])) \\\n            for training_sample in sentence_batch]\n\n        # sort sentence batch (in order to fit torch pack_pad_sequence)\n        #index_label_batch.sort(key=lambda x: len(x[1]), reverse=True) \n        \n        # index batch\n        index_batch = [training_sample[1] for training_sample in index_label_batch]\n        label_batch = [training_sample[0] for training_sample in index_label_batch]\n\n        # record batch's length\n        lengths = [len(indexes) for indexes in index_batch]\n\n        # padded batch\n        padded_batch = self.pad_batch(index_batch)\n\n        # transform to Variable\n        if training:\n            pad_var = Variable(torch.LongTensor(padded_batch), volatile=False)\n        else:\n            pad_var = Variable(torch.LongTensor(padded_batch), volatile=True)\n\n        # label\n        if training:\n            label_var = Variable(torch.LongTensor(label_batch), volatile=False)\n        else:\n            label_var = None\n\n        \n        return pad_var, label_var, lengths\n    \n    def generator(self, df, batch_size, shuffle=False, training=True):\n        '''\n        Return sample batch Variable\n            batch['X'] is (T, B)\n        '''\n        df_matrix = df.as_matrix()\n        if shuffle == True:\n            random_permutation = np.random.permutation(len(df['seq']))\n            \n            # shuffle\n            df_matrix = df_matrix[random_permutation]\n        #\n        quotient = df.shape[0] // batch_size\n        remainder = df.shape[0] - batch_size * quotient\n\n        for i in range(quotient):\n            batch = {}\n            X, y, lengths = self.batch_to_Variable(df_matrix[i*batch_size:(i+1)*batch_size], training)\n            batch['X'] = X\n            batch['y'] = y\n            batch['lengths'] = lengths\n            yield batch\n            \n        if remainder > 0: \n            batch = {}\n            X, y, lengths = self.batch_to_Variable(df_matrix[-remainder:],training)\n            batch['X'] = X\n            batch['y'] = y\n            batch['lengths'] = lengths\n            yield batch\n\ndef trim(text_list, threshold=2):\n    result = []\n    for _, text in enumerate(text_list):\n        grouping = []\n        for _, g in itertools.groupby(text):\n            grouping.append(list(g))\n        r = ''.join([g[0] if len(g)<threshold else g[0]*threshold for g in grouping])\n        result.append(r)\n    return result\n\ndef token_counter(corpus):\n    tokenizer = Tokenizer(num_words=None,filters=\"\\n\")\n    tokenizer.fit_on_texts(corpus)\n    sequences = tokenizer.texts_to_sequences(corpus)\n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))\n \nstemmer = gensim.parsing.porter.PorterStemmer()\ndef preprocess(string, use_stem = True):\n    string = string.replace(\"i ' m\", \"im\").replace(\"you ' re\",\"youre\").replace(\"didn ' t\",\"didnt\")    .replace(\"can ' t\",\"cant\").replace(\"haven ' t\", \"havent\").replace(\"won ' t\", \"wont\")    .replace(\"isn ' t\",\"isnt\").replace(\"don ' t\", \"dont\").replace(\"doesn ' t\", \"doesnt\")    .replace(\"aren ' t\", \"arent\").replace(\"weren ' t\", \"werent\").replace(\"wouldn ' t\",\"wouldnt\")    .replace(\"ain ' t\",\"aint\").replace(\"shouldn ' t\",\"shouldnt\").replace(\"wasn ' t\",\"wasnt\")    .replace(\" ' s\",\"s\").replace(\"wudn ' t\",\"wouldnt\").replace(\" .. \",\" ... \")    .replace(\"couldn ' t\",\"couldnt\")\n    for same_char in re.findall(r'((\\w)\\2{2,})', string):\n        string = string.replace(same_char[0], same_char[1])\n    for digit in re.findall(r'\\d+', string):\n        string = string.replace(digit, \"1\")\n    for punct in re.findall(r'([-/\\\\\\\\()!\"+,&?\\'.]{2,})',string):\n        if punct[0:2] ==\"..\":\n            string = string.replace(punct, \"...\")\n        else:\n            string = string.replace(punct, punct[0])\n    return string\n\ndef getFrequencyDict(lines):\n    freq = {}\n    for s in lines:\n        for w in s:\n            if w in freq: freq[w] += 1\n            else:         freq[w] = 1\n    return freq\n\ndef initializeCmap(lines):\n    print('  Initializing conversion map...')\n    cmap = {}\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            cmap[w] = w\n    print('    Conversion map size:', len(cmap))\n    return cmap\n\ndef convertAccents(lines, cmap):\n    print('  Converting accents...')\n    for i, s in enumerate(lines):\n        s = [(''.join(c for c in udata.normalize('NFD', w) if udata.category(c) != 'Mn')) for w in s]\n        for j, w in enumerate(s):\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n    clist = 'abcdefghijklmnopqrstuvwxyz0123456789.!?'\n    for i, s in enumerate(lines):\n        s = [''.join([c for c in w if c in clist]) for w in s]\n        for j, w in enumerate(s):\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertPunctuations(lines, cmap):\n    print('  Converting punctuations...')\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            excCnt, queCnt, dotCnt = w.count('!'), w.count('?'), w.count('.')\n            if queCnt:        s[j] = '_?'\n            elif excCnt >= 5: s[j] = '_!!!'\n            elif excCnt >= 3: s[j] = '_!!'\n            elif excCnt >= 1: s[j] = '_!'\n            elif dotCnt >= 2: s[j] = '_...'\n            elif dotCnt >= 1: s[j] = '_.'\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertNotWords(lines, cmap):\n    print('  Converting not words...')\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            if w[0] == '_': continue\n            if w == '2':        s[j] = 'to'\n            elif w.isnumeric(): s[j] = '_n'\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertTailDuplicates(lines, cmap):\n    print('  Converting tail duplicates...')\n    freq = getFrequencyDict(lines)\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            w = re.sub(r'(([a-z])\\2{2,})$', r'\\g<2>\\g<2>', w)\n            s[j] = re.sub(r'(([a-cg-kmnp-ru-z])\\2+)$', r'\\g<2>', w)\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertHeadDuplicates(lines, cmap):\n    print('  Converting head duplicates...')\n    freq = getFrequencyDict(lines)\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            s[j] = re.sub(r'^(([a-km-z])\\2+)', r'\\g<2>', w)\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertInlineDuplicates(lines, cmap, minfreq=64):\n    print('  Converting inline duplicates...')\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            w = re.sub(r'(([a-z])\\2{2,})', r'\\g<2>\\g<2>', w)\n            s[j] = re.sub(r'(([ahjkquvwxyz])\\2+)', r'\\g<2>', w)  # repeated 2+ times, impossible\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n    freq = getFrequencyDict(lines)\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            if freq[w] > minfreq: continue\n            if w == 'too': continue\n            w1 = re.sub(r'(([a-z])\\2+)', r'\\g<2>', w) # repeated 2+ times, replace by 1\n            f0, f1 = freq.get(w,0), freq.get(w1,0)\n            fm = max(f0, f1)\n            if fm == f0:   pass\n            else:          s[j] = w1;\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertSlang(lines, cmap):\n    print('  Converting slang...')\n    freq = getFrequencyDict(lines)\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            if w == 'u': lines[i][j] = 'you'\n            if w == 'dis': lines[i][j] = 'this'\n            if w == 'dat': lines[i][j] = 'that'\n            if w == 'luv': lines[i][j] = 'love'\n            w1 = re.sub(r'in$', r'ing', w)\n            w2 = re.sub(r'n$', r'ing', w)\n            f0, f1, f2 = freq.get(w,0), freq.get(w1,0), freq.get(w2,0)\n            fm = max(f0, f1, f2)\n            if fm == f0:   pass\n            elif fm == f1: s[j] = w1;\n            else:          s[j] = w2;\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertSingular(lines, cmap, minfreq=512):\n    print('  Converting singular form...')\n    freq = getFrequencyDict(lines)\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            if freq[w] > minfreq: continue\n            w1 = re.sub(r's$', r'', w)\n            w2 = re.sub(r'es$', r'', w)\n            w3 = re.sub(r'ies$', r'y', w)\n            f0, f1, f2, f3 = freq.get(w,0), freq.get(w1,0), freq.get(w2,0), freq.get(w3,0)\n            fm = max(f0, f1, f2, f3)\n            if fm == f0:   pass\n            elif fm == f1: s[j] = w1;\n            elif fm == f2: s[j] = w2;\n            else:          s[j] = w3;\n            cmap[original_lines[i][j]] = s[j]\n    lines[i] = s\n\ndef convertRareWords(lines, cmap, min_count=16):\n    print('  Converting rare words...')\n    freq = getFrequencyDict(lines)\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w == '': continue\n            if freq[w] < min_count: s[j] = '_r'\n            cmap[original_lines[i][j]] = s[j]\n        lines[i] = s\n\ndef convertCommonWords(lines, cmap):\n    print('  Converting common words...')\n    #beverbs = set('is was are were am s'.split())\n    #articles = set('a an the'.split())\n    #preps = set('to for of in at on by'.split())\n\n    for i, s in enumerate(lines):\n        #s = [word if word not in beverbs else '_b' for word in s]\n        #s = [word if word not in articles else '_a' for word in s]\n        #s = [word if word not in preps else '_p' for word in s]\n        lines[i] = s\n\ndef convertPadding(lines, maxlen=38):\n    print('  Padding...')\n    for i, s in enumerate(lines):\n        lines[i] = [w for w in s if w]\n    for i, s in enumerate(lines):\n        lines[i] = s[:maxlen]\n\ndef preprocessLines(lines):\n    global original_lines\n    original_lines = lines[:]\n    cmap = initializeCmap(original_lines)\n    convertAccents(lines, cmap)\n    convertPunctuations(lines, cmap)\n    convertNotWords(lines, cmap)\n    convertTailDuplicates(lines, cmap)\n    convertHeadDuplicates(lines, cmap)\n    convertInlineDuplicates(lines, cmap)\n    convertSlang(lines, cmap)\n    convertSingular(lines, cmap)\n    convertRareWords(lines, cmap)\n    convertCommonWords(lines, cmap)\n    convertPadding(lines)\n    return lines, cmap\n\ndef readData(path, label=True):\n    print('  Loading', path+'...')\n    _lines, _labels = [], []\n    with open(path, 'r', encoding='utf_8') as f:\n        for line in f:\n            if label:\n                _labels.append(int(line[0]))\n                line = line[10:-1]\n            else:\n                line = line[:-1]\n            _lines.append(line.split())\n    if label: return _lines, _labels\n    else:     return _lines\n\ndef padLines(lines, value, maxlen):\n    maxlinelen = 0\n    for i, s in enumerate(lines):\n        maxlinelen = max(len(s), maxlinelen)\n    maxlinelen = max(maxlinelen, maxlen)\n    for i, s in enumerate(lines):\n        lines[i] = (['_r'] * max(0, maxlinelen - len(s)) + s)[-maxlen:]\n    return lines\n\ndef getDictionary(lines):\n    _dict = {}\n    for s in lines:\n        for w in s:\n            if w not in _dict:\n                _dict[w] = len(_dict) + 1\n    return _dict\n\ndef transformByDictionary(lines, dictionary):\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w in dictionary: lines[i][j] = dictionary[w]\n            else:               lines[i][j] = dictionary['']\n\ndef transformByConversionMap(lines, cmap, iter=2):\n    cmapRefine(cmap)\n    for it in range(iter):\n        for i, s in enumerate(lines):\n            s0 = []\n            for j, w in enumerate(s):\n                if w in cmap and w[0] != '_':\n                    s0 = s0 + cmap[w].split()\n                elif w[0] == '_':\n                    s0 = s0 + [w]\n            lines[i] = [w for w in s0 if w]\n\ndef transformByWord2Vec(lines, w2v):\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w in w2v.wv:\n                lines[i][j] = w2v.wv[w]\n            else:\n                lines[i][j] = w2v.wv['_r']\n\ndef readTestData(path):\n    print('  Loading', path + '...')\n    _lines = []\n    with open(path, 'r', encoding='utf_8') as f:\n        for i, line in enumerate(f):\n            if i:\n                start = int(np.log10(max(1, i-1))) + 2\n                _lines.append(line[start:].split())\n    return _lines\n\ndef savePrediction(y, path, id_start=0):\n    pd.DataFrame([[i+id_start, int(y[i])] for i in range(y.shape[0])],\n                 columns=['id', 'label']).to_csv(path, index=False)\n\ndef savePreprocessCorpus(lines, path):\n    with open(path, 'w', encoding='utf_8') as f:\n        for line in lines:\n            f.write(' '.join(line) + '\\n')\n\ndef savePreprocessCmap(cmap, path):\n    with open(path, 'wb') as f:\n        pickle.dump(cmap, f)\n\ndef loadPreprocessCmap(path):\n    print('  Loading', path + '...')\n    with open(path, 'rb') as f:\n        cmap = pickle.load(f)\n    return cmap\n\ndef loadPreprocessCorpus(path):\n    print('  Loading', path + '...')\n    lines = []\n    with open(path, 'r', encoding='utf_8') as f:\n        for line in f:\n            lines.append(line.split())\n    return lines\n\ndef removePunctuations(lines):\n    rs = {'_!', '_!!', '_!!!', '_.', '_...', '_?'}\n    for i, s in enumerate(lines):\n        for j, w in enumerate(s):\n            if w in rs:\n                s[j] = ''\n        lines[i] = [w for w in x if w]\n\ndef removeDuplicatedLines(lines):\n    lineset = set({})\n    for line in lines:\n        lineset.add(' '.join(line))\n    for i, line in enumerate(lineset):\n        lines[i] = line.split()\n    del lines[-(len(lines)-len(lineset)):]\n    return lineset\n\ndef shuffleData(lines, labels):\n    for i, s in enumerate(lines):\n        lines[i] = (s, labels[i])\n    np.random.shuffle(lines)\n    for i, s in enumerate(lines):\n        labels[i] = s[1]\n        lines[i] = s[0]\n\ndef cmapRefine(cmap):\n    cmap['teh'] = cmap['da'] = cmap['tha'] = 'the'\n    cmap['evar'] = 'ever'\n    cmap['likes'] = cmap['liked'] = cmap['lk'] = 'like'\n    cmap['wierd'] = 'weird'\n    cmap['kool'] = 'cool'\n    cmap['yess'] = 'yes'\n    cmap['pleasee'] = 'please'\n    cmap['soo'] = 'so'\n    cmap['noo'] = 'no'\n    cmap['lovee'] = cmap['loove'] = cmap['looove'] = cmap['loooove'] = cmap['looooove'] \\\n        = cmap['loooooove'] = cmap['loves'] = cmap['loved'] = cmap['wuv'] \\\n        = cmap['loovee'] = cmap['lurve'] = cmap['lov'] = cmap['luvs'] = 'love'\n    cmap['lovelove'] = 'love love'\n    cmap['lovelovelove'] = 'love love love'\n    cmap['ilove'] = 'i love'\n    cmap['liek'] = cmap['lyk'] = cmap['lik'] = cmap['lke'] = cmap['likee'] = 'like'\n    cmap['mee'] = 'me'\n    cmap['hooo'] = 'hoo'\n    cmap['sooon'] = cmap['soooon'] = 'soon'\n    cmap['goodd'] = cmap['gud'] = 'good'\n    cmap['bedd'] = 'bed'\n    cmap['badd'] = 'bad'\n    cmap['sadd'] = 'sad'\n    cmap['madd'] = 'mad'\n    cmap['redd'] = 'red'\n    cmap['tiredd'] = 'tired'\n    cmap['boredd'] = 'bored'\n    cmap['godd'] = 'god'\n    cmap['xdd'] = 'xd'\n    cmap['itt'] = 'it'\n    cmap['lul'] = cmap['lool'] = 'lol'\n    cmap['sista'] = 'sister'\n    cmap['w00t'] = 'woot'\n    cmap['srsly'] = 'seriously'\n    cmap['4ever'] = cmap['4eva'] = 'forever'\n    cmap['neva'] = 'never'\n    cmap['2day'] = 'today'\n    cmap['homee'] = 'home'\n    cmap['hatee'] = 'hate'\n    cmap['heree'] = 'here'\n    cmap['cutee'] = 'cute'\n    cmap['lemme'] = 'let me'\n    cmap['mrng'] = 'morning'\n    cmap['gd'] = 'good'\n    cmap['thx'] = cmap['thnx'] = cmap['thanx'] = cmap['thankx'] = cmap['thnk'] = 'thanks'\n    cmap['jaja'] = cmap['jajaja'] = cmap['jajajaja'] = 'haha'\n    cmap['eff'] = cmap['fk'] = cmap['fuk'] = cmap['fuc'] = 'fuck'\n    cmap['2moro'] = cmap['2mrow'] = cmap['2morow'] = cmap['2morrow'] \\\n        = cmap['2morro'] = cmap['2mrw'] = cmap['2moz'] = 'tomorrow'\n    cmap['babee'] = 'babe'\n    cmap['theree'] = 'there'\n    cmap['thee'] = 'the'\n    cmap['woho'] = cmap['wohoo'] = 'woo hoo'\n    cmap['2gether'] = 'together'\n    cmap['2nite'] = cmap['2night'] = 'tonight'\n    cmap['nite'] = 'night'\n    cmap['dnt'] = 'dont'\n    cmap['rly'] = 'really'\n    cmap['gt'] = 'get'\n    cmap['lat'] = 'late'\n    cmap['dam'] = 'damn'\n    cmap['4ward'] = 'forward'\n    cmap['4give'] = 'forgive'\n    cmap['b4'] = 'before'\n    cmap['tho'] = 'though'\n    cmap['kno'] = 'know'\n    cmap['grl'] = 'girl'\n    cmap['boi'] = 'boy'\n    cmap['wrk'] = 'work'\n    cmap['jst'] = 'just'\n    cmap['geting'] = 'getting'\n    cmap['4get'] = 'forget'\n    cmap['4got'] = 'forgot'\n    cmap['4real'] = 'for real'\n    cmap['2go'] = 'to go'\n    cmap['2b'] = 'to be'\n    cmap['gr8'] = cmap['gr8t'] = cmap['gr88'] = 'great'\n    cmap['str8'] = 'straight'\n    cmap['twiter'] = 'twitter'\n    cmap['iloveyou'] = 'i love you'\n    cmap['loveyou'] = cmap['loveya'] = cmap['loveu'] = 'love you'\n    cmap['xoxox'] = cmap['xox'] = cmap['xoxoxo'] = cmap['xoxoxox'] \\\n        = cmap['xoxoxoxo'] = cmap['xoxoxoxoxo'] = 'xoxo'\n    cmap['cuz'] = cmap['bcuz'] = cmap['becuz'] = 'because'\n    cmap['iz'] = 'is'\n    cmap['aint'] = 'am not'\n    cmap['fav'] = 'favorite'\n    cmap['ppl'] = 'people'\n    cmap['mah'] = 'my'\n    cmap['r8'] = 'rate'\n    cmap['l8'] = 'late'\n    cmap['w8'] = 'wait'\n    cmap['m8'] = 'mate'\n    cmap['h8'] = 'hate'\n    cmap['l8ter'] = cmap['l8tr'] = cmap['l8r'] = 'later'\n    cmap['cnt'] = 'cant'\n    cmap['fone'] = cmap['phonee'] = 'phone'\n    cmap['f1'] = 'fONE'\n    cmap['xboxe3'] = 'eTHREE'\n    cmap['jammin'] = 'jamming'\n    cmap['onee'] = 'one'\n    cmap['1st'] = 'first'\n    cmap['2nd'] = 'second'\n    cmap['3rd'] = 'third'\n    cmap['inet'] = 'internet'\n    cmap['recomend'] = 'recommend'\n    cmap['ah1n1'] = cmap['h1n1'] = 'hONEnONE'\n    cmap['any1'] = 'anyone'\n    cmap['every1'] = cmap['evry1'] = 'everyone'\n    cmap['some1'] = cmap['sum1'] = 'someone'\n    cmap['no1'] = 'no one'\n    cmap['4u'] = 'for you'\n    cmap['4me'] = 'for me'\n    cmap['2u'] = 'to you'\n    cmap['yu'] = 'you'\n    cmap['yr'] = cmap['yrs'] = cmap['years'] = 'year'\n    cmap['hr'] = cmap['hrs'] = cmap['hours'] = 'hour'\n    cmap['min'] = cmap['mins'] = cmap['minutes'] = 'minute'\n    cmap['go2'] = cmap['goto'] = 'go to'\n    for key, value in cmap.items():\n        if not key.isalpha():\n            if key[-1:] == 'k':\n                cmap[key] = '_n'\n            if key[-2:]=='st' or key[-2:]=='nd' or key[-2:]=='rd' or key[-2:]=='th':\n                cmap[key] = '_ord'\n            if key[-2:]=='am' or key[-2:]=='pm' or key[-3:]=='min' or key[-4:]=='mins' \\\n                    or key[-2:]=='hr' or key[-3:]=='hrs' or key[-1:]=='h' \\\n                    or key[-4:]=='hour' or key[-5:]=='hours'\\\n                    or key[-2:]=='yr' or key[-3:]=='yrs'\\\n                    or key[-3:]=='day' or key[-4:]=='days'\\\n                    or key[-3:]=='wks':\n                cmap[key] = '_time'\ndef preprocessTestingData(path):\n    print('Loading testing data...')\n    lines = readTestData(path)\n\n    cmap_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/cmap.pkl')\n    w2v_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/word2vec.pkl')\n    cmap = loadPreprocessCmap(cmap_path)\n    transformByConversionMap(lines, cmap)\n    \n    lines = padLines(lines, '_', maxlen)\n    w2v = Word2Vec.load(w2v_path)\n    transformByWord2Vec(lines, w2v)\n    return lines\n\ndef preprocessTrainingData(label_path, nolabel_path, retrain=False, punctuation=True):\n    print('Loading training data...')\n    if retrain:\n        preprocess(label_path, nolabel_path)\n\n    lines, labels = readData(label_path)\n    corpus_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/corpus.txt')\n    cmap_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/cmap.pkl')\n    w2v_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/word2vec.pkl')\n    lines = readData(corpus_path, label=False)[:len(lines)]\n    shuffleData(lines, labels)\n    labels = np.array(labels)\n\n    cmap = loadPreprocessCmap(cmap_path)\n    transformByConversionMap(lines, cmap)\n    if not punctuation:\n        removePunctuations(lines)\n\n    lines = padLines(lines, '_', maxlen)\n    w2v = Word2Vec.load(w2v_path)\n    transformByWord2Vec(lines, w2v)\n    return lines, labels\n\ndef preprocess(label_path, nolabel_path):\n    print('Preprocessing...')\n    labeled_lines, labels = readData(label_path)\n    nolabel_lines = readData(nolabel_path, label=False)\n    lines = labeled_lines + nolabel_lines\n\n    lines, cmap = preprocessLines(lines)\n    corpus_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/corpus.txt')\n    cmap_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/cmap.pkl')\n    w2v_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/word2vec.pkl')\n    savePreprocessCorpus(lines, corpus_path)\n    savePreprocessCmap(cmap, cmap_path)\n\n    transformByConversionMap(lines, cmap)\n    removeDuplicatedLines(lines)\n\n    print('Training word2vec...')\n    model = Word2Vec(lines, size=256, min_count=16, iter=16, workers=16)\n    model.save(w2v_path)","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"0fb8d819-6567-dad4-accd-4eb225a847fd","_uuid":"2f490540a988aafa137b8ee23d0adbd7b3cecbc9"},"cell_type":"markdown","source":"Some more good looking patterns here. We can however see that with 3-grams clear patterns are rare. \"great customer service\" occurs 12 times in 2362 positive responses, which really doesn't say much in general. \n\nSatisfied that our data looks possible to work with begin to construct our first classifier.\n\n# First Classifier\nLets start simple with a bag-of-words Support-Vector-Machine (SVM) classifier. Bag-of-words means that we represent each sentence by the unique words in it. To make this representation useful for our SVM classifier we transform each sentence into a vector. The vector is of the same length as our vocabulary, i.e. the list of all words observed in our training data, with each word representing an entry in the vector. If a particular word is present, that entry in the vector is 1, otherwise 0.\n\nTo create these vectors we use the CountVectorizer from [sklearn][1]. \n\n\n  [1]: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"},{"metadata":{"_cell_guid":"5f57086e-16ac-72af-6d90-a23357a65ccf","_uuid":"1af0e0a12740af5067220a8905c3f81064423b5a"},"cell_type":"markdown","source":"## Preparing the data"},{"metadata":{"_cell_guid":"9b6bc521-f902-b92a-5bdc-6e66b0d8359e","_uuid":"a5ec210417d4f272266e6f71fe8bd69f90ddb251","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer(ngram_range=(1,2))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b3831734-27e6-0922-8a11-d1fb93c20916","_uuid":"aefb5e39cead9924bdc660a326aa6a698d835d5d","trusted":false,"collapsed":true},"cell_type":"code","source":"vectorized_data = count_vectorizer.fit_transform(tweets.text)\nindexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7b1e263c-22b2-cc70-e66e-984dad8215e2","_uuid":"21c322611a026caeb257a404ef7ffa41daa7ce2e","trusted":false,"collapsed":true},"cell_type":"code","source":"def sentiment2target(sentiment):\n    return {\n        'negative': 0,\n        'neutral': 1,\n        'positive' : 2\n    }[sentiment]\ntargets = tweets.airline_sentiment.apply(sentiment2target)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3035e760-53dd-0eab-46a6-837eb42239a8","_uuid":"0a4659fa2818ab47ec61d4c20b0d61f2758a3479"},"cell_type":"markdown","source":"To check performance of our classifier we want to split our data in to train and test."},{"metadata":{"_cell_guid":"e20c5771-75da-6a5d-48a9-f5dc243184d1","_uuid":"c08abd686644f5690407bba86ec29390bc0cd912","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.4, random_state=0)\ndata_train_index = data_train[:,0]\ndata_train = data_train[:,1:]\ndata_test_index = data_test[:,0]\ndata_test = data_test[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55af50b6-5f50-5b2a-c00e-aa88d1aae156","_uuid":"75af5b70f7088867c9fbfe8c4e5fc15ea510e3c5"},"cell_type":"markdown","source":"## Fitting a classifier\n\nWe're now ready to fit a classifier to our data. We'll spend more time on hyper parameter tuning later, so for now we just pick some reasonable guesses."},{"metadata":{"_cell_guid":"6c4a6278-314a-d983-363a-23036dba77d5","_uuid":"90523dc15ede88bc3a14f15479f56db93fcac7a1","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.multiclass import OneVsRestClassifier\nclf = OneVsRestClassifier(svm.SVC(gamma=0.01, C=100., probability=True, class_weight='balanced', kernel='linear'))\nclf_output = clf.fit(data_train, targets_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"858bd634-2f5a-1f8a-3403-2cad69e9fb28","_uuid":"4780cd5ea0ccc940b9b43a8f4d05d0f552c4a8cc"},"cell_type":"markdown","source":"## Evaluation of results"},{"metadata":{"_cell_guid":"fac3f93e-2d5c-48bf-586e-92f3067e8cbd","_uuid":"6ffed579b1a003d341b332beb071bbb0589b9478","trusted":false,"collapsed":true},"cell_type":"code","source":"clf.score(data_test, targets_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b13220a-b9f7-bea9-d5c4-a157be5a3551","_uuid":"2e6cf3797fae228795c8d103d22bdff216e9843f"},"cell_type":"markdown","source":"It's most likely possible to achieve a higher score with more tuning, or a more advanced approach. Lets check on how it does on a couple of sentences."},{"metadata":{"_cell_guid":"565b4fa7-fc64-e67f-1e75-508de0f3dce2","_uuid":"eabd6e6c3e7c3320a8a023a4cb57c9fbca92b693","trusted":false,"collapsed":true},"cell_type":"code","source":"sentences = count_vectorizer.transform([\n    \"What a great airline, the trip was a pleasure!\",\n    \"My issue was quickly resolved after calling customer support. Thanks!\",\n    \"What the hell! My flight was cancelled again. This sucks!\",\n    \"Service was awful. I'll never fly with you again.\",\n    \"You fuckers lost my luggage. Never again!\",\n    \"I have mixed feelings about airlines. I don't know what I think.\",\n    \"\"\n])\nclf.predict_proba(sentences)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2030fd6-5a23-6403-4db1-43f47d66fab9","_uuid":"9fa33d85fef3803f89aca9c18c3b6e376aaf77dd"},"cell_type":"markdown","source":"So while results aren't very impressive overall, we can see that it's doing a good job on these obvious sentences. \n\n## What is hard for the classifier?\n\nIt's interesting to know which sentences are hard. To find out, lets apply the classifier to all our test sentences and sort by the marginal probability."},{"metadata":{"_cell_guid":"d995fd8a-1baf-b57b-2d59-c42b7424a2d9","_uuid":"50cf9f27dd6560d0c260bae8c1049ee92e6d787e"},"cell_type":"markdown","source":"Here are some of the hardest sentences."},{"metadata":{"_cell_guid":"a7ff6d9c-bc38-567e-142d-cd66d7168e78","_uuid":"dd0d2754ca55dd054d6dff54a5f3c17de6643708","trusted":false,"collapsed":true},"cell_type":"code","source":"predictions_on_test_data = clf.predict_proba(data_test)\nindex = np.transpose(np.array([range(0,len(predictions_on_test_data))]))\nindexed_predictions = np.concatenate((predictions_on_test_data, index), axis=1).tolist()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c701d6c7-67bc-6a57-7e79-7eb4d56e3b15","_uuid":"e040a67c67b0ed395c20026838329d614fccc8e2","trusted":false,"collapsed":true},"cell_type":"code","source":"def margin(p):\n    top2 = p.argsort()[::-1]\n    return abs(p[top2[0]]-p[top2[1]])\nmargin = sorted(list(map(lambda p : [margin(np.array(p[0:3])),p[3]], indexed_predictions)), key=lambda p : p[0])\nlist(map(lambda p : tweets.iloc[data_test_index[int(p[1])].toarray()[0][0]].text, margin[0:10]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d00c4184-a0ca-244c-d373-d12830cd50dc","_uuid":"b3680b0cd6e177adf31a7497815a889e2d3e2524"},"cell_type":"markdown","source":"and their probability distributions?"},{"metadata":{"_cell_guid":"dceb8a82-1c9b-172d-19eb-6e52ebf0f5fd","_uuid":"2ef8e2d5093806ee0a97d408d30c95ff79c530c1","trusted":false,"collapsed":true},"cell_type":"code","source":"list(map(lambda p : predictions_on_test_data[int(p[1])], margin[0:10]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8226b967-9655-e846-4827-84e9bc3ff4d8","_uuid":"9336fc8739f0cf80d57a9ceb6828139645315690"},"cell_type":"markdown","source":"How about the easiest sentences?"},{"metadata":{"_cell_guid":"e372ff3a-f123-dbaa-7499-68c01f91100d","_uuid":"0762f2de326cff6f02f3774356c5580c8bbbc586","trusted":false,"collapsed":true},"cell_type":"code","source":"list(map(lambda p : tweets.iloc[data_test_index[int(p[1])].toarray()[0][0]].text, margin[-10:]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"940f2c43-63e0-0815-6a4e-b5f9923222f2","_uuid":"c0c288aad907ecd87a5523d4a88785624e0644c6"},"cell_type":"markdown","source":"and their probability distributions?"},{"metadata":{"_cell_guid":"fe941f0a-e199-1442-e7a2-608578d28f62","_uuid":"dd800d256be35693b0053983d2ccd52337e6a94a","trusted":false,"collapsed":true},"cell_type":"code","source":"list(map(lambda p : predictions_on_test_data[int(p[1])], margin[-10:]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eaa41118-2d4a-6a83-ab6e-b5cb6b362ddc","_uuid":"4f8ab6cf13c3d4bc4cb69adc45cd87f086980d2b"},"cell_type":"markdown","source":"Looks like all of the easiest sentences are negative. What is the distribution of certainty across all sentences?"},{"metadata":{"_cell_guid":"6f4adc48-485c-7c29-0f8e-ef264f224490","_uuid":"f27e808155ce6208cc70128e4698733e1a9df3e6","trusted":false,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nmarginal_probs = list(map(lambda p : p[0], margin))\nn, bins, patches = plt.hist(marginal_probs, 25, facecolor='blue', alpha=0.75)\nplt.title('Marginal confidence histogram - All data')\nplt.ylabel('Count')\nplt.xlabel('Marginal probability [abs(p_positive - p_negative)]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0336c74-903d-184a-4eb0-41bcb4482506","_uuid":"bd956c008c452e59988706a3d1d8d05395e43f84"},"cell_type":"markdown","source":"Lets break it down by positive and negative sentiment to see if one is harder than the other.\n\n### Positive data"},{"metadata":{"_cell_guid":"45847e24-b9a5-5249-004b-815a528a819a","_uuid":"774a8883b63ea9dd5da971d9f2f1fa5372c016fa","trusted":false,"collapsed":true},"cell_type":"code","source":"positive_test_data = list(filter(lambda row : row[0]==2, hstack((targets_test[:,None], data_test)).toarray()))\npositive_probs = clf.predict_proba(list(map(lambda r : r[1:], positive_test_data)))\nmarginal_positive_probs = list(map(lambda p : abs(p[0]-p[1]), positive_probs))\nn, bins, patches = plt.hist(marginal_positive_probs, 25, facecolor='green', alpha=0.75)\nplt.title('Marginal confidence histogram - Positive data')\nplt.ylabel('Count')\nplt.xlabel('Marginal probability')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6e34fd7c-a0a2-0249-a7b4-ff401c1362b3","_uuid":"ee31820c199d52b41d70c95b33b2a0615e2ed73e"},"cell_type":"markdown","source":"### Neutral data"},{"metadata":{"_cell_guid":"9d1bbf9b-a6b6-d5b8-4ae0-522d32f62b95","_uuid":"38e3936b6385e9bc075019a835e2416074d1f325","trusted":false,"collapsed":true},"cell_type":"code","source":"positive_test_data = list(filter(lambda row : row[0]==1, hstack((targets_test[:,None], data_test)).toarray()))\npositive_probs = clf.predict_proba(list(map(lambda r : r[1:], positive_test_data)))\nmarginal_positive_probs = list(map(lambda p : abs(p[0]-p[1]), positive_probs))\nn, bins, patches = plt.hist(marginal_positive_probs, 25, facecolor='yellow', alpha=0.75)\nplt.title('Marginal confidence histogram - Neutral data')\nplt.ylabel('Count')\nplt.xlabel('Marginal probability')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"347eb5b3-3d43-9d40-14a9-459170d8a67c","_uuid":"80cb7d710ff8d76cbfb0405d775c7977684a8f89"},"cell_type":"markdown","source":"### Negative data"},{"metadata":{"_cell_guid":"29663d73-ae5a-3f42-5b87-a88482c2f155","_uuid":"0cc29110df3746add9b8b7742da484cfa1b66143","trusted":false,"collapsed":true},"cell_type":"code","source":"negative_test_data = list(filter(lambda row : row[0]==0, hstack((targets_test[:,None], data_test)).toarray()))\nnegative_probs = clf.predict_proba(list(map(lambda r : r[1:], negative_test_data)))\nmarginal_negative_probs = list(map(lambda p : abs(p[0]-p[1]), negative_probs))\nn, bins, patches = plt.hist(marginal_negative_probs, 25, facecolor='red', alpha=0.75)\nplt.title('Marginal confidence histogram - Negative data')\nplt.ylabel('Count')\nplt.xlabel('Marginal probability')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"952d7391-2a6e-0efe-d67a-57eb9bc3c5e5","_uuid":"f7381d7b2bbe14a989b422e6ce5dbe39df4c5b5b"},"cell_type":"markdown","source":"Clearly the positive data is much harder for the classifier. This makes sense since there's a lot less of it. An important challenge in building a classifier will then be how to handle positive data."},{"metadata":{"_cell_guid":"293fc524-2918-8ab6-e36b-8548a5bb93a1","_uuid":"2f808cd9f703bbfc86864ce7a4c0369db58466f0"},"cell_type":"markdown","source":"# In Progress\n# Second classifier - Convolutional Neural Network\n\nWe're going to build a classifier based on convolutional neural networks.  A good resource for learning about Deep Learning (and machine learning in general) is [Christopher Olah's blog][1]. The convolution neural network approach in particular is explained nicely in [this post][2] by WildML. Finally I recommend [this paper][3] by Yoon Kim, then at NYU. I'll leave these resources to explain the theory behind our approach, and instead focus on getting a working implementation.\n\n\n  [1]: http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n  [2]: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n  [3]: https://arxiv.org/pdf/1408.5882.pdf"},{"metadata":{"_cell_guid":"e4f31c8e-d8e8-d1ca-686c-ab4db6070d20","_uuid":"0880209b81aa979a21c68a4169f39eee66b02e12"},"cell_type":"markdown","source":"# Word Embeddings\nWord embeddings, or vector representations of words, are critical to building a CNN classifier. The vector representations of words are what will build up our input matrix. These vector space models represent words in a vector space such that similar words are mapped to nearby points. This representation rests on the [Distributional Hypothesis][1], i.e. assumption that words that appear in similar contexts share semantic meaning. We will use gensim to train word embeddings from our corpus.\n\n\n  [1]: https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis"},{"metadata":{"_cell_guid":"8f05c61d-dc5d-dbc9-dc88-c6177ffb6dba","_uuid":"89b37e135319be48a053a961fe06331feaf0264f","trusted":false,"collapsed":true},"cell_type":"code","source":"# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 7           # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2a771af8-1af3-07c8-7939-94ca0cb78407","_uuid":"4a98660b53c6fcbdd6eed3e72c648efff2aae357","trusted":false,"collapsed":true},"cell_type":"code","source":"from gensim.models import word2vec\nmodel = word2vec.Word2Vec(tweets.normalized_tweet, workers=num_workers, \\\n                          size=num_features, min_count = min_word_count, \\\n                          window = context, sample = downsampling)\nmodel.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6623ba39-1413-caf6-2f40-2f8f33aaa36a","_uuid":"a101a842b01362bbb44a38a0843281da16eea613","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nX = model[model.wv.vocab]\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\nplt.rcParams[\"figure.figsize\"] = (20,20)\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nlabels = list(model.wv.vocab.keys())\nfor label, x, y in zip(labels, X_tsne[:, 0], X_tsne[:, 1]):\n    plt.annotate(\n        label,\n        xy=(x, y), xytext=(-1, -1),\n        textcoords='offset points', ha='right', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"14a7e04c-1d52-268a-5720-1abb7ff76681","_uuid":"87adf16e212f5291fe03948e4d0d22309061c023","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}