{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Can we predict whether or not a Pokemon is a legendary?\n\nThis notebook will use a dataset that contains Pokemon from Generation I to VII. This dataset currently ommits the last 8 Pokemon from Generation VII, resulting in a total of 801 different Pokemon.\n\n**What constitutes a Pokemon to be legendary?**\nDirectly from [The Bulbapedia Wiki](https://bulbapedia.bulbagarden.net/wiki/Legendary_Pok%C3%A9mon), legendary Pokemon\n>  ...are a group of incredibly rare and often very powerful Pokémon, generally featured prominently in the legends and myths of the Pokémon world...No explicit criteria defines what makes a Pokémon a Legendary Pokémon. Instead, the only way to identify a Pokémon as belonging to this group is through statements from official media, such as the games or anime.\n\nAlthough there are no explicit criteria in what defines a Pokemon to be legendary, we know for a fact that legendary Pokemon are \"...incredibly rare and often very powerful...\". Using the stats of each Pokemon, I will attempt to use classification models to see if how accurate we can predict a legendary Pokemon!","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\npokemon_file_path = \"/kaggle/input/pokemon/pokemon.csv\"\nX_full = pd.read_csv(pokemon_file_path)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"We will then begin to plot the data. In order to see if we can find any trends, I'd figure a FactorPlot will do the trick.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolors = [\"green\", \"purple\"]\ng = sns.factorplot(\n    x='generation', \n    data=X_full,\n    kind='count', \n    hue='is_legendary',\n    palette=colors, \n    size=5, \n    aspect=1.5,\n    legend=False,\n    ).set_axis_labels('Generation', '# of Pokemon')\n\ng.ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),  shadow=True, ncol=2, labels=['Non Legendary','Legendary'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are no obvious trends between the amount of legendary Pokemon per generation. The last generation seems to have the highest amount of legendary Pokemon, but that does not particularly say much. The next step is to see if there are any common traits that the legendary Pokemon have.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We know that legendary Pokemon are both \"rare\" and \"powerful\", the attributes that reflect these qualities may give us better insight as to how we can predict what is a legendary Pokemon.\n\nBecause legendary Pokemon are rare - we know that capturing them may be a tougher than catching non-legendary Pokemon. This is demonstrated with the following plot:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_capturerate = X_full.copy()\nX_capturerate.capture_rate.iloc[773] = 255  \nX_capturerate.capture_rate = pd.to_numeric(X_capturerate.capture_rate)\n\nsns.swarmplot(x=X_capturerate['is_legendary'],\n              y=X_capturerate['capture_rate'])\n#sns.boxplot(x='generation', y='capture_rate', hue='is_legendary', data=X_capturerate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rate at which we can capture them are *significantly* lower than their non-legendary counterpart. \n\nBecause legendary Pokemon tend to be more powerful, their base stats should indicate that they are stronger than their non-legendary counterparts. This is demonstrated below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=X_full['is_legendary'],\n              y=X_full['base_total'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using these two qualities, we can come up with the following plot:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_capturerate['capture_rate'].sort_values()\nsns.lmplot(x=\"base_total\", y=\"capture_rate\", hue=\"is_legendary\", data=X_capturerate)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While capture_rate and base_total seem to demonstrate some more obvious trends, we will also use some other features for our model. I was surprised to see that the experience growth was not as strong of a feature as the other two, but there does seem to be some commonality between legendary Pokemon.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=X_full['is_legendary'],\n              y=X_full['experience_growth'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features seem to vary in data types, so we will preprocess the ones that may give us better insight in predicting legendaries. I will use the follow features within my model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pokemon_mfeatures = ['percentage_male','type1', 'type2', 'height_m', 'weight_kg', \n                     'experience_growth', 'hp', 'attack','base_total',\n                     'defense','sp_attack', 'sp_defense', 'speed', 'capture_rate']\n\nprint(pokemon_mfeatures)\n\n\npoke_mval = X_full[pokemon_mfeatures]\nX = pd.DataFrame(poke_mval)\n\n# setting my target data\ny = X_full.is_legendary\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dealing with null values**\n\nCurrently, there are three columns that contain null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returning all columns with the amount of null values in each column\nX.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Types*\n\nPokemon have special charactersitics, such as types that make them unique. For example, if their species is based off of an aquatiac mammal, their type would be considered water. Many species have more than one type, as shown below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10, 10))\nsns.heatmap(\n    X[X['type2'] != 'None'].groupby(['type1', 'type2']).size().unstack(),\n    linewidths = 1,\n    annot = True,\n    cmap = \"RdYlBu_r\" # color\n)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because there are species that only contain one type, we will set their type_2 NaN values to \"None\".\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X['type2'].fillna('None', inplace=True)\nX['percentage_male'].fillna(0, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Height and Weight*\n\nFor these null values, I will use the SimpleImputer function.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Model\n\nI will utilize a RFModel with a pipeline implementation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size= 0.8, test_size=0.2, random_state=0)\n\n# select categorical columns\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].dtype == 'object']\n\n# select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if \n                 X_train[cname].dtype in ['int64', 'float64']]\n\nprint (categorical_cols)\n# Impute weight and height values\n\n# preprocessing the numerical data\nnumerical_transform = SimpleImputer(strategy='mean')\n\n# preprocesing the categorical transform\ncategorical_transform = Pipeline(steps= [('imputer', SimpleImputer(strategy = 'most_frequent')),\n                                        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n                                        ])\n\n# bundling the preprocessing transformers\npreprocessor = ColumnTransformer(transformers = [('num', numerical_transform, numerical_cols),\n                                              ('cat', categorical_transform, categorical_cols)])\n# Define models...\nrf_model = RandomForestClassifier (n_estimators=100, random_state=0)\n\n# combine preprocesser with model in a pipeline\np1 = Pipeline(steps=[('preprocessorrf', preprocessor), ('modelrf', rf_model)])\n\n# preprocess of training data, fit the model\np1.fit(X_train, y_train)\n\n\n# proprocess validation data, obtain predictions\np1pred = p1.predict(X_valid)\n\n\npreds1 = X.index[p1pred]\n\npd.crosstab(y_valid, preds1, rownames=['Actual Legendary'], colnames=['Predicted Legendary'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model Accuracy:\", (154/161)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the accuracy as a whole had a decent result, the accuracy in which it predicted legendaries was 9/15 which is 60%. There are a few things that I can do that could improve the accuracy. Some of the columns that I had one-hot encoded appeared to have a high cardinality - which may affect the result of my model. \n\nUnfortunately the pipeline implementation does not support Label Encoding, so I will have to do it manually. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in categorical_cols:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])\n\n# Imputation\n#my_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(numerical_transform.fit_transform(label_X_train))\nimputed_X_valid = pd.DataFrame(numerical_transform.transform(label_X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = label_X_train.columns\nimputed_X_valid.columns = label_X_valid.columns\n\n# Using RandomForest Model\nrf_model.fit(imputed_X_train, y_train)\n\nrf_preds = rf_model.predict(imputed_X_valid)\nvisualizerf = X.index[rf_preds]\n\n#p2pred = XGB_model.predict(label_X_valid)\n#preds2 = X.index[p2pred]\n\npd.crosstab(y_valid, visualizerf, rownames=['Actual Legendary'], colnames=['Predicted Legendary'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There was an increase within my model prediction - it had a 80% accuracy in classifying legendary Pokemon (without utilizing a pipeline implementation + one hot encoding)\n\nBelow is a Bayes model that I attempted. It did not have good results, but I figured it would be interesting to see how accurate it classified the Pokemon.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n# defining the Bayes model \nNB_model = GaussianNB()\nNB_model.fit(imputed_X_train, y_train)\np3pred = NB_model.predict(imputed_X_valid)\n\npreds3 = X.index[p3pred]\n\npd.crosstab(y_valid, preds3, rownames=['Actual Legendary'], colnames=['Predicted Legendary'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Model\n\n\nI figured that a gradient booster model may have a high accuracy. XGBoost did not accept the categorical preprocessing transformers, so I will implement XGBoost without the pipeline implementation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nXGB_model = XGBClassifier (n_estimators = 100, learning_rate=0.1)\nXGB_model.fit(imputed_X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(imputed_X_valid, y_valid)], \n             verbose=False)\n\np2pred = XGB_model.predict(imputed_X_valid)\npreds2 = X.index[p2pred]\n\npd.crosstab(y_valid, preds2, rownames=['Actual Legendary'], colnames=['Predicted Legendary'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the gradient booster, I was able to accurately classify almost all Pokemon into the right category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model Accuracy:\", (160/161)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is my first ever machine learning project, please let me know if there is any issues with my implmentation :~)!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}