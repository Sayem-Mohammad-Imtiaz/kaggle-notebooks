{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns  \nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv') \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col= data.columns\nprint(col)\ncol.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.diagnosis                          # M or B \nlist = ['Unnamed: 32','id','diagnosis']\nx = data.drop(list,axis = 1 )\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(y,label=\"Count\")       # M = 212, B = 357\nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) / (data.std())          \ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )       \nx_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##logical Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(C=10)     \nlogreg = logreg.fit(x_train,y_train)\n\nac = accuracy_score(y_test,logreg.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,logreg.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##RFE on LR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nlogreg_2 = LogisticRegression()      \nrfe = RFE(estimator=logreg_2, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)\nprint('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])\n\nx_train_3 = rfe.transform(x_train)\nx_test_3 = rfe.transform(x_test)\nlogreg_2 = LogisticRegression()      \nlogreg_2 = logreg_2.fit(x_train_3,y_train)\nac_3 = accuracy_score(y_test,logreg_2.predict(x_test_3))\nprint('Accuracy is: ',ac_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##RFECV on LR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nlogreg_3 = LogisticRegression()\nrfecv = RFECV(estimator=logreg_3, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\nmax(rfecv.grid_scores_)\nrfecv.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nlogreg_4 = LogisticRegression()     \nlogreg_4 = logreg_4.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,logreg_4.predict(x_test_2))\nprint('Accuracy is: ',ac_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=43)    \ndt = dt.fit(x_train,y_train)\n\nac = accuracy_score(y_test,dt.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,dt.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##RFE on decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\ndt_2 = DecisionTreeClassifier()      \nrfe = RFE(estimator=dt_2, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)\nprint('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])\n\nx_train_3 = rfe.transform(x_train)\nx_test_3 = rfe.transform(x_test)\ndt_2 = DecisionTreeClassifier()      \ndt_2 = dt_2.fit(x_train_3,y_train)\nac_3 = accuracy_score(y_test,dt_2.predict(x_test_3))\nprint('Accuracy is: ',ac_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##RFECV on DT"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\ndt_3 = DecisionTreeClassifier() \nrfecv = RFECV(estimator=dt_3, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\nmax(rfecv.grid_scores_)\nrfecv.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\ndt_4 = DecisionTreeClassifier()    \ndt_4 = dt_4.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,dt_4.predict(x_test_2))\nprint('Accuracy is: ',ac_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvm = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9, probability=True), \n                                               n_jobs=-1))\nsvm = svm.fit(x_train,y_train)\n\nac = accuracy_score(y_test,svm.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,svm.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n\nknncla = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\nknncla.fit(x_train, y_train)\n\nac = accuracy_score(y_test,knncla.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,knncla.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nknncla_2 = KNeighborsClassifier()    \nknncla_2 = knncla_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,knncla_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}