{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 0. Importing packages, setting up the environment","metadata":{}},{"cell_type":"code","source":"!pip install pydot\n!pip install pydotplus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime, date\nfrom math import log\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt  # for eda and simple data visualization\nimport seaborn as sns  # for eda and simple data visualization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outline of the first notebook:\n* calculate total revenue brought by every customer as total value\n* calculate the frequency of orders by every customer\n* calculate the recency of last purchase made by every customer at the end date of this dataset\n* use an algorithm to create clusters of customers based on customers' activity in terms of recency, frequency and monetary value\n* algorithm inference and customers EDA: what time most valueable customers buy, where are customers from","metadata":{}},{"cell_type":"markdown","source":"# Step 1. Read csv, explore NA values and some pre-cleaning of data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/online-retail-ii-uci/online_retail_II.csv')  # importing online reatil dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are 3 columns of numeric types. However, Customer ID column should be treated as categorical variable because one id represents one customer, it does not imply any numeric meaning. Other five columns are categorical type.\n","metadata":{}},{"cell_type":"markdown","source":"#### Let's check NA values, and how they are spreaded along each column","metadata":{}},{"cell_type":"code","source":"null_df = df.isnull().sum().reset_index()\nnull_df.rename(columns={0: 'nan values', 'index': 'column_name'}, inplace=True)\n\ntotal_len_df = df.shape[0]\nnull_df['nan percentage'] = round(null_df['nan values'] / total_len_df * 100, 2)\n\nprint(null_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The majority of na values are in Customer ID column. Since we are interested in clients segmentation than we have no other option than just remove na values. \n\n#### 0.41% of NaNs on goods description is not a critical amount of observations, thereby we can easily remove all NaN's from dataframe.","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf = df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is  also mentioned in a description of dataset that canceled orders start with prefix C. Let's try to exclude cancellations from our first calculation of metrics.","metadata":{}},{"cell_type":"code","source":"# check some rows of cancelled orders\ndf[df['Invoice'].str.contains('C')].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['InvoiceDate'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see cancelled orders are also marked with minus sign in terms of quantity. Now we can remove all rows associated with cancelled orders. It will allow us to make a segmentation of our customers in terms of recency, frequency and monetary value regardless whether a customer made cancellations. \n\n#### Cancelled orders should be treated in a separate analyis as well as customers who cancel orders. This can be done in a different task of finding patterns in cancelled orders. Moreover, we can be interested in taking a thorough look at cancelled orders from customers that belong to the most valueable segments. So there will be a follow-up notebook with EDA on customers that cancel orders.","metadata":{}},{"cell_type":"code","source":"# filter out cancelations from datatset\nclean_df = df[~(df['Invoice'].str.contains('C'))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we have only NOT cancelled orders in dataframe. \n# Step 2. Calculate each customer's recency, frequency and amount of purchases","metadata":{}},{"cell_type":"code","source":"clean_df['Amount'] = clean_df['Quantity'] * clean_df['Price']  # creating a new column of amount spent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aggregate metrics on each customer\ncustomers_df = clean_df.groupby(['Customer ID']).agg(\n    frequency = ('Invoice', 'nunique'),\n    last_purchase = ('InvoiceDate', 'max'),\n    amount = ('Amount', 'sum')\n).reset_index()  # to turn groupby object into dataframe\n\ncustomers_df['last_purchase'] = pd.to_datetime(customers_df['last_purchase'], format='%Y-%m-%d')\n\ncustomers_df['recency'] = datetime(2011, 12, 11) - customers_df['last_purchase']  # the next day of the last invoice date\ncustomers_df['recency'] = customers_df['recency'].dt.days  # leave only days number for recency\n\ncustomers_df.drop(columns=['last_purchase'], inplace=True)  # drop last_purchase column as we would not need it futher","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The description of customers dataset looks fine. Except there are customers who spent 0 at our online-store. Probably these are the customers who used discounts or other promotional activities. \n\n### Let's look at these customers:","metadata":{}},{"cell_type":"code","source":"print(f\"There are {int(customers_df.describe()['Customer ID']['count'])} customers overall\")\nprint(f\"There are {customers_df[customers_df['amount'] == 0].shape[0]} customers who spent 0\")\nprint(customers_df[customers_df['amount'] == 0].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see around 0.05% (3 out of 5,881) of customers spent 0 at our store. They have made just 1 purchase each. \n#### Therefore we can exclude them from our analysis in order to focus on customers who spent more than zero at store.","metadata":{}},{"cell_type":"code","source":"customers_df = customers_df[customers_df['amount'] > 0].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_variable_distribution(X, column_name):\n    \n    fig, ax = plt.subplots()\n    \n    ax.boxplot(x=X, notch=True)\n    ax.set_title(f\"{column_name} distribution boxplot\")\n    \n    mu = X.mean()\n    sigma = X.std()\n    num_bins = 50\n    \n    fig1, ax1 = plt.subplots()\n    \n    # the histogram of the data\n    n, bins, patches = ax1.hist(X, num_bins, density=True)\n\n    # add a 'best fit' line\n    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n         np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n    \n    mu = round(mu, 2)\n    sigma = round(sigma, 2)\n    \n    ax1.plot(bins, y, '--')\n    ax1.set_xlabel(column_name)\n    ax1.set_ylabel('Probability density')\n    ax1.set_title(f'Histogram of {column_name} distribution: $\\mu={mu}$, $\\sigma={sigma}$')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_columns = ['frequency', 'amount', 'recency']\nfor col_name in feature_columns:\n    \n    x = customers_df[col_name]\n    \n    plot_variable_distribution(x, col_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we can see on the plots above, the distribution of variables of interest does not look like normal. There are outliers in the dataset in terms of recency, frequency of purchases and amount spent by customers. Altough these outliers probably skew the distribution of variables of intereset from normailty into non-normality, arguably we should keep these observations in our dataset. If we get rid of these \"outlier\" observations, we can lose a valueable information about users who bring a big part of revenue or number of purchases. \n\n### Instead we can try to transform the distribution of the selected variables. This will allow us to utilize these variables for customer segmentation without any concerns about non-normality of variables' distributions.","metadata":{}},{"cell_type":"markdown","source":"But first of all - let's double check whether variable samples are normal or not with Sharipo-Wilk, D’Agostino’s K^2 and Anderson-Darling statistical tests.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import shapiro\nfrom scipy.stats import normaltest\nfrom scipy.stats import anderson\n\nfor col_name in feature_columns:\n    \n    print(f'Sharipo-Wilk test for {col_name}')\n    stat, p = shapiro(customers_df[col_name])\n    print('Statistics=%.3f, p=%.3f of %s distribution' % (stat, p, col_name))\n\n    alpha = 0.05\n    if p > alpha:\n        print(f'Sample of {col_name} looks normal (fail to reject H0) with Sharipo-Wilk Test')\n    else:\n        print(f'Sample of {col_name} does not look Gaussian (reject H0) with Sharipo-Wilk Test \\n\\n')\n        \n    print(f'D’Agostino’s K^2 test for {col_name}')\n    stat, p = normaltest(customers_df[col_name])\n    print('Statistics=%.3f, p=%.3f of %s distribution' % (stat, p, col_name))\n\n    alpha = 0.05\n    if p > alpha:\n        print(f'Sample of {col_name} looks normal (fail to reject H0) with D’Agostino’s K^2 Test')\n    else:\n        print(f'Sample of {col_name} does not look normal (reject H0) with D’Agostino’s K^2 Test \\n\\n')\n        \n    print(f'Anderson-Darling test for {col_name}')\n    \n    result = anderson(customers_df[col_name])\n    print('Statistic: %.3f' % result.statistic)\n    p = 0\n    for i in range(len(result.critical_values)):\n        sl, cv = result.significance_level[i], result.critical_values[i]\n        if result.statistic < result.critical_values[i]:\n            print('%.3f: %.3f, data looks normal (fail to reject H0) with Anderson-Darling test \\n\\n' % (sl, cv))\n        else:\n            print('%.3f: %.3f, data does not look normal (reject H0) with Anderson-Darling test \\n\\n' % (sl, cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we can see from above, if we apply normality tests to our variables, the result shows that we deal probably with not normal distributions.\n\n### That is why we can try to apply natural logarithm transformation to our variables to keep the outliers and get normal-alike distribution.\n**The idea is inspired by Anatoly Karpov's [report](https://www.youtube.com/watch?v=dFCJysbOJ8c) at Matemarketing 2019 Conference.**","metadata":{}},{"cell_type":"code","source":"for col_name in feature_columns:\n    \n    x = customers_df[col_name]\n    \n    ln_x = x.apply(lambda x: log(x))  # apply ln transformation to our variables\n    \n    ln_col_name = 'ln ' + col_name\n    \n    plot_variable_distribution(ln_x, ln_col_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Log trasnformation made distribution to look more like normal distributiions. As a 'bonus' after log transformation we now have all variables in a pretty much similar absolute values range.","metadata":{}},{"cell_type":"code","source":"customers_df['r'] = customers_df['recency'].apply(lambda x: log(x))\ncustomers_df['f'] = customers_df['frequency'].apply(lambda x: log(x))\ncustomers_df['m'] = customers_df['amount'].apply(lambda x: log(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_df = customers_df[['Customer ID', 'r', 'f', 'm']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now we can go to the segmentation of our customers by recency, frequency and amount of purchases. \n### We will try to use k-means clusterization algorithm. After that we will review clusters by decistion tree algorithm to make explicit interpretation using CART.  ","metadata":{}},{"cell_type":"markdown","source":"# Step 3. Customer segmentation using K-Means","metadata":{}},{"cell_type":"code","source":"use_df.set_index('Customer ID', inplace=True)\nuse_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's import necessary modules from sklearn library","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ninertia = []\nss = []\nfor k in range(2, 15):\n    %time kmeans = KMeans(n_clusters=k, random_state=1).fit(use_df)\n    ss.append(silhouette_score(use_df, kmeans.labels_, metric='euclidean'))\n    inertia.append(np.sqrt(kmeans.inertia_))\n    \nprint('\\n\\nTOTAL CELL RUNTIME: ', )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(2, 15), inertia, marker='s')\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Optimal number of clusters is 4 according to 'elbow rule'. After k=4 the change in inertia is less dramatical than it was before k=4.","metadata":{}},{"cell_type":"code","source":"plt.plot(range(2, 15), ss, marker='h')\nplt.xlabel('$k$')\nplt.ylabel('silhouette_score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Silhouette score also confirms the suggestion that 4 is an optimal number of clusters.\n#### Let's also compare the speed and performance of MiniBatchKmeans with default KMeans algorithm from skickit-learn.","metadata":{}},{"cell_type":"code","source":"%%time\ninertia_minib = []\nss_minib = []\nfor k in range(2, 15):\n    %time kmeans_mini = MiniBatchKMeans(n_clusters=k, random_state=1).fit(use_df)\n    ss_minib.append(silhouette_score(use_df, kmeans_mini.labels_, metric='euclidean'))\n    inertia_minib.append(np.sqrt(kmeans_mini.inertia_))\n    \nprint('\\n\\nTOTAL CELL RUNTIME: ', )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(2, 15), inertia_minib, marker='s')\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$')\nplt.title('Intertia')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(2, 15), ss_minib, marker='h')\nplt.xlabel('$k$')\nplt.ylabel('silhouette_score')\nplt.title('Silhouette score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can obtain similar conclusions from MiniBatchKMeans algorithm and KMeans algorithm runs. Nevetheless, there is a dramatic improvement in terms of runtime using MiniBatchKMeans in comparison to default KMeans algorithm (17.2 seconds vs 4 min 49 seconds in total run time of cell). \n\n#### Thereby, we can use MiniBatchKMeans algorithm on large datasets, (scikit-learn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) says n >= 10k observations) to find the optimal number of clusters for KMeans algorithm to save our time.","metadata":{}},{"cell_type":"markdown","source":"However, to get just one interation of clusterization we can use default KMeans algorithm to keep results stable. We will not observe such a dramatic improvement using MiniBatchKMeans in comparison to default KMeans algorithm one just one iteration as we have seen that on n > 1 iterations.","metadata":{}},{"cell_type":"code","source":"c = KMeans(n_clusters=4, random_state=42)\nuse_df['cluster'] = c.fit_predict(use_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_df['cluster'] = use_df['cluster'] + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_df['cluster'] = use_df['cluster'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The distribution of customers among clusters is almost even without any huge overloads towards any cluster.","metadata":{}},{"cell_type":"markdown","source":"### Let's explore clusters distributions with some basic visualizattions","metadata":{}},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\n\nscatter = ax.scatter(use_df['r'], use_df['f'], c=use_df['cluster'], s=50)\n                    \nax.set_title('RFM clusters')\nax.set_xlabel('r')\nax.set_ylabel('f')\nplt.colorbar(scatter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\n\nscatter = ax.scatter(use_df['r'], use_df['m'], c=use_df['cluster'], s=50)\n                    \nax.set_title('RFM clusters')\nax.set_xlabel('r')\nax.set_ylabel('m')\nplt.colorbar(scatter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\n\nscatter = ax.scatter(use_df['f'], use_df['m'], c=use_df['cluster'], s=50)\n                    \nax.set_title('RFM clusters')\nax.set_xlabel('f')\nax.set_ylabel('m')\nplt.colorbar(scatter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's try to visualize the relationship between clusters, r, f and m in one scatter matrix to try to grasp the whole picture: ","metadata":{}},{"cell_type":"code","source":"sns.pairplot(use_df, hue=\"cluster\", markers=[\"o\", \"s\", \"D\", \"*\"], \n             palette=sns.color_palette('Set1', n_colors=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A high-level interpreation for each cluster:\n### Cluster 4: low recency, high frequency, high monetary value - did buy recently with high frequency and high monetary value. The most loyal and valueable customers.\n### Cluster 1: high recency, high frequency, high monetary value - did NOT buy recently, buy with high frequency and with high monetary value. Need to re-activate these users, need to convert them into the first cluster. \n### Cluster 3: high recency, low frequency, low monetary value. - did NOT buy recently, buy with low frequency and monetary value. Probably customers who just bought for one time and churned. Can be converted into Cluster 4 customers.\n### Cluster 2: low recency, low frequency, low monetary value. - did buy recently, buy with high frequency and monetary value. Probably new customers who just made a couple of new orders.\n\n### To get a more explicit interpretation in terms of how customers' segments differentiate from each other we will train a decision tree classifier. This will allow us to see the boundaries between segments.","metadata":{}},{"cell_type":"markdown","source":"# Step 4. Get explicit interpetation for clusters","metadata":{}},{"cell_type":"code","source":"use_df = use_df.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge info about client's cluster.","metadata":{}},{"cell_type":"code","source":"final_df = customers_df.merge(use_df[['Customer ID', 'cluster']], how='inner', on='Customer ID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop not neccesary columns for further interpretation of clusters\nfinal_df.drop(columns=['r', 'f', 'm'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier  # import decision tree classifier\nfrom sklearn.model_selection import train_test_split  # import train_test_split function\nfrom sklearn import metrics  # import scikit-learn metrics module for accuracy calculation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(feature_columns)\nX = final_df[feature_columns]\ny = final_df['cluster']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)  # 80% training and 20% test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=17)\nclf_tree = clf_tree.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf_tree.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy score: \", metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the decision tree we trained","metadata":{}},{"cell_type":"code","source":"import io\nfrom io import StringIO\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image  \nimport pydotplus\nimport graphviz\nfrom pydot import graph_from_dot_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_names = ['1', '2', '3', '4']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_decision_tree(clf, feature_cols, class_names):\n\n    dot_data = StringIO()\n    export_graphviz(clf, out_file=dot_data,  \n                    filled=True, rounded=True,\n                    special_characters=True,\n                    feature_names = feature_cols,\n                    class_names=class_names)\n    (graph, ) = graph_from_dot_data(dot_data.getvalue())\n    \n    return Image(graph.create_png())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_decision_tree(clf_tree, feature_columns, cluster_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can arguably keep the depth of decision tree up to 2 as at depth = 3 there are no rules that can be applied without any conflict with the previous rules in terms of entropy gain. Also, step at depth 3 in dividing cluster 3 customers migh look like overfitting.\n\n### Let's now train new decision tree with max_depth = 2","metadata":{}},{"cell_type":"code","source":"clf_2 = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=17)\nclf_2 = clf_2.fit(X_train,y_train)\ny_pred = clf_2.predict(X_test)\nprint(\"Accuracy score: \", metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With max_depth of 2 we even got a little bit higher accuracy of predictions.\n\n### Let's visualize the tree with max_depth = 2","metadata":{}},{"cell_type":"code","source":"plot_decision_tree(clf_2, feature_columns, cluster_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Also from the way decision tree trained rules we can see that frequency is not used because it is probably correlate with amount spent. This can also be seen on plots we had above with 4 clusters. There is no cluster with high frequency & low amount or low frequency & high amount. \n\n## This point is part of the criticism towards the RFM-methodology. An alternative solution might be to use average amount spent per one invoice instead of total amount spent by customer.","metadata":{}},{"cell_type":"markdown","source":"**Huge thanks to https://mljar.com/ project blog for providing useful custom functions to retrieve rules from decision tree algorithm!**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import _tree\ndef get_rules(tree, feature_names, class_names):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n\n    paths = []\n    path = []\n    \n    def recurse(node, path, paths):\n        \n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            p1, p2 = list(path), list(path)\n            p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n            recurse(tree_.children_left[node], p1, paths)\n            p2 += [f\"({name} > {np.round(threshold, 3)})\"]\n            recurse(tree_.children_right[node], p2, paths)\n        else:\n            path += [(tree_.value[node], tree_.n_node_samples[node])]\n            paths += [path]\n            \n    recurse(0, path, paths)\n\n    # sort by samples count\n    samples_count = [p[-1][1] for p in paths]\n    ii = list(np.argsort(samples_count))\n    paths = [paths[i] for i in reversed(ii)]\n    \n    rules = []\n    for path in paths:\n        rule = \"if \"\n        \n        for p in path[:-1]:\n            if rule != \"if \":\n                rule += \" and \"\n            rule += str(p)\n        rule += \" then \"\n        if class_names is None:\n            rule += \"response: \"+str(np.round(path[-1][0][0][0],3))\n        else:\n            classes = path[-1][0][0]\n            l = np.argmax(classes)\n            rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n        rule += f\" | based on {path[-1][1]:,} samples\"\n        rules += [rule]\n        \n    return rules","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rule_values = get_rules(clf_2, feature_columns, cluster_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rule_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the rules formulated above we can divide out customers into segments. ","metadata":{}},{"cell_type":"code","source":"conditions = [\n    (final_df['recency'] > 71.5) & (final_df['amount'] <= 636.02),\n    (final_df['recency'] > 71.5) & (final_df['amount'] > 636.02),\n    (final_df['recency'] <= 71.5) & (final_df['amount'] > 1509.575),\n    (final_df['recency'] <= 71.5) & (final_df['amount'] <= 1509.575)\n]\n\nchoices = ['high recency - low amount', 'high recency - high amount', 'low recency - high amount',\n          'low recency - low amount']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df['segment'] = np.select(conditions, choices, default='other')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_stats_df = final_df.groupby('segment').agg(\n    median_recency = ('recency', 'median'),\n    median_frequency = ('frequency', 'median'),\n    median_amount = ('amount', 'median'),\n    customers = ('Customer ID', 'nunique')\n).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_stats_df['% cutomers percentage'] = round(\n    agg_stats_df['customers'] / agg_stats_df['customers'].sum() * 100, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_stats_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Conclustion and first insights\n### Some insigths about recency of purchases by customers\n\nAccoridng to data around 57% of clients are in 'high recency' segments. This might be an indicator for improving the retention of buyers, introducing the loyalty program. The business should challenge the stategy in acquisition channels that bring a lot of customers who stay in 'high recency - low amount' segment. \nSpecial treament in re-activation should be done to 'high recency - high amount' as those are more valueable customers for the business.\n\nAs we have retrieved from rules here we can also track the percentage of users who made a purchase less than 71.5 days ago. A possible KPI of improvements in this area of business might be the percentage of customers who made purchase less than 71.5 days ago. The greater this percentage gets - the better.\n\nAt the same time, there should be a mechanism to prevent the transfer of customers from segments with low recency into segments with high recency. It might be profitable to create special offers for customers who got close to turning into 'high recency' customer. Specifically, if the customer did not buy anyting between less than 60-70 days ago, it might be worth trying to give such customer a special offer.","metadata":{}},{"cell_type":"code","source":"total_stats_df = final_df.groupby('segment').agg(\n    total_frequency = ('frequency', 'sum'),\n    total_amount = ('amount', 'sum')\n).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = total_stats_df['segment']\nsizes = total_stats_df['total_frequency']\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nax1.set_title('Total frequency by segment')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T20:33:20.153945Z","iopub.execute_input":"2021-05-23T20:33:20.154418Z","iopub.status.idle":"2021-05-23T20:33:20.209954Z","shell.execute_reply.started":"2021-05-23T20:33:20.154306Z","shell.execute_reply":"2021-05-23T20:33:20.208631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"61.7% of all orders are made by 'low recency - high amount' segment customers","metadata":{}},{"cell_type":"code","source":"labels = total_stats_df['segment']\nsizes = total_stats_df['total_amount']\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nax1.set_title('Total amount by segment')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"72.2% of all amount is spent by 'low recency - high amount' segment customers","metadata":{}},{"cell_type":"code","source":"final_df.to_csv('./customer_segments.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The further notebooks using customer segments will involve:\n1. Analysis of different segments of clients in terms geo, time of purchases;\n2. Analysis of cancelled orders - it might be useful to compares segments from cancellations perspective","metadata":{}},{"cell_type":"markdown","source":"Links and resources used in this notebook:\n\n\n2. https://habr.com/ru/company/mindbox/blog/423463/ - mindbox about how they have created similar solution for their customers\n3. https://habr.com/ru/company/mindbox/blog/420915/ - yet another article by mindbox\n4. https://stats.stackexchange.com/questions/102984/is-there-a-decision-tree-like-algorithm-for-unsupervised-clustering - stats exchange post about general way of approach\n5. https://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree - about retrieving rules\n6. https://habr.com/ru/company/ods/blog/322534/#vvedenie - ods.ai post on decision trees\n7. https://habr.com/ru/company/ods/blog/325654/#vybor-chisla-klasterov-dlya-kmeans - ods.ai post on KMeans dataset.\n8. https://mljar.com/blog/extract-rules-decision-tree/ - great overview of how rules can be extracted from decision tree classifier in human-readable text","metadata":{}},{"cell_type":"markdown","source":"#### If you manage to get here, thank you very much! Please, upvote kernel if you like it and leave your opinion on this kernel in comments section.","metadata":{}}]}