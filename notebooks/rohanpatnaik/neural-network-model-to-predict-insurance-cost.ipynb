{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-21T15:54:10.701378Z","iopub.execute_input":"2021-09-21T15:54:10.702274Z","iopub.status.idle":"2021-09-21T15:54:10.718812Z","shell.execute_reply.started":"2021-09-21T15:54:10.702234Z","shell.execute_reply":"2021-09-21T15:54:10.717919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start by importing the required libraries first\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:10.721014Z","iopub.execute_input":"2021-09-21T15:54:10.721574Z","iopub.status.idle":"2021-09-21T15:54:16.434053Z","shell.execute_reply.started":"2021-09-21T15:54:10.721519Z","shell.execute_reply":"2021-09-21T15:54:16.43324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance = pd.read_csv(\"../input/insurance/insurance.csv\")\ninsurance","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:16.435203Z","iopub.execute_input":"2021-09-21T15:54:16.435421Z","iopub.status.idle":"2021-09-21T15:54:16.483612Z","shell.execute_reply.started":"2021-09-21T15:54:16.435396Z","shell.execute_reply":"2021-09-21T15:54:16.482135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_one_hot = pd.get_dummies(insurance)\ninsurance_one_hot.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:16.486183Z","iopub.execute_input":"2021-09-21T15:54:16.486525Z","iopub.status.idle":"2021-09-21T15:54:16.515676Z","shell.execute_reply.started":"2021-09-21T15:54:16.486484Z","shell.execute_reply":"2021-09-21T15:54:16.515119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now lets create the X and y i.e. the dependent variables\nX = insurance_one_hot.drop(\"charges\",axis = 1)\nX","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:16.517043Z","iopub.execute_input":"2021-09-21T15:54:16.517285Z","iopub.status.idle":"2021-09-21T15:54:16.536696Z","shell.execute_reply.started":"2021-09-21T15:54:16.517257Z","shell.execute_reply":"2021-09-21T15:54:16.536083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = insurance_one_hot[\"charges\"]\ny","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:16.537648Z","iopub.execute_input":"2021-09-21T15:54:16.53833Z","iopub.status.idle":"2021-09-21T15:54:16.55407Z","shell.execute_reply.started":"2021-09-21T15:54:16.538299Z","shell.execute_reply":"2021-09-21T15:54:16.553069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\nlen(X), len(X_test), len(X_train), len(y), len(y_train), len(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:16.555723Z","iopub.execute_input":"2021-09-21T15:54:16.556322Z","iopub.status.idle":"2021-09-21T15:54:17.292961Z","shell.execute_reply.started":"2021-09-21T15:54:16.556277Z","shell.execute_reply":"2021-09-21T15:54:17.292153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now to build a neural net for this \n\ntf.random.set_seed(42)\n\n# Create the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(100),\n    tf.keras.layers.Dense(10),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel.compile(loss = \"mae\",\n             optimizer = \"Adam\",\n             metrics = [\"mae\"])\n\n# Fit the model\nhist = model.fit(X_train, y_train, epochs = 500)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:17.294054Z","iopub.execute_input":"2021-09-21T15:54:17.294668Z","iopub.status.idle":"2021-09-21T15:54:39.801711Z","shell.execute_reply.started":"2021-09-21T15:54:17.294633Z","shell.execute_reply":"2021-09-21T15:54:39.800701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now its time to evaluate the model\nmodel.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:39.80322Z","iopub.execute_input":"2021-09-21T15:54:39.80357Z","iopub.status.idle":"2021-09-21T15:54:39.985583Z","shell.execute_reply.started":"2021-09-21T15:54:39.803535Z","shell.execute_reply":"2021-09-21T15:54:39.984985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.median(), y_train.mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:39.987639Z","iopub.execute_input":"2021-09-21T15:54:39.988121Z","iopub.status.idle":"2021-09-21T15:54:39.994592Z","shell.execute_reply.started":"2021-09-21T15:54:39.988083Z","shell.execute_reply":"2021-09-21T15:54:39.99362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(hist.history).plot()\nplt.xlabel(\"loss\")\nplt.ylabel(\"epochs\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-21T15:54:39.995865Z","iopub.execute_input":"2021-09-21T15:54:39.996086Z","iopub.status.idle":"2021-09-21T15:54:40.292576Z","shell.execute_reply.started":"2021-09-21T15:54:39.99606Z","shell.execute_reply":"2021-09-21T15:54:40.291771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We still need to preprocess the data \nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\n\n#Create a column transformer\nct = make_column_transformer(\n    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]),\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"]))\n    \n# Create the X and y\nX = insurance.drop(\"charges\", axis = 1)\ny = insurance[\"charges\"]\n    \n# Build our train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n    \n# Fit the column transformer to our training data\nct.fit(X_train)\n    \n# Transform training and test data with normalization and OneHotEncoder\nX_train_normal = ct.transform(X_train)\nX_test_normal = ct.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:40.293901Z","iopub.execute_input":"2021-09-21T15:54:40.294228Z","iopub.status.idle":"2021-09-21T15:54:40.332813Z","shell.execute_reply.started":"2021-09-21T15:54:40.294185Z","shell.execute_reply":"2021-09-21T15:54:40.332173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.loc[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:40.334004Z","iopub.execute_input":"2021-09-21T15:54:40.334814Z","iopub.status.idle":"2021-09-21T15:54:40.342963Z","shell.execute_reply.started":"2021-09-21T15:54:40.334775Z","shell.execute_reply":"2021-09-21T15:54:40.342113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_train_normal.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:54:40.344346Z","iopub.execute_input":"2021-09-21T15:54:40.344585Z","iopub.status.idle":"2021-09-21T15:54:40.356483Z","shell.execute_reply.started":"2021-09-21T15:54:40.344557Z","shell.execute_reply":"2021-09-21T15:54:40.355643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a neural net again to fit on our normalized data\ntf.random.set_seed(42)\n\n# Create the model\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(100),\n    tf.keras.layers.Dense(10),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel2.compile(loss = \"mae\",\n             optimizer = tf.keras.optimizers.Adam(lr = 0.01),\n             metrics = [\"mae\"])\n\n# Fit the model\nhist = model2.fit(X_train_normal, y_train, epochs = 200)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:57:51.454578Z","iopub.execute_input":"2021-09-21T15:57:51.45492Z","iopub.status.idle":"2021-09-21T15:58:00.904476Z","shell.execute_reply.started":"2021-09-21T15:57:51.45489Z","shell.execute_reply":"2021-09-21T15:58:00.903594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the model now\nmodel2.evaluate(X_test_normal,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T15:58:03.225884Z","iopub.execute_input":"2021-09-21T15:58:03.226256Z","iopub.status.idle":"2021-09-21T15:58:03.393272Z","shell.execute_reply.started":"2021-09-21T15:58:03.226219Z","shell.execute_reply":"2021-09-21T15:58:03.392478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}