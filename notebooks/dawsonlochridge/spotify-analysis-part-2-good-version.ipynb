{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot as plt\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import chardet\n#traditional encoding UTF-8 was not the correct encoding, try using chardet to guess the encoding by looking at first 10000 characters\nwith open(\"../input/top-spotify-songs-from-20102019-by-year/top10s.csv\", 'rb') as raw_data:\n    result = chardet.detect(raw_data.read(10000))\n    #print(result) already found encoding so commenting this line to keep it out of the output\n\n\n#chardet guessed Windows-1252, plug in encoding and see if it is correct\nsp_data = pd.read_csv(\"../input/top-spotify-songs-from-20102019-by-year/top10s.csv\", encoding = 'Windows-1252')\n\n#look at first 10 rows of data\nsp_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Time to check missing data\nmissing_values_count = sp_data.isnull().sum()\n\n#look at the # of missing points in all of the columns\nmissing_values_count[0:15]\n\n#this is good, when working on this data set previously, it was noticed that pop has some 0 values which doesnt make sense...lets find them\npop_0 = sp_data.groupby('pop').pop.count()\nprint(pop_0)\n\nzero_pop = sp_data.loc[sp_data['pop'] == 0]\nzero_pop\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at these zero values, it looks as if the data was simply just left off, should not hurt to simply drop these values by creating a new dataframe without them\n#lets also remove all of rows with 2019 as the year since that will be the year we predict with the new data set and dont want data leakage\nsp_clean = sp_data.copy()\nsp_clean = sp_clean.loc[sp_clean['year'] != 2019]\nsp_clean = sp_clean.loc[sp_clean['pop'] != 0]\nsp_clean.loc[sp_clean['pop'] > 87]\nprint(sp_clean.groupby('pop').pop.count())\n\n#lets look at some of the other population values, is there a reason why some are so low? \nsp_clean.loc[sp_clean['pop'] <= 30].head(200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Okay, so it looks like we still have some low values, but are they outside of the norm?\nsp_clean['pop'].describe()\nsp_clean['top genre'].unique()\n#STD is relatively high for this column and the mean is 60 with teh lower percentile being in the 60 range...lets make a boxplot to spot outliers\nsns.boxplot(x=sp_clean['pop'])\nsns.stripplot(x=sp_clean['pop'], color='black', alpha = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#looks like we have some outliers, lets find them and remove them from the dataset\nsp_out = sp_clean.copy()\nQ1 = sp_out['pop'].quantile(0.25)\nQ3 = sp_out['pop'].quantile(0.75)\nIQR = Q3-Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find which rows are outliers\nfilter = (sp_out['pop'] >= Q1 - 1.5 * IQR) & (sp_out['pop'] <= Q3 + 1.5 *IQR)\nsp_out = sp_out.loc[filter]\nprint(sp_out.groupby('pop').pop.count())\n#there we go, we have removed all popularity points that were outside of the IQR assuming that they did not make sense\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets create a new copy and save it to sp\nsp = sp_out.copy()\n#remove the unnamed column\nsp = sp.loc[:, ~sp.columns.str.contains('^Unnamed')]\n#break the data into categorical and numerical data\nsp_num = sp.drop(columns = ['artist','title','top genre', 'pop','year'])\nsp_cat = sp.drop(columns = ['bpm','nrgy', 'dnce', 'dB', 'live', 'val', 'dur','acous','spch','pop'])\n\n#create the prediction variable\ny = sp['pop']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot numerical data to check distributions\nfig, axarr = plt.subplots(2, 5, figsize=(50, 15), squeeze = False)\nnr = 0\nnc = 0\n\nfor col in sp_num.columns:\n    if nc == 5:\n        nr = 1\n        nc = 0\n    sns.distplot( a = sp_num[col], kde = True, ax = axarr[nr][nc])\n    axarr[nr][nc].set_title(\"Distribution\",fontsize = 20)\n    axarr[nr][nc].set_xlabel(col, fontsize = 20)\n    axarr[nr][nc].set_ylabel('Frequency', fontsize = 20)\n    nc = nc + 1\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets standardize some of our columns\nfrom scipy import stats\n\n\n#for min_max scaling\nfrom mlxtend.preprocessing import minmax_scaling\nfig, axarr = plt.subplots(2, 5, figsize=(50, 15), squeeze = False)\nnr = 0\nnc = 0\nsp_num_scal = sp_num.copy()\nfor col in sp_num.columns:\n    if (col != 'year') and (col != 'dB'):\n        sp_num_s = (minmax_scaling(sp_num[col], columns = [0]))\n        if nc == 5:\n            nr = 1\n            nc = 0\n        sns.distplot( a = sp_num_s, kde = True, ax = axarr[nr][nc])\n        axarr[nr][nc].set_title(\"Distribution\",fontsize = 20)\n        axarr[nr][nc].set_xlabel(col, fontsize = 20)\n        axarr[nr][nc].set_ylabel('Frequency', fontsize = 20)\n        nc = nc + 1\n        sp_num_scal[col] = sp_num_s\n        \n        \nsns.distplot(a = sp_num['dB'], kde = True, ax = axarr[1][3])\n        \n        \n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets normalize and replot using a boxplot method\n#for box-cox transformation\n\nfig, axarr = plt.subplots(2, 5, figsize=(50, 15), squeeze = False)\nnr = 0\nnc = 0\nsp_num_n = []\nsp_num_norm = sp_num.copy()\nfor col in sp_num.columns:\n    if (col != 'year') and (col != 'dB'):\n        sp_num_n = stats.boxcox(sp_num[col]+1)\n        sp_num_n = sp_num_n[0]\n        if nc == 5:\n            nr = 1\n            nc = 0\n        sns.distplot( a = sp_num_n, kde = True, ax = axarr[nr][nc])\n        axarr[nr][nc].set_title(\"Distribution\",fontsize = 20)\n        axarr[nr][nc].set_xlabel(col, fontsize = 20)\n        axarr[nr][nc].set_ylabel('Frequency', fontsize = 20)\n        nc = nc + 1\n        sp_num_norm[col] = sp_num_n\n        \nsns.distplot(a = sp_num['dB'], kde = True, ax = axarr[1][3])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the year column into a string value\nsp_cat['year'] = sp_cat['year'].apply(str)\nsp['year'] = sp['year'].apply(str)\n\n\n\n\n#lets begin by joining the data we have and starting with a new data frame for our scaled and normalized data\nsp_scaled = sp_cat.join(sp_num_scal)\nsp_scaled2 = sp_scaled.copy()\nsp_norm = sp_cat.join(sp_num_norm)\nsp_original = sp\n\n\n#create a function that encodes the categorical label\nfrom sklearn.preprocessing import LabelEncoder\n\ndef label_encode(df,col):\n    from sklearn.preprocessing import LabelEncoder\n    encoder = LabelEncoder()\n    encoded = df[col].apply(encoder.fit_transform)\n    df[col] = encoded[col]\n\n    \n\nsp_cat_names = ['title','artist','top genre', 'year']\n\n\n\nlabel_encode(sp_scaled, sp_cat_names)\nlabel_encode(sp_norm, sp_cat_names)\nlabel_encode(sp_original,sp_cat_names)\n\n\nX_scal = sp_scaled.drop(columns = ['title','artist'])\nX_norm = sp_norm.drop(columns = ['title','artist'])\nX_orig = sp_original.drop(columns = ['title','artist','pop'])\n\n#going to save the encoding info for top genre\nencoder = LabelEncoder()\nencoder.fit(sp_scaled2['top genre'])\nencoder.transform(sp_scaled2['top genre'])\ny_fin = y/100\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#time to seperate the data into training, testing, and validation data and creating a model by creating a function that does so\n\ndef model_creation(X,y,data):\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.tree import DecisionTreeRegressor\n    from sklearn.model_selection import train_test_split\n    from xgboost import XGBRegressor\n    from matplotlib import pyplot as plt\n    pd.plotting.register_matplotlib_converters()\n    %matplotlib inline\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import validation_curve\n\n\n\n    \n    #data splitting\n    train_X, val_X, train_y, val_y = train_test_split(X, y, train_size = .8, test_size = .2,random_state = 0)\n    \n    #model creation\n    model_RF = RandomForestRegressor(n_estimators = 100,max_leaf_nodes = 3, random_state = 1)\n    model_DT = DecisionTreeRegressor(max_leaf_nodes = 2,random_state = 1)\n    model_XGB = XGBRegressor(n_estimators = 500, learning_rate = 0.05, n_jobs = 6)\n    #model fitting\n    RF = model_RF.fit(train_X, train_y)\n    DT = model_DT.fit(train_X, train_y)\n    XGB = model_XGB.fit(train_X, train_y,\n                 early_stopping_rounds = 5,\n                 eval_set = [(val_X,val_y)],\n                 verbose = False)\n    \n\n\n    #model predictions\n\n    RF_pred = model_RF.predict(val_X)\n    DT_pred = model_DT.predict(val_X)\n    XGB_pred = model_XGB.predict(val_X)\n    \n    \n    #Cross Validation\n    scores1 = -1 * cross_val_score(model_RF, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    scores2 = -1 * cross_val_score(model_DT, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    scores3 = -1 * cross_val_score(model_XGB, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    \n    \n   \n  \n    #print MAE\n    print(data + \" Model - Random Forest Regression Model: MAE = \" + str(mean_absolute_error(val_y,RF_pred)*100) + \"\\nAverage Cross Validation MAE Score: \" + str(scores1.mean()*100) )\n    print(data + \" Model - Decision Tree Model: MAE = \" + str(mean_absolute_error(val_y,DT_pred)*100) + \"\\nAverage Cross Validation MAE Score: \" + str(scores2.mean()*100) )\n    print(data + \" Model - Gradient Boost Model: MAE = \" + str(mean_absolute_error(val_y,XGB_pred)*100) + \"\\nAverage Cross Validation MAE Score: \" + str(scores3.mean()*100)+ \"\\n\")\n    return model_RF\n    \ndef KNN_model(X, y, data):\n    from matplotlib import pyplot as plt\n    pd.plotting.register_matplotlib_converters()\n    %matplotlib inline\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import mean_absolute_error\n    \n    for k in range(3, 15, 2):\n        model_KNN = KNeighborsClassifier(n_neighbors = k)\n        scores = -1 * cross_val_score(model_KNN, X, y,\n                                    cv = 5,\n                                    scoring = 'neg_mean_absolute_error')\n        score_mean1 = scores.mean()*100\n        if k == 3:\n            score_mean_best = scores.mean()*100\n            k_best = 3\n        elif score_mean1 > score_mean_best:\n            k_best = k\n            score_mean_best = score_mean1\n            \n    print(data + \" Model - KNN: K = \" + str(k_best) + \" Average Cross Validation MAE Score: \" + str(score_mean_best))\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Call the function and create 3 models for each data set\nmodel_creation(X_orig,y_fin,'Original')\nmodel_RF = model_creation(X_scal,y_fin,'Scaled')\nmodel_creation(X_norm,y_fin,'Normalized')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets try KNN modeling and create categories for the pop where above a 90 is a 1 (A Tier), 80 is a 2 (B Tier), and so on to where anything under 50 is a 6 (D Tier)\nfrom matplotlib import pyplot as plt\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n    \n\n\n\nsp_scaled_knn = sp_scaled.copy()\nsp_scaled_knn = sp_scaled_knn.join(y_fin)\nsp_scaled_knn['tier'] = ''\nsp_scaled_knn = sp_scaled_knn.set_index('pop')\nsp_scaled_knn = sp_scaled_knn.reset_index()\nfor i in range(0,554):\n    if sp_scaled_knn.loc[i,'pop'] >= .90:\n        sp_scaled_knn.loc[i,'tier'] = 1\n    elif sp_scaled_knn.loc[i,'pop'] >= .80:\n        sp_scaled_knn.loc[i,'tier'] = 2\n    elif sp_scaled_knn.loc[i,'pop'] >= .70:\n        sp_scaled_knn.loc[i,'tier'] = 3\n    elif sp_scaled_knn.loc[i,'pop'] >= .60:\n        sp_scaled_knn.loc[i,'tier'] = 4\n    elif sp_scaled_knn.loc[i,'pop'] >= .50:\n        sp_scaled_knn.loc[i,'tier'] = 5\n    else:\n        sp_scaled_knn.loc[i,'tier'] = 6\n        \n        \ny_tier = sp_scaled_knn['tier'].apply(int)\nX = sp_scaled_knn.drop(columns = ['pop','title','artist','tier','year','dB'])\nX['top genre'] = X['top genre']/100\n\nKNN_model(X,y_tier,'Scaled')\n\n\n#dive deeper into this later\n        \n\n        \n\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_model(X,y, data, y_test = None):\n    from tensorflow import keras\n    from tensorflow.keras import layers, callbacks\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.model_selection import train_test_split\n    from learntools.deep_learning_intro.dltools import animate_sgd\n\n    k = 16\n    best_MAE = 100000\n    best_k = 0\n    \n    train_X, val_X, train_y, val_y = train_test_split(X, y, train_size = .8, test_size = .2,random_state = 0)\n\n\n\n    early_stopping = callbacks.EarlyStopping(\n        min_delta = .001,\n        patience =10,\n        restore_best_weights = True,\n    )\n    for i in range(1,5):\n        print(i)\n        model = keras.Sequential([\n           layers.Dense(k*4, activation='relu', input_shape=[11]),\n           layers.Dropout(0.3),\n           layers.Dense(k*2, activation='relu'),\n           layers.Dropout(0.3),\n           layers.Dense(1)\n        ])\n\n        model.compile(\n            optimizer = 'adam',\n            loss = 'mae',\n        )\n\n\n        history = model.fit(\n            train_X, train_y,\n            validation_data = (val_X, val_y),\n            batch_size = 512,\n            epochs = 500,\n            callbacks = [early_stopping], # put your callbacks in a list\n            verbose = 0) #turn off training log\n\n       \n        history_df = pd.DataFrame(history.history)\n        MAE = history_df['val_loss'].min()\n        print(\"MAE \" + str(MAE) + \" K \" + str(k) + \" i \" + str(i))\n        if best_MAE > MAE:\n            best_MAE = MAE\n            best_k = k\n            history_df.loc[:,['loss','val_loss']].plot()\n            best_model = model\n        k = k*2\n     \n    print(\"Minimum validation loss: \" + str(best_MAE) + \" and best k value was \" + str(best_k))\n    \n    \n        \n    \n    return best_model\n    \n\n\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtain best NN model\nmodel_NN = nn_model(X_scal, y, 'Scaled')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load data set for predictions\nnew_data = pd.read_csv(\"../input/top50spotify2019/top50.csv\", encoding = 'Windows-1252')\nnew_data.rename(columns = {list(new_data)[0]:'ranking'}, inplace = True)\n\nlist(new_data)\n\nnew_data_PredData = new_data.drop(columns = ['Track.Name', 'Artist.Name', 'ranking'])\nnew_data = new_data_PredData\nnew_data['year'] = 9\nnew_data = new_data[['Genre',\n 'year',\n 'Beats.Per.Minute',\n 'Energy',\n 'Danceability',\n 'Loudness..dB..',\n 'Liveness',\n 'Valence.',\n 'Length.',\n 'Acousticness..',\n 'Speechiness.',\n 'Popularity']]\n\nindex = 0\ncols = X_scal.columns\n\nfor col in cols:\n    new_data.rename(columns = {list(new_data)[index]:col}, inplace = True)\n    index = index + 1\n    \nnew_data.rename(columns = {list(new_data)[11]:'pop'}, inplace = True)\n\nnew_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessing on new data\nnum_cols = ['bpm','nrgy','dnce','live','val','dur','acous','spch']\ncat_cols = ['top genre', 'year']\nunique_sp = list(sp_scaled2['top genre'].unique())\nnew_data2 = new_data.copy()\nfor col in unique_sp: \n    new_data2= new_data2.loc[new_data2['top genre'] != col]\n    \nunique_nd = list(new_data2['top genre'].unique())\nfor col in unique_nd: \n    new_data= new_data.loc[new_data['top genre'] != col]\n\nnew_data['top genre'] = encoder.transform(new_data['top genre'])\nval_y_2019 = new_data['pop']\nnew_data = new_data.drop(columns = 'pop')\nfor col in num_cols:\n    new_data[col] = (minmax_scaling(new_data[col], columns = [0]))\n    \npred_x_NN = model_NN.predict(new_data)\npred_x_RF = model_RF.predict(new_data)\nprint(\"Model Final - Neural Network: MAE = \" + str(mean_absolute_error(val_y_2019,pred_x_NN)))\nprint(\"Model Final - Random Forest: MAE = \" + str((mean_absolute_error(val_y_2019/100,pred_x_RF))*100))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Going to attempt to explain model with SHAP values\n#import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\n#explainer = shap.DeepExplainer(model_NN)\n\n# Calculate Shap values\n#shap_values = explainer.shap_values(data_for_prediction)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA of original data, going to take a look at genres of music and see if there is anything informative\n#lets create a new copy and save it to sp\nsp_EDA = sp_out.copy()\n#remove the unnamed column\nsp_EDA = sp_EDA.loc[:, ~sp_EDA.columns.str.contains('^Unnamed')]\nnr = 0\nnc = 0\nfig, axarr = plt.subplots(3, 3, figsize=(50, 25), squeeze = False)\nfig.subplots_adjust(hspace = 0.5)\nfig.suptitle('Music Popularity by Year', fontsize = 50)\n\n\nfor i in range(2010, 2019):\n    year_genre = pd.DataFrame(sp_EDA.loc[sp_EDA['year'] == i].groupby('top genre').pop.count()).sort_values(by = 'pop', ascending = False)\n    year_genre = year_genre.reset_index()\n    sns.barplot(y = year_genre['top genre'] , x = year_genre['pop'], ax = axarr[nr][nc]).set_xticklabels(year_genre['pop'])\n    plt.subplot(axarr[nr,nc]).title.set_text(str(i))\n    nc = nc +1\n    if nc == 3:\n        nr = nr+1\n        nc = 0\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"by_year= pd.DataFrame(sp_EDA.groupby(['year','top genre']).count())\nby_year = by_year.reset_index()\ngenres = list(by_year['top genre'].unique())\nnr = 0\nnc = 0\nfig, axarr = plt.subplots(3, 5, figsize=(50,15), squeeze = False)\nplt.xlim(2010,2018)\nfig.subplots_adjust(wspace = .5)\nfig.suptitle('Music Popularity by Year', fontsize = 50)\nfig.add_axes(autoscale_on = False)\n\nfor genre in genres:\n    year_plot = by_year.loc[by_year['top genre'] == genre]\n    if year_plot['pop'].count() > 3:\n        sns.lineplot(x = year_plot['year'], y = year_plot['pop'], ax = axarr[nr][nc])\n        plt.xlim(2010,2018)\n        plt.ylim(0,60)\n        plt.subplot(axarr[nr,nc]).title.set_text(\"Popularity Trend for \" + str(genre))\n        nc = nc +1\n        if nc == 5:\n            nr = nr+1\n            nc = 0\n            \nsp_EDA.head()\n            \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ncolumns = ['bpm','nrgy','dnce','dB','live','val','dur','acous','spch','pop']\nnr = 0\nnc = 0\n#fig, axarr = plt.subplots(3, 4, figsize=(50,15), squeeze = False)\nfor col in columns:\n    averages = pd.DataFrame(sp_EDA.groupby('top genre')[col].mean())\n    averages = averages.reset_index()\n    names = list(averages['top genre'])\n    #df = pd.DataFrame(dict(\n        #r=averages[col],\n        #theta=[averages['top genre']]))\n    fig = px.line_polar(r=averages[col], theta=list(averages['top genre']), line_close=True)\n    \n    fig.update_traces(fill='toself')\n    fig.show()\n    #plt.subplot(axarr[nr,nc]).title.set_text(\"Average \" + str(col) + \" for Each Category\")\n    nc = nc +1\n    if nc == 4:\n        nr = nr+1\n        nc = 0\n\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genrecount = pd.DataFrame(sp_EDA.groupby(sp_EDA['top genre'])['top genre'].count())\ngenrecount.rename(columns = {list(genrecount)[0]:'count'}, inplace = True)\ngenrecount = genrecount.reset_index()\ngenrecount = genrecount.loc[genrecount['count'] >= 12]\ngenres = list(genrecount['top genre'].unique())\n\naverages = pd.DataFrame(sp_EDA.groupby('top genre').mean())\naverages = averages.reset_index()\ntop5= averages.loc[averages['top genre'].isin(['barbadian pop','boy band','canadian pop', 'dance pop', 'pop'])]\ntop5.set_index('top genre')\ntop5 = top5.drop(columns = ['year','bpm','dur','dB'])\ntop5 = top5.set_index('top genre')\n\nimport plotly.graph_objects as go\n\ncategories = genres\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatterpolar(\n      r=top5.iloc[0],\n      theta=top5.columns,\n      fill='toself',\n      name='Barbadian Pop'\n))\nfig.add_trace(go.Scatterpolar(\n      r=top5.iloc[1],\n      theta=top5.columns,\n      fill='toself',\n      name='Boy Band'\n))\nfig.add_trace(go.Scatterpolar(\n      r=top5.iloc[2],\n      theta=top5.columns,\n      fill='toself',\n      name='Canadian Pop'\n))\nfig.add_trace(go.Scatterpolar(\n      r=top5.iloc[3],\n      theta=top5.columns,\n      fill='toself',\n      name='Dance Pop'\n))\nfig.add_trace(go.Scatterpolar(\n      r=top5.iloc[4],\n      theta=top5.columns,\n      fill='toself',\n      name='Pop'\n))\nfig.update_layout(\n  polar=dict(\n    radialaxis=dict(\n      visible=True,\n      range=[0, 100]\n    )),\n  showlegend=True\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(sp_EDA['top genre'].unique())\nsp_EDA2 = sp_EDA.loc[sp_EDA['top genre'] == 'latin']\nsp_EDA2","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}