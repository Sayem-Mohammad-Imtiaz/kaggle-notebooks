{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom geopy.geocoders import Nominatim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install geopy\n!pip install Nominatim","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_Mumbai_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Mumbai.csv')\nr_Delhi_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Delhi.csv')\nr_Chennai_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Chennai.csv')\nr_Hyderabad_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Hyderabad.csv')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = r_Mumbai_df.copy().replace(9, np.nan, inplace=False)\ndf2 = r_Delhi_df.copy().replace(9, np.nan, inplace=False)\ndf3 = r_Chennai_df.copy().replace(9, np.nan, inplace=False)\ndf4 = r_Hyderabad_df.copy().replace(9, np.nan, inplace=False)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df1.dropna()\ndf2 = df2.dropna()\ndf3 = df3.dropna()\ndf4 = df4.dropna()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amentities = [\"Resale\",\"VaastuCompliant\",\"Wardrobe\",\"Refrigerator\",\"Sofa\",\"DiningTable\",\"TV\",\"GolfCourse\",\"Microwave\",\"BED\",\"LiftAvailable\",\"Children'splayarea\",\"Wifi\",\"AC\",\"Gasconnection\",\"WashingMachine\",\"Hospital\",\"MultipurposeRoom\",\"Cafeteria\",\"StaffQuarter\",\"CarParking\",\"PowerBackup\",\"24X7Security\",\"School\",\"ClubHouse\",\"ATM\",\"SportsFacility\",\"Intercom\",\"ShoppingMall\",\"IndoorGames\",\"RainWaterHarvesting\",\"JoggingTrack\",\"LandscapedGardens\",\"SwimmingPool\",\"Gymnasium\",\"MaintenanceStaff\"]","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['NumOfAmentities'] = df1[amentities].sum(axis=1)\ndf2['NumOfAmentities'] = df2[amentities].sum(axis=1)\ndf3['NumOfAmentities'] = df3[amentities].sum(axis=1)\ndf4['NumOfAmentities'] = df4[amentities].sum(axis=1)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df1.drop(amentities,axis=1)\ndf2 = df2.drop(amentities,axis=1)\ndf3 = df3.drop(amentities,axis=1)\ndf4 = df4.drop(amentities,axis=1)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['Price'] = df1['Price']/100000\ndf2['Price'] = df2['Price']/100000\ndf3['Price'] = df3['Price']/100000\ndf4['Price'] = df4['Price']/100000","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#determine the cutoff for outliers\ndef iqr_fence(x):\n    Q1 = x.quantile(0.25)\n    Q3 = x.quantile(0.75)\n    IQR = Q3 - Q1\n    Lower_Fence = Q1 - (1.5 * IQR)\n    Upper_Fence = Q3 + (1.5 * IQR)\n    u = max(x[x<Upper_Fence])\n    l = min(x[x>Lower_Fence])\n    return [u,l]","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing Outliers\ndf1 = df1[df1['Price'] < iqr_fence(df1['Price'])[0]]\ndf2 = df2[df2['Price'] < iqr_fence(df2['Price'])[0]]\ndf3 = df3[df3['Price'] < iqr_fence(df3['Price'])[0]]\ndf4 = df4[df4['Price'] < iqr_fence(df4['Price'])[0]]","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geolocator = Nominatim(user_agent=\"testing\")\n\ndef geogeneration(df):\n    lat = []\n    long = []\n    t = 0\n    for i in df['Location']:\n        location = geolocator.geocode(i, timeout=None)\n        if t%100 == 0:\n            print(t)\n        t += 1\n        try:\n            lat.append(location.latitude)\n            long.append(location.longitude)\n        except:\n            lat.append(\"NA\")\n            long.append(\"NA\")\n    df['Latitude'] = lat\n    df['Longitude'] = long","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geogeneration(df1)\ngeogeneration(df2)\ngeogeneration(df3)\ngeogeneration(df4)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df1.to_csv('/kaggle/working/Mumbai_updated.csv')\n# df2.to_csv('/kaggle/working/Delhi_updated.csv')\n# df3.to_csv('/kaggle/working/Chennai_updated.csv')\n# df4.to_csv('/kaggle/working/Hyderabad_updated.csv')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.read_csv('../input/mdch-data/Mumbai_updated.csv')\ndf2=pd.read_csv('../input/mdch-data/Delhi_updated.csv')\ndf3=pd.read_csv('../input/mdch-data/Chennai_updated.csv')\ndf4=pd.read_csv('../input/mdch-data/Hyderabad_updated.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping features from each city\ndf1 = df1.drop(['Unnamed: 0', 'Location'],axis=1)\ndf2 = df2.drop(['Unnamed: 0', 'Location'],axis=1)\ndf3 = df3.drop(['Unnamed: 0', 'Location'],axis=1)\ndf4 = df4.drop(['Unnamed: 0', 'Location'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying null values in the data set\ndf1.isna().sum()\ndf2.isna().sum()\ndf3.isna().sum()\ndf4.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropped rows that contained at least 1 null value\ndf1 = df1.dropna()\ndf2 = df2.dropna()\ndf3 = df3.dropna()\ndf4 = df4.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizing the data and fitting the model for Mumbai\nx1 = df1.loc[:, 'Area':'Longitude']\ny1 = df1['Price']\n\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1,test_size = 0.2, random_state=365)\n\nnormalized1 = MinMaxScaler().fit(x_train1)\nn1_train1 = normalized1.transform(x_train1)\nn1_test1 = normalized1.transform(x_test1)\n\n# Fit the regression with the scaled TRAIN inputs and targets\nreg1 = LinearRegression()\nreg1.fit(x_train1,y_train1)\n\ny_pred1 = reg1.predict(x_test1)\n\n# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_test1, y_pred1)\n# Let's also name the axes\nplt.xlabel('Targets (y_test1)',size=18)\nplt.ylabel('Predictions (y_pred1)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# This is an issue as we won't be able to interpret the '45-degree line'\n# We want the x-axis and the y-axis to be the same\nplt.xlim(0,350)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation Metrics\nmae1 = mean_absolute_error(y_test1, y_pred1)\nmse1 = mean_squared_error(y_test1, y_pred1)\nr21 = r2_score(y_test1, y_pred1)\n\nprint('Model performance for testing set')\nprint('----------------------------------')\nprint('Mean Absolute Error is {}'.format(mae1))\nprint('Mean Squared Error is {}'.format(mse1))\nprint('R2 score is {}'.format(r21))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizing the data and fitting the model for Delhi\nx2 = df2.loc[:, 'Area':'Longitude']\ny2 = df2['Price']\n\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2,test_size = 0.2, random_state=0)\n\nnormalized2 = MinMaxScaler().fit(x_train2)\nn2_train2 = normalized2.transform(x_train2)\nn2_test2 = normalized2.transform(x_test2)\n\n# Fit the regression with the scaled TRAIN inputs and targets\nreg2 = LinearRegression()\nreg2.fit(x_train2,y_train2)\n\ny_pred2 = reg2.predict(x_test2)\n\n# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_test2, y_pred2)\n# Let's also name the axes\nplt.xlabel('Targets (y_test)',size=18)\nplt.ylabel('Predictions (y_pred)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# This is an issue as we won't be able to interpret the '45-degree line'\n# We want the x-axis and the y-axis to be the same\nplt.xlim(0,350)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation Metrics\nmae2 = mean_absolute_error(y_test2, y_pred2)\nmse2 = mean_squared_error(y_test2, y_pred2)\nr22 = r2_score(y_test2, y_pred2)\n\nprint('Model performance for testing set')\nprint('----------------------------------')\nprint('Mean Absolute Error is {}'.format(mae2))\nprint('Mean Squared Error is {}'.format(mse2))\nprint('R2 score is {}'.format(r22))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizing the data and fitting the model for Chennai\nx3 = df3.loc[:, 'Area':'Longitude']\ny3 = df3['Price']\n\nx_train3, x_test3, y_train3, y_test3 = train_test_split(x3, y3,test_size = 0.2, random_state=0)\n\nnormalized3 = MinMaxScaler().fit(x_train3)\nn3_train1 = normalized3.transform(x_train3)\nn3_test3 = normalized2.transform(x_test3)\n\n# Fit the regression with the scaled TRAIN inputs and targets\nreg3 = LinearRegression()\nreg3.fit(x_train3,y_train3)\n\ny_pred3 = reg3.predict(x_test3)\n\n# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_test3, y_pred3)\n# Let's also name the axes\nplt.xlabel('Targets (y_test)',size=18)\nplt.ylabel('Predictions (y_pred)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# This is an issue as we won't be able to interpret the '45-degree line'\n# We want the x-axis and the y-axis to be the same\nplt.xlim(0,350)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation Metrics\nmae3 = mean_absolute_error(y_test3, y_pred3)\nmse3 = mean_squared_error(y_test3, y_pred3)\nr23 = r2_score(y_test3, y_pred3)\n\nprint('Model performance for testing set')\nprint('----------------------------------')\nprint('Mean Absolute Error is {}'.format(mae3))\nprint('Mean Squared Error is {}'.format(mse3))\nprint('R2 score is {}'.format(r23))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizing the data and fitting the model for Hyderabad\nx4 = df4.loc[:, 'Area':'Longitude']\ny4 = df4['Price']\n\nx_train4, x_test4, y_train4, y_test4 = train_test_split(x4, y4,test_size = 0.2, random_state=0)\n\nnormalized4 = MinMaxScaler().fit(x_train4)\nn4_train4 = normalized4.transform(x_train4)\nn4_test4 = normalized2.transform(x_test4)\n\n# Fit the regression with the scaled TRAIN inputs and targets\nreg4 = LinearRegression()\nreg4.fit(x_train4,y_train4)\n\ny_pred4 = reg4.predict(x_test4)\n\n# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_test4, y_pred4)\n# Let's also name the axes\nplt.xlabel('Targets (y_test)',size=18)\nplt.ylabel('Predictions (y_pred)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# This is an issue as we won't be able to interpret the '45-degree line'\n# We want the x-axis and the y-axis to be the same\nplt.xlim(0,350)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation Metrics\nmae4 = mean_absolute_error(y_test4, y_pred4)\nmse4 = mean_squared_error(y_test4, y_pred4)\nr24 = r2_score(y_test4, y_pred4)\n\nprint('Model performance for testing set')\nprint('----------------------------------')\nprint('Mean Absolute Error is {}'.format(mae4))\nprint('Mean Squared Error is {}'.format(mse4))\nprint('R2 score is {}'.format(r24))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\npickle.dump(reg1, open('mumbai.pkl', 'wb+'))\npickle.dump(reg2, open('delhi.pkl', 'wb+'))\npickle.dump(reg3, open('chennai.pkl', 'wb+'))\npickle.dump(reg4, open('hyderabad.pkl', 'wb+'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = pickle.load(open('mumbai.pkl', 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = loaded_model.predict(x_test1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}