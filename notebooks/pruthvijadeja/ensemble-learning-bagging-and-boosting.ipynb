{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<H2>Ensemble learning</H2>\n<p>Ensemble learning is a machine learning paradigm where multiple learners are trained to solve the same problem. In contrast to ordinary machine learning approaches which try to learn one hypothesis from training data, ensemble methods try to construct a set of hypotheses and combine them to use.</p>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mnist_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtr = df.iloc[:,1:]\nytr = df.iloc[:,0]\n\nxtst = test.iloc[:,1:]\nytst = test.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree\nclassifier = DecisionTreeClassifier()\nclassifier.fit(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.score(xtr,ytr)  #Overfitting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\npred = classifier.predict(xtst)\naccuracy_score(pred,ytst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensemble\n\n#Random Forest  -  Combination of various Decision tree\nrf = RandomForestClassifier(n_estimators=10)\nrf.fit(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.score(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.score(xtst,ytst)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Bagging</h2>\n<p>Bagging is an abbreviation for \"bootstrap aggregating\". It'a meta-algorithm, which takes M subsamples (with replacement) from the initial dataset and trains the predictive model on those subsamples. The final model is obtained by averaging the \"bootstrapped\" models and usually yields better results.</p>\n\n<p>The main advantage of this technique is that it incorporates the regularization in it and all you need is to choose good parameters for the base algorithms. Averaging the models leads to eliminating (or, at least, improvement) for the unstable models which can be produced from biased data.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bagging\n\nbg = BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5, max_features=1.0, n_estimators=20)\nbg.fit(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bg.score(xtst,ytst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bg.score(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Boosting</h2>\n<p>Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners− models that are only slightly better than random guessing, such as small decision trees− to weighted versions of the data. More weight is given to examples that were misclassified by earlier rounds.\n\nThe predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boosting\nad = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=0.01)\nad.fit(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad.score(xtst,ytst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad.score(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Is boosting more vulnerable to overfitting than bagging?</b>\n<p style=\"padding:5px\">You need to understand that bagging decreases variance, while boosting decreases bias.\n\nAlso, to be noted that under-fitting means that the model has low variance and high bias and vice versa for overfitting.\n\nSo, boosting is more vulnerable to overfitting than bagging.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Voting Classifier - Multiple model ensemble\nlr = LogisticRegression()\ntree = DecisionTreeClassifier()\nsvm = SVC(kernel='poly',degree=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evc = VotingClassifier( estimators= [('lr',lr),('tree',tree),('svm',svm)], voting = 'hard')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evc.fit(xtr.iloc[1:4000],ytr.iloc[1:4000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evc.score(xtst,ytst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}