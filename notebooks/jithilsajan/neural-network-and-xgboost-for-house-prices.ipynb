{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nsc = StandardScaler()\nmx = MinMaxScaler()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/house-price-prediction-challenge/train.csv\")\ntest_data = pd.read_csv(\"../input/house-price-prediction-challenge/test.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_cols = list(train_data.columns[train_data.dtypes == 'object'])\nobj_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the train and test datasets"},{"metadata":{},"cell_type":"markdown","source":"## 1. Plotting graphs of continuous variables and getting rid of outliers\n## 2. Getting the city name from address\n## 3. Converting BHK or RK into categorical variables\n## 4. Cleaning the dataset by imputing the missing values and fixing obj type datatypes into float or int"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.histogram(train_data['TARGET(PRICE_IN_LACS)'], x = 'TARGET(PRICE_IN_LACS)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = px.histogram(train_data['SQUARE_FT'], x = 'SQUARE_FT')\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## we see that there are a lot of outliers which is impacting the graph i.e prices of houses greater than 5k lacs or 50 crores  and square foot values so we remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The below function cleans the dataset and label encodes the categorical columns\ndef data_cleaning(data):\n    for i in range(len(data)):\n        str1 = data['ADDRESS'][i].split(\",\")[-1]\n        data['ADDRESS'][i] = str1\n    encoder = LabelEncoder()\n    #for col in obj_cols:\n        #data[col] = pd.get_dummies(data[col],drop_first=True)\n    for col in obj_cols:\n        data[col] = encoder.fit_transform(data[col].astype(str))\n        \n    data.drop(\"BHK_OR_RK\",axis = 1, inplace = True) # dropping the location co-ordinates as the city name already does the job  \n    data.drop(\"POSTED_BY\", axis = 1, inplace = True)\n    data.drop(\"UNDER_CONSTRUCTION\",axis = 1,inplace = True)\n    data.drop(['LONGITUDE','LATITUDE'],axis = 1,inplace = True)\n    if 'TARGET(PRICE_IN_LACS)' in data.columns:\n        data = data[data['TARGET(PRICE_IN_LACS)']<=99] #dropping houses with prices >5crs as they affect the distribution of data\n        data = data[data['SQUARE_FT']<=2900]\n    data[\"SQUARE_FT\"] = data[\"SQUARE_FT\"].astype(int)\n    return data\ndata_train = data_cleaning(train_data)\ndata_test = data_cleaning(test_data)\ndata_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig1 = px.histogram(data_train['SQUARE_FT'], x = 'SQUARE_FT')\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig1 = px.histogram(data_train['TARGET(PRICE_IN_LACS)'], x = 'TARGET(PRICE_IN_LACS)')\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## *Now from the charts u can see that the dataset is more evenly distributed with considerably less outliers that negatively performance*"},{"metadata":{},"cell_type":"markdown","source":"# **EDA on the cleaned dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting into inputs and targets\n\ninput_data =data_train.iloc[:,:-1]\ntarget_data = data_train.iloc[:,-1]\n#input_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc.fit(input_data)\nscaled_inputs = sc.transform(input_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mx.fit(input_data)\n#scaled_inputs = mx.transform(input_data)  #StandardScaler gives better results\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We'll try different types of learning:-\n1. Linear regression\n2. ElasticNet regression\n3. Xgb boost\n4. Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"## *1.Multiple linear regression*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test split method for splitting the inputs into train and test data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression(normalize=True) #normalizing turned down the rmse even more\ntrain_inputs,test_inputs,train_targets,test_targets = train_test_split(scaled_inputs,target_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(train_inputs,train_targets)\npreds = lr.predict(test_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.DataFrame()\nd['preds']=preds\nd['real_price'] = np.array(test_targets)\nd['%_error'] = np.absolute((preds-np.array(test_targets))/np.array(test_targets))*100\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test_targets, preds))\nprint(\"RMSE: %f\" % (rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## *2. ElasticNet regression*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nelr = ElasticNet(l1_ratio = 0.85,normalize = False,selection = 'random') #l1 ratio of 0.85 gives the best results\nelr.fit(train_inputs,train_targets)\npredse = elr.predict(test_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = pd.DataFrame()\nes['preds']=predse\nes['real_price'] = np.array(test_targets)\nes['%_error'] = np.absolute((predse-np.array(test_targets))/np.array(test_targets))*100\nes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nes.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsme = np.sqrt(mean_squared_error(test_targets,predse))\nprint(\"RSME: {}\".format(rsme))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As you can see the linear regression model is underperforming so we'll try other methods"},{"metadata":{},"cell_type":"markdown","source":"## *3. Using the xgboost regressor*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2 xgboost regressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg.fit(train_inputs,train_targets)\n\npreds2 = xg_reg.predict(test_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ne = pd.DataFrame()\ne['preds2']=preds2\ne['real_price'] = np.array(test_targets)\ne['%_error'] = np.absolute((preds2-np.array(test_targets))/np.array(test_targets))*100\ne","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(test_targets, preds2))\nprint(\"RMSE: %f\" % (rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *4. Using Deep neural networks*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test1 = data_test.copy()\nsc.fit(data_test1)\ndata_test1 = sc.transform(data_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for using neural netowrks you have to save the data either in  npz format or tensorslices\nnp.savez('data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('data_validation', inputs=test_inputs, targets=test_targets)\nnp.savez('data_test', inputs= data_test1)\n#np.savez('data_test', inputs=test_inputs, targets=test_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"npz = np.load('data_train.npz')\ntrain_inputs_tf = npz['inputs'].astype(np.float)\n# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\ntrain_targets_tf = npz['targets'].astype(np.int8)\n\n# we load the validation data in the temporary variable\nnpz = np.load('data_validation.npz')\n# we can load the inputs and the targets in the same line\nvalidation_inputs_tf, validation_targets_tf = npz['inputs'].astype(np.float), npz['targets'].astype(np.int8)\n\nnpz = np.load('data_test.npz')\ntest_inputs_tf = npz['inputs'].astype(np.float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the model\n\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(32, activation='relu'),\n                             tf.keras.layers.Dense(32, activation='relu'),\n                             tf.keras.layers.Dense(1)\n                             ])\nmodel.compile(loss='mse',\n              optimizer='RMSprop',\n              metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n\nmodel.fit(\n          train_inputs_tf, # train inputs\n          train_targets_tf,\n          epochs = 15,\n          callbacks=[early_stopping], # early stopping\n          validation_data=(validation_inputs_tf, validation_targets_tf),\n          verbose = 1\n\n          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_tf = model.predict(validation_inputs_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_tf = preds_tf.reshape(validation_inputs_tf.shape[0],)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_tf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tft = pd.DataFrame()\ntft['preds_tf']=preds_tf\ntft['real_price'] = np.array(test_targets)\ntft['%_error'] = np.absolute((preds_tf-np.array(test_targets))/np.array(test_targets))*100\ntft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tft.sort_values(by=['%_error'], inplace=True,ascending = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tft.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tft.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(test_targets, preds_tf))\nprint(\"RMSE: %f\" % (rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Since the tensorflow model had the lowest RSME and mean error I choose the neural network model for test predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_inputs_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answers = test_preds.reshape(68720,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answers_r = np.round(answers,1) \nanswers_r\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission= pd.read_csv('../input/house-price-prediction-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_submission.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['TARGET(PRICE_IN_LACS)'] = answers_r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}