{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2> Welcome </h2>\n\nI try to make prediction price, data cleaning and feature engineering in this kenel. if you catch my error, please warn me :)\n\n**Prediction Result;**\n\n**Root Mean Squered Error : 28 Â± 1** \n\n\nIf you like it and want to support, please vote. This will increase my motivation :)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#load fundamental libraary\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #visualization\nimport seaborn as sns #visualization\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\ndataset = pd.read_csv(\"../input/listings.csv\")\ndataset2 = pd.read_csv('../input/listings_summary.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, We oveview datasets and get significant features according to us. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get significant features for according to me from this dataset and show 1 example\nimportant_features = [\"room_type\",\"reviews_per_month\",\"number_of_reviews\",\"minimum_nights\",\"calculated_host_listings_count\",\"availability_365\",\"longitude\",\"latitude\",\"price\"]\ndataset = dataset[important_features]\ndataset.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at nulls\nfig, ax = plt.subplots(figsize = (9, 5))\nsns.heatmap(dataset.isnull(), cmap = \"cubehelix_r\", yticklabels='')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **There are a lot of nan values in reviews_per_month. It is more meaningful than number_of_reviews a bit but There aren't a lot of diffirence. I will delete reviews_per_month.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"del dataset[\"reviews_per_month\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nNow, There is no null data in listining.csv. It is good news for us :)\n\nLet's take a look dataset2."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset2.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at nulls\nfig, ax = plt.subplots(figsize = (20, 5))\nsns.heatmap(dataset2.isna(), cmap = \"cubehelix_r\", yticklabels='')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **There are too many null values in some feature. We can ignore feature that contains too many nulls**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ignore features that contain more than %30 \ncols = dataset2.columns.values\nlenght = dataset2.shape[0]\ncol_list = [] \nfor i in cols:\n    x = dataset2[i]\n    rate = round(x.isna().sum()/lenght,3)\n    if rate < 0.30:\n        col_list.append(i)\n        print(i,\"contain :%\",100*rate ,\" ok :)\")\n    else:\n        print(i,\"contain :%\",rate*100 ,\" fail !!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selectable features\nprint(\"Selectable feature count : \",len(col_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get important feature\nget_features = [\"cancellation_policy\",\"amenities\",\"availability_365\",\"requires_license\",\"instant_bookable\",\n                \"guests_included\",\"extra_people\",\"review_scores_rating\",\"require_guest_phone_verification\",\n               \"bathrooms\",\"bedrooms\",\"beds\",\"bed_type\",\"accommodates\",\"host_total_listings_count\",\"host_has_profile_pic\",\n               \"host_identity_verified\",\"is_location_exact\",\"property_type\",\"host_is_superhost\",\"room_type\",\"maximum_nights\",\"minimum_nights\"]\ndataset2 = dataset2[get_features]\ndataset2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>What We learn from info ?</h4>\n\n* We have 22552 sample. \n\nFeature contains null;\n\n <p>**#####** => Contains too much Nan values</p>\n <p>**#**     => Contains too few Nan values</p>\n\n* review_scores_rating #####\n* bathrooms #\n* bedrooms #\n* beds #\n* host_has_profile_pic #\n* host_identity_verified #\n* host_total_listings_count #\n\n\n\n\n* **We need handle this feature. If feature contains too low nan values, I fill it with majority values of feature. Else I  will using diffirent teqniques.**\n\n* **We can create new features from exist features. Example; We can create distance from latitude and longtitude. This feature response this question. \"How far is Home/Flat to Berlin ?\"** \n\nLet's Start!!"},{"metadata":{},"cell_type":"markdown","source":"<h3>Working on listings.csv dataset</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"_dataset = dataset.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create distance feature from latitude and longtitude \nfrom geopy.distance import great_circle\ndef distance_from_berlin(lat, lon):\n    berlin_centre = (52.5027778, 13.404166666666667)\n    record = (lat, lon)\n    return great_circle(berlin_centre, record).km\n\n#add distanse dataset\n_dataset['distance'] = _dataset.apply(lambda x: distance_from_berlin(x.latitude, x.longitude), axis=1)\n\ndel _dataset['latitude']\ndel _dataset['longitude']\n_dataset.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nominal_categorical\nprint(_dataset.room_type.unique())\nx = _dataset[[\"room_type\"]]\nx.room_type = pd.Categorical(x['room_type'])\ndel _dataset['room_type']\ndummies_room_type = pd.get_dummies(x, prefix = 'room_type')\n_dataset = pd.concat([_dataset,dummies_room_type], axis=1)\n_dataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Working on listings_summary dataset</h2> "},{"metadata":{"trusted":true},"cell_type":"code","source":"_dataset2 = dataset2.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_dataset2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del same features\ncol_list2 = list(_dataset.columns.values)\ncol_list2.append(\"room_type\")\nfor i in col_list2:\n    if i in _dataset2.columns.values:\n        print(\"deleted \",i,\" feature!\")\n        del _dataset2[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert string true and false to numeric\ndef object2bool(x):\n    if x==\"t\" or x==\"T\":\n        return 1.0\n    elif x==\"f\" or x==\"F\":\n        return 0.0\n    else:\n        return None\n\nboolean_features = [\"require_guest_phone_verification\",\"host_is_superhost\",\"host_has_profile_pic\",\n                    \"host_identity_verified\",\"is_location_exact\",\"requires_license\",\"instant_bookable\"]\n\nfor i in boolean_features:\n    _dataset2[i] = _dataset2[i].map(object2bool)\n\n_dataset2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nominal_categorical bed_type and property_type\nfor i in [\"bed_type\",\"property_type\",\"cancellation_policy\"]:\n    x = _dataset2[[i]]\n    x.room_type = pd.Categorical(x[i])\n    del _dataset2[i]\n    dummies = pd.get_dummies(x, prefix = i)\n    _dataset2 = pd.concat([_dataset2,dummies], axis=1)\n\n_dataset2.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#amenities count\ndef amenities_counter(x):\n    return len(x.split(\",\"))\n\n_dataset2.amenities = _dataset2.amenities.map(amenities_counter)\n_dataset2.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean $ sign\ndef clean_price(x):\n    return float(x.replace(\"$\",\"\"))\n\n_dataset2.extra_people = _dataset2.extra_people.map(clean_price)\n_dataset2.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_dataset2 = _dataset2.astype(float)\n_dataset = _dataset.astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **We should fill nan values. I will fill nan values with median.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get features that contain nulls\nnull_cols = []\nfor i in _dataset2.columns.values:\n    if _dataset2[i].isna().sum()>0:\n        null_cols.append(i)\n\nprint(\"Features that contain nulls;  \\n\\n\",null_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill nulls with median\nfor i in null_cols:\n    median = _dataset2[i].median()\n    _dataset2[i] = _dataset2[i].fillna(median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataset2 null count :\",_dataset2.isna().any().sum())\nprint(\"Dataset null count :\",_dataset.isna().any().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>OK :) There is no Null data and all feature transformed to numeric.</h3>\n\nNow, We can concat dataset and dataset2."},{"metadata":{"trusted":true},"cell_type":"code","source":"#concat\ndata = pd.concat([_dataset,_dataset2],axis=1)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Clean Outlier with Z-score</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier detection with z score\ndef detect_outlier(data_1):\n    feature_outliers=[]\n    threshold=7\n    mean_1 = np.mean(data_1)\n    std_1 =np.std(data_1)\n    \n    counter=0\n    for y in data_1:\n        z_score= (y - mean_1)/std_1 \n        if np.abs(z_score) > threshold:\n            feature_outliers.append(counter)\n        counter += 1\n    return feature_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = np.array([])\nprint(\"--Feature and Outlier Counts--\\n\")\nfor i in data.columns:\n    f_out = detect_outlier(data[i])\n    outliers = np.concatenate((outliers,np.asarray(f_out)))\n    print(i ,\" outlier count :\",len(f_out))\n                              \noutliers = np.unique(outliers,0)\nprint(\"Total Unique Outlier Index Count:\",len(outliers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data = data.drop(outliers,axis=0)\nclean_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots( figsize=(35,35) )\nsns.heatmap(data.corr(),annot=True,linewidths=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Nice correalation matrix. There is no too much coraleted feature. **"},{"metadata":{},"cell_type":"markdown","source":"<h2>Modeling</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data test and train\nfrom sklearn.model_selection import train_test_split\n\nX = clean_data.drop([\"price\"],axis=1)\ny = clean_data[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize data\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range=(0, 1))\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.metrics import mean_squared_error\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks.callbacks import ModelCheckpoint\n\n#set parameters\nbatch_size = 32\nepochs = 25\n\nmodel = Sequential()\nmodel.add(Dense(512,input_shape=(data.shape[1]-1,), activation= 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation= 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation= 'relu'))\nmodel.add(Dense(1, activation= 'relu'))\n\nfilepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n\nmodel.compile(loss = mean_squared_error,\n              optimizer = Adam(),\n              metrics=['mean_squared_error'])\n\nhist = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test),callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot training history\nfigsize=(10,5)\nax,_ = plt.subplots(figsize=figsize)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('modelloss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'],loc='upperleft')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport copy\n\nsaved_loss_file = None\nloss_min = 100000\n# r=root, d=directories, f = files\nfor r, d, f in os.walk('./'):\n    for file in f:\n        if '.hdf5' in file:\n            filename = os.path.join(r, file)\n            loss = int(filename.split('-')[3].split('.')[0])\n            if loss < loss_min:\n                loss_min = loss\n                saved_loss_file = filename\n                \nprint(\"Saved min loss : \",loss_min,\"\\nFile :\",saved_loss_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load model\nfrom keras.models import load_model\nbest_model = load_model(saved_loss_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction\npred_mlp = best_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate\nprint('\\n# Model Evaluate')\nresults = best_model.evaluate(X_test, y_test)\nprint('Test Mse Loss:', results[0])\n\nRMSE = np.sqrt(results[0])\nprint(\"RMSE:\",RMSE)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks for read this kernal :) \n\nIf you have suggestion for me, please write comment. I want to improve myself.\n\nSee You :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}