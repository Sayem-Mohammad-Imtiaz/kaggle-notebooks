{"cells":[{"metadata":{"_uuid":"c65e1921d3461491a0a977284151674b0c6214fd"},"cell_type":"markdown","source":"# Data/Text mining course project\n### Twitter sentiment classification\nBy Lukas LÃ¶nnroth & Wille Strengell"},{"metadata":{"_uuid":"6787d31003546d60aa77cc998ecc2ecd989fc417"},"cell_type":"markdown","source":"This is our submission for the course project.\n\nIn this project we will build a nltk classifier using the naive bayes algorithm inspired by this [kernel](https://www.kaggle.com/ngyptr/python-nltk-sentiment-analysis), that we train with the [First GOP Debate Twitter Sentiment](https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment) dataset.\n\nAfter that we will apply it to our own data that we have gathered from twitter and output a file that has a column that shows the sentiment of each tweet."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7962711b17a60c227b9365d138642ea6e14415c"},"cell_type":"markdown","source":"### Praparing our training data\nWe will use the data from the Sentiment.csv file to train our classifier so we can use it on our own data. So we read the file with pandas and get the columns we need (sentiment and text)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"sentiment = pd.read_csv('../input/first-gop-debate-twitter-sentiment/Sentiment.csv')\ntraining_data = sentiment[['sentiment', 'text']]\ntraining_data = training_data[training_data.sentiment != \"Neutral\"]\ntraining_data = training_data[:100]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4d445816e765e8302764cc7b03b82abc85ad84a"},"cell_type":"markdown","source":"Then we need to remove stop words from our data, we can use the nltk stopwords corpus for this"},{"metadata":{"trusted":true,"_uuid":"0a84f101cee704b5fa434672d7e05a8535057d83"},"cell_type":"code","source":"tweets = []\nstopwords_set = set(stopwords.words(\"english\"))\n\nfor index, row in training_data.iterrows():\n    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'RT']\n    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n    tweets.append((words_without_stopwords, row.sentiment))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training our classifier"},{"metadata":{},"cell_type":"markdown","source":"Extracting our word features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_words_in_tweets(tweets):\n    all = []\n    for (words, sentiment) in tweets:\n        all.extend(words)\n    return all\n\ndef get_word_features(wordlist):\n    wordlist = nltk.FreqDist(wordlist)\n    features = wordlist.keys()\n    return features\nw_features = get_word_features(get_words_in_tweets(tweets))\n\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in w_features:\n        features['contains(%s)' % word] = (word in document_words)\n    return features\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training our classifier, as an example we only use 100 tweets that we split in half for training and testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresets = nltk.classify.apply_features(extract_features,tweets)\nlen(featuresets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = featuresets[50:]\ntest_set = featuresets[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_classifier = nltk.NaiveBayesClassifier.train(training_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nltk.classify.accuracy(naive_classifier, test_set))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gathering our own data\n\nWe have gathered our own data from twitter using a python script and tweepy. In the ```brexit-26-march.csv``` there is about 60 000 tweets with the #brexit, most of them where tweeted on the 26th of march 2019\n\n##### The script:\nOur script for getting the tweets. You run the script from the cli and enter your hashtag and filename as arguments and it will output a .csv file containing 2 weeks worth of tweets for your hashtag.\n```python\n\nif len(sys.argv) == 1:\n    print('###ERROR: Please enter required arguments: filename, hashtag')\n    exit()\n\n# sys args\nfileName = sys.argv[1]\nhashtag = '#' + sys.argv[2]\n\nprint('Filename: ' + fileName)\nprint('Hashtag: ' + hashtag)\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth,wait_on_rate_limit=True)\n\n\n# Open/Create a file to append data\ncsvFile = open(fileName+ '.csv', 'a')\n#Use csv Writer\ncsvWriter = csv.writer(csvFile)\n\nfor tweet in tweepy.Cursor(api.search,q=hashtag,tweet_mode='extended',\n                           count=100,\n                           lang=\"en\",\n                           since=\"2017-04-03\").items():\n    if 'retweeted_status' in dir(tweet):\n        text = tweet.retweeted_status.full_text\n    else:\n        text = tweet.full_text\n\n    print (tweet.created_at, text)\n    csvWriter.writerow([tweet.created_at, text])\n\n\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"brexit_data = pd.read_csv('../input/brexit-tweets/brexit-26-march.csv', header=None)\nbrexit_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at what our classifier can tell us about our data before we clean it, using only the first 100 tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tweets = []\nneg_tweets = []\npos_cnt = 0\nneg_cnt = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smaller_data_with_stopwords = brexit_data[:100]\n\nfor obj in smaller_data_with_stopwords[1]: \n    res =  naive_classifier.classify(extract_features(obj.split()))\n    if(res == 'Negative'): \n        neg_tweets.append(obj)\n        neg_cnt += 1\n    elif(res == 'Positive'): \n        pos_tweets.append(obj)\n        pos_cnt += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guess it looks alright, it seems plausible that #brexit tweets might be more on the negative side"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('positive tweets: %s' %pos_cnt)\nprint('negative tweets: %s' %neg_cnt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright so lets remove the stopwords from our brexit data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in brexit_data.iterrows():\n    words_filtered = [e.lower() for e in row[1].split() if len(e) >= 3]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'RT']\n    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n    row[1] = ' '.join(words_without_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brexit_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can add a sentiment column so we can use our data easier in the future. Even though we have 60 000 tweets we will now only use 5000 since it would take so much time to classify the whole set."},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify_data(data, classifier):\n    pos_tweets = []\n    neg_tweets = []\n    pos_cnt = 0\n    neg_cnt = 0\n    data.insert(2, 3, '')\n\n    for index, row in data.iterrows():\n        obj = row[1]\n        res =  classifier.classify(extract_features(obj.split()))\n        row[3] = res\n        if(res == 'Negative'): \n            neg_tweets.append(obj)\n            neg_cnt += 1\n        elif(res == 'Positive'): \n            pos_tweets.append(obj)\n            pos_cnt += 1\n    return pos_tweets, neg_tweets, pos_cnt, neg_cnt, data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = brexit_data[:2000]\npos_tweets, neg_tweets, pos_cnt, neg_cnt, classified_data = classify_data(data, naive_classifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There it is!\nWe have now classified our tweets and can output them to a separate file for future use:"},{"metadata":{"trusted":true},"cell_type":"code","source":"classified_data.to_csv('classified_brexit_tweets.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyzing our data\nNow we can take a look at some of the mostly used words in the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Positive tweets: %s' %pos_cnt)\nprint('Negative tweets: %s' %neg_cnt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(background_color=\"white\", width=2500, height=2000).generate(' '.join(pos_tweets))\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Positive words in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width=2500, height=2000).generate(' '.join(neg_tweets))\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Negative words in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trying it with the decision tree classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_classifier = nltk.classify.DecisionTreeClassifier.train(training_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nltk.classify.accuracy(dtree_classifier, test_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = brexit_data[:1000]\npos_tweets, neg_tweets, pos_cnt, neg_cnt, dtree_classified_data = classify_data(data, dtree_classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Positive tweets: %s' %pos_cnt)\nprint('Negative tweets: %s' %neg_cnt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(background_color=\"white\", width=2500, height=2000).generate(' '.join(pos_tweets))\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Positive words in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width=2500, height=2000).generate(' '.join(neg_tweets))\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Negative words in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nIn this project we:\n- Prepared our training data\n- Trained our classifiers\n- Gathered our own twitter data\n- Analyzed the sentiment of our gathered data\n- Got a new output with analyzed tweets\n- We have learned a bit more about using some classifiers provided by NLTK.\n\nWe could have spent some more time investigating different ways to make our classifiers more accurate and learning more about them. Now it seems that if we put more training data in the desicion-tree classifiers it hangs and can go on forever.\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}