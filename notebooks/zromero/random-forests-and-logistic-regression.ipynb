{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Basic Data analysis and munging (if needed)\nimport pandas as pd\nimport numpy as np\n\n#sklearn and all the things I used.\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\n#Seaborn/matplotlib to visualize stuff.\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n#Changing these settings allows us to view the entirety of the collumns avoiding that \"...\" in the middle of the df prints.\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n#Data Set being used is the Mushroom Data set from UCI ML\n#Found here https://www.kaggle.com/uciml/mushroom-classification\n\ndf = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quickly Checking Data to see what I'm going to work with (it is all categorical)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I want all of these non-null object Types to be integers so the Random Forests and Logistic Regression models can actually work with the Data.\nSo I used Label Encoding from sklearn.preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\n\nfor column in df:\n    df[column] = le.fit_transform(df[column])\n    \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now looking at this data, everything is numerical classes are 0 or 1 Poisonous or safe, there are either bruises or no bruises, the higher the numbers we see here the more possible answers there are to a given question(for the random forest model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Description is nice to see the max, which NOW for our purposes shows the total number of unique values, our unique features can have.\nLike I mentioned above, the \"class\" max is 1 (1 or 0) Cap-colo has 9 unique inputs for our a Random forest to work through.\n\nNo I'll get the data ready to actually be worked with, creating a X dataset and their y features dataset. Dropping duplicates before hand, with a total of 8124 total entities I figured a 30% Train Test Split was ok(?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I Create an X variable here purely because it's easier for me to recall what I was doing when I go back and look at this.\n# When it comes to classification in datasets when we split this dataset having duplicates will likely lead to an \n# Unreasonably high accuracy.\n# We can create our X and remove any duplicates if there were any.\nX = df.drop_duplicates(keep='first')\n\n#the classification \"feature\" and our final Dataset to Split\ny = X['class']\nX = X.drop(['class'], axis=1)\n#I used a random state to test the random forest classifier parameters.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the code I used to create a few different types of Random Forest models, the number of Trees as well as the Depth of the tree is altered in each test, Depth increments by 1 until 9 and Tree count increments by 5 each time. \n\nThe criterion remained \"gini\" since I had tested Entropy on this dataset before and it was performing worse.\nI had a random_state here to get the same results as I was testing things out saving the confusion matrix from each models predictions etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_estimators is the number of trees we will use\n# Max_depth determins the amount of features to be used in classification\n# Once again using a random state to keep the same sets, I could alter criterion as well.\n# The random Forest\nforest_trials = []\ntree_count = [x*5 for x in range(21) if x*5 >= 5]\nfor n in tree_count:\n    for depth in range(1,10):\n        forest = RandomForestClassifier(n_estimators=n, max_depth=depth , random_state=50, criterion='gini')  \n        forest.fit(X_train, y_train)\n        forest_trials.append({'Depth':depth,'Trees':n, 'cfm':confusion_matrix(y_test, forest.predict(X_test))})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just Checking to see if I was appending the data in a way I can work with.\nprint(forest_trials[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting the Percentage of correct predictions, Tree count and Depth for each model tested, storing it in a dictionary that can be used by Seaborn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creat X/Y to plot to show the accuracy using True Positive + True Negative / total predictions.\n\ntp_fp = {'Accuracy':[],'Trees':[], 'Depth':[]}\n\nfor trial in forest_trials:\n    true_negative = trial['cfm'][0][0]\n    false_positive = trial['cfm'][0][1]\n    false_negative = trial['cfm'][1][0]\n    true_positive = trial['cfm'][1][1]\n    summed = true_positive+true_negative+false_positive+false_negative\n    tp_fp['Trees'].append(trial['Trees'])\n    tp_fp['Depth'].append(trial['Depth']) \n    tp_fp['Accuracy'].append(round(((true_positive+true_negative)/summed),2)*100)\n\n\ntp_fp['Accuracy'] = tuple(tp_fp['Accuracy'])\ntp_fp['Trees']= tuple(tp_fp['Trees'])\ntp_fp['Depth'] = tuple(tp_fp['Depth'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initially I tried a basica lineplot of all of our Models, There this graph is showing all of the Tree amounts as well as the Depth levels, There is one glaring issue, the Depth levels beging to overlap A LOT which makes this really messy to try and read for me. The best I could say is that anything past 3 depth seems likely unnecessary and would be computationally inefficient. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\nax = sns.lineplot(x='Trees', y=\"Accuracy\", hue=\"Depth\", legend='brief', data=tp_fp)\nax.set(xlabel='Number of Trees', ylabel='Accuracy', title=\"Tree Count vs Depth\")\nax.legend(ncol=3,fontsize= 'large', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,title=\"Tree Depth\", labels=\"123456789\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Next graph I tried is a scatterplot, this one was easier for me to read as the overlapping was resolved (There is only one Tree amount testing at one depth, my y does not indicate Accuracy anymore, in fact Accuracy is indicated by the Hue and Size of our plotted points (See legen)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\n\nplt.show()\nsns.set(style=\"ticks\")\n\nax = sns.scatterplot(x=\"Trees\", y=\"Depth\", hue='Accuracy', palette='muted', legend=\"brief\", data=tp_fp)\nax.set(xlabel='Number of Trees', ylabel='Depth', title=\"Tree VS Depth Prediction Improvements\")\nax.legend(ncol=3,fontsize= 'large',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,title=\"Correct Predictions (%)\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's quite a bit easier for me to see here, that when we are at 4 depth any tree count past 30 is a waste and is likely going to lead to a larger amount time used to create the trees. When increasing the number of trees, while it does affect the accuracy a decent amount it seems to have a less signifigant impact on giving improper predictions. Higher Tree count AND higher tree depth definitely shows very high  successful predictions (often flawless)."},{"metadata":{},"cell_type":"markdown","source":"Moving on to Logistic Regression, I gathered all of the available solver types as welll as the available Regularization parameters available in the sklearn LogisticRegression module, ran the tests using a max iteration of 50k fit the data, predicted the data and made a scatterplot of it similar to the Random Forests above."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfinalVals= []\nsolvers = {\"newton-cg\":[\"l2\",'none'], \"lbfgs\":[\"l2\", 'none'], \"liblinear\":[\"l2\",\"l1\"], \"sag\":[\"l2\", 'none'], \"saga\":[\"l2\",\"l1\", 'none']}\nfor solver in solvers:\n    for penalty in range(len(solvers[solver])):\n        model = LogisticRegression(max_iter = 50000, solver=solver,penalty=solvers[solver][penalty])\n        model.fit(X_train, y_train)\n        prediction = model.predict(X_test)\n        cfm = confusion_matrix(y_test, prediction)\n        true_negative = cfm[0][0]\n        false_positive = cfm[0][1]\n        false_negative = cfm[1][0]\n        true_positive = cfm[1][1]\n        total_correct= round(((true_negative+true_positive) / (true_negative+true_positive+false_positive+false_negative)),2)\n        finalVals.append([solver, solvers[solver][penalty], total_correct])\n  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This Graph shows some of what I 've known, having no regularization does increase your percentage of correct predictions, but it's likely to not be the most effective model and may be overfitting. Adding  any type of regularization penalty didn't seem to have any drastic effect on any of our models. (Some Solvers are not compatible with some regularization penalties so dots will be missing for those.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"graph = {\"solver\":[],\"penalty\":[],\"accuracy\":[]}\nfor each_pred in finalVals:\n    graph[\"solver\"].append(each_pred[0])\n    graph[\"penalty\"].append(each_pred[1])\n    graph[\"accuracy\"].append(each_pred[2])\n    \nax = sns.scatterplot(x=\"solver\", y=\"accuracy\", hue='penalty', palette='muted', legend=\"brief\", data=graph)\nax.set(xlabel='Solver', ylabel='Accuracy')\nax.legend(ncol=1,fontsize= 'large',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, title=\"Regularization Type (Penalty)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This little learning \"thing\" I did has got me considering a larger scale approach to this, having seen these results I wonder if it would be more beneficial to not use a specified random_state and create more than one of each model type, averaging their total predictions may give me a better representation than just predicting the same 1 data set on the same 1 model, just once and plotting it. The same could be done with the random forest testing above, I used a random state to give me some control over the testing when I began working with the output data a bit. \n\nThere was also a 'elastic' penalty that sag could use, but I ran into a couple errors with the iterative process so I have it out for the moment while I figure out how to correct that error."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}