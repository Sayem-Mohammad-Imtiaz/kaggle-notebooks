{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\nThe main objective is to design a Machine Learning solution which predicts the chance of having heart attack based Personal health infomration. \n\nOur ML solution will help in answering the following questions,\n\n1. How to use Data analysis techniques in understanding Heart Attack data?\n2. What are the most significant risk factors for developing a Heart Attack?\n3. How to Identify the person who is having low/high chance of Heart Attack?\n\n\n","metadata":{}},{"cell_type":"code","source":"# Import Libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas_profiling import ProfileReport\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_recall_curve\n\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.evaluate import bias_variance_decomp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# set the pandas dataframe option to display max number of columns\npd.set_option(\"display.max_columns\", 101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Preparation\n#### 1.1 Read the Heart Attack data from the heart csv file","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# read the data feom heart file\nheart_attack_df = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\n\nprint('The shape of the heart_attack_df is : ', heart_attack_df.shape)\nprint('\\n Display of the first 5 rows of the data \\n')\nprint(heart_attack_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 Data Description \nColumn Name\t| Description\n:-|:-\n**age**| Age of the patient\n**sex**| Sex of the patient\n**cp**| Chest pain type ~ 0 = Typical Angina, 1 = Atypical Angina, 2 = Non anginal Pain, 3 = Asymptomatic\n**trtbps**| Resting blood pressure (in mm Hg)\n**chol**| Cholestoral in mg/dl fetched via BMI sensor\n**fbs**| (fasting blood sugar > 120 mg/dl) ~ 1 = True, 0 = False\n**restecg**| Resting electrocardiographic results ~ 0 = Normal, 1 = ST**|T wave normality, 2 = Left ventricular hypertrophy\n**thalachh**| Maximum heart rate achieved\n**oldpeak**| Previous peak\n**slp**| Slope\n**caa**| Number of major vessels\n**thall**| Thalium Stress Test result ~ (0,3)\n**exng**| Exercise induced angina ~ 1 = Yes, 0 = No\n**output**| Target variable (0= less chance of heart attack 1= more chance of heart attack)","metadata":{}},{"cell_type":"code","source":"# repalce acronyms with Full text/ Human friendly text\nheart_attack_df.rename(columns = {'cp':'chest_pain_type', 'trtbps':'resting_blood_pressure',\n                                  'chol':'cholestoral', 'fbs':'fasting_blood_sugar',\n                                  'restecg': 'resting_ecg_results',\n                                  'thalachh': 'maximum_heart_rate_achieved',\n                                  'oldpeak': 'previous_peak',\n                                  'slp': 'slope', \n                                  'caa': 'number_of_major_vessels',\n                                  'thall': 'thalium_stress_test_result',\n                                  'exng': 'exercise_induced_angina'}, inplace = True)\n\n# Generate descriptive statistics\nheart_attack_df.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore null values counts and column types\nheart_attack_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Data Profiling\nThe main advantage of pandas profiling is its use with the datasets in performing the descriptive statistics. \n\nFor each column the following statistics will be saved in an interactive HTML report:\n\nType inference,Essentials: type, unique values, missing values, \nQuantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\nDescriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, \ncoefficient of variation, kurtosis, skewness, Most frequent values, Histogram\nCorrelations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\nMissing values matrix, count, heatmap and dendrogram of missing values\nText analysis,File and Image analysis.","metadata":{}},{"cell_type":"code","source":"# Create the pandas profiling report\nProfileReport(heart_attack_df).to_widgets()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1 Missing Values Treatment\n\nThere are no missing values in the Heart Attack data. Missing Values Treatment isn't required.\n\n#### 2.2 Dropping duplicated rows\n\nDuplicate records wouldn't give us much information. So we can drop them from the Analysis. ","metadata":{}},{"cell_type":"code","source":"print('Heart data dataframe shape before removing the duplicates: ',heart_attack_df.shape)\nheart_attack_df.drop_duplicates(inplace=True) \nheart_attack_df.reset_index(drop=True, inplace=True)\nprint('Heart data dataframe shape after removing the duplicates: ',heart_attack_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3 Outlier Treatment\n\nData profiling visualizations help us in detecting outliers. Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR can be considered as a Outlier. Other methods will be looking at the range of 5th and 95th percentile can be considered as outlier. \n\n\nHere the dataset is too small and also we are uncertain to simply apply detection techniques to identify and replace/remove the records. Thus the reason, we aren't performing the Outlier Treatment.\n\n#### 2.4 Bi-Variate Analysis\n\nBi-Variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables.","metadata":{}},{"cell_type":"code","source":"# Define the Continues and Categorical Column names\nconti_cols = [i for i in heart_attack_df.columns if heart_attack_df[i].nunique()>5]\ncat_cols = [i for i in heart_attack_df.columns if heart_attack_df[i].nunique()<=5]\n\ncat_cols = list(set(cat_cols) - set(['output']))\nprint('Continues Column names: ', conti_cols)\nprint('\\nCategorical Column names: ', cat_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4.1 Plot Categorical Columns vs Output(target variable) distributions using Countplot ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,30)).patch.set_facecolor(\"#e6f7ff\") \nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\n\nplt.subplot(4,2,1)\nplt.title('Prevalence of Heart attack by Sex',fontsize=15)\nsns.countplot(heart_attack_df['output'], hue=heart_attack_df['sex'])\n\nplt.subplot(4,2,2)\nplt.title('Prevalence of Heart attack by Chest Pain',fontsize=15)\nsns.countplot(heart_attack_df['output'], hue=heart_attack_df['chest_pain_type'])\n\nplt.subplot(4,2,3)\nplt.title('Prevalence of Heart attack by fasting blood sugar > 120 mg/dl',fontsize=15)\nsns.countplot(heart_attack_df['output'],hue=heart_attack_df['fasting_blood_sugar'])\n\nplt.subplot(4,2,4)\nplt.title('Prevalence of Heart attack by restecg',fontsize=15)\nsns.countplot(heart_attack_df['output'],hue = heart_attack_df['resting_ecg_results'])\n\nplt.subplot(4,2,5)\nplt.title('Prevalence of Heart attack by Exercise induced angina',fontsize=15)\nsns.countplot(heart_attack_df['output'],hue=heart_attack_df['exercise_induced_angina'])\n\nplt.subplot(4,2,6)\nplt.title('Prevalence of Heart attack by slp',fontsize=15)\nsns.countplot(heart_attack_df['output'],hue=heart_attack_df['slope'])\n\nplt.subplot(4,2,7)\nplt.title('Prevalence of Heart attack by number of major vessels',fontsize=15)\nsns.countplot(heart_attack_df['output'],hue=heart_attack_df['number_of_major_vessels'])\n\nplt.subplot(4,2,8)\nplt.title('Prevalence of Heart attack by thall',fontsize=15)\nsns.countplot(heart_attack_df['output'],hue=heart_attack_df['thalium_stress_test_result'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4.2 Plot Continuous Columns vs Output(target variable) Histograms using Histplot ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,30)).patch.set_facecolor(\"#e6f7ff\") \nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\n\nplt.subplot(3,2,1)\nplt.title('Prevalence of Heart attack by age',fontsize=15)\nsns.histplot(x = heart_attack_df['age'], hue = heart_attack_df['output'])\n\nplt.subplot(3,2,2)\nplt.title('Prevalence of Heart attack by Maximum heart rate achieved',fontsize=15)\nsns.histplot(x = heart_attack_df['maximum_heart_rate_achieved'], hue = heart_attack_df['output'])\n\nplt.subplot(3,2,3)\nplt.title('Prevalence of Heart attack by cholestoral in mg/dl',fontsize=15)\nsns.histplot(x = heart_attack_df['cholestoral'], hue = heart_attack_df['output'])\n\nplt.subplot(3,2,4)\nplt.title('Prevalence of Heart attack by previous peak',fontsize=15)\nsns.histplot(x = heart_attack_df['previous_peak'],hue = heart_attack_df['output'])\n\nplt.subplot(3,2,5)\nplt.title('Prevalence of Heart attack by resting blood pressure',fontsize=15)\nsns.histplot(x = heart_attack_df['resting_blood_pressure'],hue = heart_attack_df['output'])\n\nconti_cols:  ['age', 'resting_blood_pressure', 'cholestoral', 'maximum_heart_rate_achieved', '']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.5 Variable Selection and Transformation","metadata":{}},{"cell_type":"code","source":"x = heart_attack_df.drop(\"output\",axis=1).values\ny = heart_attack_df[\"output\"].values\n\n# Spliting the data for model development\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"StandardScaler removes the mean and scales each feature/variable to unit variance. This operation is performed feature-wise in an independent way.","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Model Development\n#### 3.1 Comaparing the Models\nA list of classifiers for Comparision. \n\n* Linear Models : LogisticRegression\n* Nearest Neighbors : KNeighborsClassifier\n* Support Vector Machines : SVC\n* Naive Bayes: GaussianNB\n* Decision Trees : DecisionTreeClassifier\n* Ensemble and Boost methods: \n                              - RandomForestClassifier\n                              - GradientBoostingClassifier\n                              - AdaBoostClassifier\n                              - XGBClassifier\n                              - LGBMClassifier \n\n* neural_network : MLPClassifier\n\nMetrics for evaluation: Classification models accuracy can be measured using metrics like Accuracy, Area under curve-RUC, Precision, Recall, F-1 score, Confusion matrix. Compared to all the metrics, F-1 tell us model performance more effectively. During the development, having a single metric speeds up the ability to make a decision when we are selecting among a large number of classifiers. \n\nBias - Variance tradeoff: Measure the bias and variance, Tune the model to minimize bias and variance. \n\n","metadata":{}},{"cell_type":"code","source":"# Create the Dict with Model names and classifier functions \nmodel_names = ['LogisticRegression','KNeighborsClassifier',\n       'SVC','GaussianNB',\n       'DecisionTreeClassifier','RandomForestClassifier',\n       'GradientBoostingClassifier','AdaBoostClassifier',\n       'XGBClassifier','LGBMClassifier',\n              'MLPClassifier']\n\nmodel_functions = [LogisticRegression(),KNeighborsClassifier(),\n                   SVC(),GaussianNB(),\n                   DecisionTreeClassifier(),RandomForestClassifier(),\n                   GradientBoostingClassifier(),AdaBoostClassifier(),\n                   XGBClassifier(),LGBMClassifier(),MLPClassifier()]\n\nmodels = dict(zip(model_names,model_functions))\n\n\npredicted_f1_scores =[]\nfor name,algo in models.items():\n    model=algo\n    model.fit(x_train,y_train)\n    predict = model.predict(x_test)\n    acc_score = accuracy_score(y_test, predict)\n    c_matrix = confusion_matrix(y_test, predict)\n    prec_score = precision_score(y_test, predict)\n    rec_score = recall_score(y_test, predict)\n    f1 = f1_score(y_test, predict)\n    mse, bias, var = bias_variance_decomp(model, x_train, y_train, x_test, y_test, loss='mse',\n                                          num_rounds=200, random_seed=1)\n    predicted_f1_scores.append(f1)\n\n    print('Model name: ', name)\n    print('Accuracy score: %.3f' % acc_score)\n    print('Precision score: %.3f' % prec_score)\n    print('Recall score: %.3f' % rec_score)\n    print('f1 score: %.3f' % f1)\n    print('Confusion Matrix',c_matrix)\n    print('Bias: %.3f' % bias)\n    print('Variance: %.3f' % var)\n    \n    print('#','-'*50,'#')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nplt.title('Predicted F-1 scores',fontsize=15)\nsns.barplot(x = predicted_f1_scores, y = model_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2 Model Selection\nSVC model has the highest F-1 score (0.939) compared to other Classifier. SVC model also have the less Bias and Variance scores.\n\nIt makes sense that the Ensemble and Boosting algorithm aren't doing great in this case, the dataset needs to be high dimensional(atleast 10000 reords).\n\nSVC take cares of outliers better than KNN, GaussianNB. Run time for SVC is drastically lower compare to MLPClassifier.\n\nHowever, SVC model has the best score. Let's work on the hyperparameter tuning.\n\n#### 3.2.1 Hyperparameter Tuning\n\nSVM’s almost a standard on good enough datasets to get high accuracy. But improving them can be a bit of a trick but today we’ll improve them using some standard techniques. Randomized search and Grid search for optimizing hyperparameters, but here to keep it more simple we are using for loop.\n\n","metadata":{}},{"cell_type":"code","source":"for c in [0.1, 0.5, 1, 2, 5, 10, 100]:\n    for k in ['linear', 'poly', 'rbf', 'sigmoid']:\n        model = SVC(C=c, kernel=k)\n        model.fit(x_train,y_train)\n        predict = model.predict(x_test)\n       \n        f1 = f1_score(y_test, predict)\n        mse, bias, var = bias_variance_decomp(model, x_train, y_train, x_test, y_test, loss='mse',\n                                              num_rounds=200, random_seed=1)\n\n        print('C: ', c, 'kernel:', k)\n        print('Accuracy score: %.3f' % acc_score)\n        print('Precision score: %.3f' % prec_score)\n        print('Recall score: %.3f' % rec_score)\n        print('f1 score: %.3f' % f1)\n        print('Confusion Matrix',c_matrix)\n        print('Bias: %.3f' % bias)\n        print('Variance: %.3f' % var)\n\n        print('#','-'*50,'#')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The F-1 score remains same, even after training with multiple parameters.  Let's develop the model with best params.","metadata":{}},{"cell_type":"code","source":"# Create the model \nmodel = SVC(C=1, kernel='rbf')\nmodel.fit(x_train, y_train)\npredict = model.predict(x_test)\nprint('f1 score: %.3f' % f1_score(y_test, predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the Predicted vs Actual scores\nplt.figure(figsize = (15,5))\nplt.title('Predicted vs Actual',fontsize=15)\nsns.distplot(predict, label = 'Predicted')\nsns.distplot(y_test, label = 'Actual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For perfect prediction, we would have Predicted=Actual. In the above graph, the predicted values are pretty close to the actual values.\n\n#### 3.2.3 Visualising Top Features ","metadata":{}},{"cell_type":"code","source":"# Extract the top features with weights and visualize\nfeatures_names = list(heart_attack_df.drop(\"output\",axis=1))\nsvm = SVC(kernel='linear')\nsvm.fit(x_train, y_train)\nimp,names = zip(*sorted(zip(abs(svm.coef_[0]), features_names)))\nplt.figure(figsize = (15,5))\nplt.title('Top Features in the Model',fontsize=15)\nplt.barh(range(len(names)), imp, align='center')\nplt.yticks(range(len(names)), names)\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insites from the Heart Attack Chance data analysis\n\n1. The heart attack dataset doesn't have missing/null/nan values.\n\n2. We are uncertain to remove/replace the outliers in the continuous variables.\n\n3. Correlation plots indicates that there is no Multicollinearity between the variables. \n\n4. Only Cholesterol data is Normally distributed, where as other variables are Skewed distributed.\n\n5. It is intuitive that elder people might have higher chances of heart attack but according to the distribution plot of age vs output variables, it is evident that age group between 40-60 has higher chances.\n\n6. People belogs to sex=1 have higher chance of getting heart attack compare to sex = 0.\n\n7. People with Non-Anginal chest pain, that is with cp = 2 have higher chances of heart attack.\n\n8. Person higher higher heart rate(> 150) are more probable to suffer from Heart Attack.\n\n9. People with Resting Blood Pressure - range of 120 to 140 have higher chance of heart attack. \n\n10. People with thall = 2 have much higher chance of heart attack.\n\n11. People with no exercise induced angina(exng = 0) have higher chance of heart attack.\n\n12. People with lower pevious peak will have higher chances of heart attack.\n\n13. People with number of major vessels = 0 have higher chances of heart attack.\n\n14. Top Features plot tell us feature significance.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}