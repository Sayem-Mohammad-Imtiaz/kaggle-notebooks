{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://docs.dask.org/en/latest/_images/dask_icon.svg)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Introduction to Dask\n\n- If data is more than RAM size, certainly a Data Scientist will look for another tool and first tool which might come in sight is Dask. In Dask the Dask DataFrame is no doubtly going to be a huge support to analyse huge amount of data. A Dask DataFrame, is made of many small Pandas DataFrame.\n- Can run in parallel system like cluster or on single machine.\n- If Data is larger than RAM. No Problem some part will be in memory some part on disc but computation go on single machine. "},{"metadata":{},"cell_type":"markdown","source":"# Installation \n\n- On Kaggle machines Dask is allready installed. \n- But if you want to install Dask on any machine then it can be installed using conda and pip both\n- Installing Dask using conda using following line of code\n\nconda install dask\n\n- If you have installed Anaconda then Dask will be installed with it. Further if required it can be upgraded. \n\n- Dask can be installed using pip too.\n\npip install dask  : But it will only install core Dask Part\n\n- In order to install complete Dask following code can be used\n\npip install \"dask[complete]\"\nconda install \"dask[complete]\""},{"metadata":{},"cell_type":"markdown","source":"# About DataSet used for this kernel\n\nDataset i have used from kaggle itself. This Dataset about importing stuffs to India and Exporting Stuffs from India. Following Information i have just copy pasted from Data Official site on kaggle \n\n---------------------------------------------------\n\n- Context\nIndia is one of the fastest developing nations of the world and trade between nations is the major component of any developing nation. This dataset includes the trade data for India for commodities in the HS2 basket.\n\nFor more, visit GitHub\n\n- Content\nThe dataset consists of trade values for export and import of commodities in million US$. The dataset is tidy and each row consists of a single observation.\n\n- Acknowledgements\nThe data is scraped using Selenium Webdriver from the Department of Commerce, Government of India.\n\nData set has been collect and uploaded to Kaggle by  Lakshya Agarwal  \nhttps://www.kaggle.com/lakshyaag\n\n"},{"metadata":{},"cell_type":"markdown","source":"# More about Dataset \n\ntaken from kernel \nhttps://www.kaggle.com/shubhamsinghgharsele/analysis-on-indian-import-export\n\n\nIn both the File we have 5 columns each.\n\nHSCode - HS stands for Harmonized System. It was developed by the WCO (World Customs Organization) as a multipurpose international product nomenclature that describes the type of good that is shipped HS Code Structure\n\nThe HS code can be described as follows:\n\nIt is a six-digit identification code. It has 5000 commodity groups. Those groups have 99 chapters. Those chapters have 21 sections. Itâ€™s arranged in a legal and logical structure. Well-defined rules support it to realize uniform classification worldwide.\n\nthe HSCode in column is 99 chapters\n\nReference HSCode List\n\nCommodity - the column contain chapter wise commodity category. In each commodity Category there are various commodities.\n\nA commodity is an economic good or service that has full or substantial fungibility: that is, the market treats instances of the good as equivalent or nearly so with no regard to who produced them.\n\nReference\n\nValue - values for export and import of commodities in million US $.\nCountry - Country Imported From/ Exported To\nYear - Year in which comodities where Imported/Exported which is in between 2010 to 2018."},{"metadata":{},"cell_type":"markdown","source":"# Importing Dask "},{"metadata":{"trusted":true},"cell_type":"code","source":" import dask.dataframe as dd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the Data Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"exportDf = dd.read_csv('/kaggle/input/india-trade-data/2018-2010_export.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exportDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding information about DataFrame\nexportDf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Schema of DataFrame\nexportDf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importDf = dd.read_csv('/kaggle/input/india-trade-data/2018-2010_import.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nset(importDf.HSCode) == set(exportDf.HSCode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function compute perform the computation."},{"metadata":{"trusted":true},"cell_type":"code","source":"importDf.value.min().compute()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Filtering"},{"metadata":{"trusted":true},"cell_type":"code","source":"importDf.describe().compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting number of partition\nimportDf.npartitions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Filtering \n- Getting all the imports where import value is greater than 66"},{"metadata":{"trusted":true},"cell_type":"code","source":"filteredVal =importDf[importDf.value > 66]\nfilteredVal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filteredVal =importDf[(importDf.value > 66) & (importDf.value < 100)]\nfilteredVal.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Aggregation\n\n- Mean value grouped by  HSCode"},{"metadata":{"trusted":true},"cell_type":"code","source":"meanVal = importDf.groupby(\"HSCode\").mean()\nmeanVal.compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data joining \njoinedData = importDf.merge(exportDf, on = \"HSCode\",how=\"inner\")\njoinedData.compute().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison of performance between pandas and dask DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading a File using Pandas\n%timeit pd.read_csv(\"/kaggle/input/india-trade-data/2018-2010_import.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit dd.read_csv(\"/kaggle/input/india-trade-data/2018-2010_import.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For file reading Dask is much faster than Pandas"},{"metadata":{},"cell_type":"markdown","source":"### Comparison of code efficiency for Data Filtering"},{"metadata":{"trusted":true},"cell_type":"code","source":"importDfPd = pd.read_csv('/kaggle/input/india-trade-data/2018-2010_import.csv')\nimportDfDd = dd.read_csv('/kaggle/input/india-trade-data/2018-2010_import.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering using Dask\n%timeit val = importDfDd[importDfDd.value >66].compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering using Pandas\n%timeit val = importDfPd[importDfPd.value >66]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can observe that time is very large for Dask. Can we decrease it? Let us increase number of Partitions."},{"metadata":{"trusted":true},"cell_type":"code","source":"importDfDd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importDfDd1 = importDfDd.repartition(npartitions = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importDfDd.npartitions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering using Dask\n%timeit val = importDfDd1[importDfDd1.value >66].compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importDfDd1 = importDfDd.repartition(npartitions = 1)\n%timeit val = importDfDd1[importDfDd1.value >66].compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}