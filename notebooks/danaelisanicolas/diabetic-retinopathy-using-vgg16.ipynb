{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is about training a 224 x 224 images of diabetic retinopathy images. Since these are images with 224 x 224 size, we'll use Convolutional2d model, in this case we'll use VGG16 as our CNN model with the pretrained weights of imagenet. Firstly, we'll split the input into train and test. We'll use split_folders to do this. However we must first install as this is not a basic python package that's already preinstalled in Kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install split_folders","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import split_folders\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import metrics\n\nfrom sklearn.utils import class_weight\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\n\nimport os\nfrom os import listdir\nfrom os.path import isfile, join","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we make our folders for train and validation where the split will be placed."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.makedirs('output')\nos.makedirs('output/train')\nos.makedirs('output/val')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Double checking where our input will be..."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/diabetic-retinopathy-224x224-gaussian-filtered/gaussian_filtered_images","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then split the input into train and validation. We'll have a 80-20 split with specified seed to 1 for reproducibility"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_loc = '../input/diabetic-retinopathy-224x224-gaussian-filtered/gaussian_filtered_images/gaussian_filtered_images/'\n\nsplit_folders.ratio(img_loc, output='output', seed=1, ratio=(0.8, 0.2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loc = 'output/train/'\ntest_loc = 'output/val/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then let's use the ImageDataGenerator to get our train and validation images with their corresponding categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"trdata = ImageDataGenerator()\ntraindata = trdata.flow_from_directory(directory=train_loc, target_size=(224,224))\ntsdata = ImageDataGenerator()\ntestdata = tsdata.flow_from_directory(directory=test_loc, target_size=(224,224))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the VGG16 pretrained model with imagenet. Then set the prediction or output layer to 5 outputs which corresponds to 5 different categories -- Severe, No_DR, Moderate, Proliferate_DR, and Mild."},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16 = VGG16(weights='imagenet')\nvgg16.summary()\n\nx  = vgg16.get_layer('fc2').output\nprediction = Dense(5, activation='softmax', name='predictions')(x)\n\nmodel = Model(inputs=vgg16.input, outputs=prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imagenet is trained with different kinds of objects. Our images are health related images, which means we must set some of the VGG16 layers to trainable. For this task, I'm using only the 1st Conv2d set as non-trainable to get the edges of our images/focus object. And the rest as trainable."},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers:\n    layer.trainable = False\n\nfor layer in model.layers[-16:]:\n    layer.trainable = True\n    print(\"Layer '%s' is trainable\" % layer.name)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we'll use Adam optimiser and use accuracy as our main metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(lr=0.000001)\nmodel.compile(optimizer=opt, loss=categorical_crossentropy, \n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that our non-trainable parameters are just about 260k while trainable parameters are 134M. This was the result of setting trainable layers which we did earlier.\n\nThen we create the checkpoint (which saves the weights when we get the best validation accuracy while training) and early stopping (which stops the training when we reach a certain number of epochs and the validation accuracy is not improving). In our case, we set it to only 20 epochs (denoted by the patience attribute)."},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"vgg16_diabetes.h5\", monitor='val_accuracy', verbose=1, \n                             save_best_only=True, save_weights_only=False, mode='auto')\nearly = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make sure that we have a fair training of our data, we use class weights. We compute this by using Counter to count the number of images per class/category over the number of the supposed max number of images per class/category."},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = Counter(traindata.classes)                       \nmax_val = float(max(counter.values()))   \nclass_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}\nclass_weights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that everything's set, let's start training!\n\nNote that we are computing the steps per epoch and the validation steps. This is to get the optimal number of these variables for training our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(traindata, steps_per_epoch=traindata.samples//traindata.batch_size, validation_data=testdata, \n                 class_weight=class_weights, validation_steps=testdata.samples//testdata.batch_size, \n                 epochs=80,callbacks=[checkpoint,early])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then finally.. let's observe how the model is training. Let's plot our loss vs validation loss as well as accuracy vs validation accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='val')\nplt.title('VGG16: Loss and Validation Loss (0.0001 = Adam LR)')\nplt.legend();\nplt.show()\n\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='val')\nplt.title('VGG16: Accuracy and Validation Accuracy (0.0001 = Adam LR)')\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And that's that! If this kernel helped you in any way, please do upvote and comment.\n\nThanks!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}