{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram,linkage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Means Clustering\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms. Typically, unsupervised algorithms make inferences from datasets using only input vectors without referring to known, or labelled, outcomes. A cluster refers to a collection of data points aggregated together because of certain similarities. You’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid.","metadata":{}},{"cell_type":"markdown","source":"To start off with we will generate 150 random data coordinates and scatter them along the X and Y axis and use the K-means algorithm to find accurate cluster points among the data.","metadata":{}},{"cell_type":"code","source":"#Random Data coordinates\nX = np.random.rand(150,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The K-means algorithm starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative  calculations to optimize the positions of the centroids\n\n**This iteration stops when:**\n1. The centroids have stabilized — there is no change in their values because the clustering has been successful.\n2. The defined number of iterations has been achieved.","metadata":{}},{"cell_type":"markdown","source":"**Create K-means model**\n\n***cluster_centers_***: Is used for finding the center of the clusters and,\n\n***labels_***: Is used for getting the labels property of the K-means clustering example dataset; that is, how the data points are categorized into the two clusters.","metadata":{}},{"cell_type":"code","source":"#K-means Model\nKMM=KMeans(n_clusters = 3)  #Using 3 clusters\nKMM.fit(X)\n\ncentroids=KMM.cluster_centers_  #To find the cluster centers\nlabels=KMM.labels_   # To find label for each data coordinate\n\nprint(\"The 3 Centroid co-ordinates are: \\n{}\\n\".format(centroids))\nprint(\"The Labels are: \\n{}\".format(labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will now visualize the data along with the cluster diagram and use different colours for different labels**\n\nWe are using 3 different colors for 3 different labels, and a green colour to show the clusters.","metadata":{}},{"cell_type":"code","source":"colors = ['b.','r.','y.','g.']\n\nplt.figure(figsize=(12,6))\nfor i in range(len(X)):\n    plt.plot(X[i][0],X[i][1], colors[labels[i]], markersize = 10)  #Scatter the data points\n    \nplt.scatter(centroids[:,0],centroids[:,1],marker=\"o\", s=80000, c= \"green\",alpha=0.7) \nplt.scatter(centroids[:,0],centroids[:,1],marker=\"X\", s=50, c= \"black\")  #Cluster centers\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Elbow Method**\n\nNow you must be wondering how i arrived at the conclusion that i need 3 clusters for this data. This is decided using the \"Elbow method\" It basically plots the error against the number of clusters. To choose the appropriate number of clusters, we choose the number before which there is a steepest decrease in error. The optimum number of clusters is where the elbow occurs. (E.g. for the Iris dataset, the elbow is created at cluster number 3, hence we choose 3 clusters.)","metadata":{}},{"cell_type":"code","source":"#Elbow method\nError =[]\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i).fit(X)\n    kmeans.fit(X)\n    Error.append(kmeans.inertia_)\n    \nplt.plot(range(1,11),Error)\nplt.title(\"Elbow method\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Error\")\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Case Study\nTo understand this algorithm better, we will use the popular **Iris Flower Dataset**","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/iris-flower-dataset/IRIS.csv\")\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=data.iloc[:,[0,1,2,3]].values  #We will extract the data columns we need.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**K-Means Model**","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)  #I got 3 clusters as per the elboe method\ny_kmeans = kmeans.fit_predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualising the clusters\nplt.figure(figsize=(12,6))\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 30, c = 'red', label = 'Iris-setosa')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 30, c = 'blue', label = 'Iris-versicolour')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 30, c = 'green', label = 'Iris-virginica')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 50, c = 'black', label = 'Centroids',marker=\"X\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 40000, c = 'purple', alpha=0.5, marker=\"o\")\nplt.title(\"Cluster Diagram for Iris Dataset\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using Seabron for for scatter plot**","metadata":{}},{"cell_type":"code","source":"sns.jointplot(data=data, x=\"sepal_length\", y=\"sepal_width\",hue=\"species\",kind=\"scatter\")\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hierarchical clustering\n\nHierarchical clustering is another unsupervised machine learning algorithm, which is used to group the unlabeled datasets into a cluster. In this algorithm, we develop the hierarchy of clusters in the form of a tree, and this tree-shaped structure is known as the dendrogram. Sometimes the results of K-means clustering and hierarchical clustering may look similar, but they both differ depending on how they work. As there is no requirement to predetermine the number of clusters as we did in the K-Means algorithm.\n\nSo, as we have seen in the K-means clustering that there are some challenges with this algorithm, which are a predetermined number of clusters, and it always tries to create the clusters of the same size. To solve these two challenges, we can opt for the hierarchical clustering algorithm because, in this algorithm, we don't need to have knowledge about the predefined number of clusters.\n\nWe will be using the **Agglomerative Hierarchical clustering** technique:\n\nIt is a popular example of HCA. To group the datasets into clusters, it follows the bottom-up approach. It means, this algorithm considers each dataset as a single cluster at the beginning, and then start combining the closest pair of clusters together. It does this until all the clusters are merged into a single cluster that contains all the datasets.","metadata":{}},{"cell_type":"code","source":"#Data to cluster hierarchically\nX = np.array([[0.4,0.53],[0.22,0.38],[0.35,0.32],[0.26,0.19],[0.08,0.41],[0.45,0.3]]) \nY = np.array([[4,0.5],[0.2,8],[0.35,0.2],[0.26,0.17],[0.8,0.16],[0.43,0.78]])\nprint(\"X:{}\\n\".format(X))\nprint(\"Y:{}\".format(Y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We use single linkage in our model and distance between data will be calculated using euclidean distange method.\nHCA=AgglomerativeClustering(n_clusters = 2, affinity='euclidean', linkage =\"single\") \nHCA.fit_predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linked1 = linkage(X, 'single')\nprint(\"Link 1: \\n{}\".format(linked1))\nprint(\"\\n\")\nlinked2 = linkage(Y, 'single')\nprint(\"Link 2: \\n{}\".format(linked2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dendograms for link 1 and link 2\ndendrogram(linked1)\nplt.title(\"X\")\nplt.grid()\nplt.show()\n\n\ndendrogram(linked2)\nplt.title(\"Y\")\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wrapping Up\n**We implemented two very popular types of clustering algorithms, namely K-Means clustering and Hierarchical Clustering. They both fall under the unsupervised machine learning category as there is no labelling of data initially.**\n\n**There were some drawbacks of K-Means clustering like knowing the number of clusters and every cluster having the same size, but these drawbacks are overcome by the Hierarchical clustering and hence it is a widely used algorithm for all kinds of clustering data.**","metadata":{}},{"cell_type":"markdown","source":"# ----------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"**I hope this notebook was easy to understand and useful for y'all.**\n\n**If you found it helpful please do upvote!**\n\n**If you have and queries or suggestions for me, feel free to comment down below, i would love to improve!**\n\n# Thank You!!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}