{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('use_inf_as_na', True)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score as acs\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.utils import resample\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\ndata.drop([\"Unnamed: 32\"],axis=1,inplace=True)\ndisplay(data.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#New data with \"M\"=1 and \"B\"=0\ndata1=data.copy()\ndef classifier(data1):\n    if data1[\"diagnosis\"]==\"M\":\n        return \"1\"\n    else:\n        return \"0\"\ndata1[\"diagnosis\"] = data1.apply(classifier, axis=1)\ndata1.replace([np.inf, -np.inf], np.nan, inplace=True)\ndata1[\"diagnosis\"]=pd.to_numeric(data1[\"diagnosis\"],errors=\"coerce\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __Data Preprocessing__ ","metadata":{}},{"cell_type":"code","source":"print(data1.columns,data.shape)\n# print(data1.info())\n# print(data1.describe().T)\n# print(data1.nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nprint(data['diagnosis'].value_counts())\nfig = plt.figure(figsize = (10,6))\nsns.countplot('diagnosis', data=data, palette='gist_heat')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color = sns.color_palette(\"pastel\")\n\nfig, ax1 = plt.subplots(8,4, figsize=(30,60))\nk = 0\ncolumns = list(data1.columns)\nfor i in range(8):\n    for j in range(4):\n            sns.distplot(data1[columns[k]], ax = ax1[i][j], color = 'red')\n            plt.xlabel(columns[k],size=20)\n            k += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Note:** Most of our columns are highly skewed towards right. These columns include compactness_mean, concavity_mean, concave points_mean, radius_se, perimeter_se, smoothness_se, compactness_se, concavity_se, symmetry_se, fractal_dimension_se, area_worst, compactness_worst, concavity_worst. So we need to tranform them. Applying a log transfrom will solve the problem!","metadata":{}},{"cell_type":"code","source":"#Log transform\ndef log_transform(col):\n    return np.log(col[0])\n\ndata1[\"compactness_mean\"]=data1[[\"compactness_mean\"]].apply(log_transform, axis=1)\ndata1[\"concavity_mean\"]=data1[[\"concavity_mean\"]].apply(log_transform, axis=1)\ndata1[\"concave points_mean\"]=data1[[\"concave points_mean\"]].apply(log_transform, axis=1)\ndata1[\"radius_se\"]=data1[[\"radius_se\"]].apply(log_transform, axis=1)\ndata1[\"perimeter_se\"]=data1[[\"perimeter_se\"]].apply(log_transform, axis=1)\ndata1[\"smoothness_se\"]=data1[[\"smoothness_se\"]].apply(log_transform, axis=1)\ndata1[\"compactness_se\"]=data1[[\"compactness_se\"]].apply(log_transform, axis=1)\ndata1[\"concavity_se\"]=data1[[\"concavity_se\"]].apply(log_transform, axis=1)\ndata1[\"symmetry_se\"]=data1[[\"symmetry_se\"]].apply(log_transform, axis=1)\ndata1[\"fractal_dimension_se\"]=data1[[\"fractal_dimension_se\"]].apply(log_transform, axis=1)\ndata1[\"area_worst\"]=data1[[\"area_worst\"]].apply(log_transform, axis=1)\ndata1[\"compactness_worst\"]=data1[[\"compactness_worst\"]].apply(log_transform, axis=1)\ndata1[\"concavity_worst\"]=data1[[\"concavity_worst\"]].apply(log_transform, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color = sns.color_palette(\"pastel\")\n\nfig, ax1 = plt.subplots(8,4, figsize=(30,60))\nk = 0\ncolumns = list(data1.columns)\nfor i in range(8):\n    for j in range(4):\n        sns.distplot(data1[columns[k]], ax = ax1[i][j], color = 'green')\n        k += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation plot between the features:**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\ncorr=data1.drop([\"id\"],axis=1).corr()\nsns.heatmap(corr,annot=True,linewidth=1)\nplt.show()\n\n#Correaltion of features in descending order\nprint(data1.corr()['diagnosis'].sort_values(ascending=False))\n\nplt.figure(figsize=(16,8))\nplt.plot(data1.corr()['diagnosis'].sort_values(ascending=False)[1:],color=\"cyan\")\nplt.title(\"Correlation of different features with 'Diagnosis'\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Boxplot of top 5 corrrelated features**","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=data,x=\"diagnosis\",y=\"concave points_worst\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"perimeter_worst\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"concave points_mean\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"radius_worst\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"perimeter_mean\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we will be doing undersampling and oversampling.**\n\n* The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfishing.\n* In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.","metadata":{}},{"cell_type":"code","source":"data_M = data1[data1.diagnosis==1]     #Minority\ndata_B = data1[data1.diagnosis==0]     #Majority\n\ndata_M_upsampled=resample(data_M,replace=True, n_samples=300, random_state=12)\ndata_B_downsampled= data_B.sample(n=300).reset_index(drop=True)\n\n#New dataset for balanced data\nBalanced_df = pd.concat([data_M_upsampled, data_B_downsampled]).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Balanced_df[\"diagnosis\"].value_counts())\nplt.figure(figsize=(10,6))\nsns.countplot(x='diagnosis', data=Balanced_df, palette='gist_heat')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now the count for our output variable \"diagnosis\" has been made equal**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nBalanced_df.corr().diagnosis.apply(lambda x: abs(x)).sort_values(ascending=False).iloc[1:21][::-1].plot(kind='barh',color='cyan') \n# calculating the top 20 highest correlated features\n# with respect to the target variable i.e. \"quality\"\nplt.title(\"Top 20 highly correlated features\", size=20, pad=26)\nplt.xlabel(\"Correlation coefficient\")\nplt.ylabel(\"Features\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will only be using the top 20 correlated features to train our model, this will hellp improve the accuacy. ","metadata":{}},{"cell_type":"code","source":"selected_features=Balanced_df.corr().diagnosis.sort_values(ascending=False).iloc[1:21][::-1].index\n\nX = Balanced_df[selected_features]\nY = Balanced_df.diagnosis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split data into training and testing sets**","metadata":{}},{"cell_type":"code","source":"X=data.iloc[:,2:32]\nY=data.iloc[:,1]\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0) \n\n#Feature Scaling\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 1=>Logistic Regression","metadata":{}},{"cell_type":"code","source":"LR_model=LogisticRegression(random_state=0)\nLR_model.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=LR_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nLogistic_Regression_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Logistic_Regression_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 2=>Support Vector Machine","metadata":{}},{"cell_type":"code","source":"svm=SVC(kernel=\"rbf\",random_state=0)\nsvm.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=svm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nSVM_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), SVM_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 3=>Decision Tree","metadata":{}},{"cell_type":"code","source":"tree=DecisionTreeClassifier(random_state=10)\ntree.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=tree.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nDecision_Tree_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Decision_Tree_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 4=>Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rfc=RandomForestClassifier(n_estimators=60,random_state=0)\nrfc.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=rfc.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nRandom_forest_classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Random_forest_classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 5=>Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"gbc=GradientBoostingClassifier(random_state=11)\ngbc.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=gbc.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nGradient_Boosting_Classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Gradient_Boosting_Classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 6=>XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"xgb=XGBClassifier(random_state=0,booster=\"gbtree\")\nxgb.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=xgb.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nXG_boost_classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), XG_boost_classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 7=>K-Nearest Neighbor (KNN) classification","metadata":{}},{"cell_type":"code","source":"knn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=knn.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nKNN_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), KNN_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Method to find the best value of *n_neighbors* based on accuracy**","metadata":{}},{"cell_type":"code","source":"val=10   #Max value of n_neighbor\nmodel=knn  #Name of model you want to train (I'm training my KNN model)\nfor K in range(val):\n    K_value = K+1\n    model = KNeighborsClassifier(n_neighbors=K_value)\n    model.fit(X_train,Y_train)\n    Y_pred = model.predict(X_test)\n    print(\"Accuracy is : \", acs(Y_test,Y_pred)*100,\"% for n_neighbors: \", K_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 8 => MLP Classifier","metadata":{}},{"cell_type":"code","source":"classifier = MLPClassifier(random_state=1,hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam')\nclassifier.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=classifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nMLP_classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), MLP_classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparison of Accuracy**","metadata":{}},{"cell_type":"code","source":"accuracies={\"Random Forest\": Random_forest_classifier_accuracy,\n            \"SVM\": SVM_accuracy,\n            \"MLP Classifier\": MLP_classifier_accuracy,\n            \"Gradient Boosting\": Gradient_Boosting_Classifier_accuracy,\n            \"XG Boost\": XG_boost_classifier_accuracy,\n            \"KNN\": KNN_accuracy,\n            \"Logistic regression\": Logistic_Regression_accuracy,\n            \"Decision Tree\": Decision_Tree_accuracy}\n\n#Plot accuracy for different models\nplt.figure(figsize=(14,6))\nplt.bar(accuracies.keys(),accuracies.values(),label=\"Accuracy\")\nplt.xlabel(\"Classifier Used\")\nplt.ylabel(\"Accuracy (%)\")\nplt.ylim(90,100)\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis\n__After using 8 different algorithms, we got the following accuracies:__\n1. Logistic Regression - **95.61%**\n2. Support Vector Machine - **98.25%**\n3. Decision Tree - **93.86%**\n4. Random Forest Classifier - **98.25%**\n5. Gradient Boosting Classfier - **96.49%**\n6. XGBoost Classifier - **96.49%**\n7. K-nearest neighbor classification - **96.49%**\n8. MLP Classifier - **97.37%**\n\nThis clearly shows that **SVM** and **Random Forest Classifier** are the most efficient and accurate algorithms, and hence they are most widely used for classification problems.\n\n> **NOTE:** XGBoost is also a very powerful algorithm when it comes to classification. The reason we got just 96.49% accuracy using XGboost is because the training data (X_train) was scaled at the beginning using **StandardScaler**.\n> To obtain a better accuracy with XGBoost (almost 99%), train the model without scaling the training data.","metadata":{}},{"cell_type":"markdown","source":"**If you found this notebook useful, please do upvote!**","metadata":{}},{"cell_type":"markdown","source":"**If you have any suggestions or doubts, feel free to comment below!**","metadata":{}},{"cell_type":"markdown","source":"**Thank you!**","metadata":{}}]}