{"cells":[{"metadata":{"_uuid":"356257717264d7cb1218443d61129d8418d74de3"},"cell_type":"markdown","source":"<b>This notebook was created by</b>: <br>\nAmandas Armbruester (<amarmbru@mail.uni-mannheim.de>) <br>\nMutasim Billah (<mbillah@mail.uni-mannheim.de>) <br>\nVasili Bocicariov (<vbocicar@mail.uni-mannheim.de>) <br>\nNiklas Luedemann (<nluedema@mail.uni-mannheim.de>) <br>\nRoman Salzwedel (<rsalzwed@mail.uni-mannheim.de>) <br>\nRahul Taneja (<rtaneja@mail.uni-mannheim.de>) <br>\n\nas part of Data Mining project at the University of Mannheim, Germany. \n<br><br>\nWe would like to <b>thank the author of this dataset Kevin Arvai</b> for his useful insights, suggestions and recommendations regarding the field of human genetics."},{"metadata":{"_uuid":"82dbd9d00353b6d85d3e328ab5edefbc0c505968"},"cell_type":"markdown","source":"# 1. GET THE DATA"},{"metadata":{"trusted":true,"_uuid":"952678e241d8e29c2422c1c1021fb436ca3a649c"},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', 300)\n\nclinvar_data = pd.read_csv('../input/clinvar-conflicting/clinvar_conflicting.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d6936e83022925b0d56d7689bd4ce788688f0d5"},"cell_type":"markdown","source":"# 2. DATA TRANSFORMATION"},{"metadata":{"_uuid":"e7aec039709d2bede027219cffac4ab6619e6348"},"cell_type":"markdown","source":"+ FixChromosome()\n+ CountAlleles()\n+ ExtractPositions()\n+ MarkIntronsAndExons()\n+ MarkNotSpecifiedCLNDN()\n+ ExtractEXONPositionAndLength()\n+ AddPathways()"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4359e603d36415ce04edcde924107dd08d32b4ae"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass FixChromosome(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Transform \\'CHROM\\' feature to a numerical feature. Assign X = 23, MT = 24'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n\n        X_copy.loc[:, 'CHROM'].replace('X', 23, inplace=True)\n        X_copy.loc[:, 'CHROM'].replace('MT', 24, inplace=True)\n        X_copy.loc[:, 'CHROM'] = X_copy.CHROM.astype(int)\n\n        return X_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b30e9a7a405c806a722a1f28082d964d36f12d85"},"cell_type":"code","source":"import numpy as np\nimport functools\ndef conjunction(*conditions):\n    return functools.reduce(np.logical_and, conditions)\n\nclass CountAlleles(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Calculate the length of REF, ALT, Allele features and mark Single Nucleotide Variants'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n        \n        X_copy.loc[:, 'REF_length'] = X_copy.REF.str.len()\n        X_copy.loc[:, 'ALT_length'] = X_copy.ALT.str.len()\n        X_copy.loc[:, 'Allele_length'] = X_copy.Allele.str.len()\n        \n        ref_is_1 = X_copy.REF.str.len() == 1\n        alt_is_1 = X_copy.ALT.str.len() == 1\n\n        X_copy.loc[conjunction(ref_is_1, alt_is_1), 'SNV'] = 1\n        X_copy.SNV.fillna(0, inplace=True)\n\n        return X_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f97291c5b44239cb2f4902cca4b7a0f574fc9426"},"cell_type":"code","source":"class ExtractPositions(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Extract tstart and stop positions for cDNA_position, CDS_position, Protein_position'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n\n        ### START\n        X_copy['cDNA_position_start'] = X_copy.cDNA_position.str.split('-').str.get(0)\n        X_copy['CDS_position_start'] = X_copy.CDS_position.str.split('-').str.get(0)\n        X_copy['Protein_position_start'] = X_copy.Protein_position.str.split('-').str.get(0)\n\n        X_copy['cDNA_position_start'].replace('?', np.NaN, inplace=True)\n        X_copy['CDS_position_start'].replace('?', np.NaN, inplace=True)\n        X_copy['Protein_position_start'].replace('?', np.NaN, inplace=True)\n\n        X_copy['cDNA_position_start'] = X_copy['cDNA_position_start'].astype(float)\n        X_copy['CDS_position_start'] = X_copy['CDS_position_start'].astype(float)\n        X_copy['Protein_position_start'] = X_copy['Protein_position_start'].astype(float)\n\n        ### STOP\n        X_copy['cDNA_position_stop'] = X_copy.cDNA_position.str.split('-').str.get(1)\n        X_copy['CDS_position_stop'] = X_copy.CDS_position.str.split('-').str.get(1)\n        X_copy['Protein_position_stop'] = X_copy.Protein_position.str.split('-').str.get(1)\n\n        X_copy['cDNA_position_stop'].replace('?', np.NaN, inplace=True)\n        X_copy['CDS_position_stop'].replace('?', np.NaN, inplace=True)\n        X_copy['Protein_position_stop'].replace('?', np.NaN, inplace=True)\n\n        X_copy['cDNA_position_stop'] = X_copy['cDNA_position_stop'].astype(float)\n        X_copy['CDS_position_stop'] = X_copy['CDS_position_stop'].astype(float)\n        X_copy['Protein_position_stop'] = X_copy['Protein_position_stop'].astype(float)\n\n        for field in ['cDNA_position', 'CDS_position', 'Protein_position']:            \n            start_pos_exists = X_copy[field + '_start'].notnull()\n            stop_pos_does_not_exist = X_copy[field + '_stop'].isnull()\n            cn_filter = conjunction(start_pos_exists, stop_pos_does_not_exist)\n            X_copy.loc[cn_filter, field + '_stop'] = X_copy.loc[cn_filter, field + '_start']\n\n        for field in ['cDNA_position', 'CDS_position', 'Protein_position']:            \n            start_pos_does_not_exist = X_copy[field + '_start'].isnull()\n            stop_pos_exists = X_copy[field + '_stop'].notnull()\n            cn_filter = conjunction(start_pos_does_not_exist, stop_pos_exists)\n            X_copy.loc[cn_filter, field + '_start'] = X_copy.loc[cn_filter, field + '_stop']\n\n        for field in ['cDNA_position', 'CDS_position', 'Protein_position']:            \n            start_pos_does_not_exist = X_copy[field + '_start'].isnull()\n            stop_pos_does_not_exist = X_copy[field + '_stop'].isnull()\n            cn_filter = conjunction(start_pos_does_not_exist, stop_pos_does_not_exist)\n            X_copy.loc[cn_filter, field + '_start'] = 0\n            X_copy.loc[cn_filter, field + '_stop'] = 0\n\n        return X_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"55384fd2f9ab56eb17b74884d91a1004e4492001"},"cell_type":"code","source":"class MarkIntronsAndExons(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Mark intron and exon variants'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n\n        X_copy['Is_Exon'] = X_copy.EXON.notnull().astype(int)\n        X_copy['Is_Intron'] = X_copy.INTRON.notnull().astype(int)\n\n        return X_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"26ac7528cee72cca001e1e3bb3779e8c3010f36c"},"cell_type":"code","source":"def disjunction(*conditions):\n    return functools.reduce(np.logical_or, conditions)\n\nclass MarkNotSpecifiedCLNDN(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Create binary indicator for CLNDN: not-specified vs. rest'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n\n        c1 = X_copy.CLNDN == \"not_specified\"\n        c2 = X_copy.CLNDN == \"not_specified|not_provided\"\n        c3 = X_copy.CLNDN == \"not_provided|not_specified\"\n        c4 = X_copy.CLNDN == \"not_provided\"\n\n        X_copy.loc[disjunction(c1, c2, c3, c4), 'CLNDN_not_specified'] = 1\n        X_copy.CLNDN_not_specified.fillna(0, inplace=True)\n\n        return X_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"845f6c497167d225028b2c5e2cdc39e3b51853fa"},"cell_type":"code","source":"class ExtractEXONPositionAndLength(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Extract the position and length of EXON'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n\n        ### Position\n        X_copy['EXON_position'] = X_copy.EXON.str.split('/').str.get(0)\n        X_copy['EXON_position'] = X_copy['EXON_position'].astype(float)\n        \n        ### Length\n        X_copy['EXON_length'] = X_copy.EXON.str.split('/').str.get(1)\n        X_copy['EXON_length'] = X_copy['EXON_length'].astype(float)\n        \n        exon_pos_does_not_exist = X_copy[\"EXON_position\"].isnull()\n        exon_length_does_not_exist = X_copy[\"EXON_length\"].isnull()\n        cn_filter = conjunction(exon_pos_does_not_exist, exon_length_does_not_exist)\n        X_copy.loc[cn_filter, \"EXON_position\"] = 0\n        X_copy.loc[cn_filter, 'EXON_length'] = 0\n        \n        return X_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9a98cf0a072345695d4bfc3f728eb38e1ffa9040"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nclass AddPathways(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Get Pathway IDs from Reactome based on list of symbols from ClinVar data set'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n        \n        # Symbol_Pathways.csv generated by https://reactome.org/PathwayBrowser/#TOOL=AT based on list_of_symbols.csv\n        symbol_pathways = pd.read_csv('../input/clinvar-symbol-pathways/Symbol_Pathways.csv')\n\n        vectorizer = CountVectorizer(binary=True)\n        vec_symbols = vectorizer.fit_transform(symbol_pathways['Submitted entities found']).toarray()\n        symb_df = pd.concat([symbol_pathways['Pathway identifier'], \n                             pd.DataFrame(vec_symbols, columns=vectorizer.get_feature_names())], axis=1)\n        symb_df.set_index('Pathway identifier', inplace=True)\n        \n        pathways = []\n\n        for column in symb_df:\n            pathways.append(symb_df[symb_df[column] > 0].index.str.cat(sep=';'))\n\n        pathways_series = pd.Series(pathways, index=symb_df.columns)\n        pathways_series.head()\n        \n        symb_df = symb_df.append(pathways_series, ignore_index=True)\n        symb_df_transposed = symb_df.tail(1).transpose()\n        \n        symb_df_transposed.reset_index(level=0, inplace=True)\n        symb_df_transposed.rename(columns={'index':'SYMBOL', 140: 'Pathways'}, inplace=True)\n        \n        symb_df_transposed.SYMBOL = symb_df_transposed.SYMBOL.str.upper()\n        \n        return pd.merge(X_copy, symb_df_transposed, how='left', on=['SYMBOL'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9f4874c26780bb9ae4b883773c7c9305747f728"},"cell_type":"markdown","source":"**Note:** The *Symbol_Pathways.csv* was generated using Reactome Pathway Browser which is available <a href=\"https://reactome.org/PathwayBrowser/#TOOL=AT\">here</a>. The *list_of_symbols.csv* was obtained using the unqiue symbols in the dataset."},{"metadata":{"_uuid":"f5e030f49c72db8a541004df17ba11f548571350"},"cell_type":"markdown","source":"### Data Transformation and Simple Feature Extraction Pipeline"},{"metadata":{"trusted":true,"_uuid":"83fbae2169c182858e9d18aaccb9900df3a7ccd7","collapsed":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\n# Basic Tranformation Pipeline\ntransformation_pipeline = Pipeline([\n    ('fix_chromosome', FixChromosome()),\n    ('count_alleles', CountAlleles()),\n    ('extract_positions', ExtractPositions()),\n    ('mark_introns_and_exons', MarkIntronsAndExons()),\n    ('mark_not_specified_CLNDN', MarkNotSpecifiedCLNDN()),\n    ('extract_EXON_position_and_length', ExtractEXONPositionAndLength()),\n    ('add_pathways', AddPathways())\n])\n\nclinvar_transformed = transformation_pipeline.fit_transform(clinvar_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d3c829886096a79d68075a21b20f0106f7b1d8a"},"cell_type":"markdown","source":"# 3. DATA SET STATISTICS"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5cb7a46cd13970c1e61266ed8b9e890d6c1b919c"},"cell_type":"code","source":"# Features used for the analysis and models training\nnumeric_features = ['POS', 'AF_ESP', 'AF_EXAC', 'AF_TGP', 'LoFtool', 'CADD_PHRED', 'CADD_RAW', 'REF_length', 'ALT_length', 'Allele_length',\n                    'SNV', 'cDNA_position_start', 'CDS_position_start', 'Protein_position_start', 'cDNA_position_stop', 'CDS_position_stop',\n                    'Protein_position_stop', 'Is_Exon', 'Is_Intron', 'CLNDN_not_specified', 'EXON_position', 'EXON_length']\n                    \ncategorical_features = ['CHROM', 'IMPACT', 'STRAND', 'BAM_EDIT', 'SIFT', 'PolyPhen',\n                        'BLOSUM62', 'Consequence', 'CLNVC', 'Pathways']\n\ntarget_feature = ['CLASS']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"791892de6ed1af898811b11738641335fc02e599"},"cell_type":"code","source":"clinvar_transformed[numeric_features + categorical_features + target_feature].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e1e7b52e08adb269c9b5de31a656d7c79c754b1"},"cell_type":"code","source":"clinvar_transformed[numeric_features + categorical_features + target_feature].info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d17912fe1829fe81a81066f6be8d710661d8a498"},"cell_type":"code","source":"clinvar_transformed.describe(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17bc73601da79c06dfbe180937ef86888068f1da"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nclinvar_transformed.hist(figsize=(14,14))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b40078a0b4fad57d7b92f499b1b5ccb70daf0f28"},"cell_type":"code","source":"import seaborn as sns\n\nfeatures = ['CLNVC', 'IMPACT', 'SIFT', 'PolyPhen']\n\nfor feature in features:\n    sns.countplot(y=feature, data=clinvar_transformed)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a38b274d452c6960c54c9eeca0c7dd5514d25f"},"cell_type":"code","source":"# Calculate correlations between numeric features\ncorrelations = clinvar_transformed.corr()\n\n# Change color scheme\nsns. set_style (\"white\")\n# Generate a mask for the upper triangle\nmask = np.zeros_like(correlations, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Make the figsize 20 x 20\nplt.figure(figsize=(15,13))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Plot heatmap of correlations\nsns.heatmap(correlations * 100, annot=True, fmt='.0f', \n            mask=mask, cbar=False, cmap=cmap)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b849411430b9b7af95e98c89ae748356acfdf3d"},"cell_type":"markdown","source":"# 4. PREPARE DATA FOR MACHINE LEARNING ALGORITHM"},{"metadata":{"_uuid":"8af8c226fb4c7aec9e7e6af0425fe77c2b141053"},"cell_type":"markdown","source":"### Split the dataset into training and test sets."},{"metadata":{"trusted":true,"_uuid":"20bf3a1b79e14e0cbfef30d1654a8aec120b56bd"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = clinvar_transformed[numeric_features + categorical_features]\ny = clinvar_transformed.CLASS\n\ndef train_val_test_split(X, y):\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.2, random_state=111, stratify=y)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=111, stratify=y_temp)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4847881091d2c211238777261adf8070dfdaf8cc"},"cell_type":"code","source":"print(X_train.shape)\nprint(X_val.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13271d73b15ffd255f42130bfadccba75612b394"},"cell_type":"markdown","source":"**Note:** That we use a separate validation set here, due to extensive hyperparameter tuning of our candidate models. See below."},{"metadata":{"_uuid":"e0642d1b229987a3726745895032f65a84f8e880"},"cell_type":"markdown","source":"### Balance the Training Set"},{"metadata":{"trusted":true,"_uuid":"1a50c9acf4d5ca847f193a62884a4d31c731f6f7"},"cell_type":"code","source":"def balance_training_set(X_train, y_train):\n    X_temp = pd.concat([X_train, y_train], axis=1)\n    balanced = pd.concat([X_temp, X_temp[X_temp.CLASS == 1], X_temp[X_temp.CLASS == 1]])\n    return balanced.drop('CLASS', axis=1), balanced['CLASS']\n\nX_train_balanced , y_train_balanced = balance_training_set(X_train, y_train)\n\nprint('1/0 CLASS ratio:', sum((y_train_balanced == 1).astype(int)) / len(y_train_balanced))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec0c41bfef61d62fc4383e034bd21be9e0beb0ae"},"cell_type":"markdown","source":"### Custom Data Transformers"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b3ba46f04102d14540ff67536bc377f156cb3c72"},"cell_type":"code","source":"# Definition of the CategoricalEncoder class, copied from PR #9151.\n# https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/preprocessing/data.py\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c60e5bb054be91ae8216f5fb4cf50819c1f16240"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nclass EncodeNonNumericValues(BaseEstimator, TransformerMixin):\n    def __init__(self, consequence_list):\n        self.comment_ = 'Get feature matrix from Consequence and use CategoricalEncoder for the rest'\n        self.consequence_list = consequence_list\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy()\n        \n        consequene_vectorizer = CountVectorizer(binary=True, vocabulary=self.consequence_list)\n        consequene_matrix = consequene_vectorizer.fit_transform(X_copy['Consequence'])\n        \n        consequence_df = pd.DataFrame(consequene_matrix.toarray(), columns=consequene_vectorizer.get_feature_names())\n        \n        symbol_pathways = pd.read_csv('../input/clinvar-symbol-pathways/Symbol_Pathways.csv')\n        pathways = symbol_pathways['Pathway identifier'].unique()\n        \n        pathways_vectorizer = CountVectorizer(binary=True, vocabulary=pathways)\n        pathways_matrix = pathways_vectorizer.fit_transform(X_copy['Pathways'].values.astype('U'))\n        pathways_df = pd.DataFrame(pathways_matrix.toarray(), columns=pathways_vectorizer.get_feature_names())\n        pathways_df.fillna(0, inplace=True)\n        \n        categorical_encoder = CategoricalEncoder(encoding='onehot-dense')\n        ordinal_encoder = CategoricalEncoder(encoding='ordinal')\n        \n        impact_encoded = pd.DataFrame(ordinal_encoder.fit_transform(X_copy.IMPACT.values.reshape(-1, 1)))\n        X_copy = categorical_encoder.fit_transform(X_copy.drop(['Consequence', 'IMPACT', 'Pathways'], axis=1))\n        \n        return pd.concat([pd.DataFrame(X_copy), consequence_df, impact_encoded, pathways_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5e86e7bfedae57ba37b73f2090bcac0c8b86dfa7"},"cell_type":"code","source":"class ImputeCategoricalValues(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.comment_ = 'Custom categorical imputer'\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_copy = X.copy() # needed to prevent the subsequent code from overriding original dataframe\n        \n        X_copy['Bam_edit_was_missing'] = X_copy.BAM_EDIT.isnull()\n        X_copy['Sift_was_missing'] = X_copy.SIFT.isnull()\n        X_copy['PolyPhen_was_missing'] = X_copy.PolyPhen.isnull()\n        X_copy['BLOSUM62_was_missing'] = X_copy.BLOSUM62.isnull()\n        \n        # Since only 14 \"strands\" are imputed, 'Strand_was_missing' indicator won't be created\n        # Impute with most frequent value: -1\n        X_copy.STRAND.fillna(-1, inplace=True)\n        \n        # As half of the records does not contain BAM_EDIT, impute 'Unknown' value\n        X_copy.BAM_EDIT.fillna('Unknown', inplace=True)\n        \n        # Impute 'Unknown'\n        X_copy.SIFT.fillna('Unknown', inplace=True)\n        \n        # Impute 'Unknown'\n        X_copy.PolyPhen.fillna('Unknown', inplace=True)\n        \n        # Impute 'Unknown'\n        X_copy.BLOSUM62.fillna('Unknown', inplace=True)\n        X_copy.BLOSUM62 = X_copy.BLOSUM62.astype(str)\n        \n        return X_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d26c48810d4a1ef76ea45420128289dfaf8456df"},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5416fabf62d6a003fa4fb8d6effa71a311ac2ce9"},"cell_type":"markdown","source":"### Final Preprocessing Pipeline"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eecfb59d09409822e9856d4f38799459491eb1d7"},"cell_type":"code","source":"categorical_pipeline = Pipeline([\n    ('selector', DataFrameSelector(categorical_features)),\n    ('impute_category', ImputeCategoricalValues()),\n    ('cat_encoder', EncodeNonNumericValues(clinvar_transformed.Consequence.unique()))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a6cf6f524cdde42ee263a25f1a48a7b6a9749cc"},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\n\nnumeric_pipeline = Pipeline([\n    ('selector', DataFrameSelector(numeric_features)),\n    ('impute_numeric', Imputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"58985e5511ce20515de0a01494c315e568399620"},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('categorical_pipeline', categorical_pipeline),\n    ('numeric_pipeline', numeric_pipeline)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fab4806c6a417fa3c25a508a78fb271a2143f78d"},"cell_type":"code","source":"X_train_prepared = full_pipeline.fit_transform(X_train_balanced)\nX_val_prepared = full_pipeline.transform(X_val)\nX_test_prepared = full_pipeline.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e1704267fdd68252a12602b3535cbf58ac3da36"},"cell_type":"code","source":"print(X_train_prepared.shape)\nprint(X_val_prepared.shape)\nprint(X_test_prepared.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"535b20250f6815a73b1c5e8c1ade5ac850d16da9"},"cell_type":"markdown","source":"# 5. TRAIN AND SELECT A MODEL"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5661b19e71421ccd31f20a3634dbfe79acfaa3d"},"cell_type":"code","source":"# Define function to plot ROC-Curves\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curve(y_test, y_pred):\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\n    fig = plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n\n    plt.plot(fpr, tpr, label='l1')\n    plt.legend('lower right')\n\n    plt.plot([0, 1], [0, 1], 'k--')\n\n    plt.xlim([-0.1, 1.1])\n    plt.ylim([-0.1, 1.1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    \n    print('Area Under the Curve:', auc(fpr, tpr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c27eabd16b2b2ee7711189b7e1e3f62700dc17d"},"cell_type":"markdown","source":"**Note:** One of our main goals is to obtain a classification model that produces\na low number of False Negatives. This means, that we want to avoid predictions of\ngenetic variants as ’non-conflicting’, while they are actually ’conflicting’. Therefore, we choose the area under the ROC-curve (AUROC) as our main evaluation\nmeasure."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f08f578876218aaccee2c4ee7f7492491187306f"},"cell_type":"code","source":"# Define function that reports results of 10-fold Cross-Validations\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6b7e4454b5b3f9e1601ade4854cc0a26a74bcf4"},"cell_type":"code","source":"# Import all metrics for model evaluation\nfrom sklearn.metrics import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef6e1362ea0329a0b008a8e452bd85e9c3486cc9"},"cell_type":"markdown","source":"**Note:** Testing several classifcation methods (not shown here) showed that among those classifiers tested, the Decision Tree seems to work best for the tasked at hand. Therefore, here we will focus\non parameter tuning of the Decision Tree classifier and Decision Tree based\nEnsemble Techniques."},{"metadata":{"_uuid":"69d3ce469146066eb2ffe8de41946bd8cfe641b5"},"cell_type":"markdown","source":"### Parameter Tuning and Feature Selection"},{"metadata":{"_uuid":"a505392147f27fb90478887907d30c59cbbfe5b6"},"cell_type":"markdown","source":"**Note:** We use Scikit-learn’s `SelectFromModel` transformer to perform an automated\nfeature selection."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e5664f2ffe206e391a4894c01908a21c5dad9f88"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import NearestCentroid, KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectFromModel\n\npipelines = {\n    'rf': make_pipeline(SelectFromModel(RandomForestClassifier(random_state=42)), RandomForestClassifier(random_state=42)),\n    'gb': make_pipeline(SelectFromModel(GradientBoostingClassifier(random_state=42)), GradientBoostingClassifier(random_state=42)),\n    'decision_tree': make_pipeline(SelectFromModel(DecisionTreeClassifier(random_state=42)), DecisionTreeClassifier(random_state=42)),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"09037d6f1eb650866d5f1b9237cee280527a3e3d"},"cell_type":"code","source":"rf_hyperparameters = {\n    'randomforestclassifier__bootstrap': [True],\n    'randomforestclassifier__max_depth': [4, 5],\n    'randomforestclassifier__max_features': [4, 5],\n    'randomforestclassifier__min_samples_leaf': [3, 4],\n    'randomforestclassifier__min_samples_split': [8, 10],\n    'randomforestclassifier__n_estimators': [100]\n}\n\ngb_hyperparameters = {\n    'gradientboostingclassifier__max_depth': [4, 5],\n    'gradientboostingclassifier__max_features': [4, 5],\n    'gradientboostingclassifier__min_samples_leaf': [3, 4],\n    'gradientboostingclassifier__min_samples_split': [8, 10],\n    'gradientboostingclassifier__n_estimators': [100]\n}\n\ndecision_tree_hyperparameters = {\n    'decisiontreeclassifier__criterion': ['gini', 'entropy'],\n    'decisiontreeclassifier__max_depth': [ 6, 7, 8],\n    'decisiontreeclassifier__max_features': [6, 7, 8]\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f1d93b12a5ad7db7f2662cd5e8135595e72d853"},"cell_type":"markdown","source":"**Note:** The original set of parameters tested was much larger. However, to limit the runtime, we only report a small subset of possible hyperparameters."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e91fdf3152eb49344b6ee21e339132c62e2f2692"},"cell_type":"code","source":"hyperparameters = {\n    'rf': rf_hyperparameters,\n    'gb': gb_hyperparameters,\n    'decision_tree': decision_tree_hyperparameters\n}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ea6638858d8cc9ff81db783bd2edca50068a40b7"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\n\nfitted_models = {}\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nfor name, pipeline in pipelines.items():\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=cv, scoring='roc_auc', \n                         verbose=50, n_jobs=-1)\n    model.fit(X_train_prepared, y_train_balanced)\n    \n    fitted_models[name] = model\n    print(name, 'has been fitted.')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"903dbb1ba659b1229f332e69ef747d20416c442e"},"cell_type":"code","source":"# Best score for each model\nfor name, model in fitted_models.items():\n    print( name, model.best_score_ )","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"b78235bf06e592ddf5d052312f79937dc96a21d6"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,12))\nlist_of_models = {'rf': 'Random Forest', 'gb': 'Gradient Boosting', 'decision_tree': 'Decision Tree'}\n\nfor name, model in fitted_models.items():\n    y_pred = fitted_models[name].best_estimator_.predict(X_val_prepared)\n    fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n    \n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='%s: %0.2f' % (list_of_models[name], roc_auc), linewidth=5)\n    \nplt.title('Receiver Operating Characteristic')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.xlabel('False Positive Rate', fontsize=22)\nplt.ylabel('True Positive Rate', fontsize=22)\nplt.legend(loc=2)\n\nplt.savefig('roc_curves.jpg')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"a3467dd5210cb05127535a1a8aa7dd72bf2ddc9a"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfor name, model in fitted_models.items():\n    y_pred = fitted_models[name].best_estimator_.predict(X_val_prepared)\n    print(name, '\\n', classification_report(y_val, y_pred))\n    print(accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"043614c7707b433bd02052f5fd9c93f436be78b8"},"cell_type":"markdown","source":"# Testing the Best Model on the Test Set"},{"metadata":{"_uuid":"49ba2ed253e93a7e13c67d7ca0ffa4ac463b0a54"},"cell_type":"markdown","source":"**Note:** We train\nthe Gradient Boosting Classifier on the combined training and validation set using\nthe best hyperparameters found in the previous grid search"},{"metadata":{"trusted":true,"_uuid":"1ce25579acfc6913b7c12a244935f8d560c136f9"},"cell_type":"code","source":"X_final_train = np.concatenate((X_train_prepared, X_val_prepared), axis=0)\ny_final_train = pd.concat([y_train_balanced, y_val], axis=0)\nprint(X_final_train.shape)\nprint(y_final_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"10562da70761517396b38e6fd96afbb473740fa5"},"cell_type":"code","source":"# Best Parameters\nfitted_models['gb'].best_params_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"614d2295944213314b9b060433800283c732d0c1"},"cell_type":"code","source":"best_gb_params = {'gradientboostingclassifier__max_depth': [5],\n                  'gradientboostingclassifier__max_features': [5],\n                  'gradientboostingclassifier__min_samples_leaf': [4],\n                  'gradientboostingclassifier__min_samples_split': [8],\n                  'gradientboostingclassifier__n_estimators': [100]\n                 }\n\nfinal_gb_pipeline = make_pipeline(SelectFromModel(GradientBoostingClassifier()), \n                                  GradientBoostingClassifier(random_state=42))\n\nbest_gb = GridSearchCV(final_gb_pipeline, best_gb_params, cv=cv, scoring='roc_auc', \n                         verbose=50, n_jobs=-1)\nbest_gb.fit(X_train_prepared, y_train_balanced)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"7465c9eb3f918e70d543816c24c1137d0ef1da43"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,12))\n\ny_pred = best_gb.predict(X_test_prepared)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    \nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, label='Gradient Boosting: %0.2f' % (roc_auc))\n    \nplt.title('Receiver Operating Characteristic')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b9ec33032a8da0efdad832fef2646e652c692cce"},"cell_type":"code","source":"y_pred_test = best_gb.predict(X_test_prepared)\nprint(name, '\\n', classification_report(y_test, y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fcd4d148d21dc020896727d5229278d0ae02a63"},"cell_type":"code","source":"# Confusion matrix for the test set\nprint(confusion_matrix(y_test, y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e8b668fb99af734c949f476bbce6ca1a131488d"},"cell_type":"markdown","source":"The final model yields an average Recall\nof 0.69, and a Recall of the positive class of 0.73. This means that out of the 6573\ngenetic variants with conflicting assessment in the test set our final model captures\nca. 4800. Given the problem setting described above, we hope that this finding may be helpful for researchers and clinicians in identifying\nconflicting assessments of genetic variants. To further improve the prediction of genetic variants, one might try Support Vector Machines,\nArtificial Neural Networks, and Deep Learning. In addition, further preprocess-\ning steps based on better domain knowledge might lead to additional performance\nimprovements. In general, we hope that some of the ideas presented in this notebook will encourage people to contribute and advance solution to the problem at hand"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"04e2ea3190f0dbf17f7f9af7c47c66ecbb9e59b0"},"cell_type":"code","source":"# END","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}