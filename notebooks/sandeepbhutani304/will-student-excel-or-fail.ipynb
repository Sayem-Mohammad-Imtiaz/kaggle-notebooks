{"cells":[{"metadata":{"_uuid":"0d70fd7d3e3162f8bf3009b083b657d4dc9ba5e1"},"cell_type":"markdown","source":"# In this kernel I am going to classify the data in 3 categories **Fail, Medium and Good**  \nIf student final grade (0-20) is below 5 then fail, above 15 then Good else in Medium category\n\n## And then will predict which students are lying in which category and should be more focused. Will find out students which may fail so that attention can be paid to those failing students"},{"metadata":{"_cell_guid":"87372d85-e9ac-b3b3-c25d-33dfb357ccd8","_uuid":"cf26688056bac9eb10d60e374a61b254e937c06a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nsns.set_color_codes(\"pastel\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef0c99f37d39c78cfc845f46066f357df6d8f1b9"},"cell_type":"code","source":"import os\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a511c08-334d-d4bf-84c1-b4f3ac53b21e","_uuid":"6a440d739ad73577b00f5700d952527879245d95","trusted":true},"cell_type":"code","source":"# I'm use only student-mat.csv\ndata = pd.read_csv('../input/student-mat.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2034be1-8579-d4b9-9b18-b7bf2c6fc651","_uuid":"c38b08cb472a026b6cfe6da3eb1515f88a8988a3"},"cell_type":"markdown","source":"# Data Overview"},{"metadata":{"_cell_guid":"59eb5f69-14c4-8fda-9831-2173747d8d45","_uuid":"0d1ecf65896fa869859de06e4ce769f2ff2351c0","trusted":true},"cell_type":"code","source":"print(\"G3 range: Min={}, Max={}\".format(data[\"G3\"].min(), data[\"G3\"].max()))\ndata.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6f9700d8068016c00be2377812848d2cf77d0a6"},"cell_type":"markdown","source":"**Creating a new column G3_class with values - Fail, Medium and Good**  \nIf student final grade is below 5 then fail, above 15 then Good else in Medium category\n\n*In last print the new column - Scroll horizonatally in last to see the new values*"},{"metadata":{"trusted":true,"_uuid":"1380a268a91f751a31320655758dd64ff3e04194"},"cell_type":"code","source":"def create_g3_class(data):\n    return [\"Fail\", \"Medium\", \"Good\"][0 if data[\"G3\"] <= 5 else 1 if data[\"G3\"] <= 15 else 2]\n\ndata[\"G3_class\"] = data.apply(lambda row: create_g3_class(row), axis=1)\ndata.head()\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e9fb698-c819-074d-26ee-7a6afe00ad62","_uuid":"44032898e4ec8778d9eccd65cd76aa80e41938d4"},"cell_type":"markdown","source":"# Final Grade Prediction"},{"metadata":{"_cell_guid":"5730e570-9a3d-925c-c6fd-f96e44ac69a7","_uuid":"ed782d19877b8a82b7acde220da17c104e49243f"},"cell_type":"markdown","source":"Output target of this dataset is **Final Grade**. Let's use some regression model to predict it. I'll limit myself to 4 simple regression models (without searching the best parameters): decision tree regression, linear regression, lasso and ridge regression."},{"metadata":{"_cell_guid":"9d8e5350-8caa-ab7d-834c-bbaf3e30c2b1","_uuid":"fcdc84e0eb2bfe8d733c62df35ac29798ab64274"},"cell_type":"markdown","source":"## With G1 and G2 test results features"},{"metadata":{"_cell_guid":"35d7b6ae-6a83-d93d-0fa3-8502b8f9bee0","_uuid":"d72f730af51b88f2c2baa5b438c8365ecbb88e55","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\n\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03e74d0f-a241-a724-8b95-215bf4369d90","_uuid":"bd76959a0328d5f31b13f7c26598280c2ce2d83e","trusted":true},"cell_type":"code","source":"y = data['G3_class']\nX = data.drop(['G3', 'G3_class'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b494c8c-43ca-b815-add9-7f5dfe2a6b41","_uuid":"7b1a7006497363d0be74888d99d31adbebb98036","trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b38c54c5-b67b-d400-263f-445677e6f491","_uuid":"b893ad4bf7cf7e36a9ddb6e2e173d5f372245e4d","trusted":true},"cell_type":"code","source":"names = ['RandomForestClassifier', 'NaiveBayes' , 'DecisionTreeClassifier', 'XGBClassifier']\n\nclf_list = [RandomForestClassifier(),\n            MultinomialNB(),\n            DecisionTreeClassifier(),\n           xgb.XGBClassifier()]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a3aa3d6c-a320-82ca-365d-7ddeee162a74","_uuid":"c69ca8c16e7487b1a3a501e924c873cadb51b189","trusted":true,"scrolled":false},"cell_type":"code","source":"clf_scores = {}\nfor name, clf in zip(names, clf_list):\n    clf_scores[name]= cross_val_score(clf, X, y, cv=5).mean()\n    print(name, end=': ')\n    print(clf_scores[name])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"abc6eda6-3b62-e01d-3fa2-2de799d88081","_uuid":"5cba2f308cf735116da8ac67d22048c178eab4ab"},"cell_type":"markdown","source":"let's look at feature importances."},{"metadata":{"_cell_guid":"7847ea89-97f8-42b6-3095-e5a6eec8b9ce","_uuid":"d518a53db18d3f3f91e24fee3bcf52d643cf9691"},"cell_type":"markdown","source":"## Feature Importances"},{"metadata":{"trusted":true,"_uuid":"e4e233929eae4ab810a1acf8a146893efaa439c3"},"cell_type":"code","source":"best_classifier = sorted(clf_scores, key=clf_scores.get, reverse=True)[0]\nbest_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9af01111033eddfbf5c93a954a2de91fd2c90bf1"},"cell_type":"code","source":"clf = clf_list[names.index(best_classifier)]\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b50b217a4547141df23cd897e36187664176fa9"},"cell_type":"markdown","source":"Print main features contributing to top 1% for selection"},{"metadata":{"trusted":true,"_uuid":"28051078847ed28b86551327142245366296c42a"},"cell_type":"code","source":"importances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X.shape[1]):\n    if(importances[indices[f]] >= 0.01):\n        print(\"%d. Feature %s (%f)\" % (f + 1, X.columns.values[indices[f]], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"daf8ee9f-2d18-687c-ac41-cd738139e743","_uuid":"62e8573728587c3c9a8c7d323a8e1ecf126dd8ba"},"cell_type":"markdown","source":"We can see the top selection criteria is results achieved in last tests done by students.  \nOther than test resuts, students should also be told to focus on their - \n* Attendance\n* number of past class failures "},{"metadata":{"_cell_guid":"be01f1ef-e82b-a1da-3305-048c0c7e8f27","_uuid":"d23bf1d702b614c4b364e086cb34c5d86cf06763"},"cell_type":"markdown","source":"Let's look at scores of models, trained without G1 and G2 features (Test results)."},{"metadata":{"_cell_guid":"3013b854-7aaa-4ad6-7952-1d188121ea25","_uuid":"3fbaf8cc2f643399b1fd04923dc18050483abe31"},"cell_type":"markdown","source":"## Without G1 and G2 features"},{"metadata":{"_cell_guid":"e617ad36-9ccd-9da9-555b-9b5bc335202d","_uuid":"0278ec135c29d06eec5abdd39c3625127793ff7d","trusted":true},"cell_type":"code","source":"X = data.drop(['G3', 'G2', 'G1', 'G3_class'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a6e5eb9-4f9e-19bc-09cf-b2fe8046166a","_uuid":"3e693e509811a6d7716f7f461157bc2574db73d9","trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"90a0d376-7e65-04f3-0025-e5cbac01d592","_uuid":"8d25fea304ad9797ea9d8ca9893542424bec9772","trusted":true},"cell_type":"code","source":"clf_scores = {}\nfor name, clf in zip(names, clf_list):\n    clf_scores[name]= cross_val_score(clf, X, y, cv=5).mean()\n    print(name, end=': ')\n    print(clf_scores[name])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"095b566dcee941e897c6c8d8ce27a20f7a9ef49f"},"cell_type":"code","source":"best_classifier = sorted(clf_scores, key=clf_scores.get, reverse=True)[0]\nbest_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78d832728707be8075c94af58839d132562fc6dc"},"cell_type":"code","source":"clf = clf_list[names.index(best_classifier)]\nclf.fit(X, y)\n\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X.shape[1]):\n    if(importances[indices[f]] >= 0.01):\n        print(\"%d. Feature %s (%f)\" % (f + 1, X.columns.values[indices[f]], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1af97050-207d-fbcf-dcc3-a682fe7068d3","_uuid":"15ff4e6c34a7d9fa4d8d2cd914ccb12ce95456c9"},"cell_type":"markdown","source":"If test resuts are ignored then students should also be told to focus on their - \n\n1. Attendance\n2. Time spent on Outing \n3. Student with less and more age should focus more on studies\n4. number of past class failures \n5. Freetime after school\n6. Alcohol consumption\n7. studytime\n8. Maintenance of health\n9. Student with less educated mothers shoud focus more on studies"},{"metadata":{"_uuid":"c76127b3fbaecae5664fa9513416952e0b93b67f"},"cell_type":"markdown","source":"**Stritified Split the data in training, test **"},{"metadata":{"trusted":true,"_uuid":"642184f9589f7359b595e53bb7e1c1352e8fa233"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data.drop(['G3_class'], axis=1)\nX = pd.get_dummies(X)     #Convert to categorical\ny = data['G3_class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, stratify=y)\nX_train= X_train.drop(['G3'], axis=1)\nimport copy\nX_test_withG3 = copy.deepcopy(X_test)    #will be used in end to display actual G3 score\nX_test= X_test.drop(['G3'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1295f9e6bcae9cfbf3ed7958dfbdf7a1f9e51ed8"},"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea24133d57d71b91085ed4b4fc26b1ee646568c3"},"cell_type":"code","source":"clf = clf_list[names.index(best_classifier)]\nprint(\"using classifer: %s\"%best_classifier)\n# clf = xgb.XGBClassifier()\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nnp.set_printoptions(precision=2)\n# print(clf.classes_)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f17a619669a6315e118db2a84e18858635167065"},"cell_type":"markdown","source":"**As we can see in above confusion matrix there are a lot of wrong predictions (above and below diagnal).**  \n*Most probable reason of this is bias-ness in input data.* Let us verify this below"},{"metadata":{"trusted":true,"_uuid":"ef473316d2496b3becd6932aff4b795b4433bab6"},"cell_type":"code","source":"plt.figure()\nplt.boxplot(data['G3'], notch=True, sym='gD', vert=False)\nplt.title('G3 (final grade) score distribution in dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78557ea29438746371d18695af7af5368b85ccc9"},"cell_type":"markdown","source":"G3 output class is unevenly distributed. Above box plot shows that most values are in range approx 8-13.    \nBelow count plot also shows uneven distribution.  "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"481ea29628e8a0505f94016f7c472e172fd6a731"},"cell_type":"code","source":"p = sns.countplot(data['G3_class']).set_title('G3 class distribution')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"801123d2e35b7ba61b588a180133522a3857f0c9"},"cell_type":"markdown","source":"To counter this issue we need to oversample the data in classes which are having less samples.   \nimblearn package is used to do this."},{"metadata":{"trusted":true,"_uuid":"899072ace81ad2fc787aa3d8af56844b790f66a3"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom imblearn.under_sampling import CondensedNearestNeighbour, AllKNN, OneSidedSelection, RandomUnderSampler\nfrom imblearn.ensemble import BalanceCascade, EasyEnsemble\n\nfrom collections import Counter\n# X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)\nX_resampled, y_resampled = AllKNN(sampling_strategy=['Medium']).fit_resample(X_train, y_train)\n# X_resampled, y_resampled = SMOTETomek().fit_resample(X_train, y_train)  #sampling_strategy='minority'\n# X_resampled, y_resampled = EasyEnsemble().fit_resample(X_train, y_train)\n# X_resampled = X_resampled[0] ; y_resampled = y_resampled[0]\nprint(sorted(Counter(y_resampled).items()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"281ab4b699fdf45d8a9d88e45f4cd77395b5e655"},"cell_type":"markdown","source":"Apply PCA to find out most relevant features"},{"metadata":{"trusted":true,"_uuid":"51f241851b2100da6647a06d4813398feb4dad13"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(X_resampled) #pd.get_dummies(data)\nprincipalDf_train = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\nprincipalComponentstest = pca.fit_transform(X_test) #pd.get_dummies(data)\nprincipalDf_test = pd.DataFrame(data = principalComponentstest\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b330853dc8fa997d2ff90baa6724659d9665704","scrolled":false},"cell_type":"code","source":"clf = clf_list[names.index(best_classifier)]\nprint(\"Using classifier: %s\"%best_classifier)\n# clf = xgb.XGBClassifier()\n# clf = DecisionTreeClassifier()  #Using for demo and consistency\nclf.fit(principalDf_train, y_resampled)\n\ny_pred = clf.predict(principalDf_test)\ny_pred_prob = clf.predict_proba(principalDf_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nnp.set_printoptions(precision=2)\n# print(clf.classes_)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=clf.classes_, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ca11b0d4c8fa674dad96bc6fd1e6fa9c546fd5f"},"cell_type":"markdown","source":"Much better prediction results after over and under sampling   \n# Let us show the students which are in danger zone"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"daa266d84272b2887dc48fd5a2d60b5c07e55c28"},"cell_type":"code","source":"y_test_list = list(y_test)\nX_test_withG3_list = list(X_test_withG3['G3'])\nfor idx, item in enumerate(y_pred):\n    if(item == 'Fail'):\n        print(\"Student {} \\t [Actual Failed?: {}  \\tG3: {}]\".format(idx, y_test_list[idx], X_test_withG3_list[idx]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5a8b995db368fa540960063c182641f8ce5bc69"},"cell_type":"markdown","source":"# Few points to note and justification\n## * Undersampling is used on Fail class students as these are the students which should be focused\n## * The data is not accurate thereby resulting some incorrect prediction in above results\n## * **The results above contain failed students and a lot of students with Medium G3 scores also. The choice was to reduce the Medium G3 scorers and miss some Failed students too.  Important point is no/minimum Failed students are missed (As seen in last confusion matrix), So I resampled focusing on Failed classes.     It is important to focus on NOT MISSING ANY FAILING STUDENT, and focusing on Medium students will make them only better.**"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}