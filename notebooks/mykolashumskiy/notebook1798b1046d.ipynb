{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Churn for Bank Costumers Analysis  \nM.Shumskiy 31/12/2020  \n## Abstract  \nIn this project I analyze customer retention, determine the most relevant parameters that retain customers, implement several Machine Learning Classification Algorithms and compare to determine which better performs the classification.  \nThis project has application across a vast number of enterprises which concern themselves with client retention.  \nIt is worthy of note that this project is also of help to better plan and employ marketing campaigns to attract new customers.  \nWith this in mind, the work developed here is of great value.  \nThe analysis as well as the code is presented in this work.\n\n## 1. Introduction  \nClient retention is a subject of great importance to many enterprises, for it increases a company's performance in the market which, in turn, reflects in higher revenue and overall growth of said company.  \nTo extract insights related to the topic here discussed, data from existing customers and former costumers must be analyzed, with the proper tools, to determine the most relevant parameters in their decision to leave or to stay.  \nSince every enterprise of success must look towards the future, Machine Learning Algorithms must be implemented to give prediction power, on which the company rely.\n \n## 2. Methodology \nThe method of analysis consists of the following phases:  \n 1. Data Cleaning,  \n 2. Exploratory Data Analysis,  \n 3. Machine Learning Classification Algorithms implementation and performance comparison,  \n 4. Implementation of the most accurate Classification Algorithm.  \n\n### 2.1. The Data  \nWas analyzed data from Kaggle named Churn for Bank Customers by Mehmet A. The data looks this way:"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nfile_path=r'C:\\Users\\Pc\\Desktop\\Data Science\\Projects\\Churn for Bank Customers\\churn.csv'\ndata=pd.read_csv(file_path)\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the bank costumer data is composed of 10000 costumerâ€™s info spread by 13 columns (the first being redundant).  \n### 2.2. Data Cleaning\nFirst, I checked if there were any Nan values."},{"metadata":{"trusted":false},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it is observed, there are none. So now, some redundant columns need to be discarded."},{"metadata":{"trusted":false},"cell_type":"code","source":"data.drop(columns=['RowNumber','CustomerId','Surname'],inplace=True)\ndata_3=data.copy() # copy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the relation between client exits."},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nchurn_pos=len(data[data['Exited']==1])\nchurn_neg=len(data[data['Exited']==0])\nratio=(churn_pos/churn_neg)\nprint('The ration between the positive and negative outcomes of the variable Exited is ',round(ratio,3))\n\ndata['Exited'].hist()\nplt.title('Comparisson between churn values')\nplt.xlabel('Churn')\nplt.ylabel('Number of occurences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is observed that 25.6 % of costumers left the bank.  \nThis ratio will serve as one of the criteria to determine parameter significance later on.  \nNow columns as **Credit Score**,**Balance** and **Estimated Salary** must be categorized, for this let's examine the following graphs."},{"metadata":{"trusted":false},"cell_type":"code","source":"column=['CreditScore', 'Balance', 'EstimatedSalary']\nfor col in column:\n    plt.title('ovwerview of {}'.format(col))\n    data.boxplot(column=[col])\n    data.hist(column=[col],bins=20)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis of these graphs, the categorization of the parameters will follow this method: [x0 ,xf ,dx], where:  \n - x0 represents the smaller value of the parameter,  \n - xf represents the highest value of the parameter,  \n - dx represents the increment, i.e., the width of each category.  \n \nAnd the parameters will be categorized following these values:  \n - Credit Score: [350, 850, 25],  \n - Balance: [0, 260000, 15000],  \n - Estimated Salary: [0, 200000, 25000]  \n \nBut first, the following functions will be created:  \n - **bin_creator** : creates the slices,  \n - **label_creator** : creates the labels for the slices.  \n \nAnd the values of **Balance** and **Estimated Salary** will be rounded."},{"metadata":{"trusted":false},"cell_type":"code","source":"def bin_creator(x0,xf,dx):\n    bins=[x0]\n    n_bins=abs(xf-x0)//dx\n    i=1\n    while i<=n_bins:\n        bins.append(bins[i-1]+dx)\n        i+=1\n    return bins\ndef label_creator(x0,xf,dx):\n    labels=[]\n    n_bins=abs(xf-x0)//dx\n    i=1\n    while i<=n_bins:\n        labels.append('{},{}'.format(str(x0+(i-1)*dx),str(x0+i*dx)))\n        i+=1\n    return labels\ndata.Balance=data.Balance.round(0)\ndata.EstimatedSalary=data.EstimatedSalary.round(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's categorize the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"values=[[350,850,25],[0,260000,15000],[0,200000,25000]]\ncolumns=['CreditScore','Balance','EstimatedSalary']\ni=0\nfor column in columns:\n    category=pd.cut(data[column],bins=bin_creator(values[i][0],values[i][1],values[i][2]),labels=label_creator(values[i][0],values[i][1],values[i][2]))\n    data.insert(11+i,'{} cat'.format(column),category)\n    i+=1\ndata['Balance cat'].fillna('0,15000',inplace=True) # where balance=0 the category will be Nan\ndata.drop(columns=['CreditScore','Balance','EstimatedSalary'],inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the dataframe is more friendly to analyze.  \n### 2.3. Exploratory Data Analysis  \nLets take a look at the information that lies in the dataframe."},{"metadata":{"trusted":false},"cell_type":"code","source":"import seaborn as sns\n\nfor col in ['Geography', 'Gender', 'Age', 'Tenure',\n       'NumOfProducts', 'HasCrCard', 'IsActiveMember','CreditScore cat','Balance cat','EstimatedSalary cat',\n       'Exited']:\n    if col=='Exited':\n        break\n    else:\n        pd.crosstab(data[col],data['Exited']).plot(kind='bar',color=['g','r'],figsize=(10, 6))\n        plt.title('Client exits in relation to {}'.format(col))\n        plt.xlabel(col)\n        plt.ylabel('Number of occurences in Exited')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis of the graphics we can see the following:  \n - Geography: the French tend to stay with the bank,  \n - Gender: males tend to stay with the bank,  \n - Age: There seem to be 2 distributions, apparently Poisson distributions with 2 different average values for those clients who stayed and those who left,  \n - Tenure: does not seem to be related to exits,  \n - NumOfProducts: People with 2 products tend to stay,  \n - HasCrCard: presence of a credit card does not seem to affect the exits,  \n - IsActiveMember: active members seem to stay but the difference is small,  \n - CreditScore cat: credit score does not seem to affect the exists (all data entries follow the ratio between exits and stays),  \n - Balance cat: balance does not seem to affect the exists (all data entries follow the ratio between exits and stays),  \n - EstimatedSalary cat: estimated salary does not seem to affect the exists (all data entries follow the ratio between exits and stays).  \n \nWith these points in mind, we can predict that the most relevant factors in the decision to stay or leave will be:  \n - Age,  \n - Activity,  \n - Gender,  \n - Geography.  \n \nTo calculate correlation, we need to convert string values to integer values. To do such, we must prepare the dataframe:  \n - The categorical columns (Balance cat, CreditScore cat, EstimatedSalary cat) must be converted to integer,  \n - Any Nan values must be dealt with."},{"metadata":{"trusted":false},"cell_type":"code","source":"data_4=data_3\ndef bin_creator(x0,xf,dx):\n    bins=[x0]\n    n_bins=abs(xf-x0)//dx\n    i=1\n    while i<=n_bins:\n        bins.append(bins[i-1]+dx)\n        i+=1\n    return bins\ndef label_creator(x0,xf,dx):\n    labels=[]\n    n_bins=abs(xf-x0)//dx\n    i=1\n    while i<=n_bins:\n        labels.append(str(i-1)) #here is the change. This gives an integer instead of interval\n        i+=1\n    return labels\nvalues=[[350,850,25],[0,260000,15000],[0,200000,25000]]\ncolumns=['CreditScore','Balance','EstimatedSalary']\ni=0\nfor column in columns:\n    category=pd.cut(data_3[column],bins=bin_creator(values[i][0],values[i][1],values[i][2]),labels=label_creator(values[i][0],values[i][1],values[i][2]))\n    data_4.insert(11+i,'{} cat'.format(column),category)\n    i+=1\n    \ndata_4['Balance cat'].fillna('0',inplace=True)\ndata_4['CreditScore cat'].fillna('0',inplace=True)\n\ndata_map_geo={'France':0,'Germany':1,'Spain':2}\ndata_map_gender={'Female':0,'Male':1}\ndata_map_credscore={}\ndata_4['Geography'],data_4['Gender']=data_4['Geography'].map(data_map_geo),data_4['Gender'].map(data_map_gender)\n\ndata_5=data_4[['Geography', 'Gender', 'Age', 'Tenure',\n       'NumOfProducts', 'HasCrCard', 'IsActiveMember','CreditScore cat', 'Balance cat', 'EstimatedSalary cat','Exited']]\ndata_5=data_5.astype({'Balance cat': 'int64','CreditScore cat':'int64','EstimatedSalary cat':'int64'})\n\ndata_5.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the dataframe is suited for correlation calculation."},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a dataframe only for the parameters\ndata_6=data_5[['Geography', 'Gender', 'Age', 'Tenure',\n       'NumOfProducts', 'HasCrCard', 'IsActiveMember','CreditScore cat', 'Balance cat', 'EstimatedSalary cat']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"abscorwithdep=[]\nfor var in data_6.columns:\n    if var=='Exited':\n        break\n    else:\n        abscorwithdep.append((abs(data_5['Exited'].corr(data_6[var]))))\n    \nparameters=data_6.columns.to_list()\ncorr_table={'parameters':parameters,\n           'corr':abscorwithdep}\n\ncorr_table_df=pd.DataFrame.from_dict(corr_table)\ncorr_table_df.sort_values('corr', ascending=False,inplace=True)\n\nsns.barplot(x='corr',y='parameters',data=corr_table_df)\nplt.title('Absolute Correlation between parameters and Exits')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure()\ncorrelation_matrix = data_6.corr().abs()\nsns.heatmap(correlation_matrix,cmap='Blues')\nplt.title('Correlation matrix between parameters',fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that there are no crossed correlations between parameter, so no corrections must be made here.  \nHaving in mind the small percentage of clients that left the bank, I implement the SMOTE (Synthetic Minority Oversampling Technique) to correct the discrepancy."},{"metadata":{"trusted":false},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nX=data_6\nY=data['Exited']\n\nos=SMOTE(random_state=0)\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)\ncolumns=X_train.columns\n\nos_data_X,os_data_Y=os.fit_sample(X_train,Y_train)\nos_data_X=pd.DataFrame(data=os_data_X,columns=columns)\nos_data_Y=pd.DataFrame(data=os_data_Y,columns=['Exited'])\n\nprint('lenght of oversampled data is',len(os_data_X))\nprint('lenght of exits=0 in oversampled data',len(os_data_Y[os_data_Y['Exited']==0]))\nprint('proportion of exits=0 data in oversampled data is ',len(os_data_Y[os_data_Y['Exited']==0])/len(os_data_X))\nprint('proportion of exits=1 data in oversampled data is ',len(os_data_Y[os_data_Y['Exited']==1])/len(os_data_X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the data is as clean as it can get and corrected for exits discrepancies for Machine Learning Algorithms implementation.  "},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Machine Learning Classification Algorithms implementation and performance comparison  \nThe classification algorithms to be used are the following:  \n - Naive Bayes,  \n - Random Forest Classifier,   \n - Logistic Regression Classifier,  \n - K Nearest Neighbors."},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.1. Naive Bayes"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(os_data_X, os_data_Y)\ny_pred=nb.predict(X_test)\n\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.2. Random Forest Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(os_data_X, os_data_Y)\ny_pred=clf.predict(X_test)\n\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.3. Logistic Regression Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg=LogisticRegression()\nlogreg.fit(os_data_X,os_data_Y)\ny_pred=logreg.predict(X_test)\n\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.4. K Nearest Neighbors"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=2).fit(os_data_X, os_data_Y)\ny_pred=knn.predict(X_test)    \n\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.5. Comparison of the models"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n# Naive Bayes\nnb_roc_auc=roc_auc_score(Y_test,nb.predict(X_test))\nfpr_nb,tpr_nb,thresholds_nb=roc_curve(Y_test,nb.predict_proba(X_test)[:,1])\n# Random Forest\nrf_roc_auc=roc_auc_score(Y_test,clf.predict(X_test))\nfpr_rf,tpr_rf,thresholds_rf=roc_curve(Y_test,clf.predict_proba(X_test)[:,1])\n\n# Logistic Regression\nlogit_roc_auc=roc_auc_score(Y_test,logreg.predict(X_test))\nfpr,tpr,thresholds=roc_curve(Y_test,logreg.predict_proba(X_test)[:,1])\n\n# K Nearest Neighbors\nknn_roc_auc=roc_auc_score(Y_test,knn.predict(X_test))\nfpr_knn,tpr_knn,thresholds_knn=roc_curve(Y_test,knn.predict_proba(X_test)[:,1])\n\nplt.figure(figsize=(10, 6))\nplt.plot(fpr_nb,tpr_nb,label='Naive Bayes (area=%0.2f)' % nb_roc_auc)\nplt.plot(fpr_rf,tpr_rf,label='Random Forest Classification (area=%0.2f)' % rf_roc_auc)\nplt.plot(fpr,tpr,label='Logistic Regression (area=%0.2f)' % logit_roc_auc)\nplt.plot(fpr_knn,tpr_knn,label='K Nearest Neighbors (area=%0.2f)' % knn_roc_auc)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Reciever Operating Characteristic Comparisson')\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the **Random Forest Classifier** outperforms the other models and it's this model that will be selected."},{"metadata":{},"cell_type":"markdown","source":"## 3. Results  \nFrom the Exploratory Data Analysis, the correlation between parameters and exists is displayed in the following graph:"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.barplot(x='corr',y='parameters',data=corr_table_df)\nplt.title('Absolute Correlation between parameters and Exits')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the performance of several Machine Learning algorithms is displayed on the following graph:"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(fpr_nb,tpr_nb,label='Naive Bayes (area=%0.2f)' % nb_roc_auc)\nplt.plot(fpr_rf,tpr_rf,label='Random Forest Classification (area=%0.2f)' % rf_roc_auc)\nplt.plot(fpr,tpr,label='Logistic Regression (area=%0.2f)' % logit_roc_auc)\nplt.plot(fpr_knn,tpr_knn,label='K Nearest Neighbors (area=%0.2f)' % knn_roc_auc)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Reciever Operating Characteristic Comparisson')\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Conclusion  \nThis project was developed with the intent to determine the most relevant parameters in client exits, as well as implementation of the Machine Learning model with best performance.  \nThe parameters with most relevance for the client decision to either leave or stay are ordered as follow:  \n 1. Age, the younger people tend to stay and displayed interesting distributions worth examining in the future,  \n 2. Level of Activity, the more active members tend to stay,  \n 3. Account Balance, clients with lower balance tend to stay,  \n 4. Gender, the men tend to stay more than women.  \n \nAs for the Machine Learning model, the **Random Forest Classifier** performed the best, therefore, it shall be the model to be implemented.  \nIt is worth to note of the uses of this project. With this information the bank can determine it's focus points to retain clients, as well as to direct its marketing campaigns to acquire new ones. This work can be, easily, applied to other enterprises.\n### 5. References  \n - Metrics to Evaluate your Machine Learning Algorithm by Aditya Mishra,  \n - An in-depth guide to supervised machine learning classification by Badreesh Shetty,  \n - Python Data Science Handbook by Jake VanderPlas,  \n - Personnel / Client Retention Study by M.Shumskiy"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}