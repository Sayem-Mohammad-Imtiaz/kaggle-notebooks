{"cells":[{"metadata":{"id":"47ZXaFl2mxn1"},"cell_type":"markdown","source":"* Data exploration and pre-processing\n\n* Develop a pipeline to carry out classification\n\n* Compare different metrics and classifiers\n\nThe objective is to predict whether or not a credit card client will default for their payment in the next month. We will be using the better of 2 classifiers namely, Random Forest and KNN Classifier, and determine the best of a given set of hyperparameters by using grid search.\n"},{"metadata":{"id":"QO0ED3HOmxn6","trusted":true},"cell_type":"code","source":"## Use this for consistency in graphs through out the notebook\nimport numpy as np\nimport pandas as pd\n\n# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot as plot\nimport seaborn as sns \nfrom matplotlib import gridspec \nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":null,"outputs":[]},{"metadata":{"id":"58b9hQq3mxoF","scrolled":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import randint\nfrom pandas import set_option\nplt.style.use('ggplot') # nice plots\n\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import KFold # for cross validation\nfrom sklearn.model_selection import learning_curve, train_test_split,GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score, mean_absolute_error, f1_score, roc_curve, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom numpy import bincount, linspace, mean, std, arange, squeeze\n\nimport itertools, time, datetime\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After checking the summary of missing value in the dataset, the result shows that the data has no missing values so that the data is ready to the next stage."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Few attributes require change in name and we can remove ID."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={\"PAY_0\":\"PAY_1\", \"default.payment.next.month\": \"DEFAULT\"}, inplace = True)\ndf.drop('ID', axis = 1, inplace =True) # drop column \"ID\"\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATA CLEANING"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.EDUCATION.unique())\nprint(df.MARRIAGE.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.countplot(data=df,x='EDUCATION', order = df['EDUCATION'].value_counts().index, color='salmon')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.countplot(data=df, x='MARRIAGE', order = df['MARRIAGE'].value_counts().index, color='salmon')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There exists values 0, 5 and 6 in Education column.\n#### Since these are unknown (undefined), they can be grouped into the category 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"fil = (df['EDUCATION'] == 5) | (df['EDUCATION'] == 6) | (df['EDUCATION'] == 0)\ndf.loc[fil, 'EDUCATION'] = 4\ndf['EDUCATION'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are some discrepancies in the marriage column\n#### There exists 0 in this column\n#### Since this is unknown (undefined), similar to education, they can be grouped into 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"fil = (df['MARRIAGE'] == 0)\ndf.loc[fil, 'MARRIAGE'] = 3\ndf['MARRIAGE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.EDUCATION.unique())\nprint(df.MARRIAGE.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AGE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating a Function to Distribute the Age\ndef func(x):\n    if(x >=20 and x<30 ):\n        return 1\n    elif(x>=30 and x<40):\n        return 2\n    elif(x>=40 and x<50):\n        return 3\n    elif(x>=50 and x<60):\n        return 4\n    elif(x>=60 and x<=80):\n        return 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the function\ndf['AGE'] = df['AGE'].apply(func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.countplot(data=df,x='AGE', order = df['AGE'].value_counts().index, color='salmon');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATA PREPROCESSING"},{"metadata":{},"cell_type":"markdown","source":"## MAPPING THE TARGET"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The frequency of defaults\nyes = df.DEFAULT.sum()\nno = len(df)-yes\n\n# Percentage\nyes_perc = round(yes/len(df)*100, 1)\nno_perc = round(no/len(df)*100, 1)\n\nimport sys \nplt.figure(figsize=(7,4))\nsns.set_context('notebook', font_scale=1.2)\nsns.countplot('DEFAULT',data=df, palette=\"Blues\")\nplt.annotate('Non-default: {}'.format(no), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\nplt.annotate('DEFAULT: {}'.format(yes), xy=(0.7, 15000), xytext=(0.7, 3000), size=12)\nplt.annotate(str(no_perc)+\" %\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\nplt.annotate(str(yes_perc)+\" %\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\nplt.title('COUNT OF CREDIT CARDS', size=14)\n#Removing the frame\nplt.box(False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mapping the target: categorizing From this sample of 30,000 credit card holders, there were 6,636 default credit cards; that is, the proportion of default in the data is 22,1%. We can see that they are almost 4 times as many non defaulters as there are defaulters. Hence there is a clear non uniform division in classes"},{"metadata":{},"cell_type":"markdown","source":"Upon closer inspection of our dataset, we can see that there is some class imbalance, something of which we have to keep in mind when evaluating the efficacy of our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new dataframe with just the categorical explanatory variables\ndf_categorical = df[['SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'\n                     ,'DEFAULT']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 3, figsize=(19,14), facecolor='white')\nf.suptitle(\"FREQUENCY OF CATEGORICAL VARIABLES (BY TARGET)\",size=20)\n\n# Creating plots of each categorical variable to target \nax1 = sns.countplot(x='SEX', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,0])\nax2 = sns.countplot(x='EDUCATION', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,1])\nax3 = sns.countplot(x='MARRIAGE', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,2])\nax4 = sns.countplot(x='PAY_1', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,0])\nax5 = sns.countplot(x='PAY_2', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,1])\nax6 = sns.countplot(x='PAY_3', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,2])\nax7 = sns.countplot(x='PAY_4', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,0])\nax8 = sns.countplot(x='PAY_5', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,1])\nax9 = sns.countplot(x='PAY_6', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,2])\nax10 = sns.countplot(x='AGE', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,2])\n\n# Setting legends to upper right\nax1.legend(loc=\"upper right\")\nax2.legend(loc=\"upper right\")\nax3.legend(loc=\"upper right\")\nax4.legend(loc=\"upper right\")\nax5.legend(loc=\"upper right\")\nax6.legend(loc=\"upper right\")\nax7.legend(loc=\"upper right\")\nax8.legend(loc=\"upper right\")\nax9.legend(loc=\"upper right\")\nax10.legend(loc=\"upper right\")\n\n# Changing ylabels to horizontal and changing their positions\nax1.set_ylabel('COUNTS', rotation=0, labelpad=40)  # Labelpad adjusts distance of the title from the graph\nax1.yaxis.set_label_coords(-0.1,1.02)              # (x, y)\nax2.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax2.yaxis.set_label_coords(-0.1,1.02)\nax3.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax3.yaxis.set_label_coords(-0.1,1.02)\nax4.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax4.yaxis.set_label_coords(-0.1,1.02)\nax5.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax5.yaxis.set_label_coords(-0.1,1.02)\nax6.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax6.yaxis.set_label_coords(-0.1,1.02)\nax7.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax7.yaxis.set_label_coords(-0.1,1.02)\nax8.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax8.yaxis.set_label_coords(-0.1,1.02)\nax9.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax9.yaxis.set_label_coords(-0.1,1.02)\nax10.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax10.yaxis.set_label_coords(-0.1,1.02)\n\n# Shifting the Super Title higher\nf.tight_layout()  # Prevents graphs from overlapping with each other\nf.subplots_adjust(top=0.9);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate binary values using get_dummies\nage = pd.get_dummies(df['AGE'], prefix='AGE' )\nmr = pd.get_dummies(df['MARRIAGE'], prefix='MARRIAGE' )\ned = pd.get_dummies(df['EDUCATION'],prefix='EDUCATION')\n# merge with main df bridge_df on key values\ndf = df.join(age)\ndf = df.join(mr)\ndf= df.join(ed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['AGE','MARRIAGE','EDUCATION'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['DEFAULT'].value_counts(),'\\n')\nprint(len(df['DEFAULT']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freq distribution of all data\nfig, ax = plt.subplots(figsize=(15,15))\npd.DataFrame.hist(df,ax=ax)\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Can we infer more? what about the columns for lIMIT_BALANCE?\nx1 = list(df[df['DEFAULT'] == 1]['LIMIT_BAL'])\nx2 = list(df[df['DEFAULT'] == 0]['LIMIT_BAL'])\n\nfig2, ax_lim_bal = plt.subplots(figsize=(12,4))\nsns.set_context('notebook', font_scale=1.2)\nsns.set_color_codes(\"pastel\")\nplt.hist([x1, x2], bins = 40, density=False, color=['firebrick', 'salmon'])\nplt.xlim([0,600000])\nplt.legend(['Yes', 'No'], title = 'Default', loc='upper right', facecolor='white')\nplt.xlabel('Limit Balance (NT dollar)')\nplt.ylabel('Frequency', rotation=0,labelpad=40)\nplt.title('LIMIT BALANCE HISTOGRAM BY TYPE OF CREDIT CARD', SIZE=15)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we want see the correlation between all of features and label in the dataset by using Pearson Correlation\nplt.figure(figsize=(14,14))\ncor = df.iloc[:,1:].corr()\nx = cor [['DEFAULT']]\nsns.heatmap(x, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the plot above, the repayment status of customers (PAY_1 - PAY_6) have the higher correlation towards the label (default) in compared to other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,20))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.corr()\ndf_default_corrs = data.iloc[:-1,-1:]\ndf_default_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_default_corrs.plot(kind='bar',figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CLASSIFICATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CodeTimer:\n    \n    \"\"\"\n        Utility custom contextual class for calculating the time \n        taken for a certain code block to execute\n    \n    \"\"\"\n    def __init__(self, name=None):\n        self.name = \" '\"  + name + \"'\" if name else ''\n\n    def __enter__(self):\n        self.start = time.perf_counter()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.took = (time.perf_counter() - self.start) * 1000.0\n        time_taken = datetime.timedelta(milliseconds = self.took)\n        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_y_target = df['DEFAULT'].values\n\ncolumns = df.columns.tolist()\ncolumns.remove('DEFAULT')\n\n_x_attributes = df[columns].values\n\n\n## meaning of stratify = _y_target. returns test and training data having the same proportions of class label '_y_target'\n_x_train,_x_test,_y_train, _y_test = train_test_split(_x_attributes, _y_target, test_size =0.20, stratify = _y_target, random_state = 123)\n\n## lets check the distribution. we can see 4times the lower value as was the case before as well. train/test set distributed well\nprint(\"label counts in y train %s\" %bincount(_y_train))\nprint(\"label counts in y test %s\" %bincount(_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cv is essentially value of K in k fold cross validation\n    \n## n_jobs = 1 is  non parallel execution    , -1 is all parallel , any other number say 2 means execute in 2 cpu cores\n\ndef plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  k_fold = 5, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n    \n    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n                                                                X = _x_train, \\\n                                                                y = _y_train, \\\n                                                                train_sizes = training_sample_sizes, \\\n                                                                cv = k_fold, \\\n                                                                n_jobs = jobsInParallel) \n\n\n    training_mean = mean(training_score, axis = 1)\n    training_std_deviation = std(training_score, axis = 1)\n    testing_std_deviation = std(testing_score, axis = 1)\n    testing_mean = mean(testing_score, axis = 1 )\n\n    ## we have got the estimator in this case the perceptron running in 5 fold validation with \n    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n    ## to see the accuracy results and check for bias vs variance\n\n    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n\n    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n\n    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n    \n    plot.title(\"Scoring of our training and testing data vs sample sizes\")\n    plot.xlabel(\"Number of Samples\")\n    plot.ylabel(\"Accuracy\")\n    plot.legend(loc= 'best')\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runGridSearchAndPredict(pipeline, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 5, score = 'accuracy'):\n    \n    response = {}\n    training_timer       = CodeTimer('training')\n    testing_timer        = CodeTimer('testing')\n    learning_curve_timer = CodeTimer('learning_curve')\n    predict_proba_timer  = CodeTimer('predict_proba')\n    \n    with training_timer:\n        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n\n        search = gridsearch.fit(x_train,y_train)\n\n        print(\"Grid Search Best parameters \", search.best_params_)\n        print(\"Grid Search Best score \", search.best_score_)\n        \n    with testing_timer:\n        y_prediction = gridsearch.predict(x_test)\n            \n    print(\"Accuracy score %s\" %accuracy_score(y_test,y_prediction))\n    print(\"F1 score %s\" %f1_score(y_test,y_prediction))\n    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n    \n    with learning_curve_timer:\n        plotLearningCurve(_x_train, _y_train, search.best_estimator_)\n        \n    with predict_proba_timer:\n        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n            \n            y_probability = gridsearch.predict_proba(x_test)\n            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probability[:,1])\n            response['roc_auc_score'] = roc_auc_score(y_test, y_probability[:,1])\n            response['roc_curve'] = (false_positive_rate, true_positive_rate)\n    \n        else:\n            \n            response['roc_auc_score'] = 0\n            response['roc_curve'] = None\n    \n    response['learning_curve_time'] = learning_curve_timer.took\n    response['testing_time'] = testing_timer.took\n    response['_y_prediction'] = y_prediction\n    response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n    response['training_time'] = training_timer.took\n    response['f1_score']  = f1_score(y_test, y_prediction)\n    \n    \n    return response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plot.cm.Blues):\n    \"\"\"\n    \"This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\"\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    \n    print(cm)\n\n    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n    plot.title(title)\n    plot.colorbar()\n    tick_marks = arange(len(classes))\n    plot.xticks(tick_marks, classes, rotation=45)\n    plot.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plot.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plot.ylabel('True label')\n    plot.xlabel('Predicted label')\n    plot.tight_layout()\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [\n    RandomForestClassifier(random_state = 123, criterion = 'gini'),\n    KNeighborsClassifier(metric = 'minkowski'),\n       \n]\n\n\nclassifier_names = [\n            'randomforestclassifier',\n            'kneighborsclassifier',              \n]\n\nclassifier_param_grid = [\n            {'randomforestclassifier__n_estimators':[4, 5, 10, 20, 50]} ,\n            {'kneighborsclassifier__n_neighbors':[3,5,10,20]},\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Used Standard scaler for scaling. "},{"metadata":{"trusted":true},"cell_type":"code","source":"timer = CodeTimer(name='overalltime')\nmodel_metrics = {}\n\nwith timer:\n    for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n\n        pipeline = Pipeline([\n                ('scaler', StandardScaler()),\n                (model_name, model)\n        ])\n\n        result = runGridSearchAndPredict(pipeline,_x_train, _y_train, _x_test, _y_test, model_param_grid , score = 'f1')\n\n        _y_prediction = result['_y_prediction']\n\n        _matrix = confusion_matrix(y_true = _y_test ,y_pred = _y_prediction)\n\n        model_metrics[model_name] = {}\n        model_metrics[model_name]['confusion_matrix'] = _matrix\n        model_metrics[model_name]['training_time'] = result['training_time']\n        model_metrics[model_name]['testing_time'] = result['testing_time']\n        model_metrics[model_name]['learning_curve_time'] = result['learning_curve_time']\n        model_metrics[model_name]['accuracy_score'] = result['accuracy_score']\n        model_metrics[model_name]['f1_score'] = result['f1_score']\n        model_metrics[model_name]['roc_auc_score'] = result['roc_auc_score']\n        model_metrics[model_name]['roc_curve'] = result['roc_curve']\n        \n        \nprint(timer.took)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_estimates = pd.DataFrame(model_metrics).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## convert model_metrics into panda data frame\n## print out across model estimations and accuracy score bar chart\n\n\nmodel_estimates['learning_curve_time'] = model_estimates['learning_curve_time'].astype('float64')\nmodel_estimates['testing_time'] = model_estimates['testing_time'].astype('float64')\nmodel_estimates['training_time'] = model_estimates['training_time'].astype('float64')\nmodel_estimates['f1_score'] = model_estimates['f1_score'].astype('float64')\nmodel_estimates['roc_auc_score'] = model_estimates['roc_auc_score'].astype('float64')\n\n#scaling time parameters between 0 and 1\nmodel_estimates['learning_curve_time'] = (model_estimates['learning_curve_time']- model_estimates['learning_curve_time'].min())/(model_estimates['learning_curve_time'].max()- model_estimates['learning_curve_time'].min())\nmodel_estimates['testing_time'] = (model_estimates['testing_time']- model_estimates['testing_time'].min())/(model_estimates['testing_time'].max()- model_estimates['testing_time'].min())\nmodel_estimates['training_time'] = (model_estimates['training_time']- model_estimates['training_time'].min())/(model_estimates['training_time'].max()- model_estimates['training_time'].min())\n\nprint(model_estimates)\nmodel_estimates.plot(kind='barh',figsize=(12, 10))\nplot.title(\"Scaled Estimates across different classifiers used\")\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotROCCurveAcrossModels(positive_rates_sequence, label_sequence):\n    \n\n    for plot_values, label_name in zip(positive_rates_sequence, label_sequence):\n        \n        plot.plot(list(plot_values[0]), list(plot_values[1]),  label = \"ROC Curve for model: \"+label_name)\n        \n    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n    plot.title('ROC Curve across models')\n    plot.xlabel('False Positive Rate')\n    plot.ylabel('True Positive Rate')\n    plot.legend(loc='best')\n    plot.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_curve_input = {}\nfor i , j in enumerate(model_metrics):\n    \n    _matrix = model_metrics[j]['confusion_matrix']\n    plot_confusion_matrix(_matrix, classes = [0,1], title = 'Confusion Matrix for %s'%j)\n    if model_metrics[j]['roc_curve']:\n        roc_curve_input[j]= model_metrics[j]['roc_curve']\n    \n\nplotROCCurveAcrossModels(list(roc_curve_input.values()), list(roc_curve_input.keys()))","execution_count":null,"outputs":[]},{"metadata":{"id":"LqM6c_oRmxoJ"},"cell_type":"markdown","source":"#### Conclusions"},{"metadata":{"id":"xPYauoKDmxoK"},"cell_type":"markdown","source":"### As we can see that Random Forest Classifier has performed well with accuracy score - 0.815 and f1 score - 0.46\n### The highest AUC is obtained for the Random Forest Classifier model, with a value of 0.75. This means there is 77% chance that the model will be able to distinguish between default class and non-default class.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### But we have seen how the data is so imbalanced so we can do lot of things and see how the model works."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}