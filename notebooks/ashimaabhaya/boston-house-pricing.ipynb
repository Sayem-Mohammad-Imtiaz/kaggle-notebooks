{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing the libraries \nimport pandas as pd\nfrom pandas import read_csv\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = (\"../input/boston-house-prices/housing.csv\")\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndataset = read_csv(filename, delim_whitespace=True, names=names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in dataset.items():\n    sns.boxplot(y=k, data=dataset, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k, v in dataset.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 / np.shape(dataset)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = dataset.corr()\ncorr.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I observed that INDUS, RM, TAX, PTRATIO and LSTAT shows some good correaltion with MEDV and I am interested to know more about them.\nHowever I noticed that INDUS shows good correlation with TAX and LSAT which is a pain point for us :(\n\nbecause it leads to Multicollinearity. ","metadata":{}},{"cell_type":"code","source":"prices = dataset['MEDV']\nfeatures = dataset.drop('MEDV', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\n\n# i: index\nfor i, col in enumerate(features.columns):\n    # 3 plots here hence 1, 3\n    plt.subplot(1, 6, i+1)\n    x = dataset[col]\n    y = prices\n    plt.plot(x, y, 'o')\n    # Create regression line\n    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('prices')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From my analysis, Price increases with RM and Price decreases with increase in PTRATO and LSTAT","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n#histogram and normal probability plot\nsns.distplot(dataset['MEDV'], hist=True);\nfig = plt.figure()\nres = stats.probplot(dataset['MEDV'], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It shows 'peakedness', positive skewness and does not follow the diagonal line. A simple data transformation can solve the problem. Will do in by standardizing the data\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    \n    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n    score = r2_score(y_true, y_predict)\n    \n    # Return the score\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\narray = dataset.values\nX = array[:,0:6]\nY = array[:,6]\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.20, random_state=7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test options and evaluation metric using Root Mean Square error method\nRMS = 'neg_mean_squared_error'\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state = 7, shuffle = True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standardize the dataset","metadata":{}},{"cell_type":"code","source":"pipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n        kfold = KFold(n_splits=10, random_state=7, shuffle = True)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KNN algorithm Tuning","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=10, random_state=7, shuffle = True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\ndataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensembles","metadata":{}},{"cell_type":"code","source":"ensembles = []\nensembles.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n        kfold = KFold(n_splits=10, random_state=7, shuffle = True)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\ndataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that GBM has better accuracy than KNN","metadata":{}},{"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Scaled Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tuning scaled GBM ","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=np.array([50,100,150,200,250,300,350,400]))\nmodel = GradientBoostingRegressor(random_state=7)\nkfold = KFold(n_splits=10, random_state=7, shuffle = True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Prediction on Validation Dataset","metadata":{}},{"cell_type":"code","source":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=7, n_estimators=400)\nmodel.fit(rescaledX, Y_train)\n# transform the validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(Y_validation, predictions))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('R^2:',metrics.r2_score(Y_validation, predictions))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(Y_validation, predictions))*(len(Y_validation)-1)/(len(Y_validation)-X_validation.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(Y_validation, predictions))\nprint('MSE:',metrics.mean_squared_error(Y_validation, predictions))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(Y_validation, predictions)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=predictions.astype(int)\nsubmission = pd.DataFrame({\n        \"Org House Price\": Y_validation,\n        \"Pred House Price\": predictions\n    })\n\nsubmission.to_csv(\"PredictedPrice.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}