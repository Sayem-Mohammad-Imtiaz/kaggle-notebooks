{"cells":[{"metadata":{"id":"bEmSTWZSPrgb"},"cell_type":"markdown","source":"### Decision Trees and Ensemble Models in classification and regression problems."},{"metadata":{"id":"1cUoTzQLPrgc"},"cell_type":"markdown","source":"## Learning outcomes "},{"metadata":{"id":"Q1ygYVo_Prgc"},"cell_type":"markdown","source":"- Understand how to use decision trees on a Dataset to make a prediction\n- Learning hyper-parameters tuning for decision trees by using RandomGrid \n- Learning the effectiveness of ensemble algorithms (Random Forest, Adaboost, Extra trees classifier, Gradient Boosted Tree)"},{"metadata":{"id":"9hjVbQlVPrgd"},"cell_type":"markdown","source":"I will use Classification Trees for predicting if a user has a default payment option active or not. \n\nThis dataset is aimed at the case of customer default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel Sorting Smoothing Method to estimate the real probability of default.\n"},{"metadata":{"id":"R376ZBnBPrge","trusted":true},"cell_type":"code","source":"#required imports\nimport numpy as np\nimport pandas as pd\nfrom numpy import bincount, linspace, mean, std, arange, squeeze\nimport itertools, time, datetime\nfrom scipy.stats import randint\nfrom pandas import set_option\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport graphviz \nfrom graphviz import Source\nfrom IPython.display import SVG\n\nfrom sklearn import tree\n#from sklearn.tree.export import export_text\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve, precision_score, recall_score\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ddF9R5pdPrgi"},"cell_type":"markdown","source":"After installing the necessary libraries, proceed to download the data. Since reading the excel file won't create headers by default, we added two more operations to substitute the columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"CtNCjjr7Prgj","trusted":true},"cell_type":"code","source":"#loading the data\ndf = pd.read_csv(\"/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\")\n#df.columns = df.iloc[0]\ndf.drop(['ID'], inplace=True,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"id":"cMh-sEIdPrgl"},"cell_type":"markdown","source":"In the following, you can take a look into the dataset."},{"metadata":{"id":"E0lAPOXQPrgl","outputId":"ea66ba57-f32c-4b39-c60a-e52402acbca1","trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"1Qr1SPGlPrgr","trusted":true},"cell_type":"code","source":"df.rename(columns={\"PAY_0\":\"PAY_1\", \"default.payment.next.month\": \"DEFAULT\"}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.EDUCATION.unique())\nprint(df.MARRIAGE.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fil = (df['EDUCATION'] == 5) | (df['EDUCATION'] == 6) | (df['EDUCATION'] == 0)\ndf.loc[fil, 'EDUCATION'] = 4\ndf['EDUCATION'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fil = (df['MARRIAGE'] == 0)\ndf.loc[fil, 'MARRIAGE'] = 3\ndf['MARRIAGE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AGE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating a Function to Distribute the Age\ndef func(x):\n    if(x >=20 and x<30 ):\n        return 1\n    elif(x>=30 and x<40):\n        return 2\n    elif(x>=40 and x<50):\n        return 3\n    elif(x>=50 and x<60):\n        return 4\n    elif(x>=60 and x<=80):\n        return 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AGE'] = df['AGE'].apply(func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df[df.columns[:-1]]\ny=df['DEFAULT']","execution_count":null,"outputs":[]},{"metadata":{"id":"r4jchSRoPrgr"},"cell_type":"markdown","source":"## Questions (15 points total)\n\n#### Question 1 (2 pts)\nBuild a classifier by using decision tree and calculate the confusion matrix. Try different hyper-parameters (at least two) and discuss the result."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DECISION TREE CLASSIFIER"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = tree.DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=25)\n\n# Train the estimator.\ntr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LETS PLOT A TREE"},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = tree.export_graphviz(tr, out_file=None, feature_names=X.columns, filled=True, rounded=True, special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions.\ntr_pred=tr.predict(X_test)\nprint(tr_pred)\n\n# CV score\ntr_cv=cross_val_score(tr, X_train, y_train, cv=10).mean()\nprint (\"Cross val: %.3f\" % tr_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % tr.score(X_test, y_test))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, tr_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, tr_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, tr_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### CONFUSION MATRIX FOR DECISION TREE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Decision tree.\ntr_matrix = confusion_matrix(y_test,tr_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Decision tree');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict propabilities for the test data.\ntr_probs = tr.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ntr_probs = tr_probs[:, 1]\n\n# Compute the AUC Score.\nauc_tr = roc_auc_score(y_test, tr_probs)\nprint('AUC: %.2f' % auc_tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### USING GRID SEARCH TO WORK ON HYPER - PARAMETERS"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'criterion':['gini','entropy'],\n              'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n             }\n\n# GridSearchCV estimator.\ngs_tree = GridSearchCV(tr, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_tree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\ngs_tree_pred=gs_tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best Decision tree Parameters: {}\".format(gs_tree.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % gs_tree.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_tree.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, gs_tree_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, gs_tree_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, gs_tree_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Decision tree.\ngs_tr_matrix = confusion_matrix(y_test,gs_tree_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(gs_tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV Decision tree');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngs_tree_probs = gs_tree.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ngs_tree_probs = gs_tree_probs[:, 1]\n\n# Compute the AUC Score.\ngs_tree_auc = roc_auc_score(y_test, gs_tree_probs)\nprint('AUC: %.2f' % gs_tree_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LETS SEE THEM ON ROC CURVE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ROC Curves.\ngs_tr_fpr, gs_tr_tpr, gs_tr_thresholds = roc_curve(y_test, gs_tree_probs)\ntr_fpr, tr_tpr, tr_thresholds = roc_curve(y_test, tr_probs)\n\n# Plot the ROC curves.\nplt.figure(figsize=(8,8))\nplt.plot(tr_fpr, tr_tpr, color='red', label='Decision tree ROC (AUC= %0.2f)'% auc_tr)\nplt.plot(gs_tr_fpr, gs_tr_tpr, color='green', label='GridSearch+Decision tree ROC (AUC= %0.2f)'% gs_tree_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that GRID SEARCH dint work well and we are ahving better result with only DECISION TREE . We have AUC as 0.74 with DECISON TREE whereas it dropped to 0.64 when we applied GRID SEARCH."},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=['Accuracy', 'CV accuracy', 'Precision','Recall','F1','ROC AUC']\n\nfig = go.Figure(data=[\n    go.Bar(name='Decision tree', x=metrics,\n           y=[tr.score(X_test, y_test),tr_cv,precision_score(y_test, tr_pred),recall_score(y_test, tr_pred),f1_score(y_test, tr_pred),auc_tr]),\n    \n    go.Bar(name='GridSearchCV+Decision tree',\n           x=metrics, y=[gs_tree.score(X_test,y_test),gs_tree.best_score_,precision_score(y_test, gs_tree_pred),recall_score(y_test, gs_tree_pred), f1_score(y_test, gs_tree_pred),gs_tree_auc]),\n])\n\nfig.update_layout(title_text='Metrics for each model',\n                  barmode='group',xaxis_tickangle=-45,bargroupgap=0.05)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QwcecRukPrgw"},"cell_type":"markdown","source":"#### Question 2 (4 pts)\n\nTry to build the decision tree which you built for the previous question, but this time by RandomizedSearchCV over hyper-parameters. Compare the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint\n\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"id":"4XHRmsWOPrgx","trusted":true},"cell_type":"code","source":"param_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_cv = RandomizedSearchCV(tr, param_dist, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_cv.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\nrandom_search_cv_pred=random_search_cv.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best Decision tree Parameters: {}\".format(random_search_cv.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % random_search_cv.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (random_search_cv.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, random_search_cv_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, random_search_cv_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, random_search_cv_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Decision tree.\nrandom_tr_matrix = confusion_matrix(y_test,random_search_cv_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(random_tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for RandomSearchCV Decision tree');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nrandom_search_cv_probs = random_search_cv.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\nrandom_search_cv_probs = random_search_cv_probs[:, 1]\n\n# Compute the AUC Score.\nrandom_search_cv_auc = roc_auc_score(y_test, random_search_cv_probs)\nprint('AUC: %.2f' % random_search_cv_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ROC Curves.\nrandom_tr_fpr, random_tr_tpr, random_tr_thresholds = roc_curve(y_test, random_search_cv_probs)\ngs_tr_fpr, gs_tr_tpr, gs_tr_thresholds = roc_curve(y_test, gs_tree_probs)\ntr_fpr, tr_tpr, tr_thresholds = roc_curve(y_test, tr_probs)\n\n# Plot the ROC curves.\nplt.figure(figsize=(8,8))\nplt.plot(tr_fpr, tr_tpr, color='red', label='Decision tree ROC (AUC= %0.2f)'% auc_tr)\nplt.plot(gs_tr_fpr, gs_tr_tpr, color='green', label='GridSearch+Decision tree ROC (AUC= %0.2f)'% gs_tree_auc)\nplt.plot(random_tr_fpr, random_tr_tpr, color='yellow', label='RandomSearchCV+Decision tree ROC (AUC= %0.2f)'% random_search_cv_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=['Accuracy', 'CV accuracy', 'Precision','Recall','F1','ROC AUC']\n\nfig = go.Figure(data=[\n    go.Bar(name='Decision tree', x=metrics,\n           y=[tr.score(X_test, y_test),tr_cv,precision_score(y_test, tr_pred),recall_score(y_test, tr_pred),f1_score(y_test, tr_pred),auc_tr]),\n    \n    go.Bar(name='GridSearchCV+Decision tree',\n           x=metrics, y=[gs_tree.score(X_test,y_test),gs_tree.best_score_,precision_score(y_test, gs_tree_pred),recall_score(y_test, gs_tree_pred), f1_score(y_test, gs_tree_pred),gs_tree_auc]),\n\n    go.Bar(name='RandomSearchCV+Decision tree',\n           x=metrics, y=[random_search_cv.score(X_test,y_test),random_search_cv.best_score_,precision_score(y_test, random_search_cv_pred),recall_score(y_test, random_search_cv_pred), f1_score(y_test, random_search_cv_pred),random_search_cv_auc]),\n])\n\nfig.update_layout(title_text='Metrics for each model',\n                  barmode='group',xaxis_tickangle=-45,bargroupgap=0.05)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={\n'': ['Decision Tree','GridSearchCV + Decision Tree','RandomSearchCV + Decision Tree'],\n'Accuracy': [tr.score(X_test, y_test),gs_tree.score(X_test,y_test),random_search_cv.score(X_test, y_test)],\n'CV Accuracy': [tr_cv,gs_tree.best_score_,random_search_cv.best_score_],\n'Precision': [precision_score(y_test, tr_pred),precision_score(y_test, gs_tree_pred),precision_score(y_test, random_search_cv_pred)],\n'Recall': [recall_score(y_test, tr_pred),recall_score(y_test, gs_tree_pred),recall_score(y_test, random_search_cv_pred)],\n'F1': [f1_score(y_test, tr_pred),f1_score(y_test, gs_tree_pred),f1_score(y_test, random_search_cv_pred)],\n'ROC AUC': [auc_tr, gs_tree_auc, random_search_cv_auc]\n}\n\nresults=pd.DataFrame(data=d).round(3).set_index('')\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we see the comparison RANDOMIZED SEARCH gave us better result than GRID SEARCH but they are similar to normal DECISION TREE."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Tuned Decision Tree Parameters: {}\".format(random_search_cv.best_params_))\nprint(\"Best score is {}\".format(random_search_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"id":"dEvsYwiXPrg3"},"cell_type":"markdown","source":"#### Question 3 (6 pts)\n\nTry to build the same classifier by using following ensemble models. For each of these models calculate accuracy and at least for two in the list below, plot the learning curves.\n\n* Random Forest \n* AdaBoost\n* Extra Trees Classifier \n* Gradient Boosted Trees \n"},{"metadata":{"id":"J8S4UaKdPrg3","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_clf = AdaBoostClassifier(\n    tr, n_estimators=50,\n    algorithm=\"SAMME.R\", learning_rate=0.5)\nstart = time()\nada_clf.fit(X_train, y_train)\nend = time()\ntrain_time_ada=end-start\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nada_clf_probs = ada_clf.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\nada_clf_probs = ada_clf_probs[:, 1]\n\n# Compute the AUC Score.\nada_clf_auc = roc_auc_score(y_test, ada_clf_probs)\nprint('AUC: %.2f' % ada_clf_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = ada_clf.predict(X_test)\n#from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nroc=roc_auc_score(y_test, ada_clf_probs)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Adaboost', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = ExtraTreesClassifier(n_estimators=50)\nstart = time()\nforest.fit(X_train, y_train)\nend = time()\ntrain_time_et=end-start\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nforest_probs = forest.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\nforest_probs = forest_probs[:, 1]\n\n# Compute the AUC Score.\nforest_auc = roc_auc_score(y_test, forest_probs)\nprint('AUC: %.2f' % forest_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = forest.predict(X_test)\n#from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nroc=roc_auc_score(y_test, forest_probs)\nacc = accuracy_score(y_test, y_pred)\nprec= precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Extra Tree ', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd_clf = RandomForestClassifier(n_estimators=50, max_leaf_nodes=16, n_jobs=-1)\nstart = time()\nrnd_clf.fit(X_train, y_train)\nend=time()\ntrain_time_r50=end-start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nrnd_clf_probs = rnd_clf.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\nrnd_clf_probs = rnd_clf_probs[:, 1]\n\n# Compute the AUC Score.\nrnd_clf_auc = roc_auc_score(y_test, rnd_clf_probs)\nprint('AUC: %.2f' % rnd_clf_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_r = rnd_clf.predict(X_test)\n#from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nroc=roc_auc_score(y_test, rnd_clf_probs)\nacc = accuracy_score(y_test, y_pred_r)\nprec = precision_score(y_test, y_pred_r)\nrec = recall_score(y_test, y_pred_r)\nf1 = f1_score(y_test, y_pred_r)\n\nmodel_results = pd.DataFrame([['Random_forest ', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbes = GradientBoostingClassifier(n_estimators=50)\nstart = time()\n\nend=time()\ntrain_time_g=end-start\ngbes.fit(X_train, y_train)\nend=time()\ntrain_time_g=end-start\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"y_pred = gbes.predict(X_test)\n\nroc=roc_auc_score(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Gboost', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results,ignore_index=True)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngbes_probs = gbes.predict(X_test)\n\n# Keep Probabilities of the positive class only.\n#gbes_probs = gbes_probs[:, 1]\n\n# Compute the AUC Score.\ngbes_auc = roc_auc_score(y_test, gbes_probs)\nprint('AUC: %.2f' % gbes_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we can see after using all the four ensemblers , we were able to get better results than our DECISION TREE MODEL. Out of the four Ensemblers RANDOM FOREST did best with AUC as 0.78.\n### But if we look at the other factors like ACCURACY, PRECISION, RECALL, F1 SCORE then GRADIENT BOOSTING is better than all of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ['Adaboost','ExtraTree','Random forest', 'GBOOST']\nTrain_Time = [\n    train_time_ada,\n    train_time_et,\n    train_time_r50,\n    train_time_g\n\n]\nindex = np.arange(len(model))\nplt.bar(index, Train_Time)\nplt.xlabel('Machine Learning Models', fontsize=15)\nplt.ylabel('Training Time', fontsize=15)\nplt.xticks(index, model, fontsize=8, )\nplt.title('Comparison of Training Time of all ML models')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If we compare looking at the time they toom then RANDOM FOREST is the fastest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ROC Curves.\nrnd_clf_fpr, rnd_clf_tpr, rnd_clf_thresholds = roc_curve(y_test, rnd_clf_probs)\nada_clf_fpr, ada_clf_tpr, ada_clf_thresholds = roc_curve(y_test, ada_clf_probs)\nforest_fpr, forest_tpr, forest_thresholds = roc_curve(y_test, forest_probs)\ngbes_fpr, gbes_tpr, gbes_thresholds = roc_curve(y_test, gbes_probs)\n\n# Plot the ROC curves.\nplt.figure(figsize=(8,8))\nplt.plot(ada_clf_fpr, ada_clf_tpr, color='red', label='AdaBoost ROC (AUC= %0.2f)'% ada_clf_auc)\nplt.plot(rnd_clf_fpr, rnd_clf_tpr, color='green', label='RandomForest ROC (AUC= %0.2f)'% rnd_clf_auc)\nplt.plot(forest_fpr, forest_tpr, color='black', label='ExtraTreeClassifier ROC (AUC= %0.2f)'% forest_auc)\nplt.plot(gbes_fpr, gbes_tpr, color='yellow', label='GradientBoosting ROC (AUC= %0.2f)'% gbes_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If we look at the ROC curve we can see that Random Forest is performing best out of the four ensemblers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=rnd_clf, X=X_train, y=y_train,\n                                                       cv=10, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                     n_jobs=1)\n#\n# Calculate training and test mean and std\n#\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n#\n# Plot the learning curve\n#\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.title('Random Forest Learning Curve ')\nplt.xlabel('Training Data Size')\nplt.ylabel('Model accuracy')\nplt.grid()\nplt.legend(loc='lower right')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes, train_scores, test_scores = learning_curve(estimator=ada_clf, X=X_train, y=y_train,\n                                                       cv=10, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                     n_jobs=1)\n#\n# Calculate training and test mean and std\n#\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n#\n# Plot the learning curve\n#\nplt.plot(train_sizes, train_mean, color='red', marker='o', markersize=5, label='Training Accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='red')\nplt.plot(train_sizes, test_mean, color='black', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='black')\nplt.title('AdaBoost Learning Curve ')\nplt.xlabel('Training Data Size')\nplt.ylabel('Model accuracy')\nplt.grid()\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes, train_scores, test_scores = learning_curve(estimator=forest, X=X_train, y=y_train,\n                                                       cv=10, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                     n_jobs=1)\n#\n# Calculate training and test mean and std\n#\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n#\n# Plot the learning curve\n#\nplt.plot(train_sizes, train_mean, color='red', marker='o', markersize=5, label='Training Accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='red')\nplt.plot(train_sizes, test_mean, color='black', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='black')\nplt.title('ExtraTreeClassifier Learning Curve ')\nplt.xlabel('Training Data Size')\nplt.ylabel('Model accuracy')\nplt.grid()\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(gbes, \n                                                        X, \n                                                        y,\n                                                        # Number of folds in cross-validation\n                                                        cv=10,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"red\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"black\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Gradient Boosting Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"rqh_ejaVNHXW"},"cell_type":"markdown","source":"### Often, we are not aware of optimal values for hyperparameters which would generate the best model output. So, what we tell the model is to explore and select the optimal model architecture automatically. \n### Each model has its own sets of parameters that need to be tuned to get optimal output. For every model, our goal is to minimize the error or say to have predictions as close as possible to actual values.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### RANDOMIZED SEARCH\nThe model randomly makes combinations of its own and tries to fit the dataset and test the accuracy. Here, chances are there to miss on a few combinations which could have been optimal ones. Although, random search consumes quite less amount of time and most of the time it gives optimal solutions as well.\n\n### GRID SEARCH\nIn this method, each combination of hyperparameter value is tried. This makes the process time consuming, or in short, inefficient. This method is quite an expensive method in terms of computation power and time, but this is the most efficient method as there is the least possibility of missing out on an optimal solution for a model.\n\n### As we saw above our model performance got low when we used GRID SEARCH but improved when we used RANDOMIZED SEARCH CV. SO changing hyperparameters did make a little difference to our model performance."},{"metadata":{},"cell_type":"markdown","source":"### There are a number of machine learning models to choose from. When we build these models, we always use a set of historical data to help our machine learning algorithms learn what is the relationship between a set of input features to a predicted output. Below are few reasons why the model dint work or performed better:\n\n\n## High Bias or High Variance\n\n### High Bias refers to a scenario where your model is “underfitting” your dataset . This is bad because your model is not presenting a very accurate or representative picture of the relationship between your inputs and predicted output, and is often outputting high error .\n\n### In cases of High Variance or “overfitting”, your machine learning model is so accurate that it is perfectly fitted to your example dataset. While this may seem like a good outcome, it is also a cause for concern, as such models often fail to generalize to future datasets. So while your model works well for your existing data, you don’t know how well it’ll perform on other examples.\n\n### If you can generate a model with overall low error in both your train (past) and test (future) datasets, you’ll have found a model that is “Just Right” and balanced the right levels of bias and variance.\n\n\n## Low Precision or Low Recall\n\n### Even when you have high accuracy, it’s possible that your machine learning model may be susceptible to other types of error.\n\n### Precision is a measure of how often your predictions for the positive class are actually true. It’s calculated as the number of True Positives over the sum of the True Positives and False Positives.\n\n### Recall is the measure of how often the actual positive class is predicted as such. It’s calculated as the number of True Positives over the sum of the True Positives and False Negatives.\n\n### Another way to interpret the difference between Precision and Recall, is that Precision is measuring what fraction of your predictions for the positive class are valid, while Recall is telling you how often your predictions actually capture the positive class. Hence, a situation of Low Precision emerges when very few of your positive predictions are true, and Low Recall occurs if most of your positive values are never predicted.\n\n### The goal of a good machine learning model is to get the right balance of Precision and Recall, by trying to maximize the number of True Positives while minimizing the number of False Negatives and False Positives \n\n### The above probelm can be solved by doing following changes:\n\n####    Incase of High Bias - increasing the number of input features.\n####    Incase of High Variance - reduce the number of input features. Increasing the number of training examples.\n####    Incase of Low Precision - increase the probability threshold\n####    Incase of Low Recall - reduce the probability threshold, therein predicting the positive class more often."},{"metadata":{},"cell_type":"markdown","source":"### Now we know that our data was imbalanced and need resampling with feature engineering thats why models were not able to perform as expected."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}