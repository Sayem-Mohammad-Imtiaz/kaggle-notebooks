{"metadata":{"language_info":{"version":"3.6.3","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"78ccd711-93cb-47b6-81d9-8a5ecbf9bf04","_uuid":"0fadae882f12b09793861f6ab672aab30e4fd749"},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I will first have a look on the dataset with some visualizations, then I will try to create a legendary pokemon recognizer, then I will try to create new legendary pokemons using this recognizer inside a generator."},{"execution_count":null,"metadata":{"_cell_guid":"ed4d1398-061e-4e35-9d36-04bfb4b34cd6","_uuid":"76f62f0fa2321009251f5c1f95ec117c21031296"},"outputs":[],"cell_type":"code","source":"# Let's check our input file\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"metadata":{"_cell_guid":"eeca8118-76f5-4375-b774-c9c010a5c145","_uuid":"0ed2c2692e60db2b4471c263ce45e3566a14f897"},"cell_type":"markdown","source":"# Import section"},{"execution_count":null,"metadata":{"_cell_guid":"11a85e9b-beea-4bec-b4f7-e2f81f58e226","_uuid":"fba70248de1f486b2bad0e474c00b54a8fc1f6d0"},"outputs":[],"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. I like it most for plot\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.cross_validation import KFold # use for cross validation\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm # for Support Vector Machine\nfrom sklearn import metrics # for the check the error and accuracy of the model"},{"metadata":{"_cell_guid":"6ff71bcf-7f7c-4beb-b97c-e22e8ae74bdd","_uuid":"142464dffa2b2439e091aad18a37a44115560b29"},"cell_type":"markdown","source":"# Explore data"},{"metadata":{"_cell_guid":"62b13dd6-6f8b-440b-9bd2-bbe0e21bdc76","_uuid":"d993984163d206dd12abd4cf7277938e9411a689"},"cell_type":"markdown","source":"## Basic exploration"},{"execution_count":null,"metadata":{"_cell_guid":"b50d61d6-28f2-4885-8890-6f937cfe45e7","collapsed":true,"_uuid":"327ef25dc5a1d4d54e36c970d06b7b80c3298e45"},"outputs":[],"cell_type":"code","source":"# Import data into a Pandas Dataframe\ndata = pd.read_csv(\"../input/pokemon.csv\")"},{"execution_count":null,"metadata":{"_cell_guid":"7781e528-c423-4d67-bdcb-879ae5a2d2f7","_uuid":"cc4f1619bb5a7df65db6ca6e591b8cc69dd6e8a0"},"outputs":[],"cell_type":"code","source":"# Let's check the columns we have\ndata.info()"},{"execution_count":null,"metadata":{"_cell_guid":"b60123ca-7459-4de0-bf7b-15a292532e0c","_uuid":"39587cbf9d1384062f9b26d97bd3775a2adc5b4d"},"outputs":[],"cell_type":"code","source":"# Let's see how data look like\ndata.head()"},{"metadata":{"_cell_guid":"f2daea92-a53e-4409-b2b5-8e71068d8484","_uuid":"1360aadcc3d80308ecd7bdebf997332c78f01bc6"},"cell_type":"markdown","source":"## Clean data for further exploration"},{"metadata":{"_cell_guid":"49c39884-78a7-430a-b603-d5a8d9e76dfe","_uuid":"801fdff88fc3da4677cba4dfa5ff5057f730a86a"},"cell_type":"markdown","source":"There are some null Objects in type2, so we will convert NaN values into the string \"None\"."},{"execution_count":null,"metadata":{"_cell_guid":"7854bf6e-ca99-491b-beab-ea6510b5367f","collapsed":true,"_uuid":"727c00c73f984112babcb169ee7d1ce16f1eae69"},"outputs":[],"cell_type":"code","source":"data['type2'].fillna('None', inplace=True)"},{"metadata":{"_cell_guid":"a5d196ae-ab7b-493f-b75b-a06aedf0a2b8","_uuid":"a31c376ab956900312498f56dd4d3e515e0e06b1"},"cell_type":"markdown","source":"`capture_rate` is for the most an integer, but have some strings in some rare cases. We will convert this column into integers, replacing strings by `NaN`"},{"execution_count":null,"metadata":{"_cell_guid":"a5722700-26f8-4bf6-a50c-dbddce6b5fc7","collapsed":true,"_uuid":"576665c19f2222402f4949ee156748b920b4096e"},"outputs":[],"cell_type":"code","source":"# Replace strings by NaN value, and convert numbers into an actual number type\ndata['capture_rate'] = pd.to_numeric(data['capture_rate'], errors='coerce')"},{"metadata":{"_cell_guid":"aeb7e596-93af-47d8-a437-71970964d36b","_uuid":"486e271aa63233b8c25049506006d7fa1efe01cf"},"cell_type":"markdown","source":"`type1`, `type2`, `abilities`, and `classfication` are some strings. As these parameters will likely be processed with other columns, we will convert them into numbers thanks simple conversion tables. Each string will be represented by an ID."},{"execution_count":null,"metadata":{"_cell_guid":"8d610ca5-044e-49c7-b296-40578a3f65b3","collapsed":true,"_uuid":"397d1c91296e24508e47bd614f1c932c17a9e594"},"outputs":[],"cell_type":"code","source":"# Create the type <-> ID conversion dictionary\nall_types = set(data['type1']).union(set(data['type2']))\ntype_id_dict = dict(zip(list(all_types), range(len(all_types))))\n\nall_abilities = set(data['abilities'])\nabilities_id_dict = dict(zip(all_abilities, range(len(all_abilities))))\n\nall_classf = set(data['classfication'])\nclassf_id_dict = dict(zip(all_classf, range(len(all_classf))))"},{"execution_count":null,"metadata":{"_cell_guid":"178c9ab1-744a-450e-ade5-1721b9d79ef7","_uuid":"c5041c0c69db4c84baa6ec5b08ecad98084d5aba"},"outputs":[],"cell_type":"code","source":"# Convert types in the Dataframe\nfor type_name, type_id in type_id_dict.items():\n    data['type1'].replace(type_name, type_id, inplace=True)\n    data['type2'].replace(type_name, type_id, inplace=True)\n    \nfor ability_name, ability_id in abilities_id_dict.items():\n    data['abilities'].replace(ability_name, ability_id, inplace=True)\n\nfor classf_name, classf_id in classf_id_dict.items():\n    data['classfication'].replace(classf_name, classf_id, inplace=True)\n   \ndata[['type1', 'type2', 'classfication', 'abilities']].head()"},{"metadata":{"_cell_guid":"790c0c4d-8242-42ab-84aa-941a8bdeca69","_uuid":"edba0d488efdcf28b25c2107c52da71bfbd0b05f"},"cell_type":"markdown","source":"## Data split\n\nNow we can split data into different groups. Here we choose to split columns into the following categories of info:\n* **in-combat** parameters (like attack and defense)\n* **off-combat** parameters (like gender ratio)\n* **out-of-gameplay** parameters (like name and ID)"},{"execution_count":null,"metadata":{"_cell_guid":"8cdba2c0-4268-40d7-87ea-b0380044a934","_uuid":"489ee52c1c0520fb2ac1599398f2d1e02388ce83"},"outputs":[],"cell_type":"code","source":"in_combat_col = ['against_bug', 'against_dark', 'against_dragon',\n       'against_electric', 'against_fairy', 'against_fight', 'against_fire',\n       'against_flying', 'against_ghost', 'against_grass', 'against_ground',\n       'against_ice', 'against_normal', 'against_poison', 'against_psychic',\n       'against_rock', 'against_steel', 'against_water', 'attack',\n       'base_total', 'defense', 'hp', 'sp_attack', 'sp_defense', 'speed', \n        'type1', 'type2']\noff_combat_col = ['abilities', 'base_egg_steps', 'base_happiness', 'capture_rate',\n       'classfication', 'experience_growth', 'height_m', 'percentage_male', \n       'weight_kg', 'is_legendary']\nout_of_gameplay_col = ['japanese_name', 'name', 'pokedex_number', 'generation']\n\n# Check that the list do not overlap amongst themselves\nif not set(in_combat_col).intersection(set(off_combat_col)) \\\n   and not set(in_combat_col).intersection(set(out_of_gameplay_col)) \\\n   and not set(off_combat_col).intersection(set(out_of_gameplay_col)):\n    print(\"â˜‘ Lists do not overlap :)\")\nelse:\n    print(\"Lists overlap !\")"},{"metadata":{"_cell_guid":"2ea7ee3e-04df-4534-8cb6-f1953e270213","_uuid":"929cf693de0c546c3d60b74a8f6687ede9d2a877"},"cell_type":"markdown","source":"## Data correlations"},{"execution_count":null,"metadata":{"_cell_guid":"e2a49165-930d-4818-8243-3df52c4320a8","_uuid":"955746b1bcd2bb12fc88d087d04b5ed8191fdfa3"},"outputs":[],"cell_type":"code","source":"# Let's find redondant information with a simple cross-correlation\ncorr = data[in_combat_col].corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr, cbar=True, square=True, annot=True, fmt='.2f',annot_kws={'size': 10},\n            xticklabels=in_combat_col, yticklabels=in_combat_col, cmap= 'coolwarm')"},{"metadata":{"_cell_guid":"77370c2b-5853-4e6b-a913-de2625608aca","_uuid":"b35b4cea055af89a18256437be7cabe6d1d56afe"},"cell_type":"markdown","source":"We can observe that there is not any strong correlation, so no column to remove. Nonetheless, `base_total` has a cross-correlation value over 0.5 with several other items, which indicates that this value is well represented by a set of other values and is not necessary in our dataset. Moreover, this column is not described in the dataset presentation and I could not find any documentation about it. For these reasons, I decide to remove this column from the dataset."},{"execution_count":null,"metadata":{"_kg_hide-output":true,"_cell_guid":"5ad42272-1d26-4d82-9300-a8d23daf4d0e","collapsed":true,"_kg_hide-input":false,"_uuid":"25c53d1db81cfbee2be19fec539ec58122a4dad8"},"outputs":[],"cell_type":"code","source":"# Remove base_total\ndata.drop(\"base_total\", axis=1, inplace=True)\nin_combat_col.remove(\"base_total\")"},{"execution_count":null,"metadata":{"_cell_guid":"139cd6ea-a331-4109-852b-1a5d0102f6ea","_uuid":"7d87e30317a345d83de1cc4aec5153cf6a8e0200"},"outputs":[],"cell_type":"code","source":"# Let's do the same with off-combat parameters\ncorr = data[off_combat_col].dropna(axis=0).corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr, cbar=True, square=True, annot=True, fmt='.2f',annot_kws={'size': 15},\n            xticklabels=off_combat_col, yticklabels=off_combat_col, cmap= 'coolwarm')"},{"metadata":{"_cell_guid":"68767e41-117a-4fdc-b494-5f4cd41ab85d","_uuid":"d63ca62521c9177e532cb8e5fb197c357f887e9b"},"cell_type":"markdown","source":"We can observe that there is a strong correlation between `is_legendary` and `base_egg_steps`"},{"metadata":{"_cell_guid":"aa97bae8-4b10-4595-a48b-057ca1052217","_uuid":"c13c192d94d5750193817f8b635c71d6b8d6c6a2"},"cell_type":"markdown","source":"## Data visualization about legendary Pokemon"},{"execution_count":null,"metadata":{"_cell_guid":"6222562b-494a-438e-b435-6e4aae3068fb","_uuid":"3670a6e3cd349aca3184c04be79d6f2115e12335"},"outputs":[],"cell_type":"code","source":"# Let's see the number of legendary pokemon in data\nsns.countplot(data['is_legendary'])"},{"metadata":{"_cell_guid":"46656dbf-66d8-4a4f-a3be-8633f4519260","_uuid":"2719f010ee86f76c52727d531cda63f2ddbdc18f"},"cell_type":"markdown","source":"### Legendary VS in-combat parameters"},{"execution_count":null,"metadata":{"_cell_guid":"124a435d-2de3-49f9-945f-b9906d5f33a3","_uuid":"6b934f2e39e1898b3b24e9e229566d7a91bd48b2"},"outputs":[],"cell_type":"code","source":"# Let's find relations between beeing a legendary pokemon or not, and the basic combat parameters\n# We do not use in_combat_col because there are too many columns to plot\nbasic_in_combat_col = ['attack', 'defense', 'hp', 'sp_attack', 'sp_defense', 'speed']\n\nsns.pairplot(data, vars=basic_in_combat_col, hue=\"is_legendary\")"},{"metadata":{"_cell_guid":"318fc08f-2d8a-4e38-b02f-20047d2c33b1","collapsed":true,"_uuid":"ef1f410f47ee2ceb17e82380e38503494c157aba"},"cell_type":"markdown","source":"We can see that by taking parameters in pairs, there is no clear separation between legendary and non-legendary pokemons. Meanwhile, legendary pokemons have all of their parameters with high values. We therefore may expect that the combination of every parameters shows a clear separation.\n\nTo do so, we will basically try to make the sum of these parameters, and observe the new repartition of legendary pokemons. We must take care to use normalized data to give each parameter the same weight."},{"execution_count":null,"metadata":{"_cell_guid":"f7915ada-d568-41fd-ae06-2c5905a807a6","_uuid":"2460c9c7f8d42fc549f0b94841a9af984c0401f4"},"outputs":[],"cell_type":"code","source":"# Let's compute the sum of these parameters and store them in a new column\n# We first need to normalize the data to do a relevant sum\nbasic_combat_df = data[basic_in_combat_col]\nnorm_data = (basic_combat_df - basic_combat_df.mean()) / (basic_combat_df.max() - basic_combat_df.min())\nnorm_data['sum'] = norm_data.sum(axis=1, numeric_only=True)\n# Add the is_legendary column\nnorm_data['is_legendary'] = data['is_legendary']\nnorm_data.head()"},{"execution_count":null,"metadata":{"_cell_guid":"660cbc71-786f-4ed7-8a24-74dca98108d9","_uuid":"46e8d4ef76763f069b403dfa21bf59a3cd6cfe8b"},"outputs":[],"cell_type":"code","source":"# Let's display the repartition of legendary pokemon in funciton of this sum\nsns.boxplot(data=norm_data, x='is_legendary', y='sum')"},{"metadata":{"_cell_guid":"1f55f90b-69a7-486d-bd5f-e6327e132f39","_uuid":"646cab8f38e4eeee75c6ab9c4e6091656ff79354"},"cell_type":"markdown","source":"According to this boxplot, there is a clear correlation between the legendary status and the sum of all basic combat parameters. But both classes still overlap, and this sum cannot be the single value to consider to do a legendary recognizer."},{"metadata":{"_cell_guid":"6fc45dab-b7c1-4e31-bf78-920ff086be4b","collapsed":true,"_uuid":"698a6517a5069afa8510e9565d368d30e62c93a4"},"cell_type":"markdown","source":"Let's do the same for other parameters of the list *in-combat*."},{"execution_count":null,"metadata":{"_cell_guid":"a409f8d4-81d7-483b-9e85-f6c0ac3478c5","collapsed":true,"_uuid":"8d1fc53a7241e1c43e2adda2c27facd6332e379f"},"outputs":[],"cell_type":"code","source":"other_in_combat_col = ['against_psychic', 'against_grass', 'against_flying', \n                       'against_ground', 'against_water', 'against_electric', \n                       'against_fire', 'against_fairy', 'against_dark', 'against_ice', \n                       'against_steel', 'against_bug', 'against_normal', 'against_poison', \n                       'against_ghost', 'against_rock', 'against_fight', 'against_dragon']"},{"execution_count":null,"metadata":{"_cell_guid":"a527ae6f-15f5-4133-b2e9-06733580dbc1","_uuid":"c5c83ad337e0046cf5b36b3d743f6486d71b2e4b"},"outputs":[],"cell_type":"code","source":"# Let's compute the sum of these parameters and store them in a new column\n# We first need to normalize the data to do a relevant sum\nother_in_combat_df = data[other_in_combat_col]\nnorm_data = (other_in_combat_df - other_in_combat_df.mean()) / (other_in_combat_df.max() - other_in_combat_df.min())\nnorm_data['sum'] = norm_data.sum(axis=1, numeric_only=True)\n# Add the is_legendary column\nnorm_data['is_legendary'] = data['is_legendary']\nnorm_data.head()"},{"execution_count":null,"metadata":{"_cell_guid":"79e757c1-e419-4916-a619-91b20db8c710","_uuid":"169c25430f2a86ff222522526704080f3b59b9f7"},"outputs":[],"cell_type":"code","source":"# Let's display the repartition of legendary pokemon in funciton of this sum\nsns.boxplot(data=norm_data, x='is_legendary', y='sum')"},{"metadata":{"_cell_guid":"d1f31ff1-7adc-4f50-a79c-6d878e3546af","_uuid":"64d9e8b27e88e94cb1a2b3f6cabe71265ff98cc1"},"cell_type":"markdown","source":"Legendary pokemons cannot be determined using the `against_X` parameters."},{"metadata":{"_cell_guid":"5ec4f5bb-9ebe-4925-bdd8-27db789b1631","_uuid":"bba468bc28af48754c334d804ecb6fc2f85fb122"},"cell_type":"markdown","source":"And finally, what about the types ?"},{"execution_count":null,"metadata":{"_cell_guid":"cc01b026-4234-4ec8-889f-eb025dfa143c","_uuid":"488220785884c8dbbc87a53e7dd4f738f6a42ddf"},"outputs":[],"cell_type":"code","source":"# Let's display the repartition of legendary pokemon in function of types\nfig, axs = plt.subplots(ncols=2, figsize=(12,4))\nsns.boxplot(ax=axs[0], data=data[['type1','is_legendary']], x='is_legendary', y='type1')\nsns.boxplot(ax=axs[1], data=data[['type2','is_legendary']], x='is_legendary', y='type2')"},{"metadata":{"_cell_guid":"3b8dccb9-937f-4a6c-9f94-ef1d0270f742","_uuid":"b478ec1f42c4a52e1ce7fcaf90bb4aaed056e06f"},"cell_type":"markdown","source":"## Legendary VS off-combat parameters"},{"metadata":{"_cell_guid":"3fc54e48-f51a-492d-a213-0b945dfab028","_uuid":"377fb9c1cbbb1bde1261c25e4f52abd684eff02d"},"cell_type":"markdown","source":"Instead of displaying the repartition of legendaries per pair of features, let's display it for each one."},{"execution_count":null,"metadata":{"_cell_guid":"4acaaebb-b7e2-4ddd-a252-2bae51a070a2","_uuid":"5bee976ebbdda86bcd060d011e2231c86d1ff591"},"outputs":[],"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(16,16))\nfor i, col in enumerate([c for c in off_combat_col if c !=\"is_legendary\"]):\n    sns.swarmplot(ax=axs[i//3][i%3], data=data, x='is_legendary', y=col)"},{"metadata":{"_cell_guid":"a823ff6f-f29f-4b4a-8f4a-8daae5de9b27","_uuid":"e93a9339437dc91962ad304d22be6d025b44afd3"},"cell_type":"markdown","source":"According to these vizualisations, we can safely remove the following items from the prediction variables:\n* classfication\n* abilities\n\n"},{"metadata":{"_cell_guid":"5a6f3b3c-7311-4ce0-9ff7-bc30028c8f1b","_uuid":"1afffcc8028a69ed703490bf3d2697eee26bfc0c"},"cell_type":"markdown","source":"In the columns `weight_kg`, `height_m`, and `percentage_male`, there are a lot of missing data (`NaN` value). Let's see how many legendary pokemon are concerned by these missing data."},{"execution_count":null,"metadata":{"_cell_guid":"c7f3d37d-d8eb-41bd-bfa8-5baaf40edd9a","_uuid":"aef73a8eec0c52fe18275bd10f7868f762b2f35c"},"outputs":[],"cell_type":"code","source":"d = {}\nd['with_everything'] = data['is_legendary']\nd['without_nan_capture'] = data.dropna(axis=0, how='any', subset=['capture_rate'])['is_legendary']\nd['without_nan_weight'] = data.dropna(axis=0, how='any', subset=['weight_kg'])['is_legendary']\nd['without_nan_height'] = data.dropna(axis=0, how='any', subset=['height_m'])['is_legendary']\nd['without_nan_height_weight'] = data.dropna(axis=0, how='any', subset=['height_m', 'weight_kg'])['is_legendary']\nd['without_nan_male'] = data.dropna(axis=0, how='any', subset=['percentage_male'])['is_legendary']\nfor key, s in d.items():\n    print(\"{} : \\n{}\".format(key, s.value_counts()))\n\nlegendary_serie_without_nan = data.dropna(axis=0, how='any')['is_legendary']\nlegendary_serie_with_nan = data['is_legendary']"},{"metadata":{"_cell_guid":"2e845117-128f-43ec-a8a8-cd45cfa74794","_uuid":"98fd1129e2bebce0120b1a5d7ae766d91d76f22e"},"cell_type":"markdown","source":"Result is we only lose 1 legendary pokemon by dropping NaN values in both `weight_kg` and `height_m` columns, but we lose 63 pokemons by dropping NaN values in `percentage_male` column. We now decide to remove `percentage_male` from used features for recognition, and we drop the line containing a NaN for weight and height.\n\nWe can now prepare the list `useful_off_combat_col` for the recognizer, containing only the necessary features."},{"execution_count":null,"metadata":{"_cell_guid":"d5c9a6a4-7ef7-4ac7-a8fb-8f2153ae3489","collapsed":true,"_uuid":"f9f09c423696fb927062b186fc509a78d37b793d"},"outputs":[],"cell_type":"code","source":"useful_off_combat_col = off_combat_col.copy()\nfor col in ['abilities', 'classfication', 'is_legendary', 'percentage_male']:\n    if col in useful_off_combat_col:\n        useful_off_combat_col.remove(col)\n\n# Drop rows where there is a NaN in height_m, weight_kg, or capture_rate\nsubset = ['height_m', 'weight_kg', 'capture_rate']\ndata = data.dropna(axis=0, how='any', subset=subset)"},{"metadata":{"_cell_guid":"fd2c1071-ae56-41d2-9075-a1356f99c73d","_uuid":"6477e43ebad306b8e5516b9760191c4c272571d0"},"cell_type":"markdown","source":"# Legendary recognizer"},{"metadata":{"_cell_guid":"a2cac461-06db-46ce-9221-581f1d7df969","_uuid":"2aafb51ef1d8d9a7d2a400b1ce8474911fe5f5a2"},"cell_type":"markdown","source":"Prepare data for recognizers."},{"execution_count":null,"metadata":{"_cell_guid":"0c507909-91ae-4849-be46-3768fd390722","_uuid":"40bafcb81ccae5131690a55373a7d8c41abff784"},"outputs":[],"cell_type":"code","source":"#now split our data into train and test\ntrain, test = train_test_split(data, test_size=0.3)# in this our main data is splitted into train and test\n# we can check their dimension\nprint(train.shape)\nprint(test.shape)\n\ntrain_y = train['is_legendary']# This is output of our training data\ntest_y = test['is_legendary']   #output value of test dat"},{"metadata":{"_cell_guid":"84ed1a4a-bc83-4388-b0f6-bd810e2f5fd3","_uuid":"55da62ca54b1f2013833b10981004c5363b6f0bc"},"cell_type":"markdown","source":"## With basic combat features only"},{"execution_count":null,"metadata":{"_cell_guid":"481d3322-bf95-4f5a-a889-b79bd4dfbb6f","collapsed":true,"_uuid":"3cb5f399381ea90904552fed1038e7258d37c7b3"},"outputs":[],"cell_type":"code","source":"prediction_var = basic_in_combat_col.copy()"},{"execution_count":null,"metadata":{"_cell_guid":"228d45f1-6b2c-4acf-acc0-b1d8b6aa92e7","collapsed":true,"_uuid":"cf75649c65516cc376e15034ab51f1928828fae3"},"outputs":[],"cell_type":"code","source":"train_X = train[prediction_var]# taking the training data input \ntest_X= test[prediction_var] # taking test data inputs"},{"metadata":{"_cell_guid":"d4e5d939-ccb0-4d6d-b5fc-de9a8525f818","_uuid":"b76ee79fad0b98e5819c1b1f381b0afb7f649719"},"cell_type":"markdown","source":"### Random forest"},{"execution_count":null,"metadata":{"_cell_guid":"7c628183-46eb-4cab-9144-f6d88195a834","_uuid":"eceb1bff1fddf1b10d57ba1bc0b80181107fcce1"},"outputs":[],"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)# a simple random forest model\nmodel.fit(train_X,train_y)# now fit our model for training data\nprediction=model.predict(test_X)# predict for the test data\nmetrics.accuracy_score(prediction,test_y) # to check the accuracy\n# here we will use accuracy measurement between our predicted value and our test output values"},{"metadata":{"_cell_guid":"0acc7ce3-2d6b-4f39-a30d-e624b0aed75b","_uuid":"81b2462455a5eaed06da3cf6e2113629ff944617"},"cell_type":"markdown","source":"### SVM"},{"execution_count":null,"metadata":{"_cell_guid":"729664fe-e269-4422-82b7-3698bb110e63","_uuid":"ed8c387099adfda718303b3dff3ed9b57f11395c"},"outputs":[],"cell_type":"code","source":"model = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"metadata":{"_cell_guid":"31c5d8dd-4b7d-4237-813c-51159e4d7dae","collapsed":true,"_uuid":"be02a76858663e650b598cad9e32f9d7fcbcb470"},"cell_type":"markdown","source":"## With off-combat features only"},{"metadata":{"_cell_guid":"ea902ada-ba19-4230-ae2c-4a1c38436f26","_uuid":"76adb3c96c20a414b5f07deb07488dcd8f393254"},"cell_type":"markdown","source":"We remove the features we considered as useless in the previous section."},{"execution_count":null,"metadata":{"_cell_guid":"6f9ff0ff-40f6-4a7a-a3ac-59209ab13121","_uuid":"41c8298db24ecd3006bfacd420cc5cc755b6ee3a"},"outputs":[],"cell_type":"code","source":"prediction_var = useful_off_combat_col.copy()\nprediction_var"},{"execution_count":null,"metadata":{"_cell_guid":"9707365e-ecfb-4cd9-a2bd-5452394a56f3","collapsed":true,"_uuid":"68d598ece06867890b83be9e7f8b9ad018305c1d"},"outputs":[],"cell_type":"code","source":"train_X = train[prediction_var]# taking the training data input \ntest_X = test[prediction_var] # taking test data inputs"},{"execution_count":null,"metadata":{"_cell_guid":"63091c69-658c-4212-86ec-68fbd7c24ba9","_uuid":"063639513a1c454a049467c489ef7dff86646cca"},"outputs":[],"cell_type":"code","source":"np.isfinite(train_X).all()"},{"metadata":{"_cell_guid":"aa2c8eb9-34a1-4d5b-9cea-8426c6c15cad","_uuid":"7f3289f4276a972d5bc6b5385603a9c1c52045fc"},"cell_type":"markdown","source":"### Random Forest"},{"execution_count":null,"metadata":{"_cell_guid":"80d94e8f-6ea0-4d0f-95d8-10712376fa9f","_uuid":"d77ae3186585645532a21184956f50b11eb87ac9"},"outputs":[],"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)# a simple random forest model\nmodel.fit(train_X,train_y)# now fit our model for training data\nprediction=model.predict(test_X)# predict for the test data\nmetrics.accuracy_score(prediction,test_y) # to check the accuracy\n# here we will use accuracy measurement between our predicted value and our test output values"},{"metadata":{"_cell_guid":"c1b5d3fc-2d7c-406c-ae3e-053c2b1f5031","_uuid":"ab9d0c3d7d7f4bfd080e90f15b792810bf4b3267"},"cell_type":"markdown","source":"### SVM"},{"execution_count":null,"metadata":{"_cell_guid":"2053cea1-a2f9-419c-9f41-019327337412","_uuid":"4f0defb60448dd9c399ea46df56011c18ce77955"},"outputs":[],"cell_type":"code","source":"model = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"metadata":{"_cell_guid":"fe3631ef-efa6-4452-96b1-12c167659399","_uuid":"8e4a60efb59086da4782fe113ff078dc6afbd147"},"cell_type":"markdown","source":"## Using both basic-combat and off-combat features"},{"execution_count":null,"metadata":{"_cell_guid":"d7a5d358-dc65-4657-8f0c-8295e7aeb4a4","_uuid":"007a224ef9e1c772313546ee5920e5878ad436d5"},"outputs":[],"cell_type":"code","source":"prediction_var = (useful_off_combat_col + basic_in_combat_col).copy()\nprediction_var"},{"execution_count":null,"metadata":{"_cell_guid":"a881c2e7-6399-4000-b38f-b8fb8c13d98d","collapsed":true,"_uuid":"961beb1c4b547c2da72d22dff17104d876ccb635"},"outputs":[],"cell_type":"code","source":"train_X = train[prediction_var]# taking the training data input \ntest_X = test[prediction_var] # taking test data inputs"},{"metadata":{"_cell_guid":"2ecd68f0-b024-4af5-a533-29b7dc0aa5f6","_uuid":"b8a4b6a83dcc57206d2fe20729a29f2a27f6480b"},"cell_type":"markdown","source":"### Random Forest"},{"execution_count":null,"metadata":{"_cell_guid":"173fc0a7-b842-4cc0-a8b6-b168c0fc52f4","_uuid":"92fe986bb783cdafc2af0fd0862fd16e334c1fd0"},"outputs":[],"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)# a simple random forest model\nmodel.fit(train_X,train_y)# now fit our model for training data\nprediction=model.predict(test_X)# predict for the test data\nmetrics.accuracy_score(prediction,test_y) # to check the accuracy\n# here we will use accuracy measurement between our predicted value and our test output values"},{"metadata":{},"cell_type":"markdown","source":"Let's find out what are the most important features for this algorithm"},{"execution_count":null,"metadata":{},"outputs":[],"cell_type":"code","source":"sorted_features = sorted(zip(model.feature_importances_, train_X.columns), reverse=True)\nunzip_sorted_features = list(zip(*sorted_features))\nlabels = unzip_sorted_features[1]\nscores = unzip_sorted_features[0]\n\nfig1, ax1 = plt.subplots()\nfig1.suptitle(\"Importance of features in Random Forest algorithm\")\nfig1.set_figheight(10)\nfig1.set_figwidth(10)\nax1.pie(scores, labels=labels, autopct='%1.2f%%',\n        shadow=False, startangle=0)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()"},{"execution_count":null,"metadata":{"_cell_guid":"e66c1e65-41bf-414d-9a90-eee27b6da989","_uuid":"1c434108f9772922f89e63b16ec4acf1dd52b772"},"outputs":[],"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)# a simple random forest model\nmodel.fit(train_X[['base_egg_steps', 'capture_rate', 'base_happiness']],\n          train_y)# now fit our model for training data\nprediction=model.predict(test_X[['base_egg_steps', 'capture_rate', 'base_happiness']])# predict for the test data\nmetrics.accuracy_score(prediction,test_y) # to check the accuracy"},{"metadata":{"_cell_guid":"a3b7d354-0b45-41a3-826b-854bee38502b","collapsed":true,"_uuid":"824ad7cbbc5d02143ae8294bfda67abf215baa4e"},"cell_type":"markdown","source":"As we can see, three features hide all of the other ones. This is a problem for our generator that aims at giving a relevant value for each one of them. So let us try to combine two different classifiers:\n* One using the three most important features\n* One using the other ones"},{"execution_count":null,"metadata":{"collapsed":true},"outputs":[],"cell_type":"code","source":""}],"nbformat":4}