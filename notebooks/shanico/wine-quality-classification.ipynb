{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import HTML","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{},"cell_type":"markdown","source":"### Importing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 11 features and target"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The features are not in the same magnitude order."},{"metadata":{},"cell_type":"markdown","source":"## Relationship between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, height=2.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There seem to be nonlinear correlation between features."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize = (16,10))\nsns.heatmap(corr,annot=True, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There seem to be no distinct correlation between the features."},{"metadata":{},"cell_type":"markdown","source":"### Split to features and target (X, Y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of target values"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y,bins=len(set(y)))\nplt.title('Distribution of target values')\nplt.xlabel('Classes')\nplt.ylabel('count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The division between the classes is not balanced,and can lead to poor classification results."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split dataset to train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state=10\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GBM model without tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM1 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000,max_depth=3, min_samples_split=2, min_samples_leaf=1)\nGBM1.fit(X_train,y_train)\npredictors=list(X_train)\nfeat_imp = pd.Series(GBM1.feature_importances_, predictors).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Importance of Features')\nplt.ylabel('Feature Importance Score')\nGBM1_score = GBM1.score(X_test, y_test)\nprint('Accuracy of the GBM on test set: {:.3f}'.format(GBM1_score))\nGBM1_pred=GBM1.predict(X_test)\nprint(classification_report(y_test, GBM1_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Initial score: 64%\n* Classes 5,6,7 have better f1 scores, due to bigger amount of data.\n* Alcohol is the most influential parameter."},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrixÂ¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nCM = confusion_matrix(y_test, GBM1_pred)\n\nz= list(set(y))\nsns.heatmap(CM, annot=True, fmt=\"d\", xticklabels=z, yticklabels=z )\nplt.title('confusion matrix - GBM model without tuning')\nplt.xlabel('Real')\nplt.ylabel('Predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the identifications were 5,6,7.\n* Most of the Incorrect identifications of 5 were identified as 6 or \n* Most of the Incorrect identifications of 6 were identified as 5 or 7\n\n* Removing the less common classes (3 and 8) may lead to a better classification model."},{"metadata":{},"cell_type":"markdown","source":"### Sub set of the data, without class 3,8"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub_set=df.loc[(df['quality']!=3) &(df['quality']!=8)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sub_set, X_test_sub_set, y_train_sub_set, y_test_sub_set = train_test_split(df_sub_set.iloc[:,:-1],df_sub_set.iloc[:,-1], test_size=0.2,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM1_sub_set = GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000,max_depth=3, min_samples_split=2, min_samples_leaf=1)\nGBM1_sub_set.fit(X_train_sub_set,y_train_sub_set)\npredictors_sub_set=list(X_train_sub_set)\nfeat_imp_sub_set = pd.Series(GBM1.feature_importances_, predictors_sub_set).sort_values(ascending=False)\nfeat_imp_sub_set.plot(kind='bar', title='Importance of Features')\nplt.ylabel('Feature Importance Score')\nGBM1_score_sub_set = GBM1_sub_set.score(X_test_sub_set, y_test_sub_set)\nprint('Accuracy of the GBM on test set: {:.3f}'.format(GBM1_score_sub_set))\nGBM1_pred_sub_set=GBM1_sub_set.predict(X_test_sub_set)\nprint(classification_report(y_test_sub_set, GBM1_pred_sub_set))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Score: 0.69.\n* After the removal of the less frequent classes the overall score was not improved significantly."},{"metadata":{},"cell_type":"markdown","source":"## GBM model with tuned parameters"},{"metadata":{},"cell_type":"markdown","source":"### Tuning GBM parameters with GridSearch"},{"metadata":{},"cell_type":"markdown","source":"#### Tuning n_estimators and Learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"tuning_n_estimators_prams = {'learning_rate':[0.1,0.05,0.01], 'n_estimators':[500,1000,1500,2000]}\nGBC =GradientBoostingClassifier(random_state=random_state)\ntuning_n_estimators = GridSearchCV(estimator=GBC,param_grid = tuning_n_estimators_prams, scoring='accuracy', cv=5)\ntuning_n_estimators.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score based on {} is {}\".format(tuning_n_estimators.best_params_,tuning_n_estimators.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuning max_depth"},{"metadata":{"trusted":true},"cell_type":"code","source":"tuning_max_depth_prams = {'max_depth':[3,5,7] }\ntuning_max_depth = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1,n_estimators=500), \n            param_grid = tuning_max_depth_prams, scoring='accuracy', cv=5)\ntuning_max_depth.fit(X_train,y_train)\nprint(\"Best score based on {} is {}\".format(tuning_max_depth.best_params_,tuning_max_depth.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuning min_samples_split and min_samples_leaf"},{"metadata":{"trusted":true},"cell_type":"code","source":"tuning_min_samples_prams = {'min_samples_split':[2,6,10], 'min_samples_leaf':[1,3,5]}\n\ntuning_min_samples = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1, n_estimators=500,max_depth=7), \n            param_grid = tuning_min_samples_prams, scoring='accuracy', cv=5)\ntuning_min_samples.fit(X_train,y_train)\nprint(\"Best score based on {} is {}\".format(tuning_min_samples.best_params_,tuning_min_samples.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GBM model with the tuned parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM_TUN = GradientBoostingClassifier(learning_rate=0.1, n_estimators=500,max_depth=7, min_samples_split=6, min_samples_leaf=5)\nGBM_TUN.fit(X_train,y_train)\npredictors_TUN = list(X_train)\nfeat_imp = pd.Series(GBM_TUN.feature_importances_, predictors_TUN).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Importance of Features')\nplt.ylabel('Feature Importance Score')\nprint('Accuracy of the GBM on test set: {:.3f}'.format(GBM_TUN.score(X_test, y_test)))\nGBM_TUN_pred=GBM_TUN.predict(X_test)\nprint(classification_report(y_test, GBM_TUN_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"CM_GBM_TUN = confusion_matrix(y_test, GBM_TUN_pred)\nz= list(set(y))\nsns.heatmap(CM_GBM_TUN, annot=True, fmt=\"d\", xticklabels=z, yticklabels=z )\nplt.title('confusion matrix - tuned GBM')\nplt.xlabel('Real')\nplt.ylabel('Predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The Accuracy of the GBM model with the tuned parameters (0.68) is slightly better then the GBM model without tuning (0.64)."},{"metadata":{},"cell_type":"markdown","source":"## GBM with cross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nGBC_CV = GradientBoostingClassifier(learning_rate=0.1, n_estimators=500,max_depth=7, min_samples_split=6, min_samples_leaf=5, random_state=random_state)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.00001,random_state=random_state)\nkfold = KFold(n_splits=5, random_state=random_state)\nscores = cross_val_score (GBC_CV, X_train, y_train, cv=kfold)\nprint(\"Scores on each subset:\")\nprint(scores) \navg = (100*np.mean(scores), 100*np.std(scores)/np.sqrt(scores.shape[0]))\nprint (\"Average score and uncertainty: (%.2f +- %.3f)%%\"%avg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Accuracy of the GBM model with cross-validation (0.69) is relatively close to the GBM model with tuning (0.68)."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv('../input/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2= df2.iloc[:,:-1]\ny2 =df2.iloc[:,-1]\nx_train2, x_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning n_estimators"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier(random_state=random_state)\n\nparam_grid = {'n_estimators': [10, 50, 100,200]}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(x_train2, y_train2)\n\nprint(\"Best params {}\".format(CV_rfc.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier with Tuned n_estimators"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_200 = RandomForestClassifier( n_estimators=200,random_state=random_state)\nrfc_200.fit(x_train2, y_train2)\nrfc_200_pred = rfc_200.predict(x_test2)\nprint('Accuracy of the RF on test set: {:.3f}'.format(rfc_200.score(x_test2, y_test2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_rfc = confusion_matrix(y_test2, rfc_200_pred)\nz= (3,4,5,6,7,8,9)\nsns.heatmap(cm_rfc, annot=True, fmt=\"d\", xticklabels=z, yticklabels=z )\nplt.title('confusion matrix - Random Forest')\nplt.xlabel('Real')\nplt.ylabel('Predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier with K-Fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.000001,random_state=10)\nkfold = KFold(n_splits=5, random_state=10)\nmodel = RandomForestClassifier(n_estimators=200)\nresults = cross_val_score(model, X_train, y_train, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"|__Random Forest Classifier with K-Fold resulted in the best classification scores (0.71).__"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}