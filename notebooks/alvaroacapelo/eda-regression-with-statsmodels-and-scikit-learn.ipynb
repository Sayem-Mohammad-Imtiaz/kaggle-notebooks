{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first practice Kernel in Data Science. My main goal was to use some basic Statsmodels API to evaluate some simple regression models. Later, I go from what I've learned and build on top of that with Scikit-Learn API as it's, to me, more friendly and more flexible.\n\nHope to be contributing to the Data Science community. Feel free to make on this Kernel better as well by comenting and suggesting."},{"metadata":{},"cell_type":"markdown","source":"# Workflow\n1. Load relevant libraries\n2. Problem Definition\n3. Data acquisition\n4. Target variable inspection\n5. Features inspection\n6. Exploratory Data Analysis\n    - 6.1. Univariate\n    - 6.2. Bivariate\n7. Modelling\n    - 7.1. Statsmodels\n    - 7.2. Scikit-Learn"},{"metadata":{},"cell_type":"markdown","source":"# 1. Load relevant libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic libraries for data acquisition, handling and visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# libraries for modelling\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import explained_variance_score, r2_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Problem definition\nWe want to predict diamond price (continuous, numerical) based on certain measurements (features) using use prices already available. It's a Supervised Regression task.\n\nQuestions we may want to answer:\n1. Is there a relationship between the predictors and price?\n2. If so, how strong it this relationship?\n3. Which predictors seem to have greater impact?\n4. Can we predict price with the predictors available?"},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Acquisition"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diam = pd.read_csv('../input/diamonds.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I separate a DataFrame `data` so that the changes I perform are recorded on it, but there's still one version of the data untouched."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = diam.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Target variable inspection"},{"metadata":{},"cell_type":"markdown","source":"It's a continuous variable. A few points we may want to check:\n1. Are there missing values?\n2. Are there absurd values, i.e., negative, zero, strings, data type problems etc.?\n3. Are there outliers?\n4. How is it distributed over the range of values? Does it seem to follow a particular distribution?"},{"metadata":{},"cell_type":"markdown","source":"We begin by taking a look at some 10 random observations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check fror null values on target variable\ndiam.price.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We have {:.0f} priced diamonds.\".format(diam.price.count()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the data type and some statistics."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"diam.price.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`dtype` is `float64`, so only numeric entries.\n\nWhat about the values themselves? Some very low and some very high. We follow with a visual inspection on the distribution of prices."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(12,6))\nsns.boxplot(y='price', data=diam, ax=ax[0])\nsns.boxenplot(y='price', data=diam, ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.kdeplot(diam.price, color='b', shade=True, ax=ax[0])\nsns.kdeplot(diam.price, color='r', shade=True, bw=100, ax=ax[1])\n\nax[0].set_title('KDE')\nax[1].set_title('KDE, bandwidth = 100')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we learn:\ni. No missing values.\n\nii. No data type errors or typos.\n\niii. Seemingly high number of high values above 1.5 * Inter-Quartile Range, but there are too many to be outliers: probrably are highly priced diamonds, therefore, can't lose this information.\n\niv. About the distribution:\n- highly skewed to the right\n- 1/4 of the diamonds below 950\n- 50% of the diamonds below 2,400\n- 1/4 of the diamonds between 2,400 and 5,300\n- 50% of the diamonds between 950 and 5,300\n\nGiven the skewness of the distribution, I've seen in a lot of kernel authors perfoming a log-transformation on the target variable. After this transformation, the distribution of values allows for better statistical analysis and seems to improve models' performances. We'll see with Statsmodels that this will in fact improve the model and why it is so, but for now, I won't do that."},{"metadata":{},"cell_type":"markdown","source":"# 5. Feature inspection"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Feature Explanation\nAs I know absilutely nothing about diamonds beforehand, checking some literature is always a good idead. The following quote from the Gemological Institute of America summarizes the __[diamond quality factors](https://www.gia.edu/diamond-quality-factor)__: \"Diamonds with certain qualities are more rare—and more valuable—than diamonds that lack them. These are known as the 4Cs. When used together, they describe the quality of a finished diamond. The value of a finished diamond is based on this combination.\"\n\nIn the American Gem Society there's a comprehensive explanation of the __[4 C's of diamonds](https://www.americangemsociety.org/page/4cs)__, which is presented below, very summarized."},{"metadata":{},"cell_type":"markdown","source":"### 5.1.1. Cut\nThe __[cut](https://www.americangemsociety.org/page/diamondcut)__ of a diamond refers to how well the diamond’s facets interact with light, the proportions of the diamond, and the overall finish of the diamond.\n\nIt is not to be confused with the shape, (like emerald or round,) or facet arrangement, (like brilliant, or step cut), but is instead a reference to the craftsmanship of the diamond and how it factors into the diamond’s brilliance. AGS grades cut on a scale from 0 to 10, with 0 being “Ideal” and 10 being “Poor.” AGS has a proprietary numeric and verbal descriptors for cut. The numeric descriptors for the Diamond Cut Grade follow the American Gem Society's standards for how well a diamond is cut. The verbal descriptors are AGS Ideal, Excellent, Very Good, Good, Fair, and Poor.\n\n![cut_grading](https://cdn.ymaws.com/www.americangemsociety.org/resource/resmgr/images/GemsJewelry/135781409756963.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### 5.1.2. Color\nThe __[color](https://www.americangemsociety.org/page/diamondcolor)__ of a diamond actually refers to the lack of color in a diamond, with perfectly colorless diamonds considered the highest quality with the highest value, and brown or yellow diamonds being the lowest quality. Using a master set of diamonds specifically chosen based on their range of color, a grader picks up the diamond and places it next to the individual diamonds in the master set. The diamond grader then decides the color grade based on the saturation of the color compared to the master set.\n\n![color_grading](https://cdn.ymaws.com/www.americangemsociety.org/resource/resmgr/images/GemsJewelry/164901433526626.JPG)"},{"metadata":{},"cell_type":"markdown","source":"### 5.1.3. Clarity\n__[Clarity](https://www.americangemsociety.org/page/diamondclarity)__ is the state of being clear or transparent. Diamond clarity is the presence or absence of characteristics called inclusions in the diamond. In short, inclusions are the internal or external flaws of the diamond. The size and severity of these flaws determines the grade. Since many inclusions and blemishes are very small, and can be difficult to see with the naked eye, they are graded at 10x magnification. Clarity grade is determined on a scale of decreasing clarity from the highest clarity (Flawless or FL) to the lowest clarity (Included 3, or I3).\n\n![clarity_grading](https://cdn.ymaws.com/www.americangemsociety.org/resource/resmgr/images/GemsJewelry/79661461782004.png)\n\n**AGS 0 - Flawless or Internally Flawless:** no inclusions or blemishes visible under 10x; Internally Flawless diamonds have no inclusions visible under 10x, but can have very minor blemishes (marks and features confined to the surface only).\n\n**AGS 1 or 2 - VVS:** has minute inclusions that are difficult for a skilled grader to see under 10x magnification.\n\n**AGS 3 or 4 - VS:** have minor inclusions.\n\n**AGS 5, 6, or 7 - SI:** have noticeable inclusions that are fairly easy to see under 10x magnification; sometimes, these inclusions can be visible to the unaided eye.\n\n**AGS (7, 8, 9, or 10) - I:** have inclusions that are obvious at 10x magnification; sometimes, they can be seen with the naked eye. At the lower clarities, may have an effect on the diamond’s durability.\n\nThe modern clarity scale was invented in the 1950s, by a former president of GIA, Richard T. Liddicoat, Jr. With minor modifications, it has been the universal standard ever since, using verbal descriptors most are now familiar with: Flawless, Internally Flawless, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, and I3."},{"metadata":{},"cell_type":"markdown","source":"### 5.1.4. Carat\n__[Carat](https://www.americangemsociety.org/page/diamondcarat)__ is the unit of measurement for the physical weight of diamonds. One carat equals 0.200 grams or 1/5 gram and is subdivided into 100 points.\n\n![carat_ags](https://cdn.ymaws.com/www.americangemsociety.org/resource/resmgr/images/GemsJewelry/116161409755629.jpg)\n\nLarge diamonds are rarer than smaller ones, and as the carat weight increases, the value of the diamond increases as well. However, the increase in value is not proportionate to the size increase. For example, a 1-carat diamond will cost more than twice that of a ½-carat diamond (assuming Color, Clarity and Cut grade are the same). Weight does not always enhance the value of a diamond, either. Two diamonds of equal weight may be unequal in value, depending upon other determining factors such as Cut, Color and Clarity."},{"metadata":{},"cell_type":"markdown","source":"### 5.1.5. Depth and Table\nFrom a different __[reference](https://beyond4cs.com/grading/depth-and-table-values/)__ I got information about depth and table."},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.5.1. Depth\n\nThe depth of a diamond is its height (in millimeters) measured from the culet to the table. On a grading report, there are normally two measurements of depth – the first is the actual depth measurement in millimeters, and the second is the depth percentage, which shows how deep the diamond is in relation to its width.\n\n*As explained Dataset page, `depth` here is the depth percentage, which can be approximated by $depth = [z / average(x, y)] * 100$.*\n\n![Depth percentage](https://beyond4cs.com/wp-content/uploads/2013/02/depthpercentagesofdiamond.png)\n\nThe ideal depth percentage varies with the shape of the diamond. A depth percentage that may be too much for one shape might be essential for another. For instance, a princess cut with a 75 or 77 percent depth would still be considered acceptable and can yield an attractive diamond. On the other hand, a depth of 65 percent for a round diamond would be excessive and be detrimental to its beauty."},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.5.2. Table\nThe table refers to the flat facet of the diamond which can be seen when the stone is face up. It also happens to be the largest facet on a diamond and plays a vital role on brilliance and light performance of a stone.\n\nThe main purpose of the table facet is to refract light rays entering the diamond and to allow reflected light rays from the pavilion facets back into the observer’s eye.\n\n![Table_percentage](https://beyond4cs.com/wp-content/uploads/2013/02/tableandtablepercentagesofdiamond.png)\n\nIn a grading report, table percentage is calculated based on the size of the table divided by the average girdle diameter of the diamond. So, a 60 percent table means that the table is 60 percent wide as the diamond’s outline."},{"metadata":{},"cell_type":"markdown","source":"### 5.1.6. x, y and z\n- x: length, in mm\n- y: width, in mm\n- z: depth, in mm\n\nThose are simply the dimensions of the diamonds. As most diamonds are approximately round-shaped, we expect `x` ~ `y`. `z` is the absolute value of depth, and should be coherent with the `depth`, `x` and `y` values.\n\nAlso, approximating a diamond for a prism, we should expect `carat` to be proportional to $x * y * z$."},{"metadata":{},"cell_type":"markdown","source":"### What we learn:\nFrom the basic reserach of the literature, we expect:\n- carat, clarity, color and cut to play a big role diamond price\n- depth and table are also important, but not clearly how\n- x,y and z are important as they help determine carat and depth, but seem to be of secondary importance\n\nI won't assume an order of importance between the 4 Cs since for me it isn't quite clear from the previous explanations which order this should be. On the other hand, I don't expect to any of the dimensions to be largely relevant as other variables already capture their importanca with them."},{"metadata":{},"cell_type":"markdown","source":"## 5.2. Data cleaning\nThis step is crucial. Machine Learning algorithms don't work with NaNs (how missing values are encoded in `pandas` and `numpy`, and some are very sensitive to outliers and absurd values.\n\nMoreover, Data Cleaning focuses on removing problematic data entries whenever possible, be it for computational and/or statistical reasons."},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `.info()` method helps analysing a lot on information regarding data preparation."},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nulls\nNo values missing or encoded as `NaN`."},{"metadata":{},"cell_type":"markdown","source":"### Data types\nThree variables with `object` data type. This is the data type of strings/text in Python. It is relavant to dig a little deeper here and understanding why might it be that these entries are encoded as strings:\n1. are they text data?\n2. are they categories written as text?\n3. are they numeric that due to errors got coerced into `object`?\n\nIt's important to differentiate case 2 from case 1 as there's a specific `category` data type in Python, that saves memory and allows for ordering, which helps in data analysis tasks. In the case 3, it might be that we'll need to perform some more cleaning steps.\n\nBelow, I summarize the entries on each column to see unique values."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['cut', 'color', 'clarity']:\n    print(\"Column : {}\".format(col))\n    print(diam[col].value_counts())\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected from the feature explanation section, these entries are categories written in the form of text. Chaging data type is good practice here for visualization and data analysis."},{"metadata":{},"cell_type":"markdown","source":"**Cut**\n![cut_grading](https://cdn.ymaws.com/www.americangemsociety.org/resource/resmgr/images/GemsJewelry/135781409756963.jpg)"},{"metadata":{},"cell_type":"markdown","source":"No null entries,as observed with the `.info()` method. Next, turn into `category` and set an order of importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# turn to 'categorical' data type and order\ncut_dtype = pd.api.types.CategoricalDtype(\n    categories=['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], \n    ordered=True)\ndata['cut'] = diam.cut.astype(cut_dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.cut.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Color**\n\n![color_grading](https://cdn.ymaws.com/www.americangemsociety.org/resource/resmgr/images/GemsJewelry/164901433526626.JPG)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"No null entries, as observed with the `.info()` method. Next, turn into `category` and set an order of importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# turn to 'categorical' data type and order\ncolor_dtype = pd.api.types.CategoricalDtype(\n    categories=['J', 'I', 'H', 'G', 'F', 'E', 'D'], \n    ordered=True)\ndata['color'] = diam.color.astype(color_dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.color.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clarity**\n\n![clarity_grading](https://cdn.ymaws.com/www.americangemsociety.org/resource/resmgr/images/GemsJewelry/79661461782004.png)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"No null entries, as observed with the `.info()` method. Next, turn into `category` and set an order of importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"clar_dtype = pd.api.types.CategoricalDtype(\n    categories=['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'], \n    ordered=True)\ndata['clarity'] = diam.clarity.astype(clar_dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.clarity.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Carat**\n\nContinuous numerical variable then it's good to check for: missing values, absurd values, scale and possible errors."},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.carat.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a quick glance at the distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(y='carat', data=diam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Taking a look at the dataset as a whole with heavy weight and light weight carats:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(diam[diam.carat < 3].sample(5))\nprint()\nprint(diam[diam.carat > 3].sample(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, higher `carat` weights linked to greater dimensions of diamonds. Apparently large values (above 4) are not absurds but big diamonds."},{"metadata":{},"cell_type":"markdown","source":"**Depth %**\n\nContinuous numerical variable: check for missing values, absurd values, scale and possible errors.\n\nNo null entries,as observed with the `.info()` method. In case we forgot that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"At least one null entry? {}\".format(diam.depth.isnull().any()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.depth.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(y='depth', data=diam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Values highly concentrated between ~58 and 65, but there are observations above and below. How many?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# diamonds, depth > 65:\", diam[diam.depth > 65].depth.count())\nprint()\nprint('Sample')\nprint(diam[diam.depth > 65].sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# diamonds, depth < 58:\", diam[diam.depth < 58].depth.count())\nprint()\nprint('Sample')\nprint(diam[diam.depth < 58].sample(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depth% > 65:\n- around 835 diamonds\n- values of 'x', 'y', and 'z' don't particularly stand out: 'z' just seems to be close to them, probrably due to the shape of the cut\n\nDepth% < 58:\n- around 581 diamonds\n- values of 'x', 'y', and 'z' don't particularly stand out: 'z' just seems to be far from them, probrably due to the shape of the cut\n\nNo reason to cut out points outside Inter-Quartile Range of boxplot, but it's probably a good thing to verify how cut and depth interact in Exploratory Data Analysis."},{"metadata":{},"cell_type":"markdown","source":"**Table %**\n\nContinuous numerical variable: check for missing values, absurd values, scale and possible errors.\n\nFrom `.info()` performed at the beginning of the cleaning stage, we know there are no missign entries, but if we didn't remember that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"At least one null entry? {}\".format(diam.table.isnull().any()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.table.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(y='table', data=diam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Values highly concentrated between 50 and 65. How many?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# diamonds, table% > 65:\", diam[diam.table > 65].table.count())\nprint()\nprint(\"Sample:\")\nprint(diam[diam.table > 65].sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# diamonds, table% < 50:\", diam[diam.table < 50].table.count())\nprint()\nprint(\"Sample:\")\nprint(diam[diam.table < 50].sample(4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Values of 'x', 'y' and 'z' don't particularly stand out. Table is high or low probrably due to shape of cut."},{"metadata":{},"cell_type":"markdown","source":"**x, y and z**\n\nContinuous numerical variable: check for missing values, absurd values, scale and possible errors."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Any null entry for \\'x\\'? {}\".format(diam.x.isnull().any()))\nprint(\"Any null entry for \\'y\\'? {}\".format(diam.y.isnull().any()))\nprint(\"Any null entry for \\'z\\'? {}\".format(diam.z.isnull().any()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a look at some summary statistics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"diam.loc[:, ['x', 'y', 'z']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `.describe()` method reveals that there are at least three suposedly absurd situations:\n- x = 0: no sense in a diamond with zero length\n- y = 58.9: very weird when maximum x is about 10.7\n- z = 31.8: very weird when maximum x is about 10.7\n\nVisually inspecting:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.scatterplot(x=diam.x, y=diam.y, ax=ax[0])\nax[0].set_title(\"y vs x\")\n\nsns.scatterplot(x=diam.x, y=diam.y, ax=ax[1])\nax[1].set_xlim(0, 15)\nax[1].set_ylim(0, 15)\nax[1].set_title(\"y vs x - Zoomed in\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting the axes limits between 0 and 15 reveals that the hypotheses that x is approximately equal to y was somewhat accurate. Now looking at `y` vs `carat`."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x='carat', y='y', data=diam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the potentially absurd values."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"diam[(diam.y > 10) | (diam.z > 10)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only three rows present values above 12. For simplicity, I'll arbitrarily establish that above 15 is absurd. Domain and historical knowledge would be useful here, but in the absence of any, I'll simplify.\n\nNext,let's look at the other end (low values)."},{"metadata":{"trusted":true},"cell_type":"code","source":"diam[(diam.x < 1) | (diam.y < 1) | (diam.z < 1)].sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although some preliminary analysis found no missing values (NaNs), it seems that 0.0 encodes missing values in this dataset.\n\nSince we have confirmed that `x` ~ `y`, I propose a set of steps to deal with absurd dimension values, in the following order:\n1. When 'y' is available but 'x' is absurd (> 15 or 0.0): do 'x' = 'y'\n2. For the remaining observations, when 'x' is absurd (> 15 or 0.0): replace with the mean 'x' value over the entire dataset\n3. Next, when 'y' is absurd (>15 or 0.0): do 'y' = 'x'\n4. Finally, when 'z' is absurd (> 15 or 0.0): aproximate using $depth = z / average(x, y) * 100$ (showed in the Feature Explanation)."},{"metadata":{},"cell_type":"markdown","source":"(1) When 'y' is availabele but 'x' is absurd:"},{"metadata":{"trusted":true},"cell_type":"code","source":"absurdx_i = ((diam.x == 0) | (diam.x > 15)) & (diam.y != 0)\ndata.loc[absurdx_i, 'x'] = diam.loc[absurdx_i, 'y']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(2) When 'x' is absurd but 'y' is not available\n\nFor simplicity, as this only seems to happen when both are zero, I do:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create Boolean mask to subset DataFrame\nabsurdxy_i = ((data.x == 0) | (data.y == 0))\n\n# compute mean value of x\nmean_x = np.mean(data.x)\n\n# substitute on the dataFrame\ndata.loc[absurdxy_i, ['x', 'y']] = data.loc[absurdxy_i, ['x', 'y']].replace(0, mean_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(3) Where `x` is available but `y` is absurd:"},{"metadata":{"trusted":true},"cell_type":"code","source":"absurdy_i = (((data.y > 15) | (data.y == 0)) & (data.x != 0))\ndata.loc[absurdy_i, 'y'] = data.loc[absurdy_i, 'x']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(4) When `z` is absurd:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data.z == 0) | (data.z > 15)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find rows where z is absurd\nabsurd_z = ((diam.z == 0) | (diam.z > 15))\n\n# define function to calculate z\ncalc_z = lambda row: (row['depth']/100) * (row['x'] + row['y'])/2\n\n# apply on dataframe\ndata.loc[absurd_z, 'z'] = data.loc[absurd_z, :].apply(calc_z, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['x', 'y', 'z']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all values seem to be fine!"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3. Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"I am not very skilled in this aspect, but one idea comes to mind: since 'cut' is related to 'depth' and 'table', I combine depth and table into one variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['depth_table_ratio'] = data['depth'] / data['table']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Exploratory Data Analysis - EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.1. Univariate\nHow is each predictor alone related to price?"},{"metadata":{},"cell_type":"markdown","source":"### Carat Weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(y='carat', data=data)\nprint(data[['carat']].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x='carat', y='price', data=data,\n                     edgecolors='k', alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.violinplot(y='price', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10, 5))\nsns.regplot(x='carat', y='price', data=data, ax=ax[0],\n            x_bins=10, x_estimator=np.mean, ci=None)\nsns.regplot(x='carat', y='price', data=data, ax=ax[1],\n            x_bins=10, x_estimator=np.mean, ci=None, order=3)\n\nax[0].set_title(\"Price vs Carat - 1st order linear\")\nax[1].set_title(\"Price vs Carat - 3rd order polynomial\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Partiotining carat into multiple bins for visualization shows:\n- 'carat' is positively related to 'price'\n- relationship seems non-linear\n\nLooking at how carat changes with price seems to hint yet again that log-transforming price might be a good idea (because of the shape of the relationship). Let's try."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['log_price'] = data.price.apply(np.log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=3, figsize=(15,5))\nsns.regplot(x='carat', y='log_price', data=data, x_bins=10, ax=ax[0],\n                 x_estimator=np.mean, ci=None)\nsns.regplot(x='carat', y='log_price', data=data, x_bins=10, ax=ax[1],\n                 x_estimator=np.mean, ci=None, order=2)\nsns.regplot(x='carat', y='log_price', data=data, x_bins=10, ax=ax[2],\n                 x_estimator=np.mean, ci=None, order=3)\n\nax[0].set_title(\"Log(Price) vs Carat - 1st order polynom.\")\nax[0].set_ylabel(\"Log (price)\")\n\nax[1].set_title(\"Log(Price) vs Carat - 2nd order polynom.\")\nax[1].set_ylabel(\"Log (price)\")\n\nax[2].set_title(\"Log(Price) vs Carat - 3rd order polynom.\")\nax[2].set_ylabel(\"Log (price)\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed log-transforming seems to bring the data closer to fit (with the help of a third order polynomial).\n\nNext, we do indeed create a partition in carat weight, but we split by 1.0 carat."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['carat_bin'] = pd.cut(data.carat, range(6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x='carat_bin', data=data)\n\nprint(data.carat_bin.value_counts(normalize=True, sort=True, ascending=False)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 67.6% weight 1 carat or less.\n- 96.5% weight 2 carats or less.\n- approximately 0.06% weight more than 3 carats.\n\nExpect to see majority of low prices, because majority of diamonds are are light.\nVisualizaing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(x='carat_bin', y='log_price', data=data, palette='husl', ax=ax[0])\nsns.pointplot(x='carat_bin', y='log_price', data=data, ax=ax[1])\n\nax[0].set_title(\"Distribution - log (Price) vs Carat category\")\nax[1].set_title(\"Mean log(price) vs Carat category\")\nplt.show()\n\nprint(\"Mean price per carat_bin:\")\nprint(data.groupby('carat_bin').price.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we've learned so far:\n- overall, higher the weight, higher the price\n- relationship between mean price and carat bin is not linear:\n    - first three bins: mean value increases far more than the proportional increase in carat\n    - diamonds between 2 and 3 carats, and between 3 and 4 carats: apporximately same mean price\n    - diamonds heavier than 3 carats: the price still goes up with carat but the increase is more moderate\n- there are lighter diamonds that were highly valued: why?\n\nFor the last observation, let's check highly priced diamonds with small carat weight."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data.carat <= 1) & (data.price > 12000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of highly priced diamonds:\n- Three highest prices: maximum grades for 'clarity' and 'color'\n- Fourth highest price: maximum grades for 'cut' and color, 2nd highest for 'clarity'\n\nBetween diamonds close in 'carat', exceptionally good quality drives price up. Seems to confirm that high quality grades increase price."},{"metadata":{},"cell_type":"markdown","source":"### Cut"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=3, figsize=(18, 5))\nsns.countplot(x='cut', data=data, ax=ax[0])\nsns.boxplot(x='cut', y='log_price', data=data, ax=ax[1])\nsns.stripplot(x='cut', y='log_price', data=data, ax=ax[1],\n              size=1, edgecolor='k', linewidth=.1)\nsns.pointplot(x='cut', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='cut', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per carat\")\nax[1].set_title(\"log(Price) vs Cut - Distribution\")\nax[2].set_title(\"Mean log(price) vs Cut (blue)\\nMedian log(price) vs Cut (red)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('% diamonds per cut grade:')\nprint(data.cut.value_counts(normalize=True, sort=True, ascending=False)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean log(price) per cut grade:')\nprint(data.groupby('cut').log_price.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About 'cut':\n- 87.9% above 'Very Good'\n- seems to be negatively related to 'price'\n\nHere, a non-expected behavior appears. Why would higher 'cut' grades have lower prices?\n\n**Hypotheses:**\n\nMajority of diamonds have good 'cut' grade. Also, majority of diamonds have low carat weight. 'cut' is \"receiving bad reputation\" for something it may be not responsible for.\n- most of the diamonds ~ carat < 1 ~ on average, small price\n- most of the diamonds ~ Ideal cut\n\nBy association: Ideal cut ~ small prices\n\nIt should be true, then, that 'cut' only drives up the price:\n- when comparing same carat diamonds\n- when alongside other distinctive quality factors\n\nOn Bivariate/Multivariate analysis (predictors relationships between themselves), we wnat to check that."},{"metadata":{},"cell_type":"markdown","source":"### Color"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=3, figsize=(15, 5))\nsns.countplot(x='color', data=data, ax=ax[0])\nsns.boxplot(x='color', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='color', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='color', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per color grade\")\nax[1].set_title(\"log(Price) vs Color - Distribution\")\nax[2].set_title(\"Mean log(price) vs Color (blue)\\nMedian log(price) vs Color (red)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# diamonds per color grade\")\nprint(data.color.value_counts(normalize=True, sort=True, ascending=True) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean log(price) of diamonds per color grade\")\nprint(data.groupby('color').log_price.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the basis purely of color, higher grades seem to decrease the value. Unexpected behavior.\n\n**Hypothesis:**\n\nLike with 'cut', the overall behvior is possibly being adversily affected by the majority of diamonds being light.\n\nNeed to look at combination between 'color' and other quality factors and how they affect diamond prices."},{"metadata":{},"cell_type":"markdown","source":"### Clarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=3, figsize=(15, 5))\nsns.countplot(x='clarity', data=data, ax=ax[0])\nsns.boxplot(x='clarity', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='clarity', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='clarity', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per clarity grade\")\nax[1].set_title(\"log(Price) vs clarity - Distribution\")\nax[2].set_title(\"Mean log(price) vs clarity (blue)\\nMedian log(price) vs clarity (red)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again same overall pattern:\n- majority of cases receving low price\n- lower grades with higher mean prices"},{"metadata":{},"cell_type":"markdown","source":"### Depth"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['depth']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(y='depth', data=data, ax=ax[0])\nax[0].set_title(\"Depth distribution\")\n\nsns.boxplot(y='depth', data=data, ax=ax[1])\nax[1].set_ylim(55, 70)\nax[1].set_title(\"Depth distribution - Zoomed in\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x='depth', y='log_price', data=data,\n                     alpha=0.3, edgecolor='k')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alone, doesn't help predicting price. Let's try turning to bins (\"discretizing\"):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create bins\ndepth_desc = data[['depth']].describe()\ndepth_bins = depth_desc['min':'max'].depth.tolist()\n\n# create column for bins\ndata['depth_bin'] = pd.cut(data.depth, depth_bins)\ndata.depth_bin.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=3, figsize=(15, 5))\nsns.countplot(x='depth_bin', data=data, ax=ax[0])\nsns.boxplot(x='depth_bin', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='depth_bin', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='depth_bin', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per depth bin\")\nax[1].set_title(\"log(Price) vs depth_bin - Distribution\")\nax[2].set_title(\"Mean log(price) vs depth_bin (blue)\\nMedian log(price) vs depth_bin (red)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Depth' values balanced across bins. No particular relationship. Alone, not a good predictor of price."},{"metadata":{},"cell_type":"markdown","source":"### Table"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data[['table']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(y='table', data=data, ax=ax[0])\nax[0].set_title(\"Table distribution\")\n\nsns.boxplot(y='table', data=data, ax=ax[1])\nax[1].set_ylim(50, 65)\nax[1].set_title(\"Table distribution - Zoomed in\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x='table', y='log_price', data=data,\n                     alpha=0.3, edgecolor='k')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analog to depth, doesn't seem to help predicting price. Like for depth, let's try \"discretizing\" and check for patterns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create bin list\ntable_desc = data[['table']].describe()\ntable_bins=table_desc['min':'max'].table.tolist()\ntable_bins.append(65)\ntable_bins.sort()\n\n# create column for bins\ndata['table_bin'] = pd.cut(data.table, table_bins)\ndata.table_bin.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=3, figsize=(18, 5))\nsns.countplot(x='table_bin', data=data, ax=ax[0])\nsns.boxplot(x='table_bin', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='table_bin', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='table_bin', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per table bin\")\nax[1].set_title(\"log(Price) vs table_bin - Distribution\")\nax[2].set_title(\"Mean log(price) vs table_bin (blue)\\nMedian log(price) vs table_bin (red)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When turned to bins, 'table' seems to display a positive correlation with price, except for depths greater than 65%."},{"metadata":{},"cell_type":"markdown","source":"### Depth/Table ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['depth_table_ratio'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(y='depth_table_ratio', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x='depth_table_ratio', y='log_price', data=data,\n                     edgecolor='k', alpha=.3, s=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Like depth and table, doesn't seem to be correlate with price. Again we trun to \"discretization\" for better visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create bins list\ndt_ratio_desc = data[['depth_table_ratio']].describe()\ndt_ratio_bins = dt_ratio_desc['min':'max'].depth_table_ratio.tolist()\n\n# create columns for bins\ndata['dt_ratio_bin'] = pd.cut(data.depth_table_ratio, dt_ratio_bins)\ndata.dt_ratio_bin.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=3, figsize=(6, 18))\nsns.countplot(x='dt_ratio_bin', data=data, ax=ax[0])\nsns.boxplot(x='dt_ratio_bin', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='dt_ratio_bin', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='dt_ratio_bin', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per dt_ratio_bin\")\nax[1].set_title(\"log(Price) vs dt_ratio_bin - Distribution\")\nax[2].set_title(\"Mean log(price) vs dt_ratio_bin (blue)\\nMedian log(price) vs dt_ratio_bin (red)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean price seems to decrease as depth to table ratio increases."},{"metadata":{},"cell_type":"markdown","source":"### Valuable diamonds: are they distributed differently?\nAs all boxplots displaying price display a large number of diamonds outside Inter-Quartile Range, I investigate a little further highly priced diamonds. Is there any particular pattern for the other variables?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(y='price', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's arbitrarily look at diamonds above 10,000."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['high_price'] = data.price.apply(lambda x: 1 if x >= 10000 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Carat**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x='high_price', y='carat', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With larger diamonds come higher prices."},{"metadata":{},"cell_type":"markdown","source":"**Cut**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x='cut', data=data, hue='high_price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not really good to see as there are way less highly priced diamonds. Let's try using a table, normalization and some colors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pricebin_cut_ct = pd.crosstab(data.high_price, data.cut, values=data.price, \n                              aggfunc='count', normalize='index')\npricebin_cut_ct.style.background_gradient(cmap='autumn', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"In the case of the top 3 cut grades:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pricebin_cut_ct[['Very Good', 'Premium', 'Ideal']].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Highly priced diamonds don't seem to present a different distribution across 'cut' grades: both have majority of diamonds on high grades (pprobably because overall there are more good grades than bad ones)."},{"metadata":{},"cell_type":"markdown","source":"**Color**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x='color', data=data, hue='high_price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pricebin_color_ct = pd.crosstab(data.high_price, data.color, values=data.price, \n                                aggfunc='count', normalize='index')\npricebin_color_ct.style.background_gradient(cmap='autumn', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the case of color highly priced diamonds seem to appear more often alongdside \"bad\" color grades than with \"good\" color grades."},{"metadata":{},"cell_type":"markdown","source":"**Clarity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x='clarity', data=data, hue='high_price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pricebin_clarity_ct = pd.crosstab(data.high_price, data.clarity, values=data.price, \n                                  aggfunc='count', normalize='index')\npricebin_clarity_ct.style.background_gradient(cmap='autumn', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No clear difference in distributions on the basis of clarity."},{"metadata":{},"cell_type":"markdown","source":"**Depth**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(x='high_price', y='depth', data=data, ax=ax[0])\nsns.boxplot(x='high_price', y='depth', data=data, ax=ax[1])\n\nax[0].set_title(\"Depth distribution by high_price\")\nax[1].set_ylim((58, 66))\nax[1].set_title(\"Depth distribution by high_price\\nZoomed in\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aside from having less points outside the box-and-whiskers, the distributions seem analogous."},{"metadata":{},"cell_type":"markdown","source":"**Table**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(x='high_price', y='table', data=data, ax=ax[0])\nsns.boxplot(x='high_price', y='table', data=data, ax=ax[1])\n\nax[0].set_title(\"Table distribution by high_price\")\nax[1].set_ylim((50, 65))\nax[1].set_title(\"Table distribution by high_price\\nZoomed in\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very similar distributions as well."},{"metadata":{},"cell_type":"markdown","source":"### Conclusions: What we've learned\n1. All predictors, when used solely, have large and small priced diamonds over the entire range of their values/categories.\n1. Carat weight:\n    - has positive correlation with price and relationship seems highly non-linear\n    - carat distribution is different between more valuable and less valuable diamonds.\n2. Cut, Color, Clarity:\n    - appear to have negative correlation with mean price\n    - apart from Color, frequency of diamonds is very similar across all grades.\n3. Depth:\n    - don't seem to have any clear relationsip with price\n4. Table:\n    - appear to have a negative correlation with price"},{"metadata":{},"cell_type":"markdown","source":"## 6.2. Multivariate"},{"metadata":{},"cell_type":"markdown","source":"First it's better to encode quality, categorical variables so they get visible."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cut_encod'] = LabelEncoder().fit_transform(np.asarray(data.cut))\ndata['color_encod'] = LabelEncoder().fit_transform(np.asarray(data.color))\ndata['clarity_encod'] = LabelEncoder().fit_transform(np.asarray(data.cut))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_mat = data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,10))\nax = sns.heatmap(cor_mat, cmap='autumn', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"From the heatmap, we see that carat, x, y and z are highly correlated with price and between themselves. Drawing a pair plot in order to visualize as scatter plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data, vars=['log_price', 'price', 'carat', 'x', 'y', 'z'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The same kind of relationship found between 'price' and 'carat' is reproduced between 'price' and the dimensional features 'x', 'y', and 'z'.** This is largely due to these dimensions being highly correlated with 'carat' and between themselves.\n\nFor simplicity, and to prevent high collinearity from affecteing the coefficient estimates of the model I'll drop 'x', 'y', 'z', since carat seem to"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['carat_bin', 'high_price', 'x', 'y', 'z'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data, vars=['log_price', 'price', 'depth', 'table', 'depth_table_ratio'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since 'depth_table_ratio' doesn't add information, will also drop (alongside epth and table bins."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['depth_table_ratio', 'dt_ratio_bin', 'depth_bin', 'table_bin'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define cmap for bivariate visualization:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cut vs Clarity\nHow does the interaction between 'cut' and 'clarity' affect the mean price?"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.cut, data.clarity, data.log_price, aggfunc=np.mean)\nprint(\"Table 1. Mean log(price) map - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Table 1: for a given value of cut, higher mean prices are more associated with lower clarity grades.\n\nHowever, when we look at Tables 2 and 3, we verify that the mean price is driven mainly by mean carat weight. In fact, the Table for price seems a combination of the effects of carat and count of diamonds."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.cut, data.clarity, data.carat, aggfunc=np.mean)\nprint(\"Table 2. Mean carat map - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.cut, data.clarity)\nprint(\"Table 3. Count diamonds - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cut vs Color"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.cut, data.color, data.log_price, aggfunc=np.mean)\nprint(\"Table 4. Mean log(price) - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.cut, data.color, data.carat, aggfunc=np.mean)\nprint(\"Table 5. Mean carat - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.cut, data.color)\nprint(\"Table 6. Count of diamonds - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar behavior to the observed for cut vs clarity: price is mainly affected by carat, but the count of diamonds shifts towards higher grades."},{"metadata":{},"cell_type":"markdown","source":"## Clarity vs Color"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.clarity, data.color, data.log_price, aggfunc=np.mean)\nprint(\"Table 7. Mean log(price) - clarity vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Color' seems to matter more for low 'cut' grades."},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.clarity, data.color, data.carat, aggfunc=np.mean)\nprint(\"Table 8. Mean carat - clarity vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data.clarity, data.color)\nprint(\"Table 9. Count of diamonds - clarity vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, similar behavior."},{"metadata":{},"cell_type":"markdown","source":"## Depth vs Cut"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(y='depth', kind='violin', hue='cut', data=data,\n                col='cut', col_wrap=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting result: it seems that high 'cut' grades tend to be found more often around 60 and 65% (more concentrated)."},{"metadata":{},"cell_type":"markdown","source":"## Table vs Cut"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(y='table', kind='violin', hue='cut', data=data,\n                col='cut', col_wrap=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here a very similar finding to that of table: higher cut grades tend to be found around specific table values. Specifically, it seems to be between 55 and 63."},{"metadata":{},"cell_type":"markdown","source":"## Depth vs Table vs Cut"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.relplot(x='depth', y='table', data=data, hue='cut',\n                col='cut', col_wrap=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As observed, better cuts tend to be more restrictive in terms of the range of depth and table values. However, being inside this range does not guarantee a good cut.\n\nFrom this, we learn that cut may help estimate depth and table, but the other way around is not true."},{"metadata":{},"cell_type":"markdown","source":"## Fixed carat: how is price related to other features?\nHow quality features fare for a fixed carat: does increase in quality mean increase in price?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.carat.value_counts(sort=True, ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we select carat = 0.3 for the maximum number of diamonds with same carat."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_fixcarat = data[data.carat == 0.3]\ndata_fixcarat.carat.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cut vs Color"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.color, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 10. Mean log(price) for carat = 0.3 - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.color, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 11. Mean log(price) for carat = 0.3 - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation:\n- Table 10. for a given carat and cut, higher color grade related to increase in price.\n- Table 11. for a given carat and color, higher cut grade doesn't necessarily mean increase in price.\n\nLooking at the count of diamonds below."},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.color, data_fixcarat.log_price, aggfunc='count')\nprint(\"Table 12. Count of diamonds for carat = 0.3 - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cut vs Clarity"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 13. Mean log(price) for carat = 0.3 - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 14. Mean log(price) for carat = 0.3 - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation:\n- Table 13. for a given carat and cut, higher clarity grade related to increase in price.\n- Table 14. for a given carat and clarity, higher cut grade doesn't necessarily mean increase in price.\n\nLooking at the count of diamonds below."},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc='count')\nprint(\"Table 15. Mean log(price) for carat = 0.3 - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Color vs Clarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.color, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 16. Mean log(price) for carat = 0.3 - color vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.color, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 17. Mean log(price) for carat = 0.3 - color vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation:\n- Table 16. for a given carat and cut, higher clarity grade related to increase in price.\n- Table 17. for a given carat and clarity, higher cut grade doesn't necessarily mean increase in price.\n\nHere, different than what has been seen previously, both color and clarity grades seem to increase price very clearly.\n\nBelow we look at the count of diamonds once again."},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = pd.crosstab(data_fixcarat.color, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc='count')\nprint(\"Table 18. Count of diamonds for carat = 0.3 - color vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions: what to expect from the models\n1. Carat is highly related to price in a non-linear  fashion\n    - log-transforming price + adding polynomial terms improved the fit visually\n2. Cut, Color and Clarity: given a carat value, we expect them to increase price as the grade is higher, but:\n    - Color and clarity clearly displayed that increase when combined for a fixed carat value\n    - Cut, from previous literature, is expected to increase price, but we couldn't see it so clearly (other aspects may be affecting)\n3. Table: from univariate analysis, increase in table is related to decrease in price - expect to see negative coefficient\n4. Depth: no very clear relationship with price"},{"metadata":{},"cell_type":"markdown","source":"# 7. Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.1. Statsmodels"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['cut_encod', 'color_encod', 'clarity_encod'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data, drop_first=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['price', 'log_price'], axis=1).values\ny = data.price.values\n\nassert X.ndim == 2\nassert y.ndim == 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, \n                                                    random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.1.1. First Model: price ~ carat\nAs a first model, I start with the simplest possible as a starting point."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = sm.add_constant(X_train[:, 0])\ntrain_y = y_train.copy()\n\nlm1 = sm.OLS(train_y, train_X).fit()\nlm1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- $R^2$ achieved of 0.850, good for a starting value\n- Coefficients are significant, as p-values are low\n- F-test for regression states there is a correlation between predictors and response\n- residuals are non-normal (form Omnibus and Jarque-Bera)\n\nProceeding to examine residuals:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_y = lm1.fittedvalues\nres = lm1.resid\nres_student = lm1.get_influence().resid_studentized_internal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.2, edgecolors='k', linewidths=.1)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=1, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. LOWESS (Locally WEighted Scatterplot Smoothing) curve shows a U-shape, suggesting a non-linear relationship is present\n2. Residuals should be equally scattered around zero-line for all fitted values (constant variance of residuals or homoscedacity): the plot displays a non-equally distributed behavior,i.e., hetroscedacity.\n\n(1) was expected, since we saw visually that price and carat relationship was not linear. One way to solve it would be a non-linear transformation on carat.\n\n(2) could be solved using a non-linear transformation on price, such as $\\sqrt{price}$ or $log(price)$.\n\nBoth are suggested by Hastie T. et al. in their book *Introduction to Statistical Learning*. We've visually tested both, so we'll now model them to see the results."},{"metadata":{},"cell_type":"markdown","source":"### 7.1.2. Second model: log(price) ~ carat\nFirst just the log-transformation of the response."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = sm.add_constant(X_train[:, 0])\ntrain_y = np.log(y_train)\n\nlm2 = sm.OLS(train_y, train_X).fit()\nlm2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_y = lm2.fittedvalues\nres = lm2.resid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.2, edgecolors='k', linewidths=.1)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=1, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Residuals still ehxibit highly non-linear pattern and heteroscadacity wasn't really taken care for. Regarding statistical metrics, $R^2$ got smaller and no improvement is found. We then try the polynomial approach on carat."},{"metadata":{},"cell_type":"markdown","source":"### 7.1.3. 3rd model: price ~ carat +carat^2 + carat^3 + carat^4"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = X_train[:, 0].reshape(-1, 1)\ntrain_X = PolynomialFeatures(degree=4).fit_transform(train_X)\n\ntrain_X = sm.add_constant(train_X[:, 1:])\ntrain_y = y_train.copy()\n\nlm3 = sm.OLS(train_y, train_X).fit()\nlm3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_y = lm3.fittedvalues\nres = lm3.resid\nres_student = lm3.get_influence().resid_studentized_internal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=3, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the U-shaped is practically gone, suggesting a better fit, but heteroscedacity is still present. Next, both approach are put to work together."},{"metadata":{},"cell_type":"markdown","source":"### 7.1.4. 4th model: log(price) ~ carat + carat^2 + carat^3 + carat^4"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = X_train[:, 0].reshape(-1, 1)\ntrain_X = PolynomialFeatures(degree=4).fit_transform(train_X)\n\ntrain_X = sm.add_constant(train_X[:, 1:])\ntrain_y = np.log(y_train)\n\nlm4 = sm.OLS(train_y, train_X).fit()\nlm4.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- $R^2$ increased to 0.936, a very significant increase\n- Coefficient estimates still significant\n- F statistic display there is high relationship between predictors and response still\n- Omnibus and JarqueBera suggest residuals are still far from normal\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_y = lm4.fittedvalues\nres = lm4.resid\nres_student = lm4.get_influence().resid_studentized_internal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=3, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Up until now I've only looked at the residuals plot, but now we take a look at the Studentized Residuals as well (the Residuals divied\n\n- Residuals plot displays that:\n    - variance of error terms around 0 got more or less constant\n    - LOWESS line is almost a straight line\n- Studentized Residuals Plot display there are high residuals"},{"metadata":{},"cell_type":"markdown","source":"### 7.1.5 5th model: log(price) ~ ALL features\nThis time will use data as DataFrame for better interpretation of results."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = data.drop(['carat', 'price', 'log_price'], axis=1).columns\ncarat_columns = ['carat', 'carat^2', 'carat^3', 'carat^4']\n\n# build polynomial carats, exclude cons\ncarat_poly = X_train[:, 0].reshape(-1, 1)\ncarat_poly = PolynomialFeatures(degree=4, include_bias=False).fit_transform(carat_poly)\ncarat_poly_df = pd.DataFrame(data=carat_poly, columns=carat_columns)\n\n# take quality features + concatenate carat and quality features \ntrain_X_df = pd.DataFrame(data=X_train[:, 1:], columns=columns)\ntrain_X_df = pd.concat([carat_poly_df, train_X_df], axis=1)\n\n# get responde DataFrame\ntrain_y_df = pd.DataFrame(y_train, columns=['log_price']).apply(np.log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\ntrain_X_df = sm.add_constant(train_X_df)\nlm5 = sm.OLS(train_y_df, train_X_df).fit()\nlm5.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- $R^2$ got to 0.985, a very significant increase\n- Coefficient estimates are all significant\n- F statistic's p-value suggests there is high relationship between predictors and response\n- Omnibus and JarqueBera suggest residuals are far from normal\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean\n- Condition Number is very high, indicating high collinearity between terms.\n    - Although polynomial terms tend to be somewhat collinear, this value wasn't so high when only the carat terms were used\n    - Looking back at the pairplot, we see that depth and table are fairly collinear.\n\nTo reduce collinearity and increase the accuracy of the coefficient estimates, we'll try next removing depth and table."},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_y = lm5.fittedvalues\nres = lm5.resid\nres_student = lm5.get_influence().resid_studentized_internal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Residuals plot displays that variance of error terms around 0 got more or less constant\n- Studentized Residuals Plot display there are still high residuals"},{"metadata":{},"cell_type":"markdown","source":"### 7.1.6 6th model: log(price) ~ ALL features - depth - table\nFor this model I tried first removing depth, since it has weaker relationship with price. In doing so, the coefficient estimate for table got a p-value > 0.5, meaning it became non-significant.\n\nThefere, the next model presents only the version where I removed both depth and table."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take quality features + concatenate carat and quality features \ntrain_X_df = train_X_df.drop(['depth', 'table'], axis=1)\n\n# train_y_df remains as used in the previous example","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\ntrain_X_df = sm.add_constant(train_X_df)\nlm6 = sm.OLS(train_y_df, train_X_df).fit()\nlm6.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- $R^2$ stayed at 0.985, one indication that depth and table didn't help much\n- Coefficient estimates are all significant\n- F statistic's p-value suggests there is high relationship between predictors and response\n- Omnibus and JarqueBera still high, but we'll see from residuals that theyare \"better behaved\"\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean\n- Condition Number got reduced and warning is now gone, indicating that collinearity is not a huge issue now (would need to see VIF statistic ti be sure, but for simplicity won't do that now)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_y = lm6.fittedvalues\nres = lm6.resid\nres_student = lm6.get_influence().resid_studentized_internal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Residuals plot displays that variance of error terms around 0 got more or less constant\n- Studentized Residuals Plot display there are some very high residuals, even higher then before"},{"metadata":{},"cell_type":"markdown","source":"### 7.1.7 7th model: log(price) ~ ALL features - depth - table - cut\nOne last model, now I will try removing cut: since it seemed to display a low effect on price in the multivariate analysis (cross tables), want to check."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_cut = [col for col in train_X_df.columns if 'cut' in col]\n\n# remove cut columns from DataFrame\ntrain_X_df = train_X_df.drop(columns_cut, axis=1)\n\n# train_y_df is the same as used for the previous model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\ntrain_X_df = sm.add_constant(train_X_df)\nlm7 = sm.OLS(train_y_df, train_X_df).fit()\nlm7.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model didn't suffer much:\n- $R^2$ dropped to 0.983, one indication that cut didn't help much\n- Coefficient estimates are all significant\n- F statistic's p-value suggests there is high relationship between predictors and response\n- Omnibus and JarqueBera still high, but we'll see from residuals that theyare \"better behaved\"\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_y = lm7.fittedvalues\nres = lm7.resid\nres_student = lm7.get_influence().resid_studentized_internal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Residuals plot behaves similarly but now we see a few more high studentized residuals appearing."},{"metadata":{},"cell_type":"markdown","source":"### Statsmodels: Conclusions"},{"metadata":{},"cell_type":"markdown","source":"Well, that's pretty much as far as my knowledge in Linear Regression Statistics and Statsmodels go.\n\n**Regarding the best model**\n\nModel 5 ( log(price) ~ polynomial carat) presented the best behavior in terms of residuals. However, by including categorical features like clarity and color, model 7 seemed to outperform model 5 in terms of prediction (as measured by the $R^2$), although there are high studentized residuals for all fitted values.\n\n**What we've done:**\n- used Statsmodels to build some less complex models for the price task regression\n- evaluated some statistical aspects of each model\n- developed some knowledge about the importane of the features\n- got an $R^2$ of about 0.985 on the training set\n- a few statistics pointed out that the model used might not be the best one, however the ease of interpretation is in favor of the model built.\n\n**What we do next**\n- use some powerful tools like Cross-Validation in Scikit-Learn to evaluate other models\n- evaluate the generalization capacity of each model on the test set"},{"metadata":{},"cell_type":"markdown","source":"## 7.2. Scikit-Learn"},{"metadata":{},"cell_type":"markdown","source":"Through Statsmodels we've already tried out a few models and, using its statistical API, came to a few conclusions regarding important features.\n\nTherefore, I won't start with a simple model here and build it up: rather, I'll use Ridge and Lasso regressors from Scikit-Learn (regularized regression) on the model with all features and see if I come to similar conclusions regarding feature importance and prediction capability.\n\nAlso, I'll use some of the Scikit-Learn's funcionalities to speed the preprocessing steps."},{"metadata":{},"cell_type":"markdown","source":"### 7.2.1. Get the data"},{"metadata":{},"cell_type":"markdown","source":"Instead of preparing a different, complete DataFrame as I did for Statsmodels, I'll use `FunctionTransformer()` and `FeatureUnion()` functionalities to transform the data \"on the fly\" for each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['price', 'log_price'], axis=1)    # as DataFrame\ny = data[['log_price']]    # as DataFrame\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n\nprint(type(X_train), type(X_test))\nprint(type(y_train), type(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For better understanding it's good to go step by step."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create identifiers for polynomial features and linear features\nPOLY_COLS = ['carat']\nLIN_COLS = [col for col in data.columns if col not in ['carat', 'price', 'log_price']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the functions to get each subset of the data\nget_polyfeatures = FunctionTransformer(lambda x: x[POLY_COLS], validate=False)\nget_linfeatures = FunctionTransformer(lambda x: x[LIN_COLS], validate=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create pipelines to extract and treat differently each subset of the data.\n\nBelow, I extract the features of interest for polynomial transformation and separately the feaures to go untouched (in this case)."},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_pl = Pipeline([\n    ('selector', get_polyfeatures),\n    ('polynomial', PolynomialFeatures(degree=4, include_bias=False))\n])\n\n# display polynomial features after transformation\npoly_pl.fit_transform(X_train)[:5, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_pl = Pipeline([('selector', get_linfeatures)])\n\n# display first few lines of linear features (in this case, no other operation is performed)\nlin_pl.fit_transform(X_train).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notice:** the ouput of lin_pl pipeline is a DataFrame whereas the output of poly_pl pipeline is an array. When they're put together using FeatureUnion, the final output is coerced to an array."},{"metadata":{"trusted":true},"cell_type":"code","source":"# join both pipelines into one\nprep_join = FeatureUnion([\n    ('polynomial', poly_pl),\n    ('linear', lin_pl)\n])\n\n# display final resulting array's first 5 rows\nprep_join.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to leave the whole functionality in just one cell, I reproduce the \"complete\" pipeline below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create identifiers for polynomial features and linear features\nPOLY_COLS = ['carat']\nLIN_COLS = [col for col in data.columns if col not in ['carat', 'price', 'log_price']]\n\n# create the functions to get each subset of the data\nget_polyfeatures = FunctionTransformer(lambda x: x[POLY_COLS], validate=False)\nget_linfeatures = FunctionTransformer(lambda x: x[LIN_COLS], validate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"POLY_DEG=4\n\n# join both pipelines into one\nprep_join = FeatureUnion([\n    ('polynomial', Pipeline([\n        ('selector', get_polyfeatures),\n        ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n    ])),\n    ('linear', Pipeline([('selector', get_linfeatures)]))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we use this 'union pipeline' alongside other functionalities. It just lazily treats the data on demand for the algorithms and makes it easier to change the polynomial features degree. "},{"metadata":{},"cell_type":"markdown","source":"**Procedure**\n1. build pipeline to transform the data and apply a \"grid search\" for best parameters\n2. set up and perform GridSearchCV\n3. fit model with best paramater(s)\n4. inspect coefficients\n5. predict on test set\n6. print and store metrics\n    - $R^2$,\n    - Explained Variance,\n    - RSME, \n    - MAE\n7. inspect residuals"},{"metadata":{},"cell_type":"markdown","source":"### 7.2.2. Ridge Regression: L2-norm penalization\nOn top of the Ordinary Least Squares, Ridge Regression adds a penalty term that is proportional to the square of the coefficients from the regression. The equation below is taken from Scikit-Learn's __[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)__\n\n$||y - Xw||^2_2 + alpha * ||w||^2_2$\n\nThis additional term penalizes large coefficients. The consequence is that smaller coefficients tend to produce smaller penalties and the coefficients are shrinked.\n\nThe 'alpha' term controls the regularization strength: higher alpha ~ stronger regularization ~ smaller coefficients"},{"metadata":{},"cell_type":"markdown","source":"**1. Build Pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"POLY_DEG = 4\n\n# create the regressor pipeline\nridge_pl = Pipeline([\n    ('union', FeatureUnion([\n        ('polynomial', Pipeline([\n            ('selector', get_polyfeatures),\n            ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n        ])),\n        ('linear', Pipeline([\n            ('selector', get_linfeatures)\n        ]))\n    ])),\n    ('regressor', Ridge(alpha=0.1))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform first fit and use as starting point\nridge_pl.fit(X_train, y_train)\nridge_pl.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Set up GridSearchCV, find best parameter(s)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up grid of alphas to search\nalphas = np.logspace(-4, 4, 9)\n\n# set up GridSearch object to select best alpha and fit to data\nCV_FOLDS = 5\nPARAM_GRID = {'regressor__alpha': alphas}\nSCORE = 'neg_mean_squared_error'\ngs = GridSearchCV(ridge_pl, cv=CV_FOLDS, param_grid=PARAM_GRID, scoring=SCORE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit to data and print best scores and parameters\ngs.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", gs.best_params_)\nprint(\"Best RMSE:           \", (-gs.best_score_) ** 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Fit w/ best parameter(s)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_pl.set_params(regressor__alpha=1.0)\n\nridge_pl.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Inspect coefficients visually**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_coef = np.squeeze(ridge_pl.named_steps['regressor'].coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['carat', 'carat^2', 'carat^3', 'carat^4']\npredictors.extend(X_train.columns[1:].tolist())\n\nplt.figure(figsize=(6, 8))\nplt.barh(y=range(len(ridge_coef)), width=ridge_coef)\nplt.yticks(range(len(ridge_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"Ridge Coefficient Estimates\", fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the result is very similar to the that obtained with Statsmodels:\n- carat has the biggest average weight on price, followed by the 2nd order term of carat\n- depth, table and cut have very low weight on price\n- the quality features have increasing weight with increasing quality grade, as expected."},{"metadata":{},"cell_type":"markdown","source":"**5. Predict and print metrics on test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions and actual values as arrays\ny_pred = ridge_pl.predict(X_test)\ny_true = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R^2: %.4f' % (r2_score(y_test, y_pred)))\nprint('Exp. Var.: %.4f' % (explained_variance_score(y_test, y_pred)))\nprint('RMSE: %.4f' % (mean_squared_error(y_test, y_pred) ** .5))\nprint('MAE: %.4f' % (mean_absolute_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Inspect residuals**"},{"metadata":{"trusted":true},"cell_type":"code","source":"resid = (y_true - y_pred)\nsns.jointplot(x=y_pred, y=resid, kind='reg', \n              joint_kws=dict(fit_reg=False))\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As observed in the Statsmodels' models, although there are some very high residuals, there are some good aspects to this Residuals plot:\n- Residusl resemble normality (kind of)\n- Residuals seem to be equally dispersed around zero mean for all fitted values (homoscedacity)"},{"metadata":{},"cell_type":"markdown","source":"### 7.2.3. Lasso Regression: L1-norm penalization\nOn top of the Ordinary Least Squares, Lasso Regression adds a penalty term that is proportional to the absolute magnitude of the coefficients of the regression. The equation below is taken from Scikit-Learn's __[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)__\n\n$(1 / (2 * n_{samples})) * ||y - Xw||^2_2 + alpha * ||w||_1$\n\nThis additional term penalizes large coefficients, but, different from Ridge, it actually shrinks smaller coefficients down to zero (sparsity). As a consequence, Lasso is sometimes used to perform feature selection (\"least important\" are left out).\n\nAs in Ridge, 'alpha' term controls the regularization strength: higher alpha ~ stronger regularization ~ smaller coefficients"},{"metadata":{},"cell_type":"markdown","source":"**1. Build Pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"POLY_DEG = 4\n\n# create the regressor pipeline\nlasso_pl = Pipeline([\n    ('union', FeatureUnion([\n        ('polynomial', Pipeline([\n            ('selector', get_polyfeatures),\n            ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n        ])),\n        ('linear', Pipeline([\n            ('selector', get_linfeatures)\n        ]))\n    ])),\n    ('regressor', Lasso(alpha=0.1))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform first fit and use as starting point\nlasso_pl.fit(X_train, y_train)\nlasso_pl.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Set up GridSearchCV, find best parameter(s)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up grid of alphas to search\nalphas = np.logspace(-4, 4, 9)\n\n# set up GridSearch object to select best alpha and fit to data\nCV_FOLDS = 5\nPARAM_GRID = {'regressor__alpha': alphas}\nSCORE = 'neg_mean_squared_error'\ngs = GridSearchCV(lasso_pl, cv=CV_FOLDS, param_grid=PARAM_GRID, scoring=SCORE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit to data and print best scores and parameters\ngs.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", gs.best_params_)\nprint(\"Best RMSE:           \", (-gs.best_score_) ** 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Fit w/ best parameter(s)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_pl.set_params(regressor__alpha=0.0001)\n\nlasso_pl.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Inspect coefficients visually**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coef = np.squeeze(lasso_pl.named_steps['regressor'].coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['carat', 'carat^2', 'carat^3', 'carat^4']\npredictors.extend(X_train.columns[1:].tolist())\n\nplt.figure(figsize=(6, 8))\nplt.barh(y=range(len(lasso_coef)), width=lasso_coef)\nplt.yticks(range(len(lasso_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"Lasso Coefficient Estimates, alpha = %.4f\" % 0.0001, fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have observed, the GridSearchCV actually dound the best value for alpha to be very low. In practice, this means regularization is almost absent. AS a result, the coefficients are not shrinked and feature selection is not performed.\n\nFrom the plot, in fact, the coefficients resemble those of Ridge regression."},{"metadata":{},"cell_type":"markdown","source":"**5. Predict and print metrics on test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get prediction and actual values as arrays\ny_pred = lasso_pl.predict(X_test).reshape(-1, 1)\ny_true = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R^2: %.4f' % (r2_score(y_test, y_pred)))\nprint('Exp. Var.: %.4f' % (explained_variance_score(y_test, y_pred)))\nprint('RMSE: %.4f' % (mean_squared_error(y_test, y_pred) ** .5))\nprint('MAE: %.4f' % (mean_absolute_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Inspect residuals**"},{"metadata":{"trusted":true},"cell_type":"code","source":"resid = (y_true - y_pred)\nsns.jointplot(x=y_pred, y=resid, kind='reg', \n              joint_kws=dict(fit_reg=False))\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Residuals plot resembles that of Ridge Regression as well."},{"metadata":{},"cell_type":"markdown","source":"### 7.2.4 Elastic Net: Ridge + Lasso "},{"metadata":{},"cell_type":"markdown","source":"Combines the concepts of L1 and L2 regularizations by letting one chose the \"weight\" of each through the `l1_ratio` parameter. By default, uses `alpha=1` and `l1_ratio=0.5`.\n\nMinimizes:\n\n$1 / (2 * n_{samples}) * ||y - Xw||^2_2 + alpha * l1_{ratio} * ||w||_1 + 0.5 * alpha * (1 - l1_{ratio}) * ||w||^2_2$"},{"metadata":{"trusted":true},"cell_type":"code","source":"en_pl = Pipeline([\n    ('union', FeatureUnion([\n        ('polynomial', Pipeline([\n            ('selector', get_polyfeatures),\n            ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n        ])),\n        ('linear', Pipeline([\n            ('selector', get_linfeatures)\n        ]))\n    ])),\n    ('regressor', ElasticNet())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_pl.fit(X_train, y_train)\nen_pl.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create alphas space for search\nalphas = np.logspace(-4, 4, 9)\nl1_ratios = np.linspace(0, 1, 6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since two parameters are going to be tested, nw we use `RandomizedSearchCV` to reduce the workload (instead of testing 6 x 9 = 54 combinations, it sample a number of them and return the best results)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare GridSearch arguments\nCV_FOLDS = 5\nPARAM_GRID = {'regressor__alpha': alphas, 'regressor__l1_ratio': l1_ratios}\nSCORE = 'neg_mean_squared_error'\n\ngs = RandomizedSearchCV(en_pl, cv=CV_FOLDS, param_distributions=PARAM_GRID, scoring=SCORE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", gs.best_params_)\nprint(\"Best RMSE:           \", (-gs.best_score_) ** 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Fit w/ best parameter(s)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"en_pl.set_params(regressor__alpha=0.0001, regressor__l1_ratio=0.8)\n\nen_pl.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Inspect coefficients visually**"},{"metadata":{"trusted":true},"cell_type":"code","source":"en_coef = np.squeeze(en_pl.named_steps['regressor'].coef_)\nen_coef","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['carat', 'carat^2', 'carat^3', 'carat^4']\npredictors.extend(X_train.columns[1:].tolist())\n\nplt.figure(figsize=(6, 8))\nplt.barh(y=range(len(en_coef)), width=en_coef)\nplt.yticks(range(len(en_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"ElasticNet Coefficient Estimates\\nalpha = %.4f, l1_ratio = %.1f\" % (0.0001,0.8), \n          fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As observed before, the L1 penalty term in this particular dataset makes the alpha go very low, almost \"turning off\" the regularization."},{"metadata":{},"cell_type":"markdown","source":"**5. Predict and print metrics on test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get prediction and actual values as arrays\ny_pred = en_pl.predict(X_test).reshape(-1, 1)\ny_true = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R^2: %.4f' % (r2_score(y_test, y_pred)))\nprint('Exp. Var.: %.4f' % (explained_variance_score(y_test, y_pred)))\nprint('RMSE: %.4f' % (mean_squared_error(y_test, y_pred) ** .5))\nprint('MAE: %.4f' % (mean_absolute_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Inspect residuals**"},{"metadata":{"trusted":true},"cell_type":"code","source":"resid = (y_true - y_pred)\nsns.jointplot(x=y_pred, y=resid, kind='reg', \n              joint_kws=dict(fit_reg=False))\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Residuals plot resembles that of Ridge Regression as well."},{"metadata":{},"cell_type":"markdown","source":"####  Coefficient comparison: Ridge and Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.barh(y=range(len(ridge_coef)), width=ridge_coef, color='r')\nplt.barh(y=range(len(lasso_coef)), width=lasso_coef, color='b')\nplt.yticks(range(len(ridge_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"Comparison of Coefficient Estimates\\nRidge in red, Lasso in blue\", \n          fontdict=dict(fontsize=16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that even with a very low alpha, Lasso shrinks the coefficients even more."},{"metadata":{},"cell_type":"markdown","source":"### Scikit-Learn: Conclusions"},{"metadata":{},"cell_type":"markdown","source":"After using Statsmodels to investigate linear models, non-linear transformation and analyse some easily available statistics, we've used Scikit-Learn's API to:\n- build Pipelines to create polynomial terms \"on th fly\" and \"grid search\" for the best parameters\n- perform regularized regressions with Ridge and Lasso\n- investigate results on test sets\n\nBoth API's are great tools to fit and analyse models, with some differences on outputs and capabilities. This is as far as my expertise of both APIs go for now, so I stop here. Hope you've enjoyed and that you feel like contributing, commenting, and upvoting!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}