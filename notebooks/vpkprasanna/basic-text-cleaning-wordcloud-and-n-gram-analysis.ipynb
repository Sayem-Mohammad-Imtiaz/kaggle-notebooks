{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# pd.set_option('display.max_colwidth', -1)\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport re\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport csv\nfrom matplotlib import rcParams\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.util import ngrams\nstop = stopwords.words('english')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report,plot_confusion_matrix\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the CSV Files","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"true = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nfalse = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\ntrue.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"false.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true.subject.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 15,10\ntrue.subject.value_counts().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The above Viz shows that target column is equally distributed in true category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 15,10\nfalse.subject.value_counts().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The above Viz shows that target column is not  equally distributed in False category and News label is more than other labels","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sepearting the dataset into the different dataframe based on the label column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"politics = true[true['subject']==\"politicsNews\"]\nworldnews = true[true['subject']==\"worldnews\"]\nprint(politics.shape)\nprint(worldnews.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"politics_text_len = politics['text'].str.len()\nworldnews_text_len = worldnews['text'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The maximum lenght of string in Politcs news is {} words\".format(max(politics_text_len)))\nprint(\"The maximum lenght of string in World news is {} words\".format(max(worldnews_text_len)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### since i cannot able to plot this i have just printed the maximum lenght of strin[](http://)g value in the each labels","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Simple Pre-Processing on politics and world news dataset - True Tweets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">Tokenization \n>Stop words removal","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Toekenization\n>In Python tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language.Here we are performing the word tokenization from NLTK Library","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Stopwords\n>stop words are words which are filtered out before or after processing of natural language data (text).[1] Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizeandstopwords(text):\n    tokens = nltk.word_tokenize(text)\n    # taken only words (not punctuation)\n    token_words = [w for w in tokens if w.isalpha()]\n    meaningful_words = [w for w in token_words if not w in stop]\n    joined_words = ( \" \".join(meaningful_words))\n    return joined_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"politics['text'] = politics['text'].apply(tokenizeandstopwords)\nworldnews['text'] = worldnews['text'].apply(tokenizeandstopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCloud\n> A tag cloud (word cloud or wordle or weighted list in visual design) is a novelty visual representation of text data, typically used to depict keyword metadata (tags) on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color.[2] This format is useful for quickly perceiving the most prominent terms to determine its relative prominence. When used as website navigation aids, the terms are hyperlinked to items associated with the tag","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Defining the word Cloud function to generate the word cloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_word_cloud(text):\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'black').generate(str(text))\n    fig = plt.figure(\n        figsize = (40, 30),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# World Cloud form true News Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud for Politics Label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"politics_text = politics.text.values\ngenerate_word_cloud(politics_text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud for Worldnews Label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"worldnews_text = worldnews.text.values\ngenerate_word_cloud(worldnews_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# False News Dataset Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"false.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(false.subject)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Seperating the dataset into the different dataframe based on the labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Government_News = false[false['subject']==\"Government News\"]\nMiddle_east = false[false['subject']==\"Middle-east\"]\nNews = false[false['subject']==\"News\"]\nUS_News = false[false['subject']==\"US_News\"]\npolitics = false[false['subject']==\"politics\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Government_News['text'] = Government_News['text'].apply(tokenizeandstopwords)\nMiddle_east['text'] = Middle_east['text'].apply(tokenizeandstopwords)\nNews['text'] = News['text'].apply(tokenizeandstopwords)\nUS_News['text'] = US_News['text'].apply(tokenizeandstopwords)\npolitics['text'] = politics['text'].apply(tokenizeandstopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Cloud for Fake news","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud for Goverment news Label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"govertment_news_text = Government_News['text'].values\ngenerate_word_cloud(govertment_news_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud for Middle east news Label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"middleast_news_text = Middle_east['text'].values\ngenerate_word_cloud(middleast_news_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud for General News Label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"news_text = News['text'].values\ngenerate_word_cloud(news_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud for Us News Label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"usnews_text = US_News['text'].values\ngenerate_word_cloud(usnews_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud for Politics Label in Fake dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"politicsFake_text = politics['text'].values\ngenerate_word_cloud(politicsFake_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merging true and fake news dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"false['target'] = 'fake'\ntrue['target'] = 'true'\nnews = pd.concat([false, true]).reset_index(drop = True)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['text'] = news['text'].apply((lambda y:re.sub(\"http://\\S+\",\" \", y)))\nnews['text'] = news['text'].apply((lambda x:re.sub(\"\\@\", \" \",x.lower())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_word = basic_clean(''.join(str(true['text'].tolist())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# N-Gram","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# N-gram Analysis - Bigram and Trigram ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# N-gram for true news","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"true_bigrams_series = (pd.Series(nltk.ngrams(true_word, 2)).value_counts())[:20]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## True News - Bigram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"true_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## True News - Trigram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"true_trigrams_series = (pd.Series(nltk.ngrams(true_word, 3)).value_counts())[:20]\ntrue_trigrams_series.sort_values().plot.barh(color='red', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# N-Gram -False word Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"false_word = basic_clean(''.join(str(false['text'].tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flase_bigrams_series = (pd.Series(nltk.ngrams(false_word, 2)).value_counts())[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## False News - Bigram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"flase_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## False News - Trigram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"false_trigrams_series = (pd.Series(nltk.ngrams(false_word, 3)).value_counts())[:20]\nfalse_trigrams_series.sort_values().plot.barh(color='red', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Full Dataset Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"words = basic_clean(''.join(str(news['text'].tolist())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Full Data - Bigram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Full Data - Trigram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams_series.sort_values().plot.barh(color='red', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a Basic Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train,x_test,y_train,y_test = train_test_split(news['text'], news.target, test_size=0.2, random_state=2020)\n\npipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LogisticRegression())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(model,x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}