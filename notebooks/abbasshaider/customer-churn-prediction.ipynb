{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/churn-modelling/Churn_Modelling.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Removing unimport columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.iloc[:,3:]\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Heat map to check the null values in dataset\nThis heat map shows that there is no null value. The dataframe is already clean "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull(),cmap=\"viridis\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now i'm using histogram to check the flow of data and also check the standard deviation of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , ax =plt.subplots(2,2)\nax = ax.flatten()\nfig.set_size_inches(14, 10)\nsns.distplot(df.EstimatedSalary,color='#2D008E',ax=ax[0])\nsns.distplot(df.CreditScore,color='#F9971A',ax=ax[1])\nsns.distplot(df.Balance,color='#242852',ax=ax[2])\nsns.distplot(df.Age,color='#242852',ax=ax[3])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove outlier by using standard deviation\n \n ****upper limit and Lower limit of standard deviation in CreditScore \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_limit_CreditScore= df[\"CreditScore\"].mean() + 1.5*df[\"CreditScore\"].std()\nlower_limit_CreditScore = df[\"CreditScore\"].mean() - 1.5*df[\"CreditScore\"].std()\nupper_limit_CreditScore , lower_limit_CreditScore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Upper limit and Lower limit standard deviation of EstimatedSalary****"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_limit_EstimatedSalary= df[\"EstimatedSalary\"].mean() + 1*df[\"EstimatedSalary\"].std()\nlower_limit_EstimatedSalary= df[\"EstimatedSalary\"].mean() - 1.5*df[\"EstimatedSalary\"].std()\nupper_limit_EstimatedSalary , lower_limit_EstimatedSalary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now usin upper and lower limits of stds. i am removing outliers and create new data frame.****"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df[(df[\"CreditScore\"]<upper_limit_CreditScore)&(df[\"CreditScore\"]>lower_limit_CreditScore)]\ndf2=df1[(df1[\"EstimatedSalary\"]<upper_limit_EstimatedSalary)&(df1[\"EstimatedSalary\"]>lower_limit_EstimatedSalary)]\ndf1.shape,df2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now again using histogram\nto check the flow of new data frame df2"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_1 , ax_1 =plt.subplots(1,2)\nax_1 = ax_1.flatten()\nfig_1.set_size_inches(14, 6)\nsns.distplot(df2.EstimatedSalary,color='#2D008E',ax=ax_1[0])\nsns.distplot(df2.CreditScore,color='#F9971A',ax=ax_1[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_3 , ax_3 =plt.subplots(2,2)\nax_3 = ax_3.flatten()\nfig_3.set_size_inches(16, 8)\nsns.violinplot(y=df2.CreditScore,x=df2.Tenure,palette=\"mako\",ax=ax_3[0])\nsns.barplot(y=\"CreditScore\",x=\"Tenure\",data=df2,palette=\"mako\",ci=\"sd\",ax=ax_3[1])\nsns.violinplot(y=df2.CreditScore,x=df2.NumOfProducts,palette=\"rocket\",ax=ax_3[2]) \nsns.barplot(y=\"CreditScore\",x=\"NumOfProducts\",data=df2,palette=\"rocket\",ci=\"sd\",ax=ax_3[3])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"seperate churn 0 values and 1 to know the ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_churn_1=df2[df2[\"Exited\"]==1]\ndf2_churn_0=df2[df2[\"Exited\"]==0]\ndf2_churn_1.shape,df2_churn_0.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Labeling the gender column by 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df2.convert_dtypes()\ndf3[\"Gender\"] = np.where(df3[\"Gender\"]=='Female',0,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# creating a x dataset and y labels to train a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df3.drop([\"Exited\",\"Geography\"],axis=1)\ny=df3[\"Exited\"]\ny=y.astype('int')\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\nx_train.shape,x_test.shape,y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standarized the train and test data to increase the accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train=sc.fit_transform(x_train)\nx_test=sc.fit_transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply Logistic Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import  LogisticRegression\nlr = LogisticRegression()\nmodel_logic = lr.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel_rfc = rfc.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion='entropy',max_depth=300)\nmodel_dtc = dtc.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply support vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nmodel_svc = svc.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check the accuracy of all models by using Cross Validation Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores_logic = cross_val_score(model_logic, x, y, cv=4)\nscores_rfc = cross_val_score(model_rfc, x, y, cv=4)\nscores_dtc = cross_val_score(model_dtc, x, y, cv=4)\nscores_svc = cross_val_score(model_svc, x, y, cv=4)\ndata={'model':['logistic','random_forest','decision_tree','SVM'],\n     'accuracy':[scores_logic,scores_rfc,scores_dtc,scores_svc]}\naccuracy_df = pd.DataFrame(data, columns = ['model', 'accuracy'])\naccuracy_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_df.accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #the accuracy results of show that random forest gain highest percentage which is 85%. "},{"metadata":{},"cell_type":"markdown","source":"# Using Confusion matrics to check the performance of random forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model_rfc.predict(x_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_predict)\ncm\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using heat map to plot confusion matrix. so that we can better understand the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.heatmap(cm,annot=True)\nplt.xlabel('Predict')\nplt.ylabel('Truth')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#This heat map shows that the model predict  1159 times 0 and the actual value is also 0******"},{"metadata":{},"cell_type":"markdown","source":"# Now i'm chect manually the result of my model \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_score = 608\ngender = 1 # which is male\nage = 55\ntenure = 6\nbalance = 83000\nnum_of_buy_product=12\nhas_a_creditcard = 1\nis_active_member=0\nsalary=42000\n\n\narr=np.array([credit_score,gender,age,tenure,balance,num_of_buy_product,has_a_creditcard,is_active_member,salary])\narr_res=arr.reshape(1,-1)\na=model_rfc.predict(arr_res)\na","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}