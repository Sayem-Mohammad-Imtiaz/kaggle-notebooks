{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"21aba885-ee9c-12c6-ff27-0c071e10fc15"},"source":"# Is an SMS Spam? Using Word Count, Character Count and XGBoost\n\n**Problem**: Given a text message, we would like to predict whether it is spam or whether it is not spam (ham).\n\nIn this notebook we will look at the character count, word count and word frequency of messages in the dataset and use them as features to predict whether a message is spam or not.\n\n**Using only word count and character count, we achieve a test accuracy of 91%.**\n\n**Using the 25 most common words in spam and ham messages, we achieve a test accuracy of 100% (likely rounded).**\n\n* This is surprising so I am looking into potential problems with implementation. The current version counts only the most frequent words in the training set, but this still produces a test accuracy of 100%.\n\nIf you found this helpful, do upvote or leave a comment! Likewise do post suggestions.\n\n*[Data by UCI Machine Learning, posted on Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset)*\n\n*Credits to [anokas's Quora questions kernel](https://www.kaggle.com/anokas/quora-question-pairs/data-analysis-xgboost-starter-0-35460-lb) for inspiring many of these methods.*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fbf92634-9a4a-d8f1-6e84-e20bd762432e"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\nprint(\"Data files:\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6dc3f19b-f5cf-d71f-b496-a47875538ca7"},"source":"## Read in and preview data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00687c46-fad0-63a2-1d56-399013887b14"},"outputs":[],"source":"# Read in data\ndf = pd.read_csv('../input/spam.csv', encoding='latin-1')\n\n# Preview data\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e997922-8f2b-c278-17b8-a35515be1db5"},"outputs":[],"source":"# Drop redundant columns and rename columns so the titles are meaningful\ndf = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n# Note: if you rename 'v2' as something like 'text', be sure not to\n# overwrite the column when you create words as features (and so) \n# have columns named 'text' later on!\ndf = df.rename(columns={\"v1\":\"label\", \"v2\":\"sms_text\"})\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22ec3258-3c8c-b0f0-b23e-cd227840b570"},"outputs":[],"source":"# How many spam messages are there?\ndf.label.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03ace290-5793-395d-7229-7a54b3097565"},"outputs":[],"source":"print(round(747 / (747 + 4825) * 100, 2), \"% of messages in our dataset are spam.\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"e64c0f93-1c11-185a-bf51-dad96c004d5d"},"source":"## Character Count\nLet's plot the character count for all messages, and then for ham vs spam messages."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40e24e91-cb66-2b30-3a63-8b50220d8ced"},"outputs":[],"source":"# Character count\n\n# Each line of code creates a list of messages, saving each message as a string.\n\n# We want three lists: one with all messages in them, one with only ham messages \n# and one with only spam messages.\nmessages = pd.Series(df['sms_text'].tolist()).astype(str)\nham_messages = pd.Series(df[df['label'] == 'ham']['sms_text'].tolist()).astype(str)\nspam_messages = pd.Series(df[df['label'] == 'spam']['sms_text'].tolist()).astype(str)\n\n# Create the corresponding distribution of character counts for each list.\n# We count the number of characters in each message using the `len` function.\ndist_all = messages.apply(len)\ndist_ham = ham_messages.apply(len)\ndist_spam = spam_messages.apply(len)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df29c888-59b9-04a2-9b05-48541a7656a4"},"outputs":[],"source":"# Plot distribution of character count of all messages\n\nplt.figure(figsize=(12, 8))\nplt.hist(dist_all, bins=100, range=[0,400], color=pal[3], normed=True, label='All')\nplt.title('Normalised histogram of character count in all messages', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('# Summary statistics for character count of all messages')\nprint('mean-all {:.2f} \\nstd-all {:.2f} \\nmin-all {:.2f} \\nmax-all {:.2f}'.format(dist_all.mean(), \n                          dist_all.std(), dist_all.min(), dist_all.max()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"a71b6361-97d8-682c-c5f6-e2c9d9140382"},"source":"The distribution of characters seems bimodal (two-peaked), with most messages having between 0 and 200 characters. There is a sharp decline after around 160 characters, which may be related to people being charged per SMS of 80 characters.\n\nNow for the fun part: let's plot the character counts for spam vs ham messages separately."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55bdad12-ae5b-a6c9-b804-29775f780411"},"outputs":[],"source":"# Plot distributions of character counts for spam vs ham messages\n\nplt.figure(figsize=(12,8))\nplt.hist(dist_ham, bins=100, range=[0,250], color=pal[1], normed=True, label='ham')\nplt.hist(dist_spam, bins=100, range=[0, 250], color=pal[2], normed=True, alpha=0.5, label='spam')\nplt.title('Normalised histogram of character count in messages', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('# Summary statistics for character count of ham vs spam messages')\nprint('mean-ham  {:.2f}   mean-spam {:.2f} \\nstd-ham   {:.2f}   std-spam   {:.2f} \\nmin-ham    {:.2f}   min-ham    {:.2f} \\nmax-ham  {:.2f}   max-spam  {:.2f}'.format(dist_ham.mean(), \n                         dist_spam.mean(), dist_ham.std(), dist_spam.std(), dist_ham.min(), dist_spam.min(), dist_ham.max(), dist_spam.max()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"06d3d39c-221c-0a42-9f3e-f8fac2fd5eea"},"source":"Beautiful! The **character count seems to be a good predictor of  whether a message is spam or not**. Spam messages seem to have more characters on average than ham messages. In particular, there is a huge spike in spam messages between 130 and 162.5 characters. It's seems a bit too easy.\n\nWith ham messages between 25 and 90 characters, alternating bins also have significantly higher probabilities. I wonder why.\n\n## Word Count"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"644ef349-5218-191c-e19e-f02fb0c2c58b"},"outputs":[],"source":"# Word Count\n\n# We split each message into words using `.split(' ')`\n# and count the number of words in each message using `len`.\ndist_all = messages.apply(lambda x: len(x.split(' ')))\ndist_ham = ham_messages.apply(lambda x: len(x.split(' ')))\ndist_spam = spam_messages.apply(lambda x: len(x.split(' ')))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8157bdf3-1beb-6315-2b92-95e54901e437"},"outputs":[],"source":"# Plot distribution of word count of all messages\n\nplt.figure(figsize=(12, 8))\nplt.hist(dist_all, bins=100, color=pal[3], normed=True, label='All')\nplt.title('Normalised histogram of word count in all messages', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('# Summary statistics for word count of all messages')\nprint('mean-all {:.2f} \\nstd-all {:.2f} \\nmin-all {:.2f} \\nmax-all {:.2f}'.format(dist_all.mean(), \n                          dist_all.std(), dist_all.min(), dist_all.max()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d9283e0b-0602-2b40-00c4-81eb228108a7"},"source":"The distribution of words seems unimodal and positively skewed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8cd3945a-a347-b714-1634-7fe074b8166f"},"outputs":[],"source":"# Plot distributions of word counts for spam vs ham messages\n\nplt.figure(figsize=(12,8))\nplt.hist(dist_ham, bins=65, range=[0,75], color=pal[1], normed=True, label='ham')\nplt.hist(dist_spam, bins=65, range=[0, 75], color=pal[2], normed=True, alpha=0.5, label='spam')\nplt.title('Normalised histogram of word count in messages', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('# Summary statistics for word count of ham vs spam messages')\nprint('mean-ham  {:.2f}   mean-spam {:.2f} \\nstd-ham   {:.2f}   std-spam   {:.2f} \\nmin-ham    {:.2f}   min-ham    {:.2f} \\nmax-ham  {:.2f}   max-spam  {:.2f}'.format(dist_ham.mean(), \n                         dist_spam.mean(), dist_ham.std(), dist_spam.std(), dist_ham.min(), dist_spam.min(), dist_ham.max(), dist_spam.max()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"875825db-8901-3fbd-b987-a96ab9b9a4b4"},"source":"As expected, spam messages tend to have more words than ham messages (which corresponds to our results about character count). Eyeballing the plots, it seems that character count might be a better predictor.\n\nNote also that the **maximum number of words for spam messages is only 35, whereas for ham messages it's 171. We can see from the plot that quite a few messages have over 36 words**, so that'd likely be a feature our classifier can pick up later on."},{"cell_type":"markdown","metadata":{"_cell_guid":"78d977c9-30f3-93b2-2b36-28f268190ad9"},"source":"## Simple Classifier: XGBoost\n\nBefore we move on to semantic analysis, let's try using the features we have so far - character count and word count - to predict whether a message is spam.\n\nSo far I've been exploring the whole dataset (which some might consider cheating :P), but now that we want to test the accuracy of classifiers we'll split the data into training and test datasets.\n\n### Add features to our dataframe"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73725459-3de1-9e8e-258e-4c6f1b4f8ed4"},"outputs":[],"source":"# Add our features to our dataframe\ndf['word_count'] = pd.Series(df['sms_text'].tolist()).astype(str).apply(lambda x: len(x.split(' ')))\ndf['char_count'] = pd.Series(df['sms_text'].tolist()).astype(str).apply(len)\n\n# For some models the target label has to be int, float or bool\ndf['is_spam'] = (df['label'] == 'spam')\n\n# Check things worked as expected\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"590c4740-9175-d822-16d4-3e3776276114"},"source":"### Split data into training, validation and test datasets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4929e48d-b81e-d77c-1dfb-965db078e209"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX = df[['word_count', 'char_count']]\ny = df[['is_spam']]\n\n# Split data into training and test sets\n# TODO: Might want to split such that train and test sets have equal proportion of spam messages\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Split some training data for validation\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0aa6b86-7a38-9560-bc72-238c8fa9ef29"},"source":"### Train XGBoost classifier "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfb5dcad-c192-803d-a00b-e813db607598"},"outputs":[],"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'error'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_valid = xgb.DMatrix(X_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bb3da2fb-5fed-b42f-d4ab-778f22940c14"},"source":"We have a **pretty good validation error of around 7% based only on word and character count**."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"266236da-9c11-da56-3cc7-9f33d5ee4d97"},"outputs":[],"source":"from sklearn.metrics import accuracy_score\n\n# Predict values for test set\nd_test = xgb.DMatrix(X_test)\np_test = bst.predict(d_test)\n\n# Apply function round() to each element in np array\n# so predictions are all either 0 or 1.\nnpround = np.vectorize(round)\np_test_ints = npround(p_test)\n\n# Error rate for test set\naccuracy = accuracy_score(y_test, p_test_ints)\nprint(\"Test Accuracy: \", accuracy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c59c751a-e382-30c8-7fc0-adb1e93f6cb6"},"source":"## Word Cloud\nWhich are the most common words in spam messages and ham messages? We can then use **frequencies of certain words as features**."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ece9bd3d-c180-d236-8aee-ed8ff1b5f53e"},"outputs":[],"source":"# To avoid cheating, we will first split the data into train and test sets and then only\n# count top words for our training data.\n\nX = df\ny = df[['is_spam']]\n\n# Split data into training and test sets\n# TODO: Might want to split such that train and test sets have equal proportion of spam messages\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nham_messages_train = pd.Series(X_train[X_train['label'] == 'ham']['sms_text'].tolist()).astype(str)\nspam_messages_train = pd.Series(X_train[X_train['label'] == 'spam']['sms_text'].tolist()).astype(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0eb09c03-0ebd-9d9c-d2ef-7deb48a331ad"},"outputs":[],"source":"from wordcloud import WordCloud\n# WordCloud automatically excludes stop words\n\n# Draw word cloud for spam messages\nspam_messages_one_string = \" \".join(spam_messages_train.astype(str))\nspam_cloud = WordCloud().generate(spam_messages_one_string)\nplt.figure(figsize=(12,8))\nplt.imshow(spam_cloud)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9db957e-1d04-9bbd-d690-cfa4e846dcef"},"outputs":[],"source":"# Draw word cloud for ham messages\nham_messages_one_string = \" \".join(ham_messages_train.astype(str))\nham_cloud = WordCloud().generate(ham_messages_one_string)\nplt.figure(figsize=(12,8))\nplt.imshow(ham_cloud)"},{"cell_type":"markdown","metadata":{"_cell_guid":"15389b93-f9f5-282f-16fd-508dccc40fce"},"source":"As you might expect, spam messages frequently include words such as *FREE, now, mobile, call, text*, and *txt*. The words used frequently in ham messages seem more general, e.g. *Ok, ur*. We can see that messages like 'got to go' and 'will ...' also seem common, which is intuitive.\n\nIt's odd why the words lt and gt are so common. I'm not aware that people often abbreviate e.g. later as *lt* and get as *gt*. To me they seem like abbreviations for *less than* and *greater than*.\n\n*Now* is common in both ham and spam messages, so checking whether that word is in a message probably won't be too effective in distinguishing between the two.\n\nThe next step is to get a **more precise count of which words are the most frequent**."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"780f21e0-5dd8-93e5-bdf4-18855f0f00b0"},"outputs":[],"source":"from collections import Counter\nham_words_list = ham_messages_one_string.split()\ntotal_ham_words = len(ham_words_list)\nprint(\"Total number of words in ham messages: \", total_ham_words)\nham_words_dict = Counter(ham_words_list).most_common()\nham_words_dict[:25]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67d5a810-f5fb-f1fd-2481-dfad072f1123"},"outputs":[],"source":"spam_words_list = spam_messages_one_string.split()\ntotal_spam_words = len(spam_words_list)\nprint(\"Total number of words in spam messages: \", total_spam_words)\nspam_words_dict = Counter(spam_words_list).most_common()\nspam_words_dict[:25]"},{"cell_type":"markdown","metadata":{"_cell_guid":"b2366390-5547-3b5b-fcbe-bf5850ba134f"},"source":"We can see that many of these words listed are **stop words** and so may not be as useful in classifying whether messages are spam or not.\n\nStop words are words like *the, you, and, is* which do not contain important significance to be used in search queries. For example, the most used word in both ham and spam messages was *to*.\n\nTo create a dictionary of the most frequently used non-stopwords, we will manually add non-stopwords to a dictionary instead of using `Counter()`."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f24b32e-9903-b9ef-d900-b4e0d6dc0e01"},"outputs":[],"source":"from nltk.corpus import stopwords\nfrom collections import defaultdict\nimport operator\n\nstopwords = set(stopwords.words(\"english\"))\n\nham_words_lowercase = ham_messages_one_string.lower().split()\n\nham_words_nostop = []\nfor word in ham_words_lowercase:\n    if word not in stopwords:\n        ham_words_nostop.append(word)\n\nham_words_freq = Counter(ham_words_nostop).most_common()\nham_words_freq[:25]"},{"cell_type":"markdown","metadata":{"_cell_guid":"59516719-94ce-525e-91fe-3f99c8fbcd6f"},"source":"Now we know why *lt, gt* appeared as frequently used words in ham messages - they were the emoticon >< !"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a643469-ba6d-709d-0557-4c21716572db"},"outputs":[],"source":"spam_words_lowercase = spam_messages_one_string.lower().split()\n\nspam_words_nostop = []\nfor word in spam_words_lowercase:\n    if word not in stopwords:\n        spam_words_nostop.append(word)\n\nspam_words_freq = Counter(spam_words_nostop).most_common()\nspam_words_top25 = spam_words_freq[:25]\nspam_words_top25"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7066dfd4-34ab-ba0c-f8be-b37c82871595"},"outputs":[],"source":"spam_words_top25_list = [tuple[0] for tuple in spam_words_top25]\nham_words_top25 = [tuple[0] for tuple in ham_words_freq[:25]]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fe8828b-cc8a-4402-63c2-e986cc7ea044"},"outputs":[],"source":"spam_words_top25_pruned = []\nfor word in spam_words_top25_list:\n    if word not in ham_words_top25:\n        spam_words_top25_pruned.append(word)\n        \nham_words_top25_pruned = []\nfor word in ham_words_top25:\n    if word not in spam_words_top25_list:\n        ham_words_top25_pruned.append(word)\n\nprint(\"Number of non-duplicates in each list: \", len(spam_words_top25_pruned))"},{"cell_type":"markdown","metadata":{"_cell_guid":"dedac439-6b3f-da45-63c6-e98fc70bf752"},"source":"Now we will naively include whether-or-not-a-message-includes-these-words as features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fc0ed0f-c926-dc43-cacb-1823a9e89890"},"outputs":[],"source":"for word in (spam_words_top25_pruned + ham_words_top25_pruned):\n    df[word] = (word in df['sms_text'])\n    X_train[word] = (word in X_train['sms_text'])\n    X_test[word] = (word in X_test['sms_text'])\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fe3dcfa-8856-9037-6779-be725d62e5bc"},"outputs":[],"source":"del X_train['sms_text']\ndel X_train['label']\ndel X_test['sms_text']\ndel X_test['label']\n\n# Split some training data for validation\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ef5b042-b534-8626-e13e-1c4e0d918e42"},"outputs":[],"source":"X_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b3ef04f-fb1a-0cfc-2939-5c8b2f8ae27e"},"outputs":[],"source":"d_train = xgb.DMatrix(X_train, label=y_train)\nd_valid = xgb.DMatrix(X_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d5451b2-9f8f-0814-b7d6-d7e5a3510b41"},"outputs":[],"source":"# Predict values for test set\nd_test = xgb.DMatrix(X_test)\np_test = bst.predict(d_test)\n\n# Apply function round() to each element in np array\n# so predictions are all either 0 or 1.\nnpround = np.vectorize(round)\np_test_ints = npround(p_test)\n\n# Error rate for test set\naccuracy = accuracy_score(y_test, p_test_ints)\nprint(\"Test Accuracy: \", accuracy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0439527d-3643-5145-2150-6c6d4eecec8c"},"source":"Oh, the **validation and test accuracies are 1.0**. Nice!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}