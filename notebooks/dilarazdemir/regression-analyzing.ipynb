{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n* Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'features').  \n* The most common form of regression analysis is linear regression.\n* Linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables.\n\n    * [Loading Data and Explanation of Features](#1)\n    * [Simple Linear Regression](#2)\n    * [Multiple Linear Regression](#3)\n    * [Polinomial Linear Regression](#4)\n    * [Decision Tree Regression](#5)\n    * [Random Forest Regression](#6)\n    * [R Square with Random Forest Regression](#7)\n    * [R Square with Linear Regression](#8)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## Loading Data and Explanation of Features\n* There are no missing values.\n<br>\n<font color=\"green\">\n* 'column_3C_weka.csv'\n    * has 7 features and 310 samples.\n    * 6 of them are in float data type and the other is object.,\n<br>\n<font color=\"blue\">\n* 'column_2C_weka.csv'\n    * has 7 features and 310 samples.\n    * 6 of them are in float data type and the other is object.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_3C_weka.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Between Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df1.corr(), annot = True, fmt = \".2f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## Simple Linear Regression\n\n* residual = y - y_head (In MSE, we take square of residual. Residual values can be negative and positive, in this situation their summation can be zero and we can lost error values. Reason of that, we take square of residual)\n* y = real values of y\n* y_head = predicted values of y\n* b0 = constant - bias (The point where it intersects the y axis)\n* b1 = coefficient - slope\n* y = b0 + b1*x\n\n### Mean Squared Error\n* n = sample size\n\n* MSE = 1/n(sum(residual^2)/n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pelvic_incidence = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\nsacral_slope = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\nplt.figure(figsize=(10,10))\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n\n# Predict space\npredict_space = np.linspace(min(pelvic_incidence), max(pelvic_incidence)).reshape(-1,1)\n# Fit\nreg.fit(pelvic_incidence,sacral_slope)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(pelvic_incidence, sacral_slope))\n# Plot regression line and scatter\nplt.figure(figsize=(10,10))\nplt.plot(predict_space, predicted, color='red', linewidth=2)\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## Multiple Linear Regression\n* Purpose : Minimum MSE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (df1.iloc[:,[0,2]]).values # [pelvic_incidence,lumbar_lordosis_angle]\ny = df1.sacral_slope.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\n\nprint(\"b0: \",multiple_linear_regression.intercept_)\nprint(\"b1,b2:\",multiple_linear_regression.coef_)\n\nmultiple_linear_regression.predict(np.array([[63.0278175 , 39.60911701],[40.47523153, 39.60911701]]))\n# first values of [[pelvic_incidence,lumbar_lordosis_angle],[sacral_slope,lumbar_lordosis_angle]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## Polinomial Linear Regression\n* y = b0 + b1*x + b2*x^2 + ... + b2*x^n\n\n\n* n = degree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\nlr.fit(x,y)\n# predict\ny_head = lr.predict(x)\n\nplt.plot(x,y_head,color=\"red\",label = \"linear\")\n\n# polynomial regression = y = b0 + b1*x + b2*x^2 + b3*x^3 + ... + bn*x^n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_regression = PolynomialFeatures(degree = 3)\n\nx_polynomial = polynomial_regression.fit_transform(x)\n# fit\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y) \n\n# visualize\n\ny_head2 = linear_regression2.predict(x_polynomial)\n\n\nplt.plot(x,y_head2,color = \"green\", label = \"poly\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nx = (x - np.min(x))/(np.max(x)-np.min(x))\ny = (y - np.min(y))/(np.max(y)-np.min(y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## Decision Tree Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\n\nfrom sklearn.tree import DecisionTreeRegressor # random state = 0\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\n\ntree_reg.predict([[5.5]])\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n\ny_head = tree_reg.predict(x_)\n#%% visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color = \"red\")\nplt.plot(x_,y_head,color=\"green\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## Random Forest Regression\n* n_ estimator = number of tree, \n* random_state = Count random values according to randon_state.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf = RandomForestRegressor(n_estimators = 40,random_state = 42)\n\nrf.fit(x,y)\n\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)\n\n#visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x_,y_head,color = \"red\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n## R Square with Random Forest Regression\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf = RandomForestRegressor(n_estimators = 100,random_state = 42)\n\nrf.fit(x,y)\n\ny_head = rf.predict(x)\n\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_score\", r2_score(y,y_head))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n## R Square with Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\nplt.figure(figsize=(10,10))\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\n\nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nlinear_reg = LinearRegression()\n\n\nlinear_reg.fit(x,y)\n\ny_head = linear_reg.predict(x)\nplt.plot(x, y_head , color = \"red\")\n\n\n#%% \n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_square score: \", r2_score(y, y_head))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}