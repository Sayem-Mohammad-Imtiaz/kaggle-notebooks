{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, precision_score, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\nimport warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing "},{"metadata":{},"cell_type":"markdown","source":"### Importing Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/toy-dataset/toy_dataset.csv')\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count on basis of \"Yes\" and \"No\" Class in Target Variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Illness\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"Illness\"] == 'No'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"Illness\"] == 'No'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DataFrame information"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding Categorical Variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Gender']=='Male','Gender'] = 1\ndf.loc[df['Gender']=='Female','Gender'] = 0\nfrom sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\nlb_results = lb.fit_transform(df['City'])\nnew_df = pd.DataFrame(lb_results, columns=lb.classes_)\ndf = pd.concat([df, new_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In Below code block output we can check all categorical variable are encoded as numerical value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seperating Feature matrix and target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Illness','Number', 'City'], axis=1)\ny = df.Illness\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding Target Variable using LabelEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\ny = pd.Series(encoder.fit_transform(y), name='Illness')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting data into train_set and test_set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### concatenate training data together so that we can seperate the data on basis of classes in target column"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([X_train, y_train], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we are seperating the data on the basis of majority and minority classes to apply oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_ill = X[X.Illness== 0]\nill = X[X.Illness== 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We are trying to upsample minority class using scikit learn's resample class"},{"metadata":{"trusted":true},"cell_type":"code","source":"ill_upsampled = resample(ill,\n                         replace=True, # sample with replacement\n                         n_samples=len(not_ill), # match number in majority class\n                         random_state=100) # reproducible results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we will combining the majority class and the upsampled minority class "},{"metadata":{"trusted":true},"cell_type":"code","source":"upsampled = pd.concat([not_ill, ill_upsampled])\nupsampled.Illness.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below Function will check the training performance and test performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    '''\n    print the accuracy score, classification report and confusion matrix of classifier\n    '''\n    if train:\n        '''\n        training performance\n        '''\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        '''\n        test performance\n        '''\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test)))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Testing Model"},{"metadata":{},"cell_type":"markdown","source":"### Training Logisitic Regression on Oversampled Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = upsampled.Illness\nX_train = upsampled.drop('Illness', axis=1)\nclf = LogisticRegression(solver='liblinear').fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training performance"},{"metadata":{"trusted":true},"cell_type":"code","source":" print_score(clf, X_train, y_train, X_test, y_test, train=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":" print_score(clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As resampling doesn't made any big effect we are trying SMOTE oversampling which actually loops through the existing, real minority instance. At each loop iteration, one of the K closest minority class neighbours is chosen and a new minority instance is synthesised somewhere between the minority instance and that neighbour."},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting up testing and training sets\nX = df.drop(['Illness','Number', 'City'], axis=1)\ny = df.Illness\nencoder = LabelEncoder()\ny = pd.Series(encoder.fit_transform(y), name='Illness')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMOTE oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=42)\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing Balanced Training data after Smote Sampling "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(y_train)\nplt.title('Balanced training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Random Forest over Smote Oversampling data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Still we are not getting desired as for class 1 there is only 8% precision and recall is too less,so we are using GridSearch so that we can get best parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(random_state=42)\nparams_grid = {\"max_depth\": [3, None],\n               \"min_samples_split\": [2, 3, 5],\n               \"min_samples_leaf\": [1, 3, 5],\n               \"bootstrap\": [True, False],\n               \"criterion\": ['entropy']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search = GridSearchCV(rf_clf, params_grid,\n                           n_jobs=-1, cv=3,\n                           verbose=1, scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_score(grid_search, X_train, y_train, X_test, y_test, train=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_score(grid_search, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our Model is overfit as we can see it performing well on training set but on test set it is having issues with the class 1.To Overcome this we will try it with Extra-Trees Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xt_clf = ExtraTreesClassifier(random_state=42, min_samples_leaf=3, min_samples_split=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xt_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_score(xt_clf, X_train, y_train, X_test, y_test, train=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_score(xt_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### StratifiedKfold with Random Forest classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting up testing and training sets\nX = df.drop(['Illness','Number', 'City'], axis=1)\ny = df.Illness\nencoder = LabelEncoder()\ny = pd.Series(encoder.fit_transform(y), name='Illness')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\nkf = StratifiedKFold(n_splits=3, random_state=100)\ncross_val_f1_score_lst = []\ncross_val_accuracy_lst = []\ncross_val_recall_lst = []\ncross_val_precision_lst = []\n\nfor train_index_ls, validation_index_ls in kf.split(X_train, y_train):\n    # keeping validation set apart and oversampling in each iteration using smote \n    train, validation = X_train.iloc[train_index_ls], X_train.iloc[validation_index_ls]\n    target_train, target_val = y_train.iloc[train_index_ls], y_train.iloc[validation_index_ls]\n    sm = SMOTE(random_state=100)\n    X_train_res, y_train_res = sm.fit_sample(train, target_train)\n    print (X_train_res.shape, y_train_res.shape)\n    \n    # training the model on oversampled 4 folds of training set\n    rf = RandomForestClassifier(n_estimators=100, random_state=100)\n    rf.fit(X_train_res, y_train_res)\n    # testing on 1 fold of validation set\n    validation_preds = rf.predict(validation)\n    cross_val_recall_lst.append(recall_score(target_val, validation_preds))\n    cross_val_accuracy_lst.append(accuracy_score(target_val, validation_preds))\n    cross_val_precision_lst.append(precision_score(target_val, validation_preds))\n    cross_val_f1_score_lst.append(f1_score(target_val, validation_preds))\nprint ('Cross validated accuracy: {}'.format(np.mean(cross_val_accuracy_lst)))\nprint ('Cross validated recall score: {}'.format(np.mean(cross_val_recall_lst)))\nprint ('Cross validated precision score: {}'.format(np.mean(cross_val_precision_lst)))\nprint ('Cross validated f1_score: {}'.format(np.mean(cross_val_f1_score_lst)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We have tried to build the best model to predict Illness based on Income, Age, Gender and City but  as we have shown earlier in notebook that the data is skewed so we have tried random upsampling and Smote oversampling and we have trained our model on the sampled data but our model is overfit as we have tried different techniques but the result or evaluation metrics not upto the mark we will try doing undersampling the majority class because of the availability issue uploading it till this. Because training the models are taking too much time so it was not possible to use more_splits. "},{"metadata":{},"cell_type":"markdown","source":"# Visualization Variable Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\nxgbmodel = XGBClassifier()\nxgbmodel.fit(X_train, y_train)\nfeat_importances = pd.Series(xgbmodel.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(10).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#  To operationalize the model we have to take the below steps:\n## 1. First of all we have to save the best model in pickle format and put it in a database.\n## 2. We have to create a GUI using python Django or Flask framework and there it will ask a person about the details like city, Gender, Income and Age after that when they hit enter button there will be a script which will load the model from pickle format and do the prediction , that predicted output will be shown to user."},{"metadata":{},"cell_type":"markdown","source":"## There are other ways to operationalize the model using Google cloud ML Engine and other clouds have the options."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}