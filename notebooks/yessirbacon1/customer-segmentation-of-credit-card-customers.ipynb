{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n\n* [Getting started](#getting_started)\n* [Data preprocessing](#preprocessing)\n    * [Check for missing values](#imputation)\n    * [Check for duplicates](#duplicates)\n    * [Check for unique customer identifier](#unique)\n    * [Check for outliers](#outliers)\n    * [Robust scaler](#standardize)\n* [Exploratory data](#EDA)\n    * [Data types](#dtypes)\n    * [Summary statistics](#summary_stats)\n    * [Bootstrapping](#bootstrapping)\n    * [Distributions](#distributions)\n    * [Spearman correlation](#correlation)\n    * [Multiple linear regression](#multiple_linear_regression)\n* [Modeling](#modeling)\n    * [K-means clustering](#cluster)\n        * [Find optimal number of clusters](#elbow)\n        * [Cluster customers](#cluster_customers)\n    * [SMOTE](#SMOTE)\n    * [Model selection](#model_selection)\n        * [Decision tree classifier](#decision_tree)\n        * [Multinomial logistic regression](#logistic_regression)\n    * [Feature importance](#feature_importance)\n    * [Visualize decision tree](#visualize)\n* [Summarize insights from analysis](#summary)","metadata":{}},{"cell_type":"markdown","source":"# Getting started <a class=\"anchor\" id=\"getting_started\"></a>","metadata":{}},{"cell_type":"code","source":"# Link to dataset\n# https://www.kaggle.com/arjunbhasin2013/ccdata?select=CC+GENERAL.csv","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.448634Z","iopub.execute_input":"2021-09-20T03:14:48.449301Z","iopub.status.idle":"2021-09-20T03:14:48.452687Z","shell.execute_reply.started":"2021-09-20T03:14:48.449261Z","shell.execute_reply":"2021-09-20T03:14:48.452126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import r2_score, accuracy_score, classification_report\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.453883Z","iopub.execute_input":"2021-09-20T03:14:48.454168Z","iopub.status.idle":"2021-09-20T03:14:48.465954Z","shell.execute_reply.started":"2021-09-20T03:14:48.454145Z","shell.execute_reply":"2021-09-20T03:14:48.465149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Read and preview data\ntry:\n    df = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')\nexcept:\n    df = pd.read_csv('CC GENERAL.csv')\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.485682Z","iopub.execute_input":"2021-09-20T03:14:48.48596Z","iopub.status.idle":"2021-09-20T03:14:48.564627Z","shell.execute_reply.started":"2021-09-20T03:14:48.485934Z","shell.execute_reply":"2021-09-20T03:14:48.563772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Description of columns\n\n# CUST_ID : Identification of Credit Card holder (Categorical)\n# BALANCE : Balance amount left in their account to make purchases\n# BALANCE_FREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n# PURCHASES : Amount of purchases made from account\n# ONEOFF_PURCHASES : Maximum purchase amount done in one-go\n# INSTALLMENTS_PURCHASES : Amount of purchase done in installment\n# CASH_ADVANCE : Cash in advance given by the user\n# PURCHASES_FREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n# ONEOFF_PURCHASES_FREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n# PURCHASES_INSTALLMENTS_FREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n# CASH_ADVANCE_FREQUENCY : How frequently the cash in advance being paid\n# CASH_ADVANCE_TRX : Number of Transactions made with \"Cash in Advanced\"\n# PURCHASES_TRX : Number of purchase transactions made\n# CREDIT_LIMIT : Limit of Credit Card for user\n# PAYMENTS : Amount of Payment done by user\n# MINIMUM_PAYMENTS : Minimum amount of payments made by user\n# PRC_FULL_PAYMENT : Percent of full payment paid by user\n# TENURE : Tenure of credit card service for user","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.566528Z","iopub.execute_input":"2021-09-20T03:14:48.566861Z","iopub.status.idle":"2021-09-20T03:14:48.571831Z","shell.execute_reply.started":"2021-09-20T03:14:48.56682Z","shell.execute_reply":"2021-09-20T03:14:48.570941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of all the features\nfeatures = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.57294Z","iopub.execute_input":"2021-09-20T03:14:48.573166Z","iopub.status.idle":"2021-09-20T03:14:48.589824Z","shell.execute_reply.started":"2021-09-20T03:14:48.57314Z","shell.execute_reply":"2021-09-20T03:14:48.589006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Check for missing values <a class=\"anchor\" id=\"imputation\"></a>","metadata":{}},{"cell_type":"code","source":"# Check for columns with missing values and impute them with the median.\ndef impute_nan(df):\n    \"\"\"Check for columns with missing values and impute them with the median.\"\"\"\n    nan_cols = df.columns[df.isnull().any()].tolist()\n    nan_length = len(nan_cols)\n    if nan_length == 0:\n        return df\n    else:\n        print('Imputed features:', nan_cols)\n        for x in nan_cols:\n            df[x].fillna(df[x].median(), inplace=True)\n        return df\n\ndf = impute_nan(df)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.591615Z","iopub.execute_input":"2021-09-20T03:14:48.591858Z","iopub.status.idle":"2021-09-20T03:14:48.609137Z","shell.execute_reply.started":"2021-09-20T03:14:48.591831Z","shell.execute_reply":"2021-09-20T03:14:48.608114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for duplicate rows <a class=\"anchor\" id=\"duplicates\"></a>","metadata":{}},{"cell_type":"code","source":"# Check for duplicates\ndef duplicates(x):\n    \"\"\"Check for duplicate rows. \n    Either dedupe or return original dataset if there are no duplicate rows.\"\"\"\n    y = x.drop_duplicates()\n    duplicate_rows = x.shape[0] - y.shape[0]\n    print('Duplicate rows:', duplicate_rows)\n    if duplicate_rows == 0:\n        return x\n    else:\n        return y\n\ndf = duplicates(df)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.610774Z","iopub.execute_input":"2021-09-20T03:14:48.611127Z","iopub.status.idle":"2021-09-20T03:14:48.63837Z","shell.execute_reply.started":"2021-09-20T03:14:48.611094Z","shell.execute_reply":"2021-09-20T03:14:48.637508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for unique customer identifier <a class=\"anchor\" id=\"unique\"></a>","metadata":{}},{"cell_type":"code","source":"custs = df.CUST_ID.nunique()\nrows = df.shape[0]\n\nif custs == rows:\n    print('CUST_ID is unique')\nelse:\n    print('Need to create a unique identifier')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:48.639683Z","iopub.execute_input":"2021-09-20T03:14:48.639949Z","iopub.status.idle":"2021-09-20T03:14:48.647867Z","shell.execute_reply.started":"2021-09-20T03:14:48.639912Z","shell.execute_reply":"2021-09-20T03:14:48.646884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for outliers <a class=\"anchor\" id=\"outliers\"></a>","metadata":{}},{"cell_type":"code","source":"def outliers(df, features):\n    \"\"\"Count the number of outliers for each feature using the IQR\"\"\"\n    num_outliers = []\n    pct_outliers = []\n    total_rows = []\n    interquartile_range = []\n    for i in features:\n        Q1 = df[i].quantile(.25)\n        Q3 = df[i].quantile(.65)\n        IQR = Q3 - Q1\n        outliers = len(df[(df[i] < (Q1-1.5*IQR)) | (df[i] > (Q3+1.5*IQR))])\n        rows = len(df[i])\n        pct = outliers/rows\n        interquartile_range.append(IQR)\n        num_outliers.append(outliers)\n        pct_outliers.append(pct)\n        total_rows.append(rows)\n        \n    count_outliers = pd.DataFrame({'Feature': features\n                               , 'Num_Outliers': num_outliers\n                                , 'Percent_Outliers': pct_outliers\n                                , 'IQR': interquartile_range\n                               , 'Total_Rows': total_rows}).sort_values('Percent_Outliers', ascending=False)\n    return count_outliers\n\ncount_outliers = outliers(df=df, features=features)\ncount_outliers","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:14:48.649919Z","iopub.execute_input":"2021-09-20T03:14:48.650363Z","iopub.status.idle":"2021-09-20T03:14:48.716774Z","shell.execute_reply.started":"2021-09-20T03:14:48.650319Z","shell.execute_reply":"2021-09-20T03:14:48.716153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize outliers with box plots\nfor i in features:\n    sns.boxplot(df[i])\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:14:48.71771Z","iopub.execute_input":"2021-09-20T03:14:48.718444Z","iopub.status.idle":"2021-09-20T03:14:51.236597Z","shell.execute_reply.started":"2021-09-20T03:14:48.71839Z","shell.execute_reply":"2021-09-20T03:14:51.236055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Robust scaler <a class=\"anchor\" id=\"standardize\"></a>\n    Since most of the features contain a non-trivial amount of outliers, use the robust scaler instead of the standard scaler. The robust scaler uses the median and IQR, which are better estimates of central tendency in the precense of outliers.","metadata":{}},{"cell_type":"code","source":"# Scale the features\nX = df[features]\nX_scaled = pd.DataFrame(RobustScaler().fit_transform(X), columns=X.columns, index=X.index)\ndf_scaled = pd.concat([df['CUST_ID'], X_scaled], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:51.237637Z","iopub.execute_input":"2021-09-20T03:14:51.238338Z","iopub.status.idle":"2021-09-20T03:14:51.26542Z","shell.execute_reply.started":"2021-09-20T03:14:51.238305Z","shell.execute_reply":"2021-09-20T03:14:51.264409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory analysis <a class=\"anchor\" id=\"EDA\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Data types <a class=\"anchor\" id=\"dtypes\"></a>","metadata":{}},{"cell_type":"code","source":"df_scaled[features].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:51.26872Z","iopub.execute_input":"2021-09-20T03:14:51.269065Z","iopub.status.idle":"2021-09-20T03:14:51.286524Z","shell.execute_reply.started":"2021-09-20T03:14:51.269034Z","shell.execute_reply":"2021-09-20T03:14:51.285662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary statistics <a class=\"anchor\" id=\"summary_stats\"></a>\n    Mean, median, min and max","metadata":{}},{"cell_type":"code","source":"(df_scaled[features].describe().transpose()\n     [['mean', '50%', 'min', 'max']]\n     .rename(columns={'50%': 'median'})\n     .style.background_gradient(cmap = 'RdYlGn'))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:14:51.287636Z","iopub.execute_input":"2021-09-20T03:14:51.287862Z","iopub.status.idle":"2021-09-20T03:14:51.41093Z","shell.execute_reply.started":"2021-09-20T03:14:51.287827Z","shell.execute_reply":"2021-09-20T03:14:51.409964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bootstrapping <a class=\"anchor\" id=\"bootstrapping\"></a>\n    Estiamte the population mean for each feature with bootstrap sampling.","metadata":{}},{"cell_type":"code","source":"# Bootstrap sampling\nsamples = 1000\nest_popu_means = {}\nboot = []\nfor a in features:\n    for b in range(samples):\n        c = df[a].sample(frac = 0.33, random_state = 1).mean()\n        boot.append(c)\n    p_mean = sum(boot)/len(boot)\n    est_popu_means[a] = p_mean\n    boot.clear()\n\n# Mean of each feature in the dataset\nsampling_mean = []\nfor i in features:\n    x = df[i].mean()\n    sampling_mean.append(x)\n\ndf_means = pd.DataFrame({'Feature': list(est_popu_means.keys())\n                       , 'Bootstrap_Mean': list(est_popu_means.values())\n                       , 'Mean_in_dataset': sampling_mean})\n\ndf_means","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:14:51.412713Z","iopub.execute_input":"2021-09-20T03:14:51.413428Z","iopub.status.idle":"2021-09-20T03:15:01.739745Z","shell.execute_reply.started":"2021-09-20T03:14:51.413381Z","shell.execute_reply":"2021-09-20T03:15:01.738837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distributions <a class=\"anchor\" id=\"distributions\"></a>\n    Plot the distribution of each feature with its estimate population mean from the bootstrap sampling.","metadata":{}},{"cell_type":"code","source":"for i in features:\n    boot_mean = df_means.loc[df_means['Feature'] == i, 'Bootstrap_Mean'].iloc[0]\n    print(i)\n    print('Bootstrap mean:', boot_mean)\n    sns.histplot(df[i])\n    plt.axvline(boot_mean, color = 'red')\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:15:01.741293Z","iopub.execute_input":"2021-09-20T03:15:01.741767Z","iopub.status.idle":"2021-09-20T03:15:13.956324Z","shell.execute_reply.started":"2021-09-20T03:15:01.741727Z","shell.execute_reply":"2021-09-20T03:15:13.95538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spearman correlation <a class=\"anchor\" id=\"correlation\"></a>\n    Use the spearman correlation since the features are not normally distributed.","metadata":{}},{"cell_type":"code","source":"spearman_corr = round(df_scaled[features].corr(method = 'spearman'), 2)\nspearman_corr.style.background_gradient(cmap = 'RdYlGn')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:15:13.95764Z","iopub.execute_input":"2021-09-20T03:15:13.957894Z","iopub.status.idle":"2021-09-20T03:15:14.038016Z","shell.execute_reply.started":"2021-09-20T03:15:13.957863Z","shell.execute_reply":"2021-09-20T03:15:14.037265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multiple linear regression <a class=\"anchor\" id=\"multiple_linear_regression\"></a>","metadata":{}},{"cell_type":"code","source":"response_variable = []\nr2score = []\nresiduals = pd.DataFrame()\n\nfor i in features:\n    X = df_scaled[features]\n    X = X.drop(i, axis=1)\n    y = df_scaled[i]\n    model = sm.OLS(y, X).fit()\n    y_pred = model.predict(X)\n    score = r2_score(y, y_pred)\n    \n    residuals[i] = y - y_pred\n    \n    response_variable.append(i)\n    r2score.append(score)\n\nregression_results = pd.DataFrame({'Response_variable': response_variable\n                                   , 'R2_Score': r2score})\n\nregression_results.sort_values('R2_Score', ascending=False, inplace=True)\nplt.figure(figsize=(5, 10))\nsns.barplot(y = 'Response_variable',\n            x = 'R2_Score',\n            orient = 'h',\n            data=regression_results)\nplt.title('R2 Score For Each Feature Using The Other Features As Predictors', fontsize=18)\nplt.xlabel('R2 Score')\nplt.ylabel('Feature')\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:15:14.039208Z","iopub.execute_input":"2021-09-20T03:15:14.039547Z","iopub.status.idle":"2021-09-20T03:15:14.685581Z","shell.execute_reply.started":"2021-09-20T03:15:14.03952Z","shell.execute_reply":"2021-09-20T03:15:14.684696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of the residuals\n# (Multiple linear regression assumes that the residuals are normally distributed)\nfor i in residuals.columns:\n    skew = round(residuals[i].skew(), 1)\n    sns.histplot(residuals[i])\n    plt.title('Distribution of Residuals: ' + str(i) + ' --- Skew ' + str(skew), fontsize=14)\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:15:14.686808Z","iopub.execute_input":"2021-09-20T03:15:14.687055Z","iopub.status.idle":"2021-09-20T03:15:29.425596Z","shell.execute_reply.started":"2021-09-20T03:15:14.687026Z","shell.execute_reply":"2021-09-20T03:15:29.424947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling <a class=\"anchor\" id=\"modeling\"></a>","metadata":{}},{"cell_type":"markdown","source":"## K-means clustering <a class=\"anchor\" id=\"cluster\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Find optimal number of clusters <a class=\"anchor\" id=\"elbow\"></a>","metadata":{}},{"cell_type":"code","source":"# Use the elbow method to choose the optimal number of clusters \nX = df_scaled[features]\nkmeans = KMeans(random_state=1)\nkmeans_vis = KElbowVisualizer(kmeans, k=(1,15), metric='distortion', timings=False).fit(X)\nprint('Optimal number of clusters:', kmeans_vis.elbow_value_)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:15:29.426722Z","iopub.execute_input":"2021-09-20T03:15:29.427431Z","iopub.status.idle":"2021-09-20T03:15:52.461909Z","shell.execute_reply.started":"2021-09-20T03:15:29.427399Z","shell.execute_reply":"2021-09-20T03:15:52.460918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cluster customers <a class=\"anchor\" id=\"cluster_customers\"></a>","metadata":{}},{"cell_type":"code","source":"# Cluster the customers using the optimal number of clusters\ndf_scaled['CLUSTERS'] = KMeans(n_clusters=kmeans_vis.elbow_value_, random_state=1).fit_predict(X)\n\n# Count the number of customers in each cluster\ncount_clusters = df_scaled.groupby('CLUSTERS').agg({'CUST_ID': 'nunique'})\ncustomers = df_scaled['CUST_ID'].nunique()\ncount_clusters['Percent_of_Customers'] = count_clusters['CUST_ID']/customers\ncount_clusters","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:15:52.463814Z","iopub.execute_input":"2021-09-20T03:15:52.46406Z","iopub.status.idle":"2021-09-20T03:15:54.843513Z","shell.execute_reply.started":"2021-09-20T03:15:52.46403Z","shell.execute_reply":"2021-09-20T03:15:54.842604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the average feature value for each cluster\ndescribe_clusters = df_scaled.groupby('CLUSTERS').mean()\n\nfor i in features:\n    sns.barplot(x = describe_clusters.index\n               , y = i\n               , data = describe_clusters)\n    plt.title('Mean ' + str(i) + ' by Cluster')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:15:54.847193Z","iopub.execute_input":"2021-09-20T03:15:54.847931Z","iopub.status.idle":"2021-09-20T03:15:58.538236Z","shell.execute_reply.started":"2021-09-20T03:15:54.847884Z","shell.execute_reply":"2021-09-20T03:15:58.537325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average feature value for each cluster\ndescribe_clusters.transpose().style.background_gradient(cmap = 'RdYlGn')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:15:58.541291Z","iopub.execute_input":"2021-09-20T03:15:58.541528Z","iopub.status.idle":"2021-09-20T03:15:58.566813Z","shell.execute_reply.started":"2021-09-20T03:15:58.541495Z","shell.execute_reply":"2021-09-20T03:15:58.565985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SMOTE <a class=\"anchor\" id=\"SMOTE\"></a>\nSince the clusters are not equally sized, use the synthetic minority oversampling technique (SMOTE) to oversample the smaller clusters.","metadata":{}},{"cell_type":"code","source":"# SMOTE\nX = df_scaled[features]\ny = df_scaled['CLUSTERS']\n\ncounter = Counter(y)\nprint('Before SMOTE:')\nprint(dict(sorted(counter.items())))\nprint('='*50)\n\noversample = SMOTE(random_state = 1)\nX_SMOTE, y_SMOTE = oversample.fit_resample(X, y)\n\ncounter = Counter(y_SMOTE)\nprint('After SMOTE:')\nprint(dict(sorted(counter.items())))\n\ndf_scaled_SMOTE = pd.concat([y_SMOTE, X_SMOTE], axis=1)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:15:58.568202Z","iopub.execute_input":"2021-09-20T03:15:58.568674Z","iopub.status.idle":"2021-09-20T03:15:58.692612Z","shell.execute_reply.started":"2021-09-20T03:15:58.568641Z","shell.execute_reply":"2021-09-20T03:15:58.691719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model selection <a class=\"anchor\" id=\"model_selection\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Decision tree classifier <a class=\"anchor\" id=\"decision_tree\"></a>","metadata":{}},{"cell_type":"code","source":"# Separate the feature and target variables\nX = df_scaled_SMOTE[features]\ny = df_scaled_SMOTE['CLUSTERS']\n\n# Create train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\n\n# Fit the decision tree with the training data\ntree_model = DecisionTreeClassifier(random_state=1).fit(X_train, y_train)\n\n# Predict test values\ny_pred = tree_model.predict(X_test)\n\n# Evaluate the model with accuracy on the test dataset\ntree_accuracy = accuracy_score(y_test, y_pred)\n\n# Classification report\nprint('Accuracy:', tree_accuracy)\nprint('Decision Tree Classifier')\nprint(classification_report(y_test, y_pred))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:15:58.69395Z","iopub.execute_input":"2021-09-20T03:15:58.694182Z","iopub.status.idle":"2021-09-20T03:15:59.023073Z","shell.execute_reply.started":"2021-09-20T03:15:58.694155Z","shell.execute_reply":"2021-09-20T03:15:59.02195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multinomial logistic regression <a class=\"anchor\" id=\"logistic_regression\"></a>","metadata":{}},{"cell_type":"code","source":"# The multinomial logistic regression model assumes that the predicters are not strongly correlated with one another.\n# Use the variance inflation factor (VIF) to check for multicollinearity\nX = df_scaled_SMOTE[features]\n\nvif = pd.DataFrame()\nvif[\"feature\"] = X.columns\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n\n# Create list of features with a VIF >= 10\nhigh_vif = list(vif[vif[\"VIF\"] >= 10][\"feature\"])\n\n# Remove features with a high VIF\nremove_high_vif = [x for x in features if x not in high_vif]\n\nvif.sort_values(\"VIF\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:15:59.025006Z","iopub.execute_input":"2021-09-20T03:15:59.025356Z","iopub.status.idle":"2021-09-20T03:15:59.877276Z","shell.execute_reply.started":"2021-09-20T03:15:59.025309Z","shell.execute_reply":"2021-09-20T03:15:59.876348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multinomial logistic regression\n\n# Separate the feature and target variables\nX = df_scaled_SMOTE[remove_high_vif]\nX = sm.add_constant(X)\ny = df_scaled_SMOTE['CLUSTERS']\n\n# Create train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\n\n# Fit the logistic model with the training data\nlogit_model = LogisticRegression(random_state=1, solver='liblinear').fit(X_train, y_train)\n\n# Predict test values\ny_pred = logit_model.predict(X_test)\n\n# Evaluate the model with accuracy on the test dataset\nMNLogit_accuracy = accuracy_score(y_test, y_pred)\n\n# Classification report\nprint('Accuracy:', MNLogit_accuracy)\nprint('Multinomial Logistic Regression')\nprint(classification_report(y_test, y_pred))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-20T03:15:59.879359Z","iopub.execute_input":"2021-09-20T03:15:59.880018Z","iopub.status.idle":"2021-09-20T03:16:00.544851Z","shell.execute_reply.started":"2021-09-20T03:15:59.879961Z","shell.execute_reply":"2021-09-20T03:16:00.543955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The decision tree is more accurate than the logistic model.","metadata":{}},{"cell_type":"markdown","source":"### Optimize decision tree hyperparamters","metadata":{}},{"cell_type":"code","source":"# Max depth of the decision tree classifier\nX = df_scaled_SMOTE[features]\ny = df_scaled_SMOTE['CLUSTERS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\n\nacc = []\ndepth = []\nfor i in range(1, 16):\n    tree_model = DecisionTreeClassifier(random_state=1, max_depth=i).fit(X_train, y_train)\n    y_pred = tree_model.predict(X_test)\n    tree_accuracy = accuracy_score(y_test, y_pred)\n    acc.append(tree_accuracy)\n    depth.append(i)\n\ntree_depth = pd.DataFrame({'Max_Depth': depth\n                          , 'Accuracy': acc})\nsns.lineplot(x = 'Max_Depth'\n            , y = 'Accuracy'\n            , data = tree_depth)\nplt.title('Max Depth of Decision Tree and its Accuracy', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:16:00.546821Z","iopub.execute_input":"2021-09-20T03:16:00.547436Z","iopub.status.idle":"2021-09-20T03:16:03.517841Z","shell.execute_reply.started":"2021-09-20T03:16:00.547391Z","shell.execute_reply":"2021-09-20T03:16:03.516955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The accuracy stops improving when the tree depth is greater than 10\n# Fit decision tree with the optimal max depth\noptimal_max_depth = 10\n\nX = df_scaled_SMOTE[features]\ny = df_scaled_SMOTE['CLUSTERS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=1, max_depth=optimal_max_depth).fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\ntree_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', tree_accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:16:03.519204Z","iopub.execute_input":"2021-09-20T03:16:03.519439Z","iopub.status.idle":"2021-09-20T03:16:03.772668Z","shell.execute_reply.started":"2021-09-20T03:16:03.51941Z","shell.execute_reply":"2021-09-20T03:16:03.771724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance <a class=\"anchor\" id=\"feature_importance\"></a>","metadata":{}},{"cell_type":"code","source":"tree_importance = pd.DataFrame({'Feature': features\n                               , 'Importance': tree_model.feature_importances_})\n\ntree_importance.sort_values('Importance', ascending=False, inplace=True)\n\nplt.figure(figsize=(5, 7))\nsns.barplot(y = 'Feature',\n            x = 'Importance',\n            orient = 'h',\n            data=tree_importance)\nplt.title('Feature Importance in the Decision Tree Classifier', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:16:03.774142Z","iopub.execute_input":"2021-09-20T03:16:03.774553Z","iopub.status.idle":"2021-09-20T03:16:04.095829Z","shell.execute_reply.started":"2021-09-20T03:16:03.7745Z","shell.execute_reply":"2021-09-20T03:16:04.094789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PURCHASES is the most important feature, plot its distribution for each cluster\nunique = sorted(df_scaled_SMOTE.CLUSTERS.unique())\npalette = dict(zip(unique, sns.color_palette(n_colors=len(unique))))\n\nsns.histplot(x = 'PURCHASES'\n            , hue = 'CLUSTERS'\n             , element = 'poly'\n             , palette = palette\n            , data = df_scaled_SMOTE)\nplt.title('Distribution of PURCHASES by CLUSTER', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:16:04.098963Z","iopub.execute_input":"2021-09-20T03:16:04.099195Z","iopub.status.idle":"2021-09-20T03:16:04.436412Z","shell.execute_reply.started":"2021-09-20T03:16:04.099168Z","shell.execute_reply":"2021-09-20T03:16:04.435589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize decision tree <a class=\"anchor\" id=\"visualize\"></a>","metadata":{}},{"cell_type":"code","source":"targets = sorted(df_scaled_SMOTE['CLUSTERS'].unique())\ntargets = ['Cluster_' + str(x) for x in targets]\n\nplt.figure(figsize=(175, 25))\nplot_tree(tree_model\n          , feature_names = features\n          , class_names = targets\n          , filled = True\n          , fontsize = 10)\n\nplt.savefig('customer_segmentation_decision_tree.jpg')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:16:04.437749Z","iopub.execute_input":"2021-09-20T03:16:04.437998Z","iopub.status.idle":"2021-09-20T03:16:26.106886Z","shell.execute_reply.started":"2021-09-20T03:16:04.437963Z","shell.execute_reply":"2021-09-20T03:16:26.105816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summarize insights from analysis <a class=\"anchor\" id=\"summary\"></a>","metadata":{}},{"cell_type":"code","source":"def summarize_clusters(cluster):\n    \"\"\"Returns description of cluster\"\"\"\n    counter = Counter(cluster)\n    counter = dict(sorted(counter.items()))\n    summary = dict()\n    for i in counter.keys():\n        if counter[i] >= 5400:\n            summary[i] = ['Average credit card customer (each of their features are within one standard deviation from the mean)', 'Not the highest or lowest for any feature']\n        elif counter[i] >= 1400:\n            summary[i] = ['Lowest number of transactions made with cash in advanced'\n                                , 'Lowest amount of cash in advance transactions'\n                                , 'Lowest number of purchases'\n                                , 'Lowest amount of purchases'\n                                , 'Lowest amount purchases done in installment'\n                                , 'Lowest balance frequency'\n                                , 'Lowest balance amount left in their account to make purchases'\n                                , 'Lowest tenure'\n                                , 'Lowest credit card limit'\n                                , 'Lowest amount of minimum payments']\n        elif counter[i] >= 1190:\n            summary[i] = ['Highest number of transactions made with cash in advanced'\n                                , 'Highest amount of cash in advance transactions'\n                                , 'Highest frequency of cash in advance transactions'\n                                , 'High amount of minimum payments'\n                                , 'Low tenure'\n                                , 'Low balance frequency'\n                                , 'Low purchase freqency']\n        elif counter[i] >= 700:\n            summary[i] = ['High number of purchases'\n                                , 'High amount of purchases'\n                                , 'High amount of purchases done in installment'\n                                , 'High amount of one-off purchases'\n                                , 'High frequency of one-off purchases'\n                                , 'High amount of payments'\n                                , 'High amount of full payments']\n        elif counter[i] >= 30:\n            summary[i] = ['Highest balance frequency'\n                                , 'Highest tenure'\n                                , 'Highest amount of minimum payments'\n                                , 'Lowest amount of full payments'\n                                , 'Lowest amount of one-off purchases'\n                                , 'Lowest frequency of one-off purchases']\n        else:\n            summary[i] = ['Highest number of purchases'\n                                , 'Highest amount of purchases'\n                                , 'Highest frequency of purchases'\n                                , 'Highest amount of one-off purchases'\n                                , 'Highest frequency of one-off purchases'\n                                , 'Highest amount of purchases done in installment'\n                                , 'Highest amount of payments'\n                                , 'Highest amount of full payments'\n                                , 'Highest credit limit'\n                                , 'Highest balance amount left in their account to make purchases'\n                                , 'Lowest frequency of cash in advance transactions']\n    for i in summary.keys():\n        print('Cluster:', i)\n        print(summary[i])\n        print('-'*100)\n\n\nclusters = df_scaled['CLUSTERS']\nsummarize_clusters(clusters)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T03:16:26.108399Z","iopub.execute_input":"2021-09-20T03:16:26.108907Z","iopub.status.idle":"2021-09-20T03:16:26.1251Z","shell.execute_reply.started":"2021-09-20T03:16:26.108868Z","shell.execute_reply":"2021-09-20T03:16:26.124002Z"},"trusted":true},"execution_count":null,"outputs":[]}]}