{"cells":[{"metadata":{},"cell_type":"raw","source":"id:Identification number\nbattery_power:Total energy a battery can store in one time measured in mAh\nblue:Has bluetooth or not\nclock_speed:speed at which microprocessor executes instructions\ndual_sim:Has dual sim support or not\nfc:Front Camera mega pixels\nfour_g:Has 4G or not\nint_memory:Internal Memory in Gigabytes\nm_dep:Mobile Depth in cm\nmobile_wt:Weight of mobile phone\nn_cores:Number of cores of processor\npc:Primary Camera mega pixels\npx_height:Pixel Resolution Height\npx_width:Pixel Resolution Width\nram:Random Access Memory in Megabytes\nsc_h:Screen Height of mobile in cm\nsc_w:Screen Width of mobile in cm\ntalk_time:longest time that a single battery charge will last when you are\nthree_g:Has 3G or not\ntouch_screen:Has touch screen or not\nwifi:Has wifi or not","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Loading a few of the required libraries for our Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport pylab as p\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the train and test datasets and get their head()...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/mobile-price-classification/train.csv')\ntest_data=pd.read_csv('/kaggle/input/mobile-price-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking carefully at the train and test datasets, i can see that the train data has a price_range column unlike the test data which instead has an id column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nGetting the dataset sizes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test_data : {}, train_data :{}'.format(test_data.shape, train_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get more information about the datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the information above i can see that all the feaatures have numerical values i.e either floats or integers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nData descriptions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['price_range'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = train_data['price_range'].value_counts()\nplt.figure(figsize=(8,6))\nplt.style.use('seaborn-paper')\nplt.pie(size, labels=[3,2,1,0],shadow=True, autopct='%1.1f%%', colors=['cyan','darkred', 'darkgreen', 'darkblue'])\nplt.title('A pie chart showing price range distributions among the data', fontsize=14, color='purple')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Kurtosis : {}'.format(kurtosis(train_data['price_range'])))\nprint('Skew : {}'.format(skew(train_data['price_range'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above information we can tell that our price_range data follows a platykurtic type of kurtosis hence it has thin tails and a skewness of zero shows that our price_range follows a normal distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"size = train_data['three_g'].value_counts()\nplt.figure(figsize=(8,6))\nplt.style.use('seaborn-paper')\nplt.pie(size, labels=[0,1],shadow=True, autopct='%1.1f%%', colors=['y', 'white'])\nplt.title('A pie chart showing three_g distributions among the data', fontsize=14, color='purple')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = train_data['four_g'].value_counts()\nplt.figure(figsize=(8,6))\nplt.style.use('seaborn-paper')\nplt.pie(size, labels=[0,1],shadow=True, autopct='%1.1f%%', colors=['cyan', 'green'])\nplt.title('A pie chart showing four_g distributions among the data', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,18))\ncorrelation = train_data.corr()\nsns.heatmap(correlation,square=True,annot=True,vmax=0.9, color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmap above shows the relationship between different features in the data and as you can see the px_width & px_height and sc_w & sc_h are very highly related ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='price_range', y='ram', kind='swarm', data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='price_range', y='ram', kind='box', data=train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nFrom the plot above we can see that there's a very high relationship between the price range and the ram in that as the ram increases, the price range also goes up.. meaning the ram is one of the very important faetures to look at while modeling to get the mobile price predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.catplot(x='price_range', y='mobile_wt', kind='box', data=train_data)\nplt.title('Distributions between price_range with respect to mobile_wt', color='darkgreen', fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.catplot(x='price_range', y='px_height', kind='box', data=train_data)\nplt.title('Distributions between price_range with respect to px_height', color='darkgreen', fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.catplot(x='price_range', y='px_width', kind='box', data=train_data)\nplt.title('Distributions between price_range with respect to px_width', color='green', fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.catplot(x='price_range', y='battery_power', kind='box',hue='blue', data=train_data)\nplt.title('Distributions between price_range and bluetooth with respect to battery_power', color='darkred', fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotly to try and see some interactive graphs\nimport plotly.offline as pyo\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot, plot\nimport cufflinks as cf\ncf.go_offline()\npyo.init_notebook_mode()\nprint(__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.iplot(kind='scatter', x='sc_w', y='sc_h', mode='markers', colors='black',size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.iplot(kind='scatter', x='px_width', y='px_height', mode='markers', size=8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above scatter maps we can see that we shall have to drop one of the features for our modeling since they are highly correlated hence making them very similar","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['fc'].iplot(kind='hist', bins=40, xTitle='Mega pixels', yTitle='Frequency', colors='cyan')\ntrain_data['pc'].iplot(kind='hist', bins=40, xTitle='Mega pixels',yTitle='Frequency', colors='darkred')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pointplot(train_data['talk_time'], y=train_data['price_range'], data=train_data)\nplt.title('Point plot displaying how price ranges with talk_time', fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['px_area'] = test_data['px_height'] * test_data['px_width']\ntest_data['phone_area'] = test_data['sc_w'] * test_data['sc_h']\ntest_data.drop(['px_width', 'px_height', 'sc_w', 'sc_h', 'id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['px_area'] = train_data['px_height'] * train_data['px_width']\ntrain_data['phone_area'] = train_data['sc_w'] * train_data['sc_h']\ntrain_data.drop(['px_width', 'px_height', 'sc_w', 'sc_h'], axis=1, inplace=True)\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = train_data.drop('price_range', axis=1)\ny = train_data['price_range']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrnd = RandomForestClassifier(max_depth=8, n_estimators=700, random_state=0, n_jobs=-1)\nrnd.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ideally looking at our features, few features should be important which favours the use of Lasso Regression but looking at our  heatmap, not all features are as important so i guess we try using both Lasso and Ridge Regression to see our scores and see which model to use","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.001, max_iter=1000, random_state=20)\nlasso.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Lasso raises so many questions, as i raise the values of alpha to above 1, 10, 100, 1000 and so on, i tend to get very low scores tending to zero.. Let me try ridge and see how it scores here on our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=1, max_iter=1000, random_state=20)\nridge.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ridge gives very great scores of the test and train sets of the data so i believe its one of the models we should look at","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n\nWell despite all the great scores from Lasso, Ridge and Linear Regression, we can't use either because our target variable is discrete and these are best suited for continous target variables i.e they will end up giving me predictions that are floats","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nreg = LGBMClassifier(learning_rate=0.1, n_estimators=700,  max_depth=8, random_state=0, n_jobs=-1)\nreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much as the LGBMClassifier gives the perfect score for the train_data i think it over fits since the dataset is small plus i don't think it's best for my prediction since there's a slightly big deviation between the test score and train score as compared to other algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmod = LogisticRegression(C=0.1, random_state=0, n_jobs=-1, max_iter=100)\nmod.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ntrees = DecisionTreeClassifier(random_state=20, max_depth=5, criterion='entropy')\nmodel = trees.fit(X_train, y_train)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trees.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trees.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams = {'max_depth':[5, 1], 'criterion':['entropy', 'gini'], 'random_state':[20,5]}\ngridz = GridSearchCV(DecisionTreeClassifier(), param_grid=params, refit=True,verbose=3)\ngridz.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridz.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridz.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"final_results = trees.predict(test_data)\nfinal_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/mobile-price-classification/test.csv')\ntest_data['id']\nfinal = pd.DataFrame({'id':test_data.id, 'price_range': final_results})\nfinal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}