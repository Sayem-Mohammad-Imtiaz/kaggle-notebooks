{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>DataScience Tackling Heart Diseases !</center></h1>\n<img src=\"https://cdn.dnaindia.com/sites/default/files/styles/full/public/2018/09/25/735506-heart-disease.jpg\" alt=\"drawing\" width=\"600\" height=\"400\"/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Another CVDs Dataset !\nSo, We have one more Dataset related to Heart Diseases. We tackled similar kind of problem in our [Previous Kernel](https://www.kaggle.com/rahulgulia/data-science-and-cardiovascular-diseases-cvds) based on Cardiovascular Disease Dataset provided by [Svetlana Ulianova](https://www.kaggle.com/sulianova). <br><br>\nThis time we have Heart Disease UCI Dataset but after going through some discussion forms for this dataset, I realized this dataset is quite different to the [Original Dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) without updating necessary changes in the dataset description for some unknown reasons. <br><br>\nKeeping above issue in the mind, We will be following this [Discussion Form](https://www.kaggle.com/ronitf/heart-disease-uci/discussion/105877) as our reference for our features in this Kernel. A small shoutout to our Saviour [IntiPic](https://www.kaggle.com/intipic). <br><br>\nI'll be revising some points I mentioned in my [Previous Kernel](https://www.kaggle.com/rahulgulia/data-science-and-cardiovascular-diseases-cvds), following the similar kind of pipeline<br><br>\nBy the way, if you're wondering why I used this specific pic in the title then it's because this pic reminded me of my favourite character Walter White from Breaking Bad series :D !<br><br>\n\n<center><img src=\"https://i.insider.com/5dade9bc045a3139e8686c33?width=1300&format=jpeg&auto=webp\" alt=\"drawing\" width=\"600\" height=\"400\"/></center><br>\n\n\nKeeping all jokes apart, Let's get it started !\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Necessary Libaraies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport os#Walking through directores\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport missingno as msno # Visualizing Missing Value\nfrom plotly.subplots import make_subplots #To Create Subplots\n\n\n\nimport seaborn as sns # stastical graphs \nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)']  #used for markers\nimport plotly.graph_objects as go # Generate Graphs\n\n\nfrom sklearn.preprocessing import MinMaxScaler # Scaling Purpose\n\nfrom sklearn.neighbors import KNeighborsClassifier #KNN Model\nfrom sklearn.ensemble import RandomForestClassifier #RandomForest Model\nfrom sklearn.linear_model import LogisticRegression #Logistic Model\n\nfrom sklearn.model_selection import train_test_split # Splitting into train and test\n\nfrom sklearn.model_selection import GridSearchCV# Hyperparameter Tuning\nfrom sklearn.model_selection import cross_val_score#cross validation score\n\nfrom sklearn.metrics import classification_report # text report showing the main classification metrics\nfrom sklearn.metrics import confusion_matrix #to get confusion_matirx ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Dive into our CSV File to get a glance of what we are dealing with ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = ['?', '--', ' ', 'NA', 'N/A', '-'] #Sometimes Missing Values are't in form of NaN\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv', delimiter = ',', na_values = missing_values)\nprint('There are Total {} datapoints in the dataset with {} Features listed as {}:'.format(df.shape[0], df.shape[1], df.columns.values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">So, this Dataset contains the following Features :\n* age :> Age of the Patient in Years\n* sex :> Gender of the Patient\n* cp :> Chest Pain Type\n* trestbps :> Resting Blood Pressure in mm Hg\n* chol :> Serum Cholestoral in mg/dl (Measurement of certain elements in the blood)\n* fbs :> Fasting Blood Sugar Lower than 100 mg/dL is Normal, 100 mg/dL - 125 mg/dL is considered prediabetes. 125 mg/dL + is cosidered to have Diabetes\n* restecg :> Resting Electrocardiographic Results (Test that measures the Electrical Activity of the Heart)\n* thalach :> Maximum Heart Rate achieved during Thalium Stress Test\n* exang :> Exercise induced angina (yes or no)\n* oldpeak :> ST depression induced by exercise relative to rest\n* slope :> Slope of peak exercise ST segment \n* ca :> Number of major vessels colored by Fluoroscopy\n* thal :> Thalium Stress Test result\n* target :> Target Value (Patient having Heart Disease or Not)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before futher proceeding with our analysis, let's correct some mistakes as mentioned in the [Discussion Form](https://www.kaggle.com/ronitf/heart-disease-uci/discussion/105877) <br>\nQuoting from the discussion form :\n> * Mistake 1 : data 93, 139, 164, 165 and 252 have ca=4 which is incorrect. In the original Cleveland dataset they are NaNs (so they should be removed)\n> * Mistake 2 : data 49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset.\n> * Mistake 3 : Our Target Values are swapped i.e. 0 : Heart Disease and 1 : No Heart Disease","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Mistake 1\n> * It's even mentioned in the dataset that value of 'ca' ranges from 0-3. So, clearly value 4 represents and error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['ca']==4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['ca']==4, 'ca'] = np.NaN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Mistake 2\n\n> * Data 49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[48, 'thal'] = np.NaN\ndf.loc[281, 'thal'] = np.NaN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, We can move to our analysis..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis\n\n\n<img  src=\"https://miro.medium.com/max/1400/1*PKXC0FeXQc5LVmqhJ8HnVg.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">In our Data Analysis, We will try to analyze to find out the below stuffs :\n* Missing / Duplicate Values \n* Find all the Continuous Features\n* Handling Outliers if Any\n* Distribution of the Continuous Features\n* Find all the Discrete Features\n* Cardinality of Discrete Features\n* Relation with Independent and Dependent Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Missing / Duplicate Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have dataset free of Null values (expect the one we introduced) <br>\nThough We can use tradational way of visualizing Missing Values by making a heatmap, but i came across a new library and just to introduce that to everyone, we'll gonna use that ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown by Horizontal lines, We have 5 and 2 null values in ca and thal feature respectively. <br><br>\nWe'll gonna handle them in our Feature Engineering Section. Now let's move further..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_sum = df.duplicated().sum()\nif duplicate_sum:\n    print('Duplicates Rows in Dataset are : {}'.format(duplicate_sum))\nelse:\n    print('Dataset contains no Duplicate Values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated = df[df.duplicated(keep=False)]\nduplicated.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hey Mr. Duplicate ! Found you :) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(keep = 'first', inplace = True)\nprint('Total {} datapoints remaining with {} features'.format(df.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Continuous Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Well, You know my drill :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Continuous_features = [feature for feature in df.columns if len(df[feature].unique())>25]\nprint('Continuous Values are : {}'.format(Continuous_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[Continuous_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to get some stastical stuffs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[Continuous_features].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With such a big difference between 75% percentile and max value, and 25% percentile and min value, We do possibly observe some outliers in 'trestbps', 'chol', 'thalach', 'oldpeak'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Outliers ? Ah Shit, Here We go again ..\n\n\n<img  src=\"https://pbs.twimg.com/media/EDANCjJXkAAOSjO.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I've talked about Outliers much in details in my [Previous Kernel](https://www.kaggle.com/rahulgulia/data-science-and-cardiovascular-diseases-cvds). You can have a look if you're looking for more info. <br>\nIn short, just remember Outliers are just the values that differs significantly from other observations ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Time for some Box Plot bois ..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Box(x=df['age'], name = 'age', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['trestbps'], name = 'trestbps', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['chol'], name = 'chol', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['thalach'], name = 'thalach', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['oldpeak'], name = 'oldpeak', boxpoints='outliers'))\n\nfig.update_layout(title_text=\"Box Plot for Continuous features with Outliers\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's not fair. We can hardly observe 'oldpeak' :/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(x=df['oldpeak'], name = 'oldpeak', boxpoints='outliers'))\n\nfig.update_layout(title_text=\"Box Plot for oldpeak with Outliers\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * Our Age doesn't contain Outliers. I'm proud of you man :)\n> * Though We observe some outliers in our remaining Continuous features\n\n<img  src=\"https://media.makeameme.org/created/heisenberg-approves-knhd1a.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def outliers(df_out, drop = False):\n    for each_feature in df_out.columns:\n        feature_data = df_out[each_feature]\n        Q1 = np.percentile(feature_data, 25.) # 25th percentile of the data of the given feature\n        Q3 = np.percentile(feature_data, 75.) # 75th percentile of the data of the given feature\n        IQR = Q3-Q1 #Interquartile Range\n        outlier_step = IQR * 1.5 #That's we were talking about above\n        outliers = feature_data[~((feature_data >= Q1 - outlier_step) & (feature_data <= Q3 + outlier_step))].index.tolist()  \n        if not drop:\n            print('For the feature {}, No of Outliers is {}'.format(each_feature, len(outliers)))\n        if drop:\n            df.drop(outliers, inplace = True, errors = 'ignore')\n            print('Outliers from {} feature removed'.format(each_feature))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers(df[Continuous_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since excluding outliers can cause your results to become statistically significant, We'll gonna drop our datapoints with Outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers(df[Continuous_features], drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are free from these Outliers now. <br>\nTime for Some EDA Stuffs ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" df[Continuous_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Age Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['age'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['age'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['age'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['age'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"Fequency of Age groups grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Age',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['age'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0='Age'))\n\nfig.add_trace(go.Violin(y=df['age'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0='Age'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\"Age Distribution grouped by Target Variable\",)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age is showing an expected pattern :\n> * Younger age group are less prone to Older age group\n> * Age group between 41 and 54 are less prone to Heart Diseases\n> * Age group between 57 and 63 shows a stong relation with the case of suffering from Heart Diseases\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## trestbps Distribution\n\ntrestbps or you can say Resting Blood Pressure is the ratio of Systolic and Diastolic Pressure in Resting State","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['trestbps'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['trestbps'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['trestbps'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['trestbps'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"Fequency of Resting Blood Pressure grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Resting Blood Pressure ',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['trestbps'].value_counts().index.to_list(),values=df[df['target'] ==0]['trestbps'].value_counts().values, hovertemplate = \"RPV: %{label} <br>Popularity: %{percent}</br>\" ,showlegend=False,  name = 'Heart Disease'), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['trestbps'].value_counts().index.to_list(),values=df[df['target'] ==1]['trestbps'].value_counts().values, hovertemplate = \"RPV: %{label} <br>Popularity: %{percent}</br>\" ,showlegend=False,  name = 'No Heart Disease'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4,)\nfig.update_layout(title_text=\"Distribution of Resting Blood Pressure\",title_x=0.5, title_y = 1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * We have good quantity of people having 120 and 130 Resting blood Pressure, followed by 140 and 110 ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## chol Distribution\n\nA Serum Cholesterol Level is a measurement of certain elements in the blood, including the amount of high- and low-density lipoprotein cholesterol (HDL and LDL) in a person’s blood.\n\n\n\n| Type    |      Level      |\n|----------|:-------------:|\n| Healthy Serum Cholesterol |  Less than 200 mg/dL |\n| Healthy LDL Cholesterol |  Less than 130 mg/dL |\n| Healthy HDL Cholesterol |    Higher than 55 mg/dL for Women and 45 mg/dL for Men   |\n| Healthy Triglycerides | Less than 150 mg/dL |","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['chol'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0=' Serum Cholesterol Level'))\n\nfig.add_trace(go.Violin(y=df['chol'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0=' Serum Cholesterol Level'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\" Serum Cholesterol Level Distribution grouped by Target Variable\",)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## thalach Distribution\n\nMaximum Heart Rate achieved during Thalium Stress Test <br>\nYour target heart rate during a stress test depends on your age. For adults, the maximum predicted heart rate is 220 minus your age. So, if you're 40 years old, the maximum predicted heart rate is 220 – 40 = 180","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['thalach'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['thalach'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['thalach'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['thalach'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"Fequency of Maximum Heart Rate achieved during Thalium Stress Test grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Maximum Heart Rate achieved during Thalium Stress Test ',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['thalach'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0=' Maximum Heart Rate achieved during Thalium Stress Test'))\n\nfig.add_trace(go.Violin(y=df['thalach'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0=' Maximum Heart Rate achieved during Thalium Stress Test'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\"Distribution for Maximum Heart Rate achieved during Thalium Stress Test grouped by Target Variable\",)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * We can observe a mild seperation in the spread between Heart Disease and No Heart Disease caused by Maximum Heart Rate achieved during Thalium Stress Test \n> * Patient with higher value of Maximum Heart Rate achieved during Thalium Stress Test are less prone to Heart Diseases as compared to Heart Disease Cases","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## oldpeak Distribuition\n\nST depression induced by exercise relative to rest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['oldpeak'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['oldpeak'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['oldpeak'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['oldpeak'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"ST depression Level Distribution grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='ST depression Level',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['oldpeak'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0='ST depression Level'))\n\nfig.add_trace(go.Violin(y=df['oldpeak'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0='ST depression Level'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\"ST depression Level Distribution grouped by Target Variable\",)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * We have max number of cases with ST depression level with 0.0, dominated by No Heart Disease Cases \n> * With maximum spread of No Heart Disease Cases densed around 0.0 to 1.0, We can observe a sparse spread for Heart Disease Cases around 0.4 to 2.25","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Continuous_features.append('target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(df[Continuous_features], kind='scatter',hue='target', palette=\"husl\", corner=True)\n\ng._legend.set_title('Cases')\nnew_labels = ['Heart Disease', 'No Heart Disease']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * oldpeak feautre form quite a linear seperatable relation with the remaining continuous features for Heart and No Heart Disease Cases\n> * While thalach forms mild seperation, other features doesn't form any clear seperation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Categorial Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorial_features = [feature for feature in df.columns if len(df[feature].unique())<=25]\nprint('Continuous Values are : {}'.format(Categorial_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for each_feature in Categorial_features:\n    print('No of Categorial Values in Feature {} is {} as {}'.format(each_feature, len(df[each_feature].unique()), df[each_feature].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After going through the description provided by the Distributer, Following information is gathered :\n> * sex (gender): 2 Categorial Values { 1 : Male, 0 : Female )\n> * cp (Chest Pain Type): 4 Categorial Values { 0 : asymptomatic, 1 : atypical angina, 2 : non-anginal pain, 3 : typical angina  )\n> * fbs (Fasting Blood Sugar): 2 Categorial Values { 1 : true if fbs greater than 120 mg/dl, 0 : false}\n> * restecg (Resting Electrocardiographic Results): 3 Categorial Values { 0 : showing probable or definite left ventricular hypertrophy, 1 : normal, 2 : having ST-T wave abnormality\n> * exang (Exercise induced angina): 2 Categorial Values { 1 : yes, 0 : no }\n> * slope (Slope of peak exercise ST segment): 3 Categorial Values { 0 : downsloping, 1 : flat, 2 : upsloping }\n> * ca (Number of major vessels colored by Fluoroscopy):  4 Categorial Values ranging from 0 - 3\n> * thal (Thalium Stress Test result): 3 Categorial Values { 1 : fixed defect, 2 : normal, 3 :  reversable defect }\n> * target (Target Feature): 2 Categorial Values { 1 : No Heart Disease, 0 : Heart Disease }","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = go.Figure([go.Pie(labels=['No Heart Disease', 'Heart Disease'],values=df['target'].value_counts().values,hovertemplate = '<br>Type: %{label}</br>Count: %{value}<br>Popularity: %{percent}</br>', name = '', marker_colors = colors)])\nfig.update_layout(title_text=\"Pie chart of Target Variable\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(hole=.4,)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, We are blessed with a balanced dataset :)\n\n\n<img  src=\"https://scontent.fdel25-1.fna.fbcdn.net/v/t1.0-9/48391658_625132244607946_7405943289678921728_n.png?_nc_cat=109&_nc_sid=730e14&_nc_ohc=rTg1qpGq8tMAX_hkJ7h&_nc_ht=scontent.fdel25-1.fna&oh=96e5ed6a028415dfe2dc5d17704c790d&oe=5EF44D22\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We'll gonna explore about distribution of our Categorial Features based on the Target Variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['sex'].value_counts().index.to_list(),values=df[df['target'] ==0]['sex'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Male', 'Female'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['sex'].value_counts().index.to_list(),values=df[df['target'] ==1]['sex'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Male', 'Female'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Sex grouped by Target Feature\",title_x=0.5, title_y = 1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['cp'].value_counts().index.to_list(),values=df[df['target'] ==0]['cp'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Asymptomatic', 'Non-Anginal Pain', 'Atypical Angina', 'Typical Angina'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['cp'].value_counts().index.to_list(),values=df[df['target'] ==1]['cp'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Asymptomatic', 'Non-Anginal Pain', 'Atypical Angina', 'Typical Angina'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Chest Pain Type grouped by Target Feature\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['fbs'].value_counts().index.to_list(),values=df[df['target'] ==0]['fbs'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Less than 120 mg/dl (False)', 'Greater than 120 mg/dl (True)'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['fbs'].value_counts().index.to_list(),values=df[df['target'] ==1]['fbs'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Less than 120 mg/dl (False)', 'Greater than 120 mg/dl (True)'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Fasting Blood Sugar grouped by Target Feature\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['restecg'].value_counts().index.to_list(),values=df[df['target'] ==0]['restecg'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Showing Probable or Definite Left Ventricular Hypertrophy', 'Normal', 'Having ST-T Wave Abnormality'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['restecg'].value_counts().index.to_list(),values=df[df['target'] ==1]['restecg'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Showing Probable or Definite Left Ventricular Hypertrophy', 'Normal', 'Having ST-T Wave Abnormality'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Resting Electrocardiographic Results grouped by Target Feature\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['slope'].value_counts().index.to_list(),values=df[df['target'] ==0]['slope'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Flat', 'Upsloping', 'Downsloping'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['slope'].value_counts().index.to_list(),values=df[df['target'] ==1]['slope'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Upsloping', 'Flat', 'Downsloping'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Slope of peak exercise ST segment grouped by Target Feature\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['ca'].value_counts().index.to_list(),values=df[df['target'] ==0]['ca'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['1', '0', '2', '3'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['ca'].value_counts().index.to_list(),values=df[df['target'] ==1]['ca'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['0', '1', '2', '3'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Number of major vessels colored by Fluoroscopy grouped by Target Feature\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['exang'].value_counts().index.to_list(),values=df[df['target'] ==0]['exang'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Yes', 'No'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['exang'].value_counts().index.to_list(),values=df[df['target'] ==1]['exang'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['No', 'Yes'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Exercise induced angina grouped by Target Feature\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['thal'].value_counts().index.to_list(),values=df[df['target'] ==0]['thal'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Reversable Defect', 'Normal', 'Fixed Defect'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}</br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['thal'].value_counts().index.to_list(),values=df[df['target'] ==1]['thal'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Normal', 'Reversable Defect', 'Fixed Defect'], hovertemplate = '%{customdata} <br>Count: %{value}</br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Thalium Stress Test result grouped by Target Feature\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\n<img  src=\"https://img-a.udemycdn.com/course/750x422/1304050_ee0f_8.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing np.NaN with median","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna(df.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'ca' and 'thal' aren't supposed to be float.<br>\nThey are categorial <br>\nHey! Stop wasting my memory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.astype({'ca': int, 'thal': int}) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good :) !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection\n\n<img  src=\"https://memegenerator.net/img/instances/80593855/with-many-features-comes-great-feature-selection.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nHaving irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features. <br>\nSo, We do feature selection ( automatically or manually ) to select good data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Time for a Matrix bois ..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20, 15) \nsns.heatmap(df.corr(), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features', fontsize = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly :\n> * 'cp', 'thalach', 'slope' shows good amount of positive correlation with our target\n> * 'exang', 'oldpeak', 'ca', 'thal', 'sex', 'age' shows good amount of negative correlation with our target\n> * 'fbs' being the lowest, 'chol', 'trestbps', 'restecg' carries low correlation with our target","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dropping 'fbs', 'chol', 'trestbps', 'restecg' ..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['fbs', 'chol', 'trestbps', 'restecg', 'target'], axis =1)\nY = df['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One-Hot Encoding\n\nAlways a good idea to handle your categorial features <br>\nIn the process, we'll be encoding 'cp', 'ca', 'thal', 'slope'\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_cp = pd.get_dummies(df['cp'], prefix = \"cp\")\nencoded_ca = pd.get_dummies(df['ca'], prefix = \"ca\")\nencoded_thal = pd.get_dummies(df['thal'], prefix = \"thal\")\nencoded_slope = pd.get_dummies(df['slope'], prefix = \"slope\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([X,encoded_cp, encoded_ca, encoded_thal, encoded_slope], axis = 1)\nX = X.drop(columns = ['cp', 'ca', 'thal', 'slope'], axis = 1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\n\n<img  src=\"https://miro.medium.com/max/1400/1*CpKFmbqZdjgC5B7eCFUnkw.jpeg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\n\nWe have multiple feature with various scales. We don't want our model to give priority to smaller / bigger values just because of difference in scale. <br>\nHence, Feature Scaling is performed during the data pre-processing to handle highly varying magnitudes or values or units","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Again We'll be using StandardScaler\n\n<img  src=\"https://miro.medium.com/max/992/1*dZlwWGNhFco5bmpfwYyLCQ.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n<br>\ni.e Mean as 0 and Standard Deviation as 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstandard_X = scaler.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, Our Data looks like this :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(standard_X, columns = X.columns).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\n<img  src=\"https://i.imgflip.com/2ymba8.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First of all, it's time for a classical split...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(standard_X,Y,test_size = 0.2,random_state=43, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll be training few models and then select the best model for our use case <br>\nModels will be hypertuned via GridSearchCV because : \n\n<img  src=\"https://miro.medium.com/max/612/1*iUkbA8Dlj-5B0S8u0oRNbQ.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Commenting out Hypertuning part to save time. <br>Feel free to uncomment and tune some paramteres by yourself. Who knows you may find better results than mine ;)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors\n\n> * Datapoint is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors\n\n<img  src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_neighbors':list(range(0, 101)),\n          'weights':['uniform', 'distance'],\n          'p':[1,2]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * n_neighbors :> That's our 'K'\n> * weights :> Uniform gives same weight to all points while in Distance, closer neighbors of a query point will have a greater influence than neighbors which are further away\n> * p:> if 1, use manhattan_distance to calculate distance , if 2 use euclidean_distance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"knn = KNeighborsClassifier()\nknn_grid_cv = GridSearchCV(knn, param_grid=params, cv=5) \nknn_grid_cv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",knn_grid_cv.best_params_)\"\"\"\n\nprint(\"Best Hyper Parameters: {'n_neighbors': 22, 'p': 2, 'weights': 'uniform'}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 22, p=2, weights = 'uniform') \nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest\n\n> * Consist of combination of Multiple Decision Trees\n> * Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction\n> * Based on Bagging Technique\n\n<img  src=\"https://static.javatpoint.com/tutorial/machine-learning/images/random-forest-algorithm2.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = { \n    'n_estimators': [300, 400, 500,600, 600, 700, 800, 900, 1000],\n    'max_depth' : [2,3,4,5,6,7, 8],\n    'criterion' : ['entropy','gini']\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * n_estimators :> No of Decision Trees to be used\n> * max_depth :> Depth of Each Tree\n> * criterion :> Measure the quality of a split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"rfc_gridcv = RandomForestClassifier(random_state=42)\nrfc_gridcv = GridSearchCV(estimator=rfc_gridcv, param_grid=params, cv= 5)\nrfc_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",rfc_gridcv.best_params_)\"\"\"\nprint(\"Best Hyper Parameters: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 1000}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(random_state=42, n_estimators=1000, max_depth= 2, criterion = 'gini')\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression\n\n> * Uses a Logistic Function to Model a Categorical Dependent variable\n\n<img  src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params_for_l1 = { \n    'C' :  np.logspace(0, 4, 10),\n    'solver' : ['liblinear', 'saga']\n}\n\nparams_for_l2 = { \n    'C' :  np.logspace(0, 4, 10),\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nparams_for_elasticnet = { \n    'C' :  np.logspace(0, 4, 10),\n    'l1_ratio' : np.arange (0.1, 1.0, 0.1),\n    'solver' : ['saga']\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * C :> Defines Strength of regularization ( smaller values specify stronger regularization )\n> * penalty :> Used to specify the norm used in the penalization\n> * solver :> Algorithm to use in the optimization problem. Different Solver supports different penalty. Hence we cane 3 cases here\n> * l1_ratio :> Combination of L1 and L2(l1_ratio=0 means l2 and l1_ratio=1 means l1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_l1_gridcv = LogisticRegression(penalty = 'l1')\nlogreg_with_l1_gridcv = GridSearchCV(estimator=logreg_with_l1_gridcv, param_grid=params_for_l1, cv= 5)\nlogreg_with_l1_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_l1_gridcv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_l1 = LogisticRegression(penalty = 'l1', C = 1, solver = 'liblinear')\nlogreg_with_l1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_l2_gridcv = LogisticRegression(penalty = 'l2')\nlogreg_with_l2_gridcv = GridSearchCV(estimator=logreg_with_l2_gridcv, param_grid=params_for_l2, cv= 5)\nlogreg_with_l2_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_l2_gridcv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_l2_gridcv = LogisticRegression(penalty = 'l2', C = 1, solver = 'newton-cg')\nlogreg_with_l2_gridcv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_elasticnet_gridcv = LogisticRegression(penalty = 'elasticnet')\nlogreg_with_elasticnet_gridcv = GridSearchCV(estimator=logreg_with_elasticnet_gridcv, param_grid=params_for_elasticnet, cv= 5)\nlogreg_with_elasticnet_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_elasticnet_gridcv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_elasticnet_gridcv = LogisticRegression(penalty = 'elasticnet', C = 1, solver = 'saga', l1_ratio = 0.5)\nlogreg_with_elasticnet_gridcv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\n\nTime to evaluate to find the best model for our use-case\n\n<img  src=\"https://im.indiatimes.in/content/2018/Mar/_1521531221.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We'll be using :\n> * cross_val_score :> Evaluate a Score by Cross-Validation\n> * classification_report :> Text Report showing the Main Classification Metrics. We'll gonna draw Confussion Matrix too","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you are wondering what's Cross - Validation is the it's :\n\n<img  src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nBasically, dataset is divided in k equal parts. Then one of that part is used as test part and other for training. <br>\nAnd this step is repeated until every K-fold serve as the test set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## But Why Not Evaluate on Test Set from train_test_split ?\n\n> * In train_test_split, we get our accuracy according to the split of the data\n> * If we change re-split in different order, then we'll get another new accuracy\n\nWait wait ! Let me give you an example ..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Right Now, We have a split with random state as 43 <br>\nLet's take KNN for our example","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, Let's change our random_state as 0 and notice the difference..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1, X_test1, y_train1, y_test1 = train_test_split(standard_X,Y,test_size = 0.2,random_state=0, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn1 = KNeighborsClassifier(n_neighbors = 22, p=2, weights = 'uniform') \nknn1.fit(X_train1, y_train1)\nknn1.score(X_test1, y_test1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img  src=\"http://www.quickmeme.com/img/a6/a6ec1cbce3a2b0f01dcdc59ad5f3d5fa7c03c817bcebc49dc570ec6e5dad48a2.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Evaluation for KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(knn, X_train, y_train, cv=5)\nprint('KNN Model gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = knn.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation for Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(rfc, X_train, y_train, cv=5)\nprint('Random Forest Model gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = rfc.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation for Logistic Model with L1 Penalty","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(logreg_with_l1, X_train, y_train, cv=5)\nprint('Logistic Model with L1 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = logreg_with_l1.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation for Logistic Model with L2 Penalty","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(logreg_with_l2_gridcv, X_train, y_train, cv=5)\nprint('Logistic Model with L2 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = logreg_with_l2_gridcv.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation for Logistic Model with Elasticnet Penalty","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(logreg_with_elasticnet_gridcv, X_train, y_train, cv=5)\nprint('Logistic Model with L2 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = logreg_with_elasticnet_gridcv.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n> * In Disease Detection We can't jeopardy with that person's life by predicting No Heart Disease to a patient with Heart Disease. \n> * We need to have a model with better accuracy and low False Negative Cases\n> * We trained KNN, RandomForest and Logistic Regression with different penality with Logistic Regression having slight advantage\n## With a Better Accuracy and Lower False Negative Case, Logistic Regression is our clear Winner\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# End Notes\n> * With this We end another analysis on Heart Disease related Dataset. \n> * Feel free to check out my [Previous Kernel](https://www.kaggle.com/rahulgulia/data-science-and-cardiovascular-diseases-cvds) based on similar kind of Dataset\n\n> * It was fun to analyise this data. Seems I'm starting loving Medical Datasets. There was alot of learning opportunity and yea came accross to quite \"Data Science Friendly Memes\" :D  <br>\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## If You like these types of Kernals (DataScience + Memes) then don't forget to Upvote this Kernel. It'll boost my morale, encouraging me to create more similar kinds of Kernels.  <br>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## And Yeah, I'm always open for learning. If you want to correct something, advice, wanna share new strageties or techniques then feel free to comment them out. I'll love to hear some great advices / feedback from the community.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}