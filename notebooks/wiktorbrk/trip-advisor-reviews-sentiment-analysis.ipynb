{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Simple sentiment analisys model**"},{"metadata":{},"cell_type":"markdown","source":"# Model accuracy: \n# - with parsing to sentiment ~82-85%\n# - without ~60-63%"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nfrom keras import layers\nfrom keras_preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras import Sequential\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Set random state for repeatable data\nnp.random.RandomState(21)\n\n# Set number of training samples, epochs and num words for tokenizer\ntraining_samples = 18000\nepochs = 5\nnumWords = 10000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data to csv\ndf = pd.read_csv(r'../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')\n# Lowercase columns\ndf.columns = df.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data info**"},{"metadata":{},"cell_type":"markdown","source":"***First 5 rows***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Total examples***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Ratings range***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Ratings: {sorted(df.rating.unique())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Rating counts*"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.rating.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace 1-5 rating to 0-2 sentiment where 0 is bad, 1 is neutral and 2 is good\ndef parseToSentiment(x):\n    if x == 5 or x == 4:\n        x = 2\n        return x\n    elif x == 3:\n        x = 1\n        return x\n    else:\n        x = 0\n        return x\n\n\ntoSentimentMap = map(parseToSentiment, df.rating)\n\n# Replace rating column\ndf.rating = list(toSentimentMap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set columns for data and labels\ndata = df.review\nlabels = df.rating","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Ratings range after replacing with sentiment***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Ratings: {sorted(df.rating.unique())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tokenizing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use tokenizer on text date for vectorizing it to numbers\ntokenizer = Tokenizer(num_words=numWords)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\ndata = pad_sequences(sequences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Data after tokenizing***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert rating column to array and then with to_categorical convert it to binary class matrix\nlabels = np.asarray(labels)\nlabelsCategories = len(np.unique(labels))\nlabels = to_categorical(labels, labelsCategories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle data\nindices = np.arange(len(data))\nnp.random.shuffle(indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply shuffle on the data\ndata = data[indices]\nlabels = labels[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data to train and test\nX_train = data[:training_samples]\ny_train = labels[:training_samples]\nX_test = data[training_samples:]\ny_test = labels[training_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embedding layer expects vocabulary size + 1 as input dimension\ninputDim = numWords + 1\n# For input length we need to use shape of one row of our data\ninputLength = len(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Sequential model with one Embedding and two Convolutional layers and after them use 2 max pooling operations\n# ended with one Dense layer with softmax activation for multiclass classification\nmodel = Sequential()\nmodel.add(layers.Embedding(inputDim, 128, input_length=inputLength))\nmodel.add(layers.Conv1D(64, 7, activation='relu'))\nmodel.add(layers.MaxPool1D(5))\nmodel.add(layers.Conv1D(64, 7, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(3, activation=\"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For loss in multiclass classification problem we need to use categorical_crossentropy, as optimizer set rmsprop\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training model\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs=epochs,\n                    batch_size=128,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Evaluation and plotting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evalute model for accuracy and loss information\nprint(f'Model loss(1) and accuracy(2): {model.evaluate(X_test, y_test)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save metrics to variables and plot them\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting with Plotly\n\nepochs = range(1, epochs + 1)\n\nfig = make_subplots(rows=2, cols=1, subplot_titles=('Loss', 'Accuracy'))\n\nfig.add_trace(\n    go.Scatter(x=list(epochs), y=loss, mode='lines+markers', name='Training loss'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=list(epochs), y=val_loss, mode='lines+markers', name='Validation loss'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=list(epochs), y=acc, mode='lines+markers', name='Training accuracy'),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=list(epochs), y=val_acc, mode='lines+markers', name='Validation accuracy'),\n    row=2, col=1\n)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}