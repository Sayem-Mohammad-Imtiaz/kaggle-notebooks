{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d431d3e1-f415-e66c-397c-36778f5991f4"},"source":"## Breast Cancer Wisconsin Data Set\n\nBreast Cancer Wisconsin data set:\nhttps://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n\nAttribute Information:\n\n1) ID number \n2) Diagnosis (M = malignant, B = benign)  \n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter) \nb) texture (standard deviation of gray-scale values) \nc) perimeter \nd) area \ne) smoothness (local variation in radius lengths) \nf) compactness (perimeter^2 / area - 1.0) \ng) concavity (severity of concave portions of the contour) \nh) concave points (number of concave portions of the contour) \ni) symmetry \nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7fe0e0f-3fd0-1680-38ab-e9dce3a83984"},"source":"## Import Libraries"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62326b80-7cd8-983e-47c1-3671894732c8"},"outputs":[],"source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import sem"},{"cell_type":"markdown","metadata":{"_cell_guid":"42f59285-a8df-29ab-8d62-0a186f0c7082"},"source":"## Import and Interrrogate Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57b31180-1bd9-0366-edb5-c783328dafd3"},"outputs":[],"source":"data =  pd.read_csv(\"../input/data.csv\",header = 0)\n\n# list columns\nprint(data.columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d2ddfa89-325a-174a-0de7-7f39c196331a"},"source":"We see that we have an 'Unnamed' column that appears to blank so we shall remove it. We also remove the id field in our data prep. We shall convert to numpy arrays"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80463450-c96a-4882-bba4-8dab01df8922"},"outputs":[],"source":"# Create Target\n\ny = np.array(data.diagnosis)\nlabels = LabelEncoder()\ntarget = labels.fit_transform(y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8954266e-2413-962b-9ef8-d090c5a9c5aa"},"outputs":[],"source":"# Create features normalise the data\n\ncols = data.columns[(data.columns != 'id') & (data.columns != 'diagnosis') & (data.columns != 'Unnamed: 32')]\nfeatures = data[cols]\nfeatures = (features - features.mean()) / (features.std())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45f59e7c-bd78-c198-c91f-86ce0e570302"},"outputs":[],"source":"# Look at correlation between features using seaborn\n\ncorr = features.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(25, 20))\n    sns.heatmap(corr, mask=mask, vmax=1, square=True, annot=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c501c25-7b1f-96e4-da54-da66c2da63d1"},"source":"As expected perimeter mean and area mean are very highly correlated to radius mean we shall remove them. Further we notice that radius worst, perimiter worst and area worst are also highly correlated so again we shall remove these also. Following this we shall take another look at our correlation matrix to ensure we have not missed any additional significant correlations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"385f0443-d676-239a-f0a1-a7c754fbb5cf"},"outputs":[],"source":"features = features.drop(labels=['perimeter_mean','area_mean','radius_worst','perimeter_worst', 'area_worst'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec0715bb-bf92-8ad1-cece-3758142a4c35"},"outputs":[],"source":"# Look at correlation between features using seaborn\n\ncorr = features.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(25, 20))\n    sns.heatmap(corr, mask=mask, vmax=1, square=True, annot=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b242f961-76cf-3822-5bff-9e046a8dce20"},"source":"## SVC for Cancer Diagnosis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f1bd65a-83aa-e67d-bedb-43ee821bf28b"},"outputs":[],"source":"# Split our data into training and test data\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=123)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fbb5aec-35b9-fb66-c265-448085fe124e"},"outputs":[],"source":"# Use grid search to tune hyperparameters for our SVC on training data\n\nsvc = svm.SVC()\nparameters = [{'kernel':['linear'], 'C': np.logspace(-2, 2, 25)},\n             {'kernel':['poly'], 'C': np.logspace(-2,2,25), 'gamma': np.logspace(-4,0,25), 'degree': [2, 3, 4, 5, 6, 7]},\n             {'kernel':['rbf', 'sigmoid'], 'C': np.logspace(-2, 2, 25), 'gamma': np.logspace(-4,0,25)}]\nclf = GridSearchCV(svc, parameters)\nclf.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d38d8d1f-fa56-a81c-2633-643271a96401"},"outputs":[],"source":"# Print out hyperparameter selections and set-up new classifier with these\n\nprint('Best Parameters:', clf.best_params_)\nclf = svm.SVC(**clf.best_params_)\nclf.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1764aef0-0554-6ef0-e4e2-d3e5b2cc4f58"},"outputs":[],"source":"# Test our calibrated model accuracy on our test data\n\nprint('Accuracy on training data:')\nprint(clf.score(X_train, y_train))\nprint('Accuracy on test data:')\nprint(clf.score(X_test, y_test))\n\ny_pred = clf.predict(X_test)\n\nprint('Classification report:')\nprint(metrics.classification_report(y_test, y_pred))\nprint('Confusion matrix:')\nprint(metrics.confusion_matrix(y_test, y_pred))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ffe1bc9-02f9-01df-a29a-be0c2f0e57e5"},"outputs":[],"source":"# Cross validation of support vector classifier\n\ncv = KFold(5, shuffle=True, random_state=123)\nscores = cross_val_score(clf, features, target, cv=cv)\nprint(scores)\nprint(\"Mean score: {0:.3f} (+/-{1:.3f})\".format(np.mean(scores), sem(scores)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"c24cdfb0-6dee-8b0b-03ea-a5f5c5429e3a"},"source":"## Observations on SVC:\nThe accuracy on test data is fairly high and there does not appear to be significant overfitting to the training data."},{"cell_type":"markdown","metadata":{"_cell_guid":"e574df3a-aede-6433-2f55-54d9fab94882"},"source":"## Finding Feature Importance Using a Random Forest"},{"cell_type":"markdown","metadata":{"_cell_guid":"11a981ef-9b15-ec44-09c1-948dafdec8f2"},"source":"One 'problem' with the support vector machine classifier above is it is fairly 'black box', we shall now use a random forest classifier in order to deduce feature importance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9aecc4f7-09eb-87ab-2407-9dce1790b50a"},"outputs":[],"source":"clf_rf = RandomForestClassifier(n_estimators=2500, max_features=None, criterion='entropy')\nclf_rf.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b340f32-2f0e-b9fe-90e3-e2261568dfa0"},"outputs":[],"source":"# Test our calibrated model accuracy on our test data\n\nprint('Accuracy on training data:')\nprint(clf_rf.score(X_train, y_train))\nprint('Accuracy on test data:')\nprint(clf_rf.score(X_test, y_test))\n\ny_pred = clf_rf.predict(X_test)\n\nprint('Classification report:')\nprint(metrics.classification_report(y_test, y_pred))\nprint('Confusion matrix:')\nprint(metrics.confusion_matrix(y_test, y_pred))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb1343b4-ed14-86df-d838-0e6ef046cd24"},"outputs":[],"source":"# Cross validation of random forest\n\ncv = KFold(5, shuffle=True, random_state=123)\nscores = cross_val_score(clf_rf, X_train, y_train, cv=cv)\nprint(scores)\nprint(\"Mean score: {0:.3f} (+/-{1:.3f})\".format(np.mean(scores), sem(scores)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ae4f189e-4d49-f56a-8182-c131c73499cd"},"source":"Showing slightly weaker performance than the support vector machine method, but only intending to use this for feature importance so for this purpose it is fine."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"949cf321-77f7-3387-3753-626474582784"},"outputs":[],"source":"importances = clf_rf.feature_importances_\nimportances = pd.DataFrame(importances, index=features.columns, columns=[\"Importance\"])\nimportances = importances.sort_values(by=['Importance'],ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27c07f17-ee64-4a1b-5aff-bffffee4d7a7"},"outputs":[],"source":"importances.plot(kind='bar')"},{"cell_type":"markdown","metadata":{"_cell_guid":"ecf4337f-62c9-8a70-6af2-1a15e81f5781"},"source":"From this analysis we can see that the concave points (mean and worst) are the most important features followed by radius_mean and texture_worst. "},{"cell_type":"markdown","metadata":{"_cell_guid":"45d46a5f-5bbc-e3ea-eac0-8520b5fa6155"},"source":"## Conclusions\nFrom the analysis above we can deduce the following:\n- The support vector machine classifier has a test data accuracy of 98.9% and a mean cross validation of 97.9%. There does not appear to be significant over-fitting. However the confusion matrix suggests that we have some false negatives (that is: we classify malignant examples as benign.) In a practical situation this would be a problem and we would wish to reduce this if possible.\n- From the random forest classifier we deduce that the most important features are: concave points_mean, concave points_worst, radius_mean and texture_worst."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7383dadc-81bb-dcdc-a472-9d82c771be5a","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}