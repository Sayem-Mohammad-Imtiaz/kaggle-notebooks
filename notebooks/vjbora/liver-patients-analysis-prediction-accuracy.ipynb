{"cells":[{"metadata":{"_cell_guid":"94e442bb-e3e6-4afb-9b37-5bc478db5f4f","_uuid":"51898d40216d04490c572d5ac7598091b73d3615"},"cell_type":"markdown","source":"# Analysis and Prediction-Indian Liver Patients."},{"metadata":{"_cell_guid":"77c7f1f3-9467-49d9-a113-b3b345b08110","_uuid":"c8034ad4f2e2bb5d1c932c32232753c82a6b0e33"},"cell_type":"markdown","source":"This dataset was downloaded from the UCI ML Repository. The objective of this study is not to maximize the accuracy or the ROC of the predictor, but to explore the different models and see if we can help doctors improve liver diagnosis.\n\nI would like to acknowledge Sangeetha James, whose script I forked for the work.\n\nThe dataset is relatively small (583 samples), so it is hard to draw conclusions based on it. However, there are still many things we can learn from it.\n\nThe work is primarily divided into two parts - in the first part, the dataset was loaded, cleaned and visualized. During the analysis, I came across biases in the dataset, such as more than 70% of the patients in the sample set have liver disease and  men in this sample set are more likely to have liver disease than women.\n\nIn the second part, the complete dataset was divided into training data (80%) and test data (20%). In various ML algorithms, cases were identified where data was overfit or underfit, and remedial actions were taken. In all the cases, the test score accuracies were all around 70%. As a simple estimator, which ignores all the inputs and always predicts that the person has liver disease will achieve a 71.35%, this performance is pretty bad.\n\nThe first published result on this dataset by [Ramana et. al](https://pdfs.semanticscholar.org/c92d/38a7a76c20a317de63fb9278bb10102c758b.pdf) achieves 90+ percent accuracy. I was intrigued by the large difference in scores. After a second re-read of the paper, I identified that they used 90% of their data as a training data and 10% as the test data. Their approach give more data for training, which helps make their model better, but it is risky to test the model against such a smaller amount data.\n\nWith 90% training data, the results improved marginally, but I could not achieve 90%. Another inconsequential difference in the dataset used by the paper and the UCI dataset at Kaggle is that the UCI dataset has removed columns which can be calculated from the other columns. The dataset used in the paper had columns for Albumin, Globulin and Albumin and Globulin Ratio, while the Kaggle dataset has gotten rid of Globulin column.\n\n\nThis is a work in progress and I hope to come back and replicate the 90+% accuracy, but for now, my conclusion is that we need more data to produce a better model. And secondly, the doctors should continue to use their current approach to diagnosis which was used to populate the \"Dataset\" (probably a biopsy, or ultrasound / MRI) which best shows the presence or absence of liver disease.\n"},{"metadata":{"_cell_guid":"ead33c46-262f-428c-a300-5372b4f15a7a","_uuid":"2cfb47f6cfc64578a9bff30d0e5ec6ab95a8e06b","trusted":false,"collapsed":true},"cell_type":"code","source":"#Import all required libraries for reading data, analysing and visualizing data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"d58aeb11-2795-4ba6-8dd1-f48160dec59c","_uuid":"3b99a454b4a233394a3fa3c8a5190b198cbc84ba"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_cell_guid":"518b7cef-cafc-433e-94e5-ed91f9b70ba6","_uuid":"104d9fe12aae79b3eff8c1ad92e432a77a2b92bc","trusted":false,"collapsed":true},"cell_type":"code","source":"#Read the training & test data\nliver_df = pd.read_csv('../input/indian_liver_patient.csv')","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"0e641f71-2703-4ec3-84f8-fdf5a6f27e64","_uuid":"02b022e1a75c30d06c5b88723711d0589775a1ed"},"cell_type":"markdown","source":"This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The \"Dataset\" column is a class label used to divide groups into liver patient (1=liver disease) or not (2=no disease). "},{"metadata":{"_cell_guid":"04286350-1e1c-46ac-9099-d29585d30287","_uuid":"897e4e62d506ee354a42d13005f604da99f12e59","trusted":false},"cell_type":"code","source":"liver_df.head()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"9a45eba7-cb42-4ac8-bd56-e75fee941808","_uuid":"3e60087458159fe381e86febe8f9bca1bd097ff4","trusted":false},"cell_type":"code","source":"liver_df.info()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"aa1480e6-27f0-4073-83c9-9901d677b615","_uuid":"3bc3e850db0d915e15a149433a4a3600ecc59783"},"cell_type":"markdown","source":"Here is the observation from the dataset:   \n* Only gender is a non-numeric variable. All others are numeric.\n* There are 10 features and 1 output - dataset. Value 1 indicates that the patient has liver disease and 2 indicates the patient does not have liver disease.  \n* The columns names are shortened from the original dataset at UCI. \n\nThe columns and their description reproduced from the UCI site are below.\n1. Age - Age of the patient \n2. Gender -\tGender of the patient \n3. TB -\tTotal Bilirubin \n4. DB -\tDirect Bilirubin \n5. Alkphos - Alkaline Phosphotase \n6. Sgpt - Alamine Aminotransferase \n7. Sgot - Aspartate Aminotransferase \n8. TP -\tTotal Protiens \n9. ALB - Albumin \n10. A/G Ratio - Albumin and Globulin Ratio \n11. Selector - field used to split the data into two sets (labeled by the experts) \n"},{"metadata":{"_cell_guid":"7336699b-7ef0-4ee9-9e3d-1f7741ef3692","_uuid":"f98b0529723f1b862688a5738c7afce79365e352","trusted":false},"cell_type":"code","source":"#Describe gives statistical information about NUMERICAL columns in the dataset\nliver_df.describe(include='all')\n#We can see that there are missing values for Albumin_and_Globulin_Ratio as only 579 entries have valid values indicating 4 missing values.\n#Gender has only 2 values - Male/Female","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"e379fc6e-e467-47bd-978c-c8eae32f3193","_uuid":"a371d92f66a0641259d6053f2aaf33e3cfb64994","trusted":false},"cell_type":"code","source":"#Which features are available in the dataset?\nliver_df.columns","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"b6d201e0-a049-462a-a4c3-e66fdd3c1ff1","_uuid":"0ab1acc33cd91286825dba1d5d77fcbf171348d5","trusted":false},"cell_type":"code","source":"#Check for any null values\nliver_df.isnull().sum()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"1d6972b0-3916-4ec4-9c4e-050b4be86f0d","_uuid":"5d2737231e0a088725b3403778a48e9c0ba2c629"},"cell_type":"markdown","source":"The only data that is null is the Albumin_and_Globulin_Ratio - Only 4 rows are null. Lets see whether this is an important feature    "},{"metadata":{"_cell_guid":"888444b8-b4a9-45b6-b2c2-11b826ff9da7","_uuid":"2d518d8a9f73f3ef58ebef6cd7c918a15178a656"},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"_cell_guid":"544cd4dc-5cb5-45ac-ba26-c6c2be5d3db1","_uuid":"0f20a9162e6c41a4f602f2bbcdfd21c012e035da","trusted":false},"cell_type":"code","source":"sns.countplot(data=liver_df, x = 'Dataset', label='Count')\n\nLD, NLD = liver_df['Dataset'].value_counts()\nprint('Percentage of patients diagnosed with liver disease: ',LD / (LD+NLD) * 100)\n","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"f7e6bc16f801537fb74b5994f54ae8b1ba2332fb"},"cell_type":"markdown","source":"### Subjects in this study are more likely to have liver patients than the general public\nOut of the 583 subjects, 71.35% (416) of cases had liver disease and 28.64% (167) did not. In the general population, we do not expect to see two out of three people with liver disease. This points to a bias in the data collection, in which subjects who were more likely to have issues with the liver were more likely to be picked. In a larger study with 633,323 subjects, approximately 0.27% were found to have liver cirrhosis [1]. \n\nFor example in this dataset, if a predictor always predicts that the subject has liver disease, he will be correct about 70% of the time, but in the real world, this predictor would be 0.27% correct.\n\nHowever, this group of subjects might better reflect the distribution of people who visited a Hepatologist (liver specialist).\n\n[1] https://www.ncbi.nlm.nih.gov/pubmed/25291348\n"},{"metadata":{"_cell_guid":"3e8bf602-3b81-4a53-b190-8ba363295904","_uuid":"d0e41e3d9a88577572fe5fea081de4de19f6ad6a","trusted":false},"cell_type":"code","source":"sns.countplot(data=liver_df, x = 'Gender', label='Count')\n\nM, F = liver_df['Gender'].value_counts()\nmalesWithLiverDisease = liver_df[(liver_df['Gender'] == 'Male') & (liver_df['Dataset'] == 1)]['Age'].count()\nfemalesWithLiverDisease = liver_df[(liver_df['Gender'] == 'Female') & (liver_df['Dataset'] == 1)]['Age'].count()\npatientsWithLiverDisease = liver_df[liver_df['Dataset'] == 1]['Age'].count()\ntotalPatients = liver_df['Age'].count()\nprint('Percent of patients that have liver disease: ',patientsWithLiverDisease /totalPatients * 100)\nprint('Percent of male patients that have liver disease: ',malesWithLiverDisease /M * 100)\nprint('Percent of female patients that have liver disease: ',femalesWithLiverDisease /F * 100)\n\nprint('Number of patients that are male: ',M)\nprint('Number of patients that are female: ',F)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"44ea7af07a9e7238e7ec5105d8e9d9de3f304f0d"},"cell_type":"markdown","source":"\n### Women are less likely to have liver disease. Is this real or biased data?\nThe fraction of women with liver disease (64.78%) was lower than the fraction of men with liver disease (73.46). This is contrary to studies that women are more susceptible  to liver disease [2]. The higher prevalence of liver disease among men in this dataset could be due to high rates of alcohol consumption among men in India (where women drinking alcohol was a social taboo). This might not be true in western countries.\n\nIf our aim is to predict the presence of liver disease outside of India, to avoid bias (as our model is likely to guess that being a woman makes having liver disease less likely instead of more), we should disregard the gender in this study. On the other hand, including gender will help improve results for a predictive task in India.\n\n[2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4388670/"},{"metadata":{"_cell_guid":"d1f71ba8-caa8-47e2-b675-e234abfab05c","_uuid":"059e213bb14b66c18d67a022679d485e00003e25"},"cell_type":"markdown","source":"Age seems to be a factor for liver disease for both male and female genders"},{"metadata":{"trusted":false,"_uuid":"7687e21483858a6fdbb1d6784d28c3a807fc22c7"},"cell_type":"code","source":"liver_df[['Dataset','Age']].groupby(['Dataset']).mean()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"c0dae30ce87d91713dbcb5fbde54dfaf18b8afa0"},"cell_type":"markdown","source":"### Age\nThe mean age of patients with liver disease is about five years more than the patients without the liver disease. The older patients seem to be at a higher risk of having liver disease. It will be interesting to see if there are any correlations between age and other tests."},{"metadata":{"_uuid":"ef5938d06149a5e5b5a35b92b90eff4982b9ed7f"},"cell_type":"markdown","source":"### Look at all the data together (Why it is not easy to diagnose)\nThe blue box plots are the patients with liver disease and the green box plots are the patients who do not have liver disease. \n\nAs you can see, for all the different measurements, there is a significant overlap in values between people with liver disease and people without liver disease. Therefore, no single metric can be used directly for diagnosis. \n\nFor instance, 5% of US population has Gilbert's syndrome - a genetic condition which results in an elevated Bilirubin in blood. In people with Gilbert's syndrome, a higher bilirubin does not indicate liver disease, but combining it with other metrics like Alkaline_Phosphotase or Total_Proteins might help the estimator. The ML models will try and see if we can use all the values together to better predict the occurrence of liver disease. \n\nThe presence of outliers in patients with liver disease and without liver disease further complicates the diagnosis. "},{"metadata":{"trusted":false,"_uuid":"9d50b3a4d7b5c7ba7a5c28fddd8d9a816f297134"},"cell_type":"code","source":"#liverDisease_df = liver_df[liver_df['Dataset'] == 1]\n#liverDisease_df.drop(['Gender', 'Dataset'], axis=1).boxplot()\n#nonLiverDisease_df = liver_df[liver_df['Dataset'] == 2]\n#nonLiverDisease_df.drop(['Gender', 'Dataset'], axis=1).boxplot()\nfig=plt.figure(figsize=(20, 24), dpi= 80, facecolor='w', edgecolor='k')\n\nax = liver_df.drop(['Gender'], axis='columns').set_index('Dataset', append=True).stack().to_frame().reset_index().rename(columns={'level_2':'quantity', 0:'value'}).pipe((sns.boxplot,'data'), x='quantity', y='value', hue='Dataset')\nax.set(ylim=(0,500))","execution_count":11,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a149c6472e7b20d050338de8734914899377e2cb"},"cell_type":"code","source":"# Correlation\nliver_corr = liver_df.corr()\nliver_corr","execution_count":12,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"1fb2f94a8c6da0846d7469641145b6fe500d0c6f"},"cell_type":"code","source":"plt.figure(figsize=(15, 15))\nsns.heatmap(liver_corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n           cmap= 'coolwarm')\nplt.title('Correlation between features');","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"79d7d365-9eab-453e-adc1-97b02461c8a1","_uuid":"730a2d0a33cdb3db4ec8e783dbc9e03470af1f4f"},"cell_type":"markdown","source":"## Observation:\nThe \"Dataset\" column which indicates the presence or absence of liver disease does not strongly correlate with one of the measured metrics. There are strong positive correlations between pairs of measured quantities (Total_Proteins & Albumin, Total_Proteins & Albumin, Alamine_Aminotransferase & Aspartate_Aminotransferase, Direct_Bilirubin & Total_Bilirubin. Without more insight into the actual meaning of these quantities, it is possible these measurements are similar in nature and we might be able to reduce the dimensionality of this problem using Principle Component Analysis (PCA)."},{"metadata":{"_cell_guid":"824ec774-42ef-4fad-849f-82893dd03849","_uuid":"5b9a21eded807cc989db8eab46435ee77bc74e50"},"cell_type":"markdown","source":"## Clean up data\n\nThe dataset is pretty clean, except for a few NA's in the Albumin_and_Globulin_Ratio. There is a column for Albumin, but not one for Globulin. So we do not have any information to fill in the values. We can ignore the four rows which have this column, or replace them with the mean or median. Because of the noisy data, I chose to use the median value to fill in the missing values.\n\nLets prepare the input and output data frames.\n"},{"metadata":{"_cell_guid":"af447fa3-486a-4c9f-a0bb-143d240a8b41","_uuid":"6b2c57a7dc86c5a48f4749fb0ff55f76bf1ce2c1","trusted":false},"cell_type":"code","source":"liver_df[\"Albumin_and_Globulin_Ratio\"] = liver_df.Albumin_and_Globulin_Ratio.fillna(liver_df['Albumin_and_Globulin_Ratio'].median())\n\nX = liver_df.drop(['Gender', 'Dataset'], axis='columns')\nX.head()","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"c6011e12-8e58-4e38-aa82-33bd336723ce","_uuid":"6b834a47790a238c390fc7b552587c6fc6bcb24d","trusted":false},"cell_type":"code","source":"X.describe()","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"134edf24-b704-483f-91f6-a000aa7525a5","_uuid":"13e14244f1f54357aba978576a2d1a9515dece6e","trusted":false,"collapsed":true},"cell_type":"code","source":"y = liver_df['Dataset'] # 1 for liver disease; 2 for no liver disease","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"d452f2ac-60ed-4330-98a6-70db275af530","_uuid":"458d0033ebbc367888af6082573eef6b7e27e2c9"},"cell_type":"markdown","source":"# Machine Learning\n\nI tried Logistic Regression, Gaussian Naive Bayes, Decision Trees, Random forest, SVM, and Nearest neighbor algorithms. If we have a biased estimator, which always predicts that the patient has liver disease, it will have a 71.35% sucess rate. So our estimator to be of any use must do substantially better than that."},{"metadata":{"_cell_guid":"ade84f4a-7907-4ff1-a20d-7493ac06c16b","_uuid":"bc317a65e95970b06de98f20e046e1845cb72f75","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)","execution_count":17,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c83fcb2bf64ddf0af8d91cc2239039a8f45905a5"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import average_precision_score\n\ndef processResults(clk, str=\"\", X_test = X_test, X_train = X_train, y_test = y_test, y_train = y_train):\n    predicted = clk.predict(X_test)\n    score = round(clk.score(X_train, y_train) * 100, 2)\n    score_test = round(clk.score(X_test, y_test) * 100, 2)\n\n    print(str + 'Training score: \\n', score)\n    print(str + 'Test Score: \\n', score_test)\n    print('Accuracy: \\n', accuracy_score(y_test,predicted))\n    print(confusion_matrix(y_test,predicted))\n    print(classification_report(y_test,predicted))\n    sns.heatmap(confusion_matrix(y_test,predicted),annot=True,fmt=\"d\")\n    return score, score_test\n    ","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"9eb1fd627a12655b2dc4e8c3771456ac922592ec"},"cell_type":"markdown","source":"## Logistic Regression observation\nThe model has an accuracy of 71.43%. The training and testing scores are very close to each other at 71.24% and 76.07% respectively.\n\nA simple way to check the performance of a classifier is to compare its accuracy to a biased estimator. If we had a biased estimator, which, independent of all the metrics, always predicted that the patient has liver disease, for the complete dataset we would have an accuracy of 71.35%.\n\nThe training and testing accuracy percentages being so close to each other indicate that the model has high bias and is underfitting the data. To get a better fit, we should increase the number of parameters that we fit to the data or use a different model."},{"metadata":{"_cell_guid":"6a33cbbf-4c26-4509-aca6-7d8160a68fa7","_uuid":"79462fc8649ec0e0916e874fc6d8766ef6f3b31a","scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n#2) Logistic Regression\n# Create logistic regression object\nlogreg = LogisticRegression()\n# Train the model using the training sets and check score\nlogreg.fit(X_train, y_train)\n\nlogreg_score, logreg_score_test = processResults(logreg, \"Logistic Regression \")\n\nprint('Coefficient: \\n', logreg.coef_)\nprint('Intercept: \\n', logreg.intercept_)\n","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"aa14ef2956cb44de27a37c76efdae754cfb624a8"},"cell_type":"markdown","source":"## Naive Bayesian\nThe Naive Bayesian estimator fares even worse than just guessing that liver disease is present! Moving on to decision trees and random forest."},{"metadata":{"_cell_guid":"133bdeb1-d6d9-4eed-8c95-88a4fa237d84","_uuid":"da565cb49c73098981463b6dd5e941cddd2cab9b","trusted":false},"cell_type":"code","source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ngauss_score, gauss_score_test = processResults(gaussian, \"Gaussian Naive Bayesian \")","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"30c71a39e7054ffccd0a0a1aff707bea8c6fd76c"},"cell_type":"markdown","source":"## Decision tree\nThe decision tree estimator achieves 100% in the training set, while only 70.09% in the test. This indicates that we have overfit our training data. We can alleviate this by finding how deep the decision tree is and prune it until we fix the issue. So, we plotted how the training and test data accuracies vary with depth ( See figure below). The ideal point when we fit everything correctly is when the training and the test curves cross each other. But in this case, that seems to happen very close to the 2."},{"metadata":{"trusted":false,"_uuid":"cc087d27f8c270c3272cf3b74bfb9ed017185cda"},"cell_type":"code","source":"# Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(random_state=42)\ndecision_tree.fit(X_train, y_train)\ndecision_tree_score, decision_tree_score_test  = processResults(decision_tree, \"Decision Tree \")\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d6b335ad303d5ca7e6ecc0687d5d961c595abbae"},"cell_type":"code","source":"# As max depth is 14, let's reduce it, until the test and train data scores are close to each other\ndef plotDecisionTreeScoresVSDepth(clk, maxDepth = 14, X_test = X_test, X_train = X_train, y_test = y_test, y_train = y_train):\n    score = []\n    score_test = []\n    allDepth = np.arange(maxDepth,1,-1)\n    for depth in allDepth:\n        clk.set_params(**{'random_state': 42, 'max_depth' : depth})\n        clk.fit(X_train, y_train)\n        \n        predicted = clk.predict(X_test)\n        score.append(round(clk.score(X_train, y_train) * 100, 2))\n        score_test.append(round(clk.score(X_test, y_test) * 100, 2))\n    plt.plot(allDepth, score)    \n    plt.plot(allDepth, score_test)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Max depth of decision tree')\n    plt.legend(['Train accuracy', 'Test accuracy'])\n    plt.show()\n\ndecision_tree = DecisionTreeClassifier()        \nplotDecisionTreeScoresVSDepth(decision_tree)","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"3961a5f8137aa36ff7375f6d23e09ec1f1344f0c"},"cell_type":"markdown","source":"## Random forest\nThe random forest is an ensemble of decision trees. Like decision trees, it too achieves 100% in the training set, while only 70.86% in the test. This indicates that we have overfit our training data. We can alleviate this by finding how deep the decision tree is and prune it until we fix the issue. The plot of the training and test data accuracies vs with depth ( See figure above). The ideal point when we fit everything correctly is when the training and the test curves cross each other. But in this case, that seems to happen very close to the 2"},{"metadata":{"_cell_guid":"3ebf85d5-2867-49a8-821d-4566f7d066a0","_uuid":"1613eb76e677ff43f9253c41775f0064a6162b1d","trusted":false},"cell_type":"code","source":"# Random Forest\n# The random forest classifier uses number of decision trees and combines the results to make a prediction\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state=42)\nrandom_forest.fit(X_train, y_train)\n\nrandom_forest_score, random_forest_score_test  = processResults(random_forest, \"Random Forest \")","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"ba25358ae6cb3c6791e8caecb6170e8bc611953f"},"cell_type":"markdown","source":"The random forest estimator achieves 100% in the training set, while only 72.65% in the test. This indicates that we have overfit our training data. So, lets plot the same "},{"metadata":{"_uuid":"54048198d59b31266b29feb08997a71b69e49a52"},"cell_type":"markdown","source":"## Support vector machines\nThe Support vector classification is a highly biased "},{"metadata":{"trusted":false,"_uuid":"a9ef40dde05e4c50c086da10befb9c25f7d4e26e"},"cell_type":"code","source":"randomForest = RandomForestClassifier(n_estimators=100)        \nplotDecisionTreeScoresVSDepth(randomForest)","execution_count":24,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"28e27b74cc199dd5332aecf75ba3001a5895a000"},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n    ('scale', StandardScaler()),    \n    ('pca', PCA(n_components=8)),\n    ('svc', SVC()),\n])\npipe.fit(X_train, y_train)\nsvcScore, svcScore_test = processResults(pipe)","execution_count":25,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3c0be6e5d6e64aee9dd83d012574822063cc4e61"},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n    ('scale', StandardScaler()),    \n    ('pca', PCA(n_components=8)),\n    ('svc', SVC()),\n])\npipe.set_params(svc__C=2)\npipe.fit(X_train, y_train)\nsvcScore, svcScore_test = processResults(pipe)","execution_count":26,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"37c803033dd8f8e7c122916f7568822cc59a7f7d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1a930b038317f5e7c2813da1dd2b02134c89c46b"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\npipeknn = Pipeline([\n    ('scale', StandardScaler()),    \n    ('knn', KNeighborsClassifier(n_neighbors=5)),\n])\npipeknn.fit(X_train, y_train)\nknnTrainScore, knnTestScore = processResults(pipeknn)","execution_count":27,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"dafe41ae94d7d52fd4e8bc3a317926bbe7a910e9"},"cell_type":"code","source":"## K nearest neighbors\n","execution_count":28,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"db96ad6d93e76f6abac8f1ec479ebf22b0337486"},"cell_type":"code","source":"liver_df.head()","execution_count":29,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f7334596fbbede1cfbeba5b2a9d02e0524b8ccab"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"43ad5917-e40f-4e2a-bae5-182d2baadb6c","_uuid":"2d8812b1ccb4a56cfde468f223a6539974b54394","trusted":false},"cell_type":"code","source":"###Model evaluation\n#We can now rank our evaluation of all the models to choose the best one for our problem. \nmodels = pd.DataFrame({\n    'Model': [ 'Logistic Regression', 'Gaussian Naive Bayes','Decision Tree', 'Random Forest', 'Support Vector Classifier', 'Nearest Neighbour'],\n    'Score': [ logreg_score, gauss_score, decision_tree_score, random_forest_score, svcScore, knnTrainScore],\n    'Test Score': [ logreg_score_test, gauss_score_test, decision_tree_score_test, random_forest_score_test, svcScore_test, knnTestScore]})\nmodels.sort_values(by='Test Score', ascending=False)","execution_count":31,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}