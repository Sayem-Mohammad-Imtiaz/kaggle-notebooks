{"cells":[{"metadata":{},"cell_type":"markdown","source":"**In this kernel we will try to predict sentiment with Logistic Regression and again with LSTM to compare results.\n**I hope you enjoy it:) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom nltk.corpus import stopwords\nfrom nltk import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\nimport string\nimport matplotlib.pyplot as plt;\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/Tweets.csv')\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# first: Logistic Regression Model"},{"metadata":{},"cell_type":"markdown","source":"### **1- Text Clean Up**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(txt):\n    \n    \"\"\"\n    removing all hashtags , punctuations, stop_words  and links, also stemming words \n    \"\"\"\n    txt = txt.lower()\n    txt = re.sub(r\"(@\\S+)\", \"\", txt)  # remove hashtags\n    txt = txt.translate(str.maketrans('', '', string.punctuation)) # remove punctuations \n    txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links \n    txt = ' '.join([PorterStemmer().stem(word=word) for word in txt.split(\" \") if word not in stop_words ]) # stem & remove stop words\n    txt = ''.join([i for i in txt if not i.isdigit()]).strip() # remove digits ()\n    return txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tryout our newly defined function "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original Text : ',df['text'][3])  \nprint('Processed Text : ',clean_text(df['text'][3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now apply our function to the dataset, \nalso we need to encode target variable (airline_sentiment) to be 0 whenever a tweet is negative and 1 otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"    df['sent_encoded'] = df['airline_sentiment'].apply(lambda x:0 if x =='negative' else 1)\n    df['cleaned_text'] = df['text'].apply(clean_text)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Train text splits.\nfor a model to run we need to:\n\n1- tokenize Text\n\n2- encode every word as a feature\n\n3- represent word accurencies in a text as a count.( done by **CountVectorizer** )"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_data():   \n    y = df['sent_encoded']   # define target and feature column\n    X = df['cleaned_text']\n     \n    text_train, text_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)    # do the split\n    vect = CountVectorizer(min_df=5, ngram_range=(1, 4)) # create Count vectorizer.\n    X_train = vect.fit(text_train).transform(text_train) # transform text_train  into a vector \n    X_test = vect.transform(text_test) \n    feature_names = vect.get_feature_names() # to return all words used in vectorizer\n  \n    return X_train, X_test, y_train, y_test, feature_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets try the newly created function "},{"metadata":{"trusted":true},"cell_type":"code","source":" X_train, X_test, y_train, y_test, feature_names = train_test_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **2- Train The Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgstc = LogisticRegressionCV(class_weight={1:0.515,0:0.485})\nlgstc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"our model of selection is Logistic Regression reason is \"simplicity\". as it's always a good idea to start with the simplest model possible then see if you need to complicate it. \n<< Start small , think big >>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3- Test model performance **"},{"metadata":{},"cell_type":"markdown","source":"to test the model performance , we create a function to print out all results, to test overfitting we must compare test performance to train performance.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_model_performance(model,X_train,X_test,y_train,y_test):\n    training_sample = model.predict(X_train)\n    testing_sample = model.predict(X_test)\n    print('training ')\n    #print(classification_report(training_sample, y_train))  #uncomment if you want to see full report \n    print('train accuracy ',accuracy_score(training_sample, y_train))\n    print('train precision_score ',precision_score(training_sample, y_train)) \n    print('train recall score',recall_score(training_sample, y_train)) \n    \n    print('\\n testing  ')\n    print(classification_report(testing_sample, y_test))   #uncomment if you want to see full report \n    print('test average accuracy ',accuracy_score(testing_sample, y_test))\n    print('test average precision_score ',precision_score(testing_sample, y_test)) \n    print('test average recall score',recall_score(testing_sample, y_test)) \n    print('test AUC ',roc_auc_score(testing_sample, y_test))\n    \n    print(confusion_matrix(testing_sample, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" print_model_performance(lgstc, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the confusion matrix bellow , we see that model is better at detecting negative tweets, reason is because we have more negative samples than positive. \n"},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning (LSTM ) : "},{"metadata":{},"cell_type":"markdown","source":"lets try to \"complicate things\" a bit. \nnow we will create a deep learning model ( RNN-lstm )  to increase our 82% accuracy."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **2- define the global variables: **"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_fatures = 2000 # maximum number of words to use\nembed_dim = 120 # embidding dimention\nlstm_out = 190 # lstm size\nbatch_size = 32 # batch size\nvalidation_size = 1500 # validation set size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I came out with these values by intuition :) also trial and error. "},{"metadata":{},"cell_type":"markdown","source":"### **3- get X and y**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_X_y(feature, target):\n    data = df.copy() # create a copy so we dont mess up our old dataframe\n    \n    data = data[[feature,target]] # cut down all dataframe to only features and target variables \n    \n    data = data.dropna(subset=[feature]) # make sure there is no (NA) values as it will not help predictions \n    \n    data[feature] = data[feature].apply(lambda x: x.lower()) # convert text to lower case\n    data[feature] = data[feature].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove all digits\n    \n    tokenizer = Tokenizer(num_words=max_fatures, split=' ') # create our tokenizer here we used NLTK as it's a preferable package for developers using deep learning \n    tokenizer.fit_on_texts(data[feature].values) \n    \n    X = tokenizer.texts_to_sequences(data[feature].values) # here is the main trick! we convert:  'hi im x' to something like [2, 12, 53] where 2 , 12 , 34 ()\n    X = pad_sequences(X)    \n    Y = pd.get_dummies(data[target]).values\n    return X, Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" here is the main trick! we convert:  'hi im x' to something like [2, 12, 53] by using text_to_sequences functions  ( where 2, 12, 53) are indexes of words hi, m , x in **order** . \n here we sense the power of LSTM , as order of words can mean different thing \n "},{"metadata":{"trusted":true},"cell_type":"code","source":" X, Y = get_X_y('text', 'sent_encoded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets try it out : "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['text'][3]) # third tweet\nprint(X[[3]]) # third tweet to sequence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4- create the model : \nwe will create an LSTM model with a sigmoid activation. you might want to use softmax instead of sigmoid . but from my personal experience sigmoid performs better in binary situations. \nbut you are free try out on your own and see what fits you more :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_trained_model(X, Y):\n    model = Sequential()\n    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n    model.add(SpatialDropout1D(rate=0.9))\n    model.add(LSTM(lstm_out))\n    model.add(Dense(2,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n    \n    model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2)\n\n    X_validate = X_test[-validation_size:]\n    Y_validate = Y_test[-validation_size:]\n    X_test = X_test[:-validation_size]\n    Y_test = Y_test[:-validation_size]\n    score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n    print(\"score: %.2f\" % (score))\n    print(\"acc: %.2f\" % (acc))\n    return model, X_validate, Y_validate, X_test, Y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as you can see I used dropout rate of 90% , how did i reach that number?? answer is trial and error :)."},{"metadata":{"trusted":true},"cell_type":"code","source":"model, X_validate, Y_validate, X_test, Y_test = get_trained_model(X,Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a model with 85% accuracy , which seems fine ( at least we improved a bit !). and all thats left is to validate model to see it's actual performance: \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate_model(model, X_validate, Y_validate, X_test, Y_test):\n    pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n    all_predictions = []\n    for x in range(len(X_validate)):\n    \n        result = model.predict(X_validate[x].reshape(1,X_test.shape[1]), batch_size=1, verbose=2)[0]\n        all_predictions.append(result)\n        if np.argmax(result) == np.argmax(Y_validate[x]):\n            if np.argmax(Y_validate[x]) == 0:\n                neg_correct += 1\n            else:\n                pos_correct += 1\n    \n        if np.argmax(Y_validate[x]) == 0:\n            neg_cnt += 1\n        else:\n            pos_cnt += 1\n\n    print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n    print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")\n    \n    all_currects = neg_correct + pos_correct\n    all_samples = neg_cnt + pos_cnt\n    \n   \n    y_predict = np.asarray(all_predictions)\n    y_actual = np.asarray(Y_validate)\n    \n    y_actual = np.argmax(y_actual, axis=1)\n    y_predict = np.argmax(y_predict, axis=1)\n    \n  \n    cm = confusion_matrix(y_predict, y_actual)\t\n    print('AUC : ',roc_auc_score(y_predict, y_actual))\n    #print(\"Average Accuracy : \", all_currects/all_samples*100, \"%\")\n    return cm.ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"although this function might look scary . but actually not so much is happeing , all im doing is checking negative and positive accuracy. and return confusion matrix to compare it with the model above. \n\nnote: no need to praise me for this function it's a straight up copy and past from keras tutorials. ( with some of my touches) "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### **5- model validation **"},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = validate_model(model, X_validate, Y_validate, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('average accuracy : ', (tp + tn) / (tn+fp+fn+tp))\nprint('average precision : ', (tp) / (fp+tp))\nprint('average Recall : ', (tp) / (tp+fn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second model is better, and there is a big room for improvement. maybe try adding more hidden layers. or even do more text preprocessing. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}