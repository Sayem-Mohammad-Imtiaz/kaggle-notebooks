{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b2c39eb4-76b5-df31-f4a7-3ad46dbf156e"},"source":"# Glass Type Classification with Machine Learning"},{"cell_type":"markdown","metadata":{"_cell_guid":"bcbbe5a5-0541-b9db-c5e8-606188cdb222"},"source":"Hey there and welcome to this kernel! My name is Elie. I'm a data science newbie and this is my first Kaggle notebook. I have been putting a lot of effort recently to start my journey in data science. I benefited a lot from the notebooks of other awesome kagglers and I hope you benefit this notebook and learn new stuff from it.\n\n<hr >\n\n# Contents\n\n## 1) Prepare Problem\n\n * Load libraries\n\n * Load and explore the shape of the dataset\n\n## 2) Summarize Data\n\n* Descriptive statistics\n\n* Data visualization\n\n## 3) Prepare Data\n\n* Data Cleaning\n\n* Split-out validation dataset\n\n*  Data transformation  \n\n## 4) Evaluate Algorithms\n\n* Dimensionality reduction\n\n* Compare Algorithms\n\n## 5) Improve Accuracy\n\n* Algorithm Tuning\n\n## 6) Diagnose the performance of the best algorithms\n\n* Diagnose overfitting by plotting the learning and validation curves\n* Further tuning\n\n## 7) Finalize Model\n\n* Create standalone model on entire training dataset\n\n* Predictions on test dataset\n\n<hr />## Heading ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d155320-691d-a65f-e083-e7cc2bf859a4"},"source":"## 1. Prepare Problem"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a175f3d-0bf7-9e49-c57b-468f3371a7ce"},"source":"### Loading the libraries "},{"cell_type":"markdown","metadata":{"_cell_guid":"bf530358-f9f2-d629-180f-73ddc7223044"},"source":"Let us first begin by loading the libraries that we'll use in the notebook"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d3603b1-2e19-47f3-90e7-fa4b017020a9"},"outputs":[],"source":"import numpy as np  # linear algebra\nimport pandas as pd  # read and wrangle dataframes\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # statistical visualizations and aesthetics\nfrom sklearn.base import TransformerMixin # To create new classes for transformations\nfrom sklearn.preprocessing import (FunctionTransformer, StandardScaler) # preprocessing \nfrom sklearn.decomposition import PCA # dimensionality reduction\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom scipy.stats import boxcox # data transform\nfrom sklearn.model_selection import (train_test_split, KFold , StratifiedKFold, \n                                     cross_val_score, GridSearchCV, \n                                     learning_curve, validation_curve) # model selection modules\nfrom sklearn.pipeline import Pipeline # streaming pipelines\nfrom sklearn.base import BaseEstimator, TransformerMixin # To create a box-cox transformation class\nfrom collections import Counter\nimport warnings\n# load models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import (XGBClassifier, plot_importance)\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom time import time\n\n%matplotlib inline \nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')"},{"cell_type":"markdown","metadata":{"_cell_guid":"a1da2060-fd99-7cf6-ade3-964c4cd303df"},"source":"### Loading and exploring the shape of the dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd4f341c-f072-efea-f0f5-891184db0a07"},"outputs":[],"source":"df = pd.read_csv('../input/glass.csv')\nfeatures = df.columns[:-1].tolist()\nprint(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"41fc1169-af80-d9c4-fc46-2ecb533475ac"},"source":"The dataset consists of 214 observations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0aa76eab-6060-b7f4-338e-23a5c543a68c"},"outputs":[],"source":"df.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c4b9854-55f6-2dc6-fd4c-cd0aea51f583"},"outputs":[],"source":"df.dtypes"},{"cell_type":"markdown","metadata":{"_cell_guid":"084010f2-de70-90c5-54e8-ceb2bc7a62e4"},"source":"## 2. Summarize data"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a1766db-4784-0c33-7a8a-a59e36bf4392"},"source":"### Descriptive statistics"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ebd9870-dd40-1db0-c27d-94605b569878"},"source":"Let's first summarize the distribution of the numerical variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c5e49d9-fb5a-3147-14c8-546c3b0b852c"},"outputs":[],"source":"df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd7726e3-e1b8-d014-a66a-74dde8b406d1"},"source":"The features are not on the same scale. For example Si has a mean of 72.65 while Fe has a mean value of 0.057. Features should be on the same scale for algorithms such as logistic regression (gradient descent) to converge smoothly. Let's go ahead and check the distribution of the glass types."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43f288e9-3640-7e9a-6707-cab227c611f3"},"outputs":[],"source":"df['Type'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f228d437-920e-5aec-f4e2-9c4522f9e113"},"source":"The dataset is pretty unbalanced. The instances of types 1 and 2 constitute more than 67 % of the glass types."},{"cell_type":"markdown","metadata":{"_cell_guid":"094e8051-c7ff-dccd-ad0e-f52ef0ce88e0"},"source":"###  Data Visualization"},{"cell_type":"markdown","metadata":{"_cell_guid":"6606ab48-9990-76d4-9c74-174bc88fb436"},"source":"* **Univariate plots**"},{"cell_type":"markdown","metadata":{"_cell_guid":"7ca57b17-6a08-2fb1-bb41-968278ac2458"},"source":"Let's go ahead an look at the distribution of the different features of this dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d530a3ab-0a42-f70f-83d3-ee102bc4a7a1"},"outputs":[],"source":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], kde= False, label='Skew = %.3f' %(skew), bins=30)\n    plt.legend(loc='best')\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1a58d0ed-7adc-f220-9e87-09f2df237b43"},"source":"None of the features is normally distributed. The features Fe, Ba, Ca and K exhibit the highest skew coefficients. Moreover, the distribution of potassium (K) and Barium (Ba) seem to contain many outliers.\nLet's identify the indices of the observations containing outliers using [Turkey's method](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93df2e16-2776-5dd8-4b02-6e3caa8472ff"},"outputs":[],"source":"# Detect observations with more than one outlier\n\ndef outlier_hunt(df):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than 2 outliers. \n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in df.columns.tolist():\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        \n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        \n        # Interquartile rrange (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 2 )\n    \n    return multiple_outliers   \n\nprint('The dataset contains %d observations with more than 2 outliers' %(len(outlier_hunt(df[features]))))   "},{"cell_type":"markdown","metadata":{"_cell_guid":"478779d9-7bfa-8411-7ae5-b18b35411873"},"source":"Aha! there exists some 14 observations with multiple outliers.  These  could harm the efficiency of our learning algorithms. We'll make sure to get rid of these in the next sections.\n\nLet's examine the boxplots for the several distributions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41e7eab7-417b-65cc-f8f9-5430bffcfc29"},"outputs":[],"source":"plt.figure(figsize=(8,6))\nsns.boxplot(df[features])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f4b72c40-45cb-dc51-8989-0eedf23e1a3c"},"source":"Unsurprisingly, Silicon has a mean that is much superior to the other constituents as we already saw in the previous section. Well, that is normal since glass is mainly based on silica."},{"cell_type":"markdown","metadata":{"_cell_guid":"2e6651e4-09cc-df09-b834-ff9a8bf64793"},"source":"* **Multivariate plots**"},{"cell_type":"markdown","metadata":{"_cell_guid":"d258ae2b-7237-9368-1ec9-9c5ec8e16542"},"source":"Let's now proceed by drawing a pairplot to visually examine the correlation between the features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d51b128-d4f4-805e-ff4f-721f0a0659b8"},"outputs":[],"source":"plt.figure(figsize=(8,8))\nsns.pairplot(df[features],palette='coolwarm')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"70086397-6dee-4d27-b722-9150d5dab75f"},"source":"Let's go ahead and examine a heatmap of the correlations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4da104e6-ebeb-9e60-be90-1d989eddb19a"},"outputs":[],"source":"corr = df[features].corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features, yticklabels= features, alpha = 0.7,   cmap= 'coolwarm')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d1c4e175-2469-d2c2-2d1e-d13072902c55"},"source":"There seems to be a strong positive correlation between RI and Ca. This could be a hint to perform Principal component analysis in order to decorrelate some of the input features."},{"cell_type":"markdown","metadata":{"_cell_guid":"4c74a8b1-2058-ec06-fa17-512578359909"},"source":"## 3. Prepare data"},{"cell_type":"markdown","metadata":{"_cell_guid":"c87d836d-3317-03b6-25b0-676c1072c279"},"source":"### - Data cleaning "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b2b3999-b0b2-c2ec-4481-559822cfa9ee"},"outputs":[],"source":"df.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1d0d12e9-8fb4-5dc5-8d2d-23bec4fc5004"},"source":"This dataset is clean; there aren't any missing values in it."},{"cell_type":"markdown","metadata":{"_cell_guid":"0d1cb3ca-754d-8011-e61d-aaa4e9c351ba"},"source":"### - Hunting and removing multiple outliers\n\nLet's remove the observations containing multiple outliers with the function we created in the previous section."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"faf410bf-5705-d9f5-1526-1d72f47c58a2"},"outputs":[],"source":"outlier_indices = outlier_hunt(df[features])\ndf = df.drop(outlier_indices).reset_index(drop=True)\nprint(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"141e5033-ca80-2b21-4542-79b152397930"},"source":"Removing observations with multiple outliers (more than 2) leaves us with 200 observations to learn from. Let's now see how our distributions look like."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f62554c7-f123-fed6-7917-9d99fbe988a0"},"outputs":[],"source":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], kde=False, label='Skew = %.3f' %(skew), bins=30)\n    plt.legend(loc='best')\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e98d7b4-b9fe-cd2b-762d-65ed27585230"},"outputs":[],"source":"df['Type'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"10bb0256-c7da-86e5-440e-fd18c0c50744"},"source":"Let's now plot a distribution of the Types."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b487fa72-e6d1-2a35-e5f6-c14fed1d0bcd"},"outputs":[],"source":"sns.countplot(df['Type'])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"077d0640-d20a-37df-588a-56ecab86a0f8"},"source":"### - Split-out validation dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a19d0852-9efa-061e-b7df-8a99342b63a0"},"outputs":[],"source":"# Define X as features and y as lablels\nX = df[features] \ny = df['Type'] \n# set a seed and a test size for splitting the dataset \nseed = 7\ntest_size = 0.2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size , random_state = seed)"},{"cell_type":"markdown","metadata":{"_cell_guid":"125d15e5-afce-d08d-185f-7444f4351166"},"source":"### - Data transformation  "},{"cell_type":"markdown","metadata":{"_cell_guid":"0d168d82-ba99-aab2-036d-546c0046cdb0"},"source":"Let's examine if a Box-Cox transform can contribute to the normalization of some features. It should be emphasized that all transformations should only be done on the training set to avoid data snooping. Otherwise the test error estimation will be biased."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6584fd74-5cf7-5f5e-c33f-8cc475c05dfa"},"outputs":[],"source":"features_boxcox = []\n\nfor feature in features:\n    bc_transformed, _ = boxcox(df[feature]+1)  # shift by 1 to avoid computing log of negative values\n    features_boxcox.append(bc_transformed)\n\nfeatures_boxcox = np.column_stack(features_boxcox)\ndf_bc = pd.DataFrame(data=features_boxcox, columns=features)\ndf_bc['Type'] = df['Type']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89c960c7-d9bd-1bce-a245-70d99daeacb0"},"outputs":[],"source":"df_bc.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1694825f-1840-bd01-7f25-5eb255120f74"},"outputs":[],"source":"for feature in features:\n    fig, ax = plt.subplots(1,2,figsize=(7,3.5))    \n    ax[0].hist(df[feature], color='blue', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df[feature].skew(),3))) )\n    ax[0].set_title(str(feature))   \n    ax[0].legend(loc=0)\n    ax[1].hist(df_bc[feature], color='red', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df_bc[feature].skew(),3))) )\n    ax[1].set_title(str(feature)+' after a Box-Cox transformation')\n    ax[1].legend(loc=0)\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30706cae-af03-6481-9a32-50b3d313f85e"},"outputs":[],"source":"# check if skew is closer to zero after a box-cox transform\nfor feature in features:\n    delta = np.abs( df_bc[feature].skew() / df[feature].skew() )\n    if delta < 1.0 :\n        print('Feature %s is less skewed after a Box-Cox transform' %(feature))\n    else:\n        print('Feature %s is more skewed after a Box-Cox transform'  %(feature))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0aae11e7-6a75-1574-76e1-87fc8be187a5"},"source":"The Box-Cox transform seems to do a good job in reducing the skews of the different distributions of features.  However, it does not lead to the normalization of the feature distributions. Trial and error showed that it doesn't lead to an improvement of the performance of the used algorithms.  Next, let's explore dimensionality reduction techniques."},{"cell_type":"markdown","metadata":{"_cell_guid":"3770d5b9-205e-4dbc-f3a1-675313c84843"},"source":"## 4. Evaluate Algorithms"},{"cell_type":"markdown","metadata":{"_cell_guid":"a970e40f-5dc8-6027-bf0e-c6d31f4387ca"},"source":"### - Dimensionality reduction"},{"cell_type":"markdown","metadata":{"_cell_guid":"64325c06-da72-1c33-0a2f-aba930c8f2db"},"source":"* **XGBoost**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddc8ecf6-6ed6-56e8-0cb2-09fa714c95ea"},"outputs":[],"source":"model_importances = XGBClassifier()\nstart = time()\nmodel_importances.fit(X_train, y_train)\nprint('Elapsed time to train XGBoost  %.3f seconds' %(time()-start))\nplot_importance(model_importances)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c6e2ea2b-a857-90a3-2d42-5b7da456da21"},"source":"It appears that no main features dominate the importance in the XGBoost modeling of the problem. "},{"cell_type":"markdown","metadata":{"_cell_guid":"fdd159d8-976f-3872-6615-5f21a929e8a2"},"source":"* **PCA**"},{"cell_type":"markdown","metadata":{"_cell_guid":"5d73a533-f96d-00a0-3605-addfe73ae7c1"},"source":"Let's go ahead and perform a PCA on the features to decorrelate the ones that are linearly dependent and then let's plot the cumulative explained variance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"314cbd53-dc8c-8178-26d9-231ccacbd414"},"outputs":[],"source":"pca = PCA(random_state = seed)\npca.fit(X_train)\nvar_exp = pca.explained_variance_ratio_\ncum_var_exp = np.cumsum(var_exp)\nplt.figure(figsize=(8,6))\nplt.bar(range(1,len(cum_var_exp)+1), var_exp, align= 'center', label= 'individual variance explained', \\\n       alpha = 0.7)\nplt.step(range(1,len(cum_var_exp)+1), cum_var_exp, where = 'mid' , label= 'cumulative variance explained', \\\n        color= 'red')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.xticks(np.arange(1,len(var_exp)+1,1))\nplt.legend(loc='center right')\nplt.show()\n\n# Cumulative variance explained\nfor i, sum in enumerate(cum_var_exp):\n    print(\"PC\" + str(i+1), \"Cumulative variance: %.3f% %\" %(cum_var_exp[i]*100))"},{"cell_type":"markdown","metadata":{"_cell_guid":"69236ff9-b5ae-4007-8f1c-d41c90663b08"},"source":"It appears that about 99 % of the variance can be explained with the first 5 principal components. However feeding the PCA features to the learning algorithms did not contribute to a better performance. This might be due to the non-linearites that PCA is not able to capture."},{"cell_type":"markdown","metadata":{"_cell_guid":"22e166b7-f499-adf8-3f59-78bfedd7ca8d"},"source":"### - Compare Algorithms"},{"cell_type":"markdown","metadata":{"_cell_guid":"b9097176-6913-d004-8943-19e6c9e801b4"},"source":"Now it's time to compare the performance of different machine learning algorithms. We'll use 10-folds cross-validation to assess the performance of each model with the metric being the classification accuracy. Pipelines encompassing Standarization and PCA are used in order to avoid data leakage. Standarization is not performed for tree-based methods."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae576fd3-697e-88a2-294b-d08a60adcc84"},"outputs":[],"source":"n_components = 5\npipelines = []\nn_estimators = 200\n\n#print(df.shape)\npipelines.append( ('SVC',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                               ('pca', PCA(n_components = n_components, random_state=seed ) ),\n                             ('SVC', SVC(random_state=seed))]) ) )\n\n\npipelines.append(('KNN',\n                  Pipeline([ \n                              ('sc', StandardScaler()),\n#                             ('pca', PCA(n_components = n_components, random_state=seed ) ),\n                            ('KNN', KNeighborsClassifier()) ])))\npipelines.append( ('RF',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('RF', RandomForestClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\n\n\npipelines.append( ('Ada',\n                   Pipeline([ \n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                    ('Ada', AdaBoostClassifier(random_state=seed,  n_estimators=n_estimators)) ]) ))\n\npipelines.append( ('ET',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('ET', ExtraTreesClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\npipelines.append( ('GB',\n                   Pipeline([ \n                             ('sc', StandardScaler()),\n#                             ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('GB', GradientBoostingClassifier(random_state=seed)) ]) ))\n\npipelines.append( ('LR',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                               ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('LR', LogisticRegression(random_state=seed)) ]) ))\n\nresults, names, times  = [], [] , []\nnum_folds = 10\nscoring = 'accuracy'\n\nfor name, model in pipelines:\n    start = time()\n    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring = scoring,\n                                n_jobs=-1) \n    t_elapsed = time() - start\n    results.append(cv_results)\n    names.append(name)\n    times.append(t_elapsed)\n    msg = \"%s: %f (+/- %f) performed in %f seconds\" % (name, 100*cv_results.mean(), \n                                                       100*cv_results.std(), t_elapsed)\n    print(msg)\n\n\nfig = plt.figure(figsize=(12,8))    \nfig.suptitle(\"Algorithms comparison\")\nax = fig.add_subplot(1,1,1)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9eeb11ca-7bfb-9df4-85f1-002770f95811"},"source":"\n**Observation:** The best performances are achieved by RF. However, RF also yields a wide distribution. It is worthy to continue our study by tuning RF. \n\nLogistic Regression performs badly. This might be due to the fact that the data is not normally distributed as these algorithms perform well when data that is normally distributed."},{"cell_type":"markdown","metadata":{"_cell_guid":"1ca0e1a8-17fe-5b39-0e7a-f21741ab1981"},"source":"## 5. Algorithm tuning"},{"cell_type":"markdown","metadata":{"_cell_guid":"7be8f814-f686-81e0-81c3-3321cb6827cc"},"source":"### Tuning Random Forests\n\nFor random forest, we can tune the number of grown trees (n_estimators), the trees' depth (max_depth), the criterion of splitting (gini or entropy) and so on.... Let's start tuning these."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa261f12-a4a0-df09-1bd2-c3a2151ea6e7"},"outputs":[],"source":"# Create a pipeline with a Random forest classifier\npipe_rfc = Pipeline([ \n                      ('scl', StandardScaler()), \n                    ('rfc', RandomForestClassifier(random_state=seed, n_jobs=-1) )])\n\n# Set the grid parameters\nparam_grid_rfc =  [ {\n    'rfc__n_estimators': [100, 200,300,400], # number of estimators\n    #'rfc__criterion': ['gini', 'entropy'],   # Splitting criterion\n    'rfc__max_features':[0.05 , 0.1], # maximum features used at each split\n    'rfc__max_depth': [None, 5], # Max depth of the trees\n    'rfc__min_samples_split': [0.005, 0.01], # mininal samples in leafs\n    }]\n# Use 10 fold CV\nkfold = StratifiedKFold(n_splits=num_folds, random_state= seed)\ngrid_rfc = GridSearchCV(pipe_rfc, param_grid= param_grid_rfc, cv=kfold, scoring=scoring, verbose= 1, n_jobs=-1)\n\n#Fit the pipeline\nstart = time()\ngrid_rfc = grid_rfc.fit(X_train, y_train)\nend = time()\n\nprint(\"RFC grid search took %.3f seconds\" %(end-start))\n\n# Best score and best parameters\nprint('-------Best score----------')\nprint(grid_rfc.best_score_ * 100.0)\nprint('-------Best params----------')\nprint(grid_rfc.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"066fdf65-461d-b87f-d6b9-78b96bfa3fd6"},"source":"## 6) Diagnose the performance of the best algorithms"},{"cell_type":"markdown","metadata":{"_cell_guid":"7bfa1ad3-fbcd-2b81-1150-2eea3b218672"},"source":"### Diagnose overfitting by plotting the learning and validation curves"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5eb36f01-70b6-be52-362f-1d2d635ec45e"},"outputs":[],"source":"# Let's define some utility functions to plot the learning & validation curves\n\ndef plot_learning_curve(train_sizes, train_scores, test_scores, title, alpha=0.1):\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(train_sizes, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(train_sizes,train_mean + train_std,\n                    train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(train_sizes, test_mean, label='test score', color='red',marker='o')\n    plt.fill_between(train_sizes,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n    plt.title(title)\n    plt.xlabel('Number of training points')\n    plt.ylabel('Accuracy')\n    plt.grid(ls='--')\n    plt.legend(loc='best')\n    plt.show()    \n    \ndef plot_validation_curve(param_range, train_scores, test_scores, title, alpha=0.1):\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(param_range, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(param_range,train_mean + train_std,\n                    train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(param_range, test_mean, label='test score', color='red', marker='o')\n    plt.fill_between(param_range,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n    plt.title(title)\n    plt.grid(ls='--')\n    plt.xlabel('Parameter value')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='best')\n    plt.show()    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f03b5dc9-99c1-4361-bf7a-0f516d3270f5"},"outputs":[],"source":"plt.figure(figsize=(9,6))\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n              estimator= grid_rfc.best_estimator_ , X= X_train, y = y_train, \n                train_sizes=np.arange(0.1,1.1,0.1), cv= 10,  scoring='accuracy', n_jobs= - 1)\n\nplot_learning_curve(train_sizes, train_scores, test_scores, title='Learning curve for RFC')"},{"cell_type":"markdown","metadata":{"_cell_guid":"153792e5-0af8-7a32-0d20-9d4a9b390209"},"source":"The algorithm suffers from high variance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5771670f-c60e-f98d-d667-c28b8b3baa1d","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}