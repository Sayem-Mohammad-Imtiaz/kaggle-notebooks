{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data implement\ndata = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\n# data.head()\n# data cleaning\ndata.drop([\"Unnamed: 32\",\"id\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.diagnosis = [ 1 if each == \"M\" else 0 for each in data.diagnosis]\n# print(data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% Train test split\n\nfrom sklearn.model_selection import train_test_split\n\n# %25 test, %75 train\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% initialize parameter \n\ndef initialize_weight_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)  # weight\n    b = 0.0                          # bias \n    return w,b\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% sigmoid function \n\ndef sigmoid(z):\n    \n    y_head = 1/(1 + np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%\n    \ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Updating(learning) parameters\n    \ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: Cost %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%  # prediction\n    \ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% logistic_regression\n    \ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weight_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 400)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%   LR with Sklearn\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlr = LogisticRegression(random_state = 42, max_iter = 400,)\nlr.fit(x_train.T,y_train.T)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\np_pred = lr.predict_proba(x_test.T)\ny_pred = lr.predict(x_test.T)\nscore_ = lr.score(x_test.T,y_test.T)\nconf_m = confusion_matrix(y_test.T, y_pred).T\nreport = classification_report(y_test, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test proba:\\n\", p_pred )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test accuracy {}\".format(score_))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confusion matrix:\\n\",conf_m )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test report:\\n\", report )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   Öncelikle gögüs kanseri veri setini kullandım bu veri seti UCI tarafından 1995 yılında oluşturulmuş,32 attribute ve 569 sample var. Diagnosis attributte B = Benign M = Malignant olmak üzere 2 sınıflandırma yapılmış.Verisetini yükleme, temizleme ve normalize işlemlerini yaptım. Datamı %25test %75train olarak parçaladıktan sonra işlemlerimi iki farklı şekilde yaptım. İlk başta hazır kütüphane kullanmadan fonksiyonları kendim yazarak egittim ikinci olarak ise sklearn kütüphanesini kullanarak egitim yaptım initialize_weight_bias fonksiyonu ile başlangıç agırlığını ve bias degerini veri seti boyutlarına uygun şekilde üretmiş oldum, agırlık degerini 0 olursa sonsuz döngüye gireceginden 0 yakın deger 0.01 olarak belirledim.\n    sigmoid fonksiyonunu, sigmoid fonksiyonunu hesaplaplamak için kullandım.\n    forward_backward_propagation fonksiyonu ağırlıklara ve bias a göre degerlerimi hesaplayıp loss ve cost degerlerini, back propagation yaparak da türev, gradient degerlerini hesaplayıp geri döndürdü.\n    update fonksiyonu parametre olarak aldıgı iterasyon sayısı kadar forward-backward yapıp agırlık ve bias degerlerini güncelledi ve her iterasyon için cost degerini grafik ile çizdirdi. iterasyon sayısı arttıkça cost 0'a yaklaştı ama belli bir iterasyon sayısından sonra cost degerinin degişimi çok azaldı, bu yüzden iterasyonu 400 olarak belirledim.\n    predict fonksiyonu sigmoid fonksiyonundan dönen test verilerimi threshold degeri ile karşılaştırıp deger threshold degerinden büyükse 1,küçükse 0 olarak döndürdü.\n    logistic_regression fonksiyonu tüm işlemleri yaptıgımız ana foksiyon işlevinde, başlangıç fonksiyonu ile başlangıç degerlerini verip, update fonksiyonu train verileri ile iterasyın kadar forward-backward yaparak agırlık degerlerimizi güncelleyip egitimi yapmış, predict fonksiyonu ile test verileri gönderip sonuçları elde etmiş ve accuracy degerini elde etmiş olduk.\n    İkinci aşamada ise sklearn kütüphanesini kullanarak aynı test ve train setleri ile işlemleri yaptım iterasyon sayısını yukarıdaki ile aynı(400) ama başarı sonuçları biraz farklı oldu, sklearn kütüphanesi daha farklı parametreler ve daha hassas degerler kullandığı için oldugunu düşünüyorum.\n\n198229002007 - BAŞAK ÖZTÜRK\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}