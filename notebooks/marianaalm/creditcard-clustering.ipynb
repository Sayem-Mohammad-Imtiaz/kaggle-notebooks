{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Segment Analysis of clients using Credit Card Data\n\nThe objective of this study is to apply clustering techniques to understand the market. \nOne application of this type of study is in marketing campaigns, by understanding the different consumer profiles. \n\nSome of the concepts that will be presented in this project are:\n- Clustering with k-means\n- Dimensionality reduction using PCA\n- Dimensionality reduction with autoencoders"},{"metadata":{},"cell_type":"markdown","source":"## 1 - Loading the Libraries and the File"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Analysis and visualization\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\n\n#For clustering\nfrom sklearn.cluster import KMeans\n\n#For reduction of dimensionality\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data = pd.read_csv('../input/ccdata/CC GENERAL.csv')\ncredit_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the size of the dataset and some basic information on the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary of this section:\n* There are 8950 registers with 18 features.\n* The data is in numerical form, except for the customer id (CUST_ID) which is an object containing letters and numbers.\n* On average, clients maintain 1564 dollars in the bank account for use with the debit card.\n* On average, clients spend 1000 USD on purchases. \n* About the purchase mode, on average clients spend 592 dollars on one-off purchases and 411 dollars on purchases with installments. \n* Good news for the bank: clients, on average, use 978 dollars as cash advancement. One must have in mind that, in general, the taxes for cash advancement are higher than the credit card taxes. \n* In regards to frequency, clients more frequently make purchases with installents (mean = 0.364) than one-off (mean = 0.202). \n* Regarding credit limits on the credit card, the maximum limit is 30,000 dollars with the minimum being 50 dollars. On average, clients have a credit card limit of 4494 dollars.\n"},{"metadata":{},"cell_type":"markdown","source":"## 2-  Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Checking for null values: "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,6))\nsns.heatmap(credit_data.isnull());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are null data in the variables 'MINIMUM_PAYMENTS' and 'CREDIT_LIMIT':"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.loc[(credit_data['MINIMUM_PAYMENTS'].isnull() == True)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many ways of replacing null numbers. In this case, the null values will be replaced with the mean as both (credit limit and minimum payments) are continuous variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data['MINIMUM_PAYMENTS'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.loc[(credit_data['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = credit_data['MINIMUM_PAYMENTS'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data['CREDIT_LIMIT'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.loc[(credit_data['CREDIT_LIMIT'].isnull() == True), 'CREDIT_LIMIT'] = credit_data['CREDIT_LIMIT'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now just checking if the null values were replaced:"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for duplicated values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom ID is not an unecessary information that will only add more complexity to the data, as it is an object and not a numerical information. This information will be deleted from the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.drop('CUST_ID', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"Set1\")\nplt.rcParams.update({'font.size': 12})\nsns.set_style(\"whitegrid\")\ncredit_data.hist(bins=40, figsize=(30, 30));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can extract some insights Ffor some of the most relevant variables:\n* BALANCE left in the account is more frequent around 1000 dollars.\n* PURCHASES values concentrate below 5000 dollars.\n* BALANCE FREQUENCY - we can see that clients frequently update the balance in their accounts. \n* ONEOFF_PURCHASES and INSTALLMENT_PURCHASES - looking at the scale of the graph we notice that purchases with installments are more frequent for values no greater than 5000 dollars and one-off purchases are more frenquent for values no greater than 10000 dollars. \n* PURCHASE FREQUENCY show a segumentation of clients: one group make purchases very frequently, while the other group rarely make purchases. \n* MINIMUM PAYMENTS and PRC FULL PAYMENT - these variables show us that many clients opt for paying the minumum of their credit card bill. Very few clients pay the full bill. This is also good for the bank as taxes are high for credit card bills. \n* TENURE shows that most of the clients are long term clients (more than 12 years)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize=(20,80))\n#sns.set_palette(\"cool_r\")\n#sns.set_style(\"darkgrid\")\n#for i in range(len(credit_data.columns)):\n # plt.subplot(9,2,i+1)\n # sns.distplot(credit_data[credit_data.columns[i]], kde = True)\n # plt.title(credit_data.columns[i])\n#plt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the correlations between variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = credit_data.corr()\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(correlations, annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation is stronger as the values approach 1. From the correlation matrix we take that:\n* PURCHASE INSTALLMENTS FREQUENCY is somehow correlated to PURCHASES FREQUENCY, and this confirms the insight.\n* PURCHASE and ONEOFF PURCHASE are strongly correlated and it seems that most of the purchases values are related to one-off purchases. When we look at INSTALLMENTS PURCHASES correlation with PURCHASES we see that the value is 0.68, not as strong as the correlation with one-off purchases. "},{"metadata":{},"cell_type":"markdown","source":"## 3 -  Clustering the data\n\nThe unsupervised learning algorithm, Kmeans, will be implemented to group the data in similar groups. \n"},{"metadata":{},"cell_type":"markdown","source":"### Scaling the data before clustering. \nWe have data on frequency, which varie from 0 to 1 and data on payments that have a much greater scale. To implement a clustering algorithm it is important to put the data in the same scale, once the distance between the data is taken into account. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ncredit_data_scaled = scaler.fit_transform(credit_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking scaling:"},{"metadata":{"trusted":true},"cell_type":"code","source":"minmax_nonscaled = min(credit_data['BALANCE']), max(credit_data['BALANCE'])\nminmax_scaled = min(credit_data_scaled[0]), max(credit_data_scaled[0])\n\nprint(\"Minimum and maximum values before scaling = {}\".format(minmax_nonscaled))\nprint(\"Minimum and maximum values after scaling = {}\".format(minmax_scaled))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Determining number of clusters with the Elbow Method"},{"metadata":{},"cell_type":"markdown","source":"To choose the best number of clusters the elbow method will be implemented. This is one of the most popular methods to determine the number of clusters. \n\nThe objective of the elbow method is to minimize WCSS, which measures the within cluster sum of squares. WSS is the sum of squares of the distances of each data point in all clusters to their respective centroids. When WCSS is minimum, you have less variability ofthe data inside the cluster. "},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss= []\nrange_values = range(1, 20)\nfor i in range_values:\n  kmeans = KMeans(n_clusters=i)\n  kmeans.fit(credit_data_scaled)\n  wcss.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(wcss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8)) \nplt.plot(wcss, 'bo-', color='c')\nplt.xlabel('Number of clusters Clusters', fontsize=14)\nplt.ylabel('WCSS', fontsize=14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the elbow method it seems that the optimum number of clusters is between 7 and 10. "},{"metadata":{},"cell_type":"markdown","source":"### Implementing the number of clusters"},{"metadata":{},"cell_type":"markdown","source":"Testing the implementation with 8 clusters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=8)\nkmeans.fit(credit_data_scaled)\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the number of clients per label:\nnp.unique(labels, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Which is the centroid for group ?\ncluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [credit_data.columns])\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [credit_data.columns])\ncluster_centers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding the cluster information to the original dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data_cluster = pd.concat([credit_data, pd.DataFrame({'GROUP': labels})], axis = 1)\ncredit_data_cluster.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in credit_data.columns:\n  plt.figure(figsize=(30,5))\n  for j in range(8):\n    plt.subplot(1, 8, j + 1)\n    cluster = credit_data_cluster[credit_data_cluster['GROUP'] == j]\n    cluster[i].hist(bins = 20)\n    plt.title('{} \\nGroup {}'.format(i, j))\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordering the data by group and saving it into a new csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"ordered_data = credit_data_cluster.sort_values(by = 'GROUP')\nordered_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordered_data.to_csv('group.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4 - Principal Component Analysis\n\nIn this section, PCA technique will be used for the reduction of the dimensionality. It creates new uncorrelated variables that successively maximize variance. \nBy doing this, PCA increases interpretability minimizing information loss. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(credit_data_scaled)\nprincipal_comp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_data = pd.DataFrame(data = principal_comp, columns=['pca1', 'pca2'])\npca_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_data = pd.concat([pca_data, pd.DataFrame({'GROUP': labels})], axis = 1)\npca_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.scatterplot(x = 'pca1', y = 'pca2', hue = 'GROUP', data = pca_data, palette = 'Set1')\nplt.xlabel(\"PCA 1\", fontsize=14)\nplt.ylabel(\"PCA 2\", fontsize=14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5- Autoencoders\n\nA technique for reduction of dimensionality as an alternative to PCA or can be used as in conjunction with PCA. Autoencoders are a branch of neural network which attempt to compress the information of the input variables into a reduced dimensional space and then recreate the input data set.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#INPUT LAYER: 17 neurons\n# 1st INTERNAL LAYER: 500 neurons, relu activated\n# 2nd INTERNAL LAYER: 2000 Layer, relu activated\ninput_data = Input(shape=(17,))\nx = Dense(500, activation='relu')(input_data)\nx = Dense(2000, activation='relu')(x)\n\nencoded = Dense(10, activation='relu')(x)\n\nx = Dense(2000, activation='relu')(encoded)\nx = Dense(500, activation='relu')(x)\n\ndecoded = Dense(17)(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder = Model(input_data, decoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoded variable to access only the encoded data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Model(input_data, encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Adam optimizer\nautoencoder.compile(optimizer = 'Adam', loss = 'mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.fit(credit_data_scaled, credit_data_scaled, epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data = encoder.predict(credit_data_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Defining new clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_data_scaled[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss_2 = []\nrange_values = range(1, 20)\nfor i in range_values:\n  kmeans = KMeans(n_clusters=i)\n  kmeans.fit(compact_data)\n  wcss_2.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(wcss_2, 'bx-')\nplt.xlabel('Clusters')\nplt.ylabel('WCSS');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(wcss_1, 'bx-', color = 'c')\nplt.plot(wcss_2, 'bx-', color = 'm');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second wcss curve shows that the results start to become more linear around 3 to 4 clusters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=4)\nkmeans.fit(compact_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = kmeans.labels_\nlabels, labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cluster_at = pd.concat([credit_data, pd.DataFrame({'cluster': labels})], axis = 1)\ndata_cluster_at.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying PCA to the new dataset, as a second reduction of dimensionality:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 2)\nprin_comp = pca.fit_transform(compact_data)\npca_df = pd.DataFrame(data = prin_comp, columns = ['pca1', 'pca2'])\npca_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_df = pd.concat([pca_df, pd.DataFrame({'cluster': labels})], axis = 1)\npca_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.scatterplot(x = 'pca1', y = 'pca2', hue = 'cluster', data = pca_df, palette = ['cyan', 'black', 'blue', 'pink']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster_ordered = data_cluster_at.sort_values(by = 'cluster')\ndf_cluster_ordered.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster_ordered.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster_ordered.to_excel('cluster_ordereded.xls')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}