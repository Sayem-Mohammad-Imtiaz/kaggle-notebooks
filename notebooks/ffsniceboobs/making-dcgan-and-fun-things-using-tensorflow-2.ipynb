{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Making DCGAN to generate faces and fun things using TensorFlow 2 & Keras\n\nWe are going to train GAN for generating faces and then we will make fun playing with it. Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the “adversarial”). One neural network, called the generator, generates new faces, while the other, the discriminator, decides whether each instance of face it reviews belongs to the actual training dataset or not.\n\nWe will use aligned faces of celebrities to train our GAN and make animations to visualize results!\n\nTF1 model source: http://bamos.github.io/2016/08/09/deep-completion/","metadata":{}},{"cell_type":"markdown","source":"## Prerequisites\n\nIn this section we will install some useful packages and extensions","metadata":{}},{"cell_type":"code","source":"!pip install -U nb_black watermark","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-24T07:31:19.556254Z","iopub.execute_input":"2021-07-24T07:31:19.556533Z","iopub.status.idle":"2021-07-24T07:31:30.051562Z","shell.execute_reply.started":"2021-07-24T07:31:19.556506Z","shell.execute_reply":"2021-07-24T07:31:30.050631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext lab_black\n%load_ext watermark\n\n%watermark -v -m -p numpy,matplotlib,tensorflow,imageio","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:31:30.054217Z","iopub.execute_input":"2021-07-24T07:31:30.054495Z","iopub.status.idle":"2021-07-24T07:31:34.629165Z","shell.execute_reply.started":"2021-07-24T07:31:30.054466Z","shell.execute_reply":"2021-07-24T07:31:34.628148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common imports and variables","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n\n%matplotlib inline\n\n\nIMAGE_SIZE_NO_CROP = 256  # Size of image before cropping\nIMAGE_SIZE = 64  # Shapes of input image\nBATCH_SIZE = 64  # Batch size\nDATA_PATH = \"/kaggle/input/celeba-dataset/img_align_celeba\"\nRANDOM_SEED = 42\n\ntf.random.set_seed(RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:31:34.631115Z","iopub.execute_input":"2021-07-24T07:31:34.631403Z","iopub.status.idle":"2021-07-24T07:31:34.648436Z","shell.execute_reply.started":"2021-07-24T07:31:34.631371Z","shell.execute_reply":"2021-07-24T07:31:34.647265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking available GPUs","metadata":{}},{"cell_type":"code","source":"print(tf.config.experimental.list_physical_devices(\"GPU\"))\nprint(tf.test.gpu_device_name())","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:31:34.650297Z","iopub.execute_input":"2021-07-24T07:31:34.650888Z","iopub.status.idle":"2021-07-24T07:31:36.3115Z","shell.execute_reply.started":"2021-07-24T07:31:34.650846Z","shell.execute_reply":"2021-07-24T07:31:36.310224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the dataset\n\nHere we will check and prepare our data. \nWe need the faces only. Images in the dataset are centered on eyes, so we will crop faces utilizing that fact.\n\nI've found caching is extremely useful in this task. The whole dataset can be put into memory if you have >12GB RAM.\nPrefetching will also help us to utilize resources better.\n\nSometimes image_dataset_from_directory is slow as fuck. Also Kaggle won't let us to cache everything in memory and will kill the kernel during the training, that's frustrating.\n\nNevertheless training on the whole dataset will take some time (first epoch with BATCH_SIZE=64 takes ~1800 seconds to finish with GPU accelerator here, ~1300 seconds for BATCH_SIZE=512, after caching it's ~300 seconds per epoch).","metadata":{}},{"cell_type":"code","source":"num_images = len(os.listdir(os.path.join(DATA_PATH, \"img_align_celeba\")))\nprint(f\"Num images: {num_images}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:32:02.286606Z","iopub.execute_input":"2021-07-24T07:32:02.286961Z","iopub.status.idle":"2021-07-24T07:32:06.983564Z","shell.execute_reply.started":"2021-07-24T07:32:02.286929Z","shell.execute_reply":"2021-07-24T07:32:06.982638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"celeb_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    DATA_PATH,\n    label_mode=None,\n    color_mode=\"rgb\",\n    batch_size=BATCH_SIZE,\n    image_size=(IMAGE_SIZE_NO_CROP, IMAGE_SIZE_NO_CROP),\n    seed=RANDOM_SEED,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:32:06.985005Z","iopub.execute_input":"2021-07-24T07:32:06.985533Z","iopub.status.idle":"2021-07-24T08:03:05.237846Z","shell.execute_reply.started":"2021-07-24T07:32:06.985493Z","shell.execute_reply":"2021-07-24T08:03:05.236939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CACHE_FILE = \"cache\"\n\n\ndef process(image):\n    #     images are centered on eyes, we will crop faces utilizing that fact\n    height, width = image.shape[1], image.shape[2]\n\n    offset_height = int(height * 0.35)\n    offset_width = int(height * 0.27)\n\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, int(width * 0.45), int(height * 0.45)\n    )\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE], preserve_aspect_ratio=True)\n\n    image = tf.cast((image - 127.5) / 127.5, tf.float32)\n    return image\n\n\nceleb_dataset = (\n    celeb_dataset.map(process)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    .cache(filename=CACHE_FILE)\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:03:05.239517Z","iopub.execute_input":"2021-07-24T08:03:05.239868Z","iopub.status.idle":"2021-07-24T08:03:05.420373Z","shell.execute_reply.started":"2021-07-24T08:03:05.239828Z","shell.execute_reply":"2021-07-24T08:03:05.419525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what we have got","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 4), constrained_layout=True)\n\n\nfor images in celeb_dataset.take(1):\n    for i in range(8):\n        ax = plt.subplot(2, 4, i + 1)\n        plt.imshow((images[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:03:05.423277Z","iopub.execute_input":"2021-07-24T08:03:05.423547Z","iopub.status.idle":"2021-07-24T08:03:11.461629Z","shell.execute_reply.started":"2021-07-24T08:03:05.423521Z","shell.execute_reply":"2021-07-24T08:03:11.460608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice!","metadata":{}},{"cell_type":"markdown","source":"# Defining a network\n\nWe will build two models: generator and discriminator.\nThe generator is producing images from the noise while the discriminator are trying to distinguish those images from the faces of celebrities.","metadata":{}},{"cell_type":"markdown","source":"### Some network parameters","metadata":{}},{"cell_type":"code","source":"Z_DIM = 100  # Dimension of face's manifold\nGENERATOR_DENSE_SIZE = 512  # Length of first tensor in generator\nN_CHANNELS = 3  # Number channels of input image\nNUM_CONV_DISCRIMINATOR = 4  # amount of convolution layers in discriminator model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:21.463974Z","iopub.execute_input":"2021-07-24T08:04:21.464306Z","iopub.status.idle":"2021-07-24T08:04:21.473124Z","shell.execute_reply.started":"2021-07-24T08:04:21.464272Z","shell.execute_reply":"2021-07-24T08:04:21.472259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generator\n\n\nGenerator has the folllowing architecture:\n\n<img src=\"http://bamos.github.io/data/2016-08-09/discrim-architecture.png\">\n\nHere we have dense input and the rest of layers are transposed convolutions.\n\nA transposed convolution will reverse the spatial transformation of a regular convolution with the same parameters.\nIf you perform a regular convolution followed by a transposed convolution and both have the same settings (kernel size, padding, stride), then the input and output will have the same shape. This makes it super easy to build encoder-decoder networks with them. \n\nHere are some notes on the architecture of the generator:\n1. The deeper the convolution, the less filters it uses.\n1. Deconvolutions-relu layers are applied to achieve input image shape.\n1. Batch normalization is used before nonlinearity for speed and stability of learning.\n1. Tanh activation at the end of network allows to scale images to [-1, 1].\n1. To force generator not to collapse and produce different outputs bias is initialized with zero.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import Sequential\n\n\ndef make_generator_model():\n    model = Sequential()\n    model.add(\n        layers.Dense(\n            4 * 4 * GENERATOR_DENSE_SIZE,\n            use_bias=False,\n            input_shape=(Z_DIM,),\n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((4, 4, GENERATOR_DENSE_SIZE)))\n\n    assert model.output_shape == (\n        None,\n        4,\n        4,\n        GENERATOR_DENSE_SIZE,\n    )  # Note: None is the batch size\n\n    depth_mul = 1  # Depth decreases as spatial component increases.\n    size = 4  # Size increases as depth decreases.\n\n    while size < IMAGE_SIZE // 2:\n        filters = int(GENERATOR_DENSE_SIZE * depth_mul)\n        model.add(\n            layers.Conv2DTranspose(\n                filters,\n                (5, 5),\n                strides=(2, 2),\n                padding=\"same\",\n                use_bias=False,\n            )\n        )\n        assert model.output_shape == (\n            None,\n            size * 2,\n            size * 2,\n            filters,\n        )\n        model.add(layers.BatchNormalization())\n        model.add(layers.LeakyReLU())\n\n        size *= 2\n        depth_mul /= 2\n\n    model.add(\n        layers.Conv2DTranspose(\n            3,\n            (5, 5),\n            strides=(2, 2),\n            padding=\"same\",\n            use_bias=False,\n            activation=\"tanh\",\n        )\n    )\n    assert model.output_shape == (None, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:23.633459Z","iopub.execute_input":"2021-07-24T08:04:23.633776Z","iopub.status.idle":"2021-07-24T08:04:23.671138Z","shell.execute_reply.started":"2021-07-24T08:04:23.633747Z","shell.execute_reply":"2021-07-24T08:04:23.670287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = make_generator_model()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:24.638187Z","iopub.execute_input":"2021-07-24T08:04:24.638574Z","iopub.status.idle":"2021-07-24T08:04:24.769752Z","shell.execute_reply.started":"2021-07-24T08:04:24.638542Z","shell.execute_reply":"2021-07-24T08:04:24.768665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discriminator\n\n\nDiscriminator takes 3D tensor as input and outputs one number that is a probability of input being a face. Its architecture is quite similar to \"reverse\" generator.","metadata":{}},{"cell_type":"code","source":"def make_discriminator_model():\n    model = Sequential()\n\n    for i in range(NUM_CONV_DISCRIMINATOR):\n        model.add(\n            layers.Conv2D(\n                64 * 2 ** i,\n                (5, 5),\n                strides=(2, 2),\n                padding=\"same\",\n                input_shape=[IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS],\n            )\n        )\n        model.add(layers.LeakyReLU())\n        model.add(layers.Dropout(0.2))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:25.644374Z","iopub.execute_input":"2021-07-24T08:04:25.644848Z","iopub.status.idle":"2021-07-24T08:04:25.674314Z","shell.execute_reply.started":"2021-07-24T08:04:25.644805Z","shell.execute_reply":"2021-07-24T08:04:25.673196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = make_discriminator_model()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:26.45821Z","iopub.execute_input":"2021-07-24T08:04:26.458571Z","iopub.status.idle":"2021-07-24T08:04:26.534807Z","shell.execute_reply.started":"2021-07-24T08:04:26.45854Z","shell.execute_reply":"2021-07-24T08:04:26.533313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss functions\n\n\nWe will use the following loss functions:\n$$ D\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m}[\\log{D(x_i)} + \\log{(1 - D(G(z_i)))}]$$\n$$ G\\_loss = \\frac{1}{m} \\sum_{i=1}^{m} \\log{(1 - D(G(z_i)))}$$","metadata":{}},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:30.342955Z","iopub.execute_input":"2021-07-24T08:04:30.343295Z","iopub.status.idle":"2021-07-24T08:04:30.357615Z","shell.execute_reply.started":"2021-07-24T08:04:30.343246Z","shell.execute_reply":"2021-07-24T08:04:30.35648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizers\n\nThere are different optimizers for discriminator and generator","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import optimizers\n\n\ngenerator_optimizer = optimizers.Adam(1e-4)\ndiscriminator_optimizer = optimizers.Adam(1e-4)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:31.67387Z","iopub.execute_input":"2021-07-24T08:04:31.674188Z","iopub.status.idle":"2021-07-24T08:04:31.683173Z","shell.execute_reply.started":"2021-07-24T08:04:31.674157Z","shell.execute_reply":"2021-07-24T08:04:31.682272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\nHere we will define some functions to determine the training process. We will train both models simultaneously.\nThe generator will be taking BATCH_SIZE random vectors at the every step and make images. The discriminator at first will check true images, then output and finally we will compute losses using both real and fake ones.\n\nThe model will be trained for 30 epochs. Every 15 epochs the weights of the model are being saved.\nWe will show progress on generating 8 images for the same random vectors after the end of every epoch.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 30\nNUM_SAMPLES_TO_GENERATE = 8\nNUM_CHECKPOINT = 10\n\n\n# You will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.uniform([NUM_SAMPLES_TO_GENERATE, Z_DIM])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:53.046719Z","iopub.execute_input":"2021-07-24T08:04:53.047038Z","iopub.status.idle":"2021-07-24T08:04:53.05643Z","shell.execute_reply.started":"2021-07-24T08:04:53.047009Z","shell.execute_reply":"2021-07-24T08:04:53.055436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = \"./training_checkpoints\"\n\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n\ncheckpoint = tf.train.Checkpoint(\n    generator_optimizer=generator_optimizer,\n    discriminator_optimizer=discriminator_optimizer,\n    generator=generator,\n    discriminator=discriminator,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:55.39142Z","iopub.execute_input":"2021-07-24T08:04:55.391747Z","iopub.status.idle":"2021-07-24T08:04:55.403206Z","shell.execute_reply.started":"2021-07-24T08:04:55.391718Z","shell.execute_reply":"2021-07-24T08:04:55.402156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.uniform([BATCH_SIZE, Z_DIM])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n        gradients_of_generator = gen_tape.gradient(\n            gen_loss, generator.trainable_variables\n        )\n        gradients_of_discriminator = disc_tape.gradient(\n            disc_loss, discriminator.trainable_variables\n        )\n\n        generator_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n        discriminator_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:04:57.296159Z","iopub.execute_input":"2021-07-24T08:04:57.296504Z","iopub.status.idle":"2021-07-24T08:04:57.319289Z","shell.execute_reply.started":"2021-07-24T08:04:57.296475Z","shell.execute_reply":"2021-07-24T08:04:57.318379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\nimport time\n\n\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for image_batch in dataset:\n            train_step(image_batch)\n\n        # Produce images for the GIF as you go\n        clear_output(wait=True)\n        generate_and_save_images(generator, epoch + 1, seed)\n\n        # Save the model every 15 epochs\n        if (epoch + 1) % NUM_CHECKPOINT == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n\n        print(\"Time for epoch {} is {} sec\".format(epoch + 1, time.time() - start))\n\n    # Generate after the final epoch\n    clear_output(wait=True)\n    generate_and_save_images(generator, epochs, seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:05:02.568522Z","iopub.execute_input":"2021-07-24T08:05:02.568837Z","iopub.status.idle":"2021-07-24T08:05:02.589402Z","shell.execute_reply.started":"2021-07-24T08:05:02.568807Z","shell.execute_reply":"2021-07-24T08:05:02.58852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)\n\n    fig = plt.figure(figsize=(8, 4), constrained_layout=True)\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(2, 4, i + 1)\n        plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n        plt.axis(\"off\")\n\n    plt.savefig(\"image_at_epoch_{:04d}.png\".format(epoch))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:05:06.782615Z","iopub.execute_input":"2021-07-24T08:05:06.782938Z","iopub.status.idle":"2021-07-24T08:05:06.801671Z","shell.execute_reply.started":"2021-07-24T08:05:06.782908Z","shell.execute_reply":"2021-07-24T08:05:06.800772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run next cell to train and subsequent cell to load model","metadata":{}},{"cell_type":"code","source":"train(celeb_dataset, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:05:12.081237Z","iopub.execute_input":"2021-07-24T08:05:12.081574Z","iopub.status.idle":"2021-07-24T11:01:13.828665Z","shell.execute_reply.started":"2021-07-24T08:05:12.081544Z","shell.execute_reply":"2021-07-24T11:01:13.82662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to load weights execute this\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T18:40:55.43356Z","iopub.execute_input":"2021-07-23T18:40:55.434028Z","iopub.status.idle":"2021-07-23T18:41:05.584579Z","shell.execute_reply.started":"2021-07-23T18:40:55.433996Z","shell.execute_reply":"2021-07-23T18:41:05.582096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the results\n\nLet's make an animation showing the progress of our model","metadata":{}},{"cell_type":"code","source":"import imageio\nimport glob\nfrom IPython.display import Image\n\n\nanim_file = \"dcgan.gif\"\n\nfilenames = glob.glob(\"image_at_epoch_*.png\")\nimages = [imageio.imread(filename) for filename in sorted(filenames)]\nimageio.mimsave(anim_file, images, fps=8)\n\n# to show in notebook\n# Image(anim_file)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:36:45.125973Z","iopub.execute_input":"2021-07-24T11:36:45.126365Z","iopub.status.idle":"2021-07-24T11:36:49.477198Z","shell.execute_reply.started":"2021-07-24T11:36:45.126312Z","shell.execute_reply":"2021-07-24T11:36:49.4764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/AYvPzML6qfOPVGZ3I8/giphy.gif\">","metadata":{}},{"cell_type":"markdown","source":"# Face interpolation\n\nOur model performs quite well. Time for fun things. \n\nAt first, let's try to interpolate between faces: we will generate two vectors $z_1$ and $z_2$ and get a batch of vectors of the form $\\alpha\\cdot z_1 + (1- \\alpha)\\cdot  z_2, \\alpha \\in [0,1]$ for generating faces on them and looking at results.","metadata":{}},{"cell_type":"code","source":"vectors = tf.random.uniform([16, Z_DIM])\n\npredictions = generator(vectors, training=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n\nfor i in range(predictions.shape[0]):\n    plt.subplot(4, 4, i + 1).set_title(i)\n    plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:32:36.655016Z","iopub.execute_input":"2021-07-24T11:32:36.65542Z","iopub.status.idle":"2021-07-24T11:32:37.936108Z","shell.execute_reply.started":"2021-07-24T11:32:36.655381Z","shell.execute_reply":"2021-07-24T11:32:37.935025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take 6 and 14. The first is looking more feminine, so it's a woman, the second is a man.","metadata":{}},{"cell_type":"code","source":"NUM_ALPHAS = 30\n\nidx_first, idx_second = 6, 14\nalphas = np.linspace(0, 1, NUM_ALPHAS)\n\nto_interpolate = [vectors[idx_first]]\nfor alpha in alphas[::-1]:\n    to_interpolate.append(\n        alpha * vectors[idx_first] + (1 - alpha) * vectors[idx_second]\n    )\n\nto_interpolate.append(vectors[idx_second])\nto_interpolate = np.array(to_interpolate)\n\npredictions = generator(to_interpolate, training=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:34:43.064546Z","iopub.execute_input":"2021-07-24T11:34:43.064912Z","iopub.status.idle":"2021-07-24T11:34:43.179285Z","shell.execute_reply.started":"2021-07-24T11:34:43.06488Z","shell.execute_reply":"2021-07-24T11:34:43.17847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will save images for every prediction and two original vectors","metadata":{}},{"cell_type":"code","source":"for i in range(predictions.shape[0]):\n    fig = plt.figure(figsize=(4, 4), constrained_layout=True)\n    plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n    plt.axis(\"off\")\n    plt.savefig(f\"face_{i:02d}.png\")\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:34:45.569176Z","iopub.execute_input":"2021-07-24T11:34:45.569534Z","iopub.status.idle":"2021-07-24T11:34:48.643919Z","shell.execute_reply.started":"2021-07-24T11:34:45.569503Z","shell.execute_reply":"2021-07-24T11:34:48.642823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And make animation!","metadata":{}},{"cell_type":"code","source":"anim_file = \"face.gif\"\n\nfilenames = glob.glob(\"face*.png\")\nimages = [imageio.imread(filename) for filename in sorted(filenames)]\nimageio.mimsave(anim_file, images, fps=5)\n\n# to show in notebook\n# Image(anim_file)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:36:32.081345Z","iopub.execute_input":"2021-07-24T11:36:32.081715Z","iopub.status.idle":"2021-07-24T11:36:32.794276Z","shell.execute_reply.started":"2021-07-24T11:36:32.081682Z","shell.execute_reply":"2021-07-24T11:36:32.793363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/pvqX59RXU4GAKMUqm2/giphy.gif\"></img>","metadata":{}},{"cell_type":"markdown","source":"Very cool! Isn't it?","metadata":{}},{"cell_type":"markdown","source":"# Making smiling faces\n\nHere we will check for some faces that are smiling and some that are not to extract \"smiling\" vector. Later we will apply that vector to inputs for getting smiliing images as outputs and vice versa.\n\nWe denote a \"smile vector\" as mean of vectors z with generated smile on it minus mean of vectors z without generated smile on it.","metadata":{}},{"cell_type":"markdown","source":"### Building a smile vector","metadata":{}},{"cell_type":"code","source":"to_test = tf.random.uniform([36, Z_DIM])\n\n\npredictions = generator(to_test, training=False)\n\nfig = plt.figure(figsize=(16, 16))\n\nfor i in range(predictions.shape[0]):\n    plt.subplot(6, 6, i + 1).set_title(i)\n    plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:42:30.292372Z","iopub.execute_input":"2021-07-24T11:42:30.292705Z","iopub.status.idle":"2021-07-24T11:42:32.581301Z","shell.execute_reply.started":"2021-07-24T11:42:30.292675Z","shell.execute_reply":"2021-07-24T11:42:32.580188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's actually hard to find not smiling ones. I've executed previous cell for several times.\n\nSome faces are really strange. Who's 10th guy? Tiger man? The hell is 29th?\n\nLet's choose up to 5 images for the following groups:\n1. Big smiles: 4, 8, 14, 30, 31\n2. No smiles: 3 (poker face), 6 (reminds me of vampires), 15, 25, 26","metadata":{}},{"cell_type":"code","source":"smiliing_indices = [4, 8, 14, 30, 31]\nnot_smiling_indices = [3, 6, 15, 25, 26]\n\nsmiliing_array = np.array([to_test[i] for i in smiliing_indices])\nnot_smiling_array = np.array([to_test[i] for i in not_smiling_indices])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:48:32.964137Z","iopub.execute_input":"2021-07-24T11:48:32.964638Z","iopub.status.idle":"2021-07-24T11:48:32.981813Z","shell.execute_reply.started":"2021-07-24T11:48:32.964594Z","shell.execute_reply":"2021-07-24T11:48:32.980852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_and_plot(array, title=None):\n    predictions = generator(array, training=False)\n\n    fig = plt.figure(figsize=(16, 4))\n    if title:\n        fig.suptitle(title)\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(1, 5, i + 1)\n        plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:48:35.732382Z","iopub.execute_input":"2021-07-24T11:48:35.732716Z","iopub.status.idle":"2021-07-24T11:48:35.751097Z","shell.execute_reply.started":"2021-07-24T11:48:35.732685Z","shell.execute_reply":"2021-07-24T11:48:35.750082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_and_plot(smiliing_array, \"Smiling faces\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:48:37.712022Z","iopub.execute_input":"2021-07-24T11:48:37.712503Z","iopub.status.idle":"2021-07-24T11:48:38.179736Z","shell.execute_reply.started":"2021-07-24T11:48:37.712457Z","shell.execute_reply":"2021-07-24T11:48:38.178881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_and_plot(not_smiling_array, \"Not smiling faces\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:48:42.717295Z","iopub.execute_input":"2021-07-24T11:48:42.71774Z","iopub.status.idle":"2021-07-24T11:48:43.070463Z","shell.execute_reply.started":"2021-07-24T11:48:42.717684Z","shell.execute_reply":"2021-07-24T11:48:43.069471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's what we have chosen to build a \"smile vector\"","metadata":{}},{"cell_type":"code","source":"smile_arr_vec = smiliing_array.mean(axis=0)\nnot_smile_arr_vec = not_smiling_array.mean(axis=0)\n\nsmile_vec = smile_arr_vec - not_smile_arr_vec","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:48:53.470599Z","iopub.execute_input":"2021-07-24T11:48:53.470923Z","iopub.status.idle":"2021-07-24T11:48:53.479812Z","shell.execute_reply.started":"2021-07-24T11:48:53.470891Z","shell.execute_reply":"2021-07-24T11:48:53.478913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying smile vector\n\n\nTime to apply it to smiling faces, not smiling faces, it's also worth to try to apply anti-smiling vector that should make faces sad. Oof!","metadata":{}},{"cell_type":"markdown","source":"#### Not smiling faces","metadata":{}},{"cell_type":"code","source":"predict_and_plot(not_smiling_array, \"Original\")\npredict_and_plot(not_smiling_array + smile_vec, \"+ smile vector\")\npredict_and_plot(not_smiling_array - smile_vec, \"- smile vector\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:48:56.162842Z","iopub.execute_input":"2021-07-24T11:48:56.163163Z","iopub.status.idle":"2021-07-24T11:48:57.109929Z","shell.execute_reply.started":"2021-07-24T11:48:56.163132Z","shell.execute_reply":"2021-07-24T11:48:57.109139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gosh! The faces at the last row are looking so judgmental. \"Look what you've done\"","metadata":{}},{"cell_type":"markdown","source":"#### Smiling faces","metadata":{}},{"cell_type":"code","source":"predict_and_plot(smiliing_array, \"Original\")\npredict_and_plot(smiliing_array + smile_vec, \"+ smile vector\")\npredict_and_plot(smiliing_array - smile_vec, \"- smile vector\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:52:02.841344Z","iopub.execute_input":"2021-07-24T11:52:02.841682Z","iopub.status.idle":"2021-07-24T11:52:03.791845Z","shell.execute_reply.started":"2021-07-24T11:52:02.841652Z","shell.execute_reply":"2021-07-24T11:52:03.790981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}