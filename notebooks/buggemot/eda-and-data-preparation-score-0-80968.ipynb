{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### EDA and Data Preparation\n\n\nIn this notebook we perform the Exploratory Data Analytics(EDA) and some technics of Data Preparation.<br>\nOur work is divided on the following steps<br>\n- Load necessary libraries<br>\n- Load data<br>\n- First look on dataset<br>\n    * Shape\n    * Check the missing values\n- Data Preparation\n    * Find new feautures\n    * Outlier Handling\n    * Encoding categorial data","metadata":{}},{"cell_type":"markdown","source":"### Load necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load data","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntest_df = pd.read_csv('/kaggle/input/house-price-prediction-challenge/test.csv')\ntrain_df = pd.read_csv('/kaggle/input/house-price-prediction-challenge/train.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First look","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shape","metadata":{}},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Missing values","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation","metadata":{}},{"cell_type":"markdown","source":"As we see the column of the address contains two parts. Let's divide this value and add a column of cities in the dataset.<br>\nMay be **city** is important feature for predictions model. ","metadata":{}},{"cell_type":"code","source":"train_df['ADDRESS_PART1'] = train_df['ADDRESS'].apply(lambda x: x.split(',')[0].strip())\ntrain_df['CITY'] = train_df['ADDRESS'].apply(lambda x: x.split(',')[1].strip())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Count unique values for City column","metadata":{}},{"cell_type":"code","source":"len(train_df['CITY'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the correlation of columns","metadata":{}},{"cell_type":"code","source":"train_df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(train_df.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This isn't a surprise - max value of correlation between price and square of flat","metadata":{}},{"cell_type":"markdown","source":"#### Outlier Handling","metadata":{}},{"cell_type":"markdown","source":"**Outliers** are the values, which are too far from the rest of our observations in the columns.<br>\nOutliers can distort statistic data.<br>\nThe best way to visualize outliers is by plotting box plots","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(y='SQUARE_FT', data=train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a few outliers in the form of black dots. Let's remove the outliers from the dataset. One of the possible ways to do this - using Inter Quartile Range (IQR) <a href=\"https://pypi.org/project/remove-outliers/#:~:text=Multiply%20the%20interquartile%20range%20(IQR,IQR)%20from%20the%20first%20quartile\"> More details </a> <br>\nI'm going to create function for this","metadata":{}},{"cell_type":"code","source":"def get_outliers(df, column_name):\n    \n    IQR = df[column_name].quantile(0.75) - df[column_name].quantile(0.25)\n    lower_sq_limit = df[column_name].quantile(0.25) - (IQR * 1.5)\n    upper_sq_limit = df[column_name].quantile(0.75) + (IQR * 1.5)\n    outliers = np.where(df[column_name] > upper_sq_limit, True,\n    np.where(df[column_name] < lower_sq_limit, True, False))\n    return outliers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sqr_ft_outliers = get_outliers(train_df, 'SQUARE_FT')\ndf_without_outliers = train_df.loc[~(sqr_ft_outliers),]\nprint(train_df.shape, df_without_outliers.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"{} rows was been deleted\".format(\n    train_df.shape[0] - df_without_outliers.shape[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check in","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(y='SQUARE_FT', data=df_without_outliers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check in outliers in target column","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(y='TARGET(PRICE_IN_LACS)', data=df_without_outliers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same situation, so I'm going to remove outliers from dataset","metadata":{}},{"cell_type":"code","source":"price_outliers = get_outliers(df_without_outliers, 'TARGET(PRICE_IN_LACS)')\nlen(price_outliers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_df = df_without_outliers.loc[~(price_outliers),]\nprint(train_df.shape, prepared_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(y='TARGET(PRICE_IN_LACS)', data=prepared_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"{} rows was been deleted\".format(\n    train_df.shape[0] - prepared_df.shape[0]))\n\npercent_of_deleted_rows = round((train_df.shape[0] - prepared_df.shape[0]) / train_df.shape[0] * 100, 2)\nprint(\"{}% data was been deleted\".format(percent_of_deleted_rows))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_df.index = np.arange(prepared_df.shape[0])\nprepared_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look to the CITY column","metadata":{}},{"cell_type":"code","source":"head_values = prepared_df['CITY'].value_counts().head(20).index.to_list()\nhead_city = prepared_df[prepared_df['CITY'].isin(head_values)]\nplt.figure(figsize=(10,8))\nsns.boxplot(y='TARGET(PRICE_IN_LACS)', x='CITY', data=head_city)\nplt.xticks(rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, city is an important feature for prediction model. It's not surprise :)","metadata":{}},{"cell_type":"markdown","source":"#### Encoding categorical data\n<a href=\"https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\"> More about encoding data </a>","metadata":{}},{"cell_type":"code","source":"prepared_df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_df = pd.concat([prepared_df, pd.get_dummies(prepared_df['POSTED_BY'])], axis=1)\nprepared_df = pd.concat([prepared_df, pd.get_dummies(prepared_df['BHK_OR_RK'])], axis=1)\n\nle = LabelEncoder()\nle.fit(prepared_df['CITY'])\nprepared_df['LE_CITY'] = le.transform(prepared_df['CITY'])\n\nle.fit(prepared_df['ADDRESS_PART1'])\nprepared_df['LE_ADDRESS_PART1'] = le.transform(prepared_df['ADDRESS_PART1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_df.drop(['POSTED_BY', 'BHK_OR_RK', 'ADDRESS', 'CITY', 'ADDRESS_PART1'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = prepared_df[['SQUARE_FT','LONGITUDE', 'LATITUDE', 'TARGET(PRICE_IN_LACS)']]\nscaler = StandardScaler()\nscaler.fit(temp)\ntemp_scaled = scaler.transform(temp)\n\ntemp_scaled = pd.DataFrame(temp_scaled, \n                           columns=temp.columns)\n\ntemp_scaled\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_df.drop(['SQUARE_FT','LONGITUDE', 'LATITUDE', 'TARGET(PRICE_IN_LACS)'], axis=1, inplace=True)\nprepared_df = pd.concat([prepared_df, temp_scaled], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create model\n\nData has been ready, next step – create prediction model.","metadata":{}},{"cell_type":"code","source":"X = prepared_df.loc[:, prepared_df.columns != 'TARGET(PRICE_IN_LACS)']\ny = prepared_df['TARGET(PRICE_IN_LACS)']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gbr = GradientBoostingRegressor()\n\n# parameters = {'max_depth':[9,],\n#               'n_estimators':[154,],\n#               'max_features': [6,],\n#               'learning_rate':[x/10 for x in map(float, range(1,5))],\n#              }\n# clf = GridSearchCV(gbr, parameters)\n# clf.fit(X_train, y_train)\n# clf.best_score_, clf.best_params_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(max_depth=9, n_estimators=154)\ncross_val_score(gbr, X_train, y_train, cv=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(max_depth=9, n_estimators=154, max_features=6)\ncross_val_score(gbr, X_train, y_train, cv=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Coefficients of features","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(gbr.feature_importances_,X_train.columns, columns=['coef']).sort_values(by='coef', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Difference between real and predicted data\n\nLet' create plot for first 150 predicted and real data.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(30, 10))\nax.plot(y_test.to_list()[:150], \n        label='First 150 values', color='red', linewidth=2)\nax.plot(gbr.predict(X_test)[:150], \n        label='Predicted first 150 values', \n        linestyle='dashed', linewidth=2)\nax.legend(prop={\"size\":20})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank you","metadata":{}}]}