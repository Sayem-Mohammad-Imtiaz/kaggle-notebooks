{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Extract entities linked to UMLS (Unified Medical Language System)\nSource notebook -> https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy <br>\nMaking it great again. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# install SciSpacy and download a full spaCy pipeline for biomedical data\n!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Dict, Iterable, Tuple\n\nimport os\nimport json\n\nfrom tqdm import tqdm\n\nimport spacy\nfrom spacy.tokens import Span\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scispacy\nSpaCy models for biomedical text processing.\n\n## Scispacy additional Pipeline Components\n\n### AbbreviationDetector\nAbbreviation detection algorithm. You can access the list of abbreviations via the *doc._.abbreviations*\n\n### EntityLinker\nPerforms linking to a knowledge base. The linker simply performs a string overlap - based search on named entities, comparing them with the concepts in a knowledge base using an approximate nearest neighbours search.\n\n**UMLS EntityLinker**\n- umls: Links to the **Unified Medical Language System**, levels 0,1,2 and 9. This has ~3M concepts.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture \n# instantiate language ScispaCy model\nfull_nlp = spacy.load('en_core_sci_sm')\n\n# Add the abbreviation pipe to the spacy pipeline.\nabbreviation_pipe = AbbreviationDetector(full_nlp)\nfull_nlp.add_pipe(abbreviation_pipe)\n\n# Add the entity linking pipe to the spacy pipeline\nlinker = UmlsEntityLinker(resolve_abbreviations=True, filter_for_definitions=False)\nfull_nlp.add_pipe(linker)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Kaggle dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"ROOT_PATH = os.path.join(\"/kaggle\", \"input\", \"CORD-19-research-challenge\")\nJSON_PATH = os.path.join(ROOT_PATH, \"document_parses\", \"pdf_json\")\n\ndef load_json_files_lazy(directory_path: str):\n    \"\"\"Load the json files from a directory \"\"\"\n    loaded_files = []\n    for filename in os.listdir(directory_path):\n        full_path = os.path.join(directory_path, filename)\n        with open(full_path) as _json_file:\n            loaded_file = json.load(_json_file)\n            yield loaded_file\n\njson_loaded_files = load_json_files_lazy(JSON_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokens vs Spans\n- Token represents a single word, punctuation symbol, whitespace, etc. from a document\n- Span is a slice from the document. It is an ordered sequence of Tokens.\n\n*source: https://stackoverflow.com/a/58893287*"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def build_doc_with_entities(full_text: str, abstract_text: List, body_text: List):\n    \"\"\"Build a doc using mention spans from the input file, but the scispacy linker\"\"\"\n    \n    # disable entity linker and named entity recognition pipelines\n    with full_nlp.disable_pipes(['UmlsEntityLinker', 'ner']):\n        doc = full_nlp(full_text)\n        \n    entities = []\n    character_offset = 0\n    for paragraph in abstract_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        for entity in paragraph['entity_spans']:\n            entity_start = character_offset + entity['start']\n            entity_end = character_offset + entity['end']\n            entity_span = doc.char_span(entity_start, entity_end)\n\n            # just skip for now if the character span does not align\n            if entity_span is not None:\n                entities.append(entity_span)\n\n        character_offset += len(paragraph_text) + 1\n\n    for paragraph in body_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        for entity in paragraph['entity_spans']:\n            entity_start = character_offset + entity['start']\n            entity_end = character_offset + entity['end']\n            entity_span = doc.char_span(entity_start, entity_end)\n\n            # just skip for now if the character span does not align\n            if entity_span is not None:\n                entities.append(entity_span)\n\n        character_offset += len(paragraph_text) + 1\n    \n    new_entity_spans = [Span(doc, entity_span.start, entity_span.end, label=\"Entity\") for entity_span in entities]\n    doc.ents = new_entity_spans\n    doc = linker(doc)\n    \n    return doc\n\ndef add_entities_to_file(input_json: Dict, use_existing_mentions: bool = False):\n    \"\"\"Copies the input json and adds the linked entities to it. \n       If you want to use entity annotations already present in the input json file,\n       set the use_existing_mentions flag, otherwise scispacy's base model will be used for NER\"\"\"\n    \n    body_text = input_json[\"body_text\"]\n    abstract_text = input_json['abstract']\n    \n    paragraph_char_spans = []\n    char_span_index = 0\n    paragraph_index = 0\n    full_text = \"\"\n    for paragraph in abstract_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        full_text += paragraph_text + \" \"\n        paragraph_char_spans.append((\"abstract\", paragraph_index, char_span_index, char_span_index + len(paragraph_text)))\n        char_span_index = char_span_index + len(paragraph_text) + 1\n        paragraph_index += 1\n    \n    paragraph_index = 0\n    for paragraph in body_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        full_text += paragraph_text + \" \"\n        paragraph_char_spans.append((\"body_text\", paragraph_index, char_span_index, char_span_index + len(paragraph_text)))\n        char_span_index = char_span_index + len(paragraph_text) + 1\n        paragraph_index += 1\n    \n    full_text = full_text[:-1]\n    \n    if not use_existing_mentions:\n        doc = full_nlp(full_text)\n    else:\n        doc = build_doc_with_entities(full_text, abstract_text, body_text)\n    \n    input_copy = input_json.copy()\n    for i, (paragraph, (section, paragraph_index, start_char, end_char)) in enumerate(zip(abstract_text + body_text, paragraph_char_spans)):\n        entities = []\n        paragraph_span = doc.char_span(start_char, end_char)\n        for mention_span in paragraph_span.ents:\n            linked_cuis_and_scores = mention_span._.umls_ents\n            # the definition, aliases, and type can be accessed via linker.umls.cui_to_entity[cui]\n            entity = {}\n            entity['start'] = mention_span.start_char - paragraph_span.start_char\n            entity['end'] = mention_span.end_char - paragraph_span.start_char\n            entity['text'] = mention_span.text\n            \n            # could filter out specific UMLS types here, if desired\n            entity['links'] = [(cui, linker.umls.cui_to_entity[cui].types[0], score) for (cui, score) in linked_cuis_and_scores]\n            entities.append(entity)\n\n        input_copy[section][paragraph_index]['entity_spans'] = entities\n    return input_copy\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_json_file(directory_path: str, file_name: str, output_json: Dict):\n    \"\"\"Write a json file out\"\"\"\n    with open(os.path.join(directory_path, file_name), 'w') as _json_file:\n        json.dump(output_json, _json_file, indent=4)\n\ndef write_subset_directory_with_entities(directory_path: str, inputs: List[Dict], num_files_to_process: int = 0):\n    \"\"\"Write the transformed jsons for a full subset directory\"\"\"\n    if not os.path.exists(directory_path):\n        os.makedirs(directory_path, exist_ok=True)\n\n    for i, file in tqdm(enumerate(inputs), desc=f\"Processing {directory_path}\"):\n        if i >= num_files_to_process:\n            break\n        new_json = add_entities_to_file(file)\n        write_json_file(directory_path, file['paper_id'] + '.json', new_json)\n\nKAGGLE_OUTPUT_DIRECTORY = os.path.join(\"/kaggle\", \"working\")\nROOT_OUTPUT_DIRECTORY = os.path.join(KAGGLE_OUTPUT_DIRECTORY, \"CORD-19-with-entities\")\nJSON_OUTPUT_DIRECTORY = os.path.join(ROOT_OUTPUT_DIRECTORY, \"comm_use_subset\", \"comm_use_subset\")\n\n# process a few samples\nwrite_subset_directory_with_entities(JSON_OUTPUT_DIRECTORY, json_loaded_files, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## UMLS definitions explained\n\n### Concept Unique Identifiers (CUI) \n\nA concept is a meaning. A meaning can have many different names. A key goal of Metathesaurus construction is to understand the intended meaning of each name in each source vocabulary and to link all the names from all of the source vocabularies that mean the same thing (the synonyms). \n\n\n### Type Unique Identifier (TUI)\n\nEach concept is assigned at least one semantic type which is one of the broad categories like \"Clinical Drug\" or \"Disease or Syndrome\" described in the UMLS Semantic Network.\n\nFor example: \n> |T060|Diagnostic Procedure <br>\n> |T056|Daily or Recreational Activity <br>\n> |T203|Drug Delivery Device <br>\n> |T047|Disease or Syndrome\n\n\n*source: https://www.nlm.nih.gov/research/umls/new_users/online_learning/Meta_005.html*\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# interpret the output - links to UMLS \nwith open(os.path.join(JSON_OUTPUT_DIRECTORY, list(os.listdir(JSON_OUTPUT_DIRECTORY))[0])) as _json_file:\n    loaded_file = json.load(_json_file)\n\nbody = loaded_file['body_text']\nfirst_paragraph = body[0]\n\n\nprint(\"[INFO] Take a look at the analysis of the first paragraph:\\n\")\nprint(first_paragraph['text'])\nprint()\nfor entity in first_paragraph['entity_spans']:\n    top_link = entity['links'][0] if len(entity['links']) > 0 else None\n    mention_text = entity['text']\n    print(f\"Mention: {mention_text}\")\n    print(linker.umls.cui_to_entity[top_link[0]] if top_link else \"No links passed the threshold\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary "},{"metadata":{},"cell_type":"markdown","source":"The entities were successfully extracted from Covid-19 articles and then linked to UMLS. \n\n> Examples of exctracted entities: \n\n> a) specialist terminology: T-cell lymphotropic virus 1 <br>\n> b) daily vocabulary: dairy\n\n\nSciSpacy library deals with abbreviations (such as USDA : United States Department of Agriculture) and searches for synonyms for these terms (ex. United States. Dept. of Agriculture, Agriculture Department). "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}