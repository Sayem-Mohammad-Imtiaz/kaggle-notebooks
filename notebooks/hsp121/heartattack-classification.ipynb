{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Do EDA and then classify if a person is prone to heart attack or not.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\npd.options.display.max_columns = None\npd.options.display.width = None\n\nwarnings.filterwarnings(\"ignore\")\n\ndtset = pd.read_csv('G:\\Kaggle\\heart.csv')\n\ndtset.head()\n\ndtset.info(verbose=True)\n\nAbove shows there is no null-values in the dataset.\n\n\nsns.histplot(data=dtset,x=\"output\")\n\nsns.histplot(data=dtset,x=\"sex\")\n\nsns.swarmplot(data=dtset,x=\"output\",y=\"age\",hue=\"sex\")\nplt.show()\n\nFrom the above plot its clear that patients of HA is distributed across all age segments, but is sigificantly lower below later 30's and above initial 60's. Bulk patients are present roughly within early 40's to latter 50's age segment. Also gender 1 is more prone to heart attack as compared to gender 0, but for this dataset out of the gender 0 datapoints present, most are heart-attack prone. Also this dataset is gender-wise imbalanced, there is more datapoints of gender1 relative to gender2.\n\nsns.set_theme(style=\"darkgrid\")\nsns.displot(dtset,x=\"cp\",col=\"output\")\n\nsns.histplot(data=dtset,x=\"cp\",kde=True)\n\ncp type 0 is most prevalent, and persons having this is mostly not prone to heart-attack. After that chest-apin type 2 is prevalent & persons having this is mostly prone to heart-attack. Chest pain 3 is the least prevalent.\n\nsns.swarmplot(data=dtset,x=\"output\",y=\"chol\",hue=\"sex\")\n\nAgain, gender type 1 is more prone to heart-attacks & that can be attributed to their cholestrol levels. Gender type 2 is relatively less prone to heart attack. \n\nsns.set_theme(style=\"ticks\")\nsns.boxplot(x=dtset[\"chol\"])\n\nsns.boxplot(x=dtset[\"trtbps\"])\n\nsns.boxplot(x=dtset[\"thalachh\"])\n\nsns.boxplot(x=dtset[\"age\"])\n\nsns.boxplot(x=dtset[\"oldpeak\"])\n\n#dtset.oldpeak.value_counts()\n\nsns.boxplot(x=dtset[\"caa\"])\n\ndtset[\"caa\"].value_counts()  # It's clear, its more of a sort of categorical variable.\n\ndtset.slp.value_counts()\n\ndtset.thall.value_counts()\n\ndtset.describe()\n\nplt.figure(figsize=(20,6))\nsns.heatmap(dtset.corr(),annot=True,mask=np.triu(dtset.corr()))\n\n| sex,cp,fbs,restecg,exng,slp,caa,thall | --------------- are the features that looks categorical (needed dummy creation).\n\n| chol,trtbps,thalachh,oldpeak | ------ are the variables that need outlier-handling.\n\n----------------------------------------------------------------------------------------------------\n\nprint(dtset.cp.value_counts())\nprint(dtset.fbs.value_counts())\nprint(dtset.restecg.value_counts())\nprint(dtset.exng.value_counts())\nprint(dtset.slp.value_counts())\nprint(dtset.caa.value_counts())\nprint(dtset.thall.value_counts())\n\ndmy=dtset[[\"sex\",\"cp\",\"fbs\",\"restecg\",\"exng\",\"slp\",\"caa\",\"thall\"]]\ndmy.replace({\"sex\":{1:\"M\",2:\"F\"},\"cp\":{0:\"L'cp\",1:\"Lcp\",2:\"Mcp\",3:\"Hcp\"},\"fbs\":{0:\"Lfbs\",1:\"Hfbs\"},\"restecg\":{0:\"Lecg\",1:\"Mecg\",2:\"Hecg\"},\"exng\":{0:\"Lexng\",1:\"Hexng\"},\"caa\":{0:\"L_caa\",1:\"Lcaa\",2:\"Mcaa\",3:\"Hcaa\",4:\"H'caa\"},\"slp\":{0:\"Lslp\",1:\"Mslp\",2:\"Hslp\"},\"thall\":{0:\"L_thall\",1:\"Lthall\",2:\"Mthall\",3:\"Hthall\"}},inplace=True)\n\nd1=pd.get_dummies(dmy,prefix=\"d_\",drop_first=True)\n\nd1\n\ndtset2 = dtset[dtset.columns.difference([\"sex\",\"cp\",\"fbs\",\"restecg\",\"exng\",\"slp\",\"caa\",\"thall\",\"age\"])]\n\n# Age doesn't need outlier-handling, thalachh needs only lower-quantile outlier handling.\n\noutlr1 = dtset[dtset.columns.difference([\"sex\",\"cp\",\"fbs\",\"restecg\",\"exng\",\"slp\",\"caa\",\"thall\",\"age\",\"thalachh\"])]\noutlr1.apply(lambda x : x.clip(upper=x.quantile(0.99)))\n\noutlr2=pd.DataFrame(dtset[\"thalachh\"])\noutlr2.apply(lambda x : x.clip(lower=x.quantile(0.01)))\n\ndtset3=pd.concat([outlr1,outlr2,d1],axis=1)       # final data-set.\n\ndtset3\n\n------------------------------------------------------------------------------------------------------\n\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,roc_auc_score,roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\ntrain,test=train_test_split(dtset3,test_size=0.3,random_state=123)\n\n# Statistical model.\n\nmdl1=smf.logit(formula='''output~chol+oldpeak+trtbps+thalachh+d__M+d__Lcp+d__Lcp+d__Mcp+d__Lfbs+d__Lecg+d__Mecg+d__Lexng+\nd__Lslp+d__Mslp+d__Hcaa+d__L_caa+d__Lcaa+d__Mcaa+d__L_thall+d__Lthall+d__Mthall''',data=train).fit()\n\nprint(mdl1.summary())\n\npredicted=mdl1.predict(test)\n\nytest=test[\"output\"]\nytest_prd=list(map(round,predicted))\n\ncm = confusion_matrix(ytest,ytest_prd) \nprint (\"Confusion Matrix : \\n\", cm) \n\nprint('Test accuracy: ', accuracy_score(ytest,ytest_prd))\nprint(\"Precision:\",precision_score(ytest,ytest_prd))\nprint(\"Precision:\",recall_score(ytest,ytest_prd))\nprint(\"ROC score:\",roc_auc_score(ytest,ytest_prd))\n\nx=pd.concat([ytest,predicted],axis=1)\nx.rename(columns={\"output\":\"ytest\",0:\"ytest_pred\"},inplace=True)\nx[\"ytest_pred_abs\"]=x[\"ytest_pred\"].apply(lambda x:round(x))\n\nsns.distplot(x[x[\"ytest_pred_abs\"]==1],color='r',kde=True)\nsns.distplot(x[x[\"ytest_pred_abs\"]==0],color='g',kde=True)\n\nObservation: From above distplot, it's clear that the model developed is able to segregate patients likely to get prone to heart-attack and not prone to heart-attack, to a large extent.\n\nx_train,x_test,y_train,y_test=train_test_split(dtset3[dtset3.columns.difference([\"output\"])],dtset[\"output\"],test_size=0.3,\n                                               random_state=12345)\n\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\n\ny_logreg_pred=logreg.predict(x_test)\n\nprint('Test accuracy: ', accuracy_score(y_test,y_logreg_pred))\nprint(\"Precision:\",precision_score(y_test,y_logreg_pred))\nprint(\"Precision:\",recall_score(y_test,y_logreg_pred))\nprint(\"ROC score:\",roc_auc_score(y_test,y_logreg_pred))\n\npred_probability=pd.DataFrame(logreg.predict_proba(x_test)).rename(columns={0:'zero',1:'one'})\nroc_table=pd.concat([pd.DataFrame(y_test).reset_index(drop=True),pd.DataFrame(y_logreg_pred),pred_probability],axis=1).rename(columns={0:'predicted','output':'actual'})\nroc_table\n\nfpr,tpr,threshold=roc_curve(roc_table[\"actual\"],roc_table[\"one\"],drop_intermediate=False)\n\nauc_score=roc_auc_score(y_test,y_logreg_pred)\nplt.figure(figsize=(6,4))\nplt.plot(fpr,tpr,'b',label='ROC Curve (area = %0.2f)'%auc_score)\nplt.legend(loc = 'lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nTPR=tpr[(np.abs(tpr-0.82).argmin())]\ncutoff = threshold[np.abs(tpr-0.8).argmin()]\nprint(TPR)\nprint(cutoff)\n\nroc_table[\"new_labels\"]=roc_table[\"one\"].apply(lambda x:1 if x>0.6093127021234298 else 0)\nroc_table\n\naccuracy_score(roc_table[\"actual\"],roc_table[\"new_labels\"])\n\nObservation: Even after changing the cut-off there isn't any significant improvement in ROC score of model. Now other algorithms will be implemented\n\nXG~Boost\n\npargrid_rf={\"n_estimators\":[10,20,30,50,70,80,90,110,130,150],\"learning_rate\":[10**x for x in range (-5,1)]}\ngscv_xgbm=GridSearchCV(estimator=XGBClassifier(),param_grid=pargrid_rf,cv=10,verbose=True,n_jobs=-1)\ngscv_result=gscv_xgbm.fit(x_train,y_train)\ngscv_result.best_params_\n\nxgbm=gscv_xgbm.best_estimator_\nt=xgbm.fit(x_train,y_train)\n\ngscv_result.best_score_\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}