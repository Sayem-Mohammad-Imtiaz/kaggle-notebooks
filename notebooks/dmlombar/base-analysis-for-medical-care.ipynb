{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nThis notebook is offered as a solution to the question 'What has been published about medical care'\n\nThe approach is to use ScispaCy with BERT to make embeddings of sentences and paragraphs, then use cosine similarity to find papers that match the topic. Autogenerated summaries are included for highly relevant papers.\n\nWe are given an overall topic, a paragraph explaination of the topic, and a list of specific sub-topics to answer. The dataset is ~47K papers as of now, and includes all the text, as well as author and reference information. \n\n# Details of Approach\n\n1. Read in the metadata file. Filter out any duplicates, or papers that are missing title/abstract\n2. use ScispaCy to create an embedding for each of the titles. Use the average of the word embeddings provided by Scispacy\n3. Repeat '2' for the abstract\n4. Repeat '2' for the topic description. \n5. Use cosine similarity to score the similarity of all titles and abstracts with the topic description\n6. Split the data between Cov19 and non-Cov19 papers, and save the 1000 most relevant papers from each.\n7. Load the body text for the 2000 chosen papers.\n8. Some papers are missing the body text. Attempt to use the doi url to mine the text from the web\n9. Use BERT and the sentence-transformation package to make sentence embeddings for every sentence in the body text\n\n**Then for each subtopic**\n\n10. Use keywords of the subtopic to filter out irrelevant papers. Keywords can be autogenerated with ScispaCy or user specified.\n11. Use cosine similarity between each of the body text sentences and the sub-topic sentence.\n12. Save papers that have a large number of sentences with high similarity scores. \n13. For each saved paper, look at the results section and pull out the two most relevant senteces to the subtopic. If results is missing, use the abstract instead.\n14. Display results"},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\n### Thanks Kaggle user xhulu for their work on loading and processing the json files\n### Thanks Dr. Levine at Accenture for their work on mining missing text data from websites. Sadly he forgot his Kaggle username!"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"!pip install scispacy\n!pip install -U sentence-transformers\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import sys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport json\nfrom glob import glob\nimport gc\n\nimport scispacy\nimport spacy\nimport en_ner_bc5cdr_md\nfrom bs4 import BeautifulSoup\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom wordcloud import WordCloud\n\npd.set_option('max_columns', 100)\npd.set_option('max_colwidth',200)\nfrom IPython.core.display import display","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"#load the scispacy model relevant to diseases\nnlp = spacy.load('en_ner_bc5cdr_md')\n\n#load bert sentence transformer\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#threshold for scores matching to general topic\nMATCH_LIMIT = 1000\n\n#threshold for scores matching the subtopics\nSUBMATCH_THRESHOLD = 0.85","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def json_reader(file):\n    #takes a json file, processes the body, ref, and bib data into a dataframe\n    #based off xhlulu's work at https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv\n    with open(file) as f:\n        j = json.load(f)\n        \n    #format the body text so the sections are clear, but it's easy to view the whole thing\n    body_text = '\\n\\n'.join([x['section'] + '\\n\\n' + x['text'] for n,x in enumerate(j['body_text'])])\n\n    df = pd.DataFrame(index=[0], data={'body_text':body_text, \n                                            'paper_id': j['paper_id']})\n    \n    return df\n\n\ndef parse_folder(data_folder):\n    filelist = glob('/kaggle/input/CORD-19-research-challenge/{0}/{0}/*'.format(data_folder))\n    filelist.sort()\n    print('{} has {} files'.format(data_folder, len(filelist)))\n\n    df_ls=[]\n    for n,file in enumerate(filelist):\n        if n%1000==0:\n            print(n,file[-46:])\n        df = json_reader(file)\n        df_ls.append(df)\n    return pd.concat(df_ls)\n\n\ndef load_meta():\n    meta = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\n    meta.rename(columns={'sha':'paper_id'}, inplace=True)\n    return meta\n\n\n#go through each of the four folders of json files and put everything into one dataframe\n#takes around 3-4min to complete\ndef combine_datasets():\n    df_ls = []\n    for folder in ['comm_use_subset', 'noncomm_use_subset', 'custom_license', 'biorxiv_medrxiv']:\n        t = parse_folder(folder)\n        df_ls.append(t)\n    df = pd.concat(df_ls)\n    \n    meta = load_meta()\n    df = meta.merge(df, on='paper_id', how='left')\n    return df\n\n\ndef get_doc_vec(tokens):\n    #combine word embeddings from a document into a single document vector\n    #filter out any stop words like 'the', and remove any punction/numbers\n    w_all = np.zeros(tokens[0].vector.shape)\n    n=0\n    for w in tokens:\n        if (not w.is_stop) and (len(w)>1) and (not w.is_punct) and (not w.is_digit):\n            w_all += w.vector\n            n+=1\n    return (w_all / n) if n>0 else np.zeros(tokens[0].vector.shape)\n\n\ndef process_all_docs(col,id_col):\n    vecs = {}\n    for n,row in df.iterrows():\n        if n%5000==0:\n            print(n)\n        if isinstance(row[col], str)==False:\n            print(row[col])\n        if len(row[col]) > 0:\n            vecs[row[id_col]] = get_doc_vec(nlp(row[col]))\n    return vecs\n\n\ndef get_matching_papers(df, q_str, sent_df, keyword_list = []):\n    q_nlp = nlp(q_str)\n    \n    #use nouns and objects from the question to find keywords and phrases\n    if len(keyword_list) == 0:\n        #print('keywords not provided, created some from subjects and objects of question')\n        noun_ls = []\n        for noun in q_nlp.noun_chunks:\n            if ('obj' in noun.root.dep_ or 'subj' in noun.root.dep_ or noun.root.dep_ == 'appos') and len(str(noun.root)) > 1 and noun.root.is_stop == False:\n                noun_ls.append(str(noun.root).lower())\n\n        #also use any entities found in the text. Don't take the root of these\n        noun_ls += [x.text for x in q_nlp.ents]\n        keyword_list = list(set(noun_ls))\n        #print('auto keywords are: {}'.format(keyword_list))\n\n    key_condition = df['text'].str.contains(r'|'.join(keyword_list))\n\n    #get similarity to all available sentences and make a dataframe with the sentences\n    sent_sims = cosine_similarity(sent_df['vecs'].tolist(), model.encode([q_str], show_progress_bar=False))\n    sent_df['score'] = sent_sims.max(axis=1)\n    \n    #filter out sentences that don't belong to papers which include the keywords\n    sent_df = sent_df[sent_df['title'].isin(df[key_condition]['title'])]\n    \n    #filter out sentences with a low match score\n    sent_df = sent_df[sent_df['score'] > SUBMATCH_THRESHOLD * sent_df['score'].max()]\n\n    #sort by papers with a high number of relevant sentences and return the results\n    return sent_df.groupby('title').agg({'sent':'count', 'score':'mean'}).sort_values(by='sent', ascending=False).reset_index().rename(columns={'sent':'relevant sentences'})\n\n\ndef get_text(url,abstract=False,body=True,bib=False):\n    '''Returns the full text of a paper, given the source html, provided the paper is in the rough format of the Wiley Online Library\n    Ex: https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24357\n    abstract = True will return abstract as part of the text\n    body = True will return the body of the paper as part of the text\n    bib = True will return the bibliography as part of the text\n\n    Author: Aaron Levine\n    '''\n    text = ''\n    try:\n        html_text = !wget -qO- --timeout=60 $url\n        soup = BeautifulSoup('\\n'.join(html_text),'html.parser')\n        if abstract:\n            abstract_txt = '\\n'.join([x.text for x in soup.find('div',{'class':'abstract-group'}).findChildren(recursive=False)])\n            text += abstract_txt\n        if body:\n            body_txt = '\\n'.join([x.text for x in soup.find('section',{'class':'article-section article-section__full'}).findChildren(recursive=False) if True not in [tag.has_attr('data-bib-id') for tag in x.find_all()]])\n            text += body_txt\n        if bib:\n            bib_txt = '\\n'.join([x.text for x in soup.find('section',{'class':'article-section article-section__full'}).findChildren(recursive=False) if True in [tag.has_attr('data-bib-id') for tag in x.find_all()]])\n            text += bib_txt\n    except:\n        print('failed to load paper')\n\n    if len(text) > 0:\n        print('found the paper!')\n    return text\n\n\ndef get_summary(df, q, title):\n    if df[df['title'] == title]['body_text'].isna().values.item():\n        text = df[df['title'] == title]['abstract'].values.item() #abstract might still have a results section, some parsers are weird\n    else:\n        text = df[df['title'] == title]['body_text'].values.item()\n\n    if isinstance(text,float):\n        print(text, title)\n    sent_ls = []\n    summary_text = ''\n    if '\\n\\nResults\\n\\n' in text:\n        #grab everything from first 'result section onward'\n        results = text.split('\\n\\nResults\\n\\n')[1:]\n\n        #cut off everything after results\n        results[-1] = results[-1].split('\\n\\n')[0]\n        results = ' '.join(results)\n\n        #get the sentences\n        sents = nlp(results).sents\n        sent_ls = []\n        for sent in sents:\n            row = sent_df[(sent_df['title'] == title) & (sent_df['sent'].str.contains(sent.text, case=False, regex=False))]\n            sent_ls.append(row)\n    else: #no results, try just using the abstract\n        results = df[df['title'] == title]['abstract'].values.item()\n\n        #get the sentences\n        sents = nlp(results).sents\n        sent_ls = []\n        for sent in sents:\n            row = sent_df[(sent_df['title'] == title) & (sent_df['sent'].str.contains(sent.text, case=False, regex=False))]\n            sent_ls.append(row)\n\n    if len(sent_ls) > 0:\n        result_df = pd.concat(sent_ls)\n        result_df.drop_duplicates(subset='sent', inplace=True)\n        sent_sims = cosine_similarity(result_df['vecs'].tolist(), model.encode([q], show_progress_bar=False))\n        result_df['score'] = sent_sims.max(axis=1)\n        summary_text = ' '.join(result_df.sort_values(by='score', ascending=False).head(2)['sent'].tolist())\n    return summary_text\n\n\ndef get_results(q, keyword_list=None):\n    color_indexer = {'1':9, '2':9}\n    text_indexer = {'1':0, '2':0}\n    cm = sns.light_palette(\"green\", as_cmap=True)\n    cm2 = sns.light_palette(\"blue\", as_cmap=True)\n    cm_list = [cm(i*0.1) for i in range(0,10)]\n    cm2_list = [cm2(i*0.1) for i in range(0,10)]\n    \n    title_match = get_matching_papers(match_df, q, sent_df, keyword_list=kw)[['title', 'relevant sentences']].head(10)\n    title_nocov_match = get_matching_papers(match_nocov_df, q, sent_df, keyword_list=kw)[['title', 'relevant sentences']].head(10)\n    \n    title_match['is_covid'] = True\n    title_nocov_match['is_covid'] = False\n    title_match = title_match.merge(match_df[['title','publish_time']], on='title', how='left')\n    title_nocov_match = title_nocov_match.merge(match_nocov_df[['title','publish_time']], on='title', how='left')\n    title_match['publish_time'] = title_match['publish_time'].str[:4].astype(float)\n    title_nocov_match['publish_time'] = title_nocov_match['publish_time'].str[:4].astype(float)\n    \n    all_match = pd.concat([title_match,title_nocov_match]).reset_index(drop=True).sort_values(by='relevant sentences', ascending=False)\n    all_match['summary'] = all_match.apply(lambda x: get_summary(match_df, q, x['title']) if x['is_covid'] else get_summary(match_nocov_df, q, x['title']), axis=1)\n    all_match['display_score'] = all_match['relevant sentences'] + all_match['is_covid'] - (2020 - all_match['publish_time'])\n    all_match = all_match.sort_values(by='display_score', ascending=False)\n    display(all_match[['is_covid','relevant sentences', 'publish_time', 'title', 'summary']].style.hide_index()\n            .apply(color_rows, cm=cm_list, cm2=cm2_list, d_idx=color_indexer,axis=1, subset=['is_covid', 'relevant sentences', 'publish_time'])\n            .apply(color_text, d_idx=text_indexer, axis=1, subset=['is_covid', 'relevant sentences', 'publish_time']))\n    \n    \ndef color_rows(s, cm, cm2, d_idx):\n    #takes in a row from a dataframe and applies necessary colors for display\n    if s['is_covid']:\n        style = ['background-color: {}'.format(cmap_to_hex(cm[d_idx['1']]))]*s.shape[0]\n        d_idx['1'] -= 1\n        return style\n    else:\n        style = ['background-color: {}'.format(cmap_to_hex(cm2[d_idx['2']]))]*s.shape[0]\n        d_idx['2'] -= 1\n        return style\ndef color_text(s, d_idx):\n    if s['is_covid']:\n        if d_idx['1'] < 5:\n            style = ['color: white']*s.shape[0]\n        else:\n            style = ['color: black']*s.shape[0]\n        d_idx['1'] += 1\n        return style\n    else:\n        if d_idx['2'] < 5:\n            style = ['color: white']*s.shape[0]\n        else:\n            style = ['color: black']*s.shape[0]\n        d_idx['2'] += 1\n        return style\n    \n    \ndef cmap_to_hex(rgb_color):\n    [r, g, b] = [int(x*255) for x in rgb_color[:3]]\n \n    r = hex(r).lstrip('0x')\n    g = hex(g).lstrip('0x')\n    b = hex(b).lstrip('0x')\n    # re-write '7' to '07'\n    r = (2 - len(r)) * '0' + r\n    g = (2 - len(g)) * '0' + g\n    b = (2 - len(b)) * '0' + b\n \n    hex_color = '#' + r + g + b\n    return hex_color","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading and Cleaning\n#### Only need to load the meta data at frist, find relevant papers based on abstract and title\n#### Handle Duplicate papers"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df = load_meta()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#drop duplicates and any publications missing abstracts\ndf.drop_duplicates(['title'], inplace=True)\ndf.dropna(subset=['abstract','title'], inplace=True)\n\n#want an identifier for every row, but paper_id is missing from a lot of them\n#cord_uid might work as well, but that was added after I started\ndf = df.reset_index()\ndf.rename(columns={'index':'uid'},inplace=True)\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic Definition\n\nFirst use the topic description to find relevant papers. Alternatively, could just load the body text and do sentence embeddings on all papers, but that drastically increases runtime and memory consumption.\n\nFor the overall similarity score, take the mean of the title and abstract similarity scores. This helps promote papers that are highly focused on the topic. Using the max instead of mean would be another option, but this leads to some papers that only briefly mention the topic without directly addressing it."},{"metadata":{},"cell_type":"markdown","source":"*\"What has been published about medical care? What has been published concerning surge capacity and nursing homes? What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment? What has been published concerning alternative methods to advise on disease management? What has been published concerning processes of care? What do we know about the clinical characterization and management of the virus?\"*"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"covid_selection = (df['abstract'].str.contains('covid-19',case=False)) | (df['title'].str.contains('covid-19',case=False)) | (df['abstract'].str.contains('sars-cov-2',case=False)) | (df['title'].str.contains('sars-cov-2',case=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"q_str = 'What has been published about medical care? What has been published concerning surge capacity and nursing homes? What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment? What has been published concerning alternative methods to advise on disease management? What has been published concerning processes of care? What do we know about the clinical characterization and management of the virus?'\nq_vec = [get_doc_vec(nlp(q_str))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Find Relevant Papers\n# Define title and abstract document vectors by averaging the word vectors, after filtering stop words. \n# Next, calculate a similarity score of each document vector to the topic shown above. \n# Then average the title and abstract similarity scores. \n# Another option would be to combine the title and abstract, and compute a single document vector. \n# However, I like keeping them separate as it gives the title more weight in the final score\n\nabstract_vectors = process_all_docs('abstract', 'uid')\ntitle_vectors = process_all_docs('title', 'uid')\n\nabstract_vals = list(abstract_vectors.values())\nabstarct_vals = [v for v in abstract_vals if all(v==0)==False]\n\ntitle_vals = list(title_vectors.values())\ntitle_vals = [v for v in title_vals if all(v==0)==False]\n\nabstract_sims = cosine_similarity(abstract_vals, q_vec)\ntitle_sims = cosine_similarity(title_vals, q_vec)\n\nsim_df = pd.concat([pd.Series(dict(zip(abstract_vectors.keys(), abstract_sims[:,0]))),\n                    pd.Series(dict(zip(title_vectors.keys(), title_sims[:,0])))], axis=1).reset_index().rename(columns={'index':'uid'})\nsim_df.rename(columns={0:'abstract_score', 1:'title_score'},inplace=True)\nsim_df['mean_score'] = sim_df[['abstract_score', 'title_score']].mean(axis=1)\n\n#merge the scores into the dataframe\ndf = df.merge(sim_df,on='uid',how='left')\n\n#find papers with a high match score to the topic. \nmatch_df = df[covid_selection].sort_values(by='mean_score', ascending=False).head(MATCH_LIMIT)\nmatch_nocov_df = df[covid_selection==False].sort_values(by='mean_score', ascending=False).head(MATCH_LIMIT)\n#match_df = df[((df['mean_score'] > df['mean_score'].max()*MATCH_THRESHOLD))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Body Text\n#### Use BERT sentence embeddings to find more precise matches to specific questions"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Load Body Text\n# First load the body text for all matched papers. \n#Next, compute a vector for each of the sentences in each papers abstract and body_text. \n#Then define a similarity score for each sentence to a given question. Use SUBMATCH_THRESHOLD to define which sentences are highly relevant.\n# Also parse each question for a set of keywords. Filter out any papers that don't contain any of these words\n\ndf_ls = []\nfor data_folder in ['comm_use_subset', 'noncomm_use_subset', 'custom_license', 'biorxiv_medrxiv']:\n    filelist = glob('/kaggle/input/CORD-19-research-challenge/{0}/{0}/pdf_json/*'.format(data_folder))\n    filelist.sort()\n    print('{} has {} files'.format(data_folder, len(filelist)))\n\n    for n,file in enumerate(filelist):\n        if n%1000==0:\n            print(n,file[-46:])\n        t = json_reader(file)\n        t = t[(t['paper_id'].isin(match_df['paper_id'])) | (t['paper_id'].isin(match_nocov_df['paper_id']))]\n        df_ls.append(t[['paper_id','body_text']])\ndf_body = pd.concat(df_ls)\ndel df_ls\ngc.collect()\nmatch_df = match_df.merge(df_body, on='paper_id', how='left')\nmatch_nocov_df = match_nocov_df.merge(df_body, on='paper_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Missing Body Text\n #### Some articles missing the body text can be filled in. The helper functions above contain a function for mining the url for the paper text. It doesn't work for every format, but it does fill in some of them.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Missng Text Before: {}'.format(match_df['body_text'].fillna('').apply(lambda x: len(x)==0).sum() + match_nocov_df['body_text'].fillna('').apply(lambda x: len(x)==0).sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"missed_text = match_df[match_df['body_text'].fillna('').apply(lambda x: len(x)==0)]['url'].apply(lambda x: get_text(x))\nmatch_df.loc[match_df['has_full_text']==False,'body_text'] = missed_text\n\nmissed_text = match_nocov_df[match_nocov_df['body_text'].fillna('').apply(lambda x: len(x)==0)]['url'].apply(lambda x: get_text(x))\nmatch_nocov_df.loc[match_nocov_df['has_full_text']==False,'body_text'] = missed_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Missing Text After: {}'.format(match_df['body_text'].fillna('').apply(lambda x: len(x)==0).sum() + match_nocov_df['body_text'].fillna('').apply(lambda x: len(x)==0).sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Sentence Vectors"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#make all text lower case to help with matching keywords\nmatch_df['text'] = (match_df['abstract'] + match_df['body_text'].fillna('')).str.lower()\nmatch_nocov_df['text'] = (match_nocov_df['abstract'] + match_nocov_df['body_text'].fillna('')).str.lower()\n\n#make sentence vectors from the abstract and body text\n#use spacy to split documents by sentences\n#try using BERT to make embeddings of sentences\nsent_data = {'sent':[], 'title':[], 'uid':[]}\nfor n,row in match_df.iterrows():\n    if n%100==0:\n        print(n)\n    sents = nlp(row['text']).sents\n    for s in sents:\n        if len(s) > 0:\n            #new_vec = get_doc_vec(nlp(str(s)))\n            #if all(new_vec==0)==False:\n                #sent_vecs.append(new_vec)\n            sent_data['sent'].append(str(s))\n            sent_data['title'].append(row['title'])\n            sent_data['uid'].append(row['uid'])\n            \nfor n,row in match_nocov_df.iterrows():\n    if n%100==0:\n        print(n)\n    sents = nlp(row['text']).sents\n    for s in sents:\n        if len(s) > 0:\n            #new_vec = get_doc_vec(nlp(str(s)))\n            #if all(new_vec==0)==False:\n                #sent_vecs.append(new_vec)\n            sent_data['sent'].append(str(s))\n            sent_data['title'].append(row['title'])\n            sent_data['uid'].append(row['uid'])\n            \n\nprint(len(sent_data['sent']))\nsent_df = pd.DataFrame(sent_data)\nprint('making sentence embeddings with BERT')\nsent_df['vecs'] = model.encode(sent_df['sent'].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save or Load Data Here"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# match_df.to_csv('match_df.csv')\n# match_nocov_df.to_csv('match_nocov_df.csv')\n# sent_df.to_pickle('sent_vecs_bert_all.pkl')\n\n#add kernel output to data tab, then load the match_df file\nmatch_df = pd.read_csv('/kaggle/input/base-analysis-for-medical-care/match_df.csv')\nsent_df = pd.read_pickle('/kaggle/input/base-analysis-for-medical-care/sent_vecs_bert_all.pkl')\nmatch_nocov_df = pd.read_csv('/kaggle/input/base-analysis-for-medical-care/match_nocov_df.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{},"cell_type":"markdown","source":"## Resources to support skilled nursing facilities and long term care facilities"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Resources to support skilled nursing facilities and long term care facilities'\nkw = ['nurse', 'long term care', 'facilities']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mobilization of surge medical staff to address shortages in overwhelmed communities"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Mobilization of surge medical staff to address shortages in overwhelmed communities'\nkw = ['mobilization', 'staff', 'overwhelmed']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies'\nkw = ['ards', 'acute respiratory distress syndrome']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients'\nkw = ['ecmo']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outcomes data for COVID-19 after mechanical ventilation adjusted for age"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Outcomes data for COVID-19 after mechanical ventilation adjusted for age'\nkw = ['mechanical ventilation']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest'\nkw = ['cardica arrest', 'cadiomyopathy','extrapulmonary']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level'\nkw = ['clia', 'care level', 'eua', 'care']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks'\nkw = ['elastomeric respirator']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best telemedicine practices, barriers and facilitators, and specific actions to remove/expand them within and across state boundaries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Best telemedicine practices, barriers and facilitators, and specific actions to remove/expand them within and across state boundaries'\nkw = ['telemedicine']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Guidance on the simple things people can do at home to take care of sick people and manage disease"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Guidance on the simple things people can do at home to take care of sick people and manage disease'\nkw = ['home']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Oral medications that might potentially work"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Oral medications that might potentially work'\nkw = ['oral']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually'\nkw = ['artificial intelligence']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes'\nkw = [] #autogenerate\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials'\nkw = ['history']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials'\nkw = ['outcome', 'trials']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q = 'Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)'\nkw = ['interventions']\nget_results(q,kw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":4}