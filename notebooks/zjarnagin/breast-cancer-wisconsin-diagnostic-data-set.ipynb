{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Setup\nFirst, we must setup our environment. We import various libraries (which you can view in the code cell below) and setup our directories according to the result we get from walking the filenames in our supplied files."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # splitting our data into training and testing data\nimport seaborn as sns # for creating a correlation heatmap\nimport matplotlib.pyplot as plt # for displaying our heatmap for analysis\nfrom xgboost import XGBClassifier # eventually, we will use an XGBClassifier for our model\nfrom sklearn.metrics import accuracy_score # to score our model\n\n# Input data files are available in the \"../input/\" directory.\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading our Data\nHere, we read our data from the supplied .csv file using Pandas and representing it as a Pandas dataframe.\n\nWe are interested in predicting a diagnosis based on cell features, so we assign a 'y' variable to the diagnosis column.\n\nIt is worth noting that when using the 'id' feature as an index col, we get a column full of NaN entries. We remove this column as it provides no use to us. We also change some Pandas options. This is intended so that whenever we call 'head', we can see all the features and column names without truncation.\n\nWe then replace the diagnoses with 1 for malignant (original represented as an 'M') and 0 for benign (originally represented as a 'B'). This is useful for when we fit our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the dataset\nX_full = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv', index_col='id')\n\n# Assign y to the diagnosis column\ny = X_full.diagnosis\n\n# Assigning our index_col to be the column 'id' shifted our data over, leaving a column with all NaN entries.\n# We drop that here\nX = X_full.drop(columns=['Unnamed: 32'])\n\n# Show all values whenever we call head.\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n# If we run .dtypes on our data frame, we notice that all columns, aside from the diagnosis being a string, our integers.\n\n# We replace a malignant diagnosis with 1, and benign with 0\nX['diagnosis'].replace('M', 1, inplace=True)\nX['diagnosis'].replace('B', 0, inplace=True)\ny.replace('M', 1, inplace=True)\ny.replace('B', 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis\nTo avoide overfitting, we find the features which seem to have a low impact on the diagnosis. We do this by using a heatmap correlation chart.\n\nThe figure we get will display the correlation one attribute has on another. We are interested in which attributes do (or don't) affect the diagnosis column. We analyze the results on the figure, and ignore the features which have less than an absolute value of 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, we use the seaborn correlation heatmap to visualize the correlatons of features in our dataset on one another.\n# Using the filter method, we will drop features which have an absolute value of less than 0.5 on the feature 'diagnosis'\n\n# Setting up and displaying our heatmap correlation\nplt.figure(figsize=(20,20))\ncor = X.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds, fmt='.2f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying our Analysis\nNow, we run some code to do exactly what we said we would do above: ignore the features which have a low impact on the diagnosis column.\n\nWe also split our data into training and testing data to both train and fit our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep features which have a med-high correlation on the diagnosis\nfeatures = ['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', \n            'concave points_mean', 'radius_se', 'perimeter_se', 'area_se', 'radius_worst', 'perimeter_worst',\n           'area_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst']\nX = X[features]\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating and Testing a Model\nNow we are ready to create, train, and test a model. We must use a classifying model, as the predictions are to either be a 0 (for benign) and 1 (for malignant). We assess the accuracy of this model using SKLearn's \"accuracy_score\" function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use an XGBoostClassifier, and score the model using SKLearn Accuracy Score\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\naccuracy_score(y_valid, preds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}