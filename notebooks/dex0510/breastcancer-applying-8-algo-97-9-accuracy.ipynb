{"cells":[{"metadata":{"id":"tIAk8x9FCiRx","outputId":"570784b7-9f34-469c-d84d-073a493932df","trusted":true},"cell_type":"code","source":"!pip install xgboost\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport keras \nfrom keras.models import Sequential \nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"id":"R9j1ai7fCzeB","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"hBHJLf85C43I","outputId":"691060dd-990c-4c07-95ae-54249ae5f0d7","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"y0oOYnjUDbw2","outputId":"09e80e0d-f739-4091-fc5b-ba26a65afe23","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"eZBJ3pRTDxWE","outputId":"e2e02edd-5ba4-4064-85f0-b42e01495b5a","trusted":true},"cell_type":"code","source":"#count the number of empty values in each column\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"Q3TtlvJfEGMc","outputId":"08db6d6a-58e7-4a27-fc69-634b6ef04b39","trusted":true},"cell_type":"code","source":"#drop NA column\ndf.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"9lwHcdAQEQ-Q","outputId":"8766a14b-c1c8-43fe-90f6-7ae54cf8e479","trusted":true},"cell_type":"code","source":"#Get the count of number of Malignant(M) or Beglin(B) cells\ndf['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"DAH9fhcFFBCA","outputId":"a480f6d4-4e48-4aea-d621-cac28768da66","trusted":true},"cell_type":"code","source":"#Visualize\nsns.countplot(df['diagnosis'], label = 'Count')","execution_count":null,"outputs":[]},{"metadata":{"id":"Epq4rhTBK-bX","outputId":"86596786-53f5-4ce2-e200-b5d0c742ae1a","trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"7k59qcnKdg_O","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\ndf.iloc[:,1] = labelencoder_Y.fit_transform(df.iloc[:,1].values)","execution_count":null,"outputs":[]},{"metadata":{"id":"V2SLbjLVkQB-","outputId":"a617f497-3bfd-4163-f819-efa057fd70e8","trusted":true},"cell_type":"code","source":"df.iloc[:,1]","execution_count":null,"outputs":[]},{"metadata":{"id":"5KkPIlcQkYF-","outputId":"4366e286-de33-4547-ff15-fd4ce0e772c8","trusted":true},"cell_type":"code","source":"#Create a pair plot\nsns.pairplot(df.iloc[:,1:8], hue='diagnosis')","execution_count":null,"outputs":[]},{"metadata":{"id":"VfC8k60dlfZT","outputId":"c1ea9166-a7ec-4ada-c534-165c1cd59c49","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"xv-xUbVjnzEX","outputId":"486886ee-72f2-485e-902a-8da46aa637fd","trusted":true},"cell_type":"code","source":"#Correlation of the columns\ndf.corr()","execution_count":null,"outputs":[]},{"metadata":{"id":"SHwOVh7doKsH","outputId":"d0bc177f-988a-4b12-f67c-b0cc64f845cd","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (40,40))\nsns.heatmap(df.corr(), annot=True , fmt = '.0%')","execution_count":null,"outputs":[]},{"metadata":{"id":"mx9y5XYlppdw","trusted":true},"cell_type":"code","source":"#Split data into independent (X) and dependent(Y) columns\n\nX = df.iloc[:,2:31].values\nY = df.iloc[:,1].values","execution_count":null,"outputs":[]},{"metadata":{"id":"_InJydm1uVup","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"tQ2auSkEvWfz","outputId":"af1baacf-5f48-40d8-aaa4-a9cddbffdc6e","trusted":true},"cell_type":"code","source":"#Scale the data (Feature Scaling)\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nsc.fit_transform(X_train)\nsc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Cm6SHpRWwigV","trusted":true},"cell_type":"code","source":"M = {0:'Logistic Regression', 1:'Decision Tree', 2:'RandomForest', 3:'KNN', 4:'SVM', 5:'Naive Bayes', 6:'XGBoost'}\n#Create a function for the models\ndef models(X_train,Y_train):\n\n  # Logistic  Regression\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train,Y_train)\n\n  # Decision Tree\n  from sklearn.tree import DecisionTreeClassifier\n  tree = DecisionTreeClassifier(random_state = 0)\n  tree.fit(X_train,Y_train)\n\n  # RandomFoest\n  from sklearn.ensemble import RandomForestClassifier\n  forest = RandomForestClassifier(n_estimators = 10 , criterion = 'entropy', random_state = 0)\n  forest.fit(X_train,Y_train)\n  \n  # KNN\n  from sklearn.neighbors import KNeighborsClassifier\n  knn = KNeighborsClassifier(n_neighbors=5) \n  knn.fit(X_train, Y_train)  \n\n  # \"Support Vector Classifier\" \n  from sklearn.svm import SVC\n  clf = SVC(kernel='linear')\n  clf.fit(X_train, Y_train) \n\n  # Naive Bayes\n  from sklearn.naive_bayes import GaussianNB \n  gnb = GaussianNB() \n  gnb.fit(X_train, Y_train) \n  \n  #XGBoost\n  import xgboost as xgb \n  xg = xgb.XGBClassifier() \n  xg.fit(X_train, Y_train) \n  \n  #Print Model Accuracy\n  print('[0] LogisticRegression Training Accuracy', log.score(X_train,Y_train))\n  print('[1] DecisionTree Training Accuracy', tree.score(X_train,Y_train))\n  print('[2] RandomForest Training Accuracy', forest.score(X_train,Y_train))\n  print('[3] KNN Training Accuracy',knn.score(X_train,Y_train))\n  print('[4] SVM Training Accuracy',clf.score(X_train,Y_train))\n  print('[5] Naive Bayes Training Accuracy',gnb.score(X_train,Y_train))\n  print('[6] XGBoost Training Accuracy',xg.score(X_train,Y_train))\n  return log,tree,forest,knn,clf,gnb,xg","execution_count":null,"outputs":[]},{"metadata":{"id":"7aCZERZbzNZz","outputId":"0f1b6c86-35ab-485b-b24c-ebc00c80effe","trusted":true},"cell_type":"code","source":"#Getting all the models\nmodel = models(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"trpqvI360OO_","outputId":"93980f0e-9302-463c-e884-7af2cbce4aef","trusted":true},"cell_type":"code","source":"#Test model Accuracy from Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nfor i in (M.keys()):\n  print(\"\\n\",M[i])\n  cm = confusion_matrix(Y_test, model[i].predict(X_test))\n  TP = cm[0][0]\n  TN = cm[0][1]\n  FP = cm[1][0]\n  FN = cm[1][1]\n\n  print(\"Training Accuracy\", ((TP + FN)/(TP+FN+TN+FP))*100,\"%\")\n  print(cm)","execution_count":null,"outputs":[]},{"metadata":{"id":"DrQ6K0Sx1N-x","outputId":"405acc09-777a-4d63-f000-20461ace4b6a","trusted":true},"cell_type":"code","source":"#show another way to get metrices of the model\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n\nfor i in (M.keys()):\n  print(\"\\n\",M[i])\n  print(classification_report(Y_test, model[i].predict(X_test)))\n  print(accuracy_score(Y_test, model[i].predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"id":"4TTnFN9OOGKR","outputId":"33047b64-0579-474e-b0a1-7d05b7056bb0","trusted":true},"cell_type":"code","source":"#Artificial Neural Network(ANN) Model\n\nclassifier = Sequential() \nclassifier.add(Dense(activation = \"relu\", input_dim = 29,  \n                     units = 8, kernel_initializer = \"uniform\")) \nclassifier.add(Dense(activation = \"relu\", units = 14,  \n                     kernel_initializer = \"uniform\")) \nclassifier.add(Dense(activation = \"sigmoid\", units = 1,  \n                     kernel_initializer = \"uniform\")) \nclassifier.compile(optimizer = 'adam' , loss = 'binary_crossentropy',  \n                   metrics = ['accuracy'] ) \n\n\n\nclassifier.fit(X_train , Y_train , batch_size = 8 ,epochs = 400 ) \n\n\n\nY_pred = classifier.predict(X_test) \nY_pred = (Y_pred > 0.5) \n\n\ncmann = confusion_matrix(Y_test,Y_pred) \nprint(cmann)\n\n\naccuracy = (cmann[0][0]+cmann[1][1])/(cmann[0][1] + cmann[1][0] +cmann[0][0] +cmann[1][1]) \nprint(accuracy*100) \n","execution_count":null,"outputs":[]},{"metadata":{"id":"m_Vx7sNNcgIU","outputId":"99f73d14-59d7-4f8f-84c9-f1d96a7f7e41","trusted":true},"cell_type":"code","source":"accuracy = (cmann[0][0]+cmann[1][1])/(cmann[0][1] + cmann[1][0] +cmann[0][0] +cmann[1][1])  \nprint(\"Accuracy of ANN is\", accuracy*100, \"%\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}