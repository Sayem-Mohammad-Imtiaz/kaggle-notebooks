{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load essential libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Conv1D, GlobalMaxPooling1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data converted to data frame. A Data frame is a two-dimensional data structure\ndata = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding = 'latin1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop features having NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop unavailable attributes\n\ndata = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename and display first five columns \n\ndata = data.rename(columns ={\"v1\":\"target\", \"v2\":\"text\"})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count number of ham and spam\n\ndata.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting  distribution graph\n\nsns.countplot(x = \"target\", data = data)\ndata.loc[:, 'target'].value_counts()\nplt.title('Distribution of Spam and Ham')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting graph by length of text\n\nham =data[data['target'] == 'ham']['text'].str.len()\nsns.distplot(ham, label='Ham')\nspam = data[data['target'] == 'spam']['text'].str.len()\nsns.distplot(spam, label='Spam')\nplt.title('Distribution by Length')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the data into train and test (80-20)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], test_size = 0.2, random_state = 37)\nprint (\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for counting frequently occurence of spam and ham.\n\ncount1 = Counter(\" \".join(data[data['target']=='ham'][\"text\"]).split()).most_common(10)\ndata1 = pd.DataFrame.from_dict(count1)\ndata1 = data1.rename(columns={0: \"words of ham\", 1 : \"count\"})\n\n#Graph of top 30 words of HAM\ndata1.plot.bar(legend = False, color = 'purple',figsize = (20,15))\ny_pos = np.arange(len(data1[\"words of ham\"]))\nplt.xticks(y_pos, data1[\"words of ham\"])\nplt.title('Top 10 words of ham')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Datasets of natural language are referred to as corpora, and a single set of data annotated with the same specification is called an annotated corpus. Annotated corpora can be used to train ML algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"#for removing the commoner morphological and inflexional endings from words in English.\n#like playing, played, plays--> common root play\ncorpus = []\n#print(data['text'][0])\n#review = re.sub('[^a-zA-Z]', ' ', data['text'][0])\n#review = review.lower()\n#review = review.split()\n#print(review)\n#print()\n\nfor i in range(0, 5572): # for the entire data set 4825+747=5572\n    review = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n    review = review.lower()\n    review = review.split()\n    #Stemming is the process of reducing inflection in words to their root forms such as mapping \n    #a group of words to the same stem even if the stem itself is not a valid word in the Language\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    #Stopwords are the English words which does not add much meaning to a sentence. \n    #They can safely be ignored without sacrificing the meaning of the sentence.\n    review = ' '.join(review)\n    corpus.append(review)\n#print(corpus[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to tokenize a collection of text documents and build a vocabulary of known words\n#use it as follows:\n\n#Create an instance of the CountVectorizer class.\n#Call the fit() function in order to learn a vocabulary from one or more documents.\n#Call the transform() function on one or more documents as needed to encode each as a vector.\ncv = CountVectorizer(max_features = 1500)\ncv.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv = cv.transform(X_train)\nX_train_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_cv = cv.transform(X_test)\nX_test_cv\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating model Naive Bayes Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB(alpha = 0.5)\nmnb.fit(X_train_cv,y_train)\n\ny_mnb = mnb.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Naive Bayes Accuracy: ', accuracy_score( y_mnb , y_test))\nprint('Naive Bayes confusion_matrix: ', confusion_matrix(y_mnb, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='sigmoid', gamma=1.0)\nsvc.fit(X_train_cv,y_train)\ny_svc = svc.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SVM Accuracy: ', accuracy_score( y_svc , y_test))\nprint('SVM confusion_matrix: ', confusion_matrix(y_svc, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=37, random_state=252)\nrfc.fit(X_train_cv,y_train)\ny_rfc = rfc.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Forest Accuracy_score: ',accuracy_score(y_test,y_rfc))\nprint('Random Forest confusion_matrix: ', confusion_matrix(y_rfc, y_test)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes giving the best accuracy"},{"metadata":{},"cell_type":"markdown","source":"# DEEP LEARNING"},{"metadata":{"trusted":true},"cell_type":"code","source":"## For enumeration up to a maximum of 1000\n##The enumerate() function assigns an index to each item in an iterable object \n##that can be used to reference the item later\ntags = data[\"target\"]\ntexts = data[\"text\"]\n\nnum_max = 1000\n\n## Tags make 0 and 1 .....spam or ham\n#LabelEncoder encode labels with a value between 0 and n_classes-1 \n#where n is the number of distinct labels.\n\n#fit_transform is used on training data to scale the data and also learn the scaling parameters.\n# We do that on the training set of data. But then you have to apply the same transformation to your \n#testing set (e.g. in cross-validation), or to newly obtained examples before forecast. \n#But you have to use the exact same two parameters μ and σ (values) that you used for centering \n#the training set.... x'= x-mu/ sigma\nle = LabelEncoder()\ntags = le.fit_transform(tags)\n\n## The process of enumerating words,tokenizer class allows an application to break a string into tokens.\ntok = Tokenizer(num_words=num_max)\ntok.fit_on_texts(texts)\n\n# Number of word counts\n#print(tok.word_docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## A maximum of 100 words and sentences are maintained\nmax_len = 100\ncnn_texts_seq = tok.texts_to_sequences(texts)\nfor i in range(len(cnn_texts_seq)):\n    if(len(cnn_texts_seq[i])>100):\n        print('Word Counts:', len(cnn_texts_seq[i]),'Indeks:',i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## A maximum of 100 words and sentences are maintained\n## The number of words is made from 100. Missing words are written to 0.\n##Pad_sequences-This function transforms a list (of length num_samples) \n##of sequences (lists of integers) into a 2D Numpy array of shape \n##(num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided,\n##or the length of the longest sequence in the list.\n\ncnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n\n## There are 23 words in the second sentence.\n## All words are indexed\n## The most used 1000 words are taken.\n## Less used words are removed.\n## If the number of words is less than 100, 0 is added for padding. \n## If the number of words is greater than 100, then they are deleted\nprint('***************************************************')\nprint(texts[2156])\nprint()\nprint(cnn_texts_mat[2156])\nprint('***************************************************')\nprint('hey index:',tok.word_index['hey'])\nprint('cutie index:',tok.word_index['cutie'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Number of words 101\n## \n## There are 100 words left.\nprint('***************************************************')\nprint(texts[2156])\nprint('***************************************************')\nprint(cnn_texts_mat[2156])\nprint('***************************************************')\n\nprint('hey index:',tok.word_index['hey'])#, 'WALES index:',tok.word_index['WALES')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#conv1d\n#The model extracts features from sequences data and maps the internal features of the sequence.\n#1st laye - sequential - A Sequential model is appropriate for a plain \n#stack of layers where each layer has exactly one input tensor and one output tensor.\n\nmodel = Sequential()\nmodel.add(Embedding(1000,20,input_length=max_len))\n#embedding layer takes ids per row and convert it into one hot encoding\n# with a vocabulary of 1000 (0- 999),a vector space of 20 dimensions in which words will be embedded,\n#and input documents that have max_len i.e. 100.\nmodel.add(Dropout(0.2))\n#dropout- ignoring neurons during the training phase, there will be no\n#forard or backword pass by these neurons\nmodel.add(Conv1D(64,5,padding='valid',activation='relu',strides=1))\n# A 1D CNN is very effective for deriving features from a fixed-length segment of the overall dataset,\n#where it is not so important where the feature is located in the segment.\n#ReLU-Rectified lines activation Functio\n#ReLU for short is a piecewise linear function that will output the input directly\n#if it is positive, otherwise, it will output zero.\n#overcomes the vanishing gradient problem, allowing models to learn faster and perform better.\nmodel.add(GlobalMaxPooling1D())\n#aking the maximum value over the time dimension\nmodel.add(Dense(128,activation='relu'))\n#A Dense layer feeds all outputs from the previous layer to\n#all its neurons, each neuron providing one output to the next layer.\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\n#sigmoid  The input to the function is transformed into a value between 0.0 and 1.0.\nmodel.summary() #Prints a string summary of the network.\nmodel.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n#Binary crossentropy is a loss function that is used in binary classification tasks. \n#These are tasks that answer a question with only two choices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(cnn_texts_mat,tags,batch_size=32,epochs=10,verbose=1,validation_split=0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nepochs = range(1, 11)\nacc = history.history['acc']\nval_acc = history.history['val_acc']\n\nplt.plot(epochs, acc, 'b+', label='Acc')\nplt.plot(epochs, val_acc, 'bo', label='Val Acc')#validation accuracy\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EPOCH 10 - val_loss: 0.0623 - val_acc: 0.9857****"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}