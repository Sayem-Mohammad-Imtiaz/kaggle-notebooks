{"cells":[{"metadata":{},"cell_type":"markdown","source":"Note: This Red-Wine Analysis using both Supervised Learning & Unsupervised Learning models is a group project which I had done together with my team-mates. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import libraries \n\n#structures\nimport numpy as np\nimport pandas as pd\n\n#visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#get model duration\nimport time\nfrom datetime import date\n\n#analysis\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Description of Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load dataset\ndata = '../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv'\ndataset = pd.read_csv(data)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The red wine data consists of 1599 rows and 12 columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing data\ndataset.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for unreasonable data\ndataset.applymap(np.isreal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns_plot = sns.pairplot(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns_plot = sns.distplot(dataset['quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create new column; \"quality_class\"\ndataset['quality_class'] = dataset['quality'].apply(lambda value: 1 if value < 5 else 2 if value < 7 else 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set x and y\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = dataset.iloc[:,0:11]\ny = dataset['quality_class']\n\n#stadardize data\nX_scaled = StandardScaler().fit_transform(X)\n\n#get feature names\nX_columns = dataset.columns[:11]\n\n#split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Feature extraction: Principal component analysis\n2. Feature selection: Pearson's correlation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Principal component analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=6)\npc_X = pca.fit_transform(X_scaled)\npc_columns = ['pc1','pc2','pc3','pc4','pc5','pc6']\nprint(pca.explained_variance_ratio_.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train and test data for pca\nXpc_train, Xpc_test, ypc_train, ypc_test = train_test_split(pc_X, y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Pearson's Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlation map\ncorr_mat=dataset.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualise data\nplt.figure(figsize=(13,5))\nsns_plot=sns.heatmap(data=corr_mat, annot=True, cmap='GnBu')\nplt.show()\n\n#save file\n#sns_plot.get_figure().savefig('corr_mat.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a correlation of 0.6 to -0.5 as benchmark, a correlation matrix has been created to sieve out features that are highly correlated to the quality of red wine. Our results show that all features are within the acceptable range of 0.6 to -0.5.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the heatmap, it can be seen that most features are weakly correlated to the quality of wine the exception of alcohol (0.48) which is a moderate correlation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Direction of relationship** <br>\nAcidity (-0.39), chlorides (-0.13), free sulfur dioxide (-0.051), total sulfur dioxide (-0.19), density (-0.17) and PH (-0.058) are negatively correlated to the quality of wine; as these variables decrease, the quality of wine will increase vice versa. <br> <br>\n\n\nConversely, fixed acidity (0.12), citric acid, residual sugar (0.014), sulphates (0.25) and alcohol (0.48) are positively correlated to the quality of wine; as these variables increase, the quality of wine improves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for highly correlated values to be removed\ntarget = 'quality'\ncandidates = corr_mat.index[\n    (corr_mat[target] > 0.5) | (corr_mat[target] < -0.5)\n].values\ncandidates = candidates[candidates != target]\nprint('Correlated to', target, ': ', candidates)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Supervised Machine Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Regression Model <br>\n1.1 Linear Regression <br>\n2. Classification Models <br>\n2.1 Logistic Regression <br>\n2.2 K-NN <br>\n2.3 Decision Tree <br>\n2.4 Neural Network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Regression Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import model\nfrom sklearn.linear_model import LinearRegression\n\n#instantiate\nlinReg = LinearRegression()\n\nstart_time = time.time()\n# fit out linear model to the train set data\nlinReg_model = linReg.fit(X_train, y_train)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get coefficient values\ncoeff_df = pd.DataFrame(linReg.coef_, X_columns, columns=['Coefficient'])  \ncoeff_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features seem to have little effect on the wine quality. <br> <br>\n\nThe coefficient scores suggest that for a unit increase in any feature, there is less than 0.12 units increase/decrease in the wine [“quality_class”]. Similarly, for wine [“quality”], although coefficient scores are higher, they remain low with alcohol having the highest coefficient score of 0.3.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate model\ny_pred = linReg.predict(X_test)\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf1 = df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.plot(kind='bar',figsize=(5,5))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean square error for the model (0.30) is rather low and indicative of high prediction accuracy. However, this could possibly mean that the model is overfitting. <br> <br>\n\nThe root mean squared error (0.55) is slightly less than 10% of the mean wine quality (5.63), this asserts that the model can make reasonable predictions although not entirely accurate. <br> <br>\n\nHowever, we have to keep in mind that the correlations in the model is rather low.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the intercept and coefficients\nprint('Intercept: ',linReg.intercept_)\nprint('r2 score: ',linReg.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R2 (0.22) score is small (i.e. the residuals are big); only 22% of the variance in wine quality can be explained by the variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Classification Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns_plot = sns.distplot(dataset['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the dataset contains 6 unique values.\nlen(dataset['quality'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Logistics Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logReg=LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42)\n\nstart_time = time.time()\n# Building a Logistic Regression Model\nlogReg.fit(X_train, y_train)\n\n#print duration of model\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Accuracy Score\ny_pred = logReg.predict(X_test)\nprint('Accuracy score: ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate Confusion Matrix\nprint('confusion matrix: ','\\n',confusion_matrix(y_test,y_pred, labels=[1,2,3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1.1 Logistics Regression with PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply pca\nstart_time = time.time()\n\n# Building a Logistic Regression Model\nlogReg.fit(Xpc_train, ypc_train)\n\n#print duration of model\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Accuracy Score\ny_pred = logReg.predict(Xpc_test)\nprint('Accuracy score with PCA applied: ', accuracy_score(ypc_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Confusion Matrix\nprint('confusion matrix: ','\\n',confusion_matrix(ypc_test,y_pred, labels=[1,2,3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An accuracy score of 84.5% looks good enough for Logistic Regression model as a classification technique. Out of 400 testing samples used, 338 are correctly predicted and 62 are classified wrongly. When PCA is applied to reduce the dimensions of the dataset, the accuracy score did not improve but decreased marginally to 82.75% and there were 69 classification errors using the test data. <br> <br>\n\nThe advantages of using Logistic Regression are high efficiency, does not require much computational resources, highly interpretable and it can produce predicted probabilities of possible outcomes. <br> <br>\n\nFeature Engineering is important to Logistic Regression in order to apply it. Each sample must belong in one of the categories and the categories must be mutually exclusive. There must be no missing values in the dataset. For Logistic Regression to work better, the independent variables should not be correlated with each other (i.e. no multi-collinearity). <br> <br>\n\nThe disadvantages of Logistic Regression are that it requires large amounts of samples and it cannot be used to predict continuous values. It can only be used to predict a categorical outcome.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.2 K-NN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_array = np.arange(1, 17, 2)\nfor k in k_array:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    y_pred=knn.predict(X_test)\n    ac = accuracy_score(y_test, y_pred)\n    print('n_neighbours: ',k)\n    print('accuracy score: ',ac)\n    print('confusion matrix: ','\\n',confusion_matrix(y_test, y_pred))\n    print('-------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.1 K-NN with PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply pca\nk_array = np.arange(1, 17, 2)\nfor k in k_array:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(Xpc_train, ypc_train)\n    y_pred=knn.predict(Xpc_test)\n    ac = accuracy_score(ypc_test, y_pred)\n    print('n_neighbours: ',k)\n    print('accuracy score: ',ac)\n    print('confusion matrix: ','\\n',confusion_matrix(ypc_test, y_pred))\n    print('-------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train model\nstart_time = time.time()\ndt.fit(X_train,y_train)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Accuracy Score\ndt_predict = dt.predict(X_test)\ndt_acc_score = accuracy_score(y_test, dt_predict)\nprint(dt_acc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Confusion Matrix\ndt_conf_matrix = confusion_matrix(y_test, dt_predict)\nprint('confusion matrix: ','\\n',dt_conf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training with Gini\ndef decTreeScore2(crit = 'gini',  maxDepth = 2, minSamples = 1, minSplit = 2):\n    dect = DecisionTreeClassifier(criterion = crit, max_depth = maxDepth, min_samples_leaf = minSamples, \n                                 min_samples_split = minSplit, random_state= 42)\n    dect.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test, dect.predict(X_test))\n    print(accuracy)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time=time.time()\ndecTreeScore2()\ntoday=date.today()\nprint(\"---%s seconds---\"% (time.time()-start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decTreeScore2(crit = 'entropy')\n#if we use entropy to calculate infomation gain instead of gini score, the accuracy drops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the max allowed depth for the decision tree\nfor i in np.arange(1, 15, 1):\n    decTreeScore2(maxDepth = i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find maximum_samples leaf of the tree\nfor i in np.arange(1, 10, 1):\n    decTreeScore2(minSamples = i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find minimum_samples_split of the tree\nfor i in np.arange(2, 10,1):\n    decTreeScore2(minSplit = i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# decision tree model\n# import graphviz and sklearn.tree\nfrom sklearn import tree\nimport graphviz\nfrom graphviz import Source","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = tree.export_graphviz(dt, out_file=None, max_depth=2,class_names=True,feature_names= X_columns, filled=True, rounded=True)\ngraph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3.1 Decision Tree with PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply pca\ndt = tree.DecisionTreeClassifier(max_depth=2)\ndt.fit(Xpc_train, ypc_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training with Gini\ndef decTreeScore2(crit = 'gini',  maxDepth = 2, minSamples = 1, minSplit = 2):\n    dect = DecisionTreeClassifier(criterion = crit, max_depth = maxDepth, min_samples_leaf = minSamples, \n                                 min_samples_split = minSplit, random_state= 42)\n    dect.fit(Xpc_train, ypc_train)\n    accuracy = accuracy_score(ypc_test, dect.predict(Xpc_test))\n    print(accuracy)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time=time.time()\ndecTreeScore2()\ntoday=date.today()\nprint(\"---%s seconds---\"% (time.time()-start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decTreeScore2(crit = 'entropy')\n#if we use entropy to calculate infomation gain instead of gini score, the accuracy drops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use different maximum depth of the tree\nfor i in np.arange(1, 15, 1):\n    decTreeScore2(maxDepth = i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use different maximum_samples leaf of the tree\nfor i in np.arange(1, 10, 1):\n    decTreeScore2(minSamples = i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = tree.export_graphviz(dt, out_file=None, max_depth=2,class_names=True, filled=True, rounded=True)\ngraph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Neural Network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Neural Network also known as Deep Learning is a type of machine learning with a series of algorithms used to identify relationship in a given data set. For this assignment we will be using Karen library to construct a Neural Network used to estimate the quality of wine in our chosen data set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In this step we will be importing and preparing dataset that is to be analyzed, in this case we will be using\n#‘winequality-red.csv’ dataset. \n#dataset = pd.read_csv('winequality-red.csv',sep=';')\ndataset['quality_class'] = dataset['quality'].apply(lambda value: 1 if value < 5 else 2 if value < 7 else 3)\ndataset['quality_class'] = pd.Categorical(dataset['quality_class'], categories=[1,2,3])\ndataset['quality_class'] = dataset['quality_class'].astype(int)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quality_label_sums= dataset['quality_class'].value_counts()\nquality_label_percentage = quality_label_sums/len('quality_class')\nprint(quality_label_sums)\nprint(quality_label_percentage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize quality_class\nj = sns.countplot(x='quality_class', data=dataset)\nplt.show(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['quality_class'] = dataset['quality_class'].astype(int)\ndataset = pd.get_dummies(dataset, columns=['quality_class'])\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we are going to determine the input and output variable of our dataset. We will also be doing scaling of feature using StandardScaler() function in sklearn library to ensure that our data is arranged in a standard normal distribution with mean of 0 and standard deviation of 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Xn = dataset.iloc[:,0:11].values\nYn = dataset.iloc[:,12:].values\n\nXn = StandardScaler().fit_transform(Xn)\n\nXn_train, Xn_test, Yn_train, Yn_test = train_test_split(Xn, Yn,random_state=42)\n\nprint(Xn_train.shape, Yn_train.shape, Xn_test.shape, Yn_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After preparing our dataset, we will be moving on to create our Neural Network model using keras library that will be used to determine wine quality.\n\nwe are going to use Sequential class from keras.models to allow us to define all of the layer in constructor.\nwe are going to use Dense from keras.layers, to allow us to run our model operation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(30, input_dim=11, activation='sigmoid'))\nmodel.add(Dense(50, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nstart_time = time.time()\n#train model\nhistory = model.fit(x = Xn_train, y = Yn_train,batch_size=128, epochs = 800,verbose=1,validation_data=(Xn_test, Yn_test))\n\n#get model training duration\ntoday= date.today()\nprint('---%s seconds---'%(time.time()-start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculation of Loss and Accuracy metrics\nloss, accuracy = model.evaluate(Xn_test, Yn_test)\nprint('loss: ', loss, ', accuracy: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(Xn_test)\nprint('\\nPrediction:')\nfor i in np.arange(len(predictions)):\n    print('Actual: ', Yn_test[i], ', Predicted: ', predictions[i])\n\npredictions=np.argmax(predictions, axis=1)\nYn_test = np.argmax(Yn_test, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training History - Model Accuracy\nprint(history.history.keys())\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training History - Loss Accuracy\nprint(history.history.keys())\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculation of confusion matrix\n#from sklearn.metrics import confusion_matrix\nconfusion_matrix(Yn_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4.1 Neural Network with PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = dataset.iloc[:,12:].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(pc_X, Y, random_state=42)\n\nprint(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(30, input_dim=6, activation='sigmoid'))\nmodel.add(Dense(50, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nstart_time = time.time()\nhistory = model.fit(x = X_train, y = Y_train,batch_size=128, epochs = 800,verbose=1,validation_data=(X_test, Y_test))\n\ntoday= date.today()\nprint('---%s seconds---'%(time.time()-start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculation of Loss and Accuracy metrics\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint('loss: ', loss, ', accuracy: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)\nprint('\\nPrediction:')\nfor i in np.arange(len(predictions)):\n    print('Actual: ', Y_test[i], ', Predicted: ', predictions[i])\n    \npredictions=np.argmax(predictions, axis=1)\nY_test = np.argmax(Y_test, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training History - Model Accuracy\nprint(history.history.keys())\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training History - Loss Accuracy\nprint(history.history.keys())\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculation of confusion matrix\nconfusion_matrix(Y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unsupervised Machine Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will apply 2 clustering ML models to the dataset to try uncover possible clusters.\n\n1. K-Means (centriod based)\n2. Hierarchical Agglomerative Clustering (similarity based)\n3. Dbscan (density based)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. K-Means","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries\nfrom sklearn.metrics import f1_score\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this model, the entire dataset has been used as a training data. <br>\nThen an elbow method will be used to find out an optimal number of “K” clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#try to find optimal k using the elbow method\nwcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300, n_init=12, random_state=0)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\nf3, ax = plt.subplots(figsize=(8, 6))\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"“K” value of 2 will be used as a dip can be seen around 2 which is our elbow in a graph above. <br> <br> <br>\n\n\nFirst, clustering will be performed with K-Means on dataset without applying principle component analysis (PCA). Note that the total dimension of dataset is 11.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Time – 0.062 seconds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(X_scaled[clusters==i,0],\n               X_scaled[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that clusters are not well separated. Some members of Cluster 2 can be seen in Cluster 1 and vice versa.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualise the clusterds considerig fixed acidity, residual sugar, and alcohol\nfig = plt.figure(figsize=(20, 15))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=15, azim=40)\n\nax.scatter(X_scaled[:,0], X_scaled[:,3], X_scaled[:,10],c=y, edgecolor='k')\nax.set_xlabel('Acidity')\nax.set_ylabel('Sugar')\nax.set_zlabel('Alcohol')\nax.set_title('K=2: Acidity, Sugar, Alcohol', size=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the silhouette score of the model will be measured. The silhouette score ranges from -1 to +1. <br>\nThe high silhouette score indicates that the objects are well matched to its own cluster and not to its neighbouring clusters. <br>\n(The higher the silhouette score – the better the clustering)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nfrom sklearn.metrics import pairwise_distances\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The silhouette score obtained is considered low. It means clusters are neither dense nor well separated. <br>\nNext, let’s measure the inertia value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An extremely high inertia value of 14330.119 was obtained. It is an indicative of the “curse of dimensionality”. <br>\nWe are using 11 dimensions of data in this model. <br>\nIn this case, we will explore the model again using PCA (principle component analysis).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.1 K-Means with PCA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our purpose of applying principal component analysis is to reduce dimension. <br>\nIn this dataset, we reduced the 11-dimensional data to 6-dimensional data during PCA.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training time – 0.05 seconds Training time is observed to have reduced slightly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(pc_X[clusters==i,0],\n               pc_X[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After implementing PCA, it can be seen that clustering is improved. So it is expected to see a higher silhouette score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, we can see an improvement in the silhouette score. But it is still considered low which means there are still some overlapping of clusters or incorrect grouping. <br> <br>\n\nAlthough the silhouette score increased with PCA, it still low; clusters are overlapping or incorrectly grouped.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The inertia value is also decreased but still extremely high. <br> <br>\n\nK-means clustering has poor clustering result for high dimensional data. Even with the implementation of PCA, the silhouette score can only be improved to some extent but is considered low. Also the inertia value is observed to be extremely high. In an ideal situation, the inertia value should be as low as possible. Hence, we can conclude that this is not a good model fit to the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Agglormerative Clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Apply agglomerative clustering to pick the best number of clusters, we need to draw the dendrogram graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\nfrom scipy.cluster.hierarchy import dendrogram, linkage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot dendrogram to determine number of clusters\nplt.figure(figsize=(25, 10))\nplt.title('Dendrogram')\nplt.xlabel('Wine Details')\nplt.ylabel('Euclidean distances')\n\ndendrogram (\n    linkage(X_scaled, 'ward')  # generate the linkage matrix\n    ,leaf_font_size=8 # font size for the x axis labels\n)\nplt.axhline(y=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the dengrogram above we can see that the features after the 3rd branch are very similar to each other (i.e. shorter in height). The dataset should optimally have are 3 clusters; where the distance between the clusters are the highest.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering = AgglomerativeClustering(linkage=\"ward\", n_clusters=3)\n#train model\nstart_time = time.time()\nclustering.fit(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize clustering\ncolors = 'rgbkcmy'\n\nfor i in np.unique(clustering.labels_):\n    plt.scatter(X_scaled[clustering.labels_ == i, 0], X_scaled[clustering.labels_ == i, 1],\n                color=colors[i], label='Cluster ' + str(i + 1))\n\nplt.legend()\nplt.title('Hierarchical Clustering')\nplt.xlabel(X_columns[1])\nplt.ylabel(X_columns[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph above, we can tell that that clusters not clearly defined. Lets explore Agglormerative Clustering agin with PCA.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nlabels = clustering.labels_\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Agglormerative Clustering with PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering = AgglomerativeClustering(linkage=\"ward\", n_clusters=3)\nstart_time = time.time()\nclustering.fit(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize clustering\ncolors = 'rgbkcmy'\n\nfor i in np.unique(clustering.labels_):\n    plt.scatter(pc_X[clustering.labels_ == i, 0], \n                pc_X[clustering.labels_ == i, 1],\n                color=colors[i], label='Cluster ' + str(i + 1))\n\nplt.legend()\n\nplt.title('Hierarchical Clustering')\nplt.xlabel(pc_columns[0])\nplt.ylabel(pc_columns[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the clusters are not entirely segreggated, they appear to be clearer after applying PCA.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nlabels = clustering.labels_\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Dbscan","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Higher min_samples or lower eps indicate higher density necessary to form a cluster.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan = DBSCAN(eps=2, min_samples=7)\nstart_time = time.time()\nclusters= dbscan.fit_predict(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = 'rgbkcmy'\nax = plt.axes(projection='3d')\n\nfor i in np.unique(clusters):\n    label = 'Outlier' if i == -1 else 'Cluster ' + str(i + 1)\n    ax.scatter3D(X_scaled[clusters==i,0], X_scaled[clusters==i,1],X_scaled[clusters==i,4],\n                #color=colors[i], \n                 label=label)\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nlabels = dbscan.labels_\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Dbscan with PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan = DBSCAN(eps=2, min_samples=7)\nstart_time = time.time()\nclusters= dbscan.fit_predict(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.axes(projection='3d')\n\nfor i in np.unique(clusters):\n    label = 'Outlier' if i == -1 else 'Cluster ' + str(i + 1)\n    ax.scatter3D(pc_X[clusters==i,0], \n                 pc_X[clusters==i,1],\n                 pc_X[clusters==i,2],\n                 label=label)\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nlabels = dbscan.labels_\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}