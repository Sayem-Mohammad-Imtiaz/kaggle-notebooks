{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my personal note for common functions in data analysis so that I can refer to in times of need.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Basics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra - arrays & matrices\nimport pandas as pd # for data structures & tools, data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp #for integrals, solving differential equations, optimization\nimport matplotlib.pyplot as plt #for plots, graphs - data visualization\n%matplotlib inline \nimport seaborn as sns #plots - heat maps, time series & violin plots\nimport sklearn as sklearn #machine learning models\nimport statsmodels as stmodels #explore data, estimate statistical models, & perform statistical test\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading a dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load train dataset\ndata = '../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv'\ndataset = pd.read_csv(data) #for those datasets without headers can use dataset = pd.read_csv(data, header = None)\ndataset_withoutheaders = pd.read_csv(data, header = None)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring a dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns #finding name of columns of the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Description on columns**\n* sl_no          :           Serial Number\n* gender         :           Gender - Male='M, Female='F'\n* ssc_p          :           Secondary Education percentage - 10th Grade\n* ssc_b          :           Board of Education - Central/ Others\n* hsc_p          :           Higher Secondary Education percentage- 12th Grade\n* hsc_b          :           Board of Education - Central/ Others\n* hsc-s          :           Specialization in Higher Secondary Education\n* degree_p       :           Degree Percentage\n* degree_t       :           Under Graduation(Degree type) - Field of degree education\n* workex         :           Work Experience\n* etest_p        :           Employability test percentage (conducted by college)\n* specialisation :           Post Graduation(MBA) - Specialization\n* mba_p          :           MBA percentage\n* status         :           Status of placement- Placed/Not placed\n* salary         :           Salary offered by corporate to candidates\n\n<br> \n\n**What is in it?** <br>\nThis dataset consists of Placement data of students at Jain University Bangalore. It includes secondary and higher secondary school percentage and specialization. It also includes degree specialization, type and Work experience and salary offers to the placed students.\n\n<br>\n\n**Questions we can ask about this dataset** <br>\nWhich factor influenced a candidate in getting placed? <br>\nDoes percentage matters for one to get placed? <br>\nWhich degree specialization is much demanded by corporate? <br>\nPlay with the data conducting all statistical tests. <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_withoutheaders.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this dataset consists of headers already, when I use header = None, it treated the headers as rows or data points. <br> <br>\nLets say the dataset doesn't contain headers and so we used header = None to import data, we can assign column names/headers with a panda method as below: <br> <br>\nheaders = [\"sl_no\",\"gender\",\"ssc_p\",\"hsc_p\",\"hsc_s\" etc...] <br>\ndataset.columns = headers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To export the modified dataset to file <br> <br>\nexport_path = \"C:\\Windows\\....\\exportedfile.csv\" <br>\ndf.to_csv(export_path) <br> <br>\n\n**Different formats** <br>\n* For csv format, reading: pd.read_csv(), exporting: df.to_csv()\n* For json format, reading: pd.read_json(), exporting: df.to_json()\n* For Excel format, reading: pd.read_excel(), exporting: df.to_excel()\n* For SQL format, reading: pd.read_sql(), exporting: df.to_sql()","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Different Datatypes**\n* Pandas Type: object, Native Python Type: string, Description: numbers and strings\n* Pandas Type: int64, Native Python Type: int, Description: Numeric characters\n* Pandas Type: float64, Native Python Type: float, Description: Numberic characters with decimals\n* Pandas Type: datetime64,timedelta[ns], Native Python Type: refer to datetime module in Python's standard library, Description: time data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking datatypes of features\n\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"General rule of thumb, prices & measurement values should be float64 - otherwise, it will be a problem in an analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe() #returns a statistical summary - but this method does not include object datatype columns, basically skip rows & columns that do not contain numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for full summary with every column & row\ndataset.describe(include = \"all\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* unique - number of distinct objects in the column\n* top - most frequently occuring object\n* freq - number of times the top object appears in the column\n* NaN - Not A Number","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[['sl_no', 'salary', 'gender']].describe(include = \"all\") #describing only sl_no & salary columns in the dataset, notice I can arrange the way columns appear sl_no --> salary --> gender\n\n#include = \"all\" is added to display gender column since it is an object type, else not needed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accessing databases with Python","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#DB-API\nfrom dbmodule import connect\n\n#Create a connection object\nconnection = connect('databasename', 'username', 'password')\n\n#Create a cursor object\ncursor = connection.cursor()\n\n#Run queries\ncursor.execute('select * from tablename')\nresults = cursor.fetchall()\n\n#Free resources\nCursor.close()\nconnection.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Identifying & handling missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nMissing values appear in the dataset as \"?\", \"NaN\", \"0\" or a blank cell.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Dealing with missing values**\n\n* Check with the data collection source\n* Drop the missing values - either by dropping the variable (whole column) or just the data entry with the missing value (just the row)\n* Replacing the missing values - replacing with an mean/ average value (of similar datapoints) -> this will not work on categorical data, replacing it by mode/ most frequent value, replacing it based on other functions (based on domain knowledge or experience)\n* Leaving it as a missing data - least preferable","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dropping rows with NaN values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#output missing data\nmissing_data = dataset.isnull()\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dropna(subset=[\"salary\"], axis = 0, inplace = True) #axis = 0 will drop the entire row with NaN value & axis = 1 will drop the entire column with NaN value, inplace = True will write the result back to the original dataset\n\n#dataset.dropna(subset=[\"salary\"], axis = 0, inplace = True) is the same as dataset = dataset.dropna(subset=[\"salary\"], axis = 0)\n#if want to drop all rows with NaN value regardless of column/variable just use dataset.dropna(axis = 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing missing values with new values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset the dataset to original\ndataset = pd.read_csv(data)\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataframe.replace(missing_value, new_value)\n#replacing with mean value\nmean = dataset[\"salary\"].mean()\ndataset[\"salary\"].replace(np.nan, mean, inplace = True)\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data formatting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data entries can be in different formats for one expression/ value. <br>\nFor example, Singapore can be in S.G., SG, S.G, S.G.P, singapore etc. <br>\nStandardizing different formats of same expression is necessary in data analysis. <br>\nSometimes values can also be in a unit that is not preferred in the calculation/analysis such as entries are in miles instead of kilometers etc. <br> <br>\n\n**Changing format of data entry values** <br>\ndf[\"city-mpg\"] = 235/df[\"city-mpg\"] <br>\ndf.rename(columns={\"city-mpg\" : \"city-L/100km\"}, inplace = True) <br> \n-------------------------------------------------------------------------------------------------------------------<br>\nAnother scenario could be numerical values being in object datatype and not included in the calculation. <br> <br>\n**Converting datatypes** <br>\ndf[\"salary\"] = df[\"salary\"].astype(\"float\")","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Normalization (Centering/Scaling)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data normalization is uniforming the features values with different ranges to have a fair comparison between different features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see salary feature has ridiculously high values compared to other features. This will produce a data bias in an analysis. <br> <br>\n\n**3 methods of normalization** <br>\n* Simple Feature Sacling - dividing each value with the maximum value of that feature, will give a new value range between 0 & 1.\n* Min-Max Scaling - ((current value - minimun value)/(maximun value - minimum value)), will give a new value range between 0 & 1.\n* Z-Score Scaling - (current value - average value of feature)/standard deviation (sigma) - will give a value around 0 (typically between -3 & +3).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Simple Feature Scaling\ndataset[\"salary\"] = dataset[\"salary\"]/data[\"salary\"].max()\n\n#Min-Max Scaling\ndataset[\"salary\"] = (dataset[\"salary\"] - dataset[\"salary\"].min())/(dataset[\"salary\"].max()-dataset[\"salary\"].min())\n\n#Z-Score Sacling\ndataset[\"salary\"] = (dataset[\"salary\"]-dataset[\"salary\"].mean())/dataset[\"salary\"].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Binning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Binning is grouping of values into bins to have a better understanding of data distribution or to improve accuracy of the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = np.linspace(min(dataset[\"salary\"]), max(dataset[\"salary\"]),4) #to have 3 bins, need 4 equally spaced numbers hence 4 in code\ngroup_names = [\"Low\", \"Medium\", \"High\"]\ndataset[\"salary-binned\"] = pd.cut(dataset[\"salary\"], bins, labels = group_names, include_lowest = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib as plt\nfrom matplotlib import pyplot\nplt.pyplot.hist(dataset[\"salary\"])\n\n# set x/y labels and plot title\nplt.pyplot.xlabel(\"Salary\")\nplt.pyplot.ylabel(\"Count\")\nplt.pyplot.title(\"Salary Bins\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.bar(group_names, dataset[\"salary-binned\"].value_counts())\n\n# set x/y labels and plot title\nplt.pyplot.xlabel(\"Salary\")\nplt.pyplot.ylabel(\"Count\")\nplt.pyplot.title(\"Salary Bins\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Turning Categorical values to numeric variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.gender.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To see which values are present in a particular column, we can use the \".value_counts()\" method:\ndataset['gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can see that males are the most common type. We can also use the \".idxmax()\" method to calculate for us the most common type automatically:\ndataset['gender'].value_counts().idxmax()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One-hot encoding method** <br>\nSo we have 2 categorical values in gender feature. <br>\nWe can encode these categorical values of gender into numeric values by adding dummy variables for each unique category and assign 0/1 in each category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_variable_1 =  pd.get_dummies(dataset[\"gender\"])\ndummy_variable_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge data frame \"dataset\" and \"dummy_variable_1\" \ndataset = pd.concat([dataset, dummy_variable_1], axis=1)\n\n# drop original column \"gender\" from \"dataset\"\ndataset.drop(\"gender\", axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"EDA is used to -:\n* Summarize main characteristics of the data\n* Gain better understanding of the data set\n* Uncover relationships between variables\n* Uncover important variables <br> <br>\n\nIn this dataset, using EDA, we can uncover what are the characteristics that have the most impact on the salary? But as mentioned before, my focus on this notebook is not performing a full analysis on the dataset, rather testing out and noting down the Data Analysis functions for my future reference.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Descriptive Statistics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Descriptive analysis provide understanding on basic features of the data & give short summary on the sample and measure of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset the dataset to original\ndataset = pd.read_csv(data)\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#basic descriptive statistics function\ndataset.describe(include = \"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#summarizing categorical data\nprint(dataset[\"gender\"].value_counts())\nprint()\nprint(dataset[\"ssc_b\"].value_counts())\nprint()\nprint(dataset[\"hsc_b\"].value_counts())\nprint()\nprint(dataset[\"hsc_s\"].value_counts())\nprint()\nprint(dataset[\"degree_t\"].value_counts())\nprint()\nprint(dataset[\"workex\"].value_counts())\nprint()\nprint(dataset[\"specialisation\"].value_counts())\nprint()\nprint(dataset[\"status\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Box Plots\nsns.boxplot(x=\"gender\", y=\"salary\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How to read a box plot**\n* the horizontal line at the top is upper extreme value\n* the horizontal line at the bottom is lower extreme value\n* the horizontal line at the center of the box is median vlaue (middle datapoint)\n* the upper quartile is the 75th percentile of data\n* the lower quartile is the 25th percentile of data\n* the dots outside the upper & lower extreme values are outliers\n<br> <br>\nThe data between the upper and lower quartile represents the interquartile range. <br>\nThe upper & lower extreme values are calculated as 1.5 times the interquartile range above the 75th percentile and as 1.5 times the interquartile range below the 25th percentile.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scatter Plots\nx = dataset[\"etest_p\"]\ny = dataset[\"salary\"]\nplt.scatter(x,y)\n\nplt.title(\"Employment Test Percentage vs Salary offered by corporate to candidates\")\nplt.xlabel(\"Employment Test Percentage\")\nplt.ylabel(\"Salary Offered\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter plots are suitable to show  the relationship between 2 continuous data. <br>\nScatter plot has 2 variables - independent variable & dependent variable. Independent variable is used to predict the dependent variable. <br>\nNormally, an independent variable is placed on the x-axis & a dependent variable is placed on the y-axis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Groupby Function\ndataset_test = dataset[['gender', 'degree_t', 'salary']]\ndataset_grp = dataset_test.groupby(['gender', 'degree_t'], as_index = False).mean() #.mean() is used to see how average salary differ across different groups\ndataset_grp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pivot Table\ndataset_pivot = dataset_grp.pivot(index = 'gender', columns = 'degree_t')\ndataset_pivot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap Plot\nfig, ax = plt.subplots()\nim = ax.pcolor(dataset_pivot, cmap='RdBu')\n\n#label names\nrow_labels = dataset_pivot.columns.levels[1]\ncol_labels = dataset_pivot.index\n\n#move ticks and labels to the center\nax.set_xticks(np.arange(dataset_pivot.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(dataset_pivot.shape[0]) + 0.5, minor=False)\n\n#insert labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(col_labels, minor=False)\n\n#rotate label if too long\nplt.xticks(rotation=90)\n\nfig.colorbar(im)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation\nsns.regplot (x=\"etest_p\", y=\"salary\", data=dataset)\nplt.ylim(0,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be a positive linear relationship between 2 variables albeit rather weak. <br>\nBut note that correlation does not mean causation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation Strength\n#### Pearson Correlation Method\n\n* Measure the strength of the correlation between two features (Correlation coefficient & P-value)\n* Correlation coefficient (Close to +1: Strong positive relationship, Close to -1: Strong negative relationship, Close to 0: No relationship)\n* P-value (P-value<0.001 Highly confident in the result/relationship, P-value<0.05 Moderately confident, P-value<0.1 Slightly confident, P-value>0.1 Not confident)","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from scipy import stats\npearson_coef, p_value = stats.pearsonr(dataset['etest_p'], dataset['salary'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Error encountered due to NaN values involved.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing with mean value\netest_mean = dataset[\"etest_p\"].mean()\ndataset[\"etest_p\"].replace(np.nan, etest_mean, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salary_mean = dataset[\"salary\"].mean()\ndataset[\"salary\"].replace(np.nan, salary_mean, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\npearson_coef, p_value = stats.pearsonr(dataset['etest_p'], dataset['salary'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Correlation Coefficient is \" + str(pearson_coef))\nprint(\"p_value is \" + str(p_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation coefficient is not much. So it is a weak positive correlation. <br>\nAnd p_value is less than 0.05, so we are moderately certain that it is a weak positive correlation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding correlation value between multiple features\ndataset[['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Heatmap\ncorr_mat = dataset.corr()\n\nplt.figure(figsize = (13,5))\nsns_plot = sns.heatmap(data = corr_mat, annot = True, cmap='GnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the correlation heat map left out features with categorical values. To include them, we need to encode them to numerical labels such as 1 for Male, 0 for Female in gender etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Analysis of Variance (ANOVA)\n#### ANOVA is finding a correlation between different groups of a categorical variable\n\nANOVA returns 2 values:\n* F-test score: variation between sample group means divided by variation within sample group\n* p-value : confidence degree\n\nSmall F-test score means a weak correlation between variable categories & a target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.specialisation.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_anova = dataset[[\"specialisation\", \"salary\"]]\ngrouped_anova = dataset_anova.groupby([\"specialisation\"])\n\nf_val, p_val = stats.f_oneway(grouped_anova.get_group(\"Mkt&HR\")[\"salary\"],grouped_anova.get_group(\"Mkt&Fin\")[\"salary\"])\nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is not good. A small F test score showing a weak correlation and a not so small P value implying that it is not certain about statistical significance.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}