{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport random\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, scale\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"music_df = pd.read_csv('../input/data.csv')\nmusic_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d0e2bddf991244ae13d50e7ed8e55e9b5b25a07"},"cell_type":"code","source":"genres = music_df.groupby('label')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"43876eb9c2194ae39171b4c3878ec0ff0805c9f2"},"cell_type":"code","source":"features = list(music_df.columns)\nfeatures.remove('filename')\nfeatures.remove('label')\nprint(features)\n\nfor feat in features:\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n    sns.violinplot(data=music_df, x='label', y=feat, figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5e90269be2b7992bd945aa454ceedd627ddb0fc"},"cell_type":"markdown","source":"# Covariance vs. Correlation\n## Covariance\n$$\n\n$$"},{"metadata":{"trusted":true,"_uuid":"7f989fb30db0b8cef549ad423e5834159a5fbdc2"},"cell_type":"code","source":"music_features_df = music_df[features]\nmusic_features_norm_df = scale(music_features_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fd57615051b5ff6e0baacbd0808b11db9fce0a9"},"cell_type":"code","source":"def lower_diag_matrix_plot(matrix, title=None):\n    \"\"\" Args:\n        matrix - the full size symmetric matrix of any type that is lower diagonalized\n        title - title of the plot\n    \"\"\"\n    plt.style.use('default')\n    \n    # Create lower triangular matrix to mask the input matrix\n    triu = np.tri(len(matrix), k=0, dtype=bool) == False\n    matrix = matrix.mask(triu)\n    fig, ax = plt.subplots(figsize=(20,20))\n    if title:\n        fig.suptitle(title, fontsize=32, verticalalignment='bottom')\n        fig.tight_layout()\n    plot = ax.matshow(matrix)\n    \n    # Add grid lines to separate the points\n    # Adjust the ticks to create visually appealing grid/labels\n    # Puts minor ticks every half step and bases the grid off this\n    ax.set_xticks(np.arange(-0.5, len(matrix.columns), 1), minor=True)\n    ax.set_yticks(np.arange(-0.5, len(matrix.columns), 1), minor=True)\n    ax.grid(which='minor', color='w', linestyle='-', linewidth=3)\n    # Puts major ticks every full step and bases the labels off this\n    ax.set_xticks(np.arange(0, len(matrix.columns), 1))\n    ax.set_yticks(np.arange(0, len(matrix.columns), 1))\n    plt.yticks(range(len(matrix.columns)), matrix.columns)\n    # Must put this here for x axis grid to show\n    plt.xticks(range(len(matrix.columns)))\n    ax.tick_params(axis='both', which='major', labelsize=24)\n    # Whitens (transparent) x labels\n    ax.tick_params(axis='x', colors=(0,0,0,0))\n    \n    # Add a colorbar for reference\n    cax = make_axes_locatable(ax)\n    cax = cax.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cax.tick_params(axis='both', which='major', labelsize=24)\n    fig.colorbar(plot, cax=cax, cmap='hot')\n    \n    # Get rid of borders of plot\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc4d95c7aa56eed9f26b28e7a4f31f6aa82088ac","scrolled":false},"cell_type":"code","source":"cov_matrix = music_features_df.cov()\nlower_diag_matrix_plot(cov_matrix, 'Covariance Matrix')\n\ncorr_matrix = music_features_df.corr()\nlower_diag_matrix_plot(corr_matrix, 'Correlation Matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17f1fbe355d7fb6721ab8d107eadfca6d7ed7f3e"},"cell_type":"markdown","source":"##### As one can see from above, the Correlation Matrix is much easier to garner information from. This is because the data in it's original form has large values for 'rolloff', 'spectral_centroid' and 'spectral_bandwidth'. Once standardized, correlations can be deduced as in the second plot."},{"metadata":{"trusted":true,"_uuid":"f4b1e16981ab232415684d8e1606e96fcda511a2"},"cell_type":"code","source":"music_df_no_categories = music_features_df.copy()\nmusic_df_no_categories['label'] = music_df['label']\n# sns.pairplot(music_df, hue='label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4b29b20f0398f047fb1c84bf13f70e4748ca319"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ef58484f089435c3a4f0eb3db9ff3b9807466fc"},"cell_type":"code","source":"enc = OneHotEncoder()\nsolvers = ['svd', 'eigen']\nfor solver in solvers:\n    clf = LinearDiscriminantAnalysis(solver=solver, n_components=2)\n    le = LabelEncoder()\n\n    new_labels = pd.DataFrame(le.fit_transform(music_df['label']))\n    music_df['label'] = new_labels\n\n    params = clf.fit_transform(music_features_norm_df, new_labels,)\n    fig, ax = plt.subplots()\n    labels_list = list(set(list(new_labels)))\n    ax.scatter(params[:,0], params[:,1], c=new_labels.as_matrix().reshape(params[:,0].shape))\n    ax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a57104fc818015d4a19c00503b75248f4969b4a"},"cell_type":"markdown","source":"##### As can be seen above, LDA to two variables doesn't give us a ton of separation in the data. Maybe we should keep most of our features?"},{"metadata":{"trusted":true,"_uuid":"0ef3118a493248342d09ac93850375e2f9b48b9a"},"cell_type":"markdown","source":"###### Let's try to do just this, and run a kNN classifier on the data"},{"metadata":{"trusted":true,"_uuid":"183573d4286a1e504810b208bb182ee627c0b731"},"cell_type":"code","source":"def param_search(param_names, params_list, model, data, plot=True, seed=None, verbose=False):\n    params_accuracies = []\n    train_data, train_labels, test_data, test_labels = data\n    for param_name, param_list in zip(param_names, params_list):\n        accuracies = []\n        kwargs = {}\n        if verbose:\n            print(\"--------------------------------\")\n            print(\"Parameter Under Test: {}\\n\".format(param_name))\n        if type(param_list) != list:\n            kwargs[param_name] = param_list\n            continue\n        for param_val in param_list:\n            if verbose: print(\"Parameter search ({0} -> {1})\".format(param_name, param_val))\n            kwargs[param_name] = param_val\n            classifier = model(**kwargs)\n            classifier.fit(train_data, train_labels)\n            accuracy = classifier.score(test_data, test_labels)\n            accuracies.append(accuracy)\n        params_accuracies.append(accuracies)\n        if plot:\n            fig, ax = plt.subplots()\n            ax.plot(param_list, accuracies)\n        if verbose: print(\"--------------------------------\")\n    if type(param_list) != list:\n        kwargs[param_name] = param_list\n        classifier = model(**kwargs)\n        classifier.fit(train_data, train_labels)\n        accuracy = classifier.score(test_data, test_labels)\n        params_accuracies.append([accuracy])\n    return params_accuracies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"155de1dde6dded5c35d66d274760fc0924938057","scrolled":false},"cell_type":"code","source":"param_names = ['n_neighbors']\nparams_list = [[i+3 for i in range(20)]]\ndummy_param_names = ['strategy']\ndummy_params_list = [['stratified', 'most_frequent', 'uniform', 'prior']]\ntree_param_names = ['min_samples_split', 'max_depth']\ntree_params_list = [10, 8]\ntree_params_lists_only = [x for x in tree_params_list if type(x) == list]\ndummy_params_lists_only = [x for x in dummy_params_list if type(x) == list]\nparams_lists_only = [x for x in params_list if type(x) == list]\nfolds = 10\nrandom_state = random.randint(1, 65536)\nprint(\"Random State: {}\".format(random_state))\n\ncv = StratifiedKFold(n_splits=folds,\n                     shuffle=True,\n                     random_state=random_state,\n                     )\n\ndata = list(cv.split(music_features_df, music_df['label']))\nfig, ax = plt.subplots()\ndummy_fig, dummy_ax = plt.subplots()\ntree_fig, tree_ax = plt.subplots()\n\nfor i, indices in enumerate(data):\n    train_index, test_index = indices\n    title = \"Training on fold {}/{}...\\n\".format(i+1, len(data))\n    print(title)\n    train_data = music_features_df.iloc[train_index]\n    train_labels = music_df['label'].iloc[train_index]\n    test_data = music_features_df.iloc[test_index]\n    test_labels = music_df['label'].iloc[test_index]\n    full_data = (train_data, train_labels, test_data, test_labels)\n    \n    dummy_accuracies = param_search(dummy_param_names, \n                                    dummy_params_list, \n                                    DummyClassifier, \n                                    data=full_data,\n                                    plot=False)\n    for X, y in zip(dummy_params_list, dummy_accuracies):\n        dummy_ax.scatter(X, y, label='K Fold {}'.format(i+1))\n        dummy_ax.legend()\n    \n    accuracies = param_search(param_names, \n                              params_list, \n                              KNeighborsClassifier, \n                              data=full_data,\n                              plot=False)\n    for X, y in zip(params_list, accuracies):\n        ax.plot(X, y, label='K Fold {}'.format(i+1))\n        ax.legend()\n    \n    tree_accuracies = param_search(tree_param_names, \n                                   tree_params_list, \n                                   DecisionTreeClassifier,\n                                   data=full_data,\n                                   plot=False)\n    print(tree_params_lists_only, tree_accuracies)\n    if not tree_params_lists_only:\n        tree_params_lists_only = [['K Fold Results']]\n    for X, y in zip(tree_params_lists_only, tree_accuracies):\n        tree_ax.scatter(X, y, label='K Fold {}'.format(i+1))\n        tree_ax.legend()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab5f7308d4ec53f5d5454a06e5bfd3430da71da9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"704c55aa1d54c38a21fea6bb9ba4161a0fddd890"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}