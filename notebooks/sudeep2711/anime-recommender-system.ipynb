{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Anime recommender System**\n\n\n<img src=\"https://cdna.artstation.com/p/assets/images/images/002/181/886/large/pixel-motron-lastsupper-animecrossover-by-pixelmotron.jpg?1458327976\" width =\"800\">\n\n\nIn this notebook, we will explore creating simple recommender systems using the following techniques:\n\n* **[Popularity Model](https://hackernoon.com/popularity-based-song-recommendation-system-without-any-library-in-python-12a4fbfd825e):** As the name suggests Popularity based recommendation system works with the trend. It basically uses the items which are in trend right now. For example, if any product which is usually bought by every new user then there are chances that it may suggest that item to the user who just signed up. The problem is that, it might not give personalized recommendations.This model works well most of the times because people like things other people might like. For e.g: Many people have watched Dragon Ball Z. There are so many pop culture references about DBZ that people who generally dont like anime might have watched/ would prefer watching the show.\n\n\n\n* **[Content Based Filtering Algorithm:](http://http://recommender-systems.org/content-based-filtering/)** This method will try to find similarities between the content ( Anime series in this case) using attributes and metadata about the content and try to recommend items with highest similarity to what our active user liked in the past.\n\n\n\n* **[Collaborative filtering algorithm](https://en.wikipedia.org/wiki/Collaborative_filtering):** This method will try to find similar users like the active user (the user for whom we are going to make predictions). Based on certain similarity metrics, we then suggest items ( anime titles in our case) that the similar users liked(gave higher ratings to) which were not rated by our active users.\n\n\n(Inspired by Gabriel Moreira's Kaggle Notebook on Recommender Systems)\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.max_rows = 4000\nfrom sklearn.metrics.pairwise import cosine_similarity\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read data\nanime_data= pd.read_csv('/kaggle/input/anime-recommendations-database/anime.csv')\nrating_data=pd.read_csv('/kaggle/input/anime-recommendations-database/rating.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring our Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking basic information about our dataset\nanime_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_data.describe()\n# The data consists of 12294 different Anime titles, it has 12064 ratings (which means some ratings are Null)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_data.head()\n#This data contains Iser ID, Anime Id and User rating for that anime from 0 to 10.\n#Items which have a rating of -1 have not been rated yet.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_data.describe()\n# Rating data has 7.8 Million Ratings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing -1 to np.Nan in rating data\nrating_data.rating.replace(-1, np.NaN,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting count of Nulls\nprint('Anime Data')\nfor i in anime_data.columns:\n    print('Null counts in the column',i,':',sum(anime_data[i].isna()))\n\nprint('\\n Rating Data')\nfor i in rating_data.columns:\n    print('Null counts in the column',i,':',sum(rating_data[i].isna()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting so many NAs in ratings is understandable and a very common issue in User-Rating data. \nThis is because, not every user will rate every anime in the list of anime shows out there!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing -1 to np.Nan in rating data\nanime_data.genre.replace(np.NaN,'None_Genre',inplace=True)\nanime_data.type.replace(np.NaN,'None_type',inplace=True)\nanime_data.episodes.replace('Unknown',np.NaN,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**EDA on anime_data **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport itertools\nimport collections\nimport operator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import minmax_scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On average, they have a rating of 6\nsns.distplot(anime_data['rating'], hist=True, kde=True, \n             bins=10, color = 'green', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nprint('Average Rating:',anime_data['rating'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On average, they have a rating of 6\nsns.distplot(anime_data['members'], hist=True, kde=True, \n             bins=10, color = 'green', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nprint('Average members:',anime_data['members'].mean())\nprint('Max members:',anime_data['members'].max())\nprint('Min members:',anime_data['members'].min())\nprint('Median members:',anime_data['members'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check the top 10 anime with max members\nanime_data.sort_values('members',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check the bottom 10 anime titles\nanime_data.sort_values('members',ascending=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many of these anime titles dont even have a Rating / Genre (I am not surprised! )","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_data.episodes=pd.to_numeric(anime_data.episodes, errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On average, they have aroud 12 episodes per series\nsns.distplot(anime_data['episodes'], hist=True, kde=True, \n             bins=10, color = 'green', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nprint('Average Episode Count:',anime_data['episodes'].mean())\nprint('Max Episode Count:',anime_data['episodes'].max())\nprint('Min Episode Count:',anime_data['episodes'].min())\nprint('Median Episode Count:',anime_data['episodes'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Anime series with highest number of episodes\nanime_data.sort_values('episodes', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_data['type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking rating across different type of anime\nsns.catplot(x=\"type\", y=\"rating\", kind=\"box\", data=anime_data)\n\nfor i in anime_data['type'].unique():\n    print('Average rating for',i,'anime:',anime_data[anime_data.type==i]['rating'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We notice that, on an average rating for TV anime is higher ~ 7\n \n Average rating for Musical Anime is lower ~ 5.5\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_data['genre']=anime_data['genre'].apply(lambda x : x.split(', '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_data = itertools.chain(*anime_data['genre'].values.tolist())\ngenre_counter = collections.Counter(genre_data)\ngenres = pd.DataFrame.from_dict(genre_counter,orient='index').reset_index()\ngenres.columns=['Genre','Counts']\ngenres.sort_values('Counts', ascending=False, inplace=True)\n\n# Plot genre\nf, ax = plt.subplots(figsize=(10,12))\nsns.barplot(x=\"Counts\", y=\"Genre\", data=genres, color='#719967')\nax.set(ylabel='Genre',xlabel=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Our list mostly comprises of comedy anime titles, followed by Action, adventure and fantasy titles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting rankings by genre\ngenre_rating = []\nfor i in list(genres['Genre']):\n    genre_rating.append(anime_data[anime_data['genre'].str.contains(i, regex=False)]['rating'].mean())\n\ngenre_rating_dict=pd.DataFrame({'Genre': list(genres['Genre']),\n  'rating': genre_rating })\ngenre_rating_dict.sort_values('rating', ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Genre - Ratings \nf, ax = plt.subplots(figsize=(10,12))\nsns.barplot(x=\"rating\", y=\"Genre\", data=genre_rating_dict, color='#719967')\nax.set(ylabel='Genre',xlabel=\"Rating\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much variation in average ratings for different genres. \n\n* Josei anime tends to have a higher rating averaging at 7 points. This can also because these anime titles have lower number of titles.\n\n* Dementia anime series are at the tail end of ratings and average on around 5 points.\n\n* Most of the remaining anime titles are averaging between 6-7 points ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**EDA on Rating dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_anime=rating_data.merge(anime_data[['name','genre','anime_id','type','episodes','members']],left_on='anime_id',right_on='anime_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets look at top 10 animes which have been rated the most in the dataset\ntop_rated= rating_anime.groupby(['anime_id','name']).count()['user_id'].reset_index().sort_values('user_id', ascending=False)\ntop_rated.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the top 10 numbers are some of the higher and more mainstream anime titles like Naruto, Code Geass and Attack on Titan (Shingeki no Kyojin). This list does look similar to the list in the \"anime_data\" table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(top_rated['user_id'], hist=True, kde=True, \n             bins=10, color = 'green', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Long Tail Plot:** The above plot shows what is called the Long tail phenomenon. This plot is used to explore popularity patterns in user-item interaction data such as clicks, ratings, or purchases. Typically, only a small percentage of items have a high volume of interactions, and this is referred to as the “head”. Most items are in the “long tail”, but they only make up a small percentage of interactions. \n\n\n[Reference](http://https://towardsdatascience.com/evaluation-metrics-for-recommender-systems-df56c6611093)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now that we have seen how our data looks like, let's Build our Recommender Systems!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Popularity Based Recommender System** \n* For our popularity based recommender system, we will be writing a function which takes the following parameters:\n    * **N**= Number of Shows to recommend\n    \n    * **Genre** = A list of genres that the recommender system should recommend out of. By default, it will take all genre into consideration\n    \n    * **Type** = A list of the type of show that the user wants (e.g TV, Movie, OVA etc). By default, it will take all types into consideration\n    \n    * **episodes_more_than** an integer value which indicates how many integers that particular anime show should be more than. e.g for a value of 50, the system should only show the shows which have more than 50 episodes. by default, it is set to 0.\n    * **Popularity Weight** = A value between 0 to 1.  A value of 1 means all the recommendations will be based on the popularity of the show. A value of 0 means all the recommendations will be based on the rating for the show. A value of 0.6 means, the popularity will be given a 60% weightage and 40% weightage will be given to the rating of a show","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def popularity_recommender(dataset,N = 0, \n                           Genre =[],\n                           Type= [],\n                           episodes_more_than=0,\n                           popularity_weight= 0.5\n                          ):    \n    if len(Genre)==0:\n        Genre = ['Josei', 'Thriller', 'Mystery', 'Police', 'Shounen', 'Psychological', 'Military', 'Supernatural', 'Romance', 'Shoujo Ai', 'Drama', 'School', 'Seinen', 'Harem', 'Shounen Ai', 'Super Power', 'Vampire', 'Shoujo', 'Samurai', 'Martial Arts', 'Magic', 'Action', 'Game', 'Sports', 'Historical', 'Adventure', 'Slice of Life', 'Sci-Fi', 'Space', 'Demons', 'Fantasy', 'Ecchi', 'Mecha', 'Comedy', 'Parody', 'Cars', 'Yaoi', 'Horror', 'Hentai', 'Kids', 'Yuri', 'Music', 'None_Genre', 'Dementia']\n    if len(Type) ==0:\n        Type = ['ONA', 'None_type', 'OVA', 'Special', 'Music', 'Movie', 'TV']\n\n    pop_recommender_df = dataset[\n    (dataset.episodes>=episodes_more_than) &\n    (dataset.genre.apply(len) !=(dataset.genre.apply(set)- set(Genre)).apply(len)) &\n    (dataset.type.isin(Type)) ].copy()\n       \n    if len(pop_recommender_df)==0:\n        print('No anime found with such conditions')\n\n    else:\n        # anime rating is in a range of 1 to 10 while popularity/members is in a larger range. \n        # because we want to show good shows with higher popularity, we will scale popularity on a range of 1 to 10 \n        # we shall be giving 70% weight to the popularity metric and 30% weight to show rating and calculate a new score called Popularity_quality_index\n        pop_recommender_df['scaled_members']=(minmax_scale(pop_recommender_df.members))*10\n        pop_recommender_df['Popularity_quality_index']=(popularity_weight* pop_recommender_df.scaled_members)+(1-popularity_weight)*pop_recommender_df.rating\n        df=pop_recommender_df.sort_values('Popularity_quality_index', ascending=False).iloc[0:N]\n        cols =['name','genre','type','episodes','rating','members']\n        return(df[cols])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_list=['Action']\nType_list = ['TV']\npopularity_recommender(anime_data,\n                       Genre=genre_list,\n                       episodes_more_than=0,\n                       Type=Type_list,\n                       N=20,\n                       popularity_weight=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks really good! I can see some of the popular Anime titles in this list. Because we have the ability to assign weights and tune our list of recommendations, and subset our results, its a very easy to use and intuitive solution.\n\nHowever, this method wont give you personalized recommendations.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning and creating training dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting rid of NA ratings\nprint('Original rating data size:',rating_data.shape[0])\nrating_data_clean = rating_data[rating_data.rating.notna()].reset_index(drop = True).copy()\n# Getting rid of Users with less than 250 ratings\nprint('Rating data after removing NA Ratings:',rating_data_clean.shape[0])\n# Getting rid of Anime titles with less than 250 ratings \nanime_rating_counts = pd.DataFrame(rating_data_clean.groupby('anime_id')['user_id'].nunique()).reset_index()\nreq_anime_ids= anime_rating_counts[anime_rating_counts['user_id']>250].anime_id\nuser_rating_counts=pd.DataFrame(rating_data_clean.groupby('user_id')['anime_id'].nunique()).reset_index()\nreq_user_ids= user_rating_counts[user_rating_counts['anime_id']>250].user_id\n# Splitting data into training and test sets\nrating_clean=rating_data_clean[rating_data_clean.anime_id.isin(req_anime_ids) &\n                              rating_data_clean.user_id.isin(req_user_ids)].copy().reset_index(drop = True)\nprint('Rating_data after filtering Anime Shows and Users:',rating_clean.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into training and test\ntrain_df, test_df = train_test_split(rating_clean,\n                                   test_size=0.20,\n                                   random_state=27)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Content Based Recommender System","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_genre_dummies= pd.get_dummies(anime_data.genre.apply(pd.Series).stack()).sum(level=0)\nanime_genre_dummies=pd.concat([anime_data, anime_genre_dummies], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_episode_encoding(num_episodes):\n    if(num_episodes<=13):\n        return('Xsmall')\n    elif (num_episodes<=50):\n        return('Small')\n    elif (num_episodes<=250):\n        return('Medium')\n    elif (num_episodes<=500):\n        return('Long')\n    else:\n        return('Xlong')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Engineering a few features\n\n#Making one hot encodings for type, Number of Episodes, and if the show is a very popular show, has medium popularity or lesser known popularity\n# Getting Show Type dummies\nanime_genre_dummies = pd.concat([anime_genre_dummies,pd.get_dummies(anime_data.type)],axis = 1)\n\n# Getting Episode size categories\nanime_genre_dummies = pd.concat([anime_genre_dummies,pd.get_dummies(anime_genre_dummies.episodes.apply(get_episode_encoding))],axis=1)\n\nreq_cols = list(set(anime_genre_dummies.columns) -set(['anime_id','name','genre','type','episodes','rating','members']))\nreq_cols.sort()\n\n# Creating Encodings\nanime_genre_dummies['Encoding']=anime_genre_dummies[req_cols].values.tolist()\nanime_genre_dummies","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This system will take the list of anime that the active user has watched. Based on the list of anime, we will try to find similar items in our list of anime. We are using Cosine similarity to find similarities between the active user's profile and the titles listed in our database\n\nThe function takes the following parameters:\n* **user_profile** = This will be the list of anime shows rated by our user\n* **N** = Number of recommendations to suggest to the Active User\n* **movie_profile ** = Content database","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def content_based_recommender(user_id,N,movie_profile):\n    # get user profile\n    user_profile=rating_clean[rating_clean['user_id']==user_id].copy().reset_index(drop = True)\n    user_profile = user_profile[user_profile.rating.notna()]\n    cols_to_merge= list(set(movie_profile.columns)-set(['rating']))\n    \n    user_profile =pd.merge(user_profile[['anime_id','rating']],movie_profile[cols_to_merge]\n             ,how= 'left', left_on='anime_id', right_on='anime_id')\n    \n    req_cols = list(set(movie_profile.columns) -set(['anime_id','name','genre','type','episodes','rating','members','Encoding']))\n    req_cols.sort()\n    # to generate the user profile, we are summing up the genre dummy variables that the user\n    #interacted with and weighing it with the rating that the user has given to the interacted items \n    user_profile['rating_scaled'] =  minmax_scale(user_profile['rating'])\n    \n    # Multiplying Ratings with the genre encodings\n    genre_weights=pd.DataFrame(user_profile[req_cols].multiply(user_profile['rating_scaled'],axis=\"index\").sum()).reset_index()\n    genre_weights.columns=['Genre','Weights']\n    \n    #Scaling the encodings so that we have encodings in 0-1 range to compare with movie encodings\n    genre_weights['weights_scaled']=minmax_scale(genre_weights['Weights'])\n    user_profile_weights= [list(genre_weights['weights_scaled'])]\n    \n    # Finding cosine similarity between user profile and movies \n    movie_profile['user_affinity']= cosine_similarity(user_profile_weights,list(movie_profile['Encoding']))[0]\n    \n    return(movie_profile.sort_values('user_affinity', ascending = False).reset_index().iloc[1:N][['name','genre','type','episodes','rating','members','user_affinity']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_clean.user_id.unique()[1:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"content_based_recommender(user_id =17, N= 10,movie_profile = anime_genre_dummies.copy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the recommendations for user 17, we can see that the user really likes Action,Comedy and High School based Anime Shows (Mostly TV).\n\nNote that the Content based Recommender System is not giving highly rated or popular anime series. This recommender system will provide items similar to the content that the user has seen and rated currently.\nFactors like: Genre, type of Anime and Number of episodes are the factors that the recommender system uses to find similar other content!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Collaborative Filtering using the 'Surprise' Package","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import surprise\nfrom surprise.model_selection import cross_validate\nfrom surprise import SVD,SVDpp,NMF,NormalPredictor,KNNBaseline,KNNBasic,KNNWithMeans,KNNWithZScore,BaselineOnly,CoClustering,SlopeOne, Reader, Dataset\n# we will be using the Surprise Python Package to get our recommendations.\n# We will try a few different Models for implementing Collaborative filtering. \n\nratings_dict = {'itemID': rating_clean.anime_id,\n                'userID': rating_clean.user_id,\n                'rating': rating_clean.rating}\ndf = pd.DataFrame(ratings_dict)\n# A reader is still needed but only the rating_scale param is requiered.\nreader = Reader(rating_scale=(1, 10))\n\n# The columns must correspond to user id, item id and ratings (in that order).\ndata = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)\nimport time \nstart = time.time()\n\n\nbenchmark = []\n# Iterate over all algorithms\nfor algorithm in [SVD(), NMF()]:\n    # Perform cross validation\n    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=True, n_jobs = 1)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp)\n    \npd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')  \nend = time.time()\nprint(end - start)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Looks like the SVD model has the least RMSE Error. We will be using this model and fine tune it further over the whole dataset to see if we can improve the model performance **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\nfrom surprise import SVD\nfrom surprise import Dataset\n\n\ndef get_top_n(predictions, n=10):\n    '''Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    '''\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n\n\n# Train an SVD algorithm on the Anime Rating cleaned dataset.\n\nratings_dict = {'itemID': rating_clean.anime_id,\n                'userID': rating_clean.user_id,\n                'rating': rating_clean.rating}\ndf = pd.DataFrame(ratings_dict)\n# A reader is still needed but only the rating_scale param is requiered.\nreader = Reader(rating_scale=(1, 10))\n# The columns must correspond to user id, item id and ratings (in that order).\ndata = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)\n\ntrainset = data.build_full_trainset()\nalgo = SVD()\nalgo.fit(trainset)\n\n# Than predict ratings for all pairs (u, i) that are NOT in the training set.\ntestset = trainset.build_anti_testset()\npredictions = algo.test(testset)\n\ntop_n = get_top_n(predictions, n=10)\n\n# Print the recommended items for each user\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, _) in user_ratings])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision and recall @ k","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\nfrom surprise import Dataset\nfrom surprise import SVD\nfrom surprise.model_selection import KFold\n\n\ndef precision_recall_at_k(predictions, k=10, threshold=3.5):\n    '''Return precision and recall at k metrics for each user.'''\n\n    # First map the predictions to each user.\n    user_est_true = defaultdict(list)\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n\n    precisions = dict()\n    recalls = dict()\n    for uid, user_ratings in user_est_true.items():\n\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n\n        # Recall@K: Proportion of relevant items that are recommended\n        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n\n    return precisions, recalls\n\nprecisions, recalls = precision_recall_at_k(predictions, k=10, threshold=7)\n# Precision and recall can then be averaged over all users\nprint(sum(prec for prec in precisions.values()) / len(precisions))\nprint(sum(rec for rec in recalls.values()) / len(recalls))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets use an example user","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uid_test=list(top_n.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_recommendations(uid_profile):\n    user_top_10 = rating_anime[rating_anime.user_id == uid_profile].sort_values('rating',ascending = False ).iloc[1:10]\n    rated_top_10= user_top_10[['name','genre','type','episodes','rating']]\n    anime_id_user= [iid for (iid, _) in top_n[uid_profile]]\n    top_10_recommendations = anime_data[anime_data.anime_id.isin(anime_id_user)][['name','genre','type','episodes','rating']]\n    return rated_top_10,top_10_recommendations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uid_test[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting user ratings and recommendations for user number 38\n\nuser_rated,top_10_recommendations =  get_recommendations(uid_profile=38)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('these are the top 10 anime titles rated by user 38')\nuser_rated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('these are our recommendations for the user 38')\ntop_10_recommendations","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}