{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.cluster import KMeans, MeanShift,estimate_bandwidth, AgglomerativeClustering, AffinityPropagation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\nimport scipy.cluster.hierarchy as sch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv'\ndata = pd.read_csv(file_path)\ndata.info()\nprint('\\nData shape: ', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values and all the columns except for 'Gender' column have numerical values. I'll use LabelEncoder to get rid of these categorical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ndata['GenderColumn'] = label_encoder.fit_transform(data['Gender'])\n\ngenders_before_encoding = data['Gender'].unique()\nprint('\\n--- Genders before label encoding: \\n', genders_before_encoding)\ngenders_after_encoding = data['GenderColumn'].unique()\n\nprint('\\n--- Genders after label encoding: \\n', genders_after_encoding)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Delete the Gender column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('Gender', axis = 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(data['Age'], data['GenderColumn'], 'o')\nplt.title('Age vs. Gender')\nplt.xlabel('Age')\nplt.ylabel('Gender')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(data['Annual Income (k$)'], data['GenderColumn'], 'o')\nplt.title('Annual Income vs. Gender')\nplt.xlabel('Annual Income')\nplt.ylabel('Gender')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(data['Annual Income (k$)'], data['Spending Score (1-100)'], 'o')\nplt.title('Annual Income vs. Spending Score')\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Income and spending score"},{"metadata":{"trusted":true},"cell_type":"code","source":"income_and_spending_data = data.iloc[:,[2,3]]\nincome_and_spending_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(data['Age'], data['Annual Income (k$)'], 'o')\nplt.plot(data['Age'], data['Spending Score (1-100)'], 'o')\nplt.legend(['Age vs. Annual Income', 'Age vs. Spending Score'])\nplt.title('Age vs. Annual Income\\n Age vs. Spending Score')\nplt.xlabel('Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# 1. K-Means algorithm"},{"metadata":{},"cell_type":"markdown","source":"I'll determine the optimal number of clusters. To do this I'll use the Elbow method, testing number of clusters between 1 and 10. I'll choose the number of clusters based on the obtained elbow. To obtain the elbow, I'll use the inertia which is the sum of distances of all the points\nwithin a cluster from the centriod of that cluster. The lesser the inertia, the better the clusters are."},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia_values = []\nk_values = list(range(1, 10))\n\nfor value in k_values:\n    kmeans = KMeans(n_clusters=value)\n    kmeans.fit(income_and_spending_data)\n    inertia_values.append(kmeans.inertia_)\n\nplt.figure(figsize=(6, 6))\nplt.plot(k_values, inertia_values, '-o')\nplt.xlabel(r'Number of clusters')\nplt.ylabel('Sum of squared distance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_clusters = 5\n\nkmeans_model = KMeans(n_clusters=number_of_clusters) \nkmeans_model.fit(income_and_spending_data)\n\ny_k_means = kmeans.fit_predict(income_and_spending_data)\n\nincome_and_spending_data = np.array(income_and_spending_data)\n\nplt.title(\"Clusters\", fontsize=20)\nplt.xlabel(\"Annual Income\")\nplt.ylabel(\"Spending Score\")\n\nplt.scatter(income_and_spending_data[y_k_means ==0,0], income_and_spending_data[y_k_means == 0,1], c='blue')\nplt.scatter(income_and_spending_data[y_k_means ==1,0], income_and_spending_data[y_k_means == 1,1], c='black')\nplt.scatter(income_and_spending_data[y_k_means ==2,0], income_and_spending_data[y_k_means == 2,1], s=100, c='green')\nplt.scatter(income_and_spending_data[y_k_means ==3,0], income_and_spending_data[y_k_means == 3,1], s=100, c='gray')\nplt.scatter(income_and_spending_data[y_k_means ==4,0], income_and_spending_data[y_k_means == 4,1], s=100, c='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the previous plot, the clients can be classified as follows:\n\n* clients that don't earn too much:\n      they don't spend too much(the black cluster)\n      they spend a lot(the red cluster)\n      they have medium spending levels(the green cluster)\n* clients with high incomes and low spending levels(the gray cluster)\n* clients with medium incomes but high spendings(the blue cluster)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign the label\ndata['cluster_id'] = kmeans_model.labels_\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clusters analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x='cluster_id', y='Age', data=data)\nplt.title('Age variation across clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x='cluster_id', y='Annual Income (k$)', data=data)\nplt.title('Annual Income across clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x='cluster_id', y='Spending Score (1-100)', data=data)\nplt.title('Spending Score across clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Mean Shift Algorithm"},{"metadata":{},"cell_type":"markdown","source":"As opposed to K-Means, when using Mean Shift, I don’t need to know the number of clusters beforehand. \nAlthough the Mean Shift is computationally expensive(O(n²)), I'll use it because the dataset is not that big.\n\n\nSteps:\n1. Define a window (bandwidth of the kernel) and place the window on a data point.\n2. Calculate the mean for all the points in the window.\n3. Move the center of the window to the location of the mean.\n4. Repeat steps 2 and 3 until there is convergence."},{"metadata":{"trusted":true},"cell_type":"code","source":"bandwidth = estimate_bandwidth(income_and_spending_data, quantile=0.2)\n\nmean_shift = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nmean_shift = mean_shift.fit(income_and_spending_data)\ny_mean_shift = mean_shift.predict(income_and_spending_data)\n\nmean_shift_labels = mean_shift.labels_\n\nlabels_unique = np.unique(mean_shift_labels)\nmean_shift_estimated_clusters = len(labels_unique)\n\nprint(\"number of estimated clusters : %d\" % mean_shift_estimated_clusters)\n\nplt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], c=mean_shift_labels)\nplt.xlabel('Annual income')\nplt.ylabel('Spending score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Agglomerative Clustering"},{"metadata":{},"cell_type":"markdown","source":"The dendrogram will help me figure out the optimal number of clusters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dendrogram = sch.dendrogram(sch.linkage(income_and_spending_data, method='ward'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the dendrogram we can see that the optimal number of clusters is also 5."},{"metadata":{},"cell_type":"markdown","source":"* The linkage criteria refers to how the distance between clusters is calculated. I used ward linkage which computes the distance between clusters as the sum of squared differences within all clusters. \n* The affinity is the method used to calculate the distance between data points. I used the euclidean distance."},{"metadata":{"trusted":true},"cell_type":"code","source":"agglomerative_clustering_model = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\ny_agglomerative_clustering = agglomerative_clustering_model.fit_predict(income_and_spending_data)\nagglomerative_labels = agglomerative_clustering_model.labels_\nagglomerative_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(income_and_spending_data[agglomerative_labels==0, 0], income_and_spending_data[agglomerative_labels==0, 1], s=50, marker='o', color='blue')\nplt.scatter(income_and_spending_data[agglomerative_labels==1, 0], income_and_spending_data[agglomerative_labels==1, 1], s=50, marker='o', color='red')\nplt.scatter(income_and_spending_data[agglomerative_labels==2, 0], income_and_spending_data[agglomerative_labels==2, 1], s=50, marker='o', color='green')\nplt.scatter(income_and_spending_data[agglomerative_labels==3, 0], income_and_spending_data[agglomerative_labels==3, 1], s=50, marker='o', color='purple')\nplt.scatter(income_and_spending_data[agglomerative_labels==4, 0], income_and_spending_data[agglomerative_labels==4, 1], s=50, marker='o', color='yellow')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign cluster labels\ndata['aggl_labels'] = agglomerative_clustering_model.labels_\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clusters analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plots\nsns.stripplot(x='aggl_labels', y='Age', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x='aggl_labels', y='Annual Income (k$)', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x='aggl_labels', y='Spending Score (1-100)', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Affinity Propagation"},{"metadata":{},"cell_type":"markdown","source":"This algorithm doesn't require a preset cluster number. It takes as input measures of similarity between pair of data points. As they have similarities, they can belong to the same cluster. "},{"metadata":{"trusted":true},"cell_type":"code","source":"affinity_propagation = AffinityPropagation(max_iter=150)\naffinity_propagation.fit(income_and_spending_data)\ncluster_centers_indices = affinity_propagation.cluster_centers_indices_\naffinity_estimated_clusters = len(cluster_centers_indices)\n\n# Predict the cluster for all the samples\ny_affinity_propagation = affinity_propagation.predict(income_and_spending_data)\n\nplt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], c=affinity_propagation.labels_.astype(float), marker=\"o\", picker=True)\nplt.title(f'Estimated number of clusters = {affinity_estimated_clusters}')\nplt.xlabel('Annual income')\nplt.ylabel('Spending score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"affinity_prop = AffinityPropagation(random_state=5).fit(np.array(income_and_spending_data))\naffinity_prop\naffinity_prop.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing the results\n#  Silhouette Score    "},{"metadata":{},"cell_type":"markdown","source":"The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. \nThe Silhouette Coefficient for a sample is (b - a) / max(a, b), where b is the distance between a sample and the nearest cluster that \nthe sample is not a part of. The Silhouette Coefficient is only defined if the number of labels is 2 <= n_labels <= n_samples - 1.\nThe best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been \nassigned to the wrong cluster, as a different cluster is more similar.\n"},{"metadata":{},"cell_type":"markdown","source":"The silhouette_score gives the average value for all the samples.\nThis gives a perspective into the density and separation of the formed clusters"},{"metadata":{},"cell_type":"markdown","source":"**For K-Means**"},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_avg_k_means = silhouette_score(income_and_spending_data, y_k_means, metric='euclidean')\nprint(f'For 5 clusters, the average silhouette_score is: {silhouette_avg_k_means}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For Mean Shift**"},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_avg_mean_shift = silhouette_score(income_and_spending_data, y_mean_shift, metric='euclidean')\nprint(f'For {mean_shift_estimated_clusters} clusters, the average silhouette_score is: {silhouette_avg_mean_shift}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For Agglomerative Clustering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_avg_agglomerative = silhouette_score(income_and_spending_data, y_agglomerative_clustering, metric='euclidean')\nprint(f'For 5 clusters, the average silhouette_score is: {silhouette_avg_agglomerative}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For Affinity Propagation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_avg_affinity = silhouette_score(income_and_spending_data, y_affinity_propagation, metric='euclidean')\nprint(f'For {affinity_estimated_clusters} clusters, the average silhouette_score is: {silhouette_avg_affinity}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}