{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read and check data","metadata":{}},{"cell_type":"code","source":"stroke_data = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\nstroke_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stroke_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some visual analysis","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data = stroke_data[stroke_data[\"stroke\"]==0],x = \"age\",shade = True,label = \"Healthy\")\nsns.kdeplot(data = stroke_data[stroke_data[\"stroke\"]==1],x = \"age\",shade = True,label = \"Stroke\")\nplt.legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions:\n* Most stroke patients are roughly above 40 years of age and married.\n* Chances of getting a stroke is less for people below the age of 25\n* People below the age of 30 don't get married that often","metadata":{}},{"cell_type":"code","source":"\nsns.kdeplot(data = stroke_data[stroke_data['stroke'] == 0], x = 'avg_glucose_level', shade = True,   label = \"Healthy\")\nsns.kdeplot(data = stroke_data[stroke_data['stroke'] == 1], x = 'avg_glucose_level', shade = True,   label = \"Stroke\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(x = stroke_data.stroke,y = stroke_data.age,hue = stroke_data.Residence_type,shade = True,alpha = 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion:\n* Doesn't matter if you are in urban or rural area.","metadata":{}},{"cell_type":"code","source":"sns.displot(x = stroke_data.stroke,y = stroke_data.age,hue = stroke_data.work_type,kind = 'kde',fill = True,alpha = 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n* People worked in govt jobs are not prone to stroke","metadata":{}},{"cell_type":"code","source":"h = stroke_data[stroke_data['stroke']==0].smoking_status.value_counts()\ns = stroke_data[stroke_data['stroke']==1].smoking_status.value_counts()\nplt.subplot(2,1,1)\nplt.bar(h.index,height = h.values,width = 0.2,label = \"healthy\",color = \"green\")\nplt.legend()\nplt.subplot(2,1,2)\nplt.bar(s.index,height = s.values,width = 0.2,label = \"stroke\",color = \"magenta\")\nplt.legend()\n\n\n\n#sns.histplot(data = stroke_data,x = 'stroke',y = 'age',hue = 'smoking_status')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data = stroke_data,x = 'age',y = 'gender',hue = 'stroke')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data = stroke_data[stroke_data['stroke'] == 0], x = 'avg_glucose_level', shade = True,   label = \"Healthy\")\nsns.kdeplot(data = stroke_data[stroke_data['stroke'] == 1], x = 'avg_glucose_level', shade = True,   label = \"Stroke\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data = stroke_data,x = 'bmi',y = 'age',hue = 'stroke',fill = True,alpha = 0.8)\nplt.text(-5,-70,\"Conclusion \\n    For people who had a stroke, the bmi seems to have a major role for people aged 40-80 with single dominating peak \\n   Younger people tend to have lower bmi and older people tend to have bmi in the range 25-35 indicated by the double peak.\",{'color':\"red\",'fontfamily':\"serif\",'fontsize':14,'fontweight':5,'linespacing':1.5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion:\n* For people who had a stroke, the bmi seems to have a major role for people aged 40-80 with single dominating peak\n* Younger people tend to have lower bmi and older people tend to have bmi in the range 25-35 indicated by the double peak.","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(data = stroke_data,x = 'age',y = 'avg_glucose_level',hue = 'stroke',fill = True,alpha = 0.8)\nplt.text(-20,-150,\"Conclusion \\n Aged people with irregular blood glucose(high or low) tend to be prone to stroke\",{'color':\"red\",'fontfamily':\"serif\",'fontsize':14,'fontweight':5,'linespacing':1.5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data = stroke_data,x = 'bmi',y = 'avg_glucose_level',hue = 'stroke',fill = True,alpha = 0.8)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = stroke_data.corr()\ncorrelation_matrix[\"stroke\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the data","metadata":{}},{"cell_type":"code","source":"stroke_labels = stroke_data[\"stroke\"].copy()\nstroke_data_drop = stroke_data.drop([\"bmi\",\"Residence_type\",\"id\",\"stroke\"],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stroke_data_drop.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One hot encoding the categorical variables","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nstroke_data_cat = stroke_data_drop[[\"gender\",\"ever_married\",\"work_type\",\"smoking_status\"]]\nencoder = OneHotEncoder()\nstroke_1hot = encoder.fit_transform(stroke_data_cat)\nstroke_1hot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.categories_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\ncat_attribs = [\"gender\",\"ever_married\",\"work_type\",\"smoking_status\"]\nnum_attributes = [\"age\",\"hypertension\",\"heart_disease\",\"avg_glucose_level\"]\n\nnum_pipeline = Pipeline([('std_scaler',StandardScaler())])\nfull_pipeline = ColumnTransformer([(\"num\",num_pipeline,num_attributes),(\"cat\",OneHotEncoder(),cat_attribs)])\nstroke_prepared = full_pipeline.fit_transform(stroke_data_drop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=1)\n\nfor train_index, test_index in sss.split(stroke_prepared, stroke_labels):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = stroke_prepared[train_index], stroke_prepared[test_index]\n    y_train, y_test = stroke_labels[train_index], stroke_labels[test_index]\n\n#X_train, X_test, y_train, y_test = train_test_split(stroke_prepared,stroke_labels, test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(1000,500,500,100,10), activation = 'logistic',solver='adam', alpha=0.0001, batch_size='auto',max_iter=200, shuffle=True, random_state=1, verbose=False)\nclf.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(clf,X_train,y_train,cv = 3)\nfrom sklearn.metrics import mean_squared_error\n#clf.score(X_test,y_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix\nThe confusion Matrix shows that the classifier is not at all doing it job. This is because, the data is highly skewed. 95% of the data falls under the \"non-stroke\" category. Only 5% data is labelled \"stroke\". This makes it possible for even the dumbest classifier algorithm to score atleast high since all it has to do is classify everyone are a \"non-stroke\" candidate and still stands at getting 95% accuracy. This also shows that accuracy of prediction is not a measure for this particular type of problems.\n\n\nThe confusion matrix on the other hand gives us a clearer picture. The model correctly classifies 3889 data as \"non-stroke\" ($\\textit{True-Negative}$) but it also clasifies the 199 \"stroke\" cases as \"non stroke\" ($\\textit{False-Negative}$). Note that both the \"non-stroke\" classified as \"stroke\"($\\textit{False-Positive}$) and the \"stroke\" classified correctly ($\\textit{True-Positive}$) are zero. This emphasises the point discussed above.\n\nThis gives us reason to develop either even powerful models or manipulate the data in such a was as to get a better model out of it. A powerful model with such small data will be prone to overfit. Therefore we need to augment our data to get a better model.","metadata":{}},{"cell_type":"markdown","source":"## Using a different model to try classification\n* From the sklearn, we use the stochastic gradient descent model to try and improve our prediction","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state = 2)\ny_sgd_pred = cross_val_predict(sgd_clf,X_train,y_train,cv=3)\nconfusion_matrix(y_train,y_sgd_pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = cross_val_predict(sgd_clf,X_train,y_train,cv=3,method = \"decision_function\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Precision and Recall\n* Precision is the accuracy of the model. How many ***Positive*** are actually ***Positives*** for \"stroke\"? The accuracy is given as:\n\n$$\\frac{True Positive}{True Positive + False Positive}$$\n\n* Recall is how many of the actual \"stroke\" are actually classified as \"stroke\" by the model?\nNote that actual \"stroke\" is composed in both the ***True Positive*** and the ***False Negative***\n$$\\frac{True Positive}{True Positive + False Negative}$$\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecisions,recalls,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,precisions[:-1],'b--',label = \"Precision\")\nplt.plot(thresholds,recalls[:-1],'g-',label = \"Recall\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(recalls,precisions)\nplt.xlabel(\"Recall\",fontsize = 14)\nplt.ylabel(\"Precision\",fontsize = 14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that because of the disparity in the data, as the ***Recall*** increases, there is a drastic change in ***Precision***. Since this model might be used for diagnostic purposes, we can afford to err on the side of caution. What it means is we can afford to have ***false positives for*** \"stroke\" but when we miss ***false negatives*** someone's life may be danger. Therefore what we can do here is change the threshold of the model so that the precision of the model will take a hit but we make sure that most of the people prone to \"stroke\" are identitifed. Lets say we want the Recall value at 90% and see how the model behaves.","metadata":{}},{"cell_type":"code","source":"threshold_90_recall = thresholds[np.argmax(recalls >= 0.90)]\nthreshold_90_recall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_90 = (y_scores >= threshold_90_recall)\ny_train_pred_90","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import recall_score,precision_score\nrecall_score(y_train,y_train_pred_90)\nprecision_score(y_train,y_train_pred_90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that our precision takes a hard hit. We might as well classify everyone as \"stroke\" prone. This is very bad model and negates the whole purpose of building a model in the first place. Let's explore more avenues to come up with a resaonable model.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr,tpr,thresholds = roc_curve(y_train,y_scores)\nplt.tight_layout()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate (Recall)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The higher the recall (TPR) more the false positive (FPR) that the classifier produces. A good classifier stays as far away from the line as possible (top-left corner)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFC\nforest_clf = RFC(random_state = 2)\ny_train_pred_forest = cross_val_predict(forest_clf,X_train,y_train,cv=5)\nconfusion_matrix(y_train,y_train_pred_forest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba_forest = cross_val_predict(forest_clf,X_train,y_train,cv=5,method = \"predict_proba\")\ny_scores_forest = y_proba_forest[:,-1]\nfpr_forest,tpr_forest,thresholds_forest = roc_curve(y_train,y_scores_forest)\nprec_forest,rec_forest,thresholds_forest = precision_recall_curve(y_train,y_scores_forest)\nplt.plot(thresholds_forest,prec_forest[:-1],'b--',label = \"precision\")\nplt.plot(thresholds_forest,rec_forest[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr_forest,tpr_forest)\nplt.plot([0,1],[0,1],'k--')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score #area under the roc curve\nroc_auc_score(y_train,y_scores_forest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The area under the ROC curve give us the performance of the classifier. A perfect classifier will have a value of 1 and a random classifier will have a value of 0.5 as indicated by the diagona; line above. The RandomFroest Classifier performs slightly better than the previous model.\n","metadata":{}},{"cell_type":"markdown","source":"# Data Augmentation and Training.\nUsing Data Augmentation from [Nikunj Malpani's](https://www.kaggle.com/nikunjmalpani/stroke-prediction-step-by-step-guide) notebook.","metadata":{}},{"cell_type":"code","source":"# Using SMOTE\nfrom imblearn.over_sampling import SMOTE\nsampler = SMOTE(random_state = 42)\nX = stroke_prepared\ny = stroke_labels\nX,y= sampler.fit_resample(X,y.values.ravel())\ny_cat = pd.DataFrame({'stroke':y})\nsns.countplot(data = y_cat, x = 'stroke', y= None)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\ny_pred_clf = cross_val_predict(clf,X_train,y_train,cv = 3)\nconfusion_matrix(y_train,y_pred_clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now observe how our earlier models perfrom on the new dataset.","metadata":{}},{"cell_type":"code","source":"y_scores_clf = cross_val_predict(clf,X_train,y_train,cv=3,method = \"predict_proba\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = y_scores_clf[:,-1]\nfpr,tpr,thresholds = roc_curve(y_train,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_train,y_scores)\nplt.text(0.6,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize': 16})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = cross_val_predict(sgd_clf,X_train,y_train,cv=3,method = \"decision_function\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr,tpr,thresholds = roc_curve(y_train,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_train,y_scores)\nplt.text(0.6,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize':16})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba_forest = cross_val_predict(forest_clf,X_train,y_train,cv=5,method = \"predict_proba\")\ny_scores = y_proba_forest[:,-1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr,tpr,thresholds = roc_curve(y_train,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_train,y_scores)\nplt.text(0.6,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize':16})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data Predictions\nSince the Random Forest was the best model among all the three models tested we will use that model for Test Data prediction and evaluate our scores.","metadata":{}},{"cell_type":"code","source":"y_proba_forest_test = cross_val_predict(forest_clf,X_test,y_test,cv=5,method = \"predict_proba\")\ny_scores = y_proba_forest_test[:,-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr,tpr,thresholds = roc_curve(y_test,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_test,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Precission/Recall\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_test,y_scores)\nplt.text(0.5,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize':16})\nplt.title(\"Test Prediction A_ROC score\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThe Random Forest Classifier has excellent performance for the test data.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}