{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Prepare all our necessary libraries\nimport numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\n\n#pytorch libraries\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Pad, Compose,CenterCrop, ToPILImage, Normalize, ConvertImageDtype, Resize\n\nfrom torchvision.models import resnet50\nfrom torch import nn\nfrom torch.nn import init, Linear, ReLU, Softmax\nfrom torch.nn.init import xavier_uniform_\nfrom torch.optim import SGD, Adam\nimport torch.nn.functional as F\nfrom torch.nn.init import xavier_uniform_\n\nimport datetime\n\n# libs fÃ¼r AE\n\n!pip install cleverhans --upgrade\nfrom cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T10:15:37.675407Z","iopub.execute_input":"2021-06-29T10:15:37.675789Z","iopub.status.idle":"2021-06-29T10:15:43.341017Z","shell.execute_reply.started":"2021-06-29T10:15:37.675755Z","shell.execute_reply":"2021-06-29T10:15:43.34006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREPARE DATA","metadata":{}},{"cell_type":"code","source":"#Prepare our Dataset Structure, as it has to be normalized for pytorch\n\nfrom torchvision.io import read_image\n\nclass GTSRB(Dataset):\n    def __init__(self, annotations_file, img_dir , transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)[[\"Path\",\"ClassId\"]]\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        label = self.img_labels.iloc[idx, 1]\n        image = read_image(img_path)\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:15:43.342788Z","iopub.execute_input":"2021-06-29T10:15:43.34314Z","iopub.status.idle":"2021-06-29T10:15:43.353203Z","shell.execute_reply.started":"2021-06-29T10:15:43.3431Z","shell.execute_reply":"2021-06-29T10:15:43.352422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def float_transform(tensor):\n    return tensor.float()\n\n# creating training and test datasets; Normalize pictures them to (3,90,90) size\nimg_dir = \"/kaggle/input/gtsrb-german-traffic-sign/\"\ntrain_file = \"/kaggle/input/gtsrb-german-traffic-sign/Train.csv\"\ntrain_data = GTSRB(img_dir = img_dir, annotations_file = train_file,\n                   transform = Compose([Resize((30,30)), ConvertImageDtype(torch.float32)]))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:15:43.354899Z","iopub.execute_input":"2021-06-29T10:15:43.355233Z","iopub.status.idle":"2021-06-29T10:15:43.418288Z","shell.execute_reply.started":"2021-06-29T10:15:43.355198Z","shell.execute_reply":"2021-06-29T10:15:43.417431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare DataLoader\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:15:43.41976Z","iopub.execute_input":"2021-06-29T10:15:43.420082Z","iopub.status.idle":"2021-06-29T10:15:43.428803Z","shell.execute_reply.started":"2021-06-29T10:15:43.420048Z","shell.execute_reply":"2021-06-29T10:15:43.427808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:15:43.430388Z","iopub.execute_input":"2021-06-29T10:15:43.430896Z","iopub.status.idle":"2021-06-29T10:15:43.588024Z","shell.execute_reply.started":"2021-06-29T10:15:43.430834Z","shell.execute_reply":"2021-06-29T10:15:43.587191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nprint(\"Maximum: \",train_features[0].max(),\"\\t Minimum: \", train_features[0].min() )\nimg = train_features[0]\nlabel = train_labels[0]\nimg = ToPILImage()(img).convert(\"RGB\")\nplt.imshow(img)\nplt.show()\nprint(f\"Label: {label}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:15:43.589271Z","iopub.execute_input":"2021-06-29T10:15:43.589624Z","iopub.status.idle":"2021-06-29T10:15:43.706933Z","shell.execute_reply.started":"2021-06-29T10:15:43.589586Z","shell.execute_reply":"2021-06-29T10:15:43.705983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CREATE MODEL ","metadata":{}},{"cell_type":"code","source":"#downloading resent50 pretrained on ImageNet \nresn50 = resnet50(pretrained=True, progress = True)\n\n#adjust resnet50 to my dataset\nclass r50(nn.Module):\n    def __init__(self, pretrained_model):\n        super(r50,self).__init__()\n        self.rn50 = pretrained_model\n        self.fl1 = nn.Linear(1000, 256)\n        self.fl2 = nn.Linear(256,43)\n        \n    def forward(self, X):\n        X = self.rn50(X)\n        X = F.relu(self.fl1(X))\n        X = F.dropout(X, p=0.25)\n        X = self.fl2(X)\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:15:43.708374Z","iopub.execute_input":"2021-06-29T10:15:43.708728Z","iopub.status.idle":"2021-06-29T10:15:44.386344Z","shell.execute_reply.started":"2021-06-29T10:15:43.70869Z","shell.execute_reply":"2021-06-29T10:15:44.385445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"model = r50(resn50)\ncriterion = nn.CrossEntropyLoss(reduction = \"mean\")\noptim = Adam(model.parameters(), lr = 0.001)\n\n#put on cuda if possible\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnb_epochs = 10\n\nae_predictions = []\nae_accuracy = []\naccuracy = []\n\neps = 0.2\neps_iter = 0.01\nsteps = 30\n\nfor epoch in range(nb_epochs):\n    running_loss = 0\n    ae_count = 0\n    for i, data in enumerate(train_dataloader, 0):\n        train_features, train_labels = data\n        train_features, train_labels = train_features.to(device), train_labels.to(device)\n        \n        #ae training\n        model.eval()\n            \n        ae = projected_gradient_descent(model,train_features[-15:], eps, eps_iter, steps,np.inf, clip_min=0, clip_max=1).detach()\n        \n        if i == 100:\n            image = ToPILImage()(ae[0]).convert(\"RGB\")\n            original = ToPILImage()(train_features[-15]).convert(\"RGB\")\n            \n            fig, axs = plt.subplots(2, 1, constrained_layout=True)\n            axs[0].imshow(image)\n            axs[0].set_title('Adversarial example')\n            fig.suptitle('Comparison AE vs. Original', fontsize=12)\n\n            axs[1].imshow(original)\n            axs[1].set_title('Original')\n            plt.show()\n            print(\"Maximum pixel distance: \", torch.max(train_features[-15] - ae[0]))\n        \n        \n        train_features = torch.cat((train_features[:-15], ae)).to(device)\n        model.train()\n        #ae_count += 1\n        \n        #standard training\n        optim.zero_grad()\n        prediction = model(train_features).to(torch.float32)\n        loss = criterion(prediction, train_labels.to(torch.long)) \n        loss.backward()\n        optim.step()\n\n        running_loss += loss.item()\n        \n        #calculate accuracy\n        corrects = 0\n\n        ae_corrects = 0\n        pred = torch.argmax(prediction, dim = 1)\n        \n        for i, p,l in zip(range(64), pred, train_labels):\n            if i > (64-15):\n                ae_predictions.append((p,l))\n                ae_corrects += (p==l)\n                ae_count +=1\n            corrects += (p == l)\n        ae_accuracy.append(int(ae_corrects)/15)\n        accuracy.append(int(corrects)/len(pred))\n\n\n    \n    \n    print(\"Epoch: {j}/{total_epochs} \\t Time: {time} \\t Loss: {Loss} \\t Accuracy: {acc}\".format(j = epoch+1,Loss = running_loss/len(train_dataloader),total_epochs = nb_epochs,time = datetime.datetime.now().time(), acc=np.mean(accuracy[-50:])))\n    print(\"{number} adversarial examples have been created. The accuracy against ae's is {ae_acc}\".format(number=ae_count, ae_acc = np.mean(ae_accuracy[-50:])))    \n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:15:44.388566Z","iopub.execute_input":"2021-06-29T10:15:44.388927Z","iopub.status.idle":"2021-06-29T10:32:05.550125Z","shell.execute_reply.started":"2021-06-29T10:15:44.388887Z","shell.execute_reply":"2021-06-29T10:32:05.547603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save accuracy information \naccuracy_data = pd.DataFrame({\"accuracy\":accuracy, \"ae_accuracy\" : ae_accuracy})\naccuracy_data.to_csv((\"/kaggle/working/training_eps{}_epsiter{}_st{}\".format(eps*10, eps_iter*10, steps).replace(\".\",\"\") + \".csv\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:36:17.309934Z","iopub.execute_input":"2021-06-29T10:36:17.310371Z","iopub.status.idle":"2021-06-29T10:36:17.344525Z","shell.execute_reply.started":"2021-06-29T10:36:17.310321Z","shell.execute_reply":"2021-06-29T10:36:17.339472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model for our test notebook    \nPATH = '/kaggle/working/gtsrbX.pth'\ntorch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:32:05.552432Z","iopub.status.idle":"2021-06-29T10:32:05.552884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#show an AE of the training set\nimage = ToPILImage()(ae[0]).convert(\"RGB\")\nplt.imshow(image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:32:05.553785Z","iopub.status.idle":"2021-06-29T10:32:05.554345Z"},"trusted":true},"execution_count":null,"outputs":[]}]}