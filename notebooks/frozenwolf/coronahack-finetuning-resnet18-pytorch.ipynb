{"cells":[{"metadata":{"id":"rQTBsYR66LOF"},"cell_type":"markdown","source":"# **CoronaHack - Fine tuning resnet18 pre-trained model with Pytorch**\n\n#### We will be analysing the train dataset and fine tuning a pre-trained model"},{"metadata":{"id":"6AG_VIOX2MaJ"},"cell_type":"markdown","source":"#### **Downloading Dataset from Kaggle (Optional)** "},{"metadata":{"id":"iT8AaYVh6P-D","outputId":"79e90a6c-e54b-4b7f-e42f-dc4f52043260","trusted":false},"cell_type":"code","source":"# Downloading dataset:\nfrom zipfile import ZipFile\nimport os\nimport pandas as pd\n\n# The below code is ran in google colab\nos.environ[\"KAGGLE_USERNAME\"] = \"KAGGLE_USERNAME\"  # username from the json file\nos.environ[\"KAGGLE_KEY\"] = \"KAGGLE_KEY\"  # key from the json file\n!kaggle datasets download -d praveengovi/coronahack-chest-xraydataset  # api copied from kaggle\n\n# Create a ZipFile Object and load chest-xray-pneumonia.zip in it\nwith ZipFile(\"/content/coronahack-chest-xraydataset.zip\", \"r\") as zipObj:\n    # Extract all the contents of zip file in current directory\n    zipObj.extractall()\n\n# KAGGLE LINK: https://www.kaggle.com/praveengovi/coronahack-chest-xraydataset\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Jho2dlEX6n_I"},"cell_type":"markdown","source":"#### **Moving train dataset to designated directory** \n\n#### We will move the images used for training and testing by reading the 'Chest_xray_Corona_Metadata.csv' which contains the information about each image.We will split the dataset into images which are mapped with label normal as one class and the labels mapped with Pneumonia as infected"},{"metadata":{"id":"kujDSCOv2XX7","outputId":"acfed693-1bff-4a52-eb7f-95c027a78961","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"../input/coronahack-chest-xraydataset/Chest_xray_Corona_Metadata.csv\")\ndata","execution_count":null,"outputs":[]},{"metadata":{"id":"rGvkb9t02aDZ","outputId":"d50b7177-21df-4ab5-97ee-1b16e1eee2ce","trusted":true},"cell_type":"code","source":"# Printing unique labels\nunique_labels = []\nfor i in  data['Label']:\n    if i not in unique_labels:\n        unique_labels.append(i)\n        \nprint(unique_labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"jHkYWz0F5W61","outputId":"3fa0f98c-78aa-44a1-83ef-de5fd470cf3a","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport time\nimport shutil\n\nPATH_TRAIN = \"../input/coronahack-chest-xraydataset/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/train\"\nTOTAL_IMGS = len(os.listdir(PATH_TRAIN))\nnormal = 0\ninfected = 0\n\n\n\nimg = data[\"X_ray_image_name\"]\nlabel = data[\"Label\"]\nimage_type = data[\"Dataset_type\"]\nall_dir = os.listdir(PATH_TRAIN)\n\nos.mkdir(\"train\")\nos.mkdir('train/INFECTED')\nos.mkdir(\"train/NORMAL\")\n\nwrong_info = 0  # Checking if the provided list maps the images correctly\n\n# Moving the train images to designated folders\n\nfor i in range(len(image_type)):\n    if image_type[i] == \"TRAIN\":\n        if img[i] in all_dir: # Make sure that all images in Chest_xray_Corona_Metadata.csv is mapped\n            if label[i] == \"Normal\":\n                infected = infected + 1\n                shutil.copy(\n                    PATH_TRAIN + \"/\" + img[i], \"train/NORMAL/\" + img[i]\n                )\n                normal = normal + 1\n\n            else:\n\n                shutil.copy(\n                    PATH_TRAIN + \"/\" + img[i], \"train/INFECTED/\" + img[i]\n                )\n                infected = infected + 1\n\n        else:\n            wrong_info = wrong_info + 1\n\nprint(\n    \"X-ray of Normal patients (TRAIN DATASET): \" + str(normal),\n    \"X-ray of Infected patients (TRAIN DATASET): \" + str(infected),\n    end = \"\\n\"\n)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"CZX-CuBb632R"},"cell_type":"markdown","source":"#### **Visualizing train dataset** "},{"metadata":{"id":"CY3s7QxU5cMC","outputId":"ca91bd3b-c96a-4ceb-96f8-3863e8a36e70","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\ncategories = [\"NORMAL\", \"INFECTED\"]\nnumber_of_imgs = [normal, infected]\nax.bar(0, number_of_imgs[0], color=\"g\", width=0.1)\nax.bar(0.15, number_of_imgs[1], color=\"r\", width=0.1)\nax.legend(labels=categories)\nax.set_ylabel(\"Number of images\")\nax.set_xlabel(\"Categories\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"sLzNkyd77VDi"},"cell_type":"markdown","source":"#### **Moving test dataset to designated directory** "},{"metadata":{"id":"mI5Gu23P5eD8","outputId":"2df84b8c-18ea-483e-9c4f-eeb8a86f681b","trusted":true},"cell_type":"code","source":"PATH_TEST = \"../input/coronahack-chest-xraydataset/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/test\"\nTOTAL_IMGS = len(os.listdir(PATH_TEST))\nnormal = 0\ninfected = 0\n\nimg = data[\"X_ray_image_name\"]\nlabel = data[\"Label\"]\nimage_type = data[\"Dataset_type\"]\nall_dir = os.listdir(PATH_TEST)\n\nos.mkdir(\"test\")\nos.mkdir('test/INFECTED')\nos.mkdir(\"test/NORMAL\")\n\n\n\nwrong_info = 0\n\nfor i in range(len(image_type)):\n    if image_type[i] == \"TEST\":\n        if img[i] in all_dir:\n            if label[i] == \"Normal\":\n                infected = infected + 1\n                shutil.copy(\n                    PATH_TEST + \"/\" + img[i], \"test/NORMAL/\" + img[i]\n                )\n                normal = normal + 1\n\n            else:\n\n                shutil.copy(\n                    PATH_TEST + \"/\" + img[i], \"test/INFECTED/\" + img[i]\n                )\n                infected = infected + 1\n\n        else:\n            wrong_info = wrong_info + 1\n\nprint(\n    \"X-ray of Normal patients (TEST DATASET): \" + str(normal),\n    \"X-ray of Infected patients (TEST DATASET): \" + str(infected),\n    end = \"\\n\"\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Rb1BWcCP7b0b"},"cell_type":"markdown","source":"#### **Visualizing test dataset** "},{"metadata":{"id":"8_tTMVMQ5fXB","outputId":"2e79d373-3fae-4d30-9e4b-ba8e21b694ee","trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\ncategories = [\"NORMAL\", \"INFECTED\"]\nnumber_of_imgs = [normal, infected]\nax.bar(0, number_of_imgs[0], color=\"g\", width=0.1)\nax.bar(0.15, number_of_imgs[1], color=\"r\", width=0.1)\nax.legend(labels=categories)\nax.set_ylabel(\"Number of images\")\nax.set_xlabel(\"Categories\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"jEy3twDT7p1Y"},"cell_type":"markdown","source":"#### **Exploring train dataset** "},{"metadata":{"id":"SqJYlEhzllaO","outputId":"0ed19ff8-dc77-494a-c994-08f779369390","trusted":true},"cell_type":"code","source":"import os\n\ndef list_files(startpath):\n    for root, dirs, files in os.walk(startpath):\n        level = root.replace(startpath, '').count(os.sep)\n        indent = ' ' * 4 * (level)\n        print('{}{}/'.format(indent, os.path.basename(root)))\n        subindent = ' ' * 4 * (level + 1)\n        n = 0\n        for f in files:\n            n = n+1\n            if n>5:\n                print('{}{}'.format(subindent, f),end = \"  ...... \\n\")\n                break\n            print('{}{}'.format(subindent, f))\n\n\nlist_files(\"/content/Coronahack-Chest-XRay-Dataset\")","execution_count":null,"outputs":[]},{"metadata":{"id":"m0oNqpdI2s2H"},"cell_type":"markdown","source":"## **Exploring train dataset** \n\nWhenever you see an area of increased density within the lung, it must be the result of one of these four patterns.\n\n1.   **Consolidation ** - any pathologic process that fills the alveoli with fluid, pus, blood, cells (including tumor cells) or other substances resulting in lobar, diffuse or multifocal ill-defined opacities.\n\n2.   **Interstitial ** - involvement of the supporting tissue of the lung parenchyma resulting in fine or coarse reticular opacities or small nodules.\n\n3.   **Nodule or mass** - any space occupying lesion either solitary or multiple.\n\n4.   **Atelectasis** - collapse of a part of the lung due to a decrease in the amount of air in the alveoli resulting in volume loss and increased density.\n\n<img src=\"https://www.mayoclinic.org/-/media/kcms/gbs/patient-consumer/images/2013/08/26/10/01/ds00135_im00621_pnuesmal_gif.png\" width=\"450px\">\n\n"},{"metadata":{"id":"6PKJoktM5gSl","outputId":"1b4bc527-31f1-4b1b-d135-7b9d5f790df3","trusted":true},"cell_type":"code","source":"import cv2\n\nnormal_sample = cv2.imread(\"train/NORMAL/\"+os.listdir(\"train/NORMAL\")[3])\ninfected_sample = cv2.imread(\"train/INFECTED/\"+os.listdir(\"train/INFECTED\")[2])\n\nplt.imshow(normal_sample)\nplt.title(\"NORMAL\")\nplt.show()\n\nplt.imshow(infected_sample)\nplt.title(\"INFECTED\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"EPyyju3b26XY"},"cell_type":"markdown","source":"The difference can be seen with different visualizations below"},{"metadata":{"id":"8JaVt137-DTo","outputId":"1f4eb681-662a-49ff-e53e-5581a322aad7","trusted":true},"cell_type":"code","source":"from sklearn import cluster\nimport numpy as np\n\nkmeans = cluster.KMeans(5)\ndims = np.shape(infected_sample)\npixel_matrix = np.reshape(infected_sample, (dims[0] * dims[1], dims[2]))\nclustered = kmeans.fit_predict(pixel_matrix)\n\n\nclustered_img = np.reshape(clustered, (dims[0], dims[1]))\nplt.imshow(clustered_img)\nplt.title(\"INFECTED\")\nplt.show()\n\nkmeans = cluster.KMeans(5)\ndims = np.shape(normal_sample)\npixel_matrix = np.reshape(normal_sample, (dims[0] * dims[1], dims[2]))\nclustered = kmeans.fit_predict(pixel_matrix)\n\n\nclustered_img = np.reshape(clustered, (dims[0], dims[1]))\nplt.imshow(clustered_img)\nplt.title(\"NORMAL\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"P76BSNmx2ASA","outputId":"ff0f4af5-bdf1-4ba4-def1-15f56cb0755c","trusted":true},"cell_type":"code","source":"from skimage import io\nimport matplotlib.pyplot as plt\n\nplt.imshow(normal_sample)\nplt.title(\"NORMAL\")\nplt.show()\n\nimage = io.imread(\"train/NORMAL/\"+os.listdir(\"train/NORMAL\")[3])\nax = plt.hist(image.ravel(), bins = 256)\nplt.xlabel('Intensity Value')\nplt.ylabel('Frequency')\nplt.show()\n\nplt.imshow(infected_sample)\nplt.title(\"INFECTED\")\nplt.show()\n\nimage = io.imread(\"train/INFECTED/\"+os.listdir(\"train/INFECTED\")[2])\nax = plt.hist(image.ravel(), bins = 256)\nplt.xlabel('Intensity Value')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"NSL-YrE3cHC0"},"cell_type":"markdown","source":"#### **Initializing the data loader** "},{"metadata":{"id":"ocMttMCk2_G6"},"cell_type":"markdown","source":"vgg16 pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.<br />\nThe images have to be loaded in to a range of [0, 1] and then normalized using:\n #### mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] <br />\n \n We are applying augmentation like random rotation , horizontal and vertical flips to make sre the model doesn't overfit or learn the wrong features\n "},{"metadata":{"id":"5Ro55AsGYF8v","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport time\nimport torchvision\nfrom PIL import ImageFile\nimport matplotlib.pyplot as plt\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"id":"LeIhO8ixcfcx","trusted":true},"cell_type":"code","source":"ImageFile.LOAD_TRUNCATED_IMAGES = True # To prevent error during loading broken images\n\nPATH_TRAIN = \"train\"\nPATH_TEST  = \"test\"\nEPOCHS = 10\nBATCH_SIZE = 32\nTOTAL_SIZE = len(os.listdir(PATH_TRAIN + \"/NORMAL\")) + len(\n    os.listdir(PATH_TRAIN + \"/INFECTED\")\n)\nTOTAL_TEST_SIZE = len(os.listdir(PATH_TEST + \"/NORMAL\")) + len(\n    os.listdir(PATH_TEST + \"/INFECTED\")\n)\nSTEPS_PER_EPOCH = TOTAL_SIZE // BATCH_SIZE\nSTEPS_PER_TEST_EPOCH = TOTAL_TEST_SIZE // BATCH_SIZE\nIMAGE_H, IMAGE_W = 224, 224","execution_count":null,"outputs":[]},{"metadata":{"id":"6DNcfdFSckFP","trusted":true},"cell_type":"code","source":"transform = torchvision.transforms.Compose(\n    [  # Applying Augmentation\n        torchvision.transforms.Resize((IMAGE_H, IMAGE_W)),\n        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n        torchvision.transforms.RandomVerticalFlip(p=0.5),\n        torchvision.transforms.RandomRotation(30),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        ),\n    ]\n)  # Normalizing data\n\n# Intitalizing the train data loader and applying the transformations\n\ntrain_dataset = torchvision.datasets.ImageFolder(root=PATH_TRAIN, transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, num_workers=1, shuffle=True\n)\n\n# Intitalizing the test data loader\n\ntest_dataset = torchvision.datasets.ImageFolder(\n    root=PATH_TEST, transform=transform\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=BATCH_SIZE, num_workers=1, shuffle=True\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"adXawH3Mc7dW"},"cell_type":"markdown","source":"## **Initializing pretrained resnet18 model** \n\n"},{"metadata":{"id":"SQ1nOs3Q3D9P"},"cell_type":"markdown","source":"<img src=\"https://d2l.ai/_images/residual-block.svg\" width=\"500px\">\n\n\n"},{"metadata":{"id":"0T7kGelx3JOK"},"cell_type":"markdown","source":"ResNet, short for Residual Networks is a classic neural network used as a backbone for many computer vision tasks. This model was the winner of ImageNet challenge in 2015. The fundamental breakthrough with ResNet was it allowed us to train extremely deep neural networks with 150+layers successfully. Prior to ResNet training very deep neural networks was difficult due to the problem of vanishing gradients.\nHowever, increasing network depth does not work by simply stacking layers together. Deep networks are hard to train because of the notorious vanishing gradient problem — as the gradient is back-propagated to earlier layers, repeated multiplication may make the gradient extremely small. As a result, as the network goes deeper, its performance gets saturated or even starts degrading rapidly."},{"metadata":{"id":"T8BSgrdDc5_H","outputId":"4621fc45-2277-4dac-f479-4131652700d2","trusted":true},"cell_type":"code","source":"model_ft = torchvision.models.resnet18(False)  # Initializing resnet18\nmodel_ft.load_state_dict(torch.load(\"../input/pretrained-pytorch-models/resnet18-5c106cde.pth\"))\nnum_ftrs = model_ft.fc.in_features # Getting last layer's output features\nmodel_ft.fc = nn.Linear(num_ftrs, 2) # Modifying the last layer accordng to our need","execution_count":null,"outputs":[]},{"metadata":{"id":"AX_UoSSKdNWH"},"cell_type":"markdown","source":"## **Initializing optimizers and loss function** <br/>\nWe will also specify the learning rate of the optimiser, here in this case it is set at 0.0001. If our training is bouncing a lot on epochs then we need to decrease the learning rate so that we can reach global minima."},{"metadata":{"id":"fHp99BjkdaUT","trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel_ft.to(device)  # Sending model to device\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(\n    model_ft.parameters(), lr=0.0007\n)  # lr should be kept low so that the pre-trained weights don't change easily","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n\nThe authors describe Adam as combining the advantages of two other extensions of stochastic gradient descent. Specifically:\n\n1.    Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n2.    Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n"},{"metadata":{"id":"AtqxKSkrhsui"},"cell_type":"markdown","source":"#### **TESTING THE MODEL** "},{"metadata":{"id":"OuX_nf-uhsEd","trusted":true},"cell_type":"code","source":"def get_test():\n    test_loss = []\n    correct = 0\n    incorrect = 0\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    for batch_idx, (data, target) in enumerate(test_loader):\n        if batch_idx == STEPS_PER_TEST_EPOCH:\n            break\n\n        # Model is used to predict the test data so we are switching off the gradient\n\n        with torch.no_grad():\n\n            data = data.to(device)\n            target = target.long().to(device)\n            output = model_ft(data)\n            criterion = nn.CrossEntropyLoss()\n            loss = criterion(output, target)\n\n            # Note that optimizer is not used because the model shouldn't learn the test dataset\n\n            for i in range(BATCH_SIZE):\n                a = []\n                for j in output[i]:\n                    a.append(float(j.detach()))\n\n                pred = a.index(max(a))\n\n                if pred == int(target[i]):\n                    correct = correct + 1\n\n                else:\n                    incorrect = incorrect + 1\n\n        test_loss.append(float(loss.detach()))\n    print(\"CORRECT: \" + str(correct), \"INCORRECT: \" + str(incorrect),\"TEST ACCURACY: \"+str(correct/(correct+incorrect)))\n    return (\n            correct/(incorrect+correct),\n            sum(test_loss)/len(test_loss),\n    )\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vYDLm1OJ3wNW","outputId":"50419e05-d562-4815-8d45-300f292875c0","trusted":true},"cell_type":"code","source":"acc_ , loss_ = get_test()\nprint(\"ACCURACY AND LOSS BEFORE TUNING\")\nprint(\"ACCURACY : \"+str(acc_),\"LOSS : \"+str(loss_))","execution_count":null,"outputs":[]},{"metadata":{"id":"s3B-9DuF3dz3"},"cell_type":"markdown","source":"#### **TUNING THE MODEL (TRAINING)** "},{"metadata":{"id":"mmXUZqg13eQu","outputId":"591300ce-3a2f-458a-d5bf-4a99046715e3","trusted":true},"cell_type":"code","source":"avg_test_loss_history = []\navg_test_accuracy_history = []\navg_train_loss_history = []\navg_train_accuracy_history = []\n\nloss_history = []\naccuracy_history = []\n\nnew_best = 0\n\nfor i in range(EPOCHS):\n\n    start = time.time()\n    print(\n        \"-----------------------EPOCH \"\n        + str(i)\n        + \" -----------------------------------\"\n    )\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if batch_idx == STEPS_PER_EPOCH:\n            break\n        optimizer.zero_grad()  # Resetting gradients after each optimizations\n        # Sending input , target to device\n        data = data.to(device) \n        target = target.to(device)\n        output = model_ft(data)\n        loss = criterion(output, target.reshape((BATCH_SIZE,)).long())\n        loss_history.append(loss.detach())\n        # The loss variable has gradient attached to it so we are removing it so that it can be used to plot graphs\n        loss.backward()\n        optimizer.step()  # Optimizing the model\n\n        # Checking train accuracy\n\n        correct = 0\n        incorrect = 0\n        for p in range(BATCH_SIZE):\n            a = []\n            for j in output[p]:\n                a.append(float(j.detach()))\n\n            pred = a.index(max(a))\n\n            if pred == int(target[p]):\n                correct = correct + 1\n\n            else:\n\n                incorrect = incorrect + 1\n\n        print(\n            \"\\r EPOCH \"\n            + str(i)\n            + \" MINIBATCH: \"\n            + str(batch_idx)\n            + \"/\"\n            + str(STEPS_PER_EPOCH)\n            + \" LOSS: \"\n            + str(loss_history[-1]),\n            end = \"\"\n            \n        )\n        \n        accuracy_history.append(correct/(correct+incorrect))\n\n    end = time.time()\n    print(\n        \" \\n EPOCH \"\n        + str(i)\n        + \" LOSS \"\n        + str(sum(loss_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n        + \" ETA: \"\n        + str(end - start)\n        + \" \\n MAX LOSS: \"\n        + str(max(loss_history[-STEPS_PER_EPOCH:]))\n        + \" MIN LOSS: \"\n        + str(min(loss_history[-STEPS_PER_EPOCH:]))\n        + \" TRAIN ACCURACY: \"\n        + str(sum(accuracy_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n    )\n    \n    avg_train_loss_history.append(sum(loss_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n    avg_train_accuracy_history.append(sum(accuracy_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n    \n    test_acc , test_loss  = get_test()\n    \n    avg_test_accuracy_history.append(test_acc)\n    avg_train_loss_history.append(test_loss)\n    \n    if test_acc>new_best: \n        new_best = test_acc\n        torch.save(model_ft.state_dict(), \"/model.pth\") # Saving our best model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have saved the model with best accuracy as **model.pt**"},{"metadata":{"id":"eyf41IC53m8p","outputId":"3428602e-3e2a-4b18-cd03-095038c5e493","trusted":true},"cell_type":"code","source":"print(\"ACCURACY : \",new_best)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZksqPbwS4MxP"},"cell_type":"markdown","source":"#### Plotting the loss produced by model during training"},{"metadata":{"id":"LWZw9vvf4NjM","outputId":"3d95c7a3-4f60-4b13-f0c6-9058b44eb9ca","trusted":true},"cell_type":"code","source":"plt.plot(avg_train_loss_history,label = \"Train\")\nplt.plot(avg_test_loss_history , label = \"Test\")\nplt.title('LOSS PER EPOCH')\nplt.xlabel(\"EPOCHS\")\nplt.ylabel(\"LOSS\")","execution_count":null,"outputs":[]},{"metadata":{"id":"FgcZr_lv4eA9","outputId":"b8306da3-695a-4887-eb43-dec45076abeb","trusted":true},"cell_type":"code","source":"plt.plot(loss_history)\nplt.title('LOSS PER BATCH')\nplt.xlabel(\"BATCH\")\nplt.ylabel(\"LOSS\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"HKPtR_KE4Ps7"},"cell_type":"markdown","source":"#### Plotting the accuracy produced by model during training"},{"metadata":{"id":"dFDkHDCT4TAL","outputId":"7864bd8c-7dd8-47bc-ec4c-aaa4e7ce1cba","trusted":true},"cell_type":"code","source":"plt.plot(avg_train_accuracy_history , label = \"Train\")\nplt.plot(avg_test_accuracy_history , label = \"Test\")\nplt.title('ACCURACY PER EPOCH')\nplt.xlabel(\"EPOCHS\")\nplt.ylabel(\"ACCURACY\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model was able to produce about 90 to 95% accuracy in different runs. This inconsistent result is due to model's random weight and bias initialization.Code to run in Google colab can be found [here](https://github.com/FrozenWolf-Cyber/Corona-Detection/blob/master/CoronaHack_Finetuning_resnet18_pytorch.ipynb).You can also checkout tuning of pre-trained vgg16 model with pneumonia dataset [here](https://github.com/FrozenWolf-Cyber/Pneumonia-Detection)"}],"metadata":{"colab":{"name":"CoronaHack-Finetuning resnet18-pytorch","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6f5ea8315353459692dd4a267f087d3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fa233e76af8745a3a02a8ae644ddbb07","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b42bfe83cc7447649b86a471667185a9","IPY_MODEL_1bd8b7b49e504a9ca053baeb7594f06d"]}},"fa233e76af8745a3a02a8ae644ddbb07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b42bfe83cc7447649b86a471667185a9":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_46c0854266564a7ba039b6a83dbcdceb","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a49e42ef23394a38aa7304c582d822e1"}},"1bd8b7b49e504a9ca053baeb7594f06d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_85381b86e8c942e29da96d5fee71f575","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 99.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aaeb9e078fc246f5ad960e8c30c53575"}},"46c0854266564a7ba039b6a83dbcdceb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a49e42ef23394a38aa7304c582d822e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85381b86e8c942e29da96d5fee71f575":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"aaeb9e078fc246f5ad960e8c30c53575":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":4}