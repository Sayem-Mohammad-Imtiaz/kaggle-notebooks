{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">Avocado price prediction without and with time windowing</font>"},{"metadata":{},"cell_type":"markdown","source":"In this example I will compare few regression methods on avocado prices dataset. I will try to prove the advantages of accounting time series windowing in such predictions."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/\n\n# %config IPCompleter.greedy=True\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regression_results(y_true_a, y_pred_a):\n    \n    from sklearn.metrics import r2_score, \\\n        explained_variance_score, mean_absolute_error, median_absolute_error, mean_squared_log_error, mean_squared_error\n\n    all_positive = ((y_true_a >= 0).all() and (y_pred_a >= 0).all())\n    # Regression metrics\n    l_explained_variance=explained_variance_score(y_true_a, y_pred_a)\n    l_mean_absolute_error=mean_absolute_error(y_true_a, y_pred_a)\n    l_mean_squared=mean_squared_error(y_true_a, y_pred_a)\n    l_median_absolute_error=median_absolute_error(y_true_a, y_pred_a)\n    l_r2=r2_score(y_true_a, y_pred_a)\n\n\n    print('explained_variance: ', round(l_explained_variance,4))\n    print('r2: ', round(l_r2,4))\n    print('MAE: ', round(l_mean_absolute_error,4))\n    print('MSE: ', round(l_mean_squared,4))\n    print('RMSE: ', round(np.sqrt(l_mean_squared),4))\n    print('median_absolute_error: ', round(l_median_absolute_error,4))\n    if (all_positive):\n        l_mean_squared_log_error=mean_squared_log_error(y_true_a, y_pred_a)\n        print('mean_squared_log_error: ', round(l_mean_squared_log_error,4))\n        \ndef result_plot(y_test_b, y_pred_b, name):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import matplotlib as matplotlib\n    plt.figure(figsize=(40,10))\n    plt.plot(y_pred_b, 'ro')\n    plt.plot(y_test_b,' go')\n    plt.show()\n    print(f'\\n {name}:')\n    regression_results(y_test_b, y_pred_b)\n    \n    matplotlib.rc('xtick', labelsize=15)\n    matplotlib.rc('ytick', labelsize=15)\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    plt.style.use('ggplot')\n    plt.plot(y_pred_b, y_test_b, 'ro')\n    plt.xlabel('Predictions', fontsize = 15)\n    plt.ylabel('Reality', fontsize = 15)\n    plt.title('Predictions x Reality on dataset', fontsize = 15)\n    ax.plot([y_pred_b.min(), y_pred_b.max()], [y_test_b.min(), y_test_b.max()], 'k--')\n    plt.show()\n    \ndef train_and_evaluate_keras_model(model, X_train_c, X_test_c, y_train_c, y_test_c, BATCH_SIZE, EPOCHS):\n    from keras import metrics\n    import matplotlib as matplotlib\n    import matplotlib.pyplot as plt \n    \n    print(\"\\n\")\n    print(\"Stats for Keras model:\")\n    history = model.fit(X_train_c, y_train_c, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n\n    # plot metrics\n    plt.plot(history.history['mse'])\n    plt.plot(history.history['mae'])\n\n    r2, mse, mae = model.evaluate(X_train_c, y_train_c, verbose=1)\n\n    print(f'Training stats:')\n    print(f'mean_squared_error: {mse}')\n    print(f'mean_absolute_error: {mae}')\n\n    r2, mse, mae = model.evaluate(X_test_c, y_test_c, verbose=1)\n    print(f'Test stats:')\n    print(f'mean_squared_error: {mse}')\n    print(f'mean_absolute_error: {mae}')\n    return model\n    \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we import csv data of avocado prices using pandas data frame"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\navocado = pd.read_csv(\"../input/avocado-prices/avocado.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to check if any unknown valeus are present in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"avocado.head(100)\navocado.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Formatting for pandas and statistic description"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('float_format', '{:f}'.format)\navocado.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Unnamed: 0** - not useful, we do not know what it is  **Date** - first we are going to see how regression performs without time seres (we only leave year as a reference)"},{"metadata":{"trusted":true},"cell_type":"code","source":"avocado = avocado.drop(['Unnamed: 0', 'Date'], axis = 1)\navocado.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avocado.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlate coefficients, non numeric columns are ignored by pandas corr method"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots()\ncorr = avocado.corr()\nprint(corr)\nsns.heatmap(corr, cmap='coolwarm', annot = True, fmt='.1g')\nax.set_title(\"Correlation Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion - most bags sold are small bags, then large and x-large. They are correlated with total volume in this order. Labels 4046 and 4225 are more correlated with volume then 4770. 4046 are usually in smaller bags and not common in x-large ones."},{"metadata":{},"cell_type":"markdown","source":"We can split dataset to features and labels. **AvaregePrice** is what we want to predict with regression. Data needs to be normalized, object type features encoded. We will use LabelEncoder for that"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_1 = avocado.drop(['AveragePrice'], axis = 1).values\ny_1 = avocado['AveragePrice'].values\n\n# print(X[:, 10]) for all rows 10th column\nprint('region', X_1[:, 10] )\n#print(X_1[:, 9] )\n#print(X_1[:, 8] )\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_X_1 = LabelEncoder()\nX_1[:, 8] = labelencoder_X_1.fit_transform(X_1[:, 8])\nlabelencoder_X_2 = LabelEncoder()\nX_1[:, 9] = labelencoder_X_2.fit_transform(X_1[:, 9])\nlabelencoder_X_3 = LabelEncoder()\nX_1[:, 10] = labelencoder_X_3.fit_transform(X_1[:, 10])\nprint('region', X_1[:, 10] )\n#print(X_1[:, 9] )\n#print(X_1[:, 8] )\n#print(X_1[10:,:])\nprint(y_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data needs to be scaled in order to perform well in our ml process. StandardScaler will do all the work for us"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_1)\nscaled_features = scaler.transform(X_1)\ndf_feat = pd.DataFrame(scaled_features)\n\ndf_feat.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data needs to be split to train and test. Models will be trained on training set and scored on testing one"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_feat, y_1, test_size = 0.2, random_state = 42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nimport matplotlib as matplotlib\n\nlinReg = LinearRegression()\nlinReg.fit(X_train, y_train)\npred_linreg = linReg.predict(X_test)\n\nprint(pred_linreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_plot(y_test, pred_linreg, 'LinearRegression')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=pred_linreg, y=y_test, color= 'g')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_train.shape: ', X_train.shape)\nprint('len(np.unique(y_train)): ', len(np.unique(y_train)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3,2&seed=0.40488&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adadelta, Adam\n\nprint(X_train[1])\nprint(y_train)\n\nobservation_shape = X_train.shape[1],\nprint(f'observation_shape: ${observation_shape}')\n\nBATCH_SIZE = 10\nEPOCHS = 1000\nVALIDATION_SPLIT = 0.1\n\n# Model\nkeras_model = Sequential()\nkeras_model.add(Dense(200, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\nkeras_model.add(Dense(200, kernel_initializer='normal', activation='relu'))\nkeras_model.add(Dense(200, kernel_initializer='normal', activation='relu'))\nkeras_model.add(Dense(100, kernel_initializer='normal', activation='relu'))\nkeras_model.add(Dense(100, kernel_initializer='normal', activation='relu'))\nkeras_model.add(Dense(50, kernel_initializer='normal', activation='relu'))\nkeras_model.add(Dense(25, kernel_initializer='normal', activation='relu'))\nkeras_model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n\n# Compile model\n# optimizer_adam: EPOCHS = 200 BATCH_SIZE = 10\n# explained_variance:  0.7678\n# r2:  0.7661\n# MAE:  0.1359\n# MSE:  0.0376\n# RMSE:  0.1939\n# median_absolute_error:  0.0938\n# mean_squared_log_error:  0.0056\noptimizer_adam=Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.995, amsgrad=False)\n\n# optimizer_adadelta: EPOCHS = 200 BATCH_SIZE = 10\n# explained_variance:  0.7856\n# r2:  0.7855\n# MAE:  0.1265\n# MSE:  0.0345\n# RMSE:  0.1856\n# median_absolute_error:  0.0833\n# mean_squared_log_error:  0.0052\noptimizer_adadelta=Adadelta(learning_rate=1.0, rho=0.90)\nkeras_model.compile(loss='mean_squared_error', optimizer=optimizer_adadelta, metrics=['mse', 'mae'])\n\n#Train\nkeras_model = train_and_evaluate_keras_model(keras_model, X_train, X_test, y_train, y_test, BATCH_SIZE, EPOCHS)\npred_keras = keras_model.predict(X_test)\n\n#print(prediction)\n# model.evaluate(X_train, y_train)\n\n#print(pred_keras[:100])\n#print(y_test[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_plot(y_test, pred_keras, 'Keras Model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nimport matplotlib as matplotlib\nmodel_lasso = Lasso(alpha=0.0001, fit_intercept=True, tol=0.000001,\n          max_iter=100000000, positive=True, warm_start=True)\nmodel_lasso.fit(X_train, y_train)\npred_lasso = model_lasso.predict(X_test)\n\nresult_plot(y_test, pred_lasso, 'Lasso')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nimport matplotlib as matplotlib\nmodel_ridge = Ridge(alpha=0.0001,normalize=True, max_iter=100000000, solver=\"auto\")\nmodel_ridge.fit(X_train, y_train)\npred_ridge = model_ridge.predict(X_test)\n\nresult_plot(y_test, pred_ridge, 'Ridge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://laurenscoster.com/blog/xgboost/\n# https://xgboost.readthedocs.io/en/latest/parameter.html\nfrom xgboost import XGBRegressor\nmodel_xgb = XGBRegressor(objective ='reg:squarederror', colsample_bytree=0.3, learning_rate=0.5, max_depth=5, alpha=0.01, n_estimators=10, verbosity=2)\nmodel_xgb.fit(X_train, y_train)\npred_xgb = model_xgb.predict(X_test)\n\nresult_plot(y_test, pred_xgb, 'XGBRegressor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will check if taking into account time series (windowing) will result in better performance of our models.\n[Read about time series prediction](https://machinelearningmastery.com/time-series-prediction-with-deep-learning-in-python-with-keras/)\nThis time we will take into account date. Looking at data price is recorded every week. We will split data per region and week, sort it and create windows for time series.\n> For example, given the current time (t) we want to predict the value at the next time in the sequence (t + 1), we can use the current time (t) as well as the two prior times (t-1 and t-2).\n\n> When phrased as a regression problem the input variables are t-2, t-1, t and the output variable is t+1"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\nimport sys\nimport csv\nimport os\n\nimport pandas as pd\nimport xgboost as xgb\nimport numpy as np\nfrom numpy import loadtxt\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport pickle\n\ndef read_file(file_name):\n    return pd.read_csv(file_name, sep=',')\n\ndef clean_and_fill_data(data):\n    # delete noname columns\n    data = data.drop(columns='Unnamed: 0')\n    # add week columns\n    data['weeks'] = pd.to_datetime(data.Date).dt.week\n    # data.Date is redundant - delete it\n    data = data.drop(columns='Date')\n    # Encoding categorical data\n    labelEncoder = LabelEncoder()\n    data.region = labelEncoder.fit_transform(data.region)\n    data.type = labelEncoder.fit_transform(data.type)\n    data.region = labelEncoder.fit_transform(data.region)\n\n    # sortuje i zamieniam kolejnoścą kolumny\n    new_order_column =  ['year','weeks', 'Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags', 'Large Bags'\n        , 'XLarge Bags', 'type', 'region', 'AveragePrice']\n    data = data[new_order_column]\n    #Sortowanie ma później nam pomoć jak bedziemy robić okna czasowe.\n    data = data.sort_values(['region','type','year','weeks'], ascending=True)\n    return data\n\ndef split_data_by_region_type(data):\n    regions = data.region.unique()\n    types = data.type.unique()\n    list_data = []\n\n    for r, region in enumerate(regions):\n        current_data = data[data.region == region]\n        for t, type_l in enumerate(types):\n            list_data.append(current_data[current_data.type == type_l])\n    return list_data\n\ndef prepare_data_for_model(data_splited, window_size):\n    model = []\n    for data in data_splited:\n        np_array = data.to_numpy()\n        length = np_array.shape[0]\n        \n        for index, row in enumerate(np_array):\n            if (index + window_size <= length):\n                model_row = []\n                for i_range in (range(window_size)):\n                    current_row = np_array[index + i_range]\n                    model_row = [*model_row, *current_row] #nie tworzenie nowych obiektow list przy konkatenacji\n                model.append(model_row) #model.append(extra_features, model_row)\n            else:\n                break\n    return model\n\ndef save_data(model, file_name):\n    print(\"save\")\n    with open(file_name, 'w', newline='') as csvFile:\n        writer = csv.writer(csvFile)\n        for row in model:\n            writer.writerow(row)\n    csvFile.close()\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\ndata = read_file(\"../input/avocado-prices/avocado.csv\")\ndata = clean_and_fill_data(data)\n\n# save data\nfile_name = \"01_transform.csv\"\ndata.to_csv(\"01_transform.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for_window = 5\ndata_splited = split_data_by_region_type(data)\nfor_model = prepare_data_for_model(data_splited, for_window)\n\nsave_data(for_model,\"02_data_for_model.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = \"02_data_for_model.csv\" \ndataset = pd.read_csv(file_name, header=None);\ndataset = dataset.values #bez headera, tylko wartości \nX_time = dataset[:,0:-1] #: - wszytskie wiersze, 0 - od zera bez ostatniej\nY_time = dataset[:,-1:] # ostatnia kolumna\n\ntest_size_time = 0.2\nX_train_time, X_test_time, y_train_time, y_test_time = train_test_split(X_time, Y_time, test_size=test_size_time)\ny_train_time = y_train_time # ewentualnie .flatten() żeby ubewnić się, że mamy płaskie dane [[1],[2]] = [1,2]\ny_test_time = y_test_time\n\nmodel_xgb_window = XGBRegressor(objective ='reg:squarederror', colsample_bytree=0.3, learning_rate=0.5, max_depth=5, alpha=0.01, n_estimators=10, verbosity=2)\nmodel_xgb_window.fit(X_train_time, y_train_time)\ny_pred_xgb_widnowing = model_xgb_window.predict(X_test_time)\n\nresult_plot(y_test_time, y_pred_xgb_widnowing, 'XGBRegressor with windowing (trend)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nimport matplotlib as matplotlib\nmodel_lasso = Lasso(alpha=0.01, fit_intercept=True, tol=0.00000001,\n          max_iter=1000000, positive=True, warm_start=True)\nmodel_lasso.fit(X_train_time, y_train_time)\npred_lasso_time = model_lasso.predict(X_test_time)\n\nresult_plot(y_test_time, pred_lasso_time, 'Lasso')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nimport matplotlib as matplotlib\n\nlinReg = LinearRegression()\nlinReg.fit(X_train_time, y_train_time)\npred_linreg_time = linReg.predict(X_test_time)\n\nresult_plot(y_test_time, pred_linreg_time, 'LinearRegression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: Windowing time series enabled us to have very good LinearRegression prediction scores on model with or without l1,l2 regularization and better scores with other regressions. Decision tree regressor (XGB) and keras neural network regression before adding time windowing achieved results comperable to ones after time windowing."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}