{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Shhhh\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/superbowlads/superbowl-ads.csv',error_bad_lines=False)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Product Type'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Product Type'] = [1 if i == 'Film' else 0 for i in df['Product Type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,5))\nsns.countplot(x='Product Type',data=df,order= df['Product Type'].value_counts().iloc[:10].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Company'] = df['Product/Title'].apply(lambda x: x.split(\"\\\"\")[0])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Company'].value_counts(normalize = True).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Plot/Notes'] = df['Plot/Notes'].fillna('missing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()\ndf['tokenized_notes'] = [nltk.word_tokenize(doc) for doc in df[\"Plot/Notes\"]]\n\ndf['stemmed_notes'] = [[ps.stem(word) for word in row] for row in df['tokenized_notes']]\ndf['lower_notes'] = [[word.lower() for word in row] for row in df['stemmed_notes']]\ndf['newtext_notes'] = [\" \".join(row) for row in df['lower_notes']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvec = CountVectorizer(stop_words='english', ngram_range= (1,2))\n##fit cvec with title\n\ncvec.fit(df['newtext_notes'])\nlen_features = len(cvec.get_feature_names())\nprint(len_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sports_df_cv = pd.DataFrame(cvec.transform(df['newtext_notes']).todense(),columns=cvec.get_feature_names())\nhighest_sports_cv = sports_df_cv.sum(axis=0)\ndf_cvec = highest_sports_cv.sort_values(ascending = False).head(20)\ndf_cvec = pd.DataFrame(df_cvec, columns = ['Count_Vectorizer(units)'])\nsports_top = highest_sports_cv.to_frame(name='Count_Vectorizer(units)')\nsports_top['Word'] = sports_top.index\nsports_top.reset_index(drop=True, inplace=True)\ncols = ['Word','Count_Vectorizer(units)']\nsports_top = sports_top[cols]\nsports_top.sort_values(by='Count_Vectorizer(units)',ascending=False, inplace=True)\nsports_top.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##use seaborn package\nplt.figure(figsize=(20,10))\nplt.title('Count Vectorizer: Sports Reddit Top 20 Words',fontsize=25)\nsns.set_style(\"darkgrid\")\nsns.barplot(data=sports_top.head(10),x='Count_Vectorizer(units)',y='Word',orient='h')\nplt.xlabel('Count Vectorizer Frequency (Units)',fontsize=20)\nplt.ylabel('Word(Text)',fontsize=20)\nplt.tick_params(labelsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[\"newtext_notes\"]\ny = df[\"Product Type\"]\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tvec = TfidfVectorizer(ngram_range=(1,2), stop_words = \"english\")\nX_train_counts = tvec.fit_transform(X_train)\nX_test_counts = tvec.transform(X_test)\nprint(\"Number of features:\",X_train_counts.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Product Type'].value_counts(normalize = True).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(random_state = 42 )\nlog_reg.fit(X_train_counts, y_train)\nprint(\"Train data CV score:\", cross_val_score(log_reg, X_train_counts, y_train, cv= 5))\nprint(\"Test data score:\", log_reg.score(X_test_counts, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline(steps = [('vectorizer',TfidfVectorizer(lowercase = False)),     # first tuple is for first step: vectorizer\n                         ('model', LogisticRegression(solver = 'liblinear'))\n                         \n                        ])    \n\n# Construct Grid Parameters\nhyperparams = {'vectorizer__ngram_range': [(1,1), (1,2), (2,2)],\n               'vectorizer__stop_words': ['english'],\n                                                        # use a single value that isn't built into\n                                                        # the defaults (otw: stopwords left in)\n               'model__penalty': ['l1', 'l2'],\n               'model__C': [3, 10, 1000]\n                \n              }\n\n # Perform Grid Search\ngs = GridSearchCV(pipe, # pipeline object replaces what we usually had as empty model class\n                 param_grid=hyperparams,\n                 cv = 3,\n                 scoring = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_log = gs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_score_log = results_log.best_score_\nprint('Best TRAIN accuracy: {:.4f}'.format(train_score_log))\ntest_score_log = results_log.score(X_test, y_test)\nprint('Best TEST set accuracy: {:.4f}'.format(test_score_log))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pretty_confusion_matrix(y_true, y_pred):\n    # handling data\n    cm = confusion_matrix(y_true, y_pred)\n    labels = ['Other(0)', 'Film(1)']\n    sns.set(font_scale=3)\n    plt.figure(figsize=(14,8))\n    sns.heatmap(cm, annot=True, fmt='g', cmap=\"YlGnBu\",xticklabels=labels, yticklabels=labels)\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel('Predicted Class')\n    plt.ylabel('True Class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretty_confusion_matrix(y_test, results_log.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":1}