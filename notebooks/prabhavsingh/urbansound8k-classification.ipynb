{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro"},{"metadata":{},"cell_type":"markdown","source":"This dataset contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. For a detailed description of the dataset and how it was compiled please refer to our paper.\nAll excerpts are taken from field recordings uploaded to www.freesound.org. The files are pre-sorted into ten folds (folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results reported in the article above.\n\nIn addition to the sound excerpts, a CSV file containing metadata about each excerpt is also provided."},{"metadata":{},"cell_type":"markdown","source":"### Methodology"},{"metadata":{},"cell_type":"markdown","source":"1. There are 3 basic methods to extract features from audio file :\n    a) Using the mffcs data of the audio files\n    b) Using a spectogram image of the audio and then converting the same to data points (As is done for images). This is easily done using mel_spectogram function of Librosa\n    c) Combining both features to build a better model. (Requires a lot of time to read and extract data).\n2. I have chosen to use the second method.\n3. The labels have been converted to categorical data for classification.\n4. CNN has been used as the primary layer to classify data"},{"metadata":{},"cell_type":"markdown","source":"# Importing Necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic Libraries\n\nimport pandas as pd\nimport numpy as np\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for Classification and building Models\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\nfrom tensorflow.keras.utils import to_categorical \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Project Specific Libraries\n\nimport os\nimport librosa\nimport librosa.display\nimport glob \nimport skimage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing Data Type and Format"},{"metadata":{},"cell_type":"markdown","source":"#### Analysing CSV Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/urbansound8k/UrbanSound8K.csv\")\n\n'''We will extract classes from this metadata.'''\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Column Names\n\n* slice_file_name: \nThe name of the audio file. The name takes the following format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav, where:\n[fsID] = the Freesound ID of the recording from which this excerpt (slice) is taken\n[classID] = a numeric identifier of the sound class (see description of classID below for further details)\n[occurrenceID] = a numeric identifier to distinguish different occurrences of the sound within the original recording\n[sliceID] = a numeric identifier to distinguish different slices taken from the same occurrence\n\n* fsID:\nThe Freesound ID of the recording from which this excerpt (slice) is taken\n\n* start\nThe start time of the slice in the original Freesound recording\n\n* end:\nThe end time of slice in the original Freesound recording\n\n* salience:\nA (subjective) salience rating of the sound. 1 = foreground, 2 = background.\n\n* fold:\nThe fold number (1-10) to which this file has been allocated.\n\n* classID:\nA numeric identifier of the sound class:\n0 = air_conditioner\n1 = car_horn\n2 = children_playing\n3 = dog_bark\n4 = drilling\n5 = engine_idling\n6 = gun_shot\n7 = jackhammer\n8 = siren\n9 = street_music\n\n* class:\nThe class name: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, \nsiren, street_music."},{"metadata":{},"cell_type":"markdown","source":"#### Using Librosa to analyse random sound sample - SPECTOGRAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"dat1, sampling_rate1 = librosa.load('../input/urbansound8k/fold5/100032-3-0-0.wav')\ndat2, sampling_rate2 = librosa.load('../input/urbansound8k/fold5/100263-2-0-117.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nD = librosa.amplitude_to_db(np.abs(librosa.stft(dat1)), ref=np.max)\nplt.subplot(4, 2, 1)\nlibrosa.display.specshow(D, y_axis='linear')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Linear-frequency power spectrogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nD = librosa.amplitude_to_db(np.abs(librosa.stft(dat2)), ref=np.max)\nplt.subplot(4, 2, 1)\nlibrosa.display.specshow(D, y_axis='linear')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Linear-frequency power spectrogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Using random samples to observe difference in waveforms.'''\n\narr = np.array(df[\"slice_file_name\"])\nfold = np.array(df[\"fold\"])\ncla = np.array(df[\"class\"])\n\nfor i in range(192, 197, 2):\n    path = '../input/urbansound8k/fold' + str(fold[i]) + '/' + arr[i]\n    data, sampling_rate = librosa.load(path)\n    plt.figure(figsize=(10, 5))\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(data)), ref=np.max)\n    plt.subplot(4, 2, 1)\n    librosa.display.specshow(D, y_axis='linear')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(cla[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction and Database Building"},{"metadata":{},"cell_type":"markdown","source":"#### Method\n\n1. I have used Librosa to extract features.\n2. To do so, I will go through each fold and extract the data for each file. Then I have used the mel_spectogram function of librosa to extract the spectogram data as a numpy array.\n3. After reshaping and cleaning the data, 75-25 split has been performed.\n4. Classes (Y) have been converted to Categorically Encoded Data usng Keras.utils\n\nNote : Running the parser function may take upto 45 minutes depending on your system since it has to extract spectogram data for 8732 audio files"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''EXAMPLE'''\n\ndat1, sampling_rate1 = librosa.load('../input/urbansound8k/fold5/100032-3-0-0.wav')\narr = librosa.feature.melspectrogram(y=dat1, sr=sampling_rate1)\narr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = []\nlabel = []\n\ndef parser(row):\n    # Function to load files and extract features\n    for i in range(8732):\n        file_name = '../input/urbansound8k/fold' + str(df[\"fold\"][i]) + '/' + df[\"slice_file_name\"][i]\n        # Here kaiser_fast is a technique used for faster extraction\n        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n        # We extract mfcc feature from data\n        mels = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)        \n        feature.append(mels)\n        label.append(df[\"classID\"][i])\n    return [feature, label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = parser(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = np.array(temp)\ndata = temp.transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = data[:, 0]\nY = data[:, 1]\nprint(X_.shape, Y.shape)\nX = np.empty([8732, 128])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(8732):\n    X[i] = (X_[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = to_categorical(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Final Data'''\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(6549, 16, 8, 1)\nX_test = X_test.reshape(2183, 16, 8, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dim = (16, 8, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Keras Model and Testing"},{"metadata":{},"cell_type":"markdown","source":"#### Model 1:\n\n1. CNN 2D with 64 units and tanh activation.\n2. MaxPool2D with 2*2 window.\n3. CNN 2D with 128 units and tanh activation.\n4. MaxPool2D with 2*2 window.\n5. Dropout Layer with 0.2 drop probability.\n6. DL with 1024 units and tanh activation.\n4. DL 10 units with softmax activation.\n5. Adam optimizer with categorical_crossentropy loss function.\n\n90 epochs have been used."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Conv2D(64, (3, 3), padding = \"same\", activation = \"tanh\", input_shape = input_dim))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, (3, 3), padding = \"same\", activation = \"tanh\"))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"tanh\"))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs = 90, batch_size = 50, validation_data = (X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)\nscore = model.evaluate(X_test, Y_test)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.argmax(predictions, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame(preds)\nresult.to_csv(\"UrbanSound8kResults.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}