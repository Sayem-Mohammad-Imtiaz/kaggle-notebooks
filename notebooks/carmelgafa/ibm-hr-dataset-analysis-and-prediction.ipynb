{"metadata":{"language_info":{"pygments_lexer":"ipython3","version":"3.6.4","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_uuid":"26923a5ad543e6e0394b3de2cf556e812dee3401","_cell_guid":"1869d639-e9f5-4e20-af97-5593816ca675"},"cell_type":"markdown","source":"# Introduction\n\nThe battle with attrition and its implications to productivity and morale is possibly the biggest headache that any manager will have to face. Responses to counteract and to decrease attrition  often require the manager to understand the causes and estimate the costs associated with staff turnover.\n\nIn his 2009 paper, Yazinski identified 7 main reasons apart from salary that contribute to employee turnover:\n1. Employees feel the job or workplace is not what they expected,\n2. There is a mismatch between the job and person,\n3. There is too little coaching and feedback,\n4. There are too few growth and advancement opportunities,\n5. Employees feel devalued and unrecognized,\n6. Employees feel stress from overwork and have a work/life imbalance, and\n7. There is a loss of trust and confidence in senior leaders\n\nThis Kaggle challenge invites us to “Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.”\n\nThis notebook is divided into two parts:\n1. The first part will consider the data provided and attempt to identify trends and patterns.\n2. The data is then split into training and testing sets and several machine learning techniques are used to  identify the individuals that are more likely to leave the organization.\n\nAn attempt is being done to structure this work in functions that can be used in other datasets. A copy of all the code and data used in this project can be found at https://github.com/carmelgafa/ibm-hr-analytics/."},{"metadata":{"_uuid":"65d9570f23b91fb3faa73586a79affa39ed2079b","_cell_guid":"0a9d166d-be87-4e72-abc2-3f7153d88e87","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"# imports required by this notebook\n\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.externals import joblib"},{"metadata":{"_uuid":"fc9fa9eca9c75afd5e2d20862e1d10e3e46666d2","_cell_guid":"b93f93ce-ae7f-4346-879f-eae49a2f9ddc"},"cell_type":"markdown","source":"# Part 1 - Analysis\n\nWe start by taking a high-level look at the data provided. The set contains a ‘Yes/No’ field indicating whether the employee has left the company but we are not given any information about the departure date. \n\nThere are 34 features that describe each employee, his/her role in the company, his/her level of satisfaction, the salary and share options available and his/her influence on the company.\n\nOut of the above, four features are redundant because they either contain no information or they contain information that is not relevant to an employee’s decision to leave the company.\n\n\n\nThe exercise began with the creation of a 'constants' class that contained important parameters for this project. The names of all the features were abstracted in this class."},{"metadata":{"_uuid":"183534eaf977fcb42122b97469fe12c5c2ce019a","_cell_guid":"1245d4f7-4536-495c-8fa8-448956f3643f","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"class constants():\n    \"\"\"\n    Constants required for this excercise\n    items postfixed by _T are categorized and will need to be encoded\n    items postfixed by _R are redundant\n    \"\"\"\n\n    ISRESIGNED_T = 'Attrition'\n\n    # employee related information\n    AGE = 'Age'\n    EDUCATION_T = 'EducationField'\n    GENDER_T = 'Gender'\n    COMPANIES = 'NumCompaniesWorked'\n    STATUS_T = 'MaritalStatus'\n    HOMEDISTANCE = 'DistanceFromHome'\n\n    # role of the employee in the company\n    ROLE_T = 'JobRole'\n    LEVEL = 'JobLevel'\n    DEPARTMENT_T = 'Department'\n    YEARSCOMPANY = 'YearsAtCompany'\n    YEARSEMPLOYED = 'TotalWorkingYears'\n    YEARSROLE = 'YearsInCurrentRole'\n    YEARSLASTPROMO = 'YearsSinceLastPromotion'\n    YEARSMANAGER = 'YearsWithCurrManager'\n\n    # satisfaction informaiton\n    SATISFACTION = 'JobSatisfaction'\n    TEAMCLICK = 'RelationshipSatisfaction'\n    LIFEBALANCE = 'WorkLifeBalance'\n    ENVIRONMENT = 'EnvironmentSatisfaction'\n\n    # salary and money related\n    SALARY = 'MonthlyIncome'\n    MONTHLYRATE = 'MonthlyRate'\n    DAILYRATE = 'DailyRate'  # Daily rate = the amount of money you are paid per day\n    HOURLYRATE = 'HourlyRate'\n\n    # Percent salary hike = the % change in salary from 2016 vs 2015.\n    LASTINCREMENTPERCENT = 'PercentSalaryHike'\n    STOCKOPTIONS = 'StockOptionLevel' # Stock option level = how much company stocks you own.\n    TRAINING = 'TrainingTimesLastYear'\n\n    # rating and involvement related\n    RATING = 'PerformanceRating'\n    INVOLVEMENT = 'JobInvolvement'\n    OVERTIME_T = 'OverTime'  # Y/N\n    TRAVEL_T = 'BusinessTravel'  # rare / requent\n\n    # redundant fields\n    EMPLYEENO_R = 'EmployeeNumber'  # number\n    EMPLOYEECOUNT_R = 'EmployeeCount'  # all 1\n    ISOVER18_R = 'Over18'  # all Y\n    STDHOURS_R = 'StandardHours'  # all 40\n    \n    \n    # files used in the model creation process\n    orig_file = '../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv'\n    processed_file = '../input/data-files-used-for-models/HR_attrition_orig_proc.csv'\n\n    train_x_file = '../input/data-files-used-for-models/HR_attrition_train_x.csv'\n    test_x_file = '../input/data-files-used-for-models/HR_attrition_test_x.csv'\n    train_y_file = '../input/data-files-used-for-models/HR_attrition_train_y.csv'\n    test_y_file = '../input/data-files-used-for-models/HR_attrition_test_y.csv'\n\n    train_x_nb_file = '../input/data-files-used-for-models/HR_attrition_train_x_nb.csv'\n    test_x_nb_file = '../input/data-files-used-for-models/HR_attrition_test_x_nb.csv'\n\n    train_x_lr_file = '../input/data-files-used-for-models/HR_attrition_train_x_lr.csv'\n    test_x_lr_file = '../input/data-files-used-for-models/HR_attrition_test_x_lr.csv'\n    train_x_nn_file = '../input/data-files-used-for-models/HR_attrition_train_x_nn.csv'\n    test_x_nn_file = '../input/data-files-used-for-models/HR_attrition_test_x_nn.csv'\n"},{"metadata":{"_uuid":"3be7bf5e673cf33a6e5a98e6eefb2b1beab777e6","_cell_guid":"8f3bc96d-237e-4778-870a-591c83a0c13e"},"cell_type":"markdown","source":"A specific method was created to load the data from the csv file that was provided. The method generates two datasets containing:\n\n1. The original data with no modifications whatsoever.\n2. A processed version, where specific columns are dropped because they are irrelevant and other columns containing categorical data are encoded into numerical values so that they can be processed by the functions presented later.\n\nEach dataset is stored in two numpy arrays, one contains the dataset header whilst the other contains the dataset data.\n\nThe method also generates some analytics information in a dictionary."},{"metadata":{"_uuid":"0c6589f9f1f0e8fd04df7076a3cc1e7d9ee4af84","_cell_guid":"f241cf40-113b-4b1c-a913-a9f677d77c0a","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def load_data(filename, encode_list, column_drop_list):\n    \"\"\"\n    Loads the data from a csv file\n    This method will produce two datasets:\n\n    The original data containing the data from the csv file\n\n    A 'massaged data set' that will contain the data after two operaitons take place:\n    1. drop of irrelevant columns as defined in the column_drop_list\n    2. encoding of labelled values to numeric counterparts as defined in encode_list\n\n    An analytics dictionary is also provided, with shape, description and correlation operaitons performed on the massaged dataset\n\n    Arguments:\n    encode_list -- list of the columns to encode to numerical values from the original dataset in the 'massaged dataset'\n    column_drop_list -- list of the columns to drop from the original dataset in the 'massaged dataset'\n\n    Returns:\n    header orig -- the header row of the file containing the columns titles\n    data_orig -- the data rows of the file\n    header_massaged -- header row of the massaged dataset\n    data_massaged -- data rows of the massaged dataset\n    data_analytics -- dictionary of initial analysis of the data:\n                            nulls -- flag indicationg theat the dataset conatins nulls\n                            shape -- shape structure\n                            description -- pandas anlysis\n                            correlation -- correlation matrix\n    \"\"\"\n\n    data_df = pd.read_table(filename, sep=',')\n\n    contains_nulls = False\n    # check that dataframe does not contain null values\n    if data_df.isnull().values.any():\n        contains_nulls = True\n\n    header_orig = np.array(data_df.columns.values).squeeze()\n    data_orig = np.array(data_df.values)\n\n    # create a clone dataframe that will be 'massaged'\n    data_df_copy = data_df.copy(deep=True)\n\n    for encode_item in encode_list:\n        data_df_copy[encode_item].replace(data_df_copy[encode_item].unique(), range(\n            0, len(data_df_copy[encode_item].unique())), inplace=True)\n\n    for drop_column_item in column_drop_list:\n        del data_df_copy[drop_column_item]\n\n    header_massaged = np.array(data_df_copy.columns.values).squeeze()\n    data_massaged = np.array(data_df_copy.values)\n\n    data_analytics = {\n        'nulls' : contains_nulls,\n        'shape': data_df.shape,\n        'description': data_df_copy.describe(),\n        'correlation': data_df_copy.corr()\n    }\n\n    return header_orig, data_orig, header_massaged, data_massaged, data_analytics\n"},{"metadata":{"_uuid":"812c8fa9bfd6011d6587434980b36535fcee8632","_cell_guid":"cc832a84-e8c6-4fdf-8ae0-89ef9a405074"},"cell_type":"markdown","source":"It is possible to catty out some initial analysis of this data after loading the data file.\n\nTwo functions were created to display the analysis information generated when the load function was executed. The first prints the textual information, whilst the second plots the correlation matrix for the data."},{"metadata":{"_uuid":"c168c2d13ca7d31198aa7e51784a1459493852bc","_cell_guid":"f1a10edc-ae0e-4928-9cf9-c4426ca58e25","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def analyse_data_init(header, analytics):\n    \"\"\"\n    Displays initial analysis on a loaded dataset. In particular this funciton;\n\n    1. indicates if null values are present\n    2. prints the shape of the dataset\n    3. prints the description information for the dataset, for each column\n    4. plots the correlation matrix\n\n    Arguments:\n\n    header -- the data header for the loaded set\n    analytics -- the analyics dictionary provided by the load_data function\n    \"\"\"\n\n    if(analytics['nulls'] == True):\n        print('Dataset contains null values')\n    else:\n        print('Dataset does not contain null values')\n\n    print('Dataset shape: {}'.format(analytics['shape']))\n\n    print(analytics['description'])\n\n    plot_correlation_matrix(header, analytics['correlation'])\n"},{"metadata":{"_uuid":"50b05b5ffcf809a5621b13ab05edd21ccea69d66","_cell_guid":"b9410d3b-34e1-4d47-9f1c-8a35583c8974","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def plot_correlation_matrix(headers, correlations):\n    \"\"\"\n    Plots the correlation matrix\n    \n    Arguments:\n    headers -- the headers of the correleation graph\n    correlations -- the correlation information\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(correlations)\n    fig.colorbar(cax)\n    ticks = np.arange(0, len(headers), 1)\n\n    ax.set_xticks(ticks)\n    ax.set_yticks(ticks)\n\n    ax.set_xticklabels(headers)\n    ax.set_yticklabels(headers)\n\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(80)\n\n    plt.show()\n"},{"metadata":{"_uuid":"7cdccc5f1fe109392b32f2c288c60f0529bfa33b","_cell_guid":"5f2ee441-4325-457a-8650-ccbd9515fd50"},"cell_type":"markdown","source":"The code snippet below executes the load function.\n\nIn order to execute the method, as explained above, two arrays are necessary. The first contains the columns that will be removed, and the second contains the columns that will be encoded from the original dataset so that the processed dataset is formed. These arrays used by the load function to create the resulting datasets."},{"metadata":{"_uuid":"3902c3f1e94da19a3592e1546dc51a62f8bc001d","_cell_guid":"c8f57982-456e-4dfa-9928-42153055cfd6"},"outputs":[],"execution_count":null,"cell_type":"code","source":"column_drop_list = [constants.EMPLYEENO_R, constants.EMPLOYEECOUNT_R,\n                    constants.ISOVER18_R, constants.STDHOURS_R]\nencode_list = [constants.GENDER_T, constants.STATUS_T, constants.DEPARTMENT_T, constants.ROLE_T,\n               constants.OVERTIME_T, constants.TRAVEL_T, constants.ISRESIGNED_T, constants.EDUCATION_T]\n\nheader, data, m_header, m_data, analytics = load_data(\n    constants.orig_file, encode_list, column_drop_list)\n\n\nanalyse_data_init(m_header, analytics)"},{"metadata":{"_uuid":"3c379975404481f1231702effa58071778d10f74","_cell_guid":"45538193-bb7e-4812-b4c1-cd87808d718e"},"cell_type":"markdown","source":"# Initial analysis\n\n The first step is expected, the data is loaded and examined. \n\nWe are presented with 1470 records of employees, each having 35 pieces of information. Fortunately, the data looks clean, with no missing information.\n\n\"How bad is attrition?\" We are not presented with temporal data, that is we do not know the date when an employee was employed and when the employee resigned, and therefore we cannot deduce whether the company has gone through a 'rough patch' and consequently there was a resignation spike. Out of our 1470 employees, 237 or 16% have resigned. This might be normal in some industries, yet it should raise an eyebrow at senior management level.\n\nThis company consists to 40% female workforce, with most employees living as close as 14 miles to the company’s location. Most employees are satisfied with the work environment and team members. \n\nA performance rating is given to each employee; however, we notice that all ratings are wither 3 or 4, thus making the efficacy of this measure questionable. In most cases, employees were promoted in the last 4 years.\n\nMost of the features given in this dataset are poorly correlated to each other, with the exception of YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion\t, YearsWithCurrManager and TotalWorkingYears. We also see that understandably there is a good correlation between PerformanceRating and PercentSalaryHike; and between MonthlyIncome and JobLevel."},{"metadata":{"_uuid":"8402af265202b2b7c11bc5cd5b2c419462c41c6d","_cell_guid":"028f33e0-4e4d-4123-a1d8-a2cf18adc0e7"},"cell_type":"markdown","source":"To dive a little deeper in the information, the features described by categorical data were then examined, and the attrition rate for these features analysed. In order to achieve this, two functions were created; a first function that drills into the data by and works out the attrition per category of each feature, and a second that plots the numbers of employees (remaining and left) in a bar chart and superimposes the attrition measurement for these categories."},{"metadata":{"_uuid":"a0b16cd7b2885a67646bddd3e75bb776db672b03","_cell_guid":"74411888-4ab6-4b28-85ee-9dab3c39347d","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def analyse_attrition(header, data, features_to_analyse, attrition_true, attrition_false):\n    \"\"\"\n    In oder to print graphs tiled, create a figure at the beginng of the script and\n    use the correct position when calling plot_attrition_curve; (e.g. 311, 312 313).\n    Then call the plt.show at the end of the script.\n\n    For seperate plots, create a figure for each plot, call plot_attrition_curve with\n    position 111 and plt.show for each plot.\n\n    Arguments:\n    header -- the data header\n    data -- the data\n    \"\"\"\n    for feature in features_to_analyse:\n        fig = plt.figure()\n        results = analysis_attrition_feature(\n            header, data, feature, constants.ISRESIGNED_T, attrition_true, attrition_false)\n        plot_attrition_curve(results, feature, fig, 111)\n        plt.show()\n"},{"metadata":{"_uuid":"fb702b34b9309fc7de9d044949b3d42c2d95a361","_cell_guid":"0a0ea117-bff4-40a3-9cd8-c701cd3b9474","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def analysis_attrition_feature(header, data, feature_id, attrition_feature_id,\n                               attrition_true=1, attrition_false=0):\n    \"\"\"\n    Analyses the attrition of a specified feature. For example if the feature\n    is teh departments this fucntion will give the number of employees,\n    number of terminations and attrition per department\n\n    Arguments:\n    header -- the header infomation of this dataset\n    data -- the dato to process\n    feature_id -- the column name of the feature to examine (e.g. Departments)\n    left_feature_id -- the column name of the attrition column\n    left_true -- the value of the left column when the employee has left\n    left_false -- the value of the left column when the employee is still employed\n\n    Returns:\n    results -- hashmap of results having:\n                Categories, array of categories of theis feature\n                Current, array of current employees for each feature\n                Left, array of left employees for each feature\n                Attrition, array for the attrition for each feature\n    \"\"\"\n    results = {}\n\n    # the index in the data that points to the attrition\n    left_idx = np.argwhere(header == attrition_feature_id).squeeze()\n\n    # the index in the data that points to the department of the employee\n    feature_idx = np.argwhere(header == feature_id).squeeze()\n\n    # the names of the categories in this feature\n    # e.g. all teh department names under the header departments\n    categories = np.unique(data[:, np.argwhere(header == feature_id)])\n\n    category_employees_current = np.empty((len(categories), 0), dtype=int)\n    category_employees_left = np.empty((len(categories), 0), dtype=int)\n\n    for category in categories:\n        # filter the data for the feature\n        feature_data = data[np.where(\n            data[:, feature_idx] == category), :].squeeze()\n\n        # extract emplyees still employed and add their count to the current employees array\n        still_working = len(\n            np.where(feature_data[:, left_idx] == attrition_false)[0])\n\n        category_employees_current = np.append(\n            category_employees_current, [still_working])\n\n        # extract emplyees that left and add their count to the left employees array\n        left = len(np.where(feature_data[:, left_idx] == attrition_true)[0])\n        category_employees_left = np.append(category_employees_left, [left])\n        #print('{} -{}'.format(feature, feature_employees_left.shape))\n\n    feature_percentage_attrition = category_employees_left / (category_employees_current\n                                                              + category_employees_left) * 100\n\n    # move all results in hashmap\n    results['categories'] = categories\n    results['current'] = category_employees_current\n    results['left'] = category_employees_left\n    results['attrition'] = feature_percentage_attrition\n\n    return results\n"},{"metadata":{"_uuid":"d75ad4c8568d9589468f3e8aa02c7a7f1326d3d5","_cell_guid":"3caaafd0-0f62-41c6-99c1-7ed4d70a8d5c","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def plot_attrition_curve(results, title, fig, position):\n    \"\"\"\n    Plots the the number of employees employed vs the number of employees left\n    of all the various categories for a given feature.\n    For example if we cansider the departments feature, this function will plot a \n    bar graph of the current and resigned employees for eveery department.\n    It will also superimpose the attrition values for each category\n\n    Arguments:\n    results -- dictionary containing all the data necessary\n                'Features'\n    \"\"\"\n    plt.gcf().subplots_adjust(bottom=0.4)\n\n    categories = results['categories']\n    employees_current = results['current']\n    employees_left = results['left']\n    percentage_attrition = results['attrition']\n\n    X = range(0, len(categories), 1)\n\n    ax1 = fig.add_subplot(position)\n    fig.suptitle(title)\n\n    ax1.bar(X, employees_current, color='b', alpha=0.5)\n    ax1.bar(X, employees_left, color='r', alpha=0.5, bottom=employees_current)\n    ax1.set_ylabel('Employees current & left')\n\n    ax2 = ax1.twinx()\n    ax2.plot(X, percentage_attrition, color='b')\n    ax2.set_ylim([0, np.max(percentage_attrition)+1])\n    ax2.set_ylabel('% Attrition')\n\n    ax2.set_xticks(X)\n    ax2.set_xticklabels(categories)\n\n    for tick in ax1.get_xticklabels():\n        tick.set_rotation(80)\n"},{"metadata":{"_uuid":"46415fe9b4da8a5477e20c03252d42b294dbb70e","_cell_guid":"0acba411-5f0f-4fa7-8049-975260e7cf88"},"cell_type":"markdown","source":"In order to execute this analysis step, a list of the features to be analysed is necessary. The list is used to generate the relevant plots."},{"metadata":{"_uuid":"d2df9aabc7376920b5c71a5ab1eb9dfd73944b7d","_cell_guid":"3b392d11-4bc8-4c8e-84cd-a92b06874b03"},"outputs":[],"execution_count":null,"cell_type":"code","source":"features_to_analyse = [\n    constants.EDUCATION_T, constants.GENDER_T,\n    constants.COMPANIES, constants.STATUS_T,\n    constants.ROLE_T, constants.LEVEL,\n    constants.DEPARTMENT_T,\n    constants.SATISFACTION, constants.TEAMCLICK,\n    constants.LIFEBALANCE, constants.ENVIRONMENT,\n    constants.STOCKOPTIONS, constants.TRAINING,\n    constants.RATING,\n    constants.INVOLVEMENT, constants.OVERTIME_T, constants.TRAVEL_T\n]\n\nanalyse_attrition(header, data, features_to_analyse, 'Yes', 'No')"},{"metadata":{"_uuid":"cdb2c52bffb9e143631d1afcebf0afe8a348e6d7","_cell_guid":"21bf779a-516b-4279-851b-3e65a146490f","collapsed":true},"cell_type":"markdown","source":"# Observations\n\nThe following list highlights the main thoughts that emerged from this analysis:\n1.\tEmployees that studied in HR, Marketing and Technical had a higher attrition rate than all the others.\n2.\tSales representatives had a tremendously high attrition rate, whilst managers, directors and people in the more senior roles tended to remain in the company’s workforce.\n3.\tAttrition was slightly higher for males.\n4.\tA ‘sweet spot’ for employees that were in their third fourth or fifth job was noticed. The attrition rate in these cases was remarkably low.\n5.\tSingles were more likely to leave the company than divorced and married employees.\n6.\tAttrition was slightly less in the R&D department when compared to the sales and HR departments.\n7.\tEmployees that were not satisfied in the company, that were very unhappy in their team, that did not have a healthy work life balance, and that were not happy with their work environment were more likely to leave. In all cases we noticed that the attrition rate for ‘very unhappy’ employees was around double than for the other cases.\n8.\tThere was a preferred stock option scheme, the option number 3. The attrition rate for this scheme was much lower than for the other three.\n9.\tTraining was also a very important factor. We noticed that the attrition rate for the employees that received no training was around 25%, compared to values that range from 10% to 15% for employees that did receive some training.\n10.\tThe performance rating had no effect on attrition. As we noticed before, all employees were graded as either 3 or 4. In this analysis we noticed that turnover was practically the same in both cases. In addition, we noticed that a only a small percentage (12%) of the total workforce were rated a grade 4.\n11.\tEmployees that were not involved in their work tended to leave.\n12.\tOvertime had a negative effect on attrition. We noticed that employees were more likely to leave when they did overtime. \n13.\tA similar situation was encountered when travel was considered. Employees that travelled frequently were more likely to leave."},{"metadata":{"_uuid":"978f11d513ae8eff06608638d1859cc7fa8bfb02","_cell_guid":"d0ac3233-0033-4ab4-a423-9839c1fa82b2"},"cell_type":"markdown","source":"# Further Analysis\n\nIn the above exercise the categorical data in the set was analysed with regards to attrition level. \nIn the following, the same attempt was being carried out on the real-valued data in the set. As in the first case, two functions were developed; the first function selects and formats the data, whilst the second plots it in a scatter plot."},{"metadata":{"_uuid":"abeca5d2d65f70a2346369483abb78075a3f6e83","_cell_guid":"51bbbfc3-af73-4e86-9d00-212ea659f42c","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def analysis_comparison_features(header, data, x_feature_id, y_feature_id, z_feature_id, filter_feature_id=None, filter_feature_value=None):\n    \"\"\"\n    \"\"\"\n    # the index of the number of years at the company\n    x_feature_idx = np.argwhere(header == x_feature_id).squeeze()\n\n    # the index of the salary at the company\n    y_feature_idx = np.argwhere(header == y_feature_id).squeeze()\n\n    # the index of the salary at the company\n    z_feature_idx = np.argwhere(header == z_feature_id).squeeze()\n\n    if filter_feature_id != None:\n        filter_feature_idx = np.argwhere(header == filter_feature_id)\n        filtered_data = data[np.where(\n            data[:, filter_feature_idx] == filter_feature_value), :]\n    else:\n        filtered_data = data\n\n    x_data = filtered_data[:, x_feature_idx]\n    y_data = filtered_data[:, y_feature_idx]\n    z_data = filtered_data[:, z_feature_idx]\n\n    return x_data, y_data, z_data\n"},{"metadata":{"_uuid":"5fa37e4a66b39d4ed012939c85c7f5ba4049096c","_cell_guid":"734f52c1-17ec-4ca9-87de-580b8f96fa3e","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def plot_comparison_curve(x_title, y_title, x_data, y_data, z_data):\n\n    left_color = 'lightcoral'\n    remained_color = 'green'\n\n    z_left = np.where(z_data == 0)\n    z_remained = np.where(z_data == 1)\n\n    left, width = 0.1, 0.65\n    bottom, height = 0.1, 0.65\n    bottom_h = left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.2]\n    rect_histy = [left_h, bottom, 0.2, height]\n\n    plt.figure(1, figsize=(8, 8))\n    axScatter = plt.axes(rect_scatter)\n    axHistx = plt.axes(rect_histx)\n    axHisty = plt.axes(rect_histy)\n\n    colors = [left_color, remained_color]\n\n    axScatter.set_xlabel(x_title)\n    axScatter.set_ylabel(y_title)\n\n    axScatter.scatter(x_data, y_data, c=z_data, cmap=ListedColormap(colors), alpha=0.5)\n\n    x_bins = min(30, len(np.unique(x_data)))\n\n    axHistx.hist(x_data[z_left], bins=x_bins, color=left_color, alpha=0.5)\n    axHistx.hist(x_data[z_remained], bins=x_bins, color=remained_color, alpha=0.5)\n\n    x_left_hist = np.histogram(x_data[z_left], bins=x_bins)[0]\n    x_remained_hist = np.histogram(x_data[z_remained], bins=x_bins)[0]\n    # added a small factor to avoid division errors\n    attrition = x_left_hist / (x_remained_hist + x_left_hist + 0.00005) * 100\n\n    axHistx_2 = axHistx.twinx()\n    axHistx_2.plot(np.histogram(x_data[z_left], bins=x_bins-1)[1], attrition, color='b')\n    axHistx_2.set_ylim([0, 105])\n    axHistx_2.set_ylabel('% Attrition')\n\n    y_bins = min(30, len(np.unique(y_data)))\n\n    axHisty.hist(y_data[z_left], bins=y_bins, color=left_color,\n                 alpha=0.5, orientation='horizontal')\n    axHisty.hist(y_data[z_remained], bins=y_bins, color=remained_color,\n                 alpha=0.5, orientation='horizontal')\n\n    plt.show()\n"},{"metadata":{"_uuid":"5bc9974d4d546f7002fd942c7bf1ac9d3274b64d","_cell_guid":"9649b71b-23c4-4ba9-adb1-1d09064cb3fd","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def analyse_comparison(header, data, comparison_sets):\n    for feature_1, feature_2 in comparison_sets:\n        x, y, z = analysis_comparison_features(\n            header, data, feature_1, feature_2, constants.ISRESIGNED_T)\n        plot_comparison_curve(feature_1, feature_2, x, y, z)\n"},{"metadata":{"_uuid":"acdf4c4a0412855ebf3594ee54f4c6a7af5a62ce","_cell_guid":"cd521294-4bad-4664-b002-67e1f8d85b88"},"cell_type":"markdown","source":"The execution of this analysis step is very similar to the execution of the first step. A list that contained the featured to be analysed was created and then it was used to create the plots."},{"metadata":{"_uuid":"bc4a5f6be2505b06560d796f746ad944a393d1c9","_cell_guid":"d12692e9-946b-4c81-9f92-5aa50a50100d"},"outputs":[],"execution_count":null,"cell_type":"code","source":"comparison_sets = [(constants.SALARY, constants.AGE),\n                    (constants.SALARY, constants.YEARSEMPLOYED),\n                    (constants.LASTINCREMENTPERCENT, constants.RATING)]\n\nanalyse_comparison(m_header, m_data, comparison_sets)"},{"metadata":{"_uuid":"4a64997c28786d2b37237accea47d4d07c81721b","_cell_guid":"41877018-932b-4a5d-b004-c1c85734002a"},"cell_type":"markdown","source":"# Observations\n\nThree plots were generated from this analysis step. It was noticed that salary was a major trigger to leave the company, especially for young employees; as the attrition rate was nearly 50 % for employees who were less than thirty years old and that has their salary in the lowest band.\nIt was also noticed that there was some relation between the salary and number of years in the company, and somewhat, either by natural progression in the company’s ranks or by turnover there were no employees with a seniority of over 20 years in the lowest salary bands.\nFinally, an increment structure was deduced when the salary increments were examined with the employees’ grade. There was a clear discrimination between 3 and 4 performers, where the 4 performers received an increment that had a maximum value of 25% (there were 18 such instances), whilst and the lowest increment for a 3 performer was 11% (210 cases)."},{"metadata":{"_uuid":"99b390d5f8b1f1ffffbb6ec783c54bd338682892","_cell_guid":"e0ba72f8-8efb-4f92-9be5-506f67be2f28"},"cell_type":"markdown","source":"# Part 2 - Machine Learning Models\n\nNumerous other studies that could have been carried out on this data. But having gained some insight on the characteristics of this company, an attempt to build some models was then carried out.\nThe results of several classification models, namely:\n1.\tNaïve Bayes\n2.\tLogistic Regression\n3.\tK-Nearest Neighbours\n4.\tNeural Network\nwere then compared.\n\nThe procedure adopted to engineer the data so that these models could be executed is illustrated in the diagram below:\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/16151/21316/HR%20ML.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1521191545&Signature=PwjVhdwuNsnLV53eGWLG9aIlvD5kTkcoSlPhzOQmh5tmpsqgeC7WSq5IkpBsLn5P3qzrN4h6dqBkIb99fk%2FsIJCtbZmlUOletjGvBNWYAJ%2FESOv%2FPIjOjJ5HtxdmFluzZLI%2BD2PZtMyyJbysTgXGbPmYyFffacN%2F19t5%2FUavG4iP4EanjB6iA9TG0tnV3nk708NZ5n4nf7oHEey5BJ6c9Gv21cvjmpmKNTVMSdekuKDlLvIJyde%2Fw81E2Op9WGcNke%2BOX%2F%2FxmoktT1eUdXx54HiXyMtrevncIcezhHBi8%2BiSM1PPhOBkFDbNbKz4XFScbYzINJHvee5uItVHNHx4ow%3D%3D)\n\n\nAs a first step, the data is ‘cleaned’ from redundant information and the categorical text fields are coded in a numerical equivalent as explained earlier on in this notebook. The result of this step is saved in a file.\nThe data is the split in train and test sets, where an 80/20 split was applied. The output feature, or the ‘Attrition’ field was also extracted and saved in another file, so that an X and Y files were created for both training and testing sets.\n\nThis data was then further processed to create the data required for each specific model. For example, in the Naïve Bayes case, the continuous fields were discretized using a number of buckets. Similarly, in the Neural Networks case, the continuous data was standardized and the categorical data was encoded using dummy coding.\n\nIn all cases the resulting data-sets were stored in files.\n"},{"metadata":{"_uuid":"86a4bfe7492be5dbd3c0c9ec528bac9827745ff6","_cell_guid":"a43796d7-b33d-4b98-9a39-30f8666d9130"},"cell_type":"markdown","source":"# Helper Scripts\n\nSeveral  functions were used to create the model data set files. They were not executed in this notebook as Kaggle had some limitations when new data files were created. However, they are included below for completeness sake."},{"metadata":{"_uuid":"fe0c778eee421645ebafb691e4b09ba4ab1d3214","_cell_guid":"54588e64-c493-4b29-8930-6bc4e81e1e6e","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def save_data(filename, header, data, override=False):\n    \"\"\"\n    saves a dataset to file\n\n    Arguments:\n    filename -- the name of the dataset file\n    header -- the data header\n    data -- the data information\n\n    Returns:\n    \"\"\"\n\n    script_dir = os.path.dirname(__file__)\n    abs_data_file_path = os.path.join(script_dir, filename)\n\n    if (os.path.exists(abs_data_file_path) is False) or (override is True):\n        # create dataframe with right format\n        data_frame = pd.DataFrame(data=data, columns=header)\n        # save data, remove index coluum\n        data_frame.to_csv(abs_data_file_path, index=False)"},{"metadata":{"_uuid":"62194af1c0623e383972379bb88d260b8fb9143c","_cell_guid":"f6823faf-a2cd-44ee-9c6c-4d984d4de016","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def partition_data(data_orig, train_percentage=0.6, test_percentage=0.2):\n    \"\"\"\n    Divides a dataset in three parts after randomly rearranging its rows. Default partitions\n    are as follows:\n    Training set -- 60% of data\n    Testing set -- 20% of data\n    Dev Set -- 20% of data\n\n    Arguments:\n    data_orig -- the original, complete dataset\n    train_percentage -- the percentage of the dataset that will become training data\n    test_percenntage -- the percentage of the dataset that will become testing data\n                        hence the development dataset will become the remainder\n    Returns:\n    train_set -- the training data set\n    test_set -- the testing data set\n    dev_set -- the development data set\n    \"\"\"\n    # shuffle data. as data is shuffled accross rows, it requires the double transpose\n    np.random.shuffle(data_orig)\n    data_orig = data_orig\n\n    train_set_size = (int)(data_orig.shape[0] * train_percentage)\n    test_set_size = (int)(data_orig.shape[0] * test_percentage)\n\n    train_set = data_orig[:train_set_size, :]\n    test_set = data_orig[train_set_size:train_set_size+test_set_size, :]\n    dev_set = data_orig[train_set_size+test_set_size:, :]\n\n    return train_set, test_set, dev_set\n"},{"metadata":{"_uuid":"94349891d75ebd15e258694f5cdda6f7e458900c","_cell_guid":"2978c041-d80d-4cb6-9881-0d07b6e733af","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def prepare_model_data(rewrite_files=False):\n    '''\n    Prepares the data set files used by the models.\n\n    Returns:\n    results -- dictionary containing the following\n        train_x_nb -- training x for naive bayes data set\n        train_y -- training y data set\n        test_x_nb -- test x for naive bayes data set\n        test_y -- test y data set\n        train_x_nn -- training x for neural nets (& Logistic regression)  data set\n        test_x_nn -- test x for neural nets (& Logistic regression)  data set\n    '''\n\n    # step 1:\n    column_drop_list = [constants.EMPLYEENO_R, constants.EMPLOYEECOUNT_R,\n                        constants.ISOVER18_R, constants.STDHOURS_R]\n    encode_list = [constants.GENDER_T, constants.STATUS_T, constants.DEPARTMENT_T, constants.ROLE_T,\n                   constants.OVERTIME_T, constants.TRAVEL_T, constants.ISRESIGNED_T, constants.EDUCATION_T]\n\n    _, _, m_header, m_data, _ = load_data(\n        constants.orig_file, encode_list, column_drop_list)\n\n    save_data(constants.processed_file, m_header, m_data, override=rewrite_files)\n\n    # step 2:\n    train_data, test_data, _ = partition_data(m_data, 0.8, 0.2)\n\n    output_idx = np.argwhere(m_header == constants.ISRESIGNED_T).squeeze()\n\n    train_y = train_data[:, output_idx]\n    train_x = np.delete(train_data, output_idx, 1)\n\n    test_y = test_data[:, output_idx]\n    test_x = np.delete(test_data, output_idx, 1)\n\n    traintest_header = np.delete(m_header, output_idx, 0)\n\n    save_data(constants.train_x_file, traintest_header, train_x, override=rewrite_files)\n    save_data(constants.train_y_file, np.array([constants.ISRESIGNED_T]), train_y, override=rewrite_files)\n    save_data(constants.test_x_file, traintest_header, test_x, override=rewrite_files)\n    save_data(constants.test_y_file, np.array([constants.ISRESIGNED_T]), test_y, override=rewrite_files)\n\n    # to execute naive bayes we will discretise continuous data\n    column_bins_definition = {constants.AGE: 10, constants.DAILYRATE: 10,\n                              constants.HOMEDISTANCE: 10, constants.SALARY: 10,\n                              constants.HOURLYRATE: 10, constants.MONTHLYRATE: 10,\n                              constants.YEARSEMPLOYED: 5, constants.YEARSCOMPANY: 5,\n                              constants.YEARSROLE: 5, constants.YEARSLASTPROMO: 5,\n                              constants.YEARSMANAGER: 5, constants.LASTINCREMENTPERCENT: 16}\n\n    train_x_nb = digitize_columns(\n        traintest_header, train_x, column_bins_definition)\n    test_x_nb = digitize_columns(\n        traintest_header, test_x, column_bins_definition)\n\n    save_data(constants.train_x_nb_file, traintest_header, train_x_nb, override=rewrite_files)\n    save_data(constants.test_x_nb_file, traintest_header, test_x_nb, override=rewrite_files)\n\n    columns_norm = [constants.AGE, constants.DAILYRATE,\n                    constants.HOMEDISTANCE, constants.SALARY,\n                    constants.HOURLYRATE, constants.MONTHLYRATE,\n                    constants.YEARSEMPLOYED, constants.YEARSCOMPANY,\n                    constants.YEARSROLE, constants.YEARSLASTPROMO,\n                    constants.YEARSMANAGER, constants.LASTINCREMENTPERCENT]\n\n    train_x_lr = feature_scale_columns(traintest_header, train_x, columns_norm)\n    test_x_lr = feature_scale_columns(traintest_header, test_x, columns_norm)\n\n    save_data(constants.train_x_lr_file, traintest_header, train_x_lr, override=rewrite_files)\n    save_data(constants.test_x_lr_file, traintest_header, test_x_lr, override=rewrite_files)\n\n    encode_list_nn = [constants.GENDER_T, constants.STATUS_T, constants.DEPARTMENT_T, constants.ROLE_T,\n                      constants.OVERTIME_T, constants.TRAVEL_T, constants.EDUCATION_T,\n                      constants.ENVIRONMENT, constants.INVOLVEMENT, constants.LEVEL,\n                      constants.SATISFACTION, constants.COMPANIES, constants.RATING, constants.TEAMCLICK,\n                      constants.STOCKOPTIONS, constants.TRAINING, constants.LIFEBALANCE]\n\n    train_test_header_nn, train_x_nn = encode_columns_nn(\n        traintest_header, train_x_lr, encode_list_nn)\n    _, test_x_nn = encode_columns_nn(\n        traintest_header, test_x_lr, encode_list_nn)\n\n    save_data(constants.train_x_nn_file, train_test_header_nn, train_x_nn, override=rewrite_files)\n    save_data(constants.test_x_nn_file, train_test_header_nn, test_x_nn, override=rewrite_files)\n\n    results = {}\n\n    results['train_x_nb'] = train_x_nb\n    results['train_y'] = train_y\n    results['test_x_nb'] = test_x_nb\n    results['test_y'] = test_y\n    results['train_x_nn'] = train_x_nn\n    results['test_x_nn'] = test_x_nn\n    return results"},{"metadata":{"_uuid":"106e8b9ea41ed43430720892e62b31d853f64747","_cell_guid":"c641a438-6b70-4c2b-9697-0583792211f7"},"cell_type":"markdown","source":"In the next function, data-sets that were required by the machine learning models were loaded from the files and returned in a dictionary.\n"},{"metadata":{"_uuid":"220769aafedadfaf0b6f47b3ee510c2107c39062","_cell_guid":"48214a2d-0573-4eca-80c0-4f6181c40979","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def load_model_data():\n    '''\n    Loads the model datasets from the respective files\n\n    Returns:\n    results -- dictionary containing the following\n        train_x_nb -- training x for naive bayes data set\n        train_y -- training y data set\n        test_x_nb -- test x for naive bayes data set\n        test_y -- test y data set\n        train_x_nn -- training x for neural nets (& Logistic regression)  data set\n        test_x_nn -- test x for neural nets (& Logistic regression)  data set\n    '''\n    _, train_x_nb, _, _, _ = load_data(constants.train_x_nb_file,[],[])\n    _, train_y, _, _, _  = load_data(constants.train_y_file,[],[])\n    _, test_x_nb, _, _, _ = load_data(constants.test_x_nb_file,[],[])\n    _, test_y, _, _, _ = load_data(constants.test_y_file,[],[])\n    _, train_x_nn, _, _, _ = load_data(constants.train_x_nn_file,[],[])\n    _, test_x_nn, _, _, _ = load_data(constants.test_x_nn_file,[],[])\n\n    results = {}\n    results['train_x_nb'] = train_x_nb\n    results['train_y'] = train_y\n    results['test_x_nb'] = test_x_nb\n    results['test_y'] = test_y\n    results['train_x_nn'] = train_x_nn\n    results['test_x_nn'] = test_x_nn\n\n    return results"},{"metadata":{"_uuid":"0b731cce9171f741ab1b8268e952e90f34de6c8c","_cell_guid":"081a6992-e6ca-46dc-a0e6-5c318e9276f5","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"model_data = load_model_data()"},{"metadata":{"_uuid":"21926b1f8299e68d93b08ecb07618bbeab61fabb","_cell_guid":"06d71d2f-dadd-42c9-bcf4-bf381ea9c717"},"cell_type":"markdown","source":"And finally, the models were executed. An execution function was created for this purpose. The models were used using the scikit library, although other libraries such as tensorflow could have been used."},{"metadata":{"_uuid":"895b5f7a33ebb91da4426c4bf5488cce74c20a13","_cell_guid":"66dbbfaf-968e-4433-a69f-88eeb950a8d8","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def execute_classifier(classifier_name, classifier, train_x, train_y, test_x, test_y):\n    '''\n    Executes a classifier given the test and train data. Calculates the execution time for the \n    training\n\n    Arguments:\n    classifier_name -- the name of the classifier, for printing purposes\n    classifier -- the actual classifier\n    train_x -- training x data\n    train_y -- training y data\n    test_x -- testing x data\n    test_y -- testing y data\n\n    Returns:\n    prints the result of the training and testing, together with the traioning execution time required\n    test-score -- the testing score for this classifier\n    '''\n    t_start = time.clock()\n    classifier.fit(train_x, train_y.ravel())\n    t_end = time.clock()\n    time_diff = t_end - t_start\n\n    train_score = classifier.score(train_x, train_y)\n    test_score = classifier.score(test_x, test_y)\n\n    print('{} -  \\t train score: {},\\t test score: {},\\t time:{}'.format(\n        classifier_name, train_score, test_score, time_diff))\n\n    return test_score\n"},{"metadata":{"_uuid":"2d225bfc6e3cc3f42452921becbfeeb8ac925448","_cell_guid":"c17587f0-a3ee-4951-86e9-29b9d0462752"},"outputs":[],"execution_count":null,"cell_type":"code","source":"# KNN\nclassifier = KNeighborsClassifier(n_neighbors=3)\ntest_score = execute_classifier(\"K - N N\", classifier, model_data['train_x_nb'], model_data['train_y'], model_data['test_x_nb'], model_data['test_y'])"},{"metadata":{"_uuid":"cb376e2ffb809af5dafc696b2dca70567cec8306","_cell_guid":"a1a0b1b6-4078-4232-8df9-dcbf1bdac797"},"outputs":[],"execution_count":null,"cell_type":"code","source":"# naive bayes\nclassifier = MultinomialNB(alpha=1)\ntest_score = execute_classifier(\"Naive Bayes\", classifier, model_data['train_x_nb'], model_data['train_y'], model_data['test_x_nb'], model_data['test_y'])"},{"metadata":{"_uuid":"8e32bb304a22b4e1cb1b2c1002dd3d3e20cd8bd1","_cell_guid":"68bdb321-0fad-47ba-93f9-a955fdc0a8ea"},"outputs":[],"execution_count":null,"cell_type":"code","source":"# logistic regression\nclassifier = LogisticRegression()\ntest_score = execute_classifier(\"Log Reg\", classifier, model_data['train_x_nn'], model_data['train_y'], model_data['test_x_nn'], model_data['test_y'])"},{"metadata":{"_uuid":"5102b72dc7d03a5eab4820fcbf59d1a222d90386","_cell_guid":"bc0c6ca5-d69a-4e3e-af96-95d6126cd3cb"},"outputs":[],"execution_count":null,"cell_type":"code","source":"# neural net\nclassifier = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(256,64), max_iter=300)\ntest_score = execute_classifier(\"N Net\", classifier, model_data['train_x_nn'], model_data['train_y'], model_data['test_x_nn'], model_data['test_y'])"},{"metadata":{"_uuid":"88e5feecaf4bac0cf20027ebd6ae233cde5a7ee8","_cell_guid":"d1248ae2-ba3f-4fe0-ab4d-76ab91f7d968","collapsed":true},"cell_type":"markdown","source":"# Conclusion\n\nIn this first attempt to address the attrition issue in this company the data provided was analysed and a simple system where data was engineered and transferred into machine learning models that predicted employee attrition was developed. Four machine learning models were implemented in this release, resulting in a best prediction of 90%. It can be observed that a decent result was produced by the neural network, when considering the small amount of data that was provided. We can also deduce that the neural network model is most probably overfitted and therefore some further adjustments are necessary. Several hyperparameter configurations were attempted for this classifier and the best results were included in this notebook.\n\nThere is obviously room for further analysis and improvement and tuning of the models. In addition, more classifiers and different architectures can be tested in future attempts."},{"metadata":{"_uuid":"c0526bc561e65a032e065a877930095cdd9fe296","_cell_guid":"9d16cd62-f0de-4cc0-b4ed-28aa7827265f","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":""}]}