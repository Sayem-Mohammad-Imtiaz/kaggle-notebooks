{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Table of contents\n\n* [1. Load libraries](#load)\n* [2. Introduction](#intro)\n* [3. Dataset](#dataset)\n* [4. Initial and exploratory data analysis](#eda)\n    * [4.1. Label column](#label)\n    * [4.2. Each feature](#features)\n    * [4.3. Features vs label](#vslabel)\n* [5. Modelling](#model)\n    * [5.1. Using accuracy](#acc)\n        * [5.1.1. Evaluation on the test set](#testacc)\n    * [5.2. Using balanced accuracy score](#balacc)\n        * [5.2.1. Evaluation on the test set](#testbalacc)\n    * [5.3. With oversampling](#over)\n        * [5.3.1. Evaluation on the test set](#testover)\n* [6. Comparing the scores for all models](#compare)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"load\"></a>\n# 1. Load libraries","metadata":{}},{"cell_type":"code","source":"# -- Data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# -- Data visualisation\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter # to format decimal points on axis\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set_palette(\"pastel\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:25.734781Z","iopub.execute_input":"2021-09-10T15:12:25.73517Z","iopub.status.idle":"2021-09-10T15:12:25.741789Z","shell.execute_reply.started":"2021-09-10T15:12:25.735138Z","shell.execute_reply":"2021-09-10T15:12:25.740412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"intro\"></a>\n# 2. Introduction","metadata":{}},{"cell_type":"markdown","source":"This is a super quick little notebook with some basic EDA and just one estimator model (KNN) with its hyperparameters tuned. The notebook is presented as a tutorial for beginners. I also use two different metrics - mean accuracy and balanced accuracy to tune the model and discuss which is best. Another pre-processing method used is oversampling with SMOTE to deal with minority classes.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"dataset\"></a>\n# 3. Dataset","metadata":{}},{"cell_type":"code","source":"# -- Load the dataset\ndf = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\n\n# -- Have a look at the top of the dataframe\ndf.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:25.849331Z","iopub.execute_input":"2021-09-10T15:12:25.849792Z","iopub.status.idle":"2021-09-10T15:12:25.882268Z","shell.execute_reply.started":"2021-09-10T15:12:25.849741Z","shell.execute_reply":"2021-09-10T15:12:25.881101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Get the shape of the dataframe\nprint(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns\\n')\n\n# -- See if the data types for each column make sense\ndf.info()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:25.918684Z","iopub.execute_input":"2021-09-10T15:12:25.919129Z","iopub.status.idle":"2021-09-10T15:12:25.937462Z","shell.execute_reply.started":"2021-09-10T15:12:25.919092Z","shell.execute_reply":"2021-09-10T15:12:25.935953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 12 columns, 11 of those are features and 1 is a label (the last column). There's no missing values and each column has the right data type. The label indicates the quality of the wine as rated by tasters. 1 is for the poorest quality wines and 10 is for the best wines. Each row represents an individual wine, so all the observations are independent. Now let's see if there's duplicated rows in the dataset.","metadata":{}},{"cell_type":"code","source":"# -- Count the number of duplicated rows\nprint(f'There are {df.duplicated().sum()} duplicated rows in the dataframe.')\n\n# -- Remove duplicated rows\ndf = df.drop_duplicates()\n\nprint(f'After removing duplicated rows there are {df.shape[0]} rows in the dataframe.')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-09-10T15:12:25.998436Z","iopub.execute_input":"2021-09-10T15:12:25.99885Z","iopub.status.idle":"2021-09-10T15:12:26.013842Z","shell.execute_reply.started":"2021-09-10T15:12:25.998818Z","shell.execute_reply":"2021-09-10T15:12:26.012865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why should we remove the rows that are duplicated? Well, it is unlikely that different wines will have the exact same physicochemical composition, especially with features that have 4 decimal digits (density). Therefore, this must be an error or the wines are so similar that we can count them as one by removing the duplicates.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# -- Separate features and label\n# (a) drop target column\nX = df.drop(columns=['quality'])\n# (b) make an array with the target column\ny = df['quality'].copy()\n\n# -- Split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:12:26.10618Z","iopub.execute_input":"2021-09-10T15:12:26.107009Z","iopub.status.idle":"2021-09-10T15:12:26.121304Z","shell.execute_reply.started":"2021-09-10T15:12:26.106959Z","shell.execute_reply":"2021-09-10T15:12:26.120147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When splitting the dataset into train and test sets it's important to keep the proportions of classes in the label column consistent. We want to have the sameish proportion of classes in our train and test sets as in the original set. To do this we used the *stratify* parameter.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"eda\"></a>\n# 4. Initial and exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"label\"></a>\n## 4.1. Label column","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=y_train)\nplt.gca().set(xlabel='Red wine quality', ylabel='Count', title='Distribution of quality categories')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:26.181091Z","iopub.execute_input":"2021-09-10T15:12:26.181896Z","iopub.status.idle":"2021-09-10T15:12:26.352471Z","shell.execute_reply.started":"2021-09-10T15:12:26.18185Z","shell.execute_reply":"2021-09-10T15:12:26.351394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Get the actual count of wines of each category\ny_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:12:26.353855Z","iopub.execute_input":"2021-09-10T15:12:26.354144Z","iopub.status.idle":"2021-09-10T15:12:26.362909Z","shell.execute_reply.started":"2021-09-10T15:12:26.354115Z","shell.execute_reply":"2021-09-10T15:12:26.362019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the poor quality wines (3 and 4) and high quality wines (7 and 8) don't have a lot of instances in the dataset. It makes sense to group the wines into three groups to give the model something to work with. Let's say 3 and 4 wines are poor (3rd place), 5 and 6 wines are ok (2nd place), and 7 and 8 wines are great (1st place).","metadata":{}},{"cell_type":"code","source":"def rename_labels(labels):\n    # -- Assign new labels for wine quality\n    # (a) form an array with new labels\n    new_labels_array = np.where( (labels == 3) | (labels == 4), 3, labels)\n    new_labels_array = np.where( (labels == 5) | (labels == 6), 2, new_labels_array)\n    new_labels_array = np.where( (labels == 7) | (labels == 8), 1, new_labels_array)\n    # (b) make a pandas series out of the array to later assign correct indices\n    new_labels = pd.Series(new_labels_array)\n    # (c) assign correct indices\n    new_labels.index = labels.index\n    # (d) rename the series\n    labels = new_labels\n    return labels\n\n# -- Rename labels for both train and test sets\ny_train = rename_labels(y_train)\ny_test = rename_labels(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:12:26.364523Z","iopub.execute_input":"2021-09-10T15:12:26.364851Z","iopub.status.idle":"2021-09-10T15:12:26.383929Z","shell.execute_reply.started":"2021-09-10T15:12:26.364821Z","shell.execute_reply":"2021-09-10T15:12:26.382859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Have a look at the new labels\nsns.countplot(x=y_train)\nplt.gca().set(xlabel='Red wine quality', ylabel='Count', title='Distribution of quality categories after renaming')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:26.385917Z","iopub.execute_input":"2021-09-10T15:12:26.386376Z","iopub.status.idle":"2021-09-10T15:12:26.524659Z","shell.execute_reply.started":"2021-09-10T15:12:26.386306Z","shell.execute_reply":"2021-09-10T15:12:26.52392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"features\"></a>\n## 4.2. Each feature","metadata":{}},{"cell_type":"code","source":"# -- Set the plot number for the first subplot\nplot_number = 1\n\nplt.figure(figsize=(25, 40)) # set the size of the whole set of plots\nplt.subplots_adjust(hspace=0.9, wspace=0.15) # set the space between subplots\n\nfor col in X_train.columns:\n    plt.subplot(12, 2, plot_number)\n    sns.histplot(X_train[col], color=\"#9bd0b7\")\n    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n    plt.title(f'{col.capitalize()} histogram')\n    plt.xlabel('')\n    plt.ylabel('')\n\n    plt.subplot(12, 2, plot_number+1)\n    \n    sns.boxplot(x=X_train[col], color='#badfda', width=0.7, linewidth=0.6)\n    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n    plt.title(f'{col.capitalize()} boxplot')\n    plt.xlabel('')\n    plt.ylabel('')\n\n    plot_number = plot_number+2 # set a new plot number for the next feature\n    \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:26.525955Z","iopub.execute_input":"2021-09-10T15:12:26.52635Z","iopub.status.idle":"2021-09-10T15:12:30.428821Z","shell.execute_reply.started":"2021-09-10T15:12:26.526321Z","shell.execute_reply":"2021-09-10T15:12:30.427704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quite a few features are not normally distributed (e.g., total sulfur dioxide) and a few have a significant amount of outliers (e.g. sulphates). The distribution of data isn't important for the KNN algorithm, as the algorithm is non-parametric. The outliers also may not be a problem since the algorithm casts a majority vote to make its decision and the outliers are scarce, however, it will be interesting to see if the model's accuracy improves when we deal with the outliers.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"vslabel\"></a>\n## 4.3. Features vs label","metadata":{}},{"cell_type":"code","source":"plot_number = 1\n\nplt.figure(figsize=(25, 20))\nplt.subplots_adjust(hspace=0.2, wspace=0.15)\n\nfor col in X_train.columns:\n    plt.subplot(3, 4, plot_number)\n    sns.boxplot(x=y_train, y=X_train[col])\n    plt.title(f'{col.capitalize()}')\n    plt.xlabel('')\n    plt.ylabel('')\n    \n    plot_number = plot_number+1\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:30.430713Z","iopub.execute_input":"2021-09-10T15:12:30.431324Z","iopub.status.idle":"2021-09-10T15:12:32.187559Z","shell.execute_reply.started":"2021-09-10T15:12:30.43128Z","shell.execute_reply":"2021-09-10T15:12:32.18385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks that there are some trends - the more fixed acidity, citric acid, and alcohol the wine has the better it is rated. But, the differences are rather small and we must remember that both category 1 and 3 wines are sparse, they don't have that many wines in them to begin with so it's questionable if we can even draw such conclusions. If I am bored tomorrow, I will see if I can do some hypothesis tests to see if these box plots hold any water.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"model\"></a>\n# 5. Modelling","metadata":{}},{"cell_type":"markdown","source":"We will make a pipeline which will comprise two steps:\n1. Scale the data\n3. Run the KNN algorithm for classification.\n\nWe scale the data for the KNN algorithm as it relies on the majority vote to make the classification. This means that it will assume that points that are close together are likely to be of the same class. And this is why scaling is important - if features have very different scales the distances between these features may be uninformative. For more info on this, have a look at this [post on StackExchange](https://stats.stackexchange.com/questions/287425/why-do-you-need-to-scale-data-in-knn).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"acc\"></a>\n## 5.1. Using accuracy","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n# Which hyperparameters to tune: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n\ndef objective(trial):\n    # -- Instantiate scaler\n    scalers = trial.suggest_categorical(\"scalers\", ['minmax', 'standard', 'robust'])\n\n    if scalers == \"minmax\":\n        scaler = MinMaxScaler()\n    elif scalers == \"standard\":\n        scaler = StandardScaler()\n    else:\n        scaler = RobustScaler()\n                \n    # -- Tune estimator algorithm\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 30)\n    weights = trial.suggest_categorical(\"weights\", ['uniform', 'distance'])\n    metric = trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski'])\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n        \n    # -- Make a pipeline\n    pipeline = make_pipeline(scaler, knn)\n\n    # -- Cross-validate the features reduced by dimensionality reduction methods\n    kfold = StratifiedKFold(n_splits=10)\n    score = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=kfold)\n    score = score.mean()\n    return score\n\nsampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=300)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-10T15:12:32.189151Z","iopub.execute_input":"2021-09-10T15:12:32.189555Z","iopub.status.idle":"2021-09-10T15:13:23.48144Z","shell.execute_reply.started":"2021-09-10T15:12:32.189512Z","shell.execute_reply":"2021-09-10T15:13:23.480553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Have a look at the best trial\nprint(\"Best trial out of 300 is:\")\nstudy.best_trial","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:13:23.483234Z","iopub.execute_input":"2021-09-10T15:13:23.483521Z","iopub.status.idle":"2021-09-10T15:13:23.493313Z","shell.execute_reply.started":"2021-09-10T15:13:23.483494Z","shell.execute_reply":"2021-09-10T15:13:23.492308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's interesting to see that the best performing scaler is the RobustScaler which is specifically designed to deal with outliers. Perhaps dealing with outliers manually will lead to even better results?","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:13:23.495112Z","iopub.execute_input":"2021-09-10T15:13:23.495498Z","iopub.status.idle":"2021-09-10T15:13:23.568034Z","shell.execute_reply.started":"2021-09-10T15:13:23.495467Z","shell.execute_reply":"2021-09-10T15:13:23.567008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here it looks like the tuning algorithm quickly found the pocket for the best parameters (in the first 50 trials) and after that the accuracy score was consistent.","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:13:23.569375Z","iopub.execute_input":"2021-09-10T15:13:23.569657Z","iopub.status.idle":"2021-09-10T15:13:25.622549Z","shell.execute_reply.started":"2021-09-10T15:13:23.56963Z","shell.execute_reply":"2021-09-10T15:13:25.621616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the single most important hyperparameter is the number of neighbours. All the other parameters and the scaling options have a negligible effect on the score.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"testacc\"></a>\n### 5.1.1. Evaluation on the test set","metadata":{}},{"cell_type":"code","source":"# -- Have a look at the best parameters for the tuned model\nprint(\"Best parameters after tuning using mean accuracy:\")\nstudy.best_params","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:13:25.623767Z","iopub.execute_input":"2021-09-10T15:13:25.624052Z","iopub.status.idle":"2021-09-10T15:13:25.631945Z","shell.execute_reply.started":"2021-09-10T15:13:25.624025Z","shell.execute_reply":"2021-09-10T15:13:25.630989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import classification_report\n\n# -- Instantiate tuned model\nscaler = RobustScaler()\nbest_params = study.best_params # make a dictionary of best parameters\nbest_params.pop('scalers') # delete scaler option from the dictionary of params\nknn = KNeighborsClassifier(**best_params)\npipeline = make_pipeline(scaler, knn)\n\n# -- Make a function to print out mean accuracy scores for both train and test sets\n# and to get a dataframe for precision, recall, and f1 scores for each class\ndef get_scores(pipeline, tuning_method):\n    # -- Get scores for training data\n    kfold = StratifiedKFold(10)\n    score = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=kfold)\n    print(\"Training set: %0.2f mean accuracy with a standard deviation of %0.2f\" % (score.mean(), score.std()))\n\n    # -- Fit the tuned model\n    pipeline.fit(X_train, y_train)\n\n    # -- Evaluate on the test set\n    # (a) mean accuracy\n    test_score = pipeline.score(X_test, y_test)\n    print(\"Testing set: %0.2f mean accuracy\" % test_score)\n    # (b) predict the labels for test data\n    y_predicted = pipeline.predict(X_test)\n\n    print()\n    print(\"Classification report for the testing set:\")\n    print(classification_report(y_test, y_predicted))\n    \n    # -- Produce a dataframe of all metrics for all classes for the model\n    # (a) Get scores for each metric for all classes\n    scores = classification_report(y_test, y_predicted, output_dict=True)\n\n    # Since the output is a dictionary of various metrics and we only need information on each class\n    # we need to extract this information and store it as a dataframe for when we plot it\n    # (b) Get a list of dataframes corresponsing to each class from the scores dictionary\n    list_of_dfs = []\n    for class_number in range(1,4):\n        # -- Define columns\n        each_score = list(scores[str(class_number)].values())[:-1] # get all values into a list apart from the last value (which is support)\n        metric = list(scores[str(class_number)].keys())[:-1]       # do the same for keys\n        class_name = list(str(class_number)*3)                     # assign the class to which the three metrics belong to\n        tuning = [tuning_method for i in range(3)]                      # mark which tuning method these metrics belong to\n\n        data = list(zip(each_score, metric, class_name, tuning))\n\n        # -- Make a dataframe for each class\n        scores_df = pd.DataFrame(data=data, columns=['score', 'metric', 'class', 'tuning method'])\n\n        # -- Make a list of all dataframes\n        list_of_dfs.append(scores_df)\n\n    # -- Concatenate all class dataframes\n    class_scores = pd.concat(list_of_dfs)\n    \n    return class_scores\n\n# -- Get a dataframe for scores for each class\nmean_accuracy_trained_scores = get_scores(pipeline, \"mean accuracy\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:13:25.633424Z","iopub.execute_input":"2021-09-10T15:13:25.633848Z","iopub.status.idle":"2021-09-10T15:13:25.859173Z","shell.execute_reply.started":"2021-09-10T15:13:25.633784Z","shell.execute_reply":"2021-09-10T15:13:25.858154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Judging by accuracy the model isn't doing too bad and there's no overfitting as training scores and testing scores are almost the same (0.01 difference). However, if we look at the classification report for each category, it looks like the model essentially classifies all wines as category 2 (ok wines) which are the vast majority of all wines. The recall for category 2 is 0.95, meaning 95% of all category 2 wines were classified as such and precision for category 2 is 85%, meaning out of all wines classified as category 2, 85% were true category 2 wines. No wine was labelled category 3 and only 36% of category 1 wines were labelled as such. This is to say the model isn't great as most of the time we want accurate predictions on minority classes. At the same time, the dataset is tiny, so accurate predictions for minority classes will be out of reach for any algorithm. Maybe there's an amazing neural network that can do the job? With a little over 1000 observations I doubt it.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"balacc\"></a>\n## 5.2. Using balanced accuracy score","metadata":{}},{"cell_type":"markdown","source":"Now let's see if the model performs better if we tune the pipeline relying on the balanced accuracy metric. Balanced accuracy is essentially an average of recalls for all classes, so perhaps the pipeline will be optimised to perform better with minority classes.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # -- Instantiate scaler\n    scalers = trial.suggest_categorical(\"scalers\", ['minmax', 'standard', 'robust'])\n\n    if scalers == \"minmax\":\n        scaler = MinMaxScaler()\n    elif scalers == \"standard\":\n        scaler = StandardScaler()\n    else:\n        scaler = RobustScaler()\n        \n    # -- Tune estimator algorithm\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 30)\n    weights = trial.suggest_categorical(\"weights\", ['uniform', 'distance'])\n    metric = trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski'])\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n        \n    # -- Make a pipeline\n    pipeline = make_pipeline(scaler, knn)\n\n    # -- Cross-validate the features reduced by dimensionality reduction methods\n    kfold = StratifiedKFold(n_splits=10)\n    score = cross_val_score(pipeline, X_train, y_train, scoring='balanced_accuracy', cv=kfold)\n    score = score.mean()\n    return score\n\nsampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=300)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-10T15:13:25.86073Z","iopub.execute_input":"2021-09-10T15:13:25.861158Z","iopub.status.idle":"2021-09-10T15:14:11.687837Z","shell.execute_reply.started":"2021-09-10T15:13:25.861107Z","shell.execute_reply":"2021-09-10T15:14:11.686896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Have a look at the best trial\nprint(\"Best trial out of 300 is:\")\nstudy.best_trial","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:14:11.689079Z","iopub.execute_input":"2021-09-10T15:14:11.689371Z","iopub.status.idle":"2021-09-10T15:14:11.696915Z","shell.execute_reply.started":"2021-09-10T15:14:11.689343Z","shell.execute_reply":"2021-09-10T15:14:11.695987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:14:11.69867Z","iopub.execute_input":"2021-09-10T15:14:11.699107Z","iopub.status.idle":"2021-09-10T15:14:11.776117Z","shell.execute_reply.started":"2021-09-10T15:14:11.699064Z","shell.execute_reply":"2021-09-10T15:14:11.775104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:14:11.778874Z","iopub.execute_input":"2021-09-10T15:14:11.779181Z","iopub.status.idle":"2021-09-10T15:14:13.711329Z","shell.execute_reply.started":"2021-09-10T15:14:11.77915Z","shell.execute_reply":"2021-09-10T15:14:13.710661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"testbalacc\"></a>\n### 5.2.1. Evaluation on the test set","metadata":{}},{"cell_type":"code","source":"# -- Have a look at the best parameters for the tuned model\nprint(\"Best parameters after tuning using balanced accuracy:\")\nstudy.best_params","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:14:13.712672Z","iopub.execute_input":"2021-09-10T15:14:13.713073Z","iopub.status.idle":"2021-09-10T15:14:13.719178Z","shell.execute_reply.started":"2021-09-10T15:14:13.71304Z","shell.execute_reply":"2021-09-10T15:14:13.718425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Instantiate tuned model\nscaler = StandardScaler()\nbest_params = study.best_params # make a dictionary of best parameters\nbest_params.pop('scalers') # delete scaler option from the dictionary of params\nknn = KNeighborsClassifier(**best_params)\npipeline = make_pipeline(scaler, knn)\n\n# -- Get a dataframe for scores for each class\nbalanced_accuracy_trained_scores = get_scores(pipeline, \"balanced accuracy\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:14:13.720153Z","iopub.execute_input":"2021-09-10T15:14:13.72053Z","iopub.status.idle":"2021-09-10T15:14:13.884409Z","shell.execute_reply.started":"2021-09-10T15:14:13.720502Z","shell.execute_reply":"2021-09-10T15:14:13.883382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here it is, precision for class 2 is 87% and recall is 87% (compared to 95% with the model tuned using accuracy). And, we've found 23% of all class 3 wines and 33% of those labelled as category 3 wines were labelled correctly. Recall for category 1 wines is also better (from 36% to 50%). We can clearly see this model doesn't just label the overwhelming majority of wines as class 2 just because it's the most abundant category.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"over\"></a>\n## 5.3. With oversampling","metadata":{}},{"cell_type":"markdown","source":"Let's try another method to deal with imbalanced classes.\n\nWe will make a pipeline with three steps:\n1. Scale the data\n2. Oversample the data\n3. Run the KNN algorithm for classification.\n\nWe scale the data first and then oversample it since SMOTE uses KNN to generate new samples ([more details here](https://stats.stackexchange.com/questions/363312/normalization-standardization-should-one-do-this-before-oversampling-undersampl)).\n\nOversampling is what it sounds like - we will produce synthetic samples which will represent our minority classes (the very best and the very worst wines) and hopefully this will stop the modelfrom ignoring the minority classes. For more info on SMOTE go to [Machine Learning Mastery](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\ndef objective(trial):\n    # -- Instantiate oversampling\n    k_neighbors = trial.suggest_int(\"k_neighbors\", 1, 30)\n    over = SMOTE(k_neighbors=k_neighbors, random_state=42)\n    \n    # -- Instantiate scaler\n    scalers = trial.suggest_categorical(\"scalers\", ['minmax', 'standard', 'robust'])\n\n    if scalers == \"minmax\":\n        scaler = MinMaxScaler()\n    elif scalers == \"standard\":\n        scaler = StandardScaler()\n    else:\n        scaler = RobustScaler()\n        \n    # -- Tune estimator algorithm\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 30)\n    weights = trial.suggest_categorical(\"weights\", ['uniform', 'distance'])\n    metric = trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski'])\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n        \n    # -- Make a pipeline\n    steps = [('scaler', scaler), ('over', over), ('estimator', knn)]\n    pipeline = Pipeline(steps=steps)\n\n    # -- Cross-validate the features reduced by dimensionality reduction methods\n    kfold = StratifiedKFold(n_splits=10)\n    score = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=kfold)\n    score = score.mean()\n    return score\n\nsampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=300)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-10T15:14:13.885378Z","iopub.execute_input":"2021-09-10T15:14:13.885647Z","iopub.status.idle":"2021-09-10T15:15:23.303152Z","shell.execute_reply.started":"2021-09-10T15:14:13.885621Z","shell.execute_reply":"2021-09-10T15:15:23.30222Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Have a look at the best trial\nprint(\"Best trial out of 300 is:\")\nstudy.best_trial","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:15:23.30416Z","iopub.execute_input":"2021-09-10T15:15:23.304424Z","iopub.status.idle":"2021-09-10T15:15:23.311886Z","shell.execute_reply.started":"2021-09-10T15:15:23.304399Z","shell.execute_reply":"2021-09-10T15:15:23.310874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:15:23.313118Z","iopub.execute_input":"2021-09-10T15:15:23.313419Z","iopub.status.idle":"2021-09-10T15:15:23.395394Z","shell.execute_reply.started":"2021-09-10T15:15:23.313388Z","shell.execute_reply":"2021-09-10T15:15:23.394331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:15:23.396825Z","iopub.execute_input":"2021-09-10T15:15:23.397131Z","iopub.status.idle":"2021-09-10T15:15:26.628865Z","shell.execute_reply.started":"2021-09-10T15:15:23.397101Z","shell.execute_reply":"2021-09-10T15:15:26.628109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"testover\"></a>\n### 5.3.1. Evaluate on the test set","metadata":{}},{"cell_type":"code","source":"# -- Have a look at the best parameters for the tuned model\nprint(\"Best parameters after tuning using balanced accuracy and oversampling:\")\nstudy.best_params","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-10T15:15:26.629978Z","iopub.execute_input":"2021-09-10T15:15:26.630234Z","iopub.status.idle":"2021-09-10T15:15:26.637281Z","shell.execute_reply.started":"2021-09-10T15:15:26.630209Z","shell.execute_reply":"2021-09-10T15:15:26.63611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Instantiate the best tuned model\nscaler = StandardScaler()\n\nover = SMOTE(k_neighbors=1, random_state=42)\n\nbest_params = study.best_params # make a dictionary of best estimator parameters\nfor keys in ('scalers', 'k_neighbors'): # delete scaler and oversampling option from the dictionary of params\n    best_params.pop(keys, None)\n    \nknn = KNeighborsClassifier(**best_params)\n\nsteps = [('scaler', scaler), ('over', over), ('estimator', knn)]\npipeline = Pipeline(steps=steps)\n\n# -- Get a dataframe for scores for each class\noversample_trained_scores = get_scores(pipeline, \"with oversampling\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T15:15:26.638727Z","iopub.execute_input":"2021-09-10T15:15:26.639133Z","iopub.status.idle":"2021-09-10T15:15:26.904933Z","shell.execute_reply.started":"2021-09-10T15:15:26.639091Z","shell.execute_reply":"2021-09-10T15:15:26.90369Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"compare\"></a>\n# 6. Comparing the scores for all models","metadata":{}},{"cell_type":"markdown","source":"Now to compare all scores for all classes across our three models we can use the dataframes we made earlier.","metadata":{}},{"cell_type":"code","source":"# -- Concatenate all dataframes with scores for all tuned models\nall_scores_list = [mean_accuracy_trained_scores, balanced_accuracy_trained_scores, oversample_trained_scores]\nall_scores = pd.concat(all_scores_list)\n\nall_scores.head(12)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:19:30.017352Z","iopub.execute_input":"2021-09-10T16:19:30.018016Z","iopub.status.idle":"2021-09-10T16:19:30.037403Z","shell.execute_reply.started":"2021-09-10T16:19:30.017966Z","shell.execute_reply":"2021-09-10T16:19:30.03605Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Make a plot comparing all the models\ng = sns.catplot(data=all_scores, x=\"class\", y=\"score\", hue=\"metric\", col=\"tuning method\", ci=None, kind=\"bar\")\n\n(g.set_axis_labels(\"\", \"\")\n.set_xticklabels([\"Class 1\", \"Class 2\", \"Class 3\"])\n.set(ylim=(0, 1),  yticks=np.arange(0, 1.1, 0.1).tolist()))\n\ng.fig.suptitle('Comparison of models tuned with different methods', fontsize=18)\ng.fig.subplots_adjust(top=0.8)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:20:05.146868Z","iopub.execute_input":"2021-09-10T16:20:05.147228Z","iopub.status.idle":"2021-09-10T16:20:05.984499Z","shell.execute_reply.started":"2021-09-10T16:20:05.147199Z","shell.execute_reply":"2021-09-10T16:20:05.983274Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using mean accuracy metric results in the weakest model. Tuning the model using balanced accuracy vs tuning the model with the mean accuracy on an oversampled dataset results in predictions that are roughly the same.","metadata":{}},{"cell_type":"markdown","source":"**Summary:**\n* When working on imbalanced datasets we need to be careful which metric we use to make our evaluation since the model can ignore the minority class or classes and still result in high scores.\n* We can also use other techniques like oversampling to deal with imbalances.\n* Some hyperparameters are more influential than others and if time of training is important, we can decide to only tune the most important ones.","metadata":{}}]}