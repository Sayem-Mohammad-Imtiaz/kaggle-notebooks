{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# You need scikit-learn 0.24 to run this notebook, you can install it with the code below if your account has been verified\n# and you have the \"Internet\" toggle switched on in the right pannel of Kaggle\n\n# pip install scikit-learn==0.24","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_train.csv\")\ndf_test = pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_test.csv\")\nsample = pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's get some infos about this Dataset, i have a rapid function for that"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_infos(df_):\n\n    df_info = pd.DataFrame(columns={\"column\", \"NaN\", \"NaN %\"})\n\n    for index, value in df_.isna().sum().iteritems():\n        df_temp = pd.DataFrame({\"column\" : index, \"NaN\" : [value], \"NaN %\" : round(value*100 /len(df_), 2)})\n        df_info = pd.concat([df_info, df_temp], ignore_index=True)\n        df_info.sort_values(by=\"NaN\", ascending=False, inplace=True)\n\n    int_ = df_.select_dtypes(include=['int64']).columns.to_list()\n    float_ = df_.select_dtypes(include=['float64']).columns.to_list()\n    object_ = df_.select_dtypes(include=['object']).columns.to_list()\n\n    print(f\"Int64 : {', '.join(int_)}\")\n    print(f\"\\nFloat64 : {', '.join(float_)}\")\n    print(f\"\\nObject : {', '.join(object_)}\\n\") \n    \n    print(\"Total detected columns =\",len(int_) + len(float_) + len(object_))\n    print(\"\\nshape =\", df_.shape)\n    print(\"\\nshape without NaNs =\", df_.dropna().shape)\n\n    print(\"\\n\\n\", df_info)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_df_infos(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Too many NaNs, gotta clean that up\n### I will drop the NaNs from the columns with <3% of NaNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_cleaner(df_):\n    under_3 = []\n    for index, value in df_.isna().sum().iteritems():\n        if 0 <  value*100 /len(df_) < 3:\n            under_3.append(index)\n    df_.dropna(subset=under_3, axis=0, inplace=True)\n    return df_\n\ndf = df_cleaner(df)\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Dataset isn't that big, i can't afford to drop all the NaNs i'll end up with too little Data to play with, i'll fill the rest of the NaN with the mode value of each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_fill = ['gender','company_size','major_discipline','company_type','relevent_experience']\n\nfor col in columns_to_fill:\n    df[col].fillna(df[col].mode()[0], inplace=True)\n\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Very well, we no longer have any NaNs, let's move onto the next problem to deal with.\n### As stated in the description, the \"target\" is unbalanced, resampling could be the way to go about it\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"round(df[\"target\"].value_counts(normalize=True)*100, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\ndf_majority = df[df[\"target\"] == 0]\ndf_minority = df[df[\"target\"] == 1]\n \n\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     \n                                 n_samples=len(df_majority),    \n                                 random_state=123) \n \ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \ndf_upsampled[\"target\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We now have equally populated \"target\" classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 2, figsize=(17, 6))\nfig.suptitle('Before | After')\n\nsns.set_style(\"darkgrid\")\n\nsns.countplot(df['target'], ax=axs[0]).set_title(\"1. Original\")\nsns.countplot(df_upsampled['target'], ax=axs[1]).set_title(\"2. Upsampled\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's initiate a Pipeline with no specific parameters yet\n### ColumnTransformer will transform our data according to our needs (categorical, ordinal)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# !!! \"handle_unknown\" for OrdinalEncoder ONLY works with sklearn 0.24 and above\n\nX = df_upsampled.drop([\"target\", \"enrollee_id\"], axis=1)\ny = df_upsampled[\"target\"]\n\ncategorical_features = df_upsampled.select_dtypes(include=['object']).columns.to_list()\n\nordinal_features = [\"training_hours\", \"city_development_index\"]\n\npreprocessor = ColumnTransformer(\n               transformers=[\n               ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n               ('ord', OrdinalEncoder(handle_unknown='ignore'), ordinal_features),         \n               ],\n               remainder = \"drop\"\n               )\n\nclassifier_pipeline = Pipeline(\n                      steps=[\n                      ('preprocessor', preprocessor),\n                      ('SVD', TruncatedSVD()),\n                      ('classifier', ExtraTreesClassifier())             \n                      ]\n                      )\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)\n\nclassifier_pipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Let's list all available parameters for my pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(classifier_pipeline.get_params().keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are a lot of them! I'll choose a few, it takes a long time to execute a Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparameters = [                           \n              {\n                'SVD__n_components': range(5, 9),\n                'classifier__max_depth': range(25, 40, 2),\n                'classifier__min_samples_leaf' : range(3, 10),\n                'classifier__criterion' : [\"gini\", \"entropy\"]\n                }\n              ]\n\ngrid_search = GridSearchCV(classifier_pipeline, param_grid=parameters, scoring=\"accuracy\")\n\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters : \\n\\n{grid_search.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's apply those parameters to our pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_upsampled.drop([\"target\", \"enrollee_id\"], axis=1)\ny = df_upsampled[\"target\"]\n\ncategorical_features = df_upsampled.select_dtypes(include=['object']).columns.to_list()\n\nordinal_features = [\"training_hours\", \"city_development_index\"]\n\npreprocessor = ColumnTransformer(\n               transformers=[\n               ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n               ('ord', OrdinalEncoder(handle_unknown='ignore'), ordinal_features),         \n               ],\n               remainder = \"drop\"\n               )\n\nclassifier_pipeline = Pipeline(\n                      steps=[\n                      ('preprocessor', preprocessor),\n                      ('SVD', TruncatedSVD(n_components=5)),\n                      ('classifier', ExtraTreesClassifier(criterion='entropy', max_depth=37, min_samples_leaf=3))             \n                      ]\n                      )\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)\n\nclassifier_pipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train score = {round(classifier_pipeline.score(X_train, y_train), 4)}\")\nprint(f\"Test score = {round(classifier_pipeline.score(X_test, y_test), 4)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import plot_roc_curve\n\nsns.set(rc={'figure.figsize':(10, 5)})\nsns.set_style(\"darkgrid\")\n\nmetrics.plot_roc_curve(classifier_pipeline, X_test, y_test)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}