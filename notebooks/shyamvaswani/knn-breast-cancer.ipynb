{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# K-Nearest Neighbour\n\n## Objective: Using the breast cancer dataset from the sklearn library, predict whether a cancer case is malignant or benign","metadata":{}},{"cell_type":"code","source":"# importing the required libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n#Apply the default default seaborn theme, scaling, and color palette\nsns.set()\n# Sklearn related imports\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:03:25.58976Z","iopub.execute_input":"2021-06-11T07:03:25.590397Z","iopub.status.idle":"2021-06-11T07:03:26.761035Z","shell.execute_reply.started":"2021-06-11T07:03:25.590286Z","shell.execute_reply":"2021-06-11T07:03:26.760063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load data\n#We have 30 columns of data on which a decision of whether breast cancer cells are malignant or benign is decided.\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n#split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n#instantiate KNN classifier n_neighbours=3\n#KNN only has 1 parameter to adjust which is n_neighbours\nclf = KNeighborsClassifier(n_neighbors=3)\n#fit classifier\nclf.fit(X_train, y_train)\n#evaluation\nprint(\"Running the model with n_neighbours=3.......\")\nprint(\"Training set score for n_neighbours=3: {:.2f}\".format(clf.score(X_train, y_train)))\nprint(\"Test set accuracy for n_neighbours=3: {:.2f}\".format(clf.score(X_test, y_test)))\n\n#we see that our model is about 86% accurate for n_neighbours=3, meaning the model predicted the class\n#correctly for 86% of the samples in the test dataset.\n\n#view the optimum n_neighbours by plotting n_neighbours vs Accuracy scove and test score \ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n    #instantiate KNN classifier\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    #fit classifier\n    clf.fit(X_train, y_train)\n    #record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy or test accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\n    \n# basic chart plot\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nprint()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nprint(\"Best value for n_neighbours seems to peak at n_neighbours=6 where test accuracy is highest\")\nprint()\n\n#lets re-try the model with n_neighbours=6\n#instantiate KNN classifier\nclf = KNeighborsClassifier(n_neighbors=6)\n#fit classifier\nclf.fit(X_train, y_train)\n#evaluation\nprint(\"Re-running the model again with n_neighbours=6....\")\nprint(\"Training set score with n_neighbours=6: {:.2f}\".format(clf.score(X_train, y_train)))\nprint(\"Test set accuracy with n_neighbours=6: {:.2f}\".format(clf.score(X_test, y_test)))\n#we see that our model is about 94% accurate for n_neighbours=3, meaning the model predicted the class\n#correctly for 94% of the samples in the test dataset.\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:03:26.762706Z","iopub.execute_input":"2021-06-11T07:03:26.763016Z","iopub.status.idle":"2021-06-11T07:03:27.624541Z","shell.execute_reply.started":"2021-06-11T07:03:26.762987Z","shell.execute_reply":"2021-06-11T07:03:27.623641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nprint(\"Classification report for KNN Model n_neighbours=6:\")\nprint()\nprint(classification_report(y_test, clf.predict(X_test)))\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:03:27.626015Z","iopub.execute_input":"2021-06-11T07:03:27.626299Z","iopub.status.idle":"2021-06-11T07:03:27.661837Z","shell.execute_reply.started":"2021-06-11T07:03:27.626271Z","shell.execute_reply":"2021-06-11T07:03:27.660752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the confusion matrix using Seaborn library\nprint(\"Correlation Matrix for KNN Model n_neighbours=6:\")\nplt.figure(figsize=(5,5))\n_ = sns.heatmap(confusion_matrix(y_test, clf.predict(X_test)), \n                annot=True,fmt='', annot_kws={\"size\": 18},cmap=plt.cm.winter_r) \n_ = plt.ylabel('Actual', fontweight='bold')\n_ = plt.xlabel('Predicted', fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:03:27.667079Z","iopub.execute_input":"2021-06-11T07:03:27.669926Z","iopub.status.idle":"2021-06-11T07:03:27.992053Z","shell.execute_reply.started":"2021-06-11T07:03:27.669859Z","shell.execute_reply":"2021-06-11T07:03:27.99131Z"},"trusted":true},"execution_count":null,"outputs":[]}]}