{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#D63B52;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 20px;\n          color:white;\">\n        <b>Guide on how to handle Skewed Distribution</b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#4c69b9;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        The distribution of a data set is the shape of the graph when all possible values are plotted on a frequency graph (showing how often they occur). Today we will be learning how to handle skewed distribution of numerical data. By skewed data, I mean to say distributions which are not perfect bell shaped. The bell shaped ones are normal distributions. The ones which are not is skewed. Some are jagged as well. We will learn how to handle that today.\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"![DataDistribution.PNG](https://img.freepik.com/free-vector/data-inform-illustration-concept_114360-864.jpg?size=626&ext=jpg)","metadata":{}},{"cell_type":"code","source":"# importing necessary libraries \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nsns.set_theme()\nsns.set_palette(palette = \"rainbow\")\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:00.265207Z","iopub.execute_input":"2021-07-30T16:35:00.26563Z","iopub.status.idle":"2021-07-30T16:35:02.049749Z","shell.execute_reply.started":"2021-07-30T16:35:00.265542Z","shell.execute_reply":"2021-07-30T16:35:02.048762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/bostonhoustingmlnd/housing.csv\")\ndf.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-30T16:35:02.051112Z","iopub.execute_input":"2021-07-30T16:35:02.051375Z","iopub.status.idle":"2021-07-30T16:35:02.099086Z","shell.execute_reply.started":"2021-07-30T16:35:02.051349Z","shell.execute_reply":"2021-07-30T16:35:02.098014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n#### There are 3 ways to know if a data is distributed. \n* QQ Plot\n* Seaborn Histogram and Kdeplot\n* Pandas Skew Function\n***","metadata":{}},{"cell_type":"code","source":"# Skew function of Pandas\nold_skew = df.skew().sort_values(ascending=False)\nold_skew","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:02.103344Z","iopub.execute_input":"2021-07-30T16:35:02.103663Z","iopub.status.idle":"2021-07-30T16:35:02.1144Z","shell.execute_reply.started":"2021-07-30T16:35:02.103636Z","shell.execute_reply":"2021-07-30T16:35:02.113236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n* *Positive value means the distribution is skewed to the right.* \n* *Negative value means the distribution is skewed to the left.* \n* *0 means perfect normal distribution. The bell shaped curve.*\n***","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(17,13))\nfor i in list(enumerate(df.columns)):\n    plt.subplot(2, 4, i[0]+1)\n    sns.histplot(data = df[i[1]], kde=True)  # Histogram with KDE line\n    \nfor i in list(enumerate(df.columns)):\n    plt.subplot(2, 4,i[0]+5)\n    stats.probplot(df[i[1]], dist=\"norm\", plot=plt)   # QQ Plot\n    plt.title(\"\")\nplt.tight_layout()  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:02.115994Z","iopub.execute_input":"2021-07-30T16:35:02.116333Z","iopub.status.idle":"2021-07-30T16:35:04.696113Z","shell.execute_reply.started":"2021-07-30T16:35:02.116302Z","shell.execute_reply":"2021-07-30T16:35:04.69538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#4c69b9;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        From the above visualization we can understand that the column RM is more or less normally distributed, with a skew value of 0.08, which is very close to 0. LSTAT and MEDV is skewed to the right, PTRATIO is skewed to the left. We have to find appropriate ways to handle these skewed data and try to turn them to as normally distributed as possible. But we will only do this if your algorithm performs better in a normally distributed data. Some algorithms do not need a normal distribution and in those cases, we can ignore. We are not getting into the details of the data today and our main goal will be to handle the skewed data and turn it into normal distribution. \n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#D63B52;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 20px;\n          color:white;\">\n        <b>Mathematical Transformers</b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"***\n*We can handle skewed data using mathematical transformers. I will discuss some of the best transformers that sklearn library\nprovides us in order to handle skewed data. There are various types of Mathematical Transformers:*\n\n* **Function Transformers**\n - Log Transform\n - Reciprocal Transform\n - Square Transform\n - Sq Root Transform\n - Custom Transform\n* **Power Transformers**\n - Box-Cox\n - Yeo-Johnson\n* **Quantile Transformer**\n***","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#AF65C1;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>1 . Log Transform</b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#59729F;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n       In this transformation technique, we will use log of each and every value in the feature. So when we use logs of a certain distribution, we will use their exponents instead of the actual value. This makes the X scale exponentially grow in value and that is the reason why only <b>Right Skewed</b> data transforms to a more or less normal distribution. Mind you, only use it in cases where your algorithm will benefit from a normal distribution, for example, in cases of linear regression models. Also, only use it when your data is skewed to the right.\n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:04.69705Z","iopub.execute_input":"2021-07-30T16:35:04.697394Z","iopub.status.idle":"2021-07-30T16:35:04.769189Z","shell.execute_reply.started":"2021-07-30T16:35:04.697367Z","shell.execute_reply":"2021-07-30T16:35:04.768466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logTrans(feature):   # function to apply transformer and check the distribution with histogram and kdeplot\n    \n    logTr = ColumnTransformer(transformers=[(\"lg\", FunctionTransformer(np.log1p), [feature])])\n\n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    plt.title(\"Distribution before Transformation\", fontsize=15)\n    sns.histplot(df[feature], kde=True, color=\"red\")\n    plt.subplot(1,2,2)\n    \n    df_log = pd.DataFrame(logTr.fit_transform(df))\n    plt.title(\"Distribution after Transformation\", fontsize=15)\n    sns.histplot(df_log,bins=20, kde=True , legend=False)\n    plt.xlabel(feature)\n    plt.show()\n    \n    print(f\"Skewness was {round(old_skew[feature],2)} before & is {round(df_log.skew()[0],2)} after Log transformation.\")\n    \nlogTrans(feature=\"LSTAT\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:04.770269Z","iopub.execute_input":"2021-07-30T16:35:04.770726Z","iopub.status.idle":"2021-07-30T16:35:05.392953Z","shell.execute_reply.started":"2021-07-30T16:35:04.770689Z","shell.execute_reply":"2021-07-30T16:35:05.392127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#4c69b9;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        The skewness decreased from an extreme right skew to a moderately normal distribution and a negligible left skew. <br>\n        Closer the skew value to 0, more normal the distribution is.<br>\n        <br>\n        Now we will apply Log Transformation to a left skewed data and you will see how the distribution gets more skewed.\n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"logTrans(feature=\"PTRATIO\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:05.395007Z","iopub.execute_input":"2021-07-30T16:35:05.395495Z","iopub.status.idle":"2021-07-30T16:35:05.956758Z","shell.execute_reply.started":"2021-07-30T16:35:05.395446Z","shell.execute_reply":"2021-07-30T16:35:05.955812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#AC0000;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        Hence proved, never use Log transformation for left skewed data. It will mess your data up! \n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#AF65C1;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>2. Square Transform </b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#59729F;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n       In this transformation technique, we will use square of each and every value in the feature. This will only work on <b>Left Skewed</b> data. So, use Square Transform for left skewed data and Log Transform for right skewed data. Simple.\n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"def squareTrans(feature):   # function to apply transformer and check the distribution with histogram and kdeplot\n    \n    logTr = ColumnTransformer(transformers=[(\"lg\", FunctionTransformer(np.square), [feature])])\n\n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    plt.title(\"Distribution before Transformation\", fontsize=15)\n    sns.histplot(df[feature], kde=True, color=\"red\")\n    plt.subplot(1,2,2)\n    \n    df_square = pd.DataFrame(logTr.fit_transform(df))\n    plt.title(\"Distribution after Transformation\", fontsize=15)\n    sns.histplot(df_square,bins=20, kde=True , legend=False)\n    plt.xlabel(feature)\n    plt.show()\n\n    print(f\"Skewness was {round(old_skew[feature],2)} before & is {round(df_square.skew()[0],2)} after Square transformation.\")\n    \nsquareTrans(feature=\"RM\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:37:05.623829Z","iopub.execute_input":"2021-07-30T16:37:05.624177Z","iopub.status.idle":"2021-07-30T16:37:06.198304Z","shell.execute_reply.started":"2021-07-30T16:37:05.624146Z","shell.execute_reply":"2021-07-30T16:37:06.197251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#4c69b9;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        RM column was a little bit left skewed, however, the square transformation did not work and the skewness increased by a lot.<br>\n        <br>\n        Now we will apply Square Transformation to a right skewed data and you will see how the distribution gets more skewed.\n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"squareTrans(feature=\"LSTAT\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:06.561808Z","iopub.execute_input":"2021-07-30T16:35:06.562147Z","iopub.status.idle":"2021-07-30T16:35:07.318393Z","shell.execute_reply.started":"2021-07-30T16:35:06.562112Z","shell.execute_reply":"2021-07-30T16:35:07.317185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#AC0000;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        Hence proved, never use Square transformation for right skewed data. It will mess your data up! \n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#AF65C1;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>3. Reciprocal Transform, Squre Root and other Transformers </b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#59729F;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n       You should already have a clear picture of how we are using column transformer to apply a Function Transformer class of sklearn to the column of the dataframe and we can use any transformation we want. For example, we have tried log and square as of now. We can try, cube, square root, reciprocal or 1/x of the value as well. This is more like a trial and error and comes under fine tuning your feature engineering so that you can optimize your model. Try out as many things as you want! Comment your results. \n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#4c69b9;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        For a demonstration I will be using cube transformer in RM feature to show you how it works compared to square transformer. \n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"def cubeTrans(feature):   # function to apply transformer and check the distribution with histogram and kdeplot\n    \n    logTr = ColumnTransformer(transformers=[(\"lg\", FunctionTransformer(lambda x: x**3), [feature])])\n\n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    plt.title(\"Distribution before Transformation\", fontsize=15)\n    sns.histplot(df[feature], kde=True, color=\"red\")\n    plt.subplot(1,2,2)\n    \n    df_cube = pd.DataFrame(logTr.fit_transform(df))\n    plt.title(\"Distribution after Transformation\", fontsize=15)\n    sns.histplot(df_cube,bins=20, kde=True , legend=False)\n    plt.xlabel(feature)\n    plt.show()\n    \n    print(f\"Skewness was {round(old_skew[feature],2)} before & is {round(df_cube.skew()[0],2)} after cube transformation.\")\n    \ncubeTrans(feature=\"RM\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:07.32026Z","iopub.execute_input":"2021-07-30T16:35:07.320727Z","iopub.status.idle":"2021-07-30T16:35:07.895259Z","shell.execute_reply.started":"2021-07-30T16:35:07.320676Z","shell.execute_reply":"2021-07-30T16:35:07.894177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#AF65C1;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>4. Box-Cox </b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#59729F;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n       A Box Cox transformation is a transformation of a non-normal dependent variables into a normal shape. So the fun part is, box cox can be used in general for all type of distributions and it will by itself find the lambda value (check the formula below to know what lambda here is). <b>Box-Cox cannot be used for negative values and 0.</b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### Formula of Box-Cox Transformation:","metadata":{}},{"cell_type":"markdown","source":"![DataDistribution.PNG](https://miro.medium.com/max/880/1*_vXfmcxyVPu6c8qr4-nEcg.png)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:07.897035Z","iopub.execute_input":"2021-07-30T16:35:07.89754Z","iopub.status.idle":"2021-07-30T16:35:07.90256Z","shell.execute_reply.started":"2021-07-30T16:35:07.897484Z","shell.execute_reply":"2021-07-30T16:35:07.90154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### By default the PowerTransformer will standard scale the column so you need to specifically mention it if you do not want to scale.","metadata":{}},{"cell_type":"code","source":"def boxcoxtrans(feature):   # function to apply transformer and check the distribution with histogram and kdeplot\n    \n    boxcoxTr = PowerTransformer(method = \"box-cox\", standardize=True)\n\n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    plt.title(\"Distribution before Transformation\", fontsize=15)\n    sns.histplot(df[feature], kde=True, color=\"red\")\n    plt.subplot(1,2,2)\n    \n    df_boxcox = pd.DataFrame(boxcoxTr.fit_transform(df[feature].values.reshape(-1,1)))\n    plt.title(\"Distribution after Transformation\", fontsize=15)\n    sns.histplot(df_boxcox,bins=20, kde=True , legend=False)\n    plt.xlabel(feature)\n    plt.show()\n    \n    print(f\"Skewness was {round(old_skew[feature],2)} before & is {round(df_boxcox.skew()[0],2)} after Box-cox transformation.\")\n    \nboxcoxtrans(feature=\"RM\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:07.904219Z","iopub.execute_input":"2021-07-30T16:35:07.904642Z","iopub.status.idle":"2021-07-30T16:35:08.506025Z","shell.execute_reply.started":"2021-07-30T16:35:07.9046Z","shell.execute_reply":"2021-07-30T16:35:08.505264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#4c69b9;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        Did you see how it automatically found out the value of the exponent that will be best for this data? Amazing isn't it?<br>\n        <b>The next method will blow your mind!</b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#AF65C1;\n       font-size:150%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>5. Yeo-Johnson </b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#59729F;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n       This is Box-cox but better.The limitation on box-cox is it does not work with negative values and 0. Yeo-Johnson fixes that and can be used for any distribution, both positive and negative. So, if you do not understand anything about distribution and know that your algorithm will be benifiting from a normal distribution, then copy pase this code and you are fine. LoL! Never copy paste codes, understand it. Atleast a basic intuition is better than mugging it up. <br>\n        <b>In sklearn.preprocessing.PowerTransformer class, this method is the default, so you know it's good! Have fun!  </b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### Formula of Yeo-Johnson Transformation:","metadata":{}},{"cell_type":"markdown","source":"![DataDistribution.PNG](https://miro.medium.com/max/1004/1*2rslF39_ATSMOEEtG0zMmw.png)","metadata":{}},{"cell_type":"code","source":"def yeojohntrans(feature):   # function to apply transformer and check the distribution with histogram and kdeplot\n    \n    yeojohnTr = PowerTransformer(standardize=True)   # not using method attribute as yeo-johnson is the default\n\n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    plt.title(\"Distribution before Transformation\", fontsize=15)\n    sns.histplot(df[feature], kde=True, color=\"red\")\n    plt.subplot(1,2,2)\n    \n    df_yeojohn = pd.DataFrame(yeojohnTr.fit_transform(df[feature].values.reshape(-1,1)))\n    plt.title(\"Distribution after Transformation\", fontsize=15)\n    sns.histplot(df_yeojohn,bins=20, kde=True , legend=False)\n    plt.xlabel(feature)\n    plt.show()\n    \n    print(f\"Skewness was {round(old_skew[feature],2)} before & is {round(df_yeojohn.skew()[0],2)} after Yeo-johnson transformation.\")\n    \nyeojohntrans(feature=\"RM\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:08.507252Z","iopub.execute_input":"2021-07-30T16:35:08.507535Z","iopub.status.idle":"2021-07-30T16:35:09.105021Z","shell.execute_reply.started":"2021-07-30T16:35:08.507508Z","shell.execute_reply":"2021-07-30T16:35:09.103714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#4c69b9;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>By far the best result for the feature RM! Let's check it out on other features as well.</b> <br>\n        Let's check the same Yeo Johnson transformation in other features. \n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"for i in df.columns[1:]:\n    yeojohntrans(i)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:09.106353Z","iopub.execute_input":"2021-07-30T16:35:09.106667Z","iopub.status.idle":"2021-07-30T16:35:10.789831Z","shell.execute_reply.started":"2021-07-30T16:35:09.106635Z","shell.execute_reply":"2021-07-30T16:35:10.788556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#95E2C1;\n       font-size:110%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:black;\">\n        <b>Thank you for reading my notebook. If you have anyyyy suggestions, I will be glad. Please comment your thoughts below. Thank you again for sticking till the end. Take care! </b>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#DE64B9;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 20px;\n          color:white;\">\n        <b>Please upvote if you like the notebook!</b>\n    </p>\n</div>","metadata":{}}]}