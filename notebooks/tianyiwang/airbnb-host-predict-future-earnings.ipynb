{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Become a data driven Airbnb host 2\n\nThis is a blog continued from the first one: https://www.kaggle.com/tianyiwang/become-a-data-driven-airbnb-host-part-1. In this blog, we try to solve a business problem through machine learning --- predicting future earnings based on the current listing information. Imagine that you are an Airbnb host and you would like to know the expected earning for one of your listings next year. Once you have the model, you can play with the information of your listing to see how you might change your future earning if there's any variations of your current listing. "},{"metadata":{},"cell_type":"markdown","source":"![](https://kylekleinphotography.com/wp-content/uploads/KKP12002-1.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# 1. Load data and packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport datetime\nfrom datetime import date\nimport sklearn\nfrom sklearn.preprocessing import Imputer\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\npd.set_option(\"display.max_columns\", 100)\npd.set_option('max_colwidth',200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv(\"../input/boston/calendar.csv\")\nlistings = pd.read_csv(\"../input/boston/listings.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Investigate rows with high missing rates\n\nFirst, let's check if there are special groups of listings with a lot of N/A columns ---- maybe they are very new or bad listings which we want to leave out when building our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# How much data is missing in each row of the dataset?\nmissing_values_in_rows = listings.isna().mean(axis=1).values\nplt.rcParams['figure.figsize'] = (5, 3)\nax = plt.hist(missing_values_in_rows)\nplt.title('Missing rates (rows)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A bad senario will be that there's a group of rows with significantly higher missing rates. In our case, it's good to see that most of the listings have at least 70% of the fields flled, and the shape of the distribution is kind of ideal."},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature engineering\n\nWe then create features from the existing columns. Once this is done, we will add some columns in the raw data that we can directly use without much engineering. The features we generated:\n1. downtown_bos, east_bos: whether the listing is in one of the two neighborhoods. From the exploration, many of the listings are from these areas, maybe they are convenient/go-to areas in Boston.\n2. host_age: generated from host_since (months)\n3. reponse_rate, acceptance_rate: convert to numbers\n4. one_or_two_listings, three_to_ten_listings, more_than_ten_listings: how many listings the host has\n5. response_time_ordinalï¼š generated from response_time\n6. real_bed: if the bed is a real bed\n7. neighborhoods_one_hot: one-hot coding to indicate if the listing is in one of 10 most popular neighborhoods\n8. amenities_len: length of the amenities field to roughly indicate how many amenities the listing offers\n9. cancellation_policy_ordinal: the larger the number the more flexible the cancellation policy is\n10. price_per_bed_compared_to_nbh: the difference between the price per bed for that listing and the average price per bed for that neighborhood (**this is a slightly complicated feature to build! We will do it in the end**)"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Generated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower(s):\n    try:\n        return s.lower()\n    except:\n        return s\n\ndef perc_to_numbers(p):\n    '''\n    \"30%\" --> 0.3\n    '''\n    try:\n        return float(p.split(\"%\")[0])/100\n    except:\n        return float(\"nan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = listings['summary'].apply(lower)\ndowntown_bos = summary.str.contains(\"downtown boston\").astype(\"float32\")\neast_bos = summary.str.contains(\"east boston\").astype(\"float32\")\nhost_age = listings['host_since'].apply(lambda x: (datetime.datetime.today() - pd.to_datetime(x)).days/30)\nreponse_rate = listings['host_response_rate'].apply(perc_to_numbers)\nacceptance_rate = listings['host_acceptance_rate'].apply(perc_to_numbers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A small investigation on the numbers of listings:"},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[['host_total_listings_count']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{:.2%} of the hosts have no more than 2 listings, {:.2%} of the hosts have more than 10 listings\"\\\n.format(sum(listings['host_total_listings_count']<=2)/len(listings), sum(listings['host_total_listings_count']>10)/len(listings)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hosts with one or two listings might just have two rooms for rent in one apartment/house. We will group them together. 40% of the hosts have multiple properties and some of them even have hundred of listings. We thus create the groups:\n- hosts with 1~2 listings\n- hosts with 3~10 listings\n- hosts with more than 10 listings"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_or_two_listings = listings['host_total_listings_count'] <= 2 \nthree_to_ten_listings = (listings['host_total_listings_count'] > 2) & (listings['host_total_listings_count'] < 10)\nmore_than_ten_listings = listings['host_total_listings_count'] > 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"response_time_dict = {\n    'a few days or more':1,\n    'within a day': 2,\n    'within a few hours': 3,\n    'within an hour': 4\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some records have `N/A` response time. Are they very new listings?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(host_age[listings['host_response_time'].isna()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of host_age for these listings is consistent with the distribution for all the listings. So we fill the nan values with `\"within a few hours\"` (the 2nd popular value for this column; We didn't choose to fill with `within an hour` to avoid overestimating the field. We believe that most hosts can reply within a few hours)."},{"metadata":{"trusted":true},"cell_type":"code","source":"response_time_ordinal = listings['host_response_time']\\\n                        .fillna(\"within a few hours\").apply(lambda x: response_time_dict[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_bed = listings['bed_type'] == \"Real Bed\"\npopular_neighborhoods = list(listings\\\n                        .groupby('neighbourhood_cleansed')\\\n                        .count()['id'].sort_values(ascending=False)[:10].index)\nneighborhoods_one_hot = pd.get_dummies(listings[['neighbourhood_cleansed']])\namenities_len = listings['amenities'].apply(len)\ncancellation_policy_dict = {\n    'flexible': 1,\n    'moderate': 2,\n    'strict': 3,\n    'super_strict_30': 4\n}\ncancellation_policy_ordinal = listings['cancellation_policy']\\\n                              .fillna(\"super_strict_30\").apply(lambda x: cancellation_policy_dict[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = pd.DataFrame({\n    'id': listings['id'],\n    'downtown_bos': downtown_bos,\n    'east_bos': east_bos,\n    'host_age': host_age,\n    'reponse_rate': reponse_rate,\n    'acceptance_rate': acceptance_rate,\n    'one_or_two_listings': one_or_two_listings,\n    'three_to_ten_listings': three_to_ten_listings,\n    'more_than_ten_listings': more_than_ten_listings,\n    'response_time_ordinal': response_time_ordinal,\n    'real_bed': real_bed,\n    'amenities_len': amenities_len,\n    'cancellation_policy_ordinal': cancellation_policy_ordinal\n}).join(neighborhoods_one_hot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Binary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_binary(x):\n    try:\n        return 1 if x == \"t\" else 0\n    except:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_pd = listings[['host_is_superhost',\n              'host_has_profile_pic',\n              'host_identity_verified',\n              'is_location_exact',\n              'requires_license',\n              'instant_bookable',\n              'require_guest_profile_picture',\n              'require_guest_phone_verification']].applymap(convert_binary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = final_df.join(binary_pd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Convert money value strings to numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_money(s):\n    '''\n    \"$250.00\" --> 250\n    '''\n    try:\n        return float(eval(s.split(\"$\")[1]))\n    except:\n        return float(\"nan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"money_pd = listings[['price','extra_people']].applymap(convert_money)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = final_df.join(money_pd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Add `price_per_bed_compared_to_nbh`\n-- The difference between the price per bed for that listing and the average price per bed for that neighborhood"},{"metadata":{"trusted":true},"cell_type":"code","source":"listings['price_num'] = final_df['price']\nlistings['price_per_bed'] = listings['price_num'] / listings['beds'].replace(0,1)\navg_price_per_bed = listings.groupby('host_neighbourhood')['price_per_bed'].mean().reset_index()\\\n                              .rename({'price_per_bed':'price_per_bed_nbh'}, axis=1)\nlistings = listings.merge(avg_price_per_bed, on=\"host_neighbourhood\")\nlistings['price_per_bed_compared_to_nbh'] = listings['price_per_bed'] - listings['price_per_bed_nbh']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = plt.hist(listings['price_per_bed_compared_to_nbh'])\nplt.title('Difference between price per bed v.s. neighborhood average')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Add other numeric columns and id column (`id`)"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = final_df.join(listings[[\n                                   'accommodates',\n                                   'bathrooms',\n                                   'bedrooms',\n                                   'beds',\n                                   'number_of_reviews',\n                                   'review_scores_rating',\n                                   'review_scores_accuracy',\n                                   'review_scores_cleanliness',\n                                   'review_scores_checkin',\n                                   'review_scores_communication',\n                                   'review_scores_location',\n                                   'review_scores_value',\n                                   'price_per_bed_compared_to_nbh'\n                                   ]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.6 Deal with missing values"},{"metadata":{},"cell_type":"markdown","source":"Columns with missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = final_df.isna().sum()\nm[m>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6.1 Columns that we want to set the NaN values as 0s"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df[['downtown_bos',\n            'east_bos',\n            'price_per_bed_compared_to_nbh',\n            'number_of_reviews']] = final_df[['downtown_bos',\n                                                            'east_bos',\n                                                            'price_per_bed_compared_to_nbh',\n                                                            'number_of_reviews']].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6.2 Columns that need be filled with the means"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df_sub1 = final_df[['reponse_rate',\n                  'review_scores_rating',\n                  'review_scores_accuracy',\n                  'review_scores_cleanliness',\n                  'review_scores_checkin',\n                  'review_scores_communication',\n                  'review_scores_location',\n                  'review_scores_value']].copy()\n\nfinal_df[['reponse_rate',\n          'review_scores_rating',\n          'review_scores_accuracy',\n          'review_scores_cleanliness',\n          'review_scores_checkin',\n          'review_scores_communication',\n          'review_scores_location',\n          'review_scores_value']] = pd.DataFrame(Imputer(missing_values=float('nan'), \n                                                         strategy=\"mean\", \n                                                         axis=0)\\\n                                                         .fit_transform(final_df_sub1),\n                                                 columns = final_df_sub1.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6.3 Columns that need be filled with the most common values"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df_sub2 = final_df[['bathrooms','bedrooms','beds','acceptance_rate']].copy()\n\nfinal_df[['bathrooms','bedrooms','beds','acceptance_rate']] = pd.DataFrame(Imputer(missing_values=float('nan'), \n                                                         strategy=\"most_frequent\", \n                                                         axis=0)\\\n                                                         .fit_transform(final_df_sub2),\n                                                         columns = final_df_sub2.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6.4 Other missing values filling"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df['accommodates'] = final_df['accommodates'].fillna(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the table again: "},{"metadata":{"trusted":true},"cell_type":"code","source":"m = final_df.isna().sum()\nm[m>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`price` is a very importance piece of information and we don't want to impute this value here. We will just drop the listings with `NaN` price."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = final_df[final_df['price'] > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.7 Make sure all the columns except the id column are numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.iloc[:,1:] = final_df.iloc[:,1:].astype(\"float32\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Calculate future earnings"},{"metadata":{},"cell_type":"markdown","source":"We first check that all listings were scraped on the same day:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(listings['calendar_last_scraped'].min())\nprint(listings['calendar_last_scraped'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another thing that we are concerning is that the availability data are probably much lower than what they actually will be, especially for the days that are in far future."},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar['available'] = calendar['available'].apply(convert_binary)\ncalendar['year_month'] = calendar['date'].apply(lambda x: x[:7])\ncalendar.groupby('year_month')['available'].mean().reset_index().plot()\nplt.title('Vacancy rates with months')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data was scraped on September 2016, and on that month about 27% of the listings are available, while in the next month 41% of rooms are available. However, for the next year until 2017 December, each month about half of the rooms are booked already. We believe that the future earnings we calculate out of the data are meaningful. Or if we have more historical data, we might find out that usually when the month actually comes, the booking rate will rise by 20% (an example). In that way we can adjust our predicted results to approximate the real values."},{"metadata":{},"cell_type":"markdown","source":"According to my understanding, the `calendar` data shows the availablity of the listings for the following one year. When the room is not available, we won't have the price information. We know that the price for a listing is very seasonal and usually the price will increase a lot during busy seasons. Since we don't know the prices for the listings that were already booked, we will just use the price in the `listing` data to approximate that."},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = calendar.merge(listings[['id','price']].rename({'id':'listing_id'}, axis=1), on='listing_id')\ncalendar['price_x'] = calendar['price_x'].apply(convert_money)\ncalendar['price_y'] = calendar['price_y'].apply(convert_money)\ncalendar_available = calendar[calendar['available']==1]\ncalendar_booked = calendar[calendar['available']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember that in `calendar` data, if a room is available on a certain date, we can see its price on that day. We also have the price information in `listing` data. Thus we would like to compare the available prices in `calendar` data and those in `listing` data to see if the theoratical prices are very different from the prices in reality."},{"metadata":{"trusted":true},"cell_type":"code","source":"a = plt.hist(calendar_available['price_x'] - calendar_available['price_y'], bins=100)\nplt.title(\"DIfference between prices in listings and actual prices\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The differences are mostly very small."},{"metadata":{"trusted":true},"cell_type":"code","source":"earnings = calendar_booked.groupby('listing_id')['price_y'].sum().reset_index().rename({'price_y':'future_earnings'}, axis=1)\na = plt.hist(earnings['future_earnings'])\nplt.title(\"Future Earnings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many records have future earnings as 0?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(earnings['future_earnings'] == 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's not a lot. The earnings data are very right skewed. We will do a log transformation on it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"earnings['log_future_earnings'] = np.log(earnings['future_earnings']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def y_to_earnings(y):\n    '''\n    The function to convert the log earnings\n    '''\n    return np.exp(y)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = plt.hist(earnings['log_future_earnings'])\nplt.title(\"Log Future Earnings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Align the features table   "},{"metadata":{"trusted":true},"cell_type":"code","source":"features = final_df.copy()\nid_earnings = features[['id']].merge(earnings.rename({'listing_id':'id'}, axis=1).drop('future_earnings', axis=1), on=\"id\", how=\"left\").fillna(0)\nprint(\"{:.2%} of the listings don't have future earnings\".format(sum(id_earnings['log_future_earnings']==0)/len(id_earnings)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Split the dataset to testing and training sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all = features.drop('id', axis=1)\ny_all = id_earnings['log_future_earnings']\nX_train, X_test, y_train, y_test = train_test_split(X_all,\n                                                                  y_all,\n                                                                  test_size = 0.2,\n                                                                  random_state = 0)\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Build the gradient boosting regressor "},{"metadata":{"trusted":true},"cell_type":"code","source":"def regressor(X, y, params, random_state=1):\n    '''\n    This function was borrowed from. https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py\n    It takes the X, y and parameters of the model and plots how deviance change as we have more iterations and the important variables\n    '''\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n    print(\"Training set has {} samples.\".format(X_train.shape[0]))\n    print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n    \n    # Fit regression model\n    clf = GradientBoostingRegressor(**params)\n    clf.fit(X_train, y_train)\n    mse = mean_squared_error(y_test, clf.predict(X_test))\n    print(\"MSE: %.4f\" % mse)\n    \n    # Plot training deviance\n    test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n    for i, y_pred in enumerate(clf.staged_predict(X_test)):\n        test_score[i] = clf.loss_(y_test, y_pred)\n\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    plt.title('Deviance')\n    plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n             label='Training Set Deviance')\n    plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n             label='Test Set Deviance')\n    plt.legend(loc='upper right')\n    plt.xlabel('Boosting Iterations')\n    plt.ylabel('Deviance')\n    \n    # Plot feature importance\n    feature_importance = clf.feature_importances_\n    \n    # make importances relative to max importance\n    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    im = pd.DataFrame({'feature': features.columns[1:], 'relative importance': feature_importance}).sort_values('relative importance')[-20:]\n    plt.subplot(1, 2, 2)\n    plt.barh(im['feature'], im['relative importance'])\n    plt.xlabel('Relative Importance')\n    plt.title('Variable Importance')\n    plt.show()\n    \n    plt.subplots_adjust(wspace=30)\n    return clf, im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the first trial\nparams = {\n    'n_estimators': 200,\n    'max_depth': 6,\n    'min_samples_split': 2,\n    'learning_rate': 0.01,\n    'loss': 'ls',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model, im = regressor(X_train, y_train, params, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our first model has a MSE of 5 --- which is about 8 dollars. The most important variable is `is_location_exact` --- we don't quite know what it means. The most important 9 features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.exp(5**0.5)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im.sort_values('relative importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20,20)\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    sns.regplot(x=X_train[im.sort_values('relative importance', ascending=False)['feature'].values[i]], y=y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course the future earnings are not determined by only one feature. From the scatter plot we can have an idea of whether a variable will positively or negatively affect the future earning.\n\nWe also see that 11% of the listings don't have future earnings. Are they very new listings? Are they of a certain kind?"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_earnings = id_earnings[id_earnings['log_future_earnings']==0].merge(features, on='id')\nhave_earnings = id_earnings[id_earnings['log_future_earnings']>0].merge(features, on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20,10)\n\nplt.subplot(2,4,1)\na = plt.hist(no_earnings['host_age'])\nplt.title(\"host_age_no_earnings\")\nplt.subplot(2,4,2)\na = plt.hist(no_earnings['is_location_exact'])\nplt.title(\"is_location_exact_no_earnings\")\nplt.subplot(2,4,3)\na = plt.hist(no_earnings['one_or_two_listings'])\nplt.title(\"one_or_two_listings_no_earnings\")\nplt.subplot(2,4,4)\na = plt.hist(no_earnings['price'])\nplt.title(\"price_no_earnings\")\n\nplt.subplot(2,4,5)\na = plt.hist(have_earnings['host_age'])\nplt.title(\"host_age_have_earnings\")\nplt.subplot(2,4,6)\na = plt.hist(have_earnings['is_location_exact'])\nplt.title(\"is_location_exact_have_earnings\")\nplt.subplot(2,4,7)\na = plt.hist(have_earnings['one_or_two_listings'])\nplt.title(\"one_or_two_listings_have_earnings\")\nplt.subplot(2,4,8)\na = plt.hist(have_earnings['price'])\nplt.title(\"price_have_earnings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that listings with no earnings are mostly the listings with `is_location_exact` of 0. Are the no earning listings with `True` is_location_exact very new listings?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (6,4)\n\na = plt.hist(no_earnings[no_earnings['is_location_exact']==1]['host_age'])\nplt.title('host_age of no earning listings with True is_location_exact')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Didn't see that pattern here."},{"metadata":{},"cell_type":"markdown","source":"# 8. Tune the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"rg = GradientBoostingRegressor()\nparameters = {'learning_rate': [0.01, 0.05, 0.08, 0.1],\n                    'max_depth': [4, 6, 8, 10],\n                    'max_features': [None, 50, 40, 30, 20, 15, 10]}\nscorer = \"neg_mean_squared_error\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_obj = GridSearchCV(rg, parameters, scoring=scorer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_fit = grid_obj.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the best model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = grid_fit.best_estimator_\nbest_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_log_earnings = best_model.predict(X_test)\nprint(\"MSE of the tuned model: {:.3}\".format(np.mean(np.square(predicted_log_earnings - y_test))))\ndifference = abs(y_to_earnings(predicted_log_earnings) - y_to_earnings(y_test))\nprint(\"{:.2%} have less than $1000 absolute error\".format(sum(difference<1000)/len(y_test)))\nprint(\"{:.2%} have less than $5000 absolute error\".format(sum(difference<5000)/len(y_test)))\nprint(\"{:.2%} have less than $10000 absolute error\".format(sum(difference<10000)/len(y_test)))\nprint(\"{:.2%} have less than $20000 absolute error\".format(sum(difference<20000)/len(y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20,5)\na = plt.hist(difference, bins=1000)\nplt.title('Absolute difference')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20,5)\n\nplt.subplot(1,2,1)\nplt.hist(y_to_earnings(y_test)[difference<1000], bins=100)\nplt.title('Earnings of listings with less than $1000 predicting error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,3)\n\nplt.subplot(1,2,1)\na = plt.hist(y_to_earnings(predicted_log_earnings))\nplt.title(\"predicted earnings\")\n\nplt.subplot(1,2,2)\na = plt.hist(y_to_earnings(y_test))\nplt.title(\"actual earnings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When tested on the test set, our model predicted half of the listings with less than $5000 absolute difference. From the histograms of the predicted earnings and actual earnings, we can see that our model is more conservative and produce smaller numbers for the listings that might have very high earnings."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 8)\nx = np.linspace(0,300000)\nsns.regplot(x=y_to_earnings(y_test), y=y_to_earnings(predicted_log_earnings))\nplt.plot(x, x, linewidth=2)\nplt.xlabel(\"actual earnings\")\nplt.ylabel(\"difference\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. What if we leave out listings with no future earnings when training the model? \n\n11% of the listings don't have future earnings --- they are not booked at all for the next year at the time when the data was scraped. Most of them have `Is_location_exact` as 0. They might be outliers. Let's train the model again only with the listings that have future earnings."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_train_test_data(X_all, y_all, test_size=0.2):\n    '''\n    Split the data into traning set and testing set\n    '''\n    X_train, X_test, y_train, y_test = train_test_split(X_all,\n                                                                      y_all,\n                                                                      test_size = 0.2,\n                                                                      random_state = 0)\n    print(\"Training set has {} samples.\".format(X_train.shape[0]))\n    print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n    return X_train, X_test, y_train, y_test\n\ndef tune_train_model(rg, parameters, scorer, X_train, y_train):\n    '''\n    Grid search on parameters to find the best model\n    '''\n    grid_obj = GridSearchCV(rg, parameters, scoring=scorer)\n    grid_fit = grid_obj.fit(X_train, y_train)\n    best_model = grid_fit.best_estimator_\n    return best_model\n\ndef evaluate_model(best_model, X_test, y_test):\n    '''\n    Fit the model on testing set; calculate the MSE; calculate percentages of the records with less than X amount of absolute error\n    '''\n    predicted_log_earnings = best_model.predict(X_test)\n    print(\"MSE of the tuned model: {:.3}\".format(np.mean(np.square(predicted_log_earnings - y_test))))\n    difference = abs(y_to_earnings(predicted_log_earnings) - y_to_earnings(y_test))\n    print(\"{:.2%} have less than $1000 absolute error\".format(sum(difference<1000)/len(y_test)))\n    print(\"{:.2%} have less than $5000 absolute error\".format(sum(difference<5000)/len(y_test)))\n    print(\"{:.2%} have less than $10000 absolute error\".format(sum(difference<10000)/len(y_test)))\n    print(\"{:.2%} have less than $20000 absolute error\".format(sum(difference<20000)/len(y_test)))\n    return predicted_log_earnings, difference","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters1 = {'learning_rate': [0.01, 0.05, 0.08, 0.1],\n                    'max_depth': [4, 6, 8, 10],\n                    'max_features': [None, 50]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"have_future_earnings_index = id_earnings['log_future_earnings'] > 0\nX_train_new, X_test_new, y_train_new, y_test_new = prepare_train_test_data(X_all[have_future_earnings_index.values], y_all[have_future_earnings_index.values])\nbest_model = tune_train_model(rg, parameters, scorer, X_train_new, y_train_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_log_earnings, difference = evaluate_model(best_model, X_test_new, y_test_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MSE is much smaller and apparently, this new model performs much better."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20,5)\na = plt.hist(difference, bins=1000)\nplt.title('Absolute difference')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 8)\nx = np.linspace(0,y_to_earnings(max(y_test_new)))\nsns.regplot(x=y_to_earnings(y_test_new), y=y_to_earnings(predicted_log_earnings))\nplt.plot(x, x, linewidth=2)\nplt.xlabel(\"actual earnings\")\nplt.ylabel(\"predicted earnings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are quite a few listings that actually don't have any future earnings but were predicted to have earnings by the model. We are then curious --- what if we only train and test the model on the listings where `Is_location_exact` is 1 and have future earnings."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_index = (id_earnings['log_future_earnings'] > 0) & (X_all.reset_index().drop(\"index\", axis=1)['is_location_exact'] == 1)\nX_train_new, X_test_new, y_train_new, y_test_new = prepare_train_test_data(X_all.reset_index().drop(\"index\", axis=1)[new_index.values], y_all[new_index.values])\nbest_model = tune_train_model(rg, parameters, scorer, X_train_new, y_train_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_log_earnings, difference = evaluate_model(best_model, X_test_new, y_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8, 8)\nx = np.linspace(0,y_to_earnings(max(y_test_new)))\nsns.regplot(x=y_to_earnings(y_test_new), y=y_to_earnings(predicted_log_earnings))\nplt.plot(x, x, linewidth=2)\nplt.xlabel(\"actual earnings\")\nplt.ylabel(\"predicted earnings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model performs slightly better."},{"metadata":{},"cell_type":"markdown","source":"# 10. Interesting take-aways "},{"metadata":{},"cell_type":"markdown","source":"From the model we can see that it's important to have `Is_location_exact` as 1 (although we are not quite sure what it means). Price and `Price_per_bed_compared_to_neighborhood` have positive effect on the future earnings but I think it's mostly because listings with higher price have better amenities, locations, etc. So don't worry if your listing is more expensive than most of the other listings in the neighbordhood. If your room is truly in better condition and provides more exciting stuffs for the guests, you will have better future earnings. The length of the amenity field also has a positive effect. Other important factors are acceptance rate and response rate."},{"metadata":{},"cell_type":"markdown","source":"To improve the model, I think it will help to get more enrichment on the address data. For example, we can get the house price data using the addresses. We can also have a look how many hotels there are in the neighborhood of the listing. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}