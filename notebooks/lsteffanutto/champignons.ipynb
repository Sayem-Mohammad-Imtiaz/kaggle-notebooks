{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset loading and visualisacion\n1st column = etiquetas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/mushroom-classification/mushrooms.csv\")\n# print(df.head())\nprint(\"Shape: \", df.shape)\n# print(df.columns)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conversion lettras to numeros","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn import preprocessing\n\nlabelencoder=preprocessing.LabelEncoder()\nfor column in df.columns:\n    df[column] = labelencoder.fit_transform(df[column])\n    \nprint(df.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separacion del dataset: \n\nX = attributes/features = data sin la primera columna que contiene las etiquetas = [8124 rows x 22 columns]\n\nY = label/etiquetas = solo una columna con las etiquetas = 8124x1\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = df[['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n       'stalk-surface-below-ring', 'stalk-color-above-ring',\n       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n       'ring-type', 'spore-print-color', 'population', 'habitat']]\nY = df[\"class\"]\n\n# X_dummy = pd.get_dummies(X) #convert letters in numerical values\n# X_dummy.shape \n    \n# print(\"all data features:\",X.shape,\"\\n\",X)\n# print(\"column con etiquetas:\",Y.shape,\"\\n\",Y)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random split del dataset para entrenar un classificador: 2/3 para entrenar, 1/3 para probar\n\nx_train = [5443 rows x 22 columns] ; x_test = [2681 rows x 22 columns]\n\ny_train = 5443 x 1 ; y_test = 2681","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n\nprint(\"x_train:\",x_train.shape,\"\\n\",x_train)\nprint(\"x_test:\",x_test.shape,\"\\n\",x_test)\nprint(\"y_train:\",y_train.shape,\"\\n\",y_train)\nprint(\"y_test:\",y_test.shape,\"\\n\",y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RandomForestClassifier como en el pdf de la clase, y con los parametros sigientes:\n\nn_estimators = 100 = 100 arboles in the forest, the result is an average of thoses 100 estimators\n\nmax_samples = (0.66)*nb_fila_del_dataset , filas tomadas de manera random para entrenar un estimator (random_state=42)\n\nmax_features = 'sqrt' = same as 'auto' as default (quedamos sqrt(nb_attributes) en cada sub_tablas, como en el pdf de la clase)\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier=RandomForestClassifier(n_estimators= 100,max_features = 'sqrt',max_samples = 0.66 , bootstrap=True, random_state=42)\nclassifier.fit(x_train, y_train)\n\ny_pred=classifier.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con el RandomForestClassifier:\n\nSin \"random_state=42\", \n\nAccuracy = 0.998 for n_estimators= 1;\nAccuracy = 0.999 for n_estimators= 2;\ny siempre Accuracy = 1 for more estimators\n\nCon \"random_state=42\", Accuracy = 1 siempre\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check precision, recall, F1 etc.\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(\"RESULTATS RANDOM FOREST CLASSIFIER\\n \")\n\nprint(\"Accuracy RandomForest:\",accuracy_score(y_test, y_pred) )\nprint(\"\\nmatrix confusion:\\n \",confusion_matrix(y_test,y_pred) )\nprint(\"\\nrecall y F1:\\n\", classification_report(y_test,y_pred) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Red neuronales: \n\n22 Inputs (los 22 champignones attributes)\n\n3 capas intermedias (ocultas) : las dos primeras con 100 neuronas cada una y la ultima con 25 neuronas : hidden_layer_sizes=(100,100,25)\n\nHay una sola neuronna de output para classificar si el champignon es comestible o no (0 o 1)\n\ntypo de activacion = default = 'relu'\n\nciclos de entrenamientos = 1000 iteraciones, this determines the number of epochs (how many times each data point will be used)\n\n‘sgd’ refers to stochastic gradient descent\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import mean_squared_error\n\nclassifier_red= MLPClassifier(hidden_layer_sizes=(100,100,25),solver=\"sgd\", max_iter=1000) # accuracy=0.999\n# classifier_red= MLPClassifier(hidden_layer_sizes=(4,2),solver=\"sgd\", max_iter=1000) # accuracy=0.5\n\nclassifier_red.fit(x_train, y_train)\n\ny_pred_red=classifier_red.predict(x_test)\n\nprint(\"RESULTATS RED NEURONALES\\n \")\n\nprint(\"Accuracy Red:\",accuracy_score(y_test, y_pred_red) )\nprint(\"\\nmatrix confusion:\\n \",confusion_matrix(y_test,y_pred_red) )\nprint(\"\\nrecall y F1:\\n\", classification_report(y_test,y_pred_red) )\nprint(\"Quadratic error Red:\",mean_squared_error(y_test, y_pred_red) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con el RandomForestClassifier:\n\nSi hidden_layer_sizes=(4,2), accuracy = 0.5 (aproximadamente)\n\nSi hidden_layer_sizes=(100,100,25), accuracy = 1.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Conclusion:\n\nPara este dataset, los dos classificadores pueden darnos una precisión maxima en la classificacion de los champignones, aunque la red neuronal sea un poquito menos precisa.\n\nSin embargo, en este ejemplo el RandomForestClassifier es mas eficaz mas rapidamente, sin necessidad de muchos arboles. Mientras que la red neuronal necessita muchas neuronas para ser realmente precisa, y el tratamiento toma mas tiempo.\n\nEntonces en este caso, si tenía que elegir, elegiría el classificador RandomForest para resolver este problema.\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print('hello wolrd')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}