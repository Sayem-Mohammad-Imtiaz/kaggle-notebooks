{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Spam Email Classification\n\n## Use hand writing function to build naive bayes model"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Read data"},{"metadata":{"trusted":true,"_uuid":"05f587a4aa8f8aff7a9882f1f6eff02ef890eed6","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bc8abaf50ab1e0bb7c9eb0c4584a86672df890b4"},"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv', encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f90b11fe287e65dec06ca38584eede16490782c9"},"cell_type":"code","source":"print(df.head())\nprint(df.dtypes)\nprint(df.describe())\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e02ba7976832ab2dba61cb9eb290841d6e893eeb"},"cell_type":"markdown","source":"### Split dataset"},{"metadata":{"trusted":true,"_uuid":"cf7dedae129a0c71e9d05140b7923ffdae31e5c4"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"177812ff28c92d82bfde15182eca3b1ca22ebdd2"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata_train, data_test, label_train, label_test = train_test_split(df.v2,\n                                                                 df.v1,\n                                                                 test_size=0.2,\n                                                                 random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739e423cb2f3eaa624f6b5873762cb766f1ec2b2"},"cell_type":"code","source":"print(data_train.head(), label_train.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f5dc00ab02df9514dd0923f8602f79312c6b513"},"cell_type":"markdown","source":"### Calculate the total number of words"},{"metadata":{"trusted":true,"_uuid":"6676935a77bd54d2e20f6e2249670de58344dcec"},"cell_type":"code","source":"def GetVocabulary(data):\n    voc_set = set()\n    for email in data:\n        words = email.split()\n        for word in words:\n            voc_set.add(word)\n    return list(voc_set)\nvocab_list = GetVocabulary(data_train)\nprint('Total number of unique words: ', str(len(vocab_list)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a9dac65c97b7e99c3cb6f14257accf97e08e2e8"},"cell_type":"markdown","source":"### Transfer emails to word vectors"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f485f4c7a044109d9c453792957d33099ea6545e"},"cell_type":"code","source":"def Document2Vector(vocab_list, data):\n    word_vectors = []\n    for document in data:\n        word_vector = np.zeros(len(vocab_list))\n        words = document.split()\n        for word in words:\n            if word in vocab_list:\n                word_vector[vocab_list.index(word)] += 1\n        word_vectors.append(word_vector)\n    return word_vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4b1bd2bdf3c61b7ae2b84995cb0a9bc95e455572"},"cell_type":"code","source":"data_train_vectors = Document2Vector(vocab_list, data_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b4dae29b01f79e6a0ee4c7ac75dc1ca9bd8e026"},"cell_type":"code","source":"print(len(data_train_vectors), len(data_train_vectors[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00409a4178776fb07c0b0fa47abfbe45684c3161"},"cell_type":"code","source":"def NaiveBayes_train(word_vectors, label_train):\n    num_docs = len(word_vectors)\n    num_words = len(word_vectors[0])\n    \n    ham_vector_cnt = np.ones(num_words)\n    spam_vector_cnt = np.ones(num_words)\n    ham_total_cnt = num_words\n    spam_total_cnt = num_words # Laplacian Smoothing -- Improve algorithm (avoid the situation that the probability is 0)\n    \n    ham_count = 0\n    spam_count = 0\n    \n    for i in range(num_docs):\n        if i % 500 == 0:\n            print('Train on the document ID: ', str(i))\n        \n        if label_train[i] == 'ham':\n            ham_vector_cnt += word_vectors[i]\n            ham_total_cnt += word_vectors[i].sum()\n            ham_count += 1\n        else:\n            spam_vector_cnt += word_vectors[i]\n            spam_total_cnt += word_vectors[i].sum()\n            spam_count += 1\n    print(ham_count, spam_count)\n    p_ham_vector = np.log(ham_vector_cnt/ham_total_cnt)\n    p_spam_vector = np.log(spam_vector_cnt/spam_total_cnt)\n    \n    p_ham = np.log(ham_count/num_docs)\n    p_spam = np.log(spam_count/num_docs)\n    \n    return p_ham_vector, p_ham, p_spam_vector, p_spam\n\np_ham_vector, p_ham, p_spam_vector, p_spam = NaiveBayes_train(data_train_vectors, label_train.values)           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df2f00b29bb93e3309aa387a2f85c3876b941c86"},"cell_type":"code","source":"data_test.values.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"30e9a41e2bd63d4edb16ecc98e09923580fa02f1"},"cell_type":"code","source":"def Predict(test_word_vector, p_ham_vector, p_ham, p_spam_vector, p_spam):\n    spam = (test_word_vector * p_spam_vector).sum() + p_spam\n    ham = (test_word_vector * p_ham_vector).sum() + p_ham\n    \n    if spam > ham:\n        return 'spam'\n    else:\n        return 'ham'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3fc902101449d6173a59275c643429f02b0f4562"},"cell_type":"code","source":"data_test_vectors = Document2Vector(vocab_list, data_test.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0654e06475151673536456301520f897919af19"},"cell_type":"markdown","source":"### Start Predicting"},{"metadata":{"trusted":true,"_uuid":"b7252dee2ea4139202886d43dc2e6bd0f629e42c"},"cell_type":"code","source":"predictions = []\nfor i in range(len(data_test_vectors)):\n    if i % 200 == 0:\n        print('Predict on the document ID: ', str(i))\n    pred = Predict(data_test_vectors[i], p_ham_vector, p_ham, p_spam_vector, p_spam)\n    predictions.append(pred)\n\nprint(len(predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a754e157cffe7658e98e34df3dbf47690901d4b4"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint('Accuracy: \\n', accuracy_score(label_test, predictions), '\\n')\nprint('Confusion Matrix: \\n', confusion_matrix(label_test, predictions), '\\n')\nprint('Classification Report: \\n', classification_report(label_test, predictions))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}