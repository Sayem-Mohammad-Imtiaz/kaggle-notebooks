{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The implementation in this notebook is influenced by https://www.youtube.com/watch?v=LDRbO9a6XPU"},{"metadata":{},"cell_type":"markdown","source":"# Configure hyper-parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecisionNode(object):\n    def __init__(self, feature, value, true_branch, false_branch):\n        self.feature = feature\n        self.value = value\n        self.true_branch = true_branch\n        self.false_branch = false_branch\n        \n    def trace(self, idx=0):\n        print(f'[{idx}] {self.feature} ({self.value})')\n        print(' ' * idx + f'/ ', end='')\n        self.true_branch.trace(idx+1)\n        print(' ' * idx + f'\\\\ ', end='')\n        self.false_branch.trace(idx+1)\n        \n        \nclass Leaf(object):\n    def __init__(self, sub_df, label_col):\n        self.predictions = sub_df[label_col].value_counts().to_dict()\n        \n    def trace(self, idx=0):\n        final_prediction = max(self.predictions, key=self.predictions.get)\n        print(f'[{idx}]' + str(final_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecisionTree(object):\n    def __init__(self, data_path, label_col='quality'):\n        self.label_col = label_col\n        self.df = pd.read_csv(data_path)\n        self.feature_cols = self.df.columns.drop(self.label_col)\n        \n    @staticmethod\n    def is_numeric(value):\n        return isinstance(value, int) or isinstance(value, float)\n    \n    def _count_classes(self, sub_df):\n        return sub_df['quality'].value_counts().to_dict()\n    \n    def _get_gini(self, sub_df):\n        ''' https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n        '''\n        count_dict = self._count_classes(sub_df)\n        impurity = 1\n        \n        for label, count in count_dict.items():\n            prob = count / len(sub_df)\n            impurity -= prob ** 2\n            \n        return impurity\n    \n    def _compute_info_gain(self, left, right, current_uncertainty):\n        p = len(left) / (len(left) + len(right))\n        \n        return current_uncertainty - p * self._get_gini(left) - (1 - p) * self._get_gini(right)\n    \n    def _partition(self, sub_df, feature, value):\n        ''' Partitions a dataset\n        '''\n        if self.is_numeric(value):\n            mask = sub_df[feature] >= value\n        else:\n            mask = sub_df[feature] == value\n            \n        return sub_df.loc[mask], sub_df.loc[~mask]\n        \n    \n    def _find_best_split(self, sub_df):\n        best_gain = 0\n        best_feature = None\n        best_value = None\n        current_uncertainty = self._get_gini(sub_df)\n        \n        for feature in self.feature_cols:\n            unique_values = sub_df[feature].unique()\n            \n            for value in unique_values:\n                true_sub_df, false_sub_df = self._partition(sub_df, feature, value)\n                \n                if len(true_sub_df) == 0 or len(false_sub_df) == 0:\n                    continue\n                    \n                gain = self._compute_info_gain(true_sub_df, false_sub_df, current_uncertainty)\n                \n                if gain >= best_gain:\n                    best_gain = gain\n                    best_feature = feature\n                    best_value = value\n                    \n        return best_gain, best_feature, best_value\n    \n    def _build_tree(self, sub_df):\n        gain, best_feature, best_value = self._find_best_split(sub_df)\n        \n        if gain == 0:\n            return Leaf(sub_df, self.label_col)\n        \n        true_sub_df, false_sub_df = self._partition(sub_df, best_feature, best_value)\n        \n        true_branch = self._build_tree(true_sub_df)\n        false_branch = self._build_tree(false_sub_df)\n        \n        return DecisionNode(best_feature, best_value, true_branch, false_branch)\n    \n    def build_tree(self):\n        return self._build_tree(self.df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTree(data_path=DATA_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = dt.build_tree()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.trace()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# root = tree\n# stack = [root]\n\n# while True:\n#     node = stack[0]\n#     del stack[0]\n\n#     if isinstance(node, DecisionNode):\n#         feature = node.feature\n#         value = node.value\n#         true_branch = node.true_branch\n#         false_branch = node.false_branch\n        \n#         print(feature, value)\n        \n#         stack.append(true_branch)\n#         stack.append(false_branch)\n#     else:\n#         predictions = node.predictions\n        \n#         print(predictions)\n        \n#     if len(stack) == 0:\n#         break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}