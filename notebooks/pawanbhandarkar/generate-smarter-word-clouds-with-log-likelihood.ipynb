{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install wordninja\n!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport random\nimport math\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\n\n\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt \n\nimport wordninja\nfrom spellchecker import SpellChecker\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))  \nstop_words.add(\"amp\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = pd.read_csv(\"/kaggle/input/80000-tweets-from-us-capitol-riotsjan-6-2021/tweets.csv\")\ndata = pd.read_csv(\"/kaggle/input/pfizer-vaccine-tweets/vaccination_tweets.csv\")\n\n# data contains one non-string entry for 'text'\nstr_mask = [isinstance(x, str) for x in data.text]\ndata = data[str_mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standard tweet preprocessing \n\ndata.text =data.text.str.lower()\n#Remove twitter handlers\ndata.text = data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n#remove hashtags\ndata.text = data.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n# Remove URLS\ndata.text = data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n# Remove all the special characters\ndata.text = data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n#remove all single characters\ndata.text = data.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n# Substituting multiple spaces with single space\ndata.text = data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n# Convert string to a list of words\ndata['words'] = data.text.apply(lambda x:re.findall(r'\\w+', x ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VADER Sentiment Analysis \nRefer [this notebook](https://www.kaggle.com/pawanbhandarkar/training-a-sith-lord) to understand why I select 0.35 and -0.05 as the threshold values. We then sort them into classes based on where the compound sentiment lies with respect to the chosen threshold.\n\n- sentiment['compound'] < -0.05 => Negative (-1)\n- -0.05 < sentiment['compound'] < 0.35 => Neutral (0)\n- sentiment['compound'] > 0.35 => Positive (1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions \ndef get_sign(x, p, n):\n    if x > p:\n        return 1\n    if x < n:\n        return -1 \n    return 0\n\ndef flatten_list(l):\n    return [x for y in l for x in y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sia = SIA()\n\nsentiments = [sia.polarity_scores(x)['compound'] for x in tqdm(data['text'])]\nclasses = [get_sign(s, 0.35, -0.05) for s in sentiments]\ndata['classes'] = classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_acceptable(word: str):\n    return word not in stop_words and len(word) > 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create one document each for all words in the negative, neutral and  positive classes respectively\nneg_doc = flatten_list(data[data['classes'] == -1]['words'])\nneg_doc = [x for x in neg_doc if is_acceptable(x)]\n\npos_doc = flatten_list(data[data['classes'] == +1]['words'])\npos_doc = [x for x in pos_doc if is_acceptable(x)]\n\nneu_doc = flatten_list(data[data['classes'] == 0]['words'])\nneu_doc = [x for x in neu_doc if is_acceptable(x)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# color coding our wordclouds \ndef red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \n\ndef green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \n\ndef yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Word Clouds\n\nAs you can see, Naively generating word clouds based on word frequencies alone do not capture any useful information. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# reusable function to generate word clouds \ndef generate_word_clouds(neg_doc, neu_doc, pos_doc):\n    # Display the generated image:\n    fig, axes = plt.subplots(1,3, figsize=(20,10))\n    \n    \n    wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neg_doc))\n    axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')\n    axes[0].set_title(\"Negative Tweets\")\n    axes[0].axis(\"off\")\n\n    wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neu_doc))\n    axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation='bilinear')\n    axes[1].set_title(\"Neutral Words\")\n    axes[1].axis(\"off\")\n\n    wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(pos_doc))\n    axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')\n    axes[2].set_title(\"Positive Words\")\n    axes[2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive word clouds \ngenerate_word_clouds(neg_doc, neu_doc, pos_doc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_percent_words(doc, percent):\n    # returns a list of \"top-n\" most frequent words in a list \n    top_n = int(percent * len(set(doc)))\n    counter = Counter(doc).most_common(top_n)\n    top_n_words = [x[0] for x in counter]\n    \n    return top_n_words\n    \ndef clean_document(doc):\n    spell = SpellChecker()\n    lemmatizer = WordNetLemmatizer()\n    \n    # lemmatize words (needed for calculating frequencies correctly )\n    doc = [lemmatizer.lemmatize(x) for x in doc]\n    \n    # get the top 10% of all words. This may include \"misspelled\" words \n    top_n_words = get_top_percent_words(doc, 0.1)\n\n    # get a list of misspelled words \n    misspelled = spell.unknown(doc)\n    \n    # accept the correctly spelled words and top_n words \n    clean_words = [x for x in doc if x not in misspelled or x in top_n_words]\n    \n    # try to split the misspelled words to generate good words (ex. \"lifeisstrange\" -> [\"life\", \"is\", \"strange\"])\n    words_to_split = [x for x in doc if x in misspelled and x not in top_n_words]\n    split_words = flatten_list([wordninja.split(x) for x in words_to_split])\n    \n    # some splits may be nonsensical, so reject them (\"llouis\" -> ['ll', 'ou', \"is\"])\n    clean_words.extend(spell.known(split_words))\n    \n    return clean_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate Log Likelihood\n\n![](https://latex.codecogs.com/gif.latex?%5CLambda%20_%7Bi%7D%20%3Dlog%20%5Cleft%20%5B%5Cfrac%7BP%28w_%7Bi%7D%7C%20positive%29%7D%7BP%28w_%7Bi%7D%20%7C%20negative%29%7D%20%5Cright%20%5D)\n\nSince we have 3 classes, we use the above formula to calculate the log likelihood for each class by treating one as positive the other two as negative. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_log_likelihood(doc1, doc2):    \n\n    doc1_counts = Counter(doc1)\n    doc1_freq = {\n        x: doc1_counts[x]/len(doc1)\n        for x in doc1_counts\n    }\n    \n    doc2_counts = Counter(doc2)\n    doc2_freq = {\n        x: doc2_counts[x]/len(doc2)\n        for x in doc2_counts\n    }\n    \n    doc_ratios = {\n        # 1 is added to prevent division by 0\n        x: math.log((doc1_freq[x] +1 )/(doc2_freq[x]+1))\n        for x in doc1_freq if x in doc2_freq\n    }\n    \n    top_ratios = Counter(doc_ratios).most_common()\n    top_percent = int(0.1 * len(top_ratios))\n    return top_ratios[:top_percent]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean all the documents\nneg_doc_clean = clean_document(neg_doc)\nneu_doc_clean = clean_document(neu_doc)\npos_doc_clean = clean_document(pos_doc)\n\n# combine classes B and C to compare against A (ex. \"positive\" vs \"non-positive\")\ntop_neg_words = get_log_likelihood(neg_doc_clean, flatten_list([pos_doc_clean, neu_doc_clean]))\ntop_neu_words = get_log_likelihood(neu_doc_clean, flatten_list([pos_doc_clean, neg_doc_clean]))\ntop_pos_words = get_log_likelihood(pos_doc_clean, flatten_list([neu_doc_clean, neg_doc_clean]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize top-5 neg and their LL values\ntop_neg_words[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize top-5 neu and their LL values\ntop_neu_words[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize top-5 pos and their LL values\ntop_pos_words[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to generate a document based on likelihood values for words \ndef get_scaled_list(log_list):\n    counts = [int(x[1]*100000) for x in log_list]\n    words = [x[0] for x in log_list]\n    cloud = []\n    for i, word in enumerate(words):\n        cloud.extend([word]*counts[i])\n    # shuffle to make it more \"real\"\n    random.shuffle(cloud)\n    return cloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate syntetic a corpus using our loglikelihood values \nneg_doc_final = get_scaled_list(top_neg_words)\nneu_doc_final = get_scaled_list(top_neu_words)\npos_doc_final = get_scaled_list(top_pos_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Smarter Word Clouds\n\nAs we can see here, the new word clouds are much better indicators of what words are REALLY characteristic of a particular sentiment. We see words that were almost invisble before! Overall, these clouds are much more informative. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualise our synthetic corpus\ngenerate_word_clouds(neg_doc_final, neu_doc_final, pos_doc_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\n- Use VADER to generate sentiment scores and assign to Negative, Neutral and Positive classes \n- Generate naive word clouds using simple prepocessing methods \n- Peform more thorough preprocessing such as spell checks, splitting spaceless phrases etc. \n- Calculate loglikelihood values \n- Generate a Synthetic corpus based on the LL values \n- Generate Smarter Wordclouds"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}