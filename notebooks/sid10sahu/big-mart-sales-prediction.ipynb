{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the Libraries\nimport pandas as pd                                  # For managing Data Structures\nimport numpy as np                                   # For mathematical functions\nimport matplotlib.pyplot as plt                      # For Data visualization\nimport seaborn as sns                                # For Data visualization\nfrom mpl_toolkits.mplot3d import Axes3D              # For 3D graphs\nfrom sklearn.impute import SimpleImputer             # For handeling the missing data (Categorical)\nfrom sklearn.preprocessing import LabelEncoder       # For Label encoding\nfrom sklearn.preprocessing import OneHotEncoder      # For One Hot Encoding\nfrom sklearn.compose import ColumnTransformer        # Fro using OneHotEncoder to transform columns\nfrom sklearn.linear_model import LinearRegression    # For linear regression model\nfrom sklearn.tree import DecisionTreeRegressor       # For Decision tree regression model\nfrom sklearn.ensemble import RandomForestRegressor   # For Random Forest Regression Model\nfrom sklearn import metrics                          # For Evaluation of the regression models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/big-mart-sales-prediction/Train.csv\")\ntest_df = pd.read_csv(\"../input/big-mart-sales-prediction/Test.csv\")\ny_df = pd.read_csv(\"../input/big-mart-sales-prediction/Submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the Column Names\ntrain_df.columns = ['Item_ID','Weight','Fat_Content','Visibility','Item_Type',\n                    'MRP','Out_ID', 'Out_year','Out_Size','Out_Loc','Out_Type', 'Sales']\ntest_df.columns = ['Item_ID','Weight','Fat_Content','Visibility','Item_Type',\n                    'MRP','Out_ID', 'Out_year','Out_Size','Out_Loc','Out_Type']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting basic information of training and test datasets\ntrain_df.info()\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Unique values for categorical dat\nprint(\"Fat_Content\\n \",train_df.Fat_Content.unique())\nprint(\"Item_Type\\n \",train_df.Item_Type.unique())\nprint(\"Out_ID\\n \",train_df.Out_ID.unique())\nprint(\"Out_Size\\n \",train_df.Out_Size.unique())\nprint(\"Out_Loc\\n \",train_df.Out_Loc.unique())\nprint(\"Out_Type\\n \",train_df.Out_Type.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are basically two categories but with different names in \"Fat_Content\". This is required to be handled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handling categories in \"Fat_Content\"\n# Training Set\ntrain_df['Fat_Content'] = train_df['Fat_Content'].replace('low fat', 'Low Fat')\ntrain_df['Fat_Content'] = train_df['Fat_Content'].replace('LF', 'Low Fat')\ntrain_df['Fat_Content'] = train_df['Fat_Content'].replace('reg', 'Regular')\n# Test Set\ntest_df['Fat_Content'] = test_df['Fat_Content'].replace('low fat', 'Low Fat')\ntest_df['Fat_Content'] = test_df['Fat_Content'].replace('LF', 'Low Fat')\ntest_df['Fat_Content'] = test_df['Fat_Content'].replace('reg', 'Regular')\n\nprint(\"New Categories: Fat_Content (Training Set)\\n \",train_df.Fat_Content.unique())\nprint(\"New Categories: Fat_Content (Test Set)\\n \",test_df.Fat_Content.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Missing values\nprint(pd.concat([train_df.isnull().sum(), (train_df.isnull().sum()/train_df.isnull().count()*100)],\n                    axis = 1,\n                    keys = ['Missing values (Train Set)','%']))\nprint(pd.concat([test_df.isnull().sum(), (test_df.isnull().sum()/test_df.isnull().count()*100)],\n                    axis = 1,\n                    keys = ['Missing values (Test Set)','%']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 17% missing values in Weight which has a dtype of float, and 28% missing values in Out_Size which is of object dtype.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------------------------- Handling the Missing Data-------------------------------------------\n# ---------------- Training Set\n# Weight\nsi1 = SimpleImputer(missing_values = np.nan, strategy = 'mean')\narr = train_df.iloc[:,1].values.reshape(-1,1)\nsi1 = si1.fit(arr)\narr = si1.transform(arr)\ntrain_df['Weight'] = arr[:,0]\n# Out Size\nsi2 = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\narr = train_df.iloc[:,8].values.reshape(-1,1)\nsi2 = si2.fit(arr)\narr = si2.transform(arr)\ntrain_df['Out_Size'] = arr[:,0]\n# Checking values after Imputing - Training set\nprint(pd.concat([train_df.isnull().sum(), (train_df.isnull().sum()/train_df.isnull().count()*100)],\n                    axis = 1,\n                    keys = ['Missing values (Train Set)','%']))\n\n#---------------- Test Set\n# Weight\narr = test_df.iloc[:,1].values.reshape(-1,1)\nsi1 = si1.fit(arr)\narr = si1.transform(arr)\ntest_df['Weight'] = arr[:,0]\n# Out_Size\narr = test_df.iloc[:,8].values.reshape(-1,1)\nsi2 = si2.fit(arr)\narr = si2.transform(arr)\ntest_df['Out_Size'] = arr[:,0]\n# Checking values after Imputing - Training set\nprint(pd.concat([test_df.isnull().sum(), (test_df.isnull().sum()/test_df.isnull().count()*100)],\n                    axis = 1,\n                    keys = ['Missing values (Test Set)','%']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------- EDA -----------------\n# No of times different types of Items has been purchased\nsns.set_style('darkgrid')\nplt.figure(figsize=(15,10))\nsns.countplot(train_df.Item_Type, hue=train_df.Fat_Content)\nplt.xticks(rotation=90)\nplt.legend(loc = 'upper right', bbox_to_anchor=(1.1, 1), title = 'Fat Content')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n* Mostly people buy \"Household\" item followed by \"Snacks Food\" and \"Fruits and Vegetables\"\n* The least bought items are seafood and breakfast\n* People tend to by Low fat items more as compares to regular fat item","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average sales from different Item Category\nplt.figure(figsize=(15,10))\nsns.barplot(x = 'Item_Type', y = 'Sales', data = train_df)\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n* Even though houshold items are sold more, but Starchy Food contributes the maximum to the total sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visibility of different Item Type\nplt.figure(figsize = (15,10))\nsns.barplot(x = 'Item_Type', y = 'Visibility', data = train_df)\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n* Breakfast and Seafood are the most visible items, even though they are the least bought items.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sales from different Outlet Location\nplt.figure(figsize=(15,10))\nsns.barplot(x = 'Out_Loc', y = 'Sales', hue = 'Out_Size', data = train_df)\nplt.xlabel(\"Outlet Location\")\nplt.legend(loc = 'upper right', bbox_to_anchor=(1.1, 1), title = 'Outlet Size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The Medium size outlets in all the three locations have alomost same amount of slaes\n* The Small size oultels of tier 2 has more sales than the tier 1\n* And only Tier 3 location has High size outlet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sales from differenr type of Outlets\nplt.figure(figsize=(15,10))\nsns.barplot(x = 'Out_Type', y = 'Sales', hue = 'Out_Size', data = train_df)\nplt.xlabel(\"Outlet Type\")\nplt.legend(loc = 'upper right', bbox_to_anchor=(1.1, 1), title = 'Outlet Size')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation between Visibility and Sales\nplt.figure(figsize=(15,10))\nplt.scatter(train_df.Visibility, train_df.Sales, marker = '.', edgecolors = 'Black')\nplt.xlabel(\"Visibility\")\nplt.ylabel(\"Sales\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, there is a negetive relation between visibilty and sales, which means that the items which provides less sales are kept at a more visible location. This may have been done to improve the sales of that particular item","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation between the price of the item and sales\nplt.figure(figsize=(15,10))\nplt.scatter(train_df.MRP, train_df.Sales, marker = '.', edgecolors = 'black')\nplt.xlabel(\"MPR\")\nplt.ylabel(\"Sales\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe different bracktes of price of the items, and there is a positive relation between the MRP and the Slaes. This may not necessaryly mean that expensive products are sold more. Sales is the product of price and no. of items sold. So, keeping the no of items sold constant, higher price will yield more sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3D representation between Visibility, MRP and Sales\nfig = plt.figure(figsize=(15,10))\nax = Axes3D(fig)\nax.scatter(train_df.Visibility,\n           train_df.MRP, \n           train_df.Sales, \n           marker = 'o', edgecolors = 'black')\nax.set_xlabel('Visibility')\nax.set_ylabel('MRP')\nax.set_zlabel('Sales')\nax.legend()\nax.grid(linestyle='-', linewidth='0.5', color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that items which have more Sales value, have higher value of MRP and lower visibilty.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ------------------- Encoding the categorical variables using Label encoder\n\nlencoder = LabelEncoder()\n# Training Set\nfor i in (2,4,6,7,8,9,10):\n    train_df.iloc[:,i] = lencoder.fit_transform(train_df.iloc[:,i])\n# Test set\nfor i in (2,4,6,7,8,9,10):\n    test_df.iloc[:,i] = lencoder.fit_transform(test_df.iloc[:,i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, we will be working with machine learning models, we have to encode different categories, as the model only understands numbers and not text. Label encoder allocates different numbers (starting from 0) to different categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Unique values for categorical data after label encoding\nprint(\"Fat_Content\\n \",train_df.Fat_Content.unique())\nprint(\"Item_Type\\n \",train_df.Item_Type.unique())\nprint(\"Out_ID\\n \",train_df.Out_ID.unique())\nprint(\"Out_Size\\n \",train_df.Out_Size.unique())\nprint(\"Out_Loc\\n \",train_df.Out_Loc.unique())\nprint(\"Out_Type\\n \",train_df.Out_Type.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a heatmap to visualise the correlation between different variables\nplt.figure(figsize = (15,10))\nsns.heatmap(train_df.corr(), annot= True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* MRP and Outlet Type have a strong positive corelation with Sales\n* Visibility, as discussed erlier also, have a weak negetive corelation with Sales\n* Item type, Fat content, weight and Outlet location have a weak positive corelation with Sales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting the appropriate factors from training and test set\n# this dataset will be later used in building the Machine Learning Model\nX_train = train_df.iloc[:, 1:11]\nX_test = test_df.iloc[:, 1:11]\ny_train = train_df.iloc[:, 11].values\ny_test = y_df.iloc[:,3].values\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Colunms like Item ID are not required to be considered as a varibale contributing to Sales, hence it is removed.\nTo train the ML models, we have seperated the dependent variable, y_train (Sales) and independent variable, X_train. For testing the ML model, test set has been declared (X_test). y_test is considerd the actual sales for the test set. y_test will be used to compare the predicted values of sales for the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding usinng One Hot Encoder\nohe = ColumnTransformer([('onehotencoder',OneHotEncoder(),[1,3,5,6,7,8,9])], remainder = 'passthrough')\nX_train = ohe.fit_transform(X_train).toarray()\nX_test = ohe.fit_transform(X_test).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The categorical variables are encoded using Label encoder, which alocates whole number to the categories. The machine can interpret the category with a higher number to be greater than a category with a lower number. \nFor example, two categories of variable Item_Type, dairy and meat, have been encoded with the number 1 and 5 respectively. Now the machine will interpret that meat is greater than dairy, which is illogical.\n\nTo rectify this, One Hot Encoder is used. This object create new columns with binary values for every category. So if a variable has 5 different category, the OneHotEncoder will create 5 new columns with binary values. So, for a specific record (row), the new columm with the assigned category of the row will have value 1 and the rest of the new columns will have 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping dummy columns to evade dummy variable trap\nX_train = np.delete(X_train, [0,2,18,28,37,40,43], axis = 1)\nX_test = np.delete(X_test, [0,2,18,28,37,40,43], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One column is removed for every varible which is one hot encoded, to tackle multi-colinearity among the categories. This is known as dummy variable trap, and this can can affect the model in a bad way. All the one hot encoded variables are aligned to the left of the data set and in the same order. So, depending on the no of categories in each variable, the first dummy column for each variable is removed. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----------------------------------------- Building Regression Models -------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression\nLR_regressor = LinearRegression(normalize=True)\nLR_regressor.fit(X_train, y_train)\ny_pred_LR = LR_regressor.predict(X_test)\n\n# Model Evaluation (Linear Regression)\nmse_LR = metrics.mean_squared_error(y_test, y_pred_LR)\nr2_LR = metrics.r2_score(y_test, y_pred_LR)\nRMSE_LR = np.sqrt(mse_LR)\n\nprint(\"---------------------- Linear Regression ----------------------\\n\",\n     \"Mean Squared Error: \", mse_LR, \"\\n\",\n     \"R Squared: \", r2_LR, \"\\n\",\n     \"Root Mean Squared Error: \", RMSE_LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Regression\nDT_regressor = DecisionTreeRegressor(random_state=0)\nDT_regressor.fit(X_train, y_train)\ny_pred_DTR = DT_regressor.predict(X_test)\n\n# Model Evaluation\nmse_DTR = metrics.mean_squared_error(y_test, y_pred_DTR)\nr2_DTR = metrics.r2_score(y_test, y_pred_DTR)\nRMSE_DTR = np.sqrt(mse_DTR)\n\nprint(\"---------------------- Decision Tree Regression ----------------------\\n\",\n     \"Mean Squared Error: \", mse_DTR, \"\\n\",\n     \"R Squared: \", r2_DTR, \"\\n\",\n     \"Root Mean Squared Error: \", RMSE_DTR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Regression\nRF_regressor = RandomForestRegressor(n_estimators = 300, random_state = 0)\nRF_regressor.fit(X_train, y_train)\ny_pred_RF = RF_regressor.predict(X_test)\n\n# Model Evaluation\nmse_RF = metrics.mean_squared_error(y_test, y_pred_RF)\nr2_RF = metrics.r2_score(y_test, y_pred_RF)\nRMSE_RF = np.sqrt(mse_RF)\n\nprint(\"---------------------- Random Forest Regression ----------------------\\n\",\n     \"Mean Squared Error: \", mse_RF, \"\\n\",\n     \"R Squared: \", r2_RF, \"\\n\",\n     \"Root Mean Squared Error: \", RMSE_RF)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The least RMSE is for the linear regression model, and it also has an r2 of 0.9. Hence, the linerar model is considerd the best among the three, for this dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exporting results to csv File\nresults = {\n            'Item_Identifier': test_df.Item_ID,\n            'Outlet_Identifier': test_df.Out_ID,\n            'Item_Outlet_Sales': y_pred_LR\n        }\nresults = pd.DataFrame(results)\nresults.to_csv('Submission_Sid.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}