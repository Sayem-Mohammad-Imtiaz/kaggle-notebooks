{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Objective: \n- To clean, analyze and apply a supervised models to Twitter US Airlines Sentiment  \n\n## Content:  \n- Import data and libraries.\n- Data Preprocessing\n- Analyzing and Visualising\n- Wordcloud plots for positive, neutral, and negative tweets.\n- Cleaning Tweets.\n- TF_DIF\n- Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Import & Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pywsd\nnltk.download('averaged_perceptron_tagger')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy as sp\nimport nltk\nimport re\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom pywsd.utils import lemmatize, lemmatize_sentence\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, cross_val_predict,  KFold, cross_validate\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom scipy.stats import zscore, chi2_contingency,normaltest\nfrom imblearn.over_sampling import SMOTE\n\ncolor= \"Spectral\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/twitter-airline-sentiment/Tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**`airline_sentiment_confidence` is left skewed, 75% of the values are bigger than 0.69, and the mean < mode**  \nThis means the confidince in the sentiment is high.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = \"airline_sentiment_confidence\", data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Missing Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**`negativereason_gold`, `airline_sentiment_gold`, and `tweet_coord`, are missing more than 99% of thier rows, so they can't be useful and should be dropped.**  \n`Time zone` , `tweet_location`, and `tweet_coord` can be processed and merged into one column using `Geopy`**  \nBut right now I'm focusing on the other columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total = data.isnull().sum().sort_values(ascending = False)\nperc = (data.isnull().sum() / data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, perc], axis = 1, keys = ['total_missing', 'perc_missing'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize = (15, 8))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=False)\nax.set_title('Missing Data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Number of Retweets  \n94% of the tweets are without any retweet, 4% with one retweet, and there're outliers.\nThe tweet with the highest number of retweet which is 44 was a negative tweet about `US Airways`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"retweet_count\"].value_counts()[1]/len(data[\"retweet_count\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"retweet_count\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data[\"retweet_count\"]==44,[\"retweet_count\",'airline_sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(1, 1, figsize = (15, 5))\nsns.boxplot(y = \"airline\", x = \"retweet_count\", data = data)\nplt.xlabel('# of Retweets', size = 14)\nplt.ylabel('Airline', size = 14)\nb, t = plt.ylim()\nb += 0.5\nt -= 0.5\nplt.ylim(b, t)\nplt.title('The Distribution of Number of Retweets for each Airline', fontsize = 14)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for positive or negative tweets the probability of getting retweeted are about the same (11 negative, 11 positive).  \nbut for a single negative tweet the likelihood of a higher number of mretweets is higher.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(1, 1, figsize = (15, 5))\nsns.boxplot(y = \"airline_sentiment\", x = \"retweet_count\", data = data)\nplt.xlabel('# of Retweets', size = 14)\nplt.ylabel('Airline', size = 14)\nb, t = plt.ylim()\nb += 0.5\nt -= 0.5\nplt.ylim(b, t)\nplt.title('The Distribution of Number of Retweets for each Airline', fontsize = 14)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Number of Words in a Tweet","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The mean and median of the number of words in positive and neutral tweets = 14\nWhile in negative tweet number of word tends to be much more, with mean of 19 and median 21","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['n_words'] = [len(t.split()) for t in data.text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Positive # words mean =\",data['n_words'][data['airline_sentiment']=='positive'].mean())\nprint(\"Neutral  # words mean =\",data['n_words'][data['airline_sentiment']=='positive'].mean())\nprint(\"Negative # words mean =\",data['n_words'][data['airline_sentiment']=='negative'].mean())\nprint()\nprint(\"Positive # words median =\",data['n_words'][data['airline_sentiment']=='positive'].median())\nprint(\"Neutral  # words median =\",data['n_words'][data['airline_sentiment']=='positive'].median())\nprint(\"Negative # words median =\",data['n_words'][data['airline_sentiment']=='negative'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15, 6))\nsns.distplot(data['n_words'][data['airline_sentiment']=='positive'], color='g', label = 'positive')\nsns.distplot(data['n_words'][data['airline_sentiment']=='negative'], color='r', label = 'negative')\nsns.distplot(data['n_words'][data['airline_sentiment']=='neutral'], color='b', label = 'neutral')\nplt.legend(loc='best')\nplt.xlabel('# of Words', size = 14)\nplt.ylabel('Count', size = 14)\nplt.title('The Distribution of Number of Words for each Class', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we saw, the mean and median of number of words in positive tweets are the same, and the plot is close to normal distribution, so will check using `normaltest`**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ND = data['n_words'][data['airline_sentiment']=='neutral']\nk2, p = normaltest(ND)\nalpha = 1e-3\nprint(\"p = {:g}\".format(p))\nif p < alpha:  # null hypothesis: x comes from a normal distribution\n    print(\"The null hypothesis can be rejected, (not normally distributed)\")\nelse:\n    print(\"The null hypothesis cannot be rejected, (normally distributed)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Convert the date from object to datetime**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['tweet_created'] =  data['tweet_created'].str[:-6]\ndata['tweet_created'] = pd.to_datetime(data['tweet_created'],format='%Y-%m-%d')\ndata['tweet_created'] = pd.to_datetime(data.tweet_created.dt.strftime(\"%Y-%m-%d\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['tweet_created']#.asfreq(freq='30S')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The classes are unbalanced, and the negative calss is three times more than the neutral or the positive. There was more than 800 negative tweet about `American Airlines`, even though there's data about it only in the last three days.  \nBUT if we considered all the days, `United` has the  highest number of negative tweets overall, 2633 and 492 positive. We can compare it with `Delta` (955 Neg, 544 Pos)   \nI think one of the way to handle thi is to seperate the three`American`, `US Airways` and `United`. The other airlines are behaving differently.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sizes = data['airline_sentiment'].value_counts()\nlabels = data['airline_sentiment'].value_counts().index\ncolors = ['#ff6666', '#ffcc99', '#99ff99']\ndata['airline_sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = plt.pie(sizes, labels=labels, colors=colors, startangle=90)\ncentre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = data.groupby([data.tweet_created.dt.date,data.airline,data.airline_sentiment]).count()#.agg({'airline_sentiment': 'count'}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = st.iloc[:,:1].reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The number of positive, negative, and neutral tweet every day**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"chart = sns.catplot(x=\"airline\", hue=\"airline_sentiment\", y=\"tweet_id\", data=st,palette=sns.diverging_palette(10, 220, sep=80, n=3,center=\"dark\"), kind=\"bar\",ci=None,height=5,aspect=1.5)\n(chart.set_xticklabels(st['tweet_created'].unique(), horizontalalignment='center', rotation=20).despine(left=True)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The number of positive, negative, and neutral tweet every day for each airline**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"chart = sns.catplot(x=\"tweet_created\", hue=\"airline_sentiment\", y=\"tweet_id\",col='airline',col_wrap=2,palette=sns.diverging_palette(10, 220, sep=80, n=3,center=\"dark\"), data=st, kind=\"bar\",aspect=1.5)\n(chart.set_xticklabels(st['tweet_created'].unique(), horizontalalignment='center', rotation=20).despine(left=True)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['airline','airline_sentiment']).count().iloc[:,0].sort_values()#/data.groupby(['airline']).count().iloc[:,0].sort_values()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Cheking if the class of a tweet and a day it was written in it are dependent, using `Chi-square test`  \np value was much less than 0.05, which mean the day and sentiment are related, which makes sense.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pvst = st.pivot_table(index='airline_sentiment', columns='tweet_created', values='tweet_id', aggfunc='sum').fillna(0)\npvst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p, dof, expected = chi2_contingency(pvst)\nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n    print('tweet_created and airline_sentiment are dependent') \nelse: \n    print('tweet_created and airline_sentiment are Independent') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### What is the reason behind the negative experiences?  \nit's customer service, in all of the airlines except for Delta, Late flight.\nAs before, Virgin America has the least negative reasons, while US Airways,United American is the highest.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ngr = data.groupby([data.negativereason,data.airline]).count()[['tweet_id']].reset_index()#.sort_values(by='tweet_id',ascending=False)\n# ngr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chart = sns.catplot(x=\"airline\", hue=\"negativereason\", y=\"tweet_id\", data=ngr,palette='Set3', kind=\"bar\",ci=None,height=5,aspect=2)\n# (chart.set_xticklabels(st['airline'].unique(), horizontalalignment='center', rotation=20).despine(left=True)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Encode the target column**  \nfrom (positive, neutral, negative) to (2,1,0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(data['airline_sentiment'])\nle.classes_\ndata['airline_sentiment'] = le.transform(data['airline_sentiment'])\ndata['airline_sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Tweets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Edit the stpowords, flight is the most common word in all classes, and `no`,`doon't`, etc. Because are more common in the negative class**  \nflight and get are common in all classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stp_wrds = stopwords.words(\"english\")\nstp_wrds.append(\"flight\")\nstp_wrds.append('get')\nstp_wrds.remove('no')\nstp_wrds.remove('don')\nstp_wrds.remove('nor')\nstp_wrds.remove('not')\nstp_wrds.remove('now')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to clean the tweets\ndef clean(tweet):\n    tweet = tweet.lower() # lowercase\n    tweet = re.sub(r'http\\S*\\b', '',tweet)# remove links\n    tweet = re.sub(r'@\\S*\\b', '',tweet) # remove mentions \n    tweet = re.sub(r'[^a-zA-Z]', ' ',tweet) # only words\n    tweet = TweetTokenizer().tokenize(tweet) \n    tweet = ' '.join([i for i in tweet if (i not in stp_wrds)])\n#     tweet = ' '.join([PorterStemmer().stem(i) for i in tweet])\n    tweet = lemmatize_sentence(tweet) #limmatizeing\n    tweet = ' '.join([i for i in tweet if (i not in stp_wrds)])\n#     print(tweet)\n    return(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cl_text'] = data['text'].apply(clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Negative Words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can see some of the most common words from the negative tweets, dely, customer service, help, (don, don't), issue, cancel, etc ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_tweets = data[data['airline_sentiment'] == 0]\nneg_string = []\nfor t in neg_tweets.cl_text:\n    neg_string.append(t)\nneg_string = pd.Series(neg_string).str.cat(sep=' ')\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(neg_string) \nplt.figure(figsize=(12,10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neutural Words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some of the words in the neutural are please, help, need, fleet, time, thank, tomorrow, today, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neu_tweets = data[data['airline_sentiment'] == 1]\nneu_string = []\nfor t in neu_tweets.cl_text:\n    neu_string.append(t)\nneu_string = pd.Series(neu_string).str.cat(sep=' ')\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(neu_string) \nplt.figure(figsize=(12,10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Positive Words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The words here are positive like great, thank, love, good, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tweets = data[data['airline_sentiment'] == 2]\npos_string = []\nfor t in pos_tweets.cl_text:\n    pos_string.append(t)\npos_string = pd.Series(pos_string).str.cat(sep=' ')\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string) \nplt.figure(figsize=(12,10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Columns to dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ = pd.get_dummies(data[['retweet_count','airline','airline_sentiment_confidence','n_words']], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_['cl_text'] = data['cl_text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split to training and testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_\ny = data['airline_sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stratfied k fold for preserving the percentage of samples for each class\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To get the best parameters for tfdif, but it takes times, so I just took the best parameters\n# pipeline = Pipeline([\n#     ('tfidf', TfidfVectorizer()),\n#     ('clf', SGDClassifier()),\n# ])\n# parameters = {\n#     'tfidf__max_df': (0.71, 0.8,0.9, 1.0),\n#     'tfidf__max_features': (None, 5000, 10000),\n#     'tfidf__ngram_range': ((1, 1), (1, 2),(2, 3)),  # unigrams or bigrams\n#     'tfidf__use_idf': (True, False),\n#     'tfidf__norm': ('l1', 'l2'),\n#     'tfidf__smooth_idf':(True, False),\n#     'clf__max_iter': (20,),\n#     'clf__alpha': (0.00001, 0.000001),\n#     'clf__penalty': ('l2', 'elasticnet'),\n#     'clf__max_iter': (10, 50, 80),\n# }\n# grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n# print(\"Performing grid search...\")\n# print(\"pipeline:\", [name for name, _ in pipeline.steps])\n# print(\"parameters:\")\n# print(parameters)\n# grid_search.fit(X_train['cl_text'], y_train)\n# print(\"Best score: %0.3f\" % grid_search.best_score_)\n# print(\"Best parameters set:\")\n# best_parameters = grid_search.best_estimator_.get_params()\n# for param_name in sorted(parameters.keys()):\n#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF_DIF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(max_df=0.71, norm='l2',max_features=None, ngram_range=(1,2), smooth_idf=True ,use_idf= False)\nX_train_tf = tfidf_vect.fit_transform(X_train['cl_text'])\nX_test_tf = tfidf_vect.transform(X_test['cl_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Add the other columns**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = sp.sparse.csr_matrix(X_train.drop('cl_text', axis=1).astype(float))\nX_train_ = sp.sparse.hstack((X_train_tf, cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = sp.sparse.csr_matrix(X_test.drop('cl_text', axis=1).astype(float))\nX_test_ = sp.sparse.hstack((X_test_tf, cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oversampling (Smote), although I didn't use it with all of the models, it was worse the the imbalanced data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"smt = SMOTE(random_state=777, k_neighbors=1)\nX_SMOTE, y_SMOTE = smt.fit_sample(X_train_, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metrics(model, kfold, X_train, X_test, y_train, y_test): \n    model.fit(X_train, y_train)\n    train_score =  model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    print('Training Score =', train_score)\n    print('Testing Score =', test_score)\n    ####\n    cv = cross_val_score(model, X_train, y_train, cv = kfold)\n    print(\"Cross Val Scores =\", cv)\n    print(\"Cross Val Standard Deviation =\", cv.std())\n    print('Cross Val Mean Score =', cv.mean()); \n    ###\n    pred = model.predict(X_test)\n    print('Confusion Matrix =\\n',confusion_matrix(y_test, pred))\n    print('Classification Report =\\n',classification_report(y_test, pred))\n    return pred, test_score, cv.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## K-Neighbors Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##KNN WITHOUT OVER-SAMPLING (SMOTE)\nprint('K-Neighbors Classifier')\nparams = {\n    \"n_neighbors\" : [5],#,15,25,30,35,40, 100],\n    \"weights\" : [\"distance\"] #\"uniform\"\n}\nknn= GridSearchCV(KNeighborsClassifier(), params, n_jobs=-1, cv=10)\nknn_pred, knn_test, knn_train = metrics(knn, kfold, X_train_, X_test_, y_train, y_test)\nprint(knn.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ##KNN WITH OVER-SAMPLING (SMOTE)\n# # Worse\n# print('K-Neighbors Classifier')\n# params = {\n#     \"n_neighbors\" : [5,15,25,30,35,40, 100],\n#     \"weights\" : [\"uniform\" , \"distance\"]\n# }\n# knn= GridSearchCV(KNeighborsClassifier(), params, n_jobs=-1, cv=10)\n# knn_pred, knn_test, knn_train = metrics(knn, kfold, X_SMOTE, X_test_, y_SMOTE, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SVC')\nparams = {\n    'C':[10],\n    'gamma':[0.1], \n    'kernel':['rbf']#'linear',\n}\n\nsvc = SVC(random_state=42)\nsvc = GridSearchCV(svc, params, n_jobs=-1, cv=10)\nsvc_pred, svc_test, svc_train = metrics(svc, kfold, X_train_, X_test_, y_train, y_test)\nprint(svc.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Decision Tree')\nparam_grid = {'max_depth':[2],#1,\n              'min_samples_leaf':[3]}#,5\ndecision_tree = DecisionTreeClassifier(random_state=42)\ndecision_tree = GridSearchCV(decision_tree, param_grid=param_grid, cv=5, n_jobs=-1)\ndt_pred, dt_test, dt_train = metrics(decision_tree, kfold, X_train_, X_test_, y_train, y_test)\nprint(decision_tree.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Logistic Regression Model')\nparams = {\n    \"penalty\": [\"l2\"],#\"l1\", \n    \"C\": [10000.0]#np.logspace(-2,4,10)\n}\nlogistic_regression = GridSearchCV(LogisticRegression(random_state=42), params, n_jobs=-1, cv=10)\nlg_pred, lg_test, lg_train = metrics(logistic_regression, kfold, X_train_, X_test_, y_train, y_test)\nprint(logistic_regression.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Forest')\nrf_params = {\n        'max_features':[2]#, 7, 8],\n        #'max_depth': [1, 2, 3, 4, 5, 8],\n        #'criterion':['gini', 'entropy']\n}\nrandom_forest = RandomForestClassifier(random_state=42,n_estimators=80)\nrandom_forest = GridSearchCV(random_forest, param_grid=rf_params, cv=5, n_jobs=-1)\nrf_pred, rf_test, rf_train = metrics(random_forest, kfold, X_train_, X_test_, y_train, y_test)\nprint(random_forest.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('AdaBoost')\nparam_grid = { \n    'n_estimators': [80],#10,50, \n    'learning_rate':[0.1]#0.01,\n}\nada_boost = AdaBoostClassifier(random_state=42)\nada_boost = GridSearchCV(ada_boost, param_grid=param_grid, cv=5, n_jobs=-1)\nab_pred, ab_test, ab_train = metrics(ada_boost, kfold, X_train_, X_test_, y_train, y_test)\nprint(ada_boost.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra Trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Extra Trees')\nrf_params = {\n    'n_estimators': [10],#, 100, 400, 800, 1100, 1850],\n    'max_features':['auto'],\n    'max_depth': [1],#, 2, 3, 4, 5, 8],\n    'criterion':['gini']\n}\nextra_trees = ExtraTreesClassifier(n_estimators=100,random_state=42)\ngs = GridSearchCV(extra_trees, param_grid=rf_params, cv=5, n_jobs=-1)\net_pred, et_test, et_train = metrics(gs, kfold, X_train_, X_test_, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1) = plt.subplots(figsize=(10,6))\ninds = range(1,8)\nlabels = [\"KNN\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\",'Extra Trees', 'AdaBoost', 'SVC' ]\nscores_all = [knn_train, lg_train, dt_train, rf_train, et_train, ab_train, svc_train]\nscores_predictive = [knn_test, lg_test, dt_test, rf_test, et_test, ab_test, svc_test]    \nax1.bar(inds, scores_all, color=sns.color_palette(color)[5], alpha=0.3, hatch=\"x\", edgecolor=\"none\",label=\"CrossValidation Set\")\nax1.bar(inds, scores_predictive, color=sns.color_palette(color)[0], label=\"Testing set\")\nax1.set_ylim(0.4, 1)\nax1.set_ylabel(\"Accuracy score\")\nax1.axhline(0.626913, color=\"black\", linestyle=\"--\")\nax1.set_title(\"Accuracy scores for basic models\", fontsize=17)\nax1.set_xticks(range(1,8))\nax1.set_xticklabels(labels, size=12, rotation=40, ha=\"right\")\nax1.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we you can see from the confusion matrix for predicted and actual values.  \nDecision Tree classified all of the tweets as positive and negative only, Extra trees classified all of them as negative, so its accuracy is equal to the acuracy baseline, and we can see it clearly in the plot.\n\nThe best model is `SVC`, it gives the best score, precision, recall compared to the other models.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We was able to apply 7 diffrient models to to classify sentiment of tweets corresponding to various airlines. And the best model was `SVC` with acuracy =76. \n\nThere're ,much more one can do, for example, process the location, better extract the words from the tweets. Dealing with tweets is not like any othersource of text. It's full of texting abbreviations and misspelled words. Some words like cool, would be writen in twitter as `COooooo1` and those words even if they belong to one class, it's not easy dealing with it. But the `SVC` performance was good.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}