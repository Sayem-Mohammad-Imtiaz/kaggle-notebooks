{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q tensorflow==2.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport re, string, time, random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom kaggle_datasets import KaggleDatasets\nimport os, csv, collections","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"USE_PREVIOUS_SAVE = True\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LOCAL_FLICKR_PATH = '/kaggle/input/flickr-image-dataset/flickr30k_images/'\nannotation_file = LOCAL_FLICKR_PATH + 'results.csv'\nLOCAL_IMG_PATH = LOCAL_FLICKR_PATH + 'flickr30k_images/'\n\n!ls {LOCAL_IMG_PATH} | wc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif strategy.num_replicas_in_sync == 8:\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path('flickr-image-dataset') # 8gb # 20-25 mins\n    print('yeah')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if strategy.num_replicas_in_sync == 8:\n    # print(GCS_DS_PATH_FLICKR)\n    # !gsutil ls $GCS_DS_PATH_FLICKR\n\n    print(GCS_DS_PATH)\n    !gsutil ls $GCS_DS_PATH\n    \n    FLICKR_PATH = GCS_DS_PATH + '/flickr30k_images/'\n    IMG_PATH = FLICKR_PATH + 'flickr30k_images/'\n    # less than 10sec\n    !gsutil ls {IMG_PATH} | wc\nelse: \n    FLICKR_PATH = LOCAL_FLICKR_PATH\n    IMG_PATH = LOCAL_IMG_PATH","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"image_caption = collections.defaultdict(list)\nprepath='/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\nanotation = '/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv'\nwith open(anotation, 'r') as f:\n    next(f)\n    anotation_list = csv.reader(f)\n    for row in anotation_list:\n        split_str = ','.join(row).split('| ')\n        caption = f\"<start> {split_str[-1]} <end>\"\n        image_caption[IMG_PATH+split_str[0]].append(caption)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_caption = []\ntrain_path = []\nval_caption = []\nval_path = []\nallkeys = list(image_caption.keys())\nrandom.shuffle(allkeys)\nkeys = allkeys[:28000]\nfor i in keys:\n    caption_list = image_caption[i]\n    train_caption.extend(caption_list)\n    train_path.extend([i]*len(caption_list))\nval_keys = allkeys[28000:30000]\nfor i in val_keys:\n    caption_list = image_caption[i]\n    val_caption.extend(caption_list)\n    val_path.extend([i]*len(caption_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Captioning Tokenization\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<unk>', filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\nlen_seq = [len(seq.split()) for seq in train_caption]\nmax_len=int(np.percentile(len_seq, 95))\ntokenizer.fit_on_texts([list[0] for list in list(image_caption.values())])\ntrain_sequences = tokenizer.texts_to_sequences(train_caption)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\ntrain_sequences = pad_sequences(train_sequences, padding='post', maxlen=max_len, truncating='post')\nval_sequences = tokenizer.texts_to_sequences(val_caption)\nval_sequences = pad_sequences(val_sequences, padding='post', maxlen=max_len, truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(image_caption))\nprint(len(train_path))\nprint(len(train_sequences))\nprint(len(val_sequences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nembed_dims=256\nunits=512\nvocab_size=len(tokenizer.word_index)+1\nif strategy.num_replicas_in_sync == 1:\n    BATCH_SIZE = 1\nBatch_size = 64 * strategy.num_replicas_in_sync\nnum_steps = len(train_path)//Batch_size\nval_num_steps = len(val_path)//Batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef image_processing(path, label=None):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = (tf.cast(img, tf.float32) / 255.0)\n    img = tf.image.resize(img, (299, 299))\n    #img = tf.keras.applications.inception_v3.preprocess_input(img)\n    \n    if label is None:\n        return img\n    else:\n        return img, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((train_path, train_sequences))\ndataset = dataset.map(image_processing, num_parallel_calls=AUTO).cache()\ndataset = dataset.map(augment, num_parallel_calls=AUTO).shuffle(Batch_size*8, reshuffle_each_iteration=True)\ndataset = dataset.batch(Batch_size, drop_remainder=False).prefetch(AUTO)\ndataset = strategy.experimental_distribute_dataset(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = tf.data.Dataset.from_tensor_slices((val_path, val_sequences))\nval_dataset = val_dataset.map(image_processing, num_parallel_calls=AUTO)\nval_dataset = val_dataset.cache().batch(Batch_size, drop_remainder=False).prefetch(AUTO)\nval_dataset = strategy.experimental_distribute_dataset(val_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, embed_dims):\n        super().__init__()\n        self.V3 = tf.keras.applications.InceptionV3(include_top=False, weights=\"imagenet\")\n        self.model = tf.keras.Model(self.V3.input, self.V3.layers[-1].output)\n        self.model.trainable = False\n        self.Dense = tf.keras.layers.Dense(embed_dims)\n    def call(self, x):\n        x = self.model(x)\n        x = tf.reshape(x, (x.shape[0], -1, x.shape[-1]))\n        x = self.Dense(x)\n        x = tf.nn.relu(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.Model):\n    def __init__(self, units):\n        super().__init__()\n        self.units = units\n        self.W1 = tf.keras.layers.Dense(self.units)\n        self.W2 = tf.keras.layers.Dense(self.units)\n        self.V = tf.keras.layers.Dense(1)\n    def call(self, x, hidden):\n        hidden_step_dim = tf.expand_dims(hidden, 1)\n        attention_hidden = tf.nn.tanh(self.W1(x)+self.W2(hidden_step_dim))\n        score = self.V(attention_hidden)\n        weights = tf.nn.softmax(score, axis=1)\n        context_vector = weights*x\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, units, embed_dims, vocab_size):\n        super().__init__()\n        self.units = units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dims)\n        self.Dense1 = tf.keras.layers.Dense(self.units)\n        self.Dense2 = tf.keras.layers.Dense(vocab_size)\n        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n        self.attention = Attention(self.units)\n    def call(self, encoder_output, x, hidden):\n        context_vector, weights = self.attention(encoder_output, hidden)\n        x = self.embedding(x)\n        x = tf.concat([tf.expand_dims(context_vector,1), x], axis=-1)\n        output, state = self.gru(x)\n        output = self.Dense1(output)\n        output = tf.reshape(output, (-1, output.shape[-1]))\n        output = self.Dense2(output)\n        return output, state, weights\n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    optimizer = tf.keras.optimizers.Adam()\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    def loss_function(target, output):\n        mask = tf.math.logical_not(tf.math.equal(target, 0))\n        loss = loss_object(target, output)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss*=mask\n        loss = tf.nn.compute_average_loss(loss, global_batch_size=Batch_size)\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    @tf.function\n    def train_step(img, target):\n        loss=0\n        hidden = decoder.reset_state(batch_size=img.shape[0])\n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']]*target.shape[0], 1)\n        with tf.GradientTape() as tape:\n            enc_output = encoder(img)\n            for i in range(1, target.shape[1]):\n                output, hidden, _ = decoder(enc_output, dec_input, hidden)\n                loss+=loss_function(target[:, i], output)\n                dec_input = tf.expand_dims(target[:, i],1)\n        total_loss = loss/target.shape[1]\n        trainable_variables = encoder.trainable_variables+decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n        return loss, total_loss\n    def distributed_train_step(inputs):\n        (img, target) = inputs\n        loss = strategy.run(train_step, args=(img, target))\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    valid_loss = tf.keras.metrics.Sum()\n    @tf.function\n    def val_step(img, target):\n        loss=0\n        hidden = decoder.reset_state(batch_size=img.shape[0])\n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']]*target.shape[0], 1)\n        with tf.GradientTape() as tape:\n            enc_output = encoder(img)\n            for i in range(1, target.shape[1]):\n                output, hidden, _ = decoder(enc_output, dec_input, hidden)\n                loss+=loss_function(target[:, i], output)\n                dec_input = tf.expand_dims(target[:, i], 1)\n        batch_loss = loss/(target.shape[1])\n        return loss, batch_loss\n    \n    @tf.function\n    def distributed_val_step(inputs):\n        (img, target) = inputs\n        loss = strategy.run(val_step, args=(img, target))\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    loss_record=[]\n    val_loss_record=[]\n    epochs=10\n    encoder = Encoder(embed_dims)\n    decoder = Decoder(units, embed_dims, vocab_size)\n    for epoch in range(1, epochs):\n        start = time.time()\n        epoch_loss=0\n        val_epoch_loss=0\n        for (batch, (inputs_batch)) in enumerate(dataset):\n            _, replica_loss = distributed_train_step(inputs_batch)\n            total_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, replica_loss, axis=None)\n            epoch_loss+=total_loss\n            if batch%100 ==0:\n                print('Train: Epoch {} Batch {} Loss {}'.format(epoch, batch, total_loss))\n            loss_record.append(epoch_loss/num_steps)\n        print('Epoch {} Loss {}'.format(epoch, epoch_loss))\n        print('Total time used for 1 epoch {} sec\\n'.format(time.time()-start))\n        for (batch, (inputs_batch)) in enumerate(val_dataset):\n            _, replica_loss = distributed_val_step(inputs_batch)\n            total_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, replica_loss, axis=None)\n            val_epoch_loss+=total_loss\n            if batch%5 ==0:\n                print('Val: Epoch {} Batch {} Loss {}'.format(epoch, batch, total_loss))\n            val_loss_record.append(val_epoch_loss/val_num_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(image):\n    attention = np.zeros((train_sequences.shape[1], 64))\n    hidden = decoder.reset_state(batch_size=1)\n    shape = image_processing(image).shape\n    img_input = tf.expand_dims(image_processing(image),0)\n    encoded_output = encoder(img_input)\n    shape = tokenizer.word_index['<start>']\n    decoder_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    generator = []\n    for i in range(train_sequences.shape[1]):\n        output, state, weights = decoder(encoded_output, decoder_input, hidden)\n        attention[i] = tf.reshape(weights, (-1,)).numpy()\n        output_id = tf.random.categorical(output, 1)[0][0]\n        generator.append(tokenizer.index_word[output_id.numpy()])\n        if tokenizer.index_word[output_id.numpy()]=='<end>':\n            return generator, attention\n        decoder_input = tf.expand_dims([output_id], 0)\n        \n    attention = attention[:len(generator),:]\n    return generator, attention","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_list = list(set(image_caption.keys()).difference(set(keys)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_generator():\n    rid = np.random.randint(0, len(test_list))\n    image = test_list[rid]\n    real_caption = ','.join(image_caption[image])\n    result, attention_plot = evaluate(image)\n    print ('Real Caption:', real_caption)\n    print('')\n    print ('Prediction Caption:', ' '.join(result))\n    image = tf.io.read_file(image)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.keras.preprocessing.image.array_to_img(image)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plota(image, generator, attention):\n    image_np = np.array(Image.open(image))\n    fig = plt.figure(figsize=(10,10))\n    len_gen = len(generator)\n    for l in range(len_gen):\n        temp_att = np.resize(attention[i], (8, 8))\n        ax = fig.add_subplot(len_gen//2, len_gen//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(image_np)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}