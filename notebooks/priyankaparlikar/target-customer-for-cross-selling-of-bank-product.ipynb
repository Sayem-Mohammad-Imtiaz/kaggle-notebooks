{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport scipy.stats as st\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score,roc_curve,classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',None)\ndf_train=pd.read_csv('../input/banking-dataset-marketing-targets/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train shape: {}'.format(df_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['ID','poutcome'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inferences:\n\n* 1.Age can be assumed to be normally distributed as mean and median are almost equal.\n* 2.For Balance and Duration (contact duration), mean > median which indicates that both are right skewed and high outliers are present.\n* 3.Campaign (no. of contacts performed during this campaign), mean > median with not much difference hence it is right skewed but it can be said that contacts to customer while campaign are almost same as difference between mean and median is not much.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['subscribed'].value_counts().plot(kind='bar')\nplt.xlabel('Subscribed')\nplt.ylabel('No. of subscription')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['subscribed'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target variable is highly imbalanced , hence imbalanced data treatment is required.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.countplot(data=df_train,x='job',hue='subscribed')\nplt.ylabel('No. of subscription')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_train,x='marital',hue='subscribed')\nplt.ylabel('No. of subscription')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_train,x='education',hue='subscribed')\nplt.ylabel('No. of subscription')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_train,x='housing',hue='subscribed')\nplt.ylabel('No. of subscription')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_train,x='loan',hue='subscribed')\nplt.ylabel('No. of subscription')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_train,x='contact',hue='subscribed')\nplt.ylabel('No. of subscription')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.countplot(data=df_train,x='month',hue='subscribed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above exploratory data analysis, bank should target the following customer segment:\n* Customer working in management, bluecollar and technical fields.\n* Married customer are most probable of subscribing the product.\n* Customer should have atleast secondary education.\n* Customer possesing houseloan and personal loan are of less probablity to subscrib the product.\n* The customers who are connected through cellular contact are of high probability for product subscription.\n* Months that are most suitable to conduct a 2nd marketing campaign are from April to August with May having the most high chances of customers subcribing product. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train = pd.get_dummies(data=df_train,columns=['job','marital','education','default','housing','loan','contact','month'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train['subscribed']=final_train['subscribed'].replace('no',0)\nfinal_train['subscribed']=final_train['subscribed'].replace('yes',1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor=final_train.corr()\nsub_cor=abs(cor['subscribed'])\nsig_features=sub_cor[sub_cor>0.05]\nprint(sig_features)\nprint(sig_features.count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above features are highly correlated with target variable i.e subscribed and thus are the significant features.But we will still perform feature selection technique to find the most significant features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=final_train.drop('subscribed',axis=1)\ny=final_train['subscribed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=X.drop(['age','job_admin.','marital_divorced','education_primary','default_no','loan_no','housing_no','contact_unknown','month_apr'],axis=1)\nvif1 = pd.DataFrame()\nvif1[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\nvif1[\"features\"] = x.columns\nvif1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf= final_train.drop(['age','job_admin.','marital_divorced','education_primary','default_no','loan_no','housing_no','contact_unknown','month_apr'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop('subscribed',axis=1)\ny=df['subscribed']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-Test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imbalanced data treatment :Over sampling minority class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Xytrain = pd.concat([X_train,y_train],axis=1)\n\nprint('before oversampling: ','\\n', Xytrain['subscribed'].value_counts())\nXytrain0 = Xytrain[Xytrain['subscribed']==0]\nXytrain1 = Xytrain[Xytrain['subscribed']==1]\n\nlen0 = len(Xytrain0)\nlen1 = len(Xytrain1)\n\nXytrain1_os = Xytrain1.sample(len0,replace = True, random_state=3)\nXytrain_os = pd.concat([Xytrain0, Xytrain1_os],axis=0)\n\nprint('after undersampling: ','\\n',Xytrain_os['subscribed'].value_counts())\n\ny_train_os = Xytrain_os['subscribed']\nX_train_os = Xytrain_os.drop('subscribed',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardizing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = StandardScaler()\n\nXtrains = ss.fit_transform(X_train_os)\nXtests = ss.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_eval(algo, Xtrains, y_train_os, Xtests, y_test):\n    algo.fit(Xtrains,y_train_os)\n    ytrain_pred = algo.predict(Xtrains)\n    ytrain_prob = algo.predict_proba(Xtrains)[:,1]\n\n    print('Overall accuracy - train:' , accuracy_score(y_train_os, ytrain_pred))\n    print('Confusion matrix - train: ','\\n',confusion_matrix(y_train_os,ytrain_pred))\n    print('AUC - train', roc_auc_score(y_train_os,ytrain_prob))\n    print('\\n')\n    print('Classification report - train: ','\\n',classification_report(y_train_os,ytrain_pred))\n\n    ytest_pred = algo.predict(Xtests)\n    ytest_prob = algo.predict_proba(Xtests)[:,1]\n\n    print('\\n')\n    print('Overall accuracy - test:' , accuracy_score(y_test, ytest_pred))\n    print('Confusion matrix - test: ','\\n',confusion_matrix(y_test,ytest_pred))\n    print('AUC - test', roc_auc_score(y_test,ytest_prob))\n    print('Classification report - test: ','\\n',classification_report(y_test,ytest_pred))\n\n    fpr,tpr,thresholds = roc_curve(y_test,ytest_prob)\n    plt.plot(fpr,tpr)\n    plt.plot(fpr,fpr,'r')\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=DecisionTreeClassifier(max_depth = 5, criterion = 'gini',random_state=3)\nmodel_eval(dt, Xtrains, y_train_os, Xtests, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100,criterion='gini',max_depth=5,random_state=3)\nmodel_eval(rf, Xtrains, y_train_os, Xtests, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(solver='liblinear', fit_intercept=True,random_state=3)\nmodel_eval(lr, Xtrains, y_train_os, Xtests, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boosting : Adaboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostClassifier(random_state = 3)\nmodel_eval(ada, Xtrains, y_train_os, Xtests, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Classifier: Gaussian","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GaussianNB()\nmodel_eval(clf, Xtrains, y_train_os, Xtests, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Through EDA we get to know the customer segment target for cross selling of fixed deposit (bank product).\n* Feature selection technique provides significant features to identify the target customers.\n* Applied different classification algorithms to check which algorithm will give the best accurate results with the significant features.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}