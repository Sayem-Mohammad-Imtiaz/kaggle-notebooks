{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://la-pistola-records.com/10466-large_default/elton-john-kiki-dee-don%60t-go-breaking-my-heart.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## \"Right from the start...\""},{"metadata":{},"cell_type":"markdown","source":"My aim with this notebook is to test the researchers' hypothesis that not only can heart failure be accurately predicted using just serum creatinine and ejection fraction, but models using the complete set of variables in their dataset actually perform worse than just the two variable model.\n\nI aim to test this by not only comparing accuracy and F1 scores across a handful of classical and ensemble machine learning models, but also importantly by studying the variation of these two scores for each respective model during cross-validation.  **My central question is does the two-variable model generalize just as well, if not better, and produce more consistent accuracy and F1 scores than the model with all variables.  As a follow-up, if the two-variable model does have more variation, is it acceptable because the scores far exceed the all variable model anyway?**\n\nI then move on to understanding if there is a distinct difference in how accurate the probability scores are for decision predictions for the two-factor model versus the model containing all variables.  Probability scores can be useful measures to guide physicians trust in a model decision but obviously only if they are accurate.\n\nI conclude by creating a Monte Carlo function that tests the significance of each model's results without assuming a distribution type. \n\nTwo notes:\n1. My code and descriptions will use the word \"focus\" to describe datasets and models where only ejection fraction and serum creatinine are used.\n2. The model that has all variables actually does not include the variable, \"time,\" which the Kaggle page describes as \"follow up period (days)\" but the research report does not explain very well.  I excluded this varariable completely from all of my analysis because it seems very likely that this isn't information that a physician would know during the screening where all the other data points are recorded.  Additionally, the \"time\" variable is extremely predictive and the correlation seems to suggest that a shorter time correlates with higher mortality, which doesn't intuitively make a lot of sense. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import Libraries\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.random import Generator, PCG64\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport os\n\nfrom scipy.cluster import hierarchy\nfrom scipy.stats import spearmanr\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import Dataset\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nraw_data = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing data and review data types.\n\nprint(\"Missing Data Proportion\")\nprint(pd.isnull(raw_data).mean())\n\nprint(\"Dataset Info\")\nprint(raw_data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set aside a test set.\n# Remove the 'time' variable since we cannot know in advance when the next follow-up will be.\n# I will only look at the test set when I am done tuning my models and will not make any adjustments to my models after running the test data.\n\ndata = raw_data.copy()\ndata.drop('time', axis=1, inplace=True)\nX, y = data.loc[:, data.columns != 'DEATH_EVENT'], data['DEATH_EVENT']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = pd.concat([x_train, y_train], axis=1)\neda_scaler = StandardScaler()\nscaled_training_data = eda_scaler.fit_transform(training_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"REFERENCE\n\n* Binary variables: anaemia, diabetes, high_blood_pressure, sex, smoking, DEATH_EVENT\n* Continuous variables: age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, and serum_sodium"},{"metadata":{},"cell_type":"markdown","source":"Crosstab Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tab_anaemia = pd.crosstab(training_data['anaemia'], training_data['DEATH_EVENT'], normalize='index')\nprint(x_tab_anaemia)\nprint( )\n\nx_tab_diabetes = pd.crosstab(training_data['diabetes'], training_data['DEATH_EVENT'], normalize='index')\nprint(x_tab_diabetes)\nprint( )\n\nx_tab_hbp = pd.crosstab(training_data['high_blood_pressure'], training_data['DEATH_EVENT'], normalize='index')\nprint(x_tab_hbp)\n# Note: Not having high blood was a decent indicator of survival\nprint( )\n\nx_tab_sex = pd.crosstab(training_data['sex'], training_data['DEATH_EVENT'], normalize='index')\nprint(x_tab_sex)\nprint( )\n\nx_tab_smoking = pd.crosstab(training_data['smoking'], training_data['DEATH_EVENT'], normalize='index')\nprint(x_tab_smoking)\n\n# High blood pressure is the only stand-out predictor in the training data.\n# Given the person has high blood pressure, they have a 38.7% chance of dying.\n# Given the person does not have high blood pressure, they have a 28.6% of dying. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quick Plots"},{"metadata":{},"cell_type":"markdown","source":"\nThe training data includes 164 men (listed as 1) and 90 women (listed as 0).\n\n\nNote on Sex: \n\nWe know which values in the binary breakdown are associated with male and female by looking at the count of 0s and 1s in the raw data where there are 194 1s and 105 0s. The \"Dataset\" section of the research reports states, \"The patients consisted of 105 women and 194 men, and their ages range between 40 and 95 years old (Table 1).\"  The plot below shows us that not only are most of the data male but most of the death events are also male."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_sex = px.histogram(training_data, x='sex', color='DEATH_EVENT')\nfig_sex.update_layout(margin=dict(l=20, r=20, t=20, b=20))\nfig_sex.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSerum Creatinine values are heavily right skewed.  While there are significantly fewer observations at higher levels, the death rate is proportionally much higher."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_serum = px.histogram(training_data, x='serum_creatinine', color='DEATH_EVENT')\nfig_serum.update_layout(margin=dict(l=20, r=20, t=20, b=20))\nfig_serum.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Low levels of ejection fraction appear to correspond with more deaths."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_eject = px.histogram(training_data, x='ejection_fraction', color='DEATH_EVENT')\nfig_eject.update_layout(margin=dict(l=20, r=20, t=20, b=20))\nfig_eject.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the researchers note in their paper, you can very much imagine a linear boundary separating survivals and deaths in this plot.  The plot shows the importance of low serum_creatinine (most people are between 0.8 - 1.2) and a sort of ideal level of ejection fraction around 35.  Lower ejection fraction than 35 for the same levels of serum creatinine ends up being deadly."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_serum_eject = px.scatter(training_data, x='serum_creatinine', y='ejection_fraction', color='DEATH_EVENT')\nfig_serum_eject.update_traces(marker=dict(size=12,\n                                          line=dict(width=2,\n                                                    color='DarkSlateGrey')),\n                              selector=dict(mode='markers'))\n\nfig_serum_eject.update_layout(margin=dict(l=20, r=20, t=20, b=20))\nfig_serum_eject.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA - Is it worthwhile to use here?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pca_x = training_data.drop('DEATH_EVENT', axis=1)\npca_scaler = StandardScaler()\npca_x = pca_scaler.fit_transform(pca_x)\npca_list = []\nfor x in range(1, (pca_x.shape[1]) + 1):\n    pca = PCA(n_components=x)\n    pca.fit(pca_x)\n    pca_list.append(pca.explained_variance_ratio_.sum())\n\npca_plot = pd.DataFrame({'PCA Components': range(1, (pca_x.shape[1]) + 1),\n                         'Explained Variance': pca_list})\npca_fig = px.line(pca_plot, x='PCA Components', y='Explained Variance')\npca_fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))\npca_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The smooth curve to this line without any \"elbows\" makes it difficult to decide which number of PCA components to choose from.  Furthermore, 8 of 11 components are required to achieve 80% explained variance and about 9 are required for 90% explanation -- meaning not a great deal of dimensionality reduction is achieved and PCA is probably not worth using for our models."},{"metadata":{},"cell_type":"markdown","source":"## Models"},{"metadata":{},"cell_type":"markdown","source":"I performed grid searches using all variables (except \"time) and just the focus variables.  My goal was to find well parameterized versions of each of the model types, not necessarily to go over the top beating the best performers in the Kaggle circuit.  Again, my goal was to understand key effects to machine learning models from a two-variable and complete variable set approach."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scale the data\n\nscaler = StandardScaler()\nx_train_std = scaler.fit_transform(x_train)\nx_test_std = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One last bit of EDA before jumping in ...\n\nThe hierarchical clustering results were fascinating to me.  It's probably important for the two-variable model performance that ejection fraction and serum creatinine do not share a common cluster at all. \n\nAdditionally, if you Google creatinine phosphokinase and diabetes (a clustering pairing), you will find studies linking the two -- one of which can be found [here](https://pubmed.ncbi.nlm.nih.gov/16995840/).\n\nFinally, ejection fraction is ascertained through imaging techniques, while serum sodium is measured through a blood sample.  For areas of the world where imaging a patient is not possible, but blood samples are, it would interesting if machine learning could use serum sodium instead of ejection fraction in a two-variable model to predict heart failure.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Review Hierarchical Clustering & Correlation\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\ncorr = spearmanr(x_train_std).correlation\ncorr_linkage = hierarchy.ward(corr)\ndendro = hierarchy.dendrogram(corr_linkage, labels=x_test.columns.tolist(), ax=ax1, leaf_rotation=90)\ndendro_idx = np.arange(0, len(dendro['ivl']))\n\nax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\nax2.set_xticks(dendro_idx)\nax2.set_yticks(dendro_idx)\nax2.set_xticklabels(dendro['ivl'], rotation='vertical')\nax2.set_yticklabels(dendro['ivl'])\nfig.tight_layout()\nplt.show()\n\n# I used code from the sklearn website for these plots.\n# https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-pyThat code is found here.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define grid search function\n\ndef grid_search(model, params, x_data=x_train_std, y_data=y_train):\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    gcv = GridSearchCV(estimator=model,\n                       param_grid=params,\n                       scoring=['accuracy', 'f1'],\n                       cv=skf,\n                       refit='accuracy')\n    gcv.fit(x_data, y_data)\n    gcv_ser = pd.DataFrame(gcv.cv_results_).loc[gcv.best_index_, :]\n\n    return gcv, gcv_ser\n\n\n# Create Focus Features DataFrame\nfocus_features = ['ejection_fraction', 'serum_creatinine']\nfocus_x_train = x_train.copy()[focus_features]\nfocus_x_test = x_test.copy()[focus_features]\n\nfocus_scaler = StandardScaler()\nfocus_x_train = focus_scaler.fit_transform(focus_x_train)\nfocus_x_test = focus_scaler.transform(focus_x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression - All features\n\nlog_reg = LogisticRegression(random_state=42)\nlog_param_grid = dict(C=np.arange(0.005, 0.1, 0.001), class_weight=[None, 'Balanced'])\nlog_param_grid_best = dict(C=[0.088])\n\nlog_gcv, log_gcv_ser = grid_search(log_reg, log_param_grid_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression Sub-section - plotting results in 3d**\n\nThe 3-D plot shows a feature space defined by the first three PCA components, which themselves explain 38.8% of training data variance.  While it's hard to find a clean boundary of deaths and survivals, there does appear to be a linear hyperplane separating the high probability model predictions from low ones.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Logistic Regression Sub-section - plotting results in 3d\n\nlog_preds = log_gcv.predict(x_train_std).tolist()\nlog_pred_probs = log_gcv.predict_proba(x_train_std).tolist()\nwinning_probs = [p[i] for p, i in zip(log_pred_probs, log_preds)]\nlog_pca = PCA(n_components=3)\npca_x = log_pca.fit_transform(pca_x)\nexp_var = log_pca.explained_variance_ratio_.sum()\nlog_df = pd.DataFrame(pca_x, columns=[1, 2, 3])\nlog_df['Prediction'] = log_preds\nlog_df['Probability'] = winning_probs\nlog_df = pd.concat([log_df, training_data['DEATH_EVENT'].reset_index()], axis=1)\nlog_df['Prediction Check'] = ['correct' if actual == pred else 'incorrect' for\n                              actual, pred in zip(log_df['DEATH_EVENT'], log_df['Prediction'])]\n\nlog_fig = px.scatter_3d(log_df, x=1, y=2, z=3,\n                        color='Probability',\n                        symbol='Prediction Check')\nlog_fig.update_layout(title_text='PCA Axes Explain ' + str(round(exp_var * 100, 1)) + '% of Total Variance',\n                      legend=dict(yanchor='top', y=0.99, xanchor='left', x=0.01))\nlog_fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression - Focus Features\n\nlog2_reg = LogisticRegression(random_state=42)\nlog2_param_grid = dict(C=np.arange(0.1, 1.6, 0.1), class_weight=[None, 'Balanced'])\nlog2_param_grid_best = dict(C=[0.6])\n\nlog2_gcv, log2_gcv_ser = grid_search(log2_reg, log2_param_grid_best, focus_x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVC - All Features\n\nsvc = SVC(probability=True, random_state=42)\nsvc_param_grid = dict(C=np.arange(0.5, 2.1, 0.1), kernel=['rbf', 'linear', 'poly', 'sigmoid'], degree=np.arange(1, 5),\n                      class_weight=[None, 'balanced'])\n\nsvc_param_grid_best = dict(C=[1.4], class_weight=[None], degree=[1], kernel=['sigmoid'])\nsvc_gcv, svc_gcv_ser = grid_search(svc, svc_param_grid_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVC - Focus Features\n\nsvc2 = SVC(probability=True, random_state=42)\nsvc2_param_grid = dict(C=np.arange(0.05, 0.16, 0.01), kernel=['rbf', 'linear', 'poly', 'sigmoid'],\n                       degree=np.arange(1, 5), class_weight=[None, 'balanced'])\n\nsvc2_param_grid_best = dict(C=[0.09], class_weight=['balanced'], degree=[1], kernel=['linear'])\n\nsvc2_gcv, svc2_gcv_ser = grid_search(svc2, svc2_param_grid_best, focus_x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN - All Features\n\nknn = KNeighborsClassifier()\nknn_param_grid = dict(n_neighbors=np.arange(3, 23, 2), weights=['uniform', 'distance'],\n                      algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'], p=[1, 2])\n\nknn_param_grid_best = dict(algorithm=['auto'], n_neighbors=[11], p=[1], weights=['uniform'])\n\nknn_gcv, knn_gcv_ser = grid_search(knn, knn_param_grid_best)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN - Focus Features\n\nknn2 = KNeighborsClassifier()\nknn2_param_grid = dict(n_neighbors=np.arange(3, 23, 2), weights=['uniform', 'distance'],\n                       algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'], p=[1, 2])\n\nknn2_param_grid_best = dict(algorithm=['auto'], n_neighbors=[13], p=[1], weights=['distance'])\n\nknn2_gcv, knn2_gcv_ser = grid_search(knn2, knn2_param_grid_best, focus_x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest - All Features\n\nrf = RandomForestClassifier(n_jobs=-1, random_state=921)\nrf_param_grid = dict(n_estimators=[10, 15, 25, 50, 75, 100], max_depth=[None, 1, 2, 3, 4, 5],\n                     max_features=['auto', 'log2', 'sqrt'], min_samples_leaf=[2, 3, 4, 5],\n                     class_weight=[None, 'balanced', 'balanced_subsample'])\n\nrf_param_grid_best = dict(class_weight=['balanced'], max_depth=[3], max_features=['auto'], min_samples_leaf=[3],\n                          n_estimators=[15])\n\nrf_gcv, rf_gcv_ser = grid_search(rf, rf_param_grid_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest - Focus Features\n\nrf2 = RandomForestClassifier(n_jobs=-1, random_state=921)\nrf2_param_grid = dict(n_estimators=[10, 15, 20, 25, 30], max_depth=[None, 5, 6, 7, 10, 15],\n                      max_features=['auto', 'log2', 'sqrt'],\n                      min_samples_leaf=[2, 3, 4], class_weight=[None, 'balanced', 'balanced_subsample'])\n\nrf2_param_grid_best = dict(class_weight=[None], max_depth=[7], max_features=['auto'], min_samples_leaf=[3],\n                           n_estimators=[15])\n\nrf2_gcv, rf2_gcv_ser = grid_search(rf2, rf2_param_grid_best, focus_x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting Classifier - All Features\ngbc = GradientBoostingClassifier(random_state=42)\n\ngbc_param_grid = dict(learning_rate=[.05, .06, .08, .09, .1, .25], n_estimators=[50, 75, 100, 125, 150, 200],\n                      subsample=[0.5, 0.7, 0.8, 0.9, 1.0], min_samples_leaf=[6, 8, 10, 12, 15],\n                      max_depth=[1, 2, 3, 4, 5, 6, 7], max_features=['auto', 'sqrt'], n_iter_no_change=[None, 5])\n\ngbc_param_grid_best = dict(learning_rate=[0.08], max_depth=[4], max_features=['sqrt'], min_samples_leaf=[10],\n                           n_estimators=[100], subsample=[1], n_iter_no_change=[None])\n\ngbc_gcv, gbc_gcv_ser = grid_search(gbc, gbc_param_grid_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting Classifier - Focus Features\n\ngbc2 = GradientBoostingClassifier(random_state=42)\ngbc2_param_grid = dict(learning_rate=np.arange(0.2, 0.8, 0.1), n_estimators=[15, 25, 35, 50, 75],\n                       subsample=np.arange(0.2, 1.0, 0.1), min_samples_leaf=np.arange(3, 10, 1),\n                       max_depth=[1, 2, 3, 4, 5, 6, 7, 10, 12], max_features=['auto', 'sqrt'],\n                       n_iter_no_change=[None, 5])\n\ngbc2_param_grid_best = dict(learning_rate=[0.4], max_depth=[5], max_features=['auto'], min_samples_leaf=[6],\n                            n_estimators=[35], subsample=[0.7], n_iter_no_change=[None])\n\ngbc2_gcv, gbc2_gcv_ser = grid_search(gbc, gbc2_param_grid_best, focus_x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a consolidated results table\n\nres_df = pd.DataFrame(dict(log_all=log_gcv_ser, log_foc=log2_gcv_ser,\n                           svc_all=svc_gcv_ser, svc_foc=svc2_gcv_ser,\n                           knn_all=knn_gcv_ser, knn_foc=knn2_gcv_ser,\n                           rf_all=rf_gcv_ser, rf_foc=rf2_gcv_ser,\n                           gbc_all=gbc_gcv_ser, gbc_foc=gbc2_gcv_ser)).T\n\nres_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Create Box Plots for F1 and Accuracy Scores of Models in Cross Validation\n\nf1_res_cols = res_df.columns[(res_df.columns.str.startswith('split')) & (res_df.columns.str.endswith('f1'))]\nacc_res_cols = res_df.columns[(res_df.columns.str.startswith('split')) & (res_df.columns.str.endswith('accuracy'))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot is one of the center points of my analysis as it strongly supports the researchers' findings in one plot and makes my hypothesis less plausible.  Recall that my hypothesis was that their two variable model would have higher scoring variation across cross validation splits.  \n\nEach column in the plot represents the F1 scores for 5 randomly selected samples of the training data.  The ideal result would be a very short boxplot located at or near the top of the grid.  Across the models the \"foc\" variable set (i.e., ejection fraction and serum creatinine variables only) have less variation in the Logistic Regression, KNN, and Random Forest models.  In the cases of the SVC and Gradient Boosted Classifier (GBC) models, the marginally higher variation is eclipsed by both higher median F1 scores and a distribution of scores across cross validation that mostly beat the all variable model.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# F1 Box Plots\n\nf1_res = res_df.copy()[f1_res_cols].reset_index()\nf1_res_melted = pd.melt(f1_res, id_vars=['index']).drop('variable', axis=1)\nf1_res_melted['Model'], f1_res_melted['Variable Set'] = list(zip(*[x.split(\"_\") for x in f1_res_melted['index']]))\nfig = px.box(f1_res_melted, x='Model', y='value', color='Variable Set')\nfig.update_layout(title_text='F1 Scores', title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The message is more mixed when we evaluate the same plot for accuracy.  Zooming in on the two best models, Random Forest and GBC, shows the case where the all variable model should be preferred as it has roughly the same variation as the focus variable model, but with a higher median score.  Alternatively, GBC produced the same median scores across variable sets, but with the key difference that the higher variation in the two-model approach meant some cases where results were significantly better."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Accuracy Box Plots\n\nacc_res = res_df.copy()[acc_res_cols].reset_index()\nacc_res_melted = pd.melt(acc_res, id_vars=['index']).drop('variable', axis=1)\nacc_res_melted['Model'], acc_res_melted['Variable Set'] = list(zip(*[x.split(\"_\") for x in acc_res_melted['index']]))\nfig = px.box(acc_res_melted, x='Model', y='value', color='Variable Set')\nfig.update_layout(title_text='Accuracy Scores', title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows the models' probabilities associated with their prediction on the x-axis and the accuracy of their prediction on the y-axis.  The idea here is that if the model tells warns the physician of a death event with a probability score of 80%, we would expect the accuracy of that prediction to be 80% in the long-run.  The gray line in the plot shows this theoretical connection between the x and y axes.  Ideally, the \"All\" and \"Focus\" lines would stay very close to the gray line and on occasions where it misses, be above the gray line (indicating a higher accuracy than might have been hoped for given a stated probability). \n\nThe blue and red lines each include all of the machine learning types for their respective set of variables.\n\nWhile up for debate, this plot tells me that while the F1 boxplot earlier might have championed two-variable models, physicians should be wary of the probability scores such an approach offers them.  One area of particular concern is for probabilities exceeding 80% where the two-variable models do not gain any more accuracy despite increasing their probability scores.  Said another way, if a two-variable model tells you its prediction with a 95% probability, you should assume it's only about 80% accurate.\n\nThe All variable model shines a little more brightly in this regard in that it hugs the gray line a little more closely (for the most part).  In the 83% - 89% probability regions its actual accuracy is just as disappointing as the two-variable model (~77%), but for probabilities above that its accuracy actually improves, whereas the two-variable models do not. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Probability Investigation\n\ntest_preds_all = []\ntest_probs_all = []\ntest_pred_checks_all = []\n\nfor mod in [log_gcv, svc_gcv, knn_gcv, rf_gcv, gbc_gcv]:\n    preds = mod.predict(x_test_std).tolist()\n    probs = mod.predict_proba(x_test_std)\n    probs = [p[i] for p, i in zip(probs, preds)]\n    pred_checks = [1 if actual == pred else 0 for actual, pred in zip(y_test, preds)]\n\n    test_preds_all = test_preds_all + preds\n    test_probs_all = test_probs_all + probs\n    test_pred_checks_all = test_pred_checks_all + pred_checks\n\ntest_preds_foc = []\ntest_probs_foc = []\ntest_pred_checks_foc = []\n\nfor mod in [log2_gcv, svc2_gcv, knn2_gcv, rf2_gcv, gbc2_gcv]:\n    preds = mod.predict(focus_x_test).tolist()\n    probs = mod.predict_proba(focus_x_test)\n    probs = [p[i] for p, i in zip(probs, preds)]\n    pred_checks = [1 if actual == pred else 0 for actual, pred in zip(y_test, preds)]\n\n    test_preds_foc = test_preds_foc + preds\n    test_probs_foc = test_probs_foc + probs\n    test_pred_checks_foc = test_pred_checks_foc + pred_checks\n\nmod_list = ['All' for _ in test_preds_all] + ['Focus' for _ in test_preds_foc]\n\ntest_prob_df = pd.DataFrame(dict(Prediction=test_preds_all + test_preds_foc,\n                                 Probability=test_probs_all + test_probs_foc,\n                                 Check=test_pred_checks_all + test_pred_checks_foc,\n                                 Variable_Selection=mod_list))\n\ntest_prob_df['Binned Probabilities'] = pd.cut(test_prob_df['Probability'], bins=12)\ntest_prob_grouped = test_prob_df.groupby(['Binned Probabilities', 'Variable_Selection']).mean()['Check'].reset_index()\ntest_prob_grouped.rename(columns={'Check': 'Accuracy'}, inplace=True)\ntest_prob_grouped['Expected'] = [x.mid for x in test_prob_grouped['Binned Probabilities']]\ntest_prob_grouped['Binned Probabilities'] = [str(x) for x in test_prob_grouped['Binned Probabilities']]\n\nfig = px.line(test_prob_grouped, x='Binned Probabilities', y='Accuracy', color='Variable_Selection')\nfig.add_trace(go.Scatter(x=test_prob_grouped['Binned Probabilities'],\n                         y=test_prob_grouped['Expected'],\n                         mode='lines',\n                         line=go.scatter.Line(color='gray'),\n                         name='Expected',\n                         opacity=0.4))\nfig.update_traces(mode='markers+lines')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical Significance - Monte Carlo"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def monte_carlo_sim(y_true, y_preds):\n    rg = Generator(PCG64(42))\n    mod_acc = accuracy_score(y_true, y_preds)\n    mod_f1 = f1_score(y_true, y_preds)\n    shuffles = [rg.permutation(y_preds) for _ in range(5000)]\n    shuffles_acc = [accuracy_score(y_true, y_shuff) for y_shuff in shuffles]\n    shuffles_f1 = [f1_score(y_true, y_shuff) for y_shuff in shuffles]\n    perct_acc = np.percentile(shuffles_acc, 95)\n    perct_f1 = np.percentile(shuffles_f1, 95)\n    return dict(model_accuracy=mod_acc, mc_accuracy=perct_acc, model_f1=mod_f1, mc_f1=perct_f1)\n\n\nlog_mc = monte_carlo_sim(y_test, log_gcv.predict(x_test_std))\nsvc_mc = monte_carlo_sim(y_test, svc_gcv.predict(x_test_std))\nknn_mc = monte_carlo_sim(y_test, knn_gcv.predict(x_test_std))\nrf_mc = monte_carlo_sim(y_test, rf_gcv.predict(x_test_std))\ngbc_mc = monte_carlo_sim(y_test, gbc_gcv.predict(x_test_std))\n\nlog2_mc = monte_carlo_sim(y_test, log2_gcv.predict(focus_x_test))\nsvc2_mc = monte_carlo_sim(y_test, svc2_gcv.predict(focus_x_test))\nknn2_mc = monte_carlo_sim(y_test, knn2_gcv.predict(focus_x_test))\nrf2_mc = monte_carlo_sim(y_test, rf2_gcv.predict(focus_x_test))\ngbc2_mc = monte_carlo_sim(y_test, gbc2_gcv.predict(focus_x_test))\n\nmc_all_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Accuracy MC 95th', 'F1', 'F1 MC 95th'])\nfor i, (s, mod) in enumerate([('log_all', log_mc), ('svc_all', svc_mc), ('knn_all', knn_mc),\n                              ('rf_all', rf_mc), ('gbc_all', gbc_mc)]):\n    mc_all_df.loc[i, :] = [s, mod['model_accuracy'], mod['mc_accuracy'], mod['model_f1'], mod['mc_f1']]\n\nmc_foc_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Accuracy MC 95th', 'F1', 'F1 MC 95th'])\nfor i, (s, mod) in enumerate([('log_foc', log2_mc), ('svc_foc', svc2_mc), ('knn_foc', knn2_mc),\n                              ('rf_foc', rf2_mc), ('gbc_foc', gbc2_mc)]):\n    mc_foc_df.loc[i, :] = [s, mod['model_accuracy'], mod['mc_accuracy'], mod['model_f1'], mod['mc_f1']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This approach to significance testing takes the predictions made by each model, shuffles them using a permuted congruential generator that randomly permutes the values using samples from different distributions and records the accuracy and F1 scores.  This is performed 5,000 times for each model.  The code then finds the 95th percentile accuracy and F1 scores and inserts it into the dataframe.  The main goal here is to avoid inferring a distribution to the dataset while testing the likelihood that a lucky guess from the model could have produced the same accuracy and F1 scores at the 95th percentile level.\n\nThe results of this exercise reveal that the logistic regression, random forest, and gradient boosted classifer have statistically significant results in the test sample at the 95th percentile for both accuracy and F1.  The support vector classifier and knn fall short on both accuracy and F1.\n\nThe results also show that in the test set then random forest and gradient boosted classifier models achieve accuracy scores of 77.8% and F1 scores of 64.3% and 61.5%, respectively. "},{"metadata":{},"cell_type":"markdown","source":"### 95% Statistical Significance Results - All Variable Model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mc_all_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few alarming things this next dataframe reveals first and foremost of which is the large drop in accuracy and F1 scores on the test dataset for the two-variable model.  This suggests that while the two-variable model performed well in cross-validation, it did not end up generalizing well to the test data -- possibly the result of a greater suspectibility the two factor model has to overfitting than the model with more variables does.\n\nThe support vector classifier ends up being the only statistically significant model in the two-variable set-up.  All other models had results at what the 95th percentile of lucky/random guesses would have achieved. "},{"metadata":{},"cell_type":"markdown","source":"### 95% Statistical Significance Results - Two Variable Model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mc_foc_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The study's claim that ejection fraction and serum creatinine alone produced not only adequate but actually better machine learning models to predict heart failure appears to have some important flaws.  First, its ability to correctly guess probability of heart failure stops appropriately progressing past the 80% probability level (an issue the all variable model has less trouble with) and, second, the two-variable models struggles to beat the 95th percentile guesses the Monte Carlo simulation produced in all cases except the SVC model. \n\nOn the training data, the two-variable model shines in cross validation and produces better F1 scores but fails to produce superior accuracy scores.\n\nMy best model overall is the all variable random forest model, which produced a 77.8% accuracy score and 64.3% F1 score in testing.  Notwithstanding the results of the Monte Carlo simulation, none of the two variable models produced that level of performance against the test data.\n\nIn summary, if we consider the two-variable model's superior F1 results in cross-validation offset by in the inferior ones in accuracy testing and probability performance, then the inability to frequently beat the 95th percentile result of a Monte Carlo simulation and inferior outright results against the test data make the two variable model a worse alternative to the all variable model for physician use."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}