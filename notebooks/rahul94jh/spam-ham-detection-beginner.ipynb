{"cells":[{"metadata":{},"cell_type":"markdown","source":"### SPAM Ham Detection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport nltk\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the file into a panda dataframe\nfilename = \"../input/sms-spam-collection-dataset/spam.csv\"\nspam = pd.read_csv(filename,encoding='latin-1')\n# drop unused columns\nspam = spam.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\nspam = spam.rename(columns={\"v1\":\"label\", \"v2\":\"message\"})\n# Let's check the shape of DataFrame\nprint(spam.shape) #we have 5572 messages\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Converting the read dataset in to a list of tuples, each tuple(row) contianing the message and it's label\ndata_set = []\nfor index,row in spam.iterrows():\n    data_set.append((row['message'], row['label']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_set[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data_set))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## initialise the inbuilt Stemmer and the Lemmatizer\nstemmer = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(document, stem=True):\n    'changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence'\n\n    # change sentence to lower case\n    document = document.lower()\n\n    # tokenize into words\n    words = word_tokenize(document)\n\n    # remove stop words\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n\n    if stem:\n        words = [stemmer.stem(word) for word in words]\n    else:\n        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n\n    # join words to make sentence\n    document = \" \".join(words)\n\n    return document","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## - Performing the preprocessing steps on all messages\nmessages_set = []\nfor (message, label) in data_set:\n    words_filtered = [e.lower() for e in preprocess(message, stem=False).split() if len(e) >= 3]\n    messages_set.append((words_filtered, label))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(messages_set[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing to create features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## - creating a single list of all words in the entire dataset for feature list creation\n\ndef get_words_in_messages(messages):\n    all_words = []\n    for (message, label) in messages:\n      all_words.extend(message)\n    return all_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## - creating a final feature list using an intuitive FreqDist, to eliminate all the duplicate words\n## Note : we can use the Frequency Distribution of the entire dataset to calculate Tf-Idf scores like we did earlier.\n\ndef get_word_features(wordlist):\n\n    #print(wordlist[:10])\n    wordlist = nltk.FreqDist(wordlist)\n    word_features = wordlist.keys()\n    return word_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## - creating the word features for the entire dataset\nword_features = get_word_features(get_words_in_messages(messages_set))\nprint(len(word_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing to create a train and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## - creating slicing index at 80% threshold\nsliceIndex = int((len(messages_set)*.8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## - shuffle the pack to create a random and unbiased split of the dataset\nrandom.shuffle(messages_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_messages, test_messages = messages_set[:sliceIndex], messages_set[sliceIndex:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_messages))\nprint(len(test_messages))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing to create feature maps for train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating a LazyMap of feature presence for each of the 8K+ features with respect to each of the SMS messages\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in word_features:\n        features['contains(%s)' % word] = (word in document_words)\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## - creating the feature map of train and test data\n\ntraining_set = nltk.classify.apply_features(extract_features, train_messages)\ntesting_set = nltk.classify.apply_features(extract_features, test_messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_set[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training set size : ', len(training_set))\nprint('Test set size : ', len(testing_set))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Training the classifier with NaiveBayes algorithm\nspamClassifier = nltk.NaiveBayesClassifier.train(training_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## - Analyzing the accuracy of the test set\nprint(nltk.classify.accuracy(spamClassifier, training_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Analyzing the accuracy of the test set\nprint(nltk.classify.accuracy(spamClassifier, testing_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing a example message with our newly trained classifier\nm = 'CONGRATULATIONS!! As a valued account holder you have been selected to receive a Â£900 prize reward! Valid 12 hours only.'\nprint('Classification result : ', spamClassifier.classify(extract_features(m.split())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Priting the most informative features in the classifier\nprint(spamClassifier.show_most_informative_features(50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## storing the classifier on disk for later usage\nimport pickle\nf = open('nb_spam_classifier.pickle', 'wb')\npickle.dump(spamClassifier,f)\nprint('Classifier stored at ', f.name)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}