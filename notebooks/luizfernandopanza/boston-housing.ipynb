{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/boston-housing-dataset/HousingData.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type (df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns = {'MEDV':'TARGET'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. CRIM: per capita crime rate by town \n2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n3. INDUS: proportion of non-retail business acres per town \n4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n5. NOX: nitric oxides concentration (parts per 10 million) \n6. RM: average number of rooms per dwelling \n7. AGE: proportion of owner-occupied units built prior to 1940 \n8. DIS: weighted distances to five Boston employment centres \n9. RAD: index of accessibility to radial highways \n10. TAX: full-value property-tax rate per 10,000 \n11. PTRATIO: pupil-teacher ratio by town \n12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n13. LSTAT: % lower status of the population \n14. TARGET: Median value of owner-occupied homes in $1000's","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile = pandas_profiling.ProfileReport(df); profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5))\n(1 - df.isnull().mean()).abs().plot.bar(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (df1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medium_value = df['TARGET'].mean(); medium_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"squared_errors = pd.Series(medium_value - df['TARGET'])**2\nSSE = np.sum(squared_errors)\nprint ('SSE: %01.f' % SSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_plot = squared_errors.plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df1.iloc[:,:-1]\ny = df1['TARGET'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# choose attributes with StatsModels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xc = sm.tools.tools.add_constant(x)\nmodelo = sm.OLS(y, xc)\nmodelo_v1 = modelo.fit()\n# this model does not allow missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo_v1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = x.drop(columns=['INDUS','AGE'])\n# col INDUS and AGE > 0,05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xc1 = sm.tools.tools.add_constant(x1)\nmodelo1 = sm.OLS(y, xc1)\nmodelo_v2 = modelo1.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo_v2.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It did not change the R squared","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# correlation Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df1.iloc[:, :-1]\nmatriz_corr = x.corr()\nprint (matriz_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"observations = len(df1)\nvariables = df1.columns[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_correlation_matrix(data, hurdle = 0.0):\n    R = np.corrcoef(data, rowvar = 0)\n    R[np.where(np.abs(R) < hurdle)] = 0.0\n    heatmap = plt.pcolor(R, cmap = mpl.cm.coolwarm, alpha = 0.8)\n    heatmap.axes.set_frame_on(False)\n    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor = False)\n    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor = False)\n    heatmap.axes.set_xticklabels(variables, minor = False)\n    plt.xticks(rotation=90)\n    heatmap.axes.set_yticklabels(variables, minor = False)\n    plt.tick_params(axis = 'both', which = 'both', bottom = 'off', top = 'off', left = 'off', right = 'off') \n    plt.colorbar()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_correlation_matrix(x, hurdle = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Eigenvalues and Eigenvectors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = np.corrcoef(x, rowvar = 0)\neigenvalues, eigenvectors = np.linalg.eig(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (eigenvalues)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (eigenvectors[:,7])\n# pos 7 is the smaller value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (variables[2], variables[8], variables[9])\n# the high values are:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"observations = len(df1)\nvariables = df1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_boston\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standardization = StandardScaler()\nxst = standardization.fit_transform(x)\noriginal_means = standardization.mean_\noriginal_stds = standardization.scale_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xst = np.column_stack((xst, np.ones(observations)))\ny = df1[\"TARGET\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df1.iloc[:, :-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport numpy as np\n\ndef random_w( p ):\n    return np.array([np.random.normal() for j in range(p)])\n\ndef hypothesis(X,w):\n    return np.dot(X,w)\n\ndef loss(X,w,y):\n    return hypothesis(X,w) - y\n\ndef squared_loss(X,w,y):\n    return loss(X,w,y)**2\n\ndef gradient(X,w,y):\n    gradients = list()\n    n = float(len( y ))\n    for j in range(len(w)):\n        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n    return gradients\n\ndef update(X,w,y, alpha = 0.01):\n    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n\ndef optimize(X,y, alpha = 0.01, eta = 10**-12, iterations = 1000):\n    w = random_w(X.shape[1])\n    path = list()\n    for k in range(iterations):\n        SSL = np.sum(squared_loss(X,w,y))\n        new_w = update(X,w,y, alpha = alpha)\n        new_SSL = np.sum(squared_loss(X,new_w,y))\n        w = new_w\n        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n            path.append(new_SSL)\n            return w, path\n        if k % (iterations / 20) == 0:\n            path.append(new_SSL)\n    return w, path                       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.01\nw, path = optimize(xst, y, alpha, eta = 10**-12, iterations = 20000)\nprint (\"Coeficientes finais padronizados: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# undo the standard scaler to see the real values\nunstandardized_betas = w[:-1] / original_stds\nunstandardized_bias  = w[-1]-np.sum((original_means / original_stds) * w[:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('%8s: %8.4f' % ('bias', unstandardized_bias))\nfor beta,varname in zip(unstandardized_betas, variables):\n    print ('%8s: %8.4f' % (varname, beta))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importance of attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)\nmodelo.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standardization = StandardScaler()\nStand_coef_linear_reg = make_pipeline(standardization, modelo)\nStand_coef_linear_reg.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for coef, var in sorted(zip(map(abs, Stand_coef_linear_reg.steps[1][1].coef_), df1.columns[:-1]), reverse = True):\n    print (\"%6.3f %s\" % (coef,var))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# R Squared","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def r2_est(X,y):\n    return r2_score(y, modelo.fit(X,y).predict(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the accuracy\nr2 = r2_est(X,y) * 100\nprint ('Coeficiente R2: %0.1f' %  r2 + ' %')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we can see the value for each variable\nr2_impact = list()\nfor j in range(X.shape[1]):\n    selection = [i for i in range(X.shape[1]) if i!=j]\n    r2_impact.append(((r2_est(X,y) - r2_est(X.values[:,selection],y)), df1.columns[j]))\n    \nfor imp, varname in sorted(r2_impact, reverse = True):\n    print ('%6.3f %s' %  (imp, varname))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}