{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport subprocess\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":208,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"../input/sales data-set.csv\")\nfeatures = pd.read_csv(\"../input/Features data set.csv\")\nstores = pd.read_csv(\"../input/stores data-set.csv\")\nprint(\"Sales Total Col.\",len(sales.columns),\"\\nShape:\",sales.shape,\"\\nColumns:\",sales.columns.tolist(),\"\\n=============\")\nprint(\"Features Total Col.\",len(features.columns),\"\\nShape:\", features.shape, \"\\nColumns:\",features.columns.tolist(),\"\\n=============\")\nprint(\"Stores Total Col.\",len(stores.columns),\"\\nShape:\",stores.shape, \"\\nColumns:\",stores.columns.tolist())\n\ndef insight(df):\n    print(\"--------------------\")\n    print(df.head())\n    \n\ninsight(sales)\ninsight(features)\ninsight(stores)","execution_count":235,"outputs":[]},{"metadata":{"_uuid":"209c9f9c7026973547682a06cc9bd39dca7c089e"},"cell_type":"markdown","source":"**Merge All Dataset**"},{"metadata":{"trusted":true,"_uuid":"e94a42da1e3bd606b0d4191818288602b96af848","collapsed":true},"cell_type":"code","source":"final = sales.merge(features,how=\"left\", on=['Store', 'Date', 'IsHoliday'])","execution_count":209,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e573dd0ba425f9ff85ee6ab3b4546adf927e9cd0"},"cell_type":"code","source":"final = final.merge(stores, how= \"left\", on=['Store'])\nfinal.head()","execution_count":210,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccf0d78b6b06802432d54beafbd220009aedc537"},"cell_type":"code","source":"print(\"Final Dataset Col:\",len(final.columns),\"\\nShape: \",final.shape,\"\\nColumns\",final.columns.tolist())","execution_count":211,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"853f3b06f5ab46facab0dbcc2fe76a61d9804570"},"cell_type":"code","source":"info = pd.DataFrame(final.dtypes).T.rename(index = {0:'Column Type'})\ninfo = info.append(pd.DataFrame(final.isnull().sum()).T.rename(index = {0:'null values (nb)'}))\ninfo = info.append(pd.DataFrame(final.isnull().sum()/final.shape[0]*100).T.rename(index = {0:'null values{%}'}))\ninfo","execution_count":212,"outputs":[]},{"metadata":{"_uuid":"1264bfd55391e7c7bbcbe6e5e9223c50bfc8cda8"},"cell_type":"markdown","source":"**Mark Down has Huge amount of missing values so, we can fill the Missing Values using  *fillna(-9999)***"},{"metadata":{"trusted":true,"_uuid":"a0f6601ab0cec0caaad9992b88970b98d392d5f1"},"cell_type":"code","source":"final.fillna(-9999, inplace=True)\ninfo = pd.DataFrame(final.dtypes).T.rename(index = {0:'Column Type'})\ninfo = info.append(pd.DataFrame(final.isnull().sum()).T.rename(index = {0:'null values (nb)'}))\ninfo = info.append(pd.DataFrame(final.isnull().sum()/final.shape[0]*100).T.rename(index = {0:'null values{%}'}))\ninfo","execution_count":213,"outputs":[]},{"metadata":{"_uuid":"26b421556865d046aa7ddd76304abc2b83383801"},"cell_type":"markdown","source":"**Now**, you can check that we have fill all missing values.\n\n**Now We are going to check duplicate values and remove them**"},{"metadata":{"trusted":true,"_uuid":"e5b83be6ec76c49049fffa5f5f25415d54ca9cc0"},"cell_type":"code","source":"print(\"Duplicate Values : \",final.duplicated().sum())","execution_count":214,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b2d16960e0d2b3fde57925a5f62191696373867c"},"cell_type":"code","source":"final = final.applymap(lambda x: 1 if x ==  True  else x)\nfinal = final.applymap(lambda x: 0 if x ==  False  else x)","execution_count":215,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"505ee9c66d971ee66e139bfc1529d0718ba99c4b"},"cell_type":"code","source":"final.head()","execution_count":216,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f69993fd29ff79df25f956393f1fe59cd600d329"},"cell_type":"code","source":"#Average Sales for all store/department for Week\n\ndf_average_sales_week = final.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()\ndf_average_sales = df_average_sales_week.sort_values('Weekly_Sales', ascending=False)\n\nprint(df_average_sales[:10])\n\n#Seasonality vs Trend Analysis\nplt.figure(figsize=(15,6))\nplt.plot(df_average_sales_week.Date, df_average_sales_week.Weekly_Sales)\nplt.show()","execution_count":217,"outputs":[]},{"metadata":{"_uuid":"bc4482794f387e7280c08de4b1ce8d3242f3894c"},"cell_type":"markdown","source":"**Once again we checked the Co-relation between weekly Sales and Holiday.**"},{"metadata":{"trusted":true,"_uuid":"5df7a73768b52d6b7cabcd9959086d1a54e50320"},"cell_type":"code","source":"#Sales variation during Holidays(Store/Dept)\nholiday =  final[['Date', 'IsHoliday', 'Weekly_Sales']].copy()\nholiday =  holiday.groupby(by=['Date','IsHoliday'], as_index=False)['Weekly_Sales'].sum()\nholiday_group =  holiday.groupby(by=['IsHoliday'], as_index=False)['Weekly_Sales'].sum()\nprint( holiday_group)\n#print( holiday[:5])\n\ndef holiday_sales(df):\n    from matplotlib import pyplot as plt\n    plt.figure(figsize=(15,6))\n    labels = ['Date', 'IsHoliday_x', 'Weekly_Sales']\n    plt.title('Sales Variation During Holidays')\n    plt.plot(df.Date, df.Weekly_Sales)\n    plt.show()\n    \nholiday_sales(holiday)","execution_count":220,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ffdf69b37d58496f7d3ddf4e5385c6e9624bafa"},"cell_type":"code","source":"final['Return'] = (final['Weekly_Sales'] < 0).astype('int')\nfinal_group = final.groupby(['Return'], as_index = False)['Weekly_Sales'].sum() \nfinal_group","execution_count":222,"outputs":[]},{"metadata":{"_uuid":"b9cc539931f482fea523c5e510d36d73c6e6ceed"},"cell_type":"markdown","source":"**For the better prediction I thought to add Weekly average MarkDown across all the MarkDowns**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4ecea39db105e6f42cf5e23fd9822d1157bd7bee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb1f0dd10c0ae0b781390141c1504aa889b088c7"},"cell_type":"code","source":"#Making Avg MarkDown\nfinal['AvgMarkDown'] = final['MarkDown1'] + final['MarkDown2'] + final['MarkDown3'] + final['MarkDown4'] + final['MarkDown5']\nfinal['AvgMarkDown'] = final['AvgMarkDown'] / 5\nfinal.AvgMarkDown[378:385]","execution_count":223,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d83a1f406b68c1900f5a7f4f15915ae2f16822ce"},"cell_type":"code","source":"#Creating Weekly sales in a 4 range\n\nfinal['cum_sum'] = final.Weekly_Sales.cumsum()\nfinal['cum_perc'] = 100*final.cum_sum/final.Weekly_Sales.sum()\n\nfinal['rangeA'] = 0\nfinal['rangeA'][final['cum_perc'] <= 25] = 1\n\nfinal['rangeB'] = 0\nfinal['rangeB'][(final['cum_perc'] > 25) & (final['cum_perc'] <= 50)] = 1\n\nfinal['rangeC'] = 0\nfinal['rangeC'][(final['cum_perc'] > 50) & (final['cum_perc'] <= 75)] = 1\n\nfinal['rangeD'] = 0\nfinal['rangeD'][final['cum_perc'] > 75] = 1\n\nfinal = final.drop(['cum_perc', 'cum_sum'], 1)\n\nfinal.head(100)","execution_count":234,"outputs":[]},{"metadata":{"_uuid":"f75421fc784c70028234fe816d2b2a39194b29cf"},"cell_type":"markdown","source":"For our exploration analysis we started aggregating the weekly sales with store type because we wanted to know which Store and Type of store was having the most sales, on average"},{"metadata":{"trusted":true,"_uuid":"4f31fba392ff1a8e3b53f6f8dfd302e74b497993"},"cell_type":"code","source":"#Aggregate the Top performing stores interms of sales\ntop_stores = final.groupby(by=['Type'], as_index=False)['Weekly_Sales'].sum()\ntop_stores","execution_count":237,"outputs":[]},{"metadata":{"_uuid":"21d387cdb1f38ce0640d421017ce4285c335e770"},"cell_type":"markdown","source":"<h2 id=\"Correlation-Matrix-&amp;-Heatmap\">Correlation Matrix &amp; Heatmap<a class=\"anchor-link\" href=\"#Correlation-Matrix-&amp;-Heatmap\" target=\"_self\">Â¶</a></h2><p><strong>Moderate Positive Correlated Features:</strong></p>\n<p>Weekly_Sales vs Size: 0.24</p>\n<p>Avg MarkDown has highly positive Correlation with MarkDown 1 to MarkDown 5</p>\n<p><strong>Moderate Negative Correlated Features:</strong></p>\n<p>Unemployment vs CPI : -0.30</p>\n"},{"metadata":{"trusted":true,"_uuid":"a5b632063e248fbc818266ace8b020fb500df4ef"},"cell_type":"code","source":"clm = final[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Type', 'Size',\n                  'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'AvgMarkDown', 'rangeA', \n                  'rangeB', 'rangeC', 'rangeD', 'Return']].copy()\nclm.corr()","execution_count":242,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4091fa8c375c154b8bd127367869644ce38350a"},"cell_type":"code","source":"clm = final[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Type', 'Size',\n                   'Return']].copy()\ndef correlation_matrix(df):\n    from matplotlib import pyplot as plt\n    from matplotlib import cm as cm\n    \n    fig = plt.figure(figsize = (25,15))\n    ax1 = fig.add_subplot(111)\n    cmap = cm.get_cmap('jet', 50)\n    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n    ax1.grid(True)\n    plt.title('Store Features Correlation')\n    labels=['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Type', 'Size']\n    ax1.set_xticklabels(labels, fontsize=6)\n    ax1.set_yticklabels(labels, fontsize=6)\n    #Add colorbar to make sure to specify a tick location to match desired tick labels\n    fig.colorbar(cax, ticks=[.75, .8, .85, .90, .95, 1])\n    plt.show()\n    \ncorrelation_matrix(clm)","execution_count":249,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1277f0db5f5d76458ea1d2f13c150212294a2ed"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import preprocessing, cross_validation, svm\nfrom sklearn.linear_model import LinearRegression\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\nstyle.use('ggplot')\n\n\n#Dropping the 'Label' from  and assigning to X\nX = np.array(final.drop(['Weekly_Sales', 'Date', 'Type', 'MarkDown1', 'MarkDown4'], 1))\nX = preprocessing.scale(X)\n\n\nfinal.dropna(inplace=True)\nY = np.array(final['Weekly_Sales'])\n\n\nX_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.2)\n\nlist1 = [X_train, X_test, Y_train, Y_test]\nfor i in list1:\n    print(i.shape)\n# Training Model\n\nclf = LinearRegression(n_jobs=-1)\nclf.fit(X_train, Y_train)\naccuracy = clf.score(X_test, Y_test)\n\nprint(accuracy)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b2aa4c62602d8eea48d5ecfaf24c7305f3fa5c25"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}