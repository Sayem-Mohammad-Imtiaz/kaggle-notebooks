{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Investment Sentiment from the News\nThis notebook compares several ML models applying them to the task of classifying news headlines sentiment. The models could be used to predict the behaviour of retail investors."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame display settings\npd.set_option('display.max_colwidth', 250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Charts display settings\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = 12, 8\nplt.rcParams.update({'font.size': 11})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data analysis and visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv',\n                   header=None,\n                   names=['sentiment', 'text'],\n                   encoding='latin-1',\n                   dtype={'sentiment': 'category'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of rows and columns\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text corpus available for analysis and classification is not large. It contains less than 5,000 samples in total. Headline samples are paired with sentiment labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examples of news classes\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Headlines differ in their length and styles. Texts could consist of words or combine words with numbers and percentages. Some samples contain irregular punctuation, double spaces or extra spaces around punctuation marks."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balance of classes\nclasses_distribution = data['sentiment'].value_counts(normalize=True)\nclasses_distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset is highly imbalanced. Majority of the samples represent one category - neutral headlines. Negative news account for only 12.5% of the samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = classes_distribution.index\nvalues = classes_distribution.values\nplt.pie(values, labels=labels, startangle=90, autopct='%1.1f%%')\nplt.title('Distribution of Classes')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Headlines length\ndata['n_words'] = data['text'].str.split()  # Split sentences by spaces and convert into lists of words\ndata['n_words'] = data['n_words'].apply(lambda x: [word for word in x if len(word) > 1])  # Remove words shorter than 2 characters\ndata['n_words'] = data['n_words'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length statistics\nmin_length = data['n_words'].min()\nmax_length = data['n_words'].max()\nmean_length = data['n_words'].mean()\nmedian_length = data['n_words'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['n_words'], bins=10)\nplt.axvline(mean_length, color='red', label='Mean')\nplt.axvline(median_length, color='green', label='Median')\nplt.legend()\nplt.title('Headlines Length')\nplt.xlabel('Number of words')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text samples are not uniform in structure. Headlines length varies greately from just one word to 50 words. Mean length value is about 20 words. Distribution is slightly skewed to the right."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Sentence length: {min_length} - {max_length} words\\nMean length = {mean_length}\\nMedian length = {median_length}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examples of the shortest headlines\ndata[data['n_words'] < 5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functions for modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_heatmap(cm):\n    \"\"\"Function produces a heatmap based on the confusion matrix.\"\"\"\n    sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Reds)\n    plt.yticks(rotation=0)\n    plt.title('Confusion Matrix')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_estimator(model, name):\n    \"\"\"Function evaluates accuracy metrics for classification model.\"\"\"\n    # Prediction on the test data\n    y_pred_class = model.predict(X_test)\n    \n    acc = accuracy_score(y_test, y_pred_class)\n    print(f'{name} model accuracy: {acc}')\n    \n    conf_matrix = confusion_matrix(y_test, y_pred_class)\n    classes_names = le.classes_\n    columns = ['pred_' + name for name in classes_names]\n    indexes = ['actual_' + name for name in classes_names]\n    conf_matrix = pd.DataFrame(conf_matrix, columns=columns, index=indexes)\n    classification_heatmap(conf_matrix)\n    \n    cls_report = classification_report(y_test, y_pred_class, target_names=classes_names)\n    print(cls_report)\n    \n    try:\n        y_pred_prob = model.predict_proba(X_test)\n        roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n        print(f'ROC AUC = {roc_auc}')\n    except Exception as e:\n        print('Probability estimations and ROC AUC are not available.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_reader(model, name):\n    \"\"\"Function creates a pipeline that includes preprocessing steps\n    and a classifier, fits the model on the training data and\n    calls a function to estimate classification accuracy on the test data.\"\"\"\n    pipe = Pipeline([\n        ('vect', CountVectorizer(ngram_range=(1, 3), max_df=0.8)),  # Transform text into tokens using individual words, pairs and triplets\n        ('tfidf', TfidfTransformer()),  # Take into account word frequency\n        ('clf', model)  # Classification model passed to the function\n    ])\n    pipe.fit(X_train, y_train)\n    accuracy_estimator(pipe, name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compare base sklearn models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform categories into numbers\nle = LabelEncoder()\ny = le.fit_transform(data['sentiment'])\nle.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Withhold 20% of the original data for test purposes.\n# Take into account class imbalances during the split stratifying the data accoording to y labels.\nX_train, X_test, y_train, y_test = train_test_split(data['text'],\n                                                    y,\n                                                    stratify=y,\n                                                    test_size=0.2,\n                                                    random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and evaluate NaiveBayes model with base parameters.\nsentiment_reader(MultinomialNB(), 'NaiveBayes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score of this model is higher than the share of the most frequent class in the dataset, but confusion matrix shows that the model is largerly useless. The model fails to correctly classify negative news - the least represented category in the training set. Majority of the samples in the negative category were attributed to the wrong classes. The highest recall was shown for the neutral news. However, the model frequently confuses positive and neutral news, and in the positive category recall is very low. ROC AUC is relatively high, but this high level could not be considered as an indicator of the model's quality. For any practical business purposes it would be more useful to correctly classify the extremes (positive and negative sentiment), which lead to changes in investment behaviour, than to be pricise in labelling neutral sentiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and evaluate LogisticRegression model specifying 'class_weight' parameter as 'balanced'\n# to compensate for class imbalances. Large 'max_iter' will ensure that the model converges.\nsentiment_reader(LogisticRegression(class_weight='balanced', max_iter=1000), 'LogisticRegression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression model demonstrated higher accuracy score and higher ROC AUC compared to the previous model. This model is more adequate for the task at hand. In each category the largest numbers in the confusion matrix are located on the main diagonal. Both precision and recall scores are above 50%, which proves that the model is not just guessing and randomly assigning classes. However, prediction accuracy for negative and positive news is still not high enough to use this model for any business decisions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and evaluate SGDClassifier with linear SVM parameters and balanced class weights.\nsentiment_reader(SGDClassifier(class_weight='balanced', loss='hinge', penalty='l2', tol=None), 'SGDClassifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGDClassifier showed the highest accuracy among the three tested sklearn models, though error rate in the least represented categories is still high. Only about 59% of the actual negative news are correctly classified, which is not nearly enough recall to use for actual investment decisions. In the positive category recall is 0.66 and precision is 0.69, which is also rather mediocre.\n\nExperiments with optimizing model parameters through grid search did not resolve this issue. The data is highly imbalanced and most important classes are underrepresented. There is evidence that grid search on imbalanced classes of data does not lead to satisfactory results."},{"metadata":{},"cell_type":"markdown","source":"### Balancing the classes\nTo resolve the issue with underrepresented classes and improve recall and precision metrics we will try to balance the classes in the dataset. Two approaches are possible:\n- Make all three classes equally represented in the training and test set.\n- Reduce the number of samples in the prevailing category to make it proportional to the second most represented group.\n\nTesting showed that the first approach leads to higher increase in accuracy scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of samples in the least represented class\nquota = data['sentiment'].value_counts().min()\nquota","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New DataFrame to add equal number of samples from each class\nbalanced_data = pd.DataFrame(columns=['sentiment', 'text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce each group to the chosen number of samples\ndata_groups = data.groupby('sentiment')\nfor group in data_groups.indices:\n    reduced_class = data_groups.get_group(group)[['sentiment', 'text']].iloc[:quota, :]\n    balanced_data = balanced_data.append(reduced_class, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class distribution in the new dataset\nbalanced_data['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target values\ny = le.fit_transform(balanced_data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and test samples\nX_train, X_test, y_train, y_test = train_test_split(balanced_data['text'],\n                                                    y,\n                                                    stratify=y,\n                                                    test_size=0.2,\n                                                    random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and evaluate SGDClassifier model\nsentiment_reader(SGDClassifier(loss='hinge', penalty='l2', tol=None), 'SGDClassifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the metrics improved considerably:\n- Recall score for negative and positive categories increased to acceptable levels. However prediction accuracy for the neutral news category slightly decreased compared to the previous trial when we used all the data available for this group.\n- Precision score increased for the underrepresented groups without negative side effects to the neutral category."},{"metadata":{},"cell_type":"markdown","source":"Lets check if this model makes sense when we train it on the balanced dataset without withholding a test portion and then evaluate the accuracy on all available samples from the original unreduced dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model and train it on balanced dataset without withholding test samples.\nX = balanced_data['text']\n\npipe = Pipeline([\n        ('vect', CountVectorizer(ngram_range=(1, 3), max_df=0.8)),\n        ('tfidf', TfidfTransformer()),\n        ('clf', SGDClassifier(loss='hinge', penalty='l2', tol=None))\n    ])\npipe.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy on this training dataset\npipe.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a prediction on the unreduced original dataset\nunreduced_X = data['text']\nunreduced_y = le.transform(data['sentiment'])\npipe.score(unreduced_X, unreduced_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix\npredicted_y = pipe.predict(unreduced_X)\nconf_matrix = confusion_matrix(unreduced_y, predicted_y)\n\nclasses_names = le.classes_\ncolumns = ['pred_' + name for name in classes_names]\nindexes = ['actual_' + name for name in classes_names]\nconf_matrix = pd.DataFrame(conf_matrix, columns=columns, index=indexes)\n\nclassification_heatmap(conf_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we compare this confusion matrix with the case, when we trained the model on unbalanced dataset and checked the accuracy on the test data, there is obvious improvement. The model learned to differentiate negative news from other news categories with high degree of accuracy. However, we should remember that in this training the model saw all available negative samples. It's still unclear if this recall score will be the same in testing on any new samples.\n\nConfusion between positive and neutral news is still visible on the chart. The model did not see all positive news samples during the training process, which confirmes that this approach only partially solves the issue with imbalanced classes.\n\nWe can conclude that the best approach to solve this classification problem would be to increase the dataset and find additional text samples to add to the negative and positive sentiment categories. All categories should be more or less equally represented. Imbalances in the original data prevent the models from correctly learning distinctive features for sentiment categories. Dropping samples from overrepresented groups inreases accuracy scores but potentially harms the training process because we limit the available training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try to get a prediction for out of sample headlines with obvious sentiment.\nnew_samples = ['Experts expect the world economy to grow at a steady rate of 3% a year.',\n               'Local retailers reported much lower revenues this year. Expansion plans are suspended.']\nprediction = pipe.predict(new_samples)\nfor pred in prediction:\n    print(le.classes_[pred])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}