{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Contents of the dataset**\n\n* Id number: 1 to 214 (removed from CSV file)\n* RI: refractive index\n* Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n* Mg: Magnesium\n* Al: Aluminum\n* Si: Silicon\n* K: Potassium\n* Ca: Calcium\n* Ba: Barium\n* Fe: Iron\n* Type of glass: (class attribute) -- \n    1.  building_windows_float_processed -- \n    2. building_windows_non_float_processed -- \n    3. vehicle_windows_float_processed -- \n    4. vehicle_windows_non_float_processed (none in this database) --\n    5. containers -- \n    6. tableware -- \n    7. headlamps"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from pandas import read_csv\ndata = read_csv('../input/glass.csv')\nX = data.drop(\"Type\",axis= 1)\ny = data[\"Type\"]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import the libraries required for visualization purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation between different features**\n\n* df.corr() compute pairwise correlation of columns.Correlation shows how the two variables are related to each other.Positive values shows as one variable increases other variable increases as well. Negative values shows as one variable increases other variable decreases.Bigger the values,more strongly two varibles are correlated and viceversa.\n\n* We'll visualize correlation by making use of Seaborn library\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = X.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, annot=True)\n\nplt.title('Correlation between different fearures')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standardization of Datset**\n\n* Standardization of a dataset makesa distribution normal with mean =0 , and SD =1. Standardization is a common method used while employing machine learning algorithms, plus they give equal independent importance to the features irrespective of what dimensions the already are in"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Eigen Vectors and Eigen Value Calculation**\n\n* We create an objective function and optimize it { argmax[1/n sigma(var(uTx)^2)] } The best value of U (Ui) gives us the direction.\n* While optimizing this, we generate a co-variance matrix\n* (V, Lambda) : (eigen vectors, eigen values ) are obtained aftre the optimization.\n* Eigen vector gives the best direction and Lambda associated to it gives an approximation of the amount of data preserved,.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T)) #using its package, numpy helps us directly find the covar \ncov_matrix= np.cov(X_std.T)\nprint(cov_matrix.shape)\nprint(X.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset was of the form N x k : no. of columns.  hence covar matrix should be form k x k; which is evidently true"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.heatmap(cov_matrix, annot=True)\n\nplt.title('Correlation between different features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Eigen Values & Vector Calculation**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"eigen_vals, eigen_vects = np.linalg.eig(cov_matrix)\n\nprint('Eigenvectors \\n%s' %eigen_vects)\nprint('\\nEigenvalues \\n%s' %eigen_vals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Selecting Principal Components**\n \n * In order to decide which eigenvectors can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a list of (eigenvalue, eigenvector) tuples\neigen_pairs = [(eigen_vals[i], eigen_vects[:,i]) for i in range(len(eigen_vals))]\neigen_pairs\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neigen_pairs.sort(key=lambda x: x[0], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets find the amount of data preserved in each eigen value"},{"metadata":{"trusted":true},"cell_type":"code","source":"total= sum(eigen_vals)\n#this will helps us while finding the amount of data preserved in %\n\npreserved_percent= [(i/ total)* 100  for i in sorted(eigen_vals, reverse= True)]\npreserved_percent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, our visualization will help us analyze the amount of data preserved within all eigen_values. We'll now decide the amount of data information(variance) we decide to keep with us while performing dimensionality reduction, based on the business problem & requirements of our clients "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.bar(range(9), preserved_percent, alpha=0.5, align='center', label='individual explained variance')\nplt.ylabel('variance percentages')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, The plot  shows that maximum variance (~ 26%) can be explained by the first principal component alone. The second,third,fourth and fifth principal component share almost equal amount of information.Comparatively 6th and 7th components share less amount of information as compared to the rest of the Principal components.But those information cannot be ignored since they both contribute almost 17% of the data.But we can drop the last component as it has less than 10% of the variance"},{"metadata":{},"cell_type":"markdown","source":"For the sake of simlicity, and understanding the purpose here, let's suppose 3 principal components together comprise of 90% of variance, Hence we can drop other components. Here, we are reducing the 9-D feature space to a 3-D feature subspace, by choosing the “top 3” eigenvectors with the highest eigenvalues to construct our d×k-dimensional eigenvector matrix W"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix_v= np.hstack((eigen_pairs[0][1].reshape(9, 1), eigen_pairs[1][1].reshape(9,1), eigen_pairs[2][1].reshape(9, 1)))\n#hstack stacks arrays horizontally\nmatrix_v","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We transform our dataset into the new changed dimension: \n* F1t * v= F1' (F1:: original feature  set, F1':: new dimension)\n\n* Here, we perform X_std (n, 9) * matrix_v (9, 3) : Y (n, 3) matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y= X_std.dot(matrix_v)\n#dot product of the 2 vectors\nprint(Y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}