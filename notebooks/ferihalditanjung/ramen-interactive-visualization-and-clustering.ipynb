{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello ðŸ™Œ, welcome to my notebook. In this notebook we will try to learn Interactive Visualization and Clustering. Also try to make Geo-Map to better visualization about Ramen Stars and Review distribution around the World. Feel free if you have any question or suggestion! Thank you!\n\n![](https://realfood.tesco.com/media/images/1400x919-BeefRamen-db0570eb-10cf-437b-a497-6cd26837ee2d-0-1400x919.jpg) (www.realfood.tesco.com)"},{"metadata":{},"cell_type":"markdown","source":"- Context: The Ramen Rater is a product review website for the hardcore ramen enthusiast (or \"ramenphile\"), with over 2500 reviews to date. This dataset is an export of \"The Big List\" (of reviews), converted to a CSV format.\n\n- Content: Each record in the dataset is a single ramen product review. Review numbers are contiguous: more recently reviewed ramen varieties have higher numbers. Brand, Variety (the product name), Country, and Style (Cup? Bowl? Tray?) are pretty self-explanatory. Stars indicate the ramen quality, as assessed by the reviewer, on a 5-point scale; this is the most important column in the dataset!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata1 = pd.read_csv('../input/ramen-ratings/ramen-ratings.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Missing Value Chart'''\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(13, 3))\ndata1.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Unique Columns'''\n\ndef unique_counts(data):\n   for i in data.columns:\n       count = data[i].nunique()\n       print(i, \": \", count)\nunique_counts(data1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1['Top Ten'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Handling Missing Values'''\n\nvar = [\"Top Ten\"]\nfor i in var:\n    data1[i].fillna(0, inplace=True)\n    \ndata1.dropna(inplace=True) #removing all missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- For the data shape, we have 2580 rows, and 7 columns\n- For the data type, all columns labeled as object. Later we must change feature such as Review and Stars to numerical\n- Top ten feature is feature which have most missing values\n- We will ignore the Top Ten because it doesn't contain complete informasion about top ten ramen each year. For the missing values, i fill it with 0\n- We check duplicate and drop it\n- For other feature that have missing values, i simply drop it. Because the size is not too much."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ncustom_aggregation = {}\ncustom_aggregation[\"Style\"] = \"count\"\ndata2 = data1.groupby(\"Style\").agg(custom_aggregation)\n\ndata2.columns = [\"Count\"]\ndata2['Style'] = data2.index\n\nfig = px.bar(data2, x='Style', y=\"Count\", color=\"Style\", title=\"Number of Ramen Style\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data1, x=\"Style\", y=\"Review #\", color=\"Style\", boxmode=\"overlay\", title=\"Style x Review Boxplot\")\nfig.update_traces(quartilemethod=\"inclusive\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The most counted ramen style in this dataset is Ramen with Pack style\n- The most reviewed ramen style in this dataset is Ramen with Cup style"},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_aggregation = {}\ncustom_aggregation[\"Country\"] = \"count\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\n\ndata2.columns = [\"Count\"]\ndata2['Country'] = data2.index\n\nfig = px.bar(data2, x='Country', y=\"Count\", color=\"Country\", title=\"Number of Country\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1['Stars'][data1['Stars'] == 'Unrated'] = 0\ndata1['Stars'] = data1['Stars'].astype(float)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Stars\"] = \"mean\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\n\ndata2.columns = [\"Stars\"]\ndata2['Country'] = data2.index\ndata2['Stars'] = data2['Stars'].round(decimals=1)\n\nfig = px.bar(data2, x='Country', y=\"Stars\", color=\"Country\", title=\"Stars Rating in Each Country\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_aggregation = {}\ncustom_aggregation[\"Variety\"] = \"count\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\n\ndata2.columns = [\"Count\"]\ndata2['Country'] = data2.index\n\nfig = px.bar(data2, x='Country', y=\"Count\", color=\"Country\", title=\"Ramen Variety in Each Country\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The most country which have ramen review is Japan, USA and South Korea\n- For the stars rating, i use mean rating in each country and search for the highest rating which is Brazil, Sarawak, Malaysia and Indonesia\n- For the ramen variety, Japan, USA and South Korea have most ramen variety"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Making TF-IDV'''\n\nimport nltk, warnings\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom string import digits, punctuation\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\nX = data1[\"Variety\"].unique()\nstemmer = nltk.stem.porter.PorterStemmer()\nstopword = nltk.corpus.stopwords.words('english')\n\ndef stem_and_filter(doc):\n    tokens = [stemmer.stem(w) for w in analyzer(doc)]\n    return [token for token in tokens if token.isalpha()]\n\nanalyzer = TfidfVectorizer().build_analyzer()\nCV = TfidfVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter, min_df=0.00, max_df=0.3)  # we remove words if it appears in more than 30 % of the corpus (not found stopwords like Box, Christmas and so on)\nTF_IDF_matrix = CV.fit_transform(X)\nprint(\"TF_IDF_matrix :\", TF_IDF_matrix.shape, \"of\", TF_IDF_matrix.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''TF-IDV Embedded'''\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\n\nsvd = TruncatedSVD(n_components = 100)\nnormalizer = Normalizer(copy=False)\nTF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\nTF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\nprint(\"TF_IDF_embedded :\", TF_IDF_embedded.shape, \"of\", TF_IDF_embedded.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Silhoutte Scoring'''\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport numpy as np\n\nscore_tfidf = []\nx = list(range(5, 155, 10))\n\nfor n_clusters in x:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n    kmeans.fit(TF_IDF_embedded)\n    clusters = kmeans.predict(TF_IDF_embedded)\n    silhouette_avg = silhouette_score(TF_IDF_embedded, clusters)\n    rep = np.histogram(clusters, bins = n_clusters-1)[0]\n    score_tfidf.append(silhouette_avg)\n    \nplt.figure(figsize=(20,16))\nplt.subplot(2, 1, 1)\nplt.plot(x, score_tfidf, label=\"TF-IDF matrix\")\nplt.title(\"Evolution of the Silhouette Score\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''KMeans Clustering'''\n\nn_clusters = 70\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30, random_state=0)\nproj = kmeans.fit_transform(TF_IDF_embedded)\nclusters = kmeans.predict(TF_IDF_embedded)\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"ACP with 135 clusters\", fontsize=\"20\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''TSNE Visualization'''\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2)\nproj = tsne.fit_transform(TF_IDF_embedded)\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"Visualization of the clustering with TSNE\", fontsize=\"20\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''WordClouding'''\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport random\n\nplt.figure(figsize=(20,8))\nwc = WordCloud()\n\nfor num, cluster in enumerate(random.sample(range(60), 12)) :\n    plt.subplot(3, 4, num+1)\n    wc.generate(\" \".join(X[np.where(clusters==cluster)]))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(\"Cluster {}\".format(cluster))\n    plt.axis(\"off\")\n    \nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- For the text generation,transformation and clustering i learned alot from this kernel: ttps://www.kaggle.com/miljan/customer-segmentation. Kindly check and upvote his kernel!\n- To transforming text into feature i used TfidfVectorizer, after get the matriks we will perform dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD).\n- And then we will try Silhoutte Scoring to search the best number of cluster and visualize our clustering using TSNE\n- Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other (dzone.com)\n\n![](https://miro.medium.com/max/700/1*cUcY9jSBHFMqCmX-fp8BvQ.jpeg) (www.towardsdatascience.com)\n\n- For the variety we get 70 cluster which the best to represent number of variety of ramen in this dataset (based on Silhoutte Scoring)\n- And then try to look up the popular word in cluster (randomly) using WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Clustering Using KMeans from Selected Feature'''\n\nfeature = ['Review #', 'Stars']\ncls_ = data1[feature]\n\nfrom sklearn.cluster import KMeans\nnc = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in nc]\nscore = [kmeans[i].fit(cls_).score(cls_) for i in range(len(kmeans))]\nplt.plot(nc,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=4, random_state=0).fit(cls_)\ncls_['Cluster'] = kmeans.labels_\ndata1 = pd.merge(data1,cls_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_article_to_cluster = {article : cluster for article, cluster in zip(X, clusters)}\ncluster = data1['Variety'].apply(lambda x : dict_article_to_cluster[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = pd.concat([data1,cluster], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.columns = [\"Review\", \"Brand\", \"Variety\", \"Style\", 'Country', 'Stars', 'Top Ten', 'Cluster', 'Label'] #changing name columns\ndata1['Review'] = data1['Review'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- I try to make clustering again based on Stars and Review feature using KMeans Clustering\n- The best number of cluster to represent the Stars and Review is 4 cluster\n- And then combine the result into our dataset"},{"metadata":{"_kg_hide-output":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"cls_0 = data1[data1['Cluster']==0]\nprint(f'Mean stars rating in Cluster 0 : {round(cls_0.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 0 : {round(cls_0.Review.mean(),2)}')\nprint(cls_0.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_0.groupby('Label').size().sort_values(ascending=False)[0:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\nwc = WordCloud()\n\nimg1 = cls_0.loc[cls_0['Label'] == 15]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_0.loc[cls_0['Label'] == 4]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_0.loc[cls_0['Label'] == 53]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_0.loc[cls_0['Label'] == 10]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Cluster 0\n    - Mean stars rating: 3.74\n    - Mean review: 1602\n    - Variety: 24, 25, 39, 55\n\n*Notes: I only take 4 variety (label) to represent ramen variety for each cluster"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cls_1 = data1[data1['Cluster']==1]\nprint(f'Mean stars rating in Cluster 1 : {round(cls_1.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 1 : {round(cls_1.Review.mean(),2)}')\nprint(cls_1.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_1.groupby('Label').size().sort_values(ascending=False)[0:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\nimg1 = cls_1.loc[cls_1['Label'] == 47]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_1.loc[cls_1['Label'] == 38]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_1.loc[cls_1['Label'] == 39]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_1.loc[cls_1['Label'] == 2]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Cluster 1\n    - Mean stars rating: 3.21\n    - Mean review: 317\n    - Variety: 47, 38, 39, 2"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cls_2 = data1[data1['Cluster']==2]\nprint(f'Mean stars rating in Cluster 2 : {round(cls_2.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 2 : {round(cls_2.Review.mean(),2)}')\nprint(cls_2.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_2.groupby('Label').size().sort_values(ascending=False)[0:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\nimg1 = cls_2.loc[cls_2['Label'] == 23]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_2.loc[cls_2['Label'] == 54]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_2.loc[cls_2['Label'] == 15]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_2.loc[cls_2['Label'] == 39]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Cluster 2\n    - Mean stars rating: 3.93\n    - Mean review: 2253\n    - Variety: 23, 54, 15, 39"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cls_3 = data1[data1['Cluster']==3]\nprint(f'Mean stars rating in Cluster 3 : {round(cls_3.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 3 : {round(cls_3.Review.mean(),2)}')\nprint(cls_3.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_3.groupby('Label').size().sort_values(ascending=False)[0:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\nimg1 = cls_3.loc[cls_3['Label'] == 26]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_3.loc[cls_3['Label'] == 39]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_3.loc[cls_3['Label'] == 30]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_3.loc[cls_3['Label'] == 49]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Cluster 3\n    - Mean stars rating: 3.71\n    - Mean review: 957\n    - Variety: 26, 39, 30, 49"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data1, x=\"Cluster\", y=\"Review\", color=\"Cluster\", boxmode=\"overlay\", title=\"Review Boxplot by Cluster\")\nfig.update_traces(quartilemethod=\"inclusive\") # or \"inclusive\", or \"linear\" by default\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data1, x=\"Cluster\", y=\"Stars\", color=\"Cluster\", boxmode=\"overlay\", title=\"Review Boxplot by Cluster\")\nfig.update_traces(quartilemethod=\"inclusive\") # or \"inclusive\", or \"linear\" by default\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\nimport warnings\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nwarnings.filterwarnings(\"ignore\")\n\ncustom_aggregation = {}\ncustom_aggregation[\"Stars\"] = \"mean\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\ndata2.columns = [\"Stars\"]\n\ntemp = data2\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\n\ndata = dict(type='choropleth',\n            locations = countries.index,\n            locationmode = 'country names', \n            z = data2['Stars'],\n            text = countries.index, \n            colorbar = {'title':'Rating'},\n            colorscale=[\n            [0, \"rgb(8, 29, 88)\"], \n            [0.125, \"rgb(37, 52, 148)\"], \n            [0.25, \"rgb(34, 94, 168)\"], \n            [0.375, \"rgb(29, 145, 192)\"], \n            [0.5, \"rgb(65, 182, 196)\"], \n            [0.625, \"rgb(127, 205, 187)\"], \n            [0.75, \"rgb(199, 233, 180)\"], \n            [0.875, \"rgb(237, 248, 217)\"], \n            [1, \"rgb(255, 255, 217)\"]],    \n\n            reversescale = False)\n\nlayout = dict(title='Ramen Rating per Country',\n\ngeo = dict(showframe = True, projection={'type':'mercator'}))\n\nchoromap = go.Figure(data = [data], layout = layout)\n\niplot(choromap, validate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_aggregation = {}\ncustom_aggregation[\"Review\"] = \"mean\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\ndata2.columns = [\"Review\"]\n\ntemp = data2\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\n\ndata = dict(type='choropleth',\n            locations = countries.index,\n            locationmode = 'country names', \n            z = data2['Review'],\n            text = countries.index, \n            colorbar = {'title':'Nb. of Review'},\n            colorscale=[\n            [0, \"rgb(8, 29, 88)\"], \n            [0.125, \"rgb(37, 52, 148)\"], \n            [0.25, \"rgb(34, 94, 168)\"], \n            [0.375, \"rgb(29, 145, 192)\"], \n            [0.5, \"rgb(65, 182, 196)\"], \n            [0.625, \"rgb(127, 205, 187)\"], \n            [0.75, \"rgb(199, 233, 180)\"], \n            [0.875, \"rgb(237, 248, 217)\"], \n            [1, \"rgb(255, 255, 217)\"]],    \n            reversescale = False)\n\nlayout = dict(title='Ramen Review per Country',\n\ngeo = dict(showframe = True, projection={'type':'mercator'}))\n\nchoromap = go.Figure(data = [data], layout = layout)\n\niplot(choromap, validate=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We try to check the distribution of Review and Stars of our clustering\n- It seems that our clustering seperate the data very well!\n- And then try to make Ramen Star Rating and Review Geo-Map"},{"metadata":{},"cell_type":"markdown","source":"Dont' Forget to Upvote! Thank you!:)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}