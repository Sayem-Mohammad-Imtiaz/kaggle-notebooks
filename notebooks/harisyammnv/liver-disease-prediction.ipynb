{"nbformat":4,"cells":[{"cell_type":"markdown","source":"** Hello Kagglers!!!**\n\nThis is my first kernel, and i am a beginner so i would like inputs from you if you find anything wrong. In this kernel i followed three steps basically:\n* Exploratory study of the given data and performed value checks of the classes with the kaggle website\n* I used SVC to predict the classes\n* As the data seems to unbalanced i used the SMOTE technique and then used RandomForestClassifier to predict the classes","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data=pd.read_csv(\"../input/indian_liver_patient.csv\")\ndata.head()","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data.tail()","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data.info() # should help us to locate if there are any missing or null values","metadata":{}},{"cell_type":"markdown","source":"The column ** Albumin_and_Globulin_Ratio ** doesnot have 583 values so we need to correct this in the data preprocessing stage. Now i would like to check if the data is balanced or not by plotting a histogram","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# checking the stats\n# given in the website 416 liver disease patients and 167 non liver disease patients\n# need to remap the classes liver disease:=1 and no liver disease:=0 (normal convention to be followed)\ncount_classes = pd.value_counts(data['Dataset'], sort = True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title(\"Liver disease classes histogram\")\nplt.xlabel(\"Dataset\")\nplt.ylabel(\"Frequency\")\n","metadata":{}},{"cell_type":"markdown","source":"Have to remap the class labels for convenience, ** no liver disease then:=0 for patients having liver disease then:=1**","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data['Dataset'] = data['Dataset'].map({2:0,1:1}) ","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"** Check from the website: 416 liver disease patients and 167 normal patients**","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data['Dataset'].value_counts()","metadata":{}},{"cell_type":"markdown","source":"Now I filled in the missing values with zeros","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data['Albumin_and_Globulin_Ratio'].fillna(value=0, inplace=True)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data_features=data.drop(['Dataset'],axis=1)\ndata_num_features=data.drop(['Gender','Dataset'],axis=1)\ndata_num_features.head()","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data_num_features.describe() # check to whether feature scaling has to be performed or not ","metadata":{"scrolled":true}},{"cell_type":"markdown","source":"** From the table above as the ranges are different for different features, feature scaling has to be performed **","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ncols=list(data_num_features.columns)\ndata_features_scaled=pd.DataFrame(data=data_features)\ndata_features_scaled[cols]=scaler.fit_transform(data_features[cols])\ndata_features_scaled.head()","metadata":{}},{"cell_type":"markdown","source":"Now the categorical data has to be encoded to numerical values, here as it is one column which has to be encoded i just used the conventional pandas **get_dummies** function","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"data_exp=pd.get_dummies(data_features_scaled)\ndata_exp.head()","metadata":{}},{"cell_type":"markdown","source":"** To look at the correlations between the features heatmap with corr() function is helpful **","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\nplt.title('Pearson Correlation of liver disease Features')\n# Draw the heatmap using seaborn\nsns.heatmap(data_num_features.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black',annot=True)","metadata":{}},{"cell_type":"markdown","source":"from this we can see that **Direct_Bilirubin ** and ** Total_Bilirubin**; ** Alamine Aminotransferase ** and ** Aspartate Aminotransferase**; ** Total Protiens **  and ** Albumin** are highly correlated.\n\n**Now i use just SVC for the dataset without using any sampling techniques just to check how it performs**","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"** The function below is a courtesy from a fellow kaggler: https://www.kaggle.com/joparga3 and his kernel: https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now **","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"X=data_exp\ny=data['Dataset'] \nX_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"len(Y_train[Y_train==0])/len(Y_train[Y_train==1])","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"len(Y_test[Y_test==0])/len(Y_test[Y_test==1])","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"clf=SVC(random_state=0,kernel='rbf')\nclf.fit(X_train,Y_train)\npredictions=clf.predict(X_test)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y_test,predictions)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{}},{"cell_type":"markdown","source":"** From the confusion matrix we can see that there are zero true negatives which is not correct for the algorithm as it is unbalanced and is always predicting that the patient is having a liver disease **\n\nSo this model should be tuned","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","metadata":{}},{"cell_type":"markdown","source":"** From the ROC curve and confusion matrix we can conclude that the number of False positives should be reduced as they are wrong predictions **\n\n* So inorder to tune the model i used GridSearchCV method","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn import grid_search\nfrom sklearn.metrics import make_scorer, fbeta_score,accuracy_score\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n# Initialize the classifier\nclf = SVC(random_state=0,kernel='rbf')\n\n#  Create the parameters list you wish to tune, using a dictionary if needed.\n#  parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\nparameters = {'C': [10,50,100,200],'kernel':['poly','rbf','linear','sigmoid']}\n\n# Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(fbeta_score,beta=0.5)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = grid_search.GridSearchCV(clf,parameters,scoring=scorer,n_jobs=-1)\n\n# Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train,Y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train,Y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Report the before-and-afterscores\nprint (\"Unoptimized model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(Y_test, predictions)))\nprint (\"F-score on testing data: {:.4f}\".format(fbeta_score(Y_test, predictions, beta = 2)))\nprint (\"\\nOptimized Model\\n------\")\nprint (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(Y_test, best_predictions)))\nprint (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(Y_test, best_predictions, beta = 2)))\nprint (best_clf)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y_test,best_predictions)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{}},{"cell_type":"markdown","source":"** Now there are true negative cases. So the ROC should be better **","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, best_predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","metadata":{}},{"cell_type":"markdown","source":"** ROC curve has AUC of 0.58 which is better than the unoptimized model, but still not very good model. May be because the dataset is unbalanced the value of AUC is not improving and also the dataset size is bit small **\n\n** Now i used the oversampling technique to make the data balanced and also to increase the amount of data **","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"from imblearn.over_sampling import SMOTE\noversampler=SMOTE(random_state=0)\nos_features,os_labels=oversampler.fit_sample(X_train,Y_train)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"len(os_labels[os_labels==1])/len(os_labels[os_labels==0])","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"clf=SVC(random_state=0,kernel='rbf') # unoptimized Model\nclf.fit(os_features,os_labels)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# perform predictions on test set\npredictions=clf.predict(X_test)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y_test,predictions)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{}},{"cell_type":"markdown","source":"** The recall metric is very low. Should optimize the model to improve it **","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n#Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn import grid_search\nfrom sklearn.metrics import make_scorer, fbeta_score,accuracy_score\n#from sklearn.ensemble import RandomForestClassifier\n# TODO: Initialize the classifier\nclf = SVC(random_state=0,kernel='rbf')\n\n#  Create the parameters list you wish to tune, using a dictionary if needed.\n#  parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\nparameters = {'C': [10,50,100,200],'kernel':['poly','rbf','linear','sigmoid']}\n\n# Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(fbeta_score,beta=2)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = grid_search.GridSearchCV(clf,parameters,scoring=scorer,n_jobs=-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(os_features,os_labels)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(os_features,os_labels)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Report the before-and-afterscores\nprint (\"Unoptimized model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(Y_test, predictions)))\nprint (\"F-score on testing data: {:.4f}\".format(fbeta_score(Y_test, predictions, beta = 2)))\nprint (\"\\nOptimized Model\\n------\")\nprint (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(Y_test, best_predictions)))\nprint (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(Y_test, best_predictions, beta = 2)))\nprint (best_clf)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y_test,best_predictions)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, best_predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","metadata":{}},{"cell_type":"markdown","source":"** Even though using SMOTE technique the SVC has not performed very well. The recall metric and AUC both are roughly 0.67. This is not enough. So now i tried using RandomForestClassifier**","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"clf=RandomForestClassifier(random_state=0) # unoptimized Model\nclf.fit(os_features,os_labels)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# perform predictions on test set\npredictions=clf.predict(X_test)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y_test,predictions)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{}},{"cell_type":"markdown","source":"** The recall metric has improved from SVC but the model has to be still tuned**","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn import grid_search\nfrom sklearn.metrics import make_scorer, fbeta_score,accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n# TODO: Initialize the classifier\nclf = RandomForestClassifier(random_state=0)\n\n# TODO: Create the parameters list you wish to tune, using a dictionary if needed.\n# HINT: parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\nparameters = {'n_estimators': [100,250,500], 'max_depth': [3,6,9]}\n\n# TODO: Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(fbeta_score,beta=2)\n\n# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = grid_search.GridSearchCV(clf,parameters,scoring=scorer,n_jobs=-1)\n\n# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(os_features,os_labels)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(os_features,os_labels)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Report the before-and-afterscores\nprint (\"Unoptimized model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(Y_test, predictions)))\nprint (\"F-score on testing data: {:.4f}\".format(fbeta_score(Y_test, predictions, beta = 2)))\nprint (\"\\nOptimized Model\\n------\")\nprint (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(Y_test, best_predictions)))\nprint (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(Y_test, best_predictions, beta = 2)))\nprint (best_clf)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y_test,best_predictions)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, best_predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","metadata":{}},{"cell_type":"markdown","source":"** After performing GridSearchCV, The optimized RandomForestClassifier gave a recall metric of 0.76 and ROC curve's AUC=0.69. **\n\n** My conclusion: ** I think if the dataset is a bit bigger then may be the ensemble methods could do better. But other than the data size i could not get any possible reasons for the classifier doing this bad.\n\n**I would like to hear from you guys! If there is any problem with this. Please help me out if you find out something wrong in the procedure!**\n\nThanks","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"","metadata":{"collapsed":true}}],"metadata":{"kernelspec":{"display_name":"Python [default]","name":"python3","language":"python"},"anaconda-cloud":{},"language_info":{"name":"python","version":"3.5.2","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":1}