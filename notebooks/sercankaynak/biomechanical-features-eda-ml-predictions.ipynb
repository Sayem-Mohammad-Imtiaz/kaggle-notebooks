{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What is Orthopedic Biomechanics?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Orthopaedic biomechanics is about discovering and potentially optimizing the mechanical stresses experienced by normal, diseased, injured, or surgically treated bones, joints, and soft tissues.\n\nThis subfield of study is particularly influenced by two groups of specialists, namely, orthopaedic surgeons and biomechanical engineers. Orthopaedic surgeons are on the “clinical frontline,” as they treat patients by performing procedures like total or partial joint replacement, bone fracture repair, soft tissue repair, limb deformity correction, and bone tumor removal. Biomechanical engineers are on the “technological frontline,” as they discover the basic mechanical properties of human tissues, design and test the structural stress limits of orthopaedic implants, and develop new and improved biological and artificial biomaterials. Consequently, the strategy for conducting cutting-edge experimental research in orthopaedic biomechanics in hospitals, universities, and industry, includes a combination of orthopaedic surgery, mechanical testing, and medical imaging","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Content : \n\n1. [Load and Check Data](#1)\n2. [Variable Description](#2)\n3. [Univariate Variable Analysis](#3)\n    * [Numerical Variable](#4)\n4. [Outlier Detection](#5)\n5. [Missing Value](#6)\n    * [Find Missing Value](#7)\n6. [Visualization](#8)\n    * [Correlation Between Features](#9)\n7. [Modeling](#10)\n    * [Train-Test Split](#11)\n    * [Simple Logistic Regression](#12)\n    * [KNN Classification](#13)\n    * [K-Fold Cross Validation](#14)\n    * [Grid Search Cross Validation with Logistic Regression](#15)\n    * [Grid Search Cross Validation with KNN](#16)\n    * [Ensemble Modeling](#17)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# matplotlib\nimport matplotlib.pyplot as plt\n\n# seaborn\nimport seaborn as sns\n\n#plotly\nimport plotly.io as pio\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected = True)\nimport plotly.graph_objs as go\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id = \"1\"></a>\n# Load and Check Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_2c = pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")\ndata_2c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data_2c,hue = \"class\",palette = \"husl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2\"></a>\n# Variable Description\n\n1. Pelvic İncidence : Pelvic incidence is defined as the angle between a line perpendicular to the sacral plate at its midpoint and a line connecting this point to the femoral head axis.\n\n2. Pelvic Tilt Numeric : Pelvic tilt is the orientation of the pelvis in respect to the thighbones and the rest of the body.\n3. Lumbar Lordosis Angle : LLA is an ideal parameter for the evaluation of lumbar lordosis. The normal value of LLA can be defined as 20-45 degrees with a range of 1 SD\n\n4. Sacral Slope : The sacral slope (SS) is the angle of the sacral plateau to the horizontal. The degree of the sacral slope determines the position of the lumbar spine, since the sacral plateau forms the base of the spine.\n\n5. Pelvic Radius\n\n6. Degree Spondylolisthesis : Spondylolisthesis can be described according to its degree of severity. One commonly used description grades spondylolisthesis, with grade 1 being least advanced, and grade 5 being most advanced. The spondylolisthesis is graded by measuring how much of a vertebral body has slipped forward over the body beneath it.\n\n7. Class : Abnormal or normal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/pelvicimage1/pelvic2.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see at the image, we can clearly see the features what it is at spine cord and sacrum.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a>\n# Univariate Variable Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Categorical Variable : Class\n* Numerical Variable : pelvic_incidence, pelvic_tilt numeric, lumbar_lordosis_angle, sacral_slope, pelvic_radius, degree_spondylolisthesis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a>\n## Numerical Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def hist_plot(variable):\n    plt.figure(figsize = (9,4))\n    plt.hist(data_2c[variable],bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution\".format(variable))\n    plt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numvar = [\"pelvic_incidence\", \"pelvic_tilt numeric\", \"lumbar_lordosis_angle\", \"sacral_slope\", \"pelvic_radius\", \"degree_spondylolisthesis\"]\nfor n in numvar:\n    hist_plot(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the histogram plots,we can see the value distribution of the features that we've used in data. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a>\n# Outlier Detection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (8,8))\nsns.boxplot(data=data_2c, orient=\"h\", palette=\"Set2\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First,with using boxplot,we can figure out and see which values are outlier.\n\nThen we code a function to detect outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indexes\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        \n        # store indexes\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i,v in outlier_indices.items() if v > 1)\n    \n    return multiple_outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this function, we decide that if each feature has at least one outlier, function must detect. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.loc[detect_outliers(data_2c,[\"pelvic_incidence\", \"pelvic_tilt numeric\", \"lumbar_lordosis_angle\", \"sacral_slope\", \"pelvic_radius\", \"degree_spondylolisthesis\"])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After detecting outliers,we can drop the outliers that we've detected","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop outliers\ndata_2c = data_2c.drop(detect_outliers(data_2c,[\"pelvic_incidence\", \"pelvic_tilt numeric\", \"lumbar_lordosis_angle\", \"sacral_slope\", \"pelvic_radius\", \"degree_spondylolisthesis\"]),axis = 0).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"6\"></a>\n# Missing Value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After outlier detection,we can search the data for missing value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"7\"></a>\n## Find Missing Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.columns[data_2c.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can, there are no missing value in the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we can visualize the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"8\"></a>\n# Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"9\"></a>\n## Correlation Between Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.zeros_like(data_2c.corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\nsns.heatmap(data_2c.corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"RdBu\", #\"BuGn_r\" to reverse \n            linecolor='w',annot=True,annot_kws={\"size\":12},mask=mask,cbar_kws={\"shrink\": .9});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can say that;\n\n    - Pelvic Incidence has a positive correlation with pelvic tilt numeric\n    - Pelvic Incidence has a positive correlation with lumbar lordosis angle\n    - Pelvic Incidence has a positive correlation with sacral slope\n    - Pelvic Incidence has a negative correlation with pelvic radius\n    - Pelvic Incidence has a positive correlation with degree spondylolisthesis\n    \n    - Pelvic Tilt Numeric has a positive correlation with lumbar lordosis angle\n    - Pelvic Tilt Numeric has a positive correlation with degree spondylolisthesis\n    \n    - Lumbar Lordosis Angle has a positive correlation with sacral slope\n    - Lumbar Lordosis Angle has a positive correlation with degree spondylolisthesis\n    - Lumbar Lordosis Angle has a neagtive correlation with pelvic radius","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (12,12))\ndata_2c_melt = pd.melt(data_2c,\"class\",var_name = \"measurement\")\nsns.swarmplot(x=\"measurement\", y=\"value\", hue=\"class\",\n              palette=[\"r\", \"c\", \"y\"], data=data_2c_melt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"10\"></a>\n# Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will use ML algorithms to predict which patient has normal or abnormal features. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will use 5 Machine Learning algorithms such as;\n\n- Simple Logistic Regression\n- KNN Classification\n- K-Fold Cross Validation\n- Grid Search Cross Validation with Logistic Regression\n- Grid Search Cross Validation with KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"11\"></a>\n## Train-Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c[\"class\"] = [1 if i == \"Normal\" else 0 for i in data_2c[\"class\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normal and abnormal has converted 1 or 0 for true classification and prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data_2c[\"class\"]\nx_data = data_2c.drop([\"class\"],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train - test split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3)\n\nprint(\"x_train\",len(x_train))\nprint(\"x_test\",len(x_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"12\"></a>\n## Simple Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logisticreg = LogisticRegression()\nlogisticreg.fit(x_train,y_train)\n\nacc_log_train = round(logisticreg.score(x_train,y_train)*100,2)\nacc_log_test = round(logisticreg.score(x_test,y_test)*100,2)\nprint(\"Training = Accuracy : % {}\".format(acc_log_train))\nprint(\"Testing = Accuracy : % {}\".format(acc_log_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"13\"></a>\n## KNN Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In the beginning of classification,we choose K=3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{} nn score : {}\".format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find k value\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see best accuracies and K-values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"14\"></a>\n## K-Fold Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nknn = KNeighborsClassifier(n_neighbors=3)\naccuracies = cross_val_score(estimator=knn,X = x_train,y = y_train,cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"average accuracy : \",np.mean(accuracies))\nprint(\"average std : \",np.std(accuracies))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\nknn.fit(x_train,y_train)\nprint(\"test accuracy : \",knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"15\"></a>\n## Grid Search Cross Validation with Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"C\" : np.logspace(-3,3,7),\"penalty\" : [\"l1\",\"l2\"]} # l1= lasso  l2 = ridge\nlogisticreg = LogisticRegression()\nlogisticreg_cv = GridSearchCV(logisticreg,param_grid,cv = 10)\nlogisticreg_cv.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"tuned hyperparameters : (best parameters) :\",logisticreg_cv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"accuracy : \",logisticreg_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logisticreg2 = LogisticRegression(C = 100.0,penalty = \"l2\")\nlogisticreg2.fit(x_train,y_train)\nprint(\"score : \",logisticreg2.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"16\"></a>\n## Grid Search Cross Validation with KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = {\"n_neighbors\" : np.arange(1,50)}\nknn = KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn,grid,cv = 10)\nknn_cv.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print hyperparameter => K value in KNN algorithm\nprint(\"tuned hyperparameter K : \",knn_cv.best_params_)\nprint(\"accuracy according to tuned parameter : \", knn_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n              SVC(random_state = random_state),\n              RandomForestClassifier(random_state = random_state),\n              LogisticRegression(random_state = random_state),\n              KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                 \"max_depth\" : range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                  \"gamma\" : [0.001,0.01,0.1,1],\n                  \"C\" : [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\" : [1,3,10],\n                 \"min_samples_split\" : [2,3,10],\n                 \"min_samples_leaf\" : [1,3,10],\n                 \"bootstrap\" : [False],\n                 \"n_estimators\" : [100,300],\n                 \"criterion\" : [\"gini\"]}\n\nlogreg_param_grid = {\"C\" : np.logspace(-3,3,7),\n                     \"penalty\" : [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\" : np.linspace(1,19,10,dtype = int).tolist(),\n                  \"weights\" : [\"uniform\",\"distance\"],\n                  \"metric\" : [\"euclidean\",\"manhattan\"]}\n\nclassifier_param = [dt_param_grid,svc_param_grid,rf_param_grid,logreg_param_grid,knn_param_grid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_result = []\nbest_estimators = []\n\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid = classifier_param[i],cv = StratifiedKFold(n_splits = 10),scoring = \"accuracy\",n_jobs = -1,verbose = 1)\n    clf.fit(x_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results = pd.DataFrame({\"Cross Validation Means\": cv_result,\"ML Models\" : [\"DecisionTreeClassifier\",\"SVM\",\"RandomForestClassifier\",\n                                                                              \"LogisticRegression\",\"KNeighborsClassifier\"]})\ng  = sns.barplot(\"Cross Validation Means\",\"ML Models\",data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the plot,with Logistic Regression model,we can make the best predictions and get the best mean accuracy for the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"17\"></a>\n## Ensemble Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                         (\"rfc\",best_estimators[2]),\n                                         (\"lr\",best_estimators[3])],\n                                         voting = \"soft\",n_jobs = -1)\nvotingC = votingC.fit(x_train,y_train)\nprint(accuracy_score(votingC.predict(x_test),y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the final, we can say that we can make predcitions with 0.88 accuracy with voting in Ensemble Modeling for the data that we use.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}