{"cells":[{"metadata":{},"cell_type":"markdown","source":"# KNN Algorithms Tutorial\n1. Importing Libraries\n1. Importing the Dataset\n1. Plotting Data\n1. Preprocessing\n1. Train Test Split\n1. Feature Scaling\n1. Training and Predictions\n1. Evaluating the Algorithm\n1. Comparing Error Rate with the K Value"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore') \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing the Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\ndata3 = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_3C_weka.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no missing value ,so we dont need to perform on it ."},{"metadata":{"trusted":true},"cell_type":"code","source":"def discrete_univariate(dataset, discrete_feature):\n    fig, axarr=plt.subplots(nrows=1,ncols=2, figsize=(8,5))\n      \n    dataset[discrete_feature].value_counts().plot(kind=\"bar\",ax=axarr[0])\n    dataset[discrete_feature].value_counts().plot.pie(autopct=\"%1.1f%%\",ax=axarr[1])\n        \n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_univariate(dataset=data , discrete_feature=\"class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_univariate(dataset=data3, discrete_feature=\"class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data ,hue =\"class\",palette=\"husl\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%  Normal =1  Abnormal =0\ndata['class'] = [1 if each == \"Normal\" else 0 for each in data['class']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny = data.loc[:,'class']\n\nx1 = data.loc[:,data.columns != 'class']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalization for better understand "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (x1 - np.min(x1))/(np.max(x1)-np.min(x1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2 ,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\nBefore making any actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\n\nX_train = scaler.transform(x_train)\nX_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 23) \nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(23,knn.score(x_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating the Algorithm\nFor evaluating an algorithm, confusion matrix, precision, recall and f1 score are the most commonly used metrics. The confusion_matrix and classification_report methods of the sklearn.metrics can be used to calculate these metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results show that our KNN algorithm was able to classify all the 62 records in the test set with 81% accuracy, which is well enough. "},{"metadata":{},"cell_type":"markdown","source":"## Comparing Error Rate with the K Value\n* One way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset.\n* In this section, we will plot the mean error for the predicted values of test set for all the K values between 1 and 40."},{"metadata":{"trusted":true},"cell_type":"code","source":"error=[]\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    pred_i = knn.predict(x_test)\n    error.append(np.mean(pred_i != y_test)) \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above script executes a loop from 1 to 40. In each iteration the mean error for predicted values of test set is calculated and the result is appended to the error list.\n\nThe next step is to plot the error values against K values. Execute the following script to create the plot:"},{"metadata":{},"cell_type":"markdown","source":"The output graph looks like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output we can see that the mean error is closest to zero when the value of the K is 20 ,21 ."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}