{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objective:\nPredict whether a person will default on loan or not, based on the given attributes.\n## Meta Data-\nThe data set HMEQ reports characteristics and delinquency information for 5,960 home equity loans. A home equity loan is a loan where the obligor uses the equity of his or her home as the underlying collateral. The data set has the following characteristics:\n\n■ BAD: 1 = applicant defaulted on loan or seriously delinquent; 0= applicant paid loan (Target Variable){Assymetric Binnary Nominal}\n\n■ LOAN: Amount of the loan request{Ratio-Scaled Numeric}\n\n■ MORTDUE: Amount due on existing mortgage{Ratio-Scaled Numeric}\n\n■ VALUE: Value of current property{Ratio-Scaled Numeric}\n\n■ REASON: DebtCon = debt consolidation; Homelmp home improvement{Nominal}\n\n■ JOB: Occupational categories{Nominal}\n\n■ YOJ: Years at present job\n\n■ DEROG: Number of major derogatory reports\n\n■ DELINQ: Number of delinquent credit lines\n\n■ CLAGE: Age of oldest credit line in months\n\n■ NINQ: Number of recent credit inquiries\n\n■ CLNO: Number of credit lines\n\n■ DEBTINC: Debt-to-income ratio{Ratio-Scaled Numeric}","metadata":{"id":"HSQGsOEZdNm_"}},{"cell_type":"markdown","source":"Importing the required libraries","metadata":{"id":"laq78veMZy7z"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.naive_bayes import GaussianNB\n# !pip install ppscore\n# import ppscore as pps\nimport statsmodels.api as sm\nfrom scipy.stats import probplot\nfrom sklearn import metrics               \nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.metrics import mean_absolute_error\n%matplotlib inline\n!pip install PyCustom\nimport PyCustom\nfrom statistics import mean, stdev\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import KFold, cross_val_score, cross_validate, StratifiedKFold\nfrom sklearn.svm import SVC \nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\n\n# Avoids scroll-in-the-scroll in the entire Notebook\nfrom IPython.display import Javascript\ndef resize_colab_cell():\n  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'))\nget_ipython().events.register('pre_run_cell', resize_colab_cell)","metadata":{"id":"urNG6OnUV2qV","outputId":"3bc4cf04-0381-4890-bde8-648c9958d75d","execution":{"iopub.status.busy":"2021-07-28T15:56:50.75254Z","iopub.execute_input":"2021-07-28T15:56:50.752945Z","iopub.status.idle":"2021-07-28T15:57:02.436141Z","shell.execute_reply.started":"2021-07-28T15:56:50.752861Z","shell.execute_reply":"2021-07-28T15:57:02.435109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking csv data as input","metadata":{"id":"7zn_SAXoZy71"}},{"cell_type":"code","source":"df = pd.read_csv(\"https://raw.githubusercontent.com/akhil14shukla/IME672A-Course-Project/master/hmeq.csv\")","metadata":{"id":"05CTTb5wWKGQ","outputId":"fd578679-d518-49f1-d761-706d2b9e2825","execution":{"iopub.status.busy":"2021-07-28T15:57:02.437801Z","iopub.execute_input":"2021-07-28T15:57:02.438115Z","iopub.status.idle":"2021-07-28T15:57:02.856958Z","shell.execute_reply.started":"2021-07-28T15:57:02.438077Z","shell.execute_reply":"2021-07-28T15:57:02.855888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding the Data\n\n \n**Initial understanding of data i.e types of variables present, missing values and their distribution**","metadata":{"id":"O-NyIzL3Zy72"}},{"cell_type":"code","source":"(df.head())","metadata":{"id":"rUpt4Dd9Zy72","outputId":"6c4569f2-4133-4f1e-c86b-4f0f12ce0364","execution":{"iopub.status.busy":"2021-07-28T15:57:02.858714Z","iopub.execute_input":"2021-07-28T15:57:02.859073Z","iopub.status.idle":"2021-07-28T15:57:02.896109Z","shell.execute_reply.started":"2021-07-28T15:57:02.85904Z","shell.execute_reply":"2021-07-28T15:57:02.895064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\n(df.info())","metadata":{"id":"N0FEAAraZy73","outputId":"8ce666bd-0fb1-4598-c929-6cf0c830ed6b","execution":{"iopub.status.busy":"2021-07-28T15:57:02.897763Z","iopub.execute_input":"2021-07-28T15:57:02.89806Z","iopub.status.idle":"2021-07-28T15:57:02.924617Z","shell.execute_reply.started":"2021-07-28T15:57:02.898029Z","shell.execute_reply":"2021-07-28T15:57:02.923438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"68JTGsH56aS5"}},{"cell_type":"code","source":"(df.describe())","metadata":{"id":"HgTBSA2sZy74","outputId":"e6dcfa75-94ce-46cb-e3fe-58cec0bdcb10","execution":{"iopub.status.busy":"2021-07-28T15:57:02.926095Z","iopub.execute_input":"2021-07-28T15:57:02.926536Z","iopub.status.idle":"2021-07-28T15:57:02.979367Z","shell.execute_reply.started":"2021-07-28T15:57:02.926491Z","shell.execute_reply":"2021-07-28T15:57:02.978464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Improving visuals of plots","metadata":{"id":"11MZCYDZzz3O"}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n# plt.style.use('white')\nplt.rcParams.update({\"grid.linewidth\":0.5, \"grid.alpha\":0.5})\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)","metadata":{"id":"fSD-y-EJzz3O","outputId":"83e2c9a8-8a96-4cb8-de11-a2cdb05d0aa1","execution":{"iopub.status.busy":"2021-07-28T15:57:02.980781Z","iopub.execute_input":"2021-07-28T15:57:02.981204Z","iopub.status.idle":"2021-07-28T15:57:03.001319Z","shell.execute_reply.started":"2021-07-28T15:57:02.981163Z","shell.execute_reply":"2021-07-28T15:57:03.000425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explaining the data<br>\nFeatures of the Data:","metadata":{"id":"oodsXg73dZW9"}},{"cell_type":"code","source":"# Heatmap for null/missing values\nsns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","metadata":{"id":"TJ_TJgNqZy75","outputId":"bdf6c52c-a7d8-4649-a052-7ab61cff1889","execution":{"iopub.status.busy":"2021-07-28T15:57:03.002857Z","iopub.execute_input":"2021-07-28T15:57:03.003323Z","iopub.status.idle":"2021-07-28T15:57:03.399434Z","shell.execute_reply.started":"2021-07-28T15:57:03.003274Z","shell.execute_reply":"2021-07-28T15:57:03.398704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the data is already numerical, only two are of strings type (_REASON_ and _JOB_).","metadata":{"id":"EU1FJVsiZy75"}},{"cell_type":"code","source":"print(df[\"REASON\"].value_counts())\nprint(df[\"JOB\"].value_counts())","metadata":{"id":"cJTN_tmOZy76","outputId":"8469f03b-726d-4c3d-8976-d836ada91b9e","execution":{"iopub.status.busy":"2021-07-28T15:57:03.401502Z","iopub.execute_input":"2021-07-28T15:57:03.401897Z","iopub.status.idle":"2021-07-28T15:57:03.413659Z","shell.execute_reply.started":"2021-07-28T15:57:03.401867Z","shell.execute_reply":"2021-07-28T15:57:03.412649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We need to fill these two variable's null values and One-Hot-Encode or Label Encode them to plot their distribution efficiently.<br><br>**\nReason/Meaning of null values in REASON and JOB, and how we will fill these:<br><br>\n\nWe can replace the missing value with \n* a new value _\"Unknown\"_ \n* values based on the distribution of non-missing values\n* the most frequent value  \n* the predicted value using other attributes and Decision Tree.\n<br><br>\n[//]: # (Hello)\nREASON - This shows the reason why the person is taking the loan. There are two available values : Debt consolidation and Home Improvement. The missing value must be showing that the Reason of taking the loan was not either of the two given optins and hence was left empty. So, **filling the missing values with a new value _\"Other reason\"_, for this attribute**.<br><br>\n\n\nJOB - This tells the occupation of the applicant. There are 6 unique values for this attribute, and the value _\"Other\"_ is the most frequent. The _\"Other\"_ value is already present, so it rules out the reasoning used above. Out of the remaining options, we can **replace the missing values with the most frequent value for simplicity**<br><br>\n\nDEROG - Around 4700 have value _0_. So, **replacing the missing values with the mode** (i.e. 0) \n<br><br>\nDELINQ - Around 5200 have same value (_0_). **Replacing the missing with the mode value**. ","metadata":{"id":"EbtB1TdBZy76"}},{"cell_type":"code","source":"# We can fill the missing values with the mode, i.e. \"Other\", or we can fill the missing values depending on the distribution of the non-null values. \ndf[\"REASON\"].fillna(\"Other reason\",inplace=True)\ndf[\"JOB\"].fillna(df[\"JOB\"].mode()[0],inplace=True)\ndf[\"DEROG\"].fillna(value=0,inplace=True)        # Filling the missing value with the mode\ndf[\"DELINQ\"].fillna(value=0,inplace=True)       # Filling the missing value with the mode\n# print(df[\"JOB\"].isna().sum())","metadata":{"id":"-PrficM3WmFB","outputId":"7a5417da-115d-41fd-833b-8609c8b28c72","execution":{"iopub.status.busy":"2021-07-28T15:57:03.417029Z","iopub.execute_input":"2021-07-28T15:57:03.417309Z","iopub.status.idle":"2021-07-28T15:57:03.431767Z","shell.execute_reply.started":"2021-07-28T15:57:03.417281Z","shell.execute_reply":"2021-07-28T15:57:03.430803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_REASON_ and _JOB_ are categorical attributes and of string type. Some plots work only with numerical values, so we need to convert them to numericals. Using Label Encoding on these two variables to convert them to numerical values. (We can also use One-Hot-Encoding)","metadata":{"id":"p54WcxXbZy77"}},{"cell_type":"code","source":"# label_encoder = preprocessing.LabelEncoder()\n# df['JOB']= label_encoder.fit_transform(df['JOB'])\n# df['REASON']= label_encoder.fit_transform(df['REASON'])\ndf = df.join(pd.get_dummies(df[\"JOB\"]))\ndf = df.join(pd.get_dummies(df[\"REASON\"]))\ndf.drop([\"JOB\",\"REASON\"],axis=1,inplace=True)","metadata":{"id":"1vbtjAzoZy77","outputId":"089f8379-2a87-4dfe-a58d-73be9b7a552d","execution":{"iopub.status.busy":"2021-07-28T15:57:03.433275Z","iopub.execute_input":"2021-07-28T15:57:03.433672Z","iopub.status.idle":"2021-07-28T15:57:03.456344Z","shell.execute_reply.started":"2021-07-28T15:57:03.433642Z","shell.execute_reply":"2021-07-28T15:57:03.455386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the attributes are of numerical type,now, we can use plots to understand the distribution of all the the attributes.\n<br><br>Plotting Boxplots for each attribute in df","metadata":{"id":"bZE4-hGAZy78"}},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 4, figsize=(25, 20),)\nfor i,ax in zip(df.columns,axes.flat):\n    sns.boxplot(data=df, x=i,ax=ax)     # we can also use violin plot \nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","metadata":{"id":"mQKIwocxZy78","outputId":"618d876c-ffb1-4cfb-aea5-a560dea1dd2e","execution":{"iopub.status.busy":"2021-07-28T15:57:03.457591Z","iopub.execute_input":"2021-07-28T15:57:03.457882Z","iopub.status.idle":"2021-07-28T15:57:06.682776Z","shell.execute_reply.started":"2021-07-28T15:57:03.457853Z","shell.execute_reply":"2021-07-28T15:57:06.681673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting Histograms to see the distribution of each attribute","metadata":{"id":"2fh4TFGnZy78"}},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    sns.histplot(data=df,x=i,ax=ax,kde=True)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n","metadata":{"id":"ofooPjocZy79","outputId":"7a32a0e1-d3e9-453a-c365-020e73b726f1","execution":{"iopub.status.busy":"2021-07-28T15:57:06.684357Z","iopub.execute_input":"2021-07-28T15:57:06.684797Z","iopub.status.idle":"2021-07-28T15:57:13.184025Z","shell.execute_reply.started":"2021-07-28T15:57:06.684754Z","shell.execute_reply":"2021-07-28T15:57:13.182718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.pairplot(df, hue=\"BAD\")","metadata":{"id":"z_whW07z_IeO","outputId":"e8fb30c9-ebc3-4d29-bfd3-c9e948f14677","execution":{"iopub.status.busy":"2021-07-28T15:57:13.185307Z","iopub.execute_input":"2021-07-28T15:57:13.185648Z","iopub.status.idle":"2021-07-28T15:57:13.191538Z","shell.execute_reply.started":"2021-07-28T15:57:13.185616Z","shell.execute_reply":"2021-07-28T15:57:13.190284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    probplot(df[i],dist='norm',plot=ax)\n    ax.set_title(i)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n","metadata":{"id":"NsC7ikvFoOHX","outputId":"82ff103f-8689-425e-dd59-40b292d7ddca","execution":{"iopub.status.busy":"2021-07-28T15:57:13.192867Z","iopub.execute_input":"2021-07-28T15:57:13.193213Z","iopub.status.idle":"2021-07-28T15:57:17.489471Z","shell.execute_reply.started":"2021-07-28T15:57:13.193181Z","shell.execute_reply":"2021-07-28T15:57:17.4885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n```\n# This is formatted as code\n```\n\nTransforming a basic data transformation(x to the power 1/8) technique to try converting the current distribution to near normal distribution","metadata":{"id":"prneP732oOHZ"}},{"cell_type":"code","source":"from scipy.stats import yeojohnson\nfig, axes = plt.subplots(5, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    probplot(np.power(df[i],1/8),dist='norm',plot=ax)\n    ax.set_title(i)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","metadata":{"id":"OYtXhiUEoOHZ","outputId":"de004581-0fba-4d27-95cd-2d0288f6237b","execution":{"iopub.status.busy":"2021-07-28T15:57:17.49076Z","iopub.execute_input":"2021-07-28T15:57:17.491058Z","iopub.status.idle":"2021-07-28T15:57:21.432608Z","shell.execute_reply.started":"2021-07-28T15:57:17.491027Z","shell.execute_reply":"2021-07-28T15:57:21.431645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df_temp.copy()\n# df_temp = df_temp.join(pd.DataFrame({\"LOAN\":yeojohnson(df[\"LOAN\"])[0].reshape(-1)}))\n# df_temp\n# # pd.DataFrame({\"LOAN\":yeojohnson(df[\"LOAN\"])[0].reshape(-1)})\n","metadata":{"id":"QsbNKHQAzz3g","execution":{"iopub.status.busy":"2021-07-28T15:57:21.434299Z","iopub.execute_input":"2021-07-28T15:57:21.434729Z","iopub.status.idle":"2021-07-28T15:57:21.441483Z","shell.execute_reply.started":"2021-07-28T15:57:21.434675Z","shell.execute_reply":"2021-07-28T15:57:21.440218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations from the above plots:\n* The **scale of each attribute is different**, we need to normalize all the features. \n* Some attributes have **skewed distribution**\n* Some attributes have a lot of **outliers** (DEBTINC, LOAN, MORTDUE, VALUE)\n[//]: # (Hello)\nFor normalizing, we can use Min-Max Scaler, but attributes like LOAN, MORTDUE, VALUE have a lot of outliers (from boxplot), so we will also try Z-Score Normalization (preferred).\n<br>\nFor fixing the skewness, we need to transform the attributes. Our basic transformation did improve the distribution of some attributes like _LOAN_, _MORTDUE_ and _VALUE_.","metadata":{"id":"Q8j5MgP2Zy79"}},{"cell_type":"markdown","source":"**Numerosity Reduction** : Apart from the above needed steps, many tuple/observations might have many missing values in their attributes. We can consider dropping them to improve the data quality. For this we need to decide a threshold value, such that the data quality is also improved and a lot of data isn't lost.<br>\n**Feature Reduction** - Dropping columns with same value for most of the observations (_DELINQ_ and _DEROG_), and after considering their Correlation and Predictive Power Score(_REASON_ and _JOB_). ","metadata":{"id":"S95znw2KgKYh"}},{"cell_type":"markdown","source":"Plotting Heatmap of correlation Matrix, to understand the type of linear relation between attributes.<br>\nWe will again plot Heatmap of correlation after cleaning and transforming the attributes.<br>\n","metadata":{"id":"_3yApHh8Zy7-"}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nheat_map = sns.heatmap(df.corr(),cmap = colormap,annot=True)\nheat_map.set_yticklabels(heat_map.get_yticklabels(), rotation=0)\n# sns.color_palette(\"mako\", as_cmap=True)\nplt.show()","metadata":{"id":"TW5ls9wiZy7-","outputId":"4c41e404-f5ae-496d-dc13-6d622dd29b8c","execution":{"iopub.status.busy":"2021-07-28T15:57:21.442791Z","iopub.execute_input":"2021-07-28T15:57:21.443097Z","iopub.status.idle":"2021-07-28T15:57:23.820624Z","shell.execute_reply.started":"2021-07-28T15:57:21.443065Z","shell.execute_reply":"2021-07-28T15:57:23.819488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,25))\nimport PyCustom as pc\nmatrix_df = PyCustom.pps(df,[\"BAD\",\"JOB\",\"REASON\"])[['Feature', 'Target', 'PPS']].pivot(columns='Feature', index='Target', values='PPS')\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=colormap, linewidths=0.5,annot=True)\n# plt.show()","metadata":{"id":"JDcFe87dzz3h","outputId":"bb80e4e4-9536-4962-95c1-4648d768ce22","execution":{"iopub.status.busy":"2021-07-28T15:57:23.822206Z","iopub.execute_input":"2021-07-28T15:57:23.822879Z","iopub.status.idle":"2021-07-28T15:57:30.07061Z","shell.execute_reply.started":"2021-07-28T15:57:23.822832Z","shell.execute_reply":"2021-07-28T15:57:30.069902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning and Transformation","metadata":{"id":"-vF50VX6oOHb"}},{"cell_type":"code","source":"df[\"PROBINC\"] = df.MORTDUE/df.DEBTINC # adding new feature, (current debt on mortgage)/(debt to income ratio)","metadata":{"id":"5L1CfSipeTvI","outputId":"c31ee8f8-d405-4c20-fb11-9f0566c1f58d","execution":{"iopub.status.busy":"2021-07-28T15:57:30.071703Z","iopub.execute_input":"2021-07-28T15:57:30.072147Z","iopub.status.idle":"2021-07-28T15:57:30.0789Z","shell.execute_reply.started":"2021-07-28T15:57:30.072103Z","shell.execute_reply":"2021-07-28T15:57:30.077992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_orig = df.copy()","metadata":{"id":"fAN74gJQdZE1","outputId":"83b3aa4b-3307-41ca-a75e-d398dba78337","execution":{"iopub.status.busy":"2021-07-28T15:57:30.08003Z","iopub.execute_input":"2021-07-28T15:57:30.080303Z","iopub.status.idle":"2021-07-28T15:57:30.090376Z","shell.execute_reply.started":"2021-07-28T15:57:30.080277Z","shell.execute_reply":"2021-07-28T15:57:30.089517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yeo-johnson transformation\n* y = ((x + 1)**lmbda - 1) / lmbda,                for x >= 0, lmbda != 0\n* log(x + 1),                                  for x >= 0, lmbda = 0\n* ((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x < 0, lmbda != 2\n* log(-x + 1),                                for x < 0, lmbda = 2","metadata":{"id":"OXst2AWdHW_c"}},{"cell_type":"code","source":"df_temp = df.copy()\ndf_temp[\"LOAN\"] = yeojohnson(df[\"LOAN\"])[0]          # transforming LOAN using yeo-johnson method\ndf_1 = df_temp.copy()\ndf_temp[\"MORTDUE\"] = np.power(df[\"MORTDUE\"],1/8)     # transforming MORTDUE by raising it to 1/8\ndf_temp[\"YOJ\"] = np.log(df[\"YOJ\"]+10) \ndf_temp[\"VALUE\"] = np.log(df[\"VALUE\"]+10) \ndf_temp[\"CLNO\"] = np.log(df[\"CLNO\"]+10) \ndf_2 = df_temp.copy()","metadata":{"id":"Xz8M7x_ndXPG","outputId":"d06d0222-9b88-44fb-9d31-16ece03ab68e","execution":{"iopub.status.busy":"2021-07-28T15:57:30.091633Z","iopub.execute_input":"2021-07-28T15:57:30.091955Z","iopub.status.idle":"2021-07-28T15:57:30.119629Z","shell.execute_reply.started":"2021-07-28T15:57:30.091925Z","shell.execute_reply":"2021-07-28T15:57:30.118809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Ratio of Number of defaulters/ Number of non-defaulters : \", df[df.BAD==1].sum().sum()/df[df.BAD==0].sum().sum())","metadata":{"id":"IeCaeGiOzz3h","outputId":"33d7baeb-5b2f-4a92-da17-a23c9768bbbe","execution":{"iopub.status.busy":"2021-07-28T15:57:30.121049Z","iopub.execute_input":"2021-07-28T15:57:30.121349Z","iopub.status.idle":"2021-07-28T15:57:30.127243Z","shell.execute_reply.started":"2021-07-28T15:57:30.121323Z","shell.execute_reply":"2021-07-28T15:57:30.126592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_3 = df_2.copy()","metadata":{"id":"hd41QJGBeOB6","outputId":"c7113687-e4e2-4380-9d84-a1a04dd8cb76","execution":{"iopub.status.busy":"2021-07-28T15:57:30.130903Z","iopub.execute_input":"2021-07-28T15:57:30.131198Z","iopub.status.idle":"2021-07-28T15:57:30.140212Z","shell.execute_reply.started":"2021-07-28T15:57:30.131171Z","shell.execute_reply":"2021-07-28T15:57:30.139075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2.drop([\"HomeImp\", \"Other reason\",\"Sales\", \"Self\"],axis=1,inplace=True)","metadata":{"id":"g0glqtDxPeWE","outputId":"5625cebb-3250-4044-fcdb-64e8b7f0f7a9","execution":{"iopub.status.busy":"2021-07-28T15:57:30.142002Z","iopub.execute_input":"2021-07-28T15:57:30.14228Z","iopub.status.idle":"2021-07-28T15:57:30.153065Z","shell.execute_reply.started":"2021-07-28T15:57:30.142254Z","shell.execute_reply":"2021-07-28T15:57:30.151971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.name = 'df : original with one added feature (PROBINC)'\ndf_1.name = 'df_1 : transformed just LOAN feature'\ndf_2.name = 'df_2 : transformed LOAN, and other features too, and dropped other features (MORTDUE, YOJ), which seemed unimportant from the visualisation section'\ndf_3.name = 'df_3 :  contains transformed features, LOAN and others (MORTDUE,YOJ, etc.)'","metadata":{"id":"TxFohx5OnSsy","outputId":"6f7e8b43-9e9b-4520-9983-6122d3f34f82","execution":{"iopub.status.busy":"2021-07-28T15:57:30.154725Z","iopub.execute_input":"2021-07-28T15:57:30.155082Z","iopub.status.idle":"2021-07-28T15:57:30.165785Z","shell.execute_reply.started":"2021-07-28T15:57:30.155048Z","shell.execute_reply":"2021-07-28T15:57:30.164637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* df -> original with one added feature\n* df_1 -> transformed just _LOAN_ feature\n* df_2 -> transformed LOAN, and other features too, and dropped other features (MORTDUE, YOJ), which seemed unimportant from the visualisation section\n* df_3 -> contains transformed features, LOAN and others (MORTDUE,YOJ, etc.)","metadata":{"id":"ldCripSDlJuW"}},{"cell_type":"markdown","source":"We can consider dropping Self and Other Reason as there PPS score is 0 for every other column.","metadata":{"id":"_-XM7mRKzz3i"}},{"cell_type":"markdown","source":"We can drop MORTDUE or VALUE, as they are highly correlated and they are PPS score is also greater than 0.5\nDropping MORTDUE, because VALUE is a better predictor of BAD","metadata":{"id":"R21bXRqyzz3i"}},{"cell_type":"code","source":"def preprocess(df):\n  df1 = df.copy()\n  # Numerosity Reduction, with a threshold of 4 null values\n  df1.dropna(axis=0,thresh=4,inplace=True)\n  # Filling the rest of the null values using interpolated values, mode and median\n  df1.fillna(value=df1.interpolate(),inplace=True)\n  df1.fillna(value=df1.mode(),inplace=True)\n  df1.fillna(value=0,inplace=True)\n  # Taking out the target column before using #standard scaler\n  y = df[\"BAD\"]\n  df1.drop([\"BAD\"],axis=1,inplace=True)\n\n  # Using Standard Scaler, as it might also take care of some outliers\n  sscaler = StandardScaler()\n  scaled_features = sscaler.fit_transform(df1)\n\n  # Standard Scaler retuen a numpy array, convertig it back into a DataFrame, for ease of understanding\n  scaled_features_df = pd.DataFrame(scaled_features, index=df1.index, columns=df1.columns)\n  return scaled_features_df,y\n\ndef preprocess_min_max(df):\n  df1 = df.copy()\n  # Numerosity Reduction, with a threshold of 4 null values\n  df1.dropna(axis=0,thresh=4,inplace=True)\n  # Filling the rest of the null values using interpolated values, mode and median\n  df1.fillna(value=df1.interpolate(),inplace=True)\n  df1.fillna(value=df1.mode(),inplace=True)\n  df1.fillna(value=0,inplace=True)\n  # Taking out the target column before using #standard scaler\n  y = df[\"BAD\"]\n  df1.drop([\"BAD\"],axis=1,inplace=True)\n\n  # Using Standard Scaler, as it might also take care of some outliers\n  sscaler = MinMaxScaler()\n  scaled_features = sscaler.fit_transform(df1)\n\n  # Standard Scaler retuen a numpy array, convertig it back into a DataFrame, for ease of understanding\n  scaled_features_df = pd.DataFrame(scaled_features, index=df1.index, columns=df1.columns)\n  return scaled_features_df,y","metadata":{"id":"8pbOmXNizz3i","outputId":"1adafd6c-9f3b-4db3-b1c6-da598059414a","execution":{"iopub.status.busy":"2021-07-28T15:57:30.16704Z","iopub.execute_input":"2021-07-28T15:57:30.167335Z","iopub.status.idle":"2021-07-28T15:57:30.180851Z","shell.execute_reply.started":"2021-07-28T15:57:30.167306Z","shell.execute_reply":"2021-07-28T15:57:30.179751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chi-Square Test for ","metadata":{"id":"IAJmsE0gvKQZ"}},{"cell_type":"code","source":"x1,y1 = preprocess_min_max(df)\nchi_scores = chi2(x1,y1)  \np_values = pd.Series(chi_scores[1],index = x1.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","metadata":{"id":"hVUtFtm9vJde","outputId":"cf30767b-5e4a-48e8-a2fe-52a4f6ab0a1d","execution":{"iopub.status.busy":"2021-07-28T15:57:30.181796Z","iopub.execute_input":"2021-07-28T15:57:30.182064Z","iopub.status.idle":"2021-07-28T15:57:30.619506Z","shell.execute_reply.started":"2021-07-28T15:57:30.182039Z","shell.execute_reply":"2021-07-28T15:57:30.61825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_outlier_IQR(df):\n    Q1=df.quantile(0.25)\n    Q3=df.quantile(0.75)\n    IQR=Q3-Q1\n    df_final = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n    return df_final\n\ndf3=remove_outlier_IQR(df)\nprint(\"Number of outliers removed : \" , df.shape[0]-df3.shape[0])","metadata":{"id":"3LD3y0CVoOHb","outputId":"65fbaa73-283c-4aa4-ab57-e04c8b76a560","execution":{"iopub.status.busy":"2021-07-28T15:57:30.621128Z","iopub.execute_input":"2021-07-28T15:57:30.621546Z","iopub.status.idle":"2021-07-28T15:57:30.648673Z","shell.execute_reply.started":"2021-07-28T15:57:30.621502Z","shell.execute_reply":"2021-07-28T15:57:30.647717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 4, figsize=(15, 10),)\nfor i,ax in zip(df3.columns,axes.flat):\n    sns.boxplot(x = df3[i],ax=ax)     # we can also use violin plot \nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","metadata":{"id":"6OCmhSWFoOHj","outputId":"a418e03b-48d3-466c-c694-74688b1d43a6","execution":{"iopub.status.busy":"2021-07-28T15:57:30.649828Z","iopub.execute_input":"2021-07-28T15:57:30.650102Z","iopub.status.idle":"2021-07-28T15:57:33.363642Z","shell.execute_reply.started":"2021-07-28T15:57:30.650076Z","shell.execute_reply":"2021-07-28T15:57:33.36272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NUmber of outliers removed are too large, this decreases the data quality.","metadata":{"id":"z-X2C22Lwdcr"}},{"cell_type":"markdown","source":"PCA can be performed for both the numeric continous variables and the categorical and numeric discrete variables but it is most effective and designed for numeric continous variables.","metadata":{"id":"Rz0xDrdhoOHk"}},{"cell_type":"code","source":"pca = PCA(n_components=6)\npca_df,pca_y = preprocess(df_2)\nprincipalComponents = pca.fit_transform(pca_df)\nprincipalDf = pd.DataFrame(data = principalComponents)\nprint(pca.explained_variance_)","metadata":{"id":"joiPfvDwoOHl","outputId":"40bde513-c33f-4ee5-c1dc-79fe8309513e","execution":{"iopub.status.busy":"2021-07-28T15:57:33.364905Z","iopub.execute_input":"2021-07-28T15:57:33.365207Z","iopub.status.idle":"2021-07-28T15:57:33.480393Z","shell.execute_reply.started":"2021-07-28T15:57:33.365177Z","shell.execute_reply":"2021-07-28T15:57:33.479333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model result was not that good after using PCA with 6 Principal Components","metadata":{"id":"JT9wUB38zz3j"}},{"cell_type":"markdown","source":"Building the Model","metadata":{"id":"ihpLl8wvZy8A"}},{"cell_type":"code","source":"\n# kfold_model = LogisticRegression()\n# kf = StratifiedKFold(n_splits=5 ,shuffle=True, random_state=1)\n# kf_scores1 = []\n# kf_scores2 = []\n# xmat1 = X1_rfe.values\n# xmat2 = X2_rfe.values\n# ymat1 = y1.values\n# ymat2 = y2.values\n# for train_index, test_index in kf.split(xmat1, ymat1):\n#     X_train, y_train = xmat1[train_index] , ymat1[train_index]\n#     kfold_model.fit(X_train,y_train)\n#     y_pre = kfold_model.predict(xmat1[test_index])\n#     kf_scores1.append(accuracy_score(ymat1[test_index], y_pre))\n# print(mean(kf_scores1))\n\n# for train_index, test_index in kf.split(xmat2, ymat2):\n#     X_train, y_train = xmat2[train_index] , ymat2[train_index]\n#     kfold_model.fit(X_train,y_train)\n#     y_pre = kfold_model.predict(xmat2[test_index])\n#     kf_scores2.append(accuracy_score(ymat2[test_index], y_pre))\n\n# print(mean(kf_scores2))","metadata":{"id":"TiX8g1DQzz3k","execution":{"iopub.status.busy":"2021-07-28T15:57:33.482216Z","iopub.execute_input":"2021-07-28T15:57:33.482958Z","iopub.status.idle":"2021-07-28T15:57:33.490371Z","shell.execute_reply.started":"2021-07-28T15:57:33.48291Z","shell.execute_reply":"2021-07-28T15:57:33.489289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining plot_roc function function\n\ndef plot_roc(y_test, y_score):\n    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(\"ROC plot for loan defaulter prediction\")\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"id":"W5m6HPhbzz3k","outputId":"d8897588-1907-40f4-c516-12574a50620b","execution":{"iopub.status.busy":"2021-07-28T15:57:33.492211Z","iopub.execute_input":"2021-07-28T15:57:33.493094Z","iopub.status.idle":"2021-07-28T15:57:33.506785Z","shell.execute_reply.started":"2021-07-28T15:57:33.493046Z","shell.execute_reply":"2021-07-28T15:57:33.505559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotconfusionmatrix(y, y_pred):\n    cf_matrix = confusion_matrix(y, y_pred)\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n    plt.show()","metadata":{"id":"FA2mpp5uzz3k","outputId":"58d27bd6-5b81-4cf6-b22d-7bd1fb63a642","execution":{"iopub.status.busy":"2021-07-28T15:57:33.508944Z","iopub.execute_input":"2021-07-28T15:57:33.509849Z","iopub.status.idle":"2021-07-28T15:57:33.525216Z","shell.execute_reply.started":"2021-07-28T15:57:33.509797Z","shell.execute_reply":"2021-07-28T15:57:33.524045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_analysis(model,x_test, y_true,y_pred):\n    plot_roc(y_true, y_pred)\n    plotconfusionmatrix(y_true, y_pred)\n    print(classification_report(y_test, y_pred))","metadata":{"id":"SNSqlRWYzz3k","outputId":"db73b5db-0b5e-4109-8ac0-188744719d6a","execution":{"iopub.status.busy":"2021-07-28T15:57:33.527308Z","iopub.execute_input":"2021-07-28T15:57:33.528087Z","iopub.status.idle":"2021-07-28T15:57:33.537589Z","shell.execute_reply.started":"2021-07-28T15:57:33.528038Z","shell.execute_reply":"2021-07-28T15:57:33.536507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_and_analysis(df,models):\n  scaled_features_df,y = preprocess(df)\n  x_train, x_test, y_train, y_test = train_test_split(scaled_features_df,y)\n  for i in models:\n    model = i()\n    model.fit(x_train,y_train)\n    print(str(type(model)).split(\".\")[-1][:-2])\n    print(\"Accuracy on Training Dataset : \",model.score(x_train,y_train))\n    print(\"Accuracy on CV Dataset : \",model.score(x_test,y_test))\n    model_analysis(model,x_test,y_test,model.predict(x_test))\n    print(\"\\n \\n \\n \\n\")","metadata":{"id":"JhiEXd98rPQG","outputId":"1e39f0b3-bca6-4a45-be0d-4565ac3eb2e2","execution":{"iopub.status.busy":"2021-07-28T15:57:33.539379Z","iopub.execute_input":"2021-07-28T15:57:33.540338Z","iopub.status.idle":"2021-07-28T15:57:33.557455Z","shell.execute_reply.started":"2021-07-28T15:57:33.540211Z","shell.execute_reply":"2021-07-28T15:57:33.55617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dividing the dataset into training and cross-validation\n# y = df[\"BAD\"]\n# df.drop([\"BAD\"],axis=1,inplace=True)\nscaled_features_df,y = preprocess(df_2)\nx_train, x_test, y_train, y_test = train_test_split(scaled_features_df,y)","metadata":{"id":"kGz9iqahZy8B","outputId":"e15b1b20-d297-4f89-cee6-e73ce745b658","execution":{"iopub.status.busy":"2021-07-28T15:57:33.559569Z","iopub.execute_input":"2021-07-28T15:57:33.560377Z","iopub.status.idle":"2021-07-28T15:57:33.624029Z","shell.execute_reply.started":"2021-07-28T15:57:33.560323Z","shell.execute_reply":"2021-07-28T15:57:33.622871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_and_analysis(df_2,[LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, SVC, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier])","metadata":{"id":"s_JIMhBUr7zE","outputId":"db2c9c29-4e9f-4a44-dc4c-4cdd0159b408","execution":{"iopub.status.busy":"2021-07-28T15:57:33.625217Z","iopub.execute_input":"2021-07-28T15:57:33.625522Z","iopub.status.idle":"2021-07-28T15:57:41.741627Z","shell.execute_reply.started":"2021-07-28T15:57:33.625493Z","shell.execute_reply":"2021-07-28T15:57:41.740581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Our objective is to minimize company loss, predicting the risk of client default, a good recall rate is desirable because we want to identify the maximum amount of clients that are indeed prone to stop paying their debts, thus, we are pursuing a small number of False Negatives.**","metadata":{"id":"4maC83NSzz3l"}},{"cell_type":"markdown","source":"Testing the models on cross-validation dataset, and comparing with training dataset","metadata":{"id":"MgXELlFwZy8B"}},{"cell_type":"markdown","source":"Analysis of Transformations:","metadata":{"id":"rIeMO6K0txv2"}},{"cell_type":"code","source":"def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n\ndef analysis(df,y):\n  x_train,x_test, y_train,y_test = train_test_split(df,y)\n  classifiers = [\n      KNeighborsClassifier(3),\n      SVC(probability=True),\n      DecisionTreeClassifier(),\n      RandomForestClassifier(),\n      AdaBoostClassifier(),\n      GradientBoostingClassifier(),\n      GaussianNB(),\n      LinearDiscriminantAnalysis(),\n      QuadraticDiscriminantAnalysis(),\n      LogisticRegression()\n  ]\n\n  log_cols = [\"Classifier\", \"Accuracy\"]\n  log = pd.DataFrame(columns=log_cols)\n\n  acc_dict = {}\n\n  for clf in classifiers:\n      name = clf.__class__.__name__\n      clf.fit(x_train, y_train)\n      train_predictions = clf.predict(x_test)\n      acc = accuracy_score(y_test, train_predictions)\n\n      if name in acc_dict:\n          acc_dict[name] += acc\n      else:\n          acc_dict[name] = acc\n\n  for clf in acc_dict:\n      acc_dict[clf] = acc_dict[clf]\n      log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns = log_cols)\n      log = log.append(log_entry)\n\n  plt.xlabel('Accuracy')\n  plt.title('Classifier Accuracy')\n\n  sns.set_color_codes(\"muted\")\n  barplots = sns.barplot(x = 'Accuracy', y = 'Classifier', data = log, color = \"b\")\n  for p in barplots.patches:\n    barplots.annotate(\"%.4f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")\n  plt.show()\n  print(acc_dict)","metadata":{"id":"fmBHHlKOzz3r","outputId":"b0ebae86-5f14-41fd-cc45-2b9d1e5bb16c","execution":{"iopub.status.busy":"2021-07-28T15:57:41.743056Z","iopub.execute_input":"2021-07-28T15:57:41.743703Z","iopub.status.idle":"2021-07-28T15:57:41.763659Z","shell.execute_reply.started":"2021-07-28T15:57:41.743646Z","shell.execute_reply":"2021-07-28T15:57:41.762827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing various transformaton, to observe their effect on performance","metadata":{"id":"427f3jOSpYxh"}},{"cell_type":"code","source":"for i in [df, df_1,df_2,df_3]:\n  x1,y1 = preprocess(i)\n  print(\"For dataset \", i.name)\n  analysis(x1,y1)\n  print()\n  print()","metadata":{"id":"mw9hTi8PgvS8","outputId":"48a3d5c8-9ac6-45b0-9d79-cf6d62bee4ca","execution":{"iopub.status.busy":"2021-07-28T15:57:41.764892Z","iopub.execute_input":"2021-07-28T15:57:41.765492Z","iopub.status.idle":"2021-07-28T15:58:08.100843Z","shell.execute_reply.started":"2021-07-28T15:57:41.765449Z","shell.execute_reply":"2021-07-28T15:58:08.099737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chi-square future selection on Logistic Regression","metadata":{"id":"joOsyOtbHvHd"}},{"cell_type":"code","source":"x1,y1 = preprocess_min_max(df)\ndf_new = pd.DataFrame(SelectKBest(chi2, k=8).fit_transform(x1,y1))","metadata":{"id":"mxg5EaR9hkVM","outputId":"0065cec0-d841-49ec-a2b7-cadcc00f9ef5","execution":{"iopub.status.busy":"2021-07-28T15:58:08.102239Z","iopub.execute_input":"2021-07-28T15:58:08.10257Z","iopub.status.idle":"2021-07-28T15:58:08.164281Z","shell.execute_reply.started":"2021-07-28T15:58:08.10254Z","shell.execute_reply":"2021-07-28T15:58:08.163009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df_new,y1)","metadata":{"id":"1QYce47riRJ3","outputId":"b143300c-7b98-42a7-8a09-c2454455ca9d","execution":{"iopub.status.busy":"2021-07-28T15:58:08.165679Z","iopub.execute_input":"2021-07-28T15:58:08.166176Z","iopub.status.idle":"2021-07-28T15:58:08.17466Z","shell.execute_reply.started":"2021-07-28T15:58:08.16613Z","shell.execute_reply":"2021-07-28T15:58:08.173778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()       # tuning hyperparameters : penalty=\"l1\",solver=\"liblinear\",max_iter=100,C=0.004\nlr.fit(x_train,y_train)\nprint(\"Accuracy on Training Dataset : \",lr.score(x_train,y_train))\nprint(\"Accuracy on CV Dataset : \",lr.score(x_test,y_test))\nmodel_analysis(lr,x_test,y_test,lr.predict(x_test))","metadata":{"id":"3bMdVlbGiMAf","outputId":"85e09946-f322-409c-ac01-3ee0723a5525","execution":{"iopub.status.busy":"2021-07-28T15:58:08.175985Z","iopub.execute_input":"2021-07-28T15:58:08.176538Z","iopub.status.idle":"2021-07-28T15:58:08.75616Z","shell.execute_reply.started":"2021-07-28T15:58:08.176505Z","shell.execute_reply":"2021-07-28T15:58:08.755169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Please Upvote if you liked the notebook**","metadata":{}}]}