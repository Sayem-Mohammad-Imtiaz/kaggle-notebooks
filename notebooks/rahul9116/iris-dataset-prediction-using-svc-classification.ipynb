{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T13:20:07.375011Z","iopub.execute_input":"2021-06-02T13:20:07.375449Z","iopub.status.idle":"2021-06-02T13:20:07.399757Z","shell.execute_reply.started":"2021-06-02T13:20:07.37536Z","shell.execute_reply":"2021-06-02T13:20:07.398767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = pd.read_csv('/kaggle/input/iris-flower-dataset/IRIS.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:58:59.046049Z","iopub.execute_input":"2021-06-02T13:58:59.046724Z","iopub.status.idle":"2021-06-02T13:58:59.090823Z","shell.execute_reply.started":"2021-06-02T13:58:59.046651Z","shell.execute_reply":"2021-06-02T13:58:59.08981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To display first 5 rows of the dataset\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:59:02.08223Z","iopub.execute_input":"2021-06-02T13:59:02.082621Z","iopub.status.idle":"2021-06-02T13:59:02.121613Z","shell.execute_reply.started":"2021-06-02T13:59:02.082582Z","shell.execute_reply":"2021-06-02T13:59:02.120555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the data format in the dataset\ndataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:59:48.829624Z","iopub.execute_input":"2021-06-02T13:59:48.8302Z","iopub.status.idle":"2021-06-02T13:59:48.865996Z","shell.execute_reply.started":"2021-06-02T13:59:48.830138Z","shell.execute_reply":"2021-06-02T13:59:48.864637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking if the missing values are present in the dataset\ndataset.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:59:53.61002Z","iopub.execute_input":"2021-06-02T13:59:53.610388Z","iopub.status.idle":"2021-06-02T13:59:53.623833Z","shell.execute_reply.started":"2021-06-02T13:59:53.610355Z","shell.execute_reply":"2021-06-02T13:59:53.622691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the from the result above that our dataset has no missing values, which is a good thing since we won't need to do any extra steps to take care of them.","metadata":{}},{"cell_type":"code","source":"#Seperating the dependant and independent data into different variables\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:00:05.342339Z","iopub.execute_input":"2021-06-02T14:00:05.342823Z","iopub.status.idle":"2021-06-02T14:00:05.349914Z","shell.execute_reply.started":"2021-06-02T14:00:05.342778Z","shell.execute_reply":"2021-06-02T14:00:05.348571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the data into train and test datasets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:00:08.746894Z","iopub.execute_input":"2021-06-02T14:00:08.74772Z","iopub.status.idle":"2021-06-02T14:00:10.294805Z","shell.execute_reply.started":"2021-06-02T14:00:08.747655Z","shell.execute_reply":"2021-06-02T14:00:10.293767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While splitting the data into train and test dataset, the test_size of 0.2 represents that we'll be using 20% of randomly chosen data to test our model whereas rest 80% of data will be used to train our model","metadata":{}},{"cell_type":"code","source":"#Training the Dataset using Support Vector Classification Model\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:00:14.926463Z","iopub.execute_input":"2021-06-02T14:00:14.926967Z","iopub.status.idle":"2021-06-02T14:00:15.088178Z","shell.execute_reply.started":"2021-06-02T14:00:14.926924Z","shell.execute_reply":"2021-06-02T14:00:15.086783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting the dependant values using test dataset\ny_pred = classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:00:21.141401Z","iopub.execute_input":"2021-06-02T14:00:21.141943Z","iopub.status.idle":"2021-06-02T14:00:21.148405Z","shell.execute_reply.started":"2021-06-02T14:00:21.141891Z","shell.execute_reply":"2021-06-02T14:00:21.14709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comparing the actual values with predicting values\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:00:37.504037Z","iopub.execute_input":"2021-06-02T14:00:37.504624Z","iopub.status.idle":"2021-06-02T14:00:37.51207Z","shell.execute_reply.started":"2021-06-02T14:00:37.504576Z","shell.execute_reply":"2021-06-02T14:00:37.510891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating confusion matrix to check no of incorrect predictions and checking the accuracy of the predicted values against the actual values\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:00:50.393329Z","iopub.execute_input":"2021-06-02T14:00:50.393787Z","iopub.status.idle":"2021-06-02T14:00:50.410915Z","shell.execute_reply.started":"2021-06-02T14:00:50.393751Z","shell.execute_reply":"2021-06-02T14:00:50.408839Z"},"trusted":true},"execution_count":null,"outputs":[]}]}