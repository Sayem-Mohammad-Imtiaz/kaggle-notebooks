{"cells":[{"cell_type":"markdown","metadata":{},"source":"# ISIS Twitter data"},{"cell_type":"markdown","metadata":{},"source":"Checking for top20 1-5grams as well as top20 mentions of usernames (@), tags (#) and urls."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"data = pd.read_csv('../input/tweets.csv')\ndata.info()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"data.head()"},{"cell_type":"markdown","metadata":{},"source":"The features were already nicely explained by [Violinbeats - Exploring ISIS(?) Tweets](https://www.kaggle.com/violinbeats/d/kzaman/how-isis-uses-twitter/notebook-0427671092ae887aa87e)."},{"cell_type":"markdown","metadata":{},"source":"## Creating function to extract different items from tweets"},{"cell_type":"markdown","metadata":{},"source":"Using [Marco Bonzaninis](https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/) function for preprocessing from his \"mining twitter\" introduction."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"regex_str = [\n    r'<[^>]+>', # HTML tags\n    r'(?:@[\\w_]+)', # @-mentions\n    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n \n    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n    r'(?:[\\w_]+)', # other words\n    r'(?:\\S)' # anything else\n]\n    \ntokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n \ndef tokenize(s):\n    return tokens_re.findall(s)\n \ndef preprocess(s, lowercase=False):\n    tokens = tokenize(s)\n    if lowercase:\n        tokens = [token.lower() for token in tokens]\n    return tokens"},{"cell_type":"markdown","metadata":{},"source":"# N-Grams"},{"cell_type":"markdown","metadata":{},"source":"Stopwords will only be removed for 1-gram but not for 2-5 grams to get the n-grams as they were written."},{"cell_type":"markdown","metadata":{},"source":"## 1-gram"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"vect1 = CountVectorizer(analyzer=\"word\", stop_words=\"english\", min_df=200, decode_error=\"ignore\", ngram_range=(1, 1), dtype=np.int32)\n\n# applying Vectorizer to preprocessed tweets\nsub11 = vect1.fit_transform(data[\"tweets\"].map(lambda x: \" \".join(preprocess(x, lowercase = True))).tolist())\n\n# creating (word, count) list\nsub12 = zip(vect1.get_feature_names(), np.asarray(sub11.sum(axis = 0)).ravel())\n\n# getting Top20 words\nsorted(sub12, key = lambda x: x[1], reverse = True)[0:20]"},{"cell_type":"markdown","metadata":{},"source":"## 2-gram"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"vect2 = CountVectorizer(analyzer=\"word\", min_df=2, decode_error=\"ignore\", ngram_range=(2, 2), dtype=np.int32)\n\n# applying Vectorizer to preprocessed tweets\nsub21 = vect2.fit_transform(data[\"tweets\"].map(lambda x: \" \".join(preprocess(x, lowercase = True))).tolist())\n\n# creating (word, count) list\nsub22 = zip(vect2.get_feature_names(), np.asarray(sub21.sum(axis = 0)).ravel())\n\n# getting Top20 words\nsorted(sub22, key = lambda x: x[1], reverse = True)[0:20]"},{"cell_type":"markdown","metadata":{},"source":"## 3-gram"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"vect3 = CountVectorizer(analyzer=\"word\", min_df=2, decode_error=\"ignore\", ngram_range=(3, 3), dtype=np.int32)\n\n# applying Vectorizer to preprocessed tweets\nsub31 = vect3.fit_transform(data[\"tweets\"].map(lambda x: \" \".join(preprocess(x, lowercase = True))).tolist())\n\n# creating (word, count) list\nsub32 = zip(vect3.get_feature_names(), np.asarray(sub31.sum(axis = 0)).ravel())\n\n# getting Top20 words\nsorted(sub32, key = lambda x: x[1], reverse = True)[0:20]"},{"cell_type":"markdown","metadata":{},"source":"## 4-gram"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"vect4 = CountVectorizer(analyzer=\"word\", min_df=2, decode_error=\"ignore\", ngram_range=(4, 4), dtype=np.int32)\n\n# applying Vectorizer to preprocessed tweets\nsub41 = vect4.fit_transform(data[\"tweets\"].map(lambda x: \" \".join(preprocess(x, lowercase = True))).tolist())\n\n# creating (word, count) list\nsub42 = zip(vect4.get_feature_names(), np.asarray(sub41.sum(axis = 0)).ravel())\n\n# getting Top20 words\nsorted(sub42, key = lambda x: x[1], reverse = True)[0:20]"},{"cell_type":"markdown","metadata":{},"source":"## 5-gram"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"vect5 = CountVectorizer(analyzer=\"word\", min_df=2, decode_error=\"ignore\", ngram_range=(5, 5), dtype=np.int32)\n\n# applying Vectorizer to preprocessed tweets\nsub51 = vect5.fit_transform(data[\"tweets\"].map(lambda x: \" \".join(preprocess(x, lowercase = True))).tolist())\n\n# creating (word, count) list\nsub52 = zip(vect5.get_feature_names(), np.asarray(sub51.sum(axis = 0)).ravel())\n\n# getting Top20 words\nsorted(sub52, key = lambda x: x[1], reverse = True)[0:20]"},{"cell_type":"markdown","metadata":{},"source":"# @/usernames in tweets"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"tags = data[\"tweets\"].map(lambda x: [tag for tag in preprocess(x, lowercase=True) if tag.startswith('@')])\ntags = sum(tags, [])\ntags[0:5]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Top20\nCounter(tags).most_common(20)"},{"cell_type":"markdown","metadata":{},"source":"# # in tweets"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"hashs = data[\"tweets\"].map(lambda x: [hashs for hashs in preprocess(x, lowercase=True) if hashs.startswith('#')])\nhashs = sum(hashs, [])\nhashs[0:5]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Top20\nCounter(hashs).most_common(20)"},{"cell_type":"markdown","metadata":{},"source":"# urls in tweets"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"urls = data[\"tweets\"].map(lambda x: [url for url in preprocess(x, lowercase=True) if url.startswith('http:') or url.startswith('https:')])\nurls = sum(urls, [])\nurls[0:5]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Top20\nCounter(urls).most_common(20)"},{"cell_type":"markdown","metadata":{},"source":"Because of the limited number of characters in a tweet it looks like some urls are separated over multiple tweets."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}