{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9a6e175c-2406-8303-5a92-3f6f5468ea2d"},"source":"First, let's load the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0adac5fd-2695-3e6b-ed29-b2a7c91a4e74"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nglassdata = pd.read_csv('../input/glass.csv') \n\nnp.unique(glassdata['Type'])\n# Notice that in the description, it has 1-7 types, but the dataset does not have Type 4"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"828957f7-b531-c622-a716-b44615ca61de"},"outputs":[],"source":"# look at the first 5 rows of data and their names \nprint(glassdata.shape)\nglassdata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9233e42c-9117-fc47-109c-453c1f8f831c"},"outputs":[],"source":"# Store unqiue types\ntypes = glassdata.Type.unique()\n\nalpha = 0.7 # training data ratio\n\n# Splitting glassdata to training and test data\ntrain = pd.DataFrame()\ntest = pd.DataFrame()\nfor i in range(len(types)):\n    tempt = glassdata[glassdata.Type == types[i]]\n    train = train.append(tempt[0:int(alpha*len(tempt))])\n    test = test.append(tempt[int(alpha*len(tempt)): len(tempt)])\n    # test.append(tempt[int(alpha*len(tempt)): len(tempt)])\n\n# Check whether the dimension match\nprint (train.shape, test.shape, glassdata.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b88b57b3-d661-a3f3-7707-488c02ae8eb3"},"outputs":[],"source":"# Take a look at the train data\ntrain.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f1bd24d-989d-c21a-b8e2-76db81a66f15"},"outputs":[],"source":"# Both Ba and Fe has too many 0, we will ignore them\n\nprint ((train['Ba']==0).sum()/len(train))\nprint ((train['Fe']==0).sum()/len(train))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"627fd600-91f1-069c-a226-83adf622ffa8"},"outputs":[],"source":"\n# Construct correlation matrix for all variables, since Ba and Fe are mostly 0s, we can ignore them\n\ntrain_variables = train.drop(['Type','Ba', 'Fe'],1)\ntrain_variable_corrmat = train_variables.corr()\nprint (train_variable_corrmat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b55c8fa-7bf0-aa7f-e4c3-ea0ecaaf3d7e"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8,8))\nsns.pairplot(train_variables,palette='coolwarm')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e417d23f-2f58-b664-d928-726ab9cbf974"},"outputs":[],"source":"# Visualize the corrlation matrix\n\ncorr = train_variables.corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(corr, cbar = True,  square = True, annot=True,\n           xticklabels= corr.columns.values, yticklabels= corr.columns.values,\n           cmap= 'coolwarm')\nplt.show()\n\nprint(corr)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a180d05-599a-39f1-f5e8-267ff9856cb0"},"outputs":[],"source":"# Find two variables that has the least correlation \n# (negative correlation counts as a 'good' correlation) \n# So we will find two variables whose correlation is closest to 0\nimport numpy as np\ncorr_min = abs(train_variable_corrmat).values.min()\nnp.where(abs(train_variable_corrmat)==corr_min)\n\ntrain_variables.columns[[3,4]]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a5c39e7-7afa-cc22-2c12-2baec12adf26"},"outputs":[],"source":"# Use Logistic Regression on All data\nX = train.drop('Type',1)\nY = train['Type']\nZ = test.drop('Type',1)\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nprediction = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, Y).predict(Z)\n# prediction = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y).predict(Z)\n\nprediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39030751-fae5-9e1c-d500-f709c0ed726d"},"outputs":[],"source":"# Calculate Accuracy\ntruth = test['Type']\ntruth = np.array(truth)\n\naccuracy = sum(prediction == truth)/(len(truth))\naccuracy\n\n# Pretty bad"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e36c8787-41ca-9179-6533-4848a1bb6bf2"},"outputs":[],"source":"# Use Logistic Regression on Al and Si only\nX1 = train[['Al', 'Si']]\nY1 = train['Type']\nZ1 = test[['Al', 'Si']]\n\n\nprediction1 = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X1, Y1).predict(Z1)\n\n\ntruth = test['Type']\ntruth = np.array(truth)\n\naccuracy = sum(prediction1 == truth)/(len(truth))\naccuracy\n\n# Still bad"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db2c90e8-a857-651c-249c-35df47d1c6bb"},"outputs":[],"source":"# It seems like by selecting Al and Si, the prediction is not any better.\n# Let's plot the histagram for each variables with respect types \nimport matplotlib.pyplot as plt\n\ntrain.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a84d357c-7fb2-7035-ee6f-450ddd531f30"},"outputs":[],"source":"# Average Bar Plot for each variables \ntypes = np.unique(train['Type'])\n\nfor i in range(len(types)):\n    fig = plt.figure()\n    average = train[[train.columns[i], \"Type\"]].groupby(['Type'],as_index=False).mean()\n    sns.barplot(x = 'Type', y = train.columns[i], data= average)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"734f9f2b-20b7-3f14-70b8-afea5a163adc"},"outputs":[],"source":"# By looking at the mean plot above, we can see the significant difference in Mg, Al and K. \n# Go ahead and plot them in 3D \n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111,projection='3d')\n\nfor i in range(len(types)+1):\n    count = i+1\n    train_tempt = train.loc[train['Type'] == count]\n    x = train_tempt['Mg']\n    y = train_tempt['Al']\n    z = train_tempt['K']\n    \n    ax.scatter(x, y, z, c= [float(i)/float(len(types)), 0.0, float(len(types)-i)/float(len(types))], marker='o')\n    \n    ax.set_xlabel(str('Mg'))\n    ax.set_ylabel(str('Al'))\n    ax.set_zlabel(str('K')) \n\n    \n   \nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bd475c2-8581-8281-7def-4b3fdc3d933c"},"outputs":[],"source":"# Since there is a red dot that is higher than the other dots, which mess up the scale\n# it will be better to show the figures individually\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n\nfor i in range(len(types)+1):\n    fig = plt.figure()\n    ax = fig.add_subplot(111,projection='3d')\n    count = i+1\n    train_tempt = train.loc[train['Type'] == count]\n    x = train_tempt['Mg']\n    y = train_tempt['Al']\n    z = train_tempt['K']\n    \n    ax.scatter(x, y, z, c= [float(i)/float(len(types)), 0.0, float(len(types)-i)/float(len(types))], marker='o')\n    \n    ax.set_xlabel(str('Mg')+str(i+1))\n    ax.set_ylabel(str('Al')+str(i+1))\n    ax.set_zlabel(str('K')+str(i+1)) \n\n    \n   \nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"240258e3-14e0-37cc-001f-ca228703b544"},"outputs":[],"source":"# Use Logistic Regression on Al, K and Mg\nX1 = train[['Al', 'K', 'Mg']]\nY1 = train['Type']\nZ1 = test[['Al', 'K', 'Mg']]\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nprediction1 = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X1, Y1).predict(Z1)\n\n\ntruth = test['Type']\ntruth = np.array(truth)\n\naccuracy = sum(prediction1 == truth)/(len(truth))\nprint (accuracy)\nprint (prediction1)\nprint (truth)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a66c49c4-852e-d4cf-daec-c9efaf4f3014"},"outputs":[],"source":"# It's getting better, but still not good at all. \n# One of the reasons is that it has very few data for type 3,5 and 6.\n# We need to find a way to fix it NEXT TIME\n\n# However, we can still try change alpha to see what will happen\n\n# Store unqiue types\ntypes = glassdata.Type.unique()\n\nalpha = 0.65 # training data ratio\n\n# Splitting glassdata to training and test data\ntrain = pd.DataFrame()\ntest = pd.DataFrame()\nfor i in range(len(types)):\n    tempt = glassdata[glassdata.Type == types[i]]\n    train = train.append(tempt[0:int(alpha*len(tempt))])\n    test = test.append(tempt[int(alpha*len(tempt)): len(tempt)])\n    \n\n# Use Logistic Regression on All data\nX = train.drop('Type',1)\nY = train['Type']\nZ = test.drop('Type',1)\n\nprediction = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, Y).predict(Z)\n# prediction = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y).predict(Z)\n\ntruth = test['Type']\ntruth = np.array(truth)\n\naccuracy = sum(prediction == truth)/(len(truth))\nprint ('The default prediction is '+ str(prediction))\nprint ('The true value is '+ str(truth))\nprint ('The default accuracy is ' + str(accuracy))\n\n# ====================================================\n\n# Use Logistic Regression on Al and Si\nX1 = train[['Al', 'K', 'Mg']]\nY1 = train['Type']\nZ1 = test[['Al', 'K', 'Mg']]\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nprediction1 = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X1, Y1).predict(Z1)\n\n\ntruth = test['Type']\ntruth = np.array(truth)\n\naccuracy1 = sum(prediction1 == truth)/(len(truth))\nprint ('The feature selection prediction is '+ str(prediction1))\nprint ('The true value is '+ str(truth))\nprint ('The feature selection accuracy is ' + str(accuracy1))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c900c59f-2a9d-a903-2749-bfe01d7e19d6"},"outputs":[],"source":"Check https://www.kaggle.com/drwhohu/d/uciml/glass/classification-after-removing-outliers/ for continued analysis."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}