{"cells":[{"metadata":{"_uuid":"5e53d7aac3fd11747215af69b80c95f584687c83"},"cell_type":"markdown","source":"# Neural network classification with *Keras*  on **birth rate** level for Countries of the World data \n\nPrediction of the birth rate class of a given country (low, medium or high) based on a set of input features using **Deep Learning**.\n\n- Data is provided for **224** countries\n- Up to **18** input features available per country \n\n**Tasks**:\n- Selecting a subset of suitable input features\n- Preparing the data for Deep Learning with Keras\n- Creating a training and testing sample \n- Running the deep learning process and evaluate predictive power  \n- Investigating prediction accuracy changes with layers and nodes tuning\n- Saved the trained models"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"12398d4667141acae4c10f3e36fc06fdb905e755"},"cell_type":"code","source":"# import required package\nimport math\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nimport keras\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\nfrom IPython.display import Image\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport itertools\nimport seaborn as sns\n\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"451e03fb595e6aebe2dbe9f944576d68329017b9"},"cell_type":"markdown","source":"#### 0. Functions for our Neural Network Classification\n- These functions are used throughout our entire deep learning process."},{"metadata":{"trusted":true,"_uuid":"695101cfd79bfa88fc88ef18fc88501e0da6b1aa"},"cell_type":"code","source":"#!/usr/bin/python\n\n# preprocess country data\ndef gendata(features, path):\n\n    # read data\n    c = pd.read_csv(path, decimal=\",\")\n\n    # set output and classes\n    output = ['Birthrate']\n    classes = ['low', 'medium', 'high']\n\n    # shorten feature names\n    c.columns = ['Country', 'Region', 'Population', 'Area', 'Density', 'Coastline', 'Migration', 'InfantMortality', \\\n                 'GDP', 'Literacy', 'Phones', 'Arable', 'Crops', 'OtherLand', 'Climate', 'Birthrate', 'Deathrate', \\\n                 'Agriculture', 'Industry', 'Service']\n\n    # strip all whitespace from all columns\n    c = c.applymap(lambda x: x.strip() if type(x) is str else x)\n\n#     # set index to country\n#     c.set_index('Country', inplace=True)\n\n    # reduce to feature and type columns\n    dataset = c[features + output]\n\n    # drop duplicates and null values\n    dataset = dataset.drop_duplicates().dropna()\n    # make new birthrate class column\n    btype = []\n    for b in dataset.Birthrate:\n        if (b < 15):\n            btype.append('low')\n        elif ((b >= 15) and (b < 30)):\n            btype.append('medium')\n        elif (b >= 30):\n            btype.append('high')\n\n    # remove original birth rate column\n    dataset = dataset.drop(columns=['Birthrate'])\n\n    # NOTE - alternative if using pandas < 0.20\n    #del dataset['Birthrate']\n\n    # append to dataset\n    dataset['BRClass'] = pd.Series(btype, index=dataset.index)\n\n    # return values\n    return dataset.values\n\n# clearer and simpler version of featureplot\n# keeping featureplot in for backwards compatibility\n# restrict to 6 features\ndef featuresplot(data, target, features=None, classes=None):\n\n    if (features is None):\n        print(\"Please provide a list of feature names\")\n        return\n    if (classes is None):\n        print(\"Please provide a list of class names\")\n        return\n\n    plt_colors = \"rybgcm\"\n    n_classes = len(classes)\n\n    if (len(features) > 6):\n        print(\"Number of features is too high to plot\")\n        return\n\n    # get pair list of permutations and get unique set\n    n_features = data.shape[1]\n    x = [sorted(i) for i in itertools.permutations(np.arange(n_features), r=2)]\n    x.sort()\n    pairs = list(k for k,_ in itertools.groupby(x))\n\n    # set subplot layout\n    sub_y = math.ceil(len(pairs)/4.)\n    full_y = sub_y * 3.5\n\n    # set figure size\n    plt.figure(1, figsize=(15, full_y))\n\n    # enumerate over combinations\n    for pairidx, pair in enumerate(pairs):\n\n        # extract data for pair\n        datapair = data[:, pair]\n\n        # define new plot\n        plt.subplot(sub_y, 4, pairidx + 1)\n        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n\n        # add labels\n        plt.xlabel(features[pair[0]])\n        plt.ylabel(features[pair[1]])\n\n        # Plot the points\n        for i, color in zip(range(n_classes), plt_colors):\n\n            idx = np.where(target == classes[i])\n\n            plt.scatter(datapair[idx, 0], datapair[idx, 1], c=color, label=classes[i],\n                            cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n\n    plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n    plt.axis(\"tight\")\n\n    return\n\n# heat map for confusion matrices and parameter scans\n# adapted from https://stackoverflow.com/questions/19233771/sklearn-plot-confusion-matrix-with-labels\ndef heatmap(d, labels=None, classes=None, title=None,\n            palette=\"Green\",\n            normalize=False,\n            annot=True):\n\n    if normalize:\n        d = d.astype('float') / d.sum(axis=1)[:, np.newaxis]\n\n    ax = plt.subplot()\n\n    # define colour map\n    my_cmap = sns.light_palette(palette, as_cmap=True)\n\n    # plot heatmap\n    sns.heatmap(d, annot=True, ax=ax, cmap=my_cmap)\n\n    # labels, title and ticks\n    if (labels is not None):\n        ax.set_xlabel(labels[0])\n        ax.set_ylabel(labels[1])\n    if (title is not None):\n        ax.set_title('Confusion Matrix')\n    if (classes is not None):\n        ax.xaxis.set_ticklabels(classes[0])\n        ax.yaxis.set_ticklabels(classes[1])\n\n    return\n\n# Note - expect convergence warning at small training sizes\ndef compare_traintest(data, target, model, split=0, scale='linear', **params):\n    \n    #preprocess target data\n    le = preprocessing.LabelEncoder()\n    target_label = le.fit_transform(target)\n\n    # convert integers to dummy variables (i.e. one hot encoded)\n    target_label = np_utils.to_categorical(target_label).astype(float)\n    \n    # define 0.01 - 0.1, 0.1 - 0.9, 0.91 - 0.99 sample if split array not defined\n    if (split == 0):\n        split = np.concatenate((np.linspace(0.01,0.09,9), np.linspace(0.1,0.9,9), np.linspace(0.91,0.99,9)), axis=None)\n\n    print(\"Parameters\")\n    print(params)\n        \n    print(\"Split sample:\")\n    print(split)\n\n    train_scores = []\n    test_scores = []\n\n    for s in split:\n\n        print(\"Running with test size of: %0.2f\" % s)\n\n        # get train/test for this split\n        d = model_selection.train_test_split(data, target_label,\n                                             test_size=s, random_state=0)\n\n        # get training and test data and targets\n        train_data, test_data, train_target, test_target = d\n\n        # Data needs to be scaled to a small range like 0 to 1 for the neural network to work well.\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        # Scale both the training inputs and outputs\n        train_data = scaler.fit_transform(train_data)\n        test_data = scaler.transform(test_data)\n        \n        # Train the model\n        model.fit(\n            train_data,\n            train_target,\n            **params,\n            validation_data=(test_data, test_target)\n        )\n\n        train_error_rate = model.evaluate(train_data, train_target, verbose=0)\n        test_error_rate = model.evaluate(test_data, test_target, verbose=0)\n\n        # get test scores for fit and prediction\n        train_scores.append(train_error_rate[1])\n        test_scores.append(test_error_rate[1])\n\n    # plot results\n    plt.figure(figsize=(15.0, 5.0))\n    if (scale == 'log'):\n        plt.yscale('log')\n    else:\n        plt.yscale('linear')\n    plt.plot(split, train_scores, label='Training accuracy', marker='o')\n    plt.plot(split, test_scores, label='Testing accuracy', marker='o')\n    plt.title('Training and Testing Accuracy')\n    plt.xlabel('Test sample proportion')\n    plt.ylabel('Accuracy')\n    plt.xticks(np.arange(0, 1.0, 0.1))\n    plt.yticks(np.arange(0, 1.1, 0.1))\n    plt.xlim([min(split),max(split)])\n    plt.ylim([0,1.01])\n    plt.grid()\n    plt.legend()\n\n    return\n\ndef evaluate_model_accuracy(model, data, target_label_1d, **params):\n    print(\"k-Fold Cross Validation\")\n    print(\"Parameters\")\n    print(params)\n\n    kfold = model_selection.StratifiedKFold(n_splits=10, shuffle=True)\n    cvscores = []\n    for train, test in kfold.split(data, target_label_1d):\n        # convert integers to dummy variables (i.e. one hot encoded)\n        target_label = np_utils.to_categorical(target_label_1d).astype(float)\n        \n        # Data needs to be scaled to a small range like 0 to 1 for the neural network to work well.\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        # Scale both the training inputs and outputs\n        data[train] = scaler.fit_transform(data[train])\n        data[test] = scaler.fit_transform(data[test])\n        \n        # Fit the model\n        model.fit(data[train], target_label[train], epochs=800, verbose=0)\n\n        # evaluate the model\n        scores = model.evaluate(data[test], target_label[test], verbose=0)\n        print('{0:} : {1:0.2f}%'.format(model.metrics_names[1], scores[1]*100))\n        cvscores.append(scores[1] * 100)\n    print('Model Accuracy : {0:0.2f}% (+/- {1:0.2f}%)'.format(np.mean(cvscores), np.std(cvscores)))\n    \n    return np.mean(cvscores)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c20cabd76cadb4c1f52a28215729bbedee55665"},"cell_type":"markdown","source":"#### 1. Data Extraction\n- The features can be any of the following: \n  - Population, Area, Density, Coastline, Migration, InfantMortality, GDP, Literacy, Phones, Arable, Crops, OtherLand, Climate, Deathrate, Agriculture, Industry, Service\n  - More information on the dataset is available [here](https://www.kaggle.com/fernandol/countries-of-the-world)\n- Only 4 features are chosen as input into the Decision Tree. "},{"metadata":{"trusted":true,"_uuid":"b3facbe31c42aa1ccd5d5f6377c5570a22989809"},"cell_type":"code","source":"# Load country dataset \nbclass = ['low', 'medium', 'high'] # birth rate class (provided)\nfeatures_all = ['Country', 'Population', 'Area', 'Density', 'Coastline', 'Migration', 'InfantMortality', 'GDP', 'Literacy', 'Phones', 'Arable', 'Crops', 'OtherLand', 'Climate', 'Deathrate', 'Agriculture', 'Industry', 'Service'] # fdefine features here \npath = '../input/countries of the world.csv'\ndata_all = gendata(features_all, path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b149966b78c65569f25e8828a747221961443b1"},"cell_type":"code","source":"# Choose only 4 features\nfeature_idx = [6, 7, 8, 12]\n\n# Show feature names\nfeatures = []\nfor i in feature_idx:\n    features.append(features_all[i])\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f89f834489b050c67e269d98828912457d742b2a"},"cell_type":"code","source":"data, target = data_all[:, feature_idx].astype(float), data_all[:, [-1]].ravel()\n\n#preprocess target data\nle = preprocessing.LabelEncoder()\ntarget_label_1d = le.fit_transform(target)\n\n# convert integers to dummy variables (i.e. one hot encoded)\ntarget_label = np_utils.to_categorical(target_label_1d).astype(float)\n\n# # Show dataset dimensions\ndata.shape, target.shape, np.unique(target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f2a35a6e62491daf9e53c1ccc2b6266db2e3ac7"},"cell_type":"markdown","source":"#### 2. The feature distribution of the selected observations"},{"metadata":{"trusted":true,"_uuid":"803f1db345814f1f20838fb3d727c5693a849bf1"},"cell_type":"code","source":"# generate 2D plots \nfeaturesplot(data, target, features, bclass)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b732f4d22c6ba0acc66618a5d8f4724733c6b98e"},"cell_type":"markdown","source":"#### 3. Constructing and training the Deep Learning based on your training data with the following requirements\n- 70% of the observations are reserved for training"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"03102b41f08e226ef1969b4075bb1dbbb1f3d944"},"cell_type":"code","source":"# split data into training and test dataset\nd = model_selection.train_test_split(\n    data, target_label, test_size=0.2, random_state=0)\n\n# get training and test data and targets\ntrain_data, test_data, train_target, test_target = d\n\nprint(train_data.shape, train_target.shape, test_data.shape, test_target.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d2be9012dcc7882b065bcab94c42af8bb2ef887"},"cell_type":"markdown","source":"#### 4. Constructing and training the Deep Learning based on your training data with the following requirements\n"},{"metadata":{"trusted":true,"_uuid":"c2e65fa228672f83497cd1cde7e4348aacfc9997"},"cell_type":"code","source":"# Data needs to be scaled to a small range like 0 to 1 for the neural network to work well.\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# Scale both the training inputs and outputs\ntrain_data = scaler.fit_transform(train_data)\ntest_data = scaler.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdeaecda1b8a92bd4757a6917284dadc0a93627f"},"cell_type":"code","source":"def baseline_model():\n    # Define the model\n    model = Sequential()\n    model.add(Dense(350, kernel_initializer='uniform', input_dim=4, activation='relu'))\n    model.add(Dense(3, kernel_initializer='uniform', activation='sigmoid'))\n    model.compile(loss='categorical_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\n    return model\nmodel = baseline_model()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62a0e64568ef4d289e1d7a0a5388ca87b1911e47"},"cell_type":"markdown","source":"#### 5. Creating Tensorboard logger to visualize and compare models performance\n- To access the TensorBoard, run 'tensorboard --logdir=logs --host localhost' inside the file directory through terminal.\n"},{"metadata":{"trusted":true,"_uuid":"556541f59cfa506680bc04d4c882ec0a4b891c44"},"cell_type":"code","source":"RUN_NAME = 'run 6 epoch 800 nodes 350 relu,3 sigmoid, adagrad'\n\n# Create a TensorBoard logger\nlogger = keras.callbacks.TensorBoard(\n    log_dir='logs/{}'.format(RUN_NAME),\n    histogram_freq=5,\n    write_graph=True\n)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"cfb68242a71d161b992bd9d6f7da3b6996993493"},"cell_type":"code","source":"# Train the model\nmodel.fit(\n    train_data,\n    train_target,\n    epochs=800,\n    shuffle=True,\n    verbose=0,\n#     callbacks=[logger],\n    validation_data=(test_data, test_target)\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0be3ed2823f8ccc81270e492fc93b4ea7cfa4ae"},"cell_type":"markdown","source":"#### 6. Displaying the categorical cross-entropy and accuracy for the predictions using the test sample"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"a03092ce62a1b05147d63a3e8480dd5d9272baba"},"cell_type":"code","source":"test_error_rate = model.evaluate(test_data, test_target, verbose=0)\nprint(\"The categorical cross-entropy (CCE) for the test data set is: {0:0.4f}\".format(test_error_rate[0]))\nprint(\"The accuracy of the test data set evaluation is: {0:0.2f}%\".format(test_error_rate[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d3c33103d7a399d7f8b43dbacda1d837d27cc89"},"cell_type":"markdown","source":"#### 7. Evaluating the performance of the model by running manual k-Fold Cross Validation.\n- This must be done manually because the data must be preprocessed(rescaling) before being used to train the model."},{"metadata":{"trusted":true,"_uuid":"00dea88e66b984a869de456d9aa2928bb4a67579"},"cell_type":"code","source":"model_acc = evaluate_model_accuracy(model, data, target_label_1d, epochs=800, shuffle=True, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2d08d25da61431040efc23a665510e2b7679c9e"},"cell_type":"markdown","source":"#### 8. Saving the trained model for future use."},{"metadata":{"trusted":true,"_uuid":"db5cb31683a59e828037838b65e45e0f3c3bc685"},"cell_type":"code","source":"# Save the model to disk\n# model.save(\"{0:} - modelACC{1:0.2f}.h5\".format(RUN_NAME, model_acc))\n# print('Model saved to disk.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}