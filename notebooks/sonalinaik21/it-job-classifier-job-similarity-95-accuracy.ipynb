{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Business Problem:"},{"metadata":{},"cell_type":"markdown","source":"Our main business objectives are to understand the dynamics of the labour market of Armenia using the online job portal post as a proxy."},{"metadata":{},"cell_type":"markdown","source":"# Import necessary packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n#for charts\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud      #need to install wordcloud package","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for text processing\nimport string\nimport re\nimport nltk\nfrom textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for tokenization\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n#for feature selection\nfrom sklearn import decomposition\n\n#for model building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import data"},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs = pd.read_csv('/kaggle/input/jobposts/data job posts.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"jobs.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lowercase the column names\njobs.columns = jobs.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing duplicate jobposts based on title and post\njobs = jobs.drop_duplicates(['jobpost', 'title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing records with null title\n#jobs = jobs[jobs.title.notna()]\n#jobs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Type of demanding jobs in Armenia"},{"metadata":{},"cell_type":"markdown","source":"**1. Preprocessing the text data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs['title'] = jobs['title'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#UDF to do basic cleaning of title column to understand type of jobs\ndef clean_data(text):\n    text = text.lower()  # convert all the text into lowercase\n    text = text.strip()  #remove starting and trailing whitespaces\n    #special_chars = re.compile('[@!#$%^&*()<>?/\\|}{~:;]')\n    #text = re.sub(special_chars,'', text)\n    special_char_reg = '([a-zA-Z0-9]+)' + '[!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]' + '([a-zA-Z0-9]+)'\n    text = re.sub(special_char_reg, ' ', text)\n    text = re.sub(r'\\s+', ' ', text) #remove all line formattings\n    text = re.sub(r'\\d+', '', text) #remove digits\n    text = ''.join(c for c in text if c not in string.punctuation)   #remove pecial symbols from job titles\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 'Ful8l-ti9me Community Connections f09:053yy'\nspecial_char_reg = '([a-zA-Z0-9]+)' + '[!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]' + '([a-zA-Z0-9]+)'\nre.sub(special_char_reg, ' ', a).strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.title.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df = jobs.title.apply(lambda x : clean_data(x))\ntitle_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import WordNetLemmatizer\n#nltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemma(text):\n    word_list = nltk.word_tokenize(text) #tokenize beofre lemmatization\n    lemma_output = ' '.join(WordNetLemmatizer().lemmatize(word) for word in word_list)\n    return lemma_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the sentence to be lemmatized\nsentence = \"public bats outreach and strengthening of a growth\"\nsentence = \"The striped bats are hanging on their feet for best\"\n# Tokenize: Split the sentence into words\nword_list = nltk.word_tokenize(sentence)\nprint(word_list)\n#> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n\n# Lemmatize list of words and join\nlemmatized_output = ' '.join([WordNetLemmatizer().lemmatize(w) for w in word_list])\nprint(lemmatized_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Lematization\nimport spacy\n#neccesary to download the english model using \"python -m spacy download en\"\nnlp = spacy.load('en_core_web_sm')\n# Parse the sentence using the loaded 'en' model object `nlp`\ndoc = nlp(sentence)\n\n# Extract the lemma for each token and join\n\" \".join([token.lemma_ for token in doc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df_1 = title_df.apply(lambda x : lemma(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stop words removal\nstop = nltk.corpus.stopwords.words('english')\n#stop.extend(['armenian', 'armenia', 'job', 'title', 'position', 'location', 'responsibilities', 'application',\n#                  'procedures', 'deadline', 'required','qualifications', 'renumeration', 'salary', 'date', 'company', 'llc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df_1 = title_df_1.apply(lambda x : ' '.join(x for x in x.split() if x not in stop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to undertand the most demanding jobs in armenia we can create a **bi/tri gram DTM** on the job titles and find the most occuring token to be the most demanding job"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenization using count vectorizer\ncount_vect = CountVectorizer(ngram_range=(1,1))\ntoken = count_vect.fit_transform(title_df_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(count_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of tokens/words in all the job titles - ', len(count_vect.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df =  pd.DataFrame(token.toarray(), columns=count_vect.get_feature_names())\ntemp_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count the accurence of each token in entire corpus\ncount_df = temp_df.apply(lambda x : x.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = pd.DataFrame(count_df).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df.columns = ['Word', 'Count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_jobs = count_df.sort_values(by= 'Count', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_jobs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the WordCloud image to show top 50 type of demanding jobs in armenia     \nwordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(top_jobs[:50].Word))\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Job Nature changing over time"},{"metadata":{},"cell_type":"markdown","source":"We can use dispersion plot to see how jobs change over time. To do this we need to get the important topics out of the jobpost and then plot their dispersion over time."},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs['jobpost'] = jobs['jobpost'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#UDF to do basic cleaning of title column to understand type of jobs\ndef clean_data(text):\n    text = text.lower()  # convert all the text into lowercase\n    text = text.strip()  #remove starting and trailing whitespaces\n    #special_chars = re.compile('[@!#$%^&*()<>?/\\|}{~:;]')\n    #text = re.sub(special_chars,'', text)\n    special_char_reg = '([a-zA-Z0-9]+)' + '[!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]' + '([a-zA-Z0-9]+)'\n    text = re.sub(special_char_reg, ' ', text)\n    text = re.sub(r'\\s+', ' ', text) #remove all line formattings\n    text = re.sub(r'\\d+', '', text) #remove digits\n    text = ''.join(c for c in text if c not in string.punctuation)   #remove pecial symbols from job titles\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.jobpost.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobpost_df = jobs.jobpost.apply(lambda x : clean_data(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobpost_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Lematization\nimport spacy\n#neccesary to download the english model using \"python -m spacy download en\"\n#nlp = spacy.load('en_core_web_sm')\nlemmatized_out = []\ncount = 0\n#for jobpost in jobpost_df:\n#    doc = nlp(jobpost)\n#    x = \" \".join(word.lemma_ for word in doc)\n#    print(count)\n#    count += 1\n#    lemmatized_out.append(x)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#lemmatized_out[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stop words removal\nstop = nltk.corpus.stopwords.words('english')\nstop.extend(['armenian', 'armenia', 'job', 'title', 'position', 'location', 'responsibility', 'application',\n             'procedure', 'deadline', 'requirement','qualification', 'renumeration', 'salary', 'date', 'company', 'llc',\n             'person', 'employement', 'post', 'follow', 'resume', 'open', 'about', 'announcement', 'link', 'website',\n             'organization', 'duration'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#jobpost_df_0 = pd.Series(lemmatized_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobpost_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobpost_df_1 = jobpost_df.apply(lambda x : ' '.join(word for word in x.split() if word not in stop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobpost_df_1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will create tokens out of this processed data\n\ntfidf_vect = TfidfVectorizer(ngram_range=(1,1), min_df=0.05, max_df=0.95)\ntfidf_vect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_jobpost = tfidf_vect.fit_transform(jobpost_df_1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"vocab = tfidf_vect.get_feature_names()\n#print(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_jobpost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_df = pd.DataFrame(token_jobpost.toarray(), columns=tfidf_vect.get_feature_names())\ntoken_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply LDA technique to understand important job nature and profiles\n\nlda = decomposition.LatentDirichletAllocation(n_components = 5, learning_method = 'online', max_iter = 50, random_state = 3)\nlda.fit_transform(token_jobpost)\ntopics = lda.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the topic models for cluster 0\nn_top_words = 10\ntopic_summaries = []\nfor i, topic_dist in enumerate(topics):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n\ntopic_summaries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vocab = []\n#def fn_token(post):\n#    list_temp = nltk.word_tokenize(post)\n#    vocab.extend(list_temp)\n\n#jobpost_df_1.apply(lambda x : fn_token(x))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#full_vocab = []\n#for word in  vocab:\n#        if word not in full_vocab:\n#            full_vocab.append(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(full_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_words_tokens = []\nfor topic in topic_summaries:\n    word_token = nltk.word_tokenize(topic)\n    topic_words_tokens.extend(word_token)\nprint(topic_words_tokens)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use lexical dispersion plot to see the topics use over time\n#Start pylab inline mode, so figures will appear in the notebook\n#%pylab inline\n\n#from nltk.draw.dispersion import dispersion_plot\n\n#dispersion_plot(vocab, topic_words_tokens[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df = pd.concat([jobpost_df_1, jobs.year], axis = 1)\n#plot_df = jobpost_df_2.apply(lambda x : fn() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_words_tokens[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"('topic', '2018')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"nt = [(topic, year)  for year in plot_df.year  for topic in topic_words_tokens[0:9] ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd = nltk.ConditionalFreqDist(nt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#conditional frequency distribution plot to see the use of topics over time\ncfd = nltk.ConditionalFreqDist(\n    (target, year)\n    for year in plot_df.year\n    for a in plot_df.jobpost\n    for w in nltk.word_tokenize(a)    \n    for target in topic_words_tokens[:10]\n    if w.lower().startswith(target))\ncfd.plot()\n\n#    for w in jobpost_df_1.words(year)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Desired characteristics and  Skill-sets\nTo understand this we can make clusters using job description column of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = jobs[jobs.jobdescription.isna() == False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Text pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"x['jobdescription'] = x['jobdescription'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_df = x.jobdescription.apply(lambda x : clean_data(x))\ndesc_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#LEmmatization\ndesc_df_1 = desc_df.apply(lambda x : lemma(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lemmatized_out[0:6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#desc_df_0 = pd.Series(lemmatized_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stop word removal\ndesc_df_1 = desc_df_1.apply(lambda x : ' '.join(x for x in x.split() if x not in stop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Tokenization\ntfidf_vect = TfidfVectorizer(ngram_range=(1,1), min_df = 0.05, max_df=0.95, stop_words='english')\nx_tdm = tfidf_vect.fit_transform(desc_df_1)\n#print(x_tdm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clust = pd.DataFrame(x_tdm.toarray(), columns=tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clust.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering"},{"metadata":{},"cell_type":"markdown","source":"#### Dimension Reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = KMeans(n_clusters=5, \n               init='k-means++', \n               max_iter=100, n_init=1,random_state=5)\nkmeans = model.fit(x_tdm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we create a kmeans model\nkm_3 = KMeans(n_clusters=3,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_4 = KMeans(n_clusters=4,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_5 = KMeans(n_clusters=5,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_6 = KMeans(n_clusters=6,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_7 = KMeans(n_clusters=7,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_8 = KMeans(n_clusters=8,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_9 = KMeans(n_clusters=9,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the cluster labels and sort by cluster\nx['cluster_3'] = km_3.labels_\nx['cluster_4'] = km_4.labels_\nx['cluster_5'] = km_5.labels_\nx['cluster_6'] = km_6.labels_\nx['cluster_7'] = km_7.labels_\nx['cluster_8'] = km_8.labels_\nx['cluster_9'] = km_9.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = np.array(tfidf_vect.get_feature_names())\nvocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_centers = np.array(km_5.cluster_centers_)\ncluster_centers[0].argsort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km_3.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['cluster_3'].value_counts()/sum(x['cluster_3'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['cluster_4'].value_counts()/sum(x['cluster_4'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"x['cluster_5'].value_counts()/sum(x['cluster_5'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['cluster_6'].value_counts()/sum(x['cluster_6'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['cluster_7'].value_counts()/sum(x['cluster_7'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['cluster_8'].value_counts()/sum(x['cluster_8'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Either 5-6 , is the optimal solution for our clusters"},{"metadata":{},"cell_type":"markdown","source":"### Evaluation clusters"},{"metadata":{},"cell_type":"markdown","source":"### 1. Silhouette Coefficient(Higher the better)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import  metrics\nmetrics.silhouette_score(x_tdm, labels=km_3.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nscores.append(metrics.silhouette_score(x_tdm, labels=km_3.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_4.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_5.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_6.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_7.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_8.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_9.labels_))\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(3,10), scores)\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.grid('True')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7 Cluster seems to be optimal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sorting the cluster centers for 5 clusters\nsorted_vals = [km_5.cluster_centers_[i].argsort() for i in range(0,np.shape(km_5.cluster_centers_)[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get top 10 words from that cluster\nwords=set()\nfor i in range(len(km_5.cluster_centers_)):\n    words = set(vocab[sorted_vals[i][-10:]])\n    print(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sorting the cluster centers for 6 clusters\nsorted_vals = [km_6.cluster_centers_[i].argsort() for i in range(0,np.shape(km_6.cluster_centers_)[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get top 10 words from that cluster\nwords=set()\nfor i in range(len(km_6.cluster_centers_)):\n    words = set(vocab[sorted_vals[i][-10:]])\n    print(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above analysis shows that in cluster 6 the tokens get repeated and clusters are more similar to each other. That means **cluster 5** is optimal. "},{"metadata":{},"cell_type":"markdown","source":"# IT Job Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_data = jobs[(jobs.title.isna() == False) & (jobs.jobrequirment.isna() == False) & (jobs.requiredqual.isna() == False) &\n                 (jobs.jobdescription.isna() == False) & (jobs.aboutc.isna() == False) & (jobs.company.isna() == False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#identify Y variable\nclass_data['it'] = class_data.it.apply(lambda x : 0 if (x is False) else 1)\ny=class_data['it']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_data = class_data['title'].str.cat(class_data['jobrequirment'], sep =\" \").str.cat(class_data['requiredqual'], sep =\" \").str.cat(\n    class_data['jobdescription'], sep =\" \").str.cat(class_data['aboutc'], sep =\" \").str.cat(class_data['company'], sep =\" \")\nclass_df = class_data","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"class_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_df = class_df.apply(lambda x : clean_data(str(x)))\nclass_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Lemmatization\nclass_df_1 = class_df.apply(lambda x : lemma(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stop word removal\nclass_df_1 = class_df_1.apply(lambda x : ' '.join(x for x in x.split() if x not in stop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Tokenization\ntfidf_vect = TfidfVectorizer(ngram_range=(1,1), min_df = 0.05, max_df=0.95, stop_words='english')\nx_tdm = tfidf_vect.fit_transform(class_df_1)\n#print(x_tdm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clust = pd.DataFrame(x_tdm.toarray(), columns=tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clust.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building a Random Forest Model"},{"metadata":{},"cell_type":"markdown","source":"#### Divide the data into train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clust.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_x, test_x,train_y, test_y = train_test_split(df_clust,y, test_size = 0.2, random_state = 5)\nprint(train_x.shape, test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators':[130,150,160,180,200],\n              'max_features':[13,15,17,19]}\n\ngrid_rf = GridSearchCV(estimator= RandomForestClassifier(),\n                      param_grid=param_grid,\n                      cv = 10,\n                      n_jobs=-1, verbose=True)\n\ngrid_rf.fit(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_rf.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_rf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the model\nrf_model = grid_rf.best_estimator_\nrf_model.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prdict the output for train and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_train_predict = pd.DataFrame({'actual' : train_y,\n                                 'predicted' : rf_model.predict(train_x)})\nrf_train_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_test_predict = pd.DataFrame({'actual' : test_y,\n                                 'predicted' : rf_model.predict(test_x)})\nrf_test_predict.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1. Check accuracy score on train and test\n\nprint('Accuracy Score for train dataset : ' , metrics.accuracy_score(rf_train_predict.actual, rf_train_predict.predicted))\nprint('Accuracy Score for test dataset : ' , metrics.accuracy_score(rf_test_predict.actual, rf_test_predict.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2. Check roc_auc score on train and test\n\nprint('ROC-AUC Score for train dataset : ' , metrics.roc_auc_score(rf_train_predict.actual, rf_train_predict.predicted))\nprint('ROC-AUC Score for validation dataset : ' , metrics.roc_auc_score(rf_test_predict.actual, rf_test_predict.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3. Create confusion matrix\n#for test\n\nconn_cm_test = metrics.confusion_matrix(rf_test_predict.actual, rf_test_predict.predicted, [1,0])\nsns.heatmap(conn_cm_test, fmt= '.2f', annot=True,  xticklabels=['IT', 'NOT IT'], yticklabels=['IT', 'NOT IT'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4. Create classification report\nprint(metrics.classification_report(rf_test_predict.actual, rf_test_predict.predicted))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"indices = np.argsort(rf_model.feature_importances_)[::-1]\nfeature_rank = pd.DataFrame(columns = ['rank', 'feature', 'importance'])\nfor f in range(train_x.shape[1]):\n    feature_rank.loc[f] = [f+1,\n                          train_x.columns[indices[f]],\n                          rf_model.feature_importances_[indices[f]]]\nfeature_rank.round(3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"feature_rank[:17]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Presence of keywords like software, developer, web, design, cs are the important feautures while clasifying any job as **IT or NON-IT**"},{"metadata":{},"cell_type":"markdown","source":"# Similarity of Jobs"},{"metadata":{"trusted":true},"cell_type":"code","source":"###Using TF-IDF as cosine similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics.pairwise import cosine_similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cosine_sim(doc): \n    vectors = [t for t in get_vectors(doc)]\n    return cosine_similarity(vectors)\n    \ndef get_vectors(doc):\n    text = [t for t in doc]\n    vectorizer = CountVectorizer(text)\n    vectorizer.fit(text)\n    return vectorizer.transform(text).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using word embeddings Doc2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import utils\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.models import Doc2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobpost_df_1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pre-processed text of jobpost column\njobpost_df_1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df_1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pre-processed text of title column column\ntitle_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Create the tags with each post"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sim = pd.concat([jobpost_df_1, title_df_1], axis = 1)\ndf_sim.loc[1810]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs=[]\ndef fn_tag_doc(jobpost, title):\n        docs.append(TaggedDocument(words = jobpost.split(), tags = [title]))     \n\ndf_sim.apply(lambda x : fn_tag_doc(x['jobpost'], x['title']), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"docs[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Build a Model to convert each document(jobpost) into vectors to be used to check similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sim = Doc2Vec(docs, dm=0, alpha = 0.025, min_alpha = 0.025, min_count = 0)  # use fixed learning rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(10):\n    model_sim.train(docs, total_examples= model_sim.corpus_count, epochs=model_sim.epochs)\n    model_sim.alpha -= 0.002  # decrease the learning rate\n    model_sim.min_alpha = model_sim.alpha  # fix the learning rate, no decay","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.Check the similarity of a given job title and get top 10 jobposts similar to that job_title"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_sim.docvecs.most_similar(positive=[model_sim.infer_vector('chief financial officer'.split())],topn=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above shows **top 10 titles** which are similar to the title 'chief financial officer'."},{"metadata":{"trusted":true},"cell_type":"code","source":"#docs[0].tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tags_list=[]\n#for i in range(0, df_sim.shape[0]):\n#    c = str(docs[i].tags).replace('[', '')\n#    c = c.replace(']', '')\n#    c = c.replace(\"'\", '')\n#    tags_list.append(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tags_list[0]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#sim_list = []\n#for tag in tags_list:\n    #print(model_sim.docvecs.similarity('bcc specialist', tag))\n#    sim_list.append(model_sim.docvecs.similarity('software developer', tag))\n    \n#sim_list[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sim_score_df = pd.concat([pd.Series(jobpost_df_1),pd.Series(tags_list), pd.Series(sim_list)], axis =1)\n#sim_score_df.columns=['jobpost', 'title', 'similarity_score']\n#sim_score_df\n#sim_score_df.sort_values(by = 'similarity_score', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}