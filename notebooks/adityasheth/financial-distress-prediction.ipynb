{"cells":[{"metadata":{"_uuid":"8d47f3fd23eee3a43e0a1759f692d518a51abfbe","_kg_hide-output":true,"_cell_guid":"2410ea3d-9502-4dbf-b781-ecf60c802d5b","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Importing the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport random\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n# Reading the dataset\ndata = pd.read_csv(\"../input/Financial Distress.csv\")\n\n# Checking the dataset\ndata.head()\ndata.tail()\ndata.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71239f65e413a6a97bb3e995de1ab31818c57679","_kg_hide-output":true,"_cell_guid":"416b6c52-fba0-4194-9c38-a1205aa2ebdf","trusted":true},"cell_type":"code","source":"print(\"Number of companies:\",data.Company.unique().shape)\ndata = data[data.columns.drop(list(data.filter(regex='x80')))] # Since it is a categorical feature with 37 features.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6504104960b77ab209510f3547f7d5ec507d9ba5","_cell_guid":"79c2ef0f-6b90-40ee-be4f-3966288b6b74"},"cell_type":"markdown","source":"\"x80\" is a categorical variable and needs to be encoded to be consideredd for further analysis, hence dropping it out of the dataset for the time being. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"2247df4be434dc9966a667fd07f3e6ff97f21bb8","_cell_guid":"c9446c3d-90b8-49a8-b43b-6171b1143028","collapsed":true,"trusted":true},"cell_type":"code","source":"# Creating target vector and feature matrix\nY = data.iloc[:,2].values\nfor y in range(0,len(Y)): # Coverting target variable from continuous to binary form\n       if Y[y] > -0.5:\n              Y[y] = 0\n       else:\n              Y[y] = 1\nX = data.iloc[:,3:].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8ab43eddd13c37ee118296919ffe78aed5f4a2d","_cell_guid":"f1a5db59-5455-432d-b473-8c8c575c9d8b"},"cell_type":"markdown","source":"We convert the continuous response variable \"Financial distress\" to a categorical variable, where '1' represents that the company is bankrupt, whereas '0' suggests a healthy company.\nAlso, we do not consider the columns \"Time\" and \"Company\" for our analysis.\n\"X\" is the feature matrix; \"Y\" is the target vector. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"084110bcffe1813bfff26546699fc3cc9bd5c07c","_cell_guid":"3fccd230-a86c-4c03-b42c-ad3e934cc3af","trusted":true},"cell_type":"code","source":"# Counting number of observations for Healthy and and Bankrupt Companies:\nnum_zeros = 0\nfor num in Y:\n       if num == 0:\n              num_zeros = num_zeros+1\nnum_ones = len(Y) - num_zeros \n\nprint(\"Number of observations for BANKRUPT companies(1's):\",num_ones)\nprint(\"Number of observations for HEALTHY companies(0's):\",num_zeros)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0265b436e4828eb40b63c98c2c92645552cf4db1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c65ed389ee5ccac54d654b8c851823b533422c9","_cell_guid":"867560c6-1324-4e58-8caf-391e688da0c6"},"cell_type":"markdown","source":"Thus, verifying that data is highly skewed, as mentioned by the publisher of the dataset.\n\nIn order to combat imbalance in the dataset, there are a several methods one can approach. To find out more in detail aboutthis, please check out https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"8759054d101604c220bb511d53abfe8827eb1240"},"cell_type":"code","source":"# Splitting the data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,Y, test_size = 0.30, random_state = 0)\nX_train_wo_sampling = X_train\ny_train_wo_sampling = y_train\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df5fed58b703343092a63627b5b814b19211b029"},"cell_type":"markdown","source":"As pointed out in the comment, to avoid information leak, splitting the dataset prior to adding the generated extra data, to the training set.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"532e5f805ce20ec00cc2591f77d4c0818c4e8ea1","_cell_guid":"cb2323e2-e5de-49c0-9987-1ba8b6aa0c7c","collapsed":true,"trusted":true},"cell_type":"code","source":"# Creating more samples units for the bankrupt companies(undersampled data)\ny_train = (np.matrix(y_train)).T\ny_train = pd.DataFrame(y_train)\ny_train.columns = [\"Financial_Distress\"]\nX_train = pd.DataFrame(X_train)\nframe = [X_train,y_train]\ntrain_data = pd.concat(frame,axis = 1)\nbankrupt_companies = train_data[train_data.Financial_Distress == 1]\n\nfeat_mat = bankrupt_companies.iloc[:,:-1].values\nresponse = bankrupt_companies.iloc[:,-1].values\ncol_mean = np.zeros(shape=(82,1)) \ncol_std = np.zeros(shape=(82,1)) \nDim_1 = np.shape(feat_mat)\nfor i in range(0,Dim_1[1]): # Logic to calculate mean and standard deviation for each column\n       col_mean[i,0] = np.mean(feat_mat[:,i])\n       col_std[i,0] = np.std(feat_mat[:,i])\ncol_mean_and_std = np.hstack((col_mean,col_std))\n\nadded_data = np.zeros(shape=(1200,Dim_1[1])) \nfor i in range (0,len(col_mean_and_std)):\n       mean_ = col_mean_and_std[i,0]\n       std_ = col_mean_and_std[i,1]\n       added_data[:,i] = np.random.normal(mean_,std_,1200)\nadded_y = np.ones(shape=(1200,1)) # Creating labels for the added data\n\nX_resampled = np.vstack((X_train,added_data)) # Combining the original data + added data\ny_train = np.array(y_train)\ny_resampled = np.vstack((y_train,added_y))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f9e3fe657841d00d4f5f9b9fb00f195122e1335","_cell_guid":"326e82bd-605e-4d2e-bf6e-6960b41b870d"},"cell_type":"markdown","source":"Here, for this case I create data for the under-sampled observations(i.e. bankrupt companies). This is achieved by knowing the mean and standard deviation of each feature for bankrupt companies, and then generating observations(1100) using np.random.normal numbers function.\n\nNow in order to maintain the originality of the test-set, the newly generated data is only added to the training set, to help improve the model.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"396f72431a82cf652b34594f214309213526473f","_cell_guid":"7685d186-f963-4481-90a1-ac39bdc6baec","collapsed":true,"trusted":false},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7e85af71d0c332fd460f4fb7fa83688844fa540","_cell_guid":"d391bc4d-24d6-40ce-a316-018fa54dd069"},"cell_type":"markdown","source":"","outputs":[],"execution_count":null},{"metadata":{"_uuid":"21a81eb86736db06ac2f5b9991857f78bb0c976c","_cell_guid":"a3210eef-ab77-49ab-b443-497685f6d58b","collapsed":true,"trusted":true},"cell_type":"code","source":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_resampled = sc.fit_transform(X_resampled)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e300b77503e7978f2c4484e04a7f18922a72e614","_cell_guid":"3d4aa5de-9d48-46dd-a071-63f1480bc138"},"cell_type":"markdown","source":"So for model comparison, I have selected 7 Models for this analaysis, and they are as follows:\nModel 1 - XGBoostClassifier\nModel 2 - Support Vector Classifier(SCV)\nModel 3 - RandomForestClassifier\nModel 4 - Logistic \nModel 5 - BalancedBaggingClassifier\nModel 6 - Decision Tree\nModel 7 - Naive Bayes Classifier","outputs":[],"execution_count":null},{"metadata":{"_uuid":"9f9b43253b85f1882a186246fcc3754d0c141b43","_cell_guid":"5e516e3f-b30a-4a5e-8e61-021c3f8e9a0a","trusted":true},"cell_type":"code","source":"# Fitting XGBClassifier to the training data: Model_1\nfrom xgboost import XGBClassifier\nclassifier_1 = XGBClassifier()\nclassifier_1.fit(X_resampled,y_resampled)\n\n# Fitting SVM to the training data: Model 2\nfrom sklearn.svm import SVC\nclassifier_2 = SVC(kernel = 'linear', C = 1, probability = True, random_state = random.seed(123)) # poly, sigmoid\nclassifier_2.fit(X_resampled, y_resampled)\n\n# Creating and Fitting Random Forest Classifier to the training data: Model 3\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_3 = RandomForestClassifier(n_estimators = 5, criterion = 'entropy')\nclassifier_3.fit(X_resampled, y_resampled)\n\n# Fitting classifier to the training data: Model 4 \nfrom sklearn.linear_model import LogisticRegression\nclassifier_4 = LogisticRegression(penalty = 'l1', random_state = 0)\nclassifier_4.fit(X_resampled, y_resampled)\n\n# Fitting Balanced Bagging Classifier to the training data: Model 5\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_5 = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n                                       n_estimators = 5, bootstrap = True)\nclassifier_5.fit(X_resampled,y_resampled)\n\n# Fitting Decision Tree to the training data: Model 6\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_6 = DecisionTreeClassifier()\nclassifier_6.fit(X_resampled,y_resampled)\n\n# Fitting Naive Bayes to the training data: Model 7\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_7 = GaussianNB()\nclassifier_7.fit(X_resampled,y_resampled)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"072f3870b2f54986c4ca345453dd3fa0adbad86c","_cell_guid":"b515f6a5-4c6f-4ed1-a1fa-e22cbee9e803","collapsed":true,"trusted":true},"cell_type":"code","source":"# Predicting the results\ny_pred_1 = classifier_1.predict(X_test)\ny_pred_2 = classifier_2.predict(X_test)\ny_pred_3 = classifier_3.predict(X_test)\ny_pred_4 = classifier_4.predict(X_test)\ny_pred_5 = classifier_5.predict(X_test)\ny_pred_6 = classifier_6.predict(X_test)\ny_pred_7 = classifier_7.predict(X_test)\n\n# Creating the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_1 = confusion_matrix(y_test,y_pred_1)\naccuracy_1 = (cm_1[0,0]+cm_1[1,1])/len(y_test)\n\ncm_2 = confusion_matrix(y_test,y_pred_2)\naccuracy_2 = (cm_2[0,0]+cm_2[1,1])/len(y_test)\n\ncm_3 = confusion_matrix(y_test,y_pred_3)\naccuracy_3 = (cm_3[0,0]+cm_3[1,1])/len(y_test)\n\ncm_4 = confusion_matrix(y_test,y_pred_4)\naccuracy_4 = (cm_4[0,0]+cm_4[1,1])/len(y_test)\n\ncm_5 = confusion_matrix(y_test,y_pred_5)\naccuracy_5 = (cm_5[0,0]+cm_5[1,1])/len(y_test)\n\ncm_6 = confusion_matrix(y_test,y_pred_6)\naccuracy_6 = (cm_6[0,0]+cm_6[1,1])/len(y_test)\n\ncm_7 = confusion_matrix(y_test,y_pred_7)\naccuracy_7 = (cm_7[0,0]+cm_7[1,1])/len(y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4edb1db03ad3ddb2c300f56538354c22428062b3","_cell_guid":"ec1fb258-8062-4753-a62d-55c7397a99aa"},"cell_type":"markdown","source":"\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"eee0243b30f097e2e442a721eecf6268a91406f2","_cell_guid":"db2bdc9f-81b5-47d3-8091-4d53109bd84c","trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\nprecision_1, recall_1, f_score_1, support = precision_recall_fscore_support(y_test, y_pred_1, average = None)\nprint(\"\\nFor Model 1 - XGBoost:\")\nprint(\"Precision:\",precision_1)\nprint(\"Recall:\",recall_1)\nprint(\"F-Score:\",f_score_1)\nprint(\"Accuracy_XGBoost:\",accuracy_1*100,'%')\n\nprecision_2, recall_2, f_score_2, support = precision_recall_fscore_support(y_test, y_pred_2, average = None)\nprint(\"\\nFor Model 2 - SVC:\")\nprint(\"Precision:\",precision_2)\nprint(\"Recall:\",recall_2)\nprint(\"F-Score:\",f_score_2)\nprint(\"Accuracy_SVC:\",accuracy_2*100,'%') \n\nprecision_3, recall_3, f_score_3, support = precision_recall_fscore_support(y_test, y_pred_3, average = None)\nprint(\"\\nFor Model 3 - Random Forest:\")\nprint(\"Precision:\",precision_3)\nprint(\"Recall:\",recall_3)\nprint(\"F-Score:\",f_score_3)\nprint(\"Accuracy_RF:\",accuracy_3*100,'%') \n\nprecision_4, recall_4, f_score_4, support = precision_recall_fscore_support(y_test, y_pred_4, average = None)\nprint(\"\\nFor Model 4 - Logistic:\")\nprint(\"Precision:\",precision_4)\nprint(\"Recall:\",recall_4)\nprint(\"F-Score:\",f_score_4)\nprint(\"Accuracy_Logistic:\",accuracy_4*100,'%') \n\nprecision_5, recall_5, f_score_5, support = precision_recall_fscore_support(y_test, y_pred_5, average = None)\nprint(\"\\nFor Model 5 - BalancedBaggingClassifier:\")\nprint(\"Precision:\",precision_5)\nprint(\"Recall:\",recall_5)\nprint(\"F-Score:\",f_score_5)\nprint(\"Accuracy_BalancedBagging:\",accuracy_5*100,'%')\n\nprecision_6, recall_6, f_score_6, support = precision_recall_fscore_support(y_test, y_pred_6, average = None)\nprint(\"\\nFor Model 6 - Decision Tree Classifier:\")\nprint(\"Precision:\",precision_6)\nprint(\"Recall:\",recall_6)\nprint(\"F-Score:\",f_score_6)\nprint(\"Accuracy_DecisionTree:\",accuracy_6*100,'%') \n\nprecision_7, recall_7, f_score_7, support = precision_recall_fscore_support(y_test, y_pred_7, average = None)\nprint(\"\\nFor Model 7 - Naive Bayes Classifier:\")\nprint(\"Precision:\",precision_7)\nprint(\"Recall:\",recall_7)\nprint(\"F-Score:\",f_score_7)\nprint(\"Accuracy_NaiveBayes:\",accuracy_7*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a678f58a02df58ac61bc1dbb6ca3c5db06b1ec4d","_cell_guid":"6673c31e-e9d4-49d8-9126-a388b257238a","trusted":true},"cell_type":"code","source":"# Comparing the Accuracies of various models\nACCURACY = np.vstack((accuracy_1,accuracy_2,accuracy_3,accuracy_4,accuracy_5,accuracy_6,accuracy_7))\nnumber = np.array([1,2,3,4,5,6,7])\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(number, ACCURACY, color = 'r', marker = 'o', linewidths = 3)\nfor j in range(0,len(ACCURACY)):\n       ax.annotate('%0.3f' % (ACCURACY[j]),(number[j], ACCURACY[j]))\n\nplt.xlabel('MODELS')    \nplt.ylabel('ACCURACY')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"263ecaf1020e94a736a81301c1e6124a0f66199a"},"cell_type":"markdown","source":"All the classifiers are performing very closely, except Naive Bayes classfier which gives extremely poor accuracy.\nBut since as the dataset is imbalanced, accuracy is perhaps not a true measure of a model's performance.\nOne can guage its model prformance using F-Score.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e72de3f7e0c49543ae7b80d76c306193864dc26c","_cell_guid":"ee3684e4-99ec-49e8-990a-1eae3b06b66b","trusted":true},"cell_type":"code","source":"# Comparing the F-Values of various models\nF_SCORE = np.vstack((f_score_1[1],f_score_2[1],f_score_3[1],f_score_4[1],f_score_5[1],f_score_6[1],f_score_7[1]))\nnumber = np.array([1,2,3,4,5,6,7])\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(number, F_SCORE, marker = 'x', linewidths = 3)\nfor i in range(0,len(F_SCORE)):\n       ax.annotate('%0.3f' % (F_SCORE[i]),(number[i], F_SCORE[i]))\n\nplt.xlabel('MODELS')    \nplt.ylabel('F-SCORE')\n\n  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"178b6e48ca138d53ba49e85be7a69b72843ec0e4","_cell_guid":"df789bd7-be45-4d8d-a9a8-8e9944393237"},"cell_type":"markdown","source":"While Naive Bayes classifier has the lowest F-Score, we can clearly observe, Model 5 - BalancedBaggingClassifier outperforms all the classifiers with the highest F-Score. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f36cd0b61263cfb5d33837d4f10b3fa4a75b23b8","_kg_hide-output":true,"_cell_guid":"07742935-dfb6-4197-a1a9-a7840849a47c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"029303c80432765545926cf2609fc6c39c48b58f","_cell_guid":"1a64d87b-8fe9-41a7-9ea3-7a0bcc0678ef"},"cell_type":"markdown","source":"In order to further evaluate the model performancer, we investigate the ROC curve and AUC.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"52acc2399622c13d7c3a6db31915b1515d680b6f","_cell_guid":"b11a7b16-dd04-48e9-b5df-499a004123ad","trusted":true},"cell_type":"code","source":"# Plot ROC curves\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\ni = 0\n\nprobas_1 = classifier_1.predict_proba(X_test)\nprobas_2 = classifier_2.predict_proba(X_test)\nprobas_3 = classifier_3.predict_proba(X_test)\nprobas_4 = classifier_4.predict_proba(X_test)\nprobas_5 = classifier_5.predict_proba(X_test)\nprobas_6 = classifier_6.predict_proba(X_test)\nprobas_7 = classifier_7.predict_proba(X_test)\n\nprobas = np.vstack((probas_1,probas_2,probas_3,probas_4,probas_5,probas_6,probas_7))\n\n# Compute ROC curve and area the curve\npointer = [0,1102,2204,3306,4408,5510,6612,7714] \nfor a in range(0,7):\n       index_1 = pointer[a]\n       index_2 = pointer[a+1]\n       fpr, tpr, thresholds = roc_curve(y_test, probas[index_1:index_2, 1])\n       tprs.append(interp(mean_fpr, fpr, tpr))\n       tprs[-1][0] = 0.0\n       roc_auc = auc(fpr, tpr)\n       aucs.append(roc_auc)\n       plt.plot(fpr, tpr, lw=2, alpha=0.8,\n                label='Model %d (AUC = %0.2f)' % (a+1, roc_auc))\n       \n       a += 1\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n         label='Luck', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06849604b648ca435d204c7328b9a0284fcfaf9d","_cell_guid":"4e4819d1-e7df-4e94-9574-e977a67d3cff"},"cell_type":"markdown","source":"From the graph we can conclude that the lowet AUC is that of Model 6 - Decision Tree Classifier, whereas Model 1,2,4 and 5 have almost the same AUC.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5cdad0794ced84451fa98b73f63ca4ff3ada902a","_cell_guid":"c2b7057d-2be5-400c-9299-70e0bcd9f520"},"cell_type":"markdown","source":"Thus, on comparing all the three results i.e. F-SCore, Accuracy and AUC, Model 5 - BalancedBaggingClassifier is perhaps the best model to use for in our dataset.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"83ee071f95b72febba1a2b58ba5857a7b08dc46b"},"cell_type":"markdown","source":"# Investigating various sampling techniques\n\nThe goal of this would be to find out the best sampling technique. Techniques evaluated in the section are as follows:\n1) Random Over Sampling technique - Model-1\n2) SMOTE - Model-2\n3) ADASYN - Model-3\n4) Without sampling - Model-4 \n5) With custom sampling - Model-5\n\nP.S. - Model 4 would be the base model (i.e. modeling with original data)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c73b67bab15ad0b03e48241d62ae382c8d5824ac","_cell_guid":"583436af-8a8b-41af-9fa9-17bc86cf5ca9","trusted":true},"cell_type":"code","source":"# Testing various oversampling techniques\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Random Oversampler\nros = RandomOverSampler(random_state=0)\nX_resampled_ros, y_resampled_ros = ros.fit_sample(X_train, y_train)\nprint(sorted(Counter(y_resampled_ros).items()))\n\n# Synthetic Minority Oversampling Technique(SMOTE) \nX_resampled_smote, y_resampled_smote = SMOTE().fit_sample(X_train, y_train)\nprint(sorted(Counter(y_resampled_smote).items()))\n\n# Adaptive Synthetic (ADASYN) sampling method\nX_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_sample(X_train, y_train)\nprint(sorted(Counter(y_resampled_adasyn).items()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c610fc7c0d72302b06057d968e927424e9520004","_cell_guid":"f78f9c6e-bacd-4a20-8a87-cbcd2ecc7497","collapsed":true,"trusted":true},"cell_type":"code","source":"# Feature scaling\nX_resampled_ros = sc.fit_transform(X_resampled_ros)\nX_resampled_smote = sc.fit_transform(X_resampled_smote)\nX_resampled_adasyn = sc.fit_transform(X_resampled_adasyn)\nX_train_wo_sampling = sc.fit_transform(X_train_wo_sampling)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8045a78a599350991ff0bd89778e261d438ac9e1","_cell_guid":"73389563-83c9-4448-aad8-8b3124a1e3c9","trusted":true},"cell_type":"code","source":"# Fitting Balanced Bagging Classifier to the training data: Model 5\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ros = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n                                       n_estimators = 5, bootstrap = True)\nclassifier_ros.fit(X_resampled_ros,y_resampled_ros)\n\nclassifier_smote = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n                                       n_estimators = 5, bootstrap = True)\nclassifier_smote.fit(X_resampled_smote,y_resampled_smote)\n\nclassifier_adasyn = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n                                       n_estimators = 5, bootstrap = True)\nclassifier_adasyn.fit(X_resampled_adasyn,y_resampled_adasyn)\n\nclassifier_wo_sampling = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n                                       n_estimators = 5, bootstrap = True)\nclassifier_wo_sampling.fit(X_train_wo_sampling,y_train_wo_sampling)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"133d0e045f76ed79354d0d8b43371fee99871e31","_cell_guid":"4c747def-b53b-4441-9392-68b40772a3be","collapsed":true,"trusted":true},"cell_type":"code","source":"# Predicting the results\ny_pred_ros = classifier_ros.predict(X_test)\ny_pred_smote = classifier_smote.predict(X_test)\ny_pred_adasyn = classifier_adasyn.predict(X_test)\ny_pred_wo_sampling = classifier_wo_sampling.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1252682efe5cc8f0f76fa0223d7c4839fbe2ba4e","_cell_guid":"8cc6f2b0-1147-415d-a21f-71bb1db8444b","collapsed":true,"trusted":true},"cell_type":"code","source":"# Creating the confusion matrix\ncm_ros = confusion_matrix(y_test,y_pred_ros)\naccuracy_ros = (cm_ros[0,0]+cm_ros[1,1])/len(y_test)\n\ncm_smote = confusion_matrix(y_test,y_pred_smote)\naccuracy_smote = (cm_smote[0,0]+cm_smote[1,1])/len(y_test)\n\ncm_adasyn = confusion_matrix(y_test,y_pred_adasyn)\naccuracy_adasyn = (cm_adasyn[0,0]+cm_adasyn[1,1])/len(y_test)\n\ncm_wo_sampling = confusion_matrix(y_test,y_pred_wo_sampling)\naccuracy_wo_sampling = (cm_wo_sampling[0,0]+cm_wo_sampling[1,1])/len(y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aa7166e7acf71550019d40d60243f443706a403","_cell_guid":"5f5ea64b-fcf6-499b-b37b-87d0800ccd6e","trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\nprecision_ros, recall_ros, f_score_ros, support = precision_recall_fscore_support(y_test, y_pred_ros, average = None)\nprint(\"\\nFor RandomOversampling:\")\nprint(\"Precision:\",precision_ros)\nprint(\"Recall:\",recall_ros)\nprint(\"F-Score:\",f_score_ros)\nprint(\"Accuracy_RandomOversampling:\",accuracy_ros*100,'%')\n\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision_smote, recall_smote, f_score_smote, support = precision_recall_fscore_support(y_test, y_pred_smote, average = None)\nprint(\"\\nFor SMOTE:\")\nprint(\"Precision:\",precision_smote)\nprint(\"Recall:\",recall_smote)\nprint(\"F-Score:\",f_score_smote)\nprint(\"Accuracy_smote:\",accuracy_smote*100,'%')\n\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision_adasyn, recall_adasyn, f_score_adasyn, support = precision_recall_fscore_support(y_test, y_pred_adasyn, average = None)\nprint(\"\\nFor ADASYN:\")\nprint(\"Precision:\",precision_adasyn)\nprint(\"Recall:\",recall_adasyn)\nprint(\"F-Score:\",f_score_adasyn)\nprint(\"Accuracy_adasyn:\",accuracy_adasyn*100,'%')\n\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision_wo_sampling, recall_wo_sampling, f_score_wo_sampling, support = precision_recall_fscore_support(y_test, y_pred_adasyn, average = None)\nprint(\"\\nWithout Sampling:\")\nprint(\"Precision:\",precision_wo_sampling)\nprint(\"Recall:\",recall_wo_sampling)\nprint(\"F-Score:\",f_score_wo_sampling)\nprint(\"Accuracy_wo_sampling:\",accuracy_wo_sampling*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60c7950495229147d981bb6d437a4e587e26501e","_cell_guid":"83583f68-2756-426e-b764-3e03fadd5c0f","trusted":true},"cell_type":"code","source":"# Comparing the F-Values of various models\nF_SCORE = np.vstack((f_score_ros[1],f_score_smote[1],f_score_adasyn[1],f_score_wo_sampling[1],f_score_5[1],)) #,f_score_2[1]\nnumber = np.array([1,2,3,4,5])\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(number, F_SCORE, marker = 'x', linewidths = 3)\nfor i in range(0,len(F_SCORE)):\n       ax.annotate('%0.3f' % (F_SCORE[i]),(number[i], F_SCORE[i]))\n\nplt.xlabel('Sampling Techniques')    \nplt.ylabel('F-SCORE')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e0c04bb020fae20ec3326ca94d0a9ac51ce5ee3"},"cell_type":"markdown","source":"We can clearly observe, that Model 5(our original Model), which uses custom sampling, is the best model..\n**So, to conclude:\nWe should use BalancedBaggingClassifier, with custom sampling to model our dataset.**\nFurther improvents can be achieved by hypertuning various parameters.","outputs":[],"execution_count":null}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}