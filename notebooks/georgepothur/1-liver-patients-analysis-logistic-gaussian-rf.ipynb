{"cells":[{"metadata":{"_uuid":"0d676435e908abb9226cd620d6d8bece11f2daf8"},"cell_type":"markdown","source":"#  Indian Liver Patients Analysis (Logistic, Gaussian, Random Forest)"},{"metadata":{"_uuid":"42f6edda3b24234d471c2003a94e80c2baee8ff2"},"cell_type":"markdown","source":"Tasks to perform\n1. Data Analysis\n2. Data cleanup\n3. Feature selection\n4. Train and test different models (Logistic, Gaussian, Random Forest)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"720987f663f04c2a23badf83acff42f5ac2122d4"},"cell_type":"code","source":"# I import the libraries as and when they are required in the code. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dadbbf240f10959a7c3956eeb7b04c3f292bf37d"},"cell_type":"code","source":"# Load the data\nraw_data = pd.read_csv('../input/indian_liver_patient.csv')\nip_data = raw_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0453f3db62507204e16e6f8bfee7ae069913313"},"cell_type":"code","source":"#ip_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b8b21b27a1920dcbe076a4673b380e890f4b1cf"},"cell_type":"code","source":"# See the types of data and missing values in the dataset.\n# Categorical data (like gender) need to be converted using dummy variables. \nip_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32ae92561113a6ec184861aef25a1b1a28ae11b7"},"cell_type":"code","source":"#Since there are only 4 records with null values in Albumin_and_Globulin_Ratio column, we can drop those records.\nip_data = ip_data.dropna(how='any', axis = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bffaf433b8596e5abea2e3df61dc76e1c85683ef"},"cell_type":"code","source":"# Convert Gender to 0s (for Male) and 1s (for Female)\nip_data['Gender'] = ip_data['Gender'].map({'Male':0, 'Female':1})\nip_data['Dataset'] = ip_data['Dataset'].map({2:0, 1:1}) # To solve ValueError: endog must be in the unit interval.\n\n# Check if mapping happened properly\nip_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c388a2eba0243801c796a0f1ff01db8975d83b"},"cell_type":"code","source":"# Correlations between variables help us identify the features that can be excluded. \n# We can exclude one in two features which has a strong correlatoin (|correlation| > 0.5) with another feature\n# e.g: In the below given case, Total_Bilirubin and Direct_Bilirubin are strongly correlated. So we can\n# discard one of those features. The exclusion of features happens 2 cells down.  \nip_corr = ip_data.drop(['Gender', 'Dataset'], axis = 1)\nip_corr.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c30d6eeb1db4fb9cc5fb306126fe6810ea47c64"},"cell_type":"markdown","source":"### Split the dataset into train and test"},{"metadata":{"trusted":true,"_uuid":"367a550137a23a50dd628f2bee28fd872b3def7b"},"cell_type":"code","source":"samples_count = ip_data.shape[0]\n# You will see drastic change in models' accuracy when you change the train and test sample proportion (80:20, 70:30 etc)\nip_train_count = int(0.8*samples_count) \nip_test_count = samples_count - ip_train_count\n\nip_train_data = ip_data[:ip_train_count]\nip_test_data = ip_data[ip_train_count:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5a58919205d48de3ec0ed28897d2058870a6301"},"cell_type":"code","source":"print(ip_train_count)\nprint(ip_test_count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d17b5912ee18ea69fce18fd3faaa447fa557145"},"cell_type":"markdown","source":"### Declare the Dependent (y_...) and Independent (X_...) variables"},{"metadata":{"trusted":true,"_uuid":"99b7dce0bd1a141639284375ff868c86e0dc1bfd"},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35e756c0bc1ac131d3d5f7ded379d12285c4a7b3"},"cell_type":"code","source":"## You can start with all features and then optimize the model by removing the features from the model.\n\nX1_train = ip_train_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',\n              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]\n\n#X1_train = ip_train_data[['Age', 'Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin']]\n\nX_train= sm.add_constant(X1_train)\n#X_train= X1_train.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant\ny_train = ip_train_data['Dataset']\n\n\nX1_test = ip_test_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',\n              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]\n\n#X1_test = ip_test_data[['Age','Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin' ]]\n\nX_test = sm.add_constant(X1_test)\n#X_test = X1_test.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant\ny_test = ip_test_data['Dataset']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbbcb74a516997cd8885db3c5dd6eb286ec65645"},"cell_type":"markdown","source":"## Logit function from statsmodels"},{"metadata":{"trusted":true,"_uuid":"0f8713514177da57b34c211163cb6ded7b6add5b"},"cell_type":"code","source":"reg_log = sm.Logit(y_train,X_train)\nresult_log = reg_log.fit()\n\nresult_log.summary2()\n\n# See the values against each feature in 'P>|z|' column. The value less than 0.05 means the feature is insignificant in the model.\n# Surprisingly, gender is insignificant in this model. \n# We saw a strong correlation between Total_Bilirubin and Direct_Bilirubin, and we could drop one of those features.\n# But in this model both Total_Bilirubin and Direct_Bilirubin are insignificant(P > 0.05). So we must discard both features.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e9a2131f2826cfa24e8eed1329f101e7c00372e"},"cell_type":"code","source":"# Confusion matrix using the train dataset\nresult_log.predict()\nresult_log.pred_table()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df92d1c6038a22f54cff7741974e6043fe793a9"},"cell_type":"code","source":"# Print X_test and X_train and see if the order of features are same in both datasets\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da069d425756caf1751081f9273f7a7b25fa1c87"},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e074d492d8896861b5fdec66baabc65ca480e41"},"cell_type":"code","source":"logit_predicted = result_log.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a234ccdec1b9382916b37a2d5609ba0f6fdf5c25"},"cell_type":"code","source":"# Uncomment below line, if you wanted to see the predicted values\n# logit_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c297f005a8b5babd19f9cb0e3478d0c8d592278"},"cell_type":"code","source":"# Confusion Matrix of predicted values. if you wanted to use sklearn's 'confusion_matrix', \n# you should convert float values in 'logit_predicted' to 0s and 1s.\n\n# Here I use a custom confusion matrix code \nbins = np.array([0,0.5,1])\ncm_log = np.histogram2d(y_test, logit_predicted, bins = bins)[0]\nlogit_accuracy = (cm_log[0,0] + cm_log[1,1])/cm_log.sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c07e217d1f085d1835dda360d9ad061497d5ea79"},"cell_type":"code","source":"print('Confusion Matrix (Logit): \\n', cm_log)\nprint('---------------------------------------------')\nprint('Accuracy (Logit): \\n', round(logit_accuracy*100,2), '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b1780e0f23414d9b9fee35708bf113acab54b43"},"cell_type":"markdown","source":"## Logitstic Regression (sklearn)"},{"metadata":{"trusted":true,"_uuid":"a3b4f2e8f164f0eca5076a2398bd0bc1fd4fbf81"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\n\nlog_predicted = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2facd5fac11c1d20dd9c35eea431364e659caaf7"},"cell_type":"code","source":"print('Confusion Matrix (Logistic Reg): \\n', confusion_matrix(y_test,log_predicted))\nprint('---------------------------------------------')\nprint('Accuracy (Logistict Reg): \\n', round(accuracy_score(y_test, log_predicted)*100,2), '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0192e0044814301cb80e66d9d23ffa6247219283"},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"9857727828e23a6d0fb4502668510b6aeb0e063e"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\nresult_gauss = gaussian.fit(X_train,y_train)\n\ngauss_predicted = gaussian.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ae3a09c35e82d01a3b921bbc21ff7afa7c74b7d"},"cell_type":"code","source":"# Uncomment below line, if you wanted to see the predicted values\n#gauss_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99dd0219b58cd48ba13c84c3500ca22c8fb7bbe4"},"cell_type":"code","source":"print('Confusion Matrix (Gaussian): \\n', confusion_matrix(y_test,gauss_predicted))\nprint('---------------------------------------------')\nprint('Accuracy (Gaussian): \\n', round(accuracy_score(y_test, gauss_predicted)*100,2), '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1f3a5b391aaaf5d28447c4bb7f1eab1587c4ec3"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true,"_uuid":"de12744a1800adc97131ed6160c08f39ab991484"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, y_train)\n\nrf_predicted = random_forest.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"538e999531a379946a55d65a457e4e92eb1091b9"},"cell_type":"code","source":"print('Confusion Matrix (Random Forest): \\n', confusion_matrix(y_test,rf_predicted))\nprint('---------------------------------------------')\nprint('Accuracy (Random Forest): \\n', round(accuracy_score(y_test, rf_predicted)*100,2), '%')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30f6f4151b0c15ff76aacdbabb66e28e71953581"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}