{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport itertools\nimport sklearn\nimport scipy\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport squarify\nimport matplotlib.ticker as ticker\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\nfrom scipy.spatial.distance import euclidean\nimport sys\nfrom sklearn.preprocessing import MinMaxScaler\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In this EDA, I'm going to solve 3 questions"},{"metadata":{},"cell_type":"markdown","source":"1. Does the frequency of crimes have any pattern?\n2. Is it possible to forecast the daily frequency of crimes? How?\n3. If the frequency of crime has some patterns, why is that?"},{"metadata":{},"cell_type":"markdown","source":"# 1 Data Overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"Rawdata = pd.read_csv('../input/crimes-in-boston/crime.csv',encoding='latin-1')\n# Drop \"INCIDENT_NUMBER\" colume, we are not going to use it in our analysis.\nRawdata.drop(\"INCIDENT_NUMBER\",axis=1, inplace=True) \n# Split 'OCCURRED_ON_DATE' colume into 'DATE' and 'Time'. 'Date' will give us the exact date of the crime\nRawdata[[\"DATE\",\"TIME\"]]=Rawdata['OCCURRED_ON_DATE'].str.split(\" \",expand=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rawdata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So totally, we have 319071 pieces of data and most columes have no NaN cell, which is a good thing. In this EDA, I will use 'DISTRICT', 'DATE' columes very frequently."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line chart\ndef lineplt(x,y,xlabel,ylabel,title,size,tick_spacing):\n    fig,ax=plt.subplots(figsize = size)\n    plt.plot(x,y)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n    plt.xlabel(xlabel,fontsize = 15)\n    plt.ylabel(ylabel,fontsize = 15)\n    plt.title(title,fontsize = 20)\n    plt.show()\n\n# Create 2 columes DateFrame\ndef createdf(c1,d1,c2,d2):\n    dic = {c1:d1,c2:d2}\n    df = pd.DataFrame(dic)\n    return df\n\n# Plot histogram\ndef plthis(d,bin, title):\n    plt.figure(figsize=(10,8))\n    plt.hist(d, bins=bin)\n    plt.title(title, fontsize = 20)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Does the frequency of crimes have any pattern?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put Date and Count into a new Dataframe\nc = createdf(\"Date\",Rawdata[\"DATE\"].value_counts().index,\"Count\",Rawdata[\"DATE\"].value_counts())\n\n# c is the total number of crimes per day\nc.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 The distribution of the Counts of Crimes by date"},{"metadata":{"trusted":true},"cell_type":"code","source":"plthis(c[\"Count\"],50, \"Crimes Count Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('skewness is ' + str(c['Count'].skew()))\nprint('kurtosis is ' + str(c['Count'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I seperate the data into 50 bins and plot the data using histogram. We can see that the distribution seems like a normal distribution. Many days have about 250 ~ 300 crimes happened. Some days can have more than 350 crimes and some days have less than 150 crimes happened.\n\nThe Skewness is negative means that the distribution is left skewed, but not that much.  \nThe kurtosis is greater than 0 means that the peak of the distribution is sharper than a standard normal distribution.\n\nLet's test if the crimes count distribution follows a normal distribution, even it seems like a normal distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"bin=pd.cut(c[\"Count\"],50)\nfre= createdf(\"Bin\",bin.value_counts().index,\"Count\",bin.value_counts())\nfre_sort = fre.sort_values(by = \"Bin\", ascending = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.1 Shapiro-Wilk test\n(For N > 5000 the W test statistic is accurate but the p-value may not be.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"(_,p) = scipy.stats.shapiro(fre_sort[\"Count\"])\nprint('p-value is ' + str(p))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2 Kolmogorov-Smirnov test"},{"metadata":{"trusted":true},"cell_type":"code","source":"(_,p) = scipy.stats.kstest(fre_sort[\"Count\"],'norm')\nprint('p-value is ' + str(p))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the result of this two tests, we can see that the p value is very small, much smaller than 5%. So we can conclude that the distribution is **Significantly different** from normal distribution under 95% confidence."},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Distribution of Crimes by Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"c=c.sort_values(by=\"Date\",ascending = True)\nlineplt(c[\"Date\"],c[\"Count\"],\"Date\",\"Count\",\"Crimes by Time\",(20,15),80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, we can see there are many peaks and troughs and they shows a kind of pattern like \"sin\" function. I'm going to use some numbers and charts to describe the pattern in detail."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,16))\nax1 = fig.add_subplot(411)\nfig = plot_acf(c[\"Count\"],lags=200,ax=ax1)\nplt.title('Autocorrelation Lag=200')\nax2 = fig.add_subplot(412)\nfig = plot_pacf(c[\"Count\"],lags=200,ax=ax2)\nplt.title('Partial Autocorrelation Lag=200')\nax3 = fig.add_subplot(413)\nfig = plot_acf(c[\"Count\"],lags=15,ax=ax3)\nplt.title('Autocorrelation Lag=15')\nax4 = fig.add_subplot(414)\nfig = plot_pacf(c[\"Count\"],lags=15,ax=ax4)\nplt.title('Partial Autocorrelation Lag=15')\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n                wspace=None, hspace=0.5)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the Autocorrelation and Partical Autocorrelation (lag = 200 and lag = 15), we can conclude that:\n\n(1) From lag 1 to lag 100, the correlation is positive and from lag 100 to lag 200, the correlation is negative. So around every 100 days, the correlation will be reversed. This can describe the \"Sin\" shape.\n\n(2) When we make the lag shorter, we can see more details about the correlation. The partical correlations are significant when lag 1, lag 6 and lag 7. So we can conclude that the crimes are correlated with yesterday and the same day in last week."},{"metadata":{"trusted":true},"cell_type":"code","source":"res = sm.tsa.seasonal_decompose(c['Count'],freq=12,model=\"additive\")\n# # original = res\ntrend = res.trend\nseasonal = res.seasonal\nresidual = res.resid\n\nfig,ax=plt.subplots(figsize = (20,15))\nax1 = fig.add_subplot(411)\nax1.xaxis.set_major_locator(ticker.MultipleLocator(80))\nax1.plot(c['Count'], label='Original')\nax1.legend(loc='best')\nax2 = fig.add_subplot(412)\nax2.xaxis.set_major_locator(ticker.MultipleLocator(80))\nax2.plot(trend, label='Trend')\nax2.legend(loc='best')\nax3 = fig.add_subplot(413)\nax3.xaxis.set_major_locator(ticker.MultipleLocator(10))\nax3.plot(seasonal[:100],label='Seasonality')\nax3.legend(loc='best')\nax4 = fig.add_subplot(414)\nax4.xaxis.set_major_locator(ticker.MultipleLocator(80))\nax4.plot(residual, label='Residuals')\nax4.legend(loc='best')\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After seasonal decomposing, we can have a clear view of the pattern of the distribution of crimes. But there is one problem. When we try to plot the data, the chart will have different shape if we use different scales. For example, if the range of y axis is between 0~300, then the variation will be very clear, and we can see if there is a trend. But if the range is between 0~3000, then the variation will be unclear and the shape of the date could be a straight line. So we need to value the stationarity of the data becasue I'm planning to use ARIMA model."},{"metadata":{},"cell_type":"markdown","source":"# 2.2.1 ADF Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(series,mlag = 365, lag = None,):\n    print('ADF Test Result')\n    res = adfuller(series, maxlag = mlag, autolag = lag)\n    output = pd.Series(res[0:4],index = ['Test Statistic', 'p value', 'used lag', 'Number of observations used'])\n    for key, value in res[4].items():\n        output['Critical Value ' + key] = value\n    print(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(c['Count'],lag = 'AIC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The report shows that, the \"used lag\" is 34 and p value is 0.19. So we can conclude, in the range of 34 days, we can **NOT Reject** the null hypotheses which is the time series is non-stationary."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 ARIMA Model"},{"metadata":{},"cell_type":"markdown","source":"Because the data is not stationary, we need to do first difference to the date in order to make it stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = c.copy()\nd1['Count'] = d1['Count'].diff(1)\nd1 = d1.dropna()\nlineplt(d1[\"Date\"],d1[\"Count\"],\"Date\",\"Count\",\"Crimes by Time\",(20,15),80)\nprint('Average= '+str(d1['Count'].mean()))\nprint('Std= ' + str(d1['Count'].std()))\nprint('SE= ' + str(d1['Count'].std()/math.sqrt(len(d1))))\nprint(test_stationarity(d1['Count'],lag = 'AIC'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the fist differencing, the chat looks much more stationary and the ADF test shows a pretty low p value. So we can rejct H0. The Time series is stational after first differencing."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_2 = plt.figure(figsize=(16,8))\nax1_2 = fig_2.add_subplot(211)\nfig_2 = plot_acf(d1[\"Count\"],lags=15,ax=ax1_2)\nax2_2 = fig_2.add_subplot(212)\nfig_2 = plot_pacf(d1[\"Count\"],lags=15,ax=ax2_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Autocorrelation and Partial Autocorrelation charts are not very perfet. We can see there is a seasonal pattern every 7 days. \n\nWe will deal with it later. But let's build the ARIMA model first"},{"metadata":{"trusted":true},"cell_type":"code","source":"timeseries = c['Count']\np,d,q = (4,1,2)\narma_mod = ARMA(timeseries,(p,d,q)).fit()\nsummary = (arma_mod.summary2(alpha=.05, float_format=\"%.8f\"))\nprint(summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_data = arma_mod.predict(start='2016-07-01', end='2017-07-01', dynamic = False)\ntimeseries.index = pd.DatetimeIndex(timeseries.index)\nfig, ax = plt.subplots(figsize=(20, 15))\nax = timeseries.plot(ax=ax)\npredict_data.plot(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the prediction of 1 year data, the yellow line is the prediction of daily crimes. It looks the predictions are always **under estimated**. \n\nIt could because the seasonal correlation. So we need to use another model called SARIMA model."},{"metadata":{},"cell_type":"markdown","source":"## 2.4 SARIMA Model"},{"metadata":{},"cell_type":"markdown","source":"In this model, we will try all combinations of (p,d,q) and (P,D,Q,7) and use AIC standard to find the best combination that can minimize AIC. I use \"7\" becasue the period is 7 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"p = d = q = range(0, 2)\n \n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n \n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 7) for x in list(itertools.product(p, d, q))]\n \nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame(columns = ['order', 'seasonal_order', 'AIC'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = SARIMAX(c['Count'],order=param,seasonal_order=param_seasonal) \n            results = mod.fit()\n            data = {'order': param, 'seasonal_order': param_seasonal, 'AIC':results.aic}\n#             print('ARIMA{}x{}7 - AIC:{}'.format(param, param_seasonal, results.aic))\n            res = res.append(data,ignore_index=True)\n        except:\n            continue\nres = res.sort_values(by = 'AIC', ascending = True)\nprint(res.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result shows that (1,1,1) (1,1,1,7) is the best combination to minimize AIC. We will use it to build our SARIMA model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=SARIMAX(c['Count'], order=(1,1,1), seasonal_order=(1,1,1, 7)).fit()\nsummary = model.summary()\nprint(summary)\n# print(c['Count'].index.inferred_freq)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.plot_diagnostics(figsize=(15, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_data = model.predict(start='2016-07-01', end='2017-07-01', dynamic = False)\ntimeseries.index = pd.DatetimeIndex(timeseries.index)\nfig, ax = plt.subplots(figsize=(20, 15))\nax = timeseries.plot(ax=ax)\npredict_data.plot(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the new prediction is much better. But it's not doing well when there are extreme numbers."},{"metadata":{},"cell_type":"markdown","source":"Let's try to forecast the data for next 30 days!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get forecast 30 steps ahead in future\npred_uc = model.get_forecast(steps=30)\n\n# Get confidence intervals of forecasts\npred_ci = pred_uc.conf_int()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = c['Count'][-60:].plot(label='observed', figsize=(15, 10))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Counts')\n \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"During our analysis, we find the weekly correlation influence a lot. So let's plot the distribution of weekly crimes."},{"metadata":{"trusted":true},"cell_type":"code","source":"week = createdf(\"Week\",Rawdata[\"DAY_OF_WEEK\"].value_counts().index,\"Count\",Rawdata[\"DAY_OF_WEEK\"].value_counts())\nweek=week.reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\nplt.bar(week[\"Week\"] , week[\"Count\"], width=0.3)\nplt.ylim(36000, 50000)\nplt.title('Crimes by WeekDay')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result shows:\n\nMost crimes happened on Friday and least crimes happened on Sunday. There are many parameters can cause this results, we need more outside data to identify the reason."},{"metadata":{},"cell_type":"markdown","source":"# 3. Why the distribution of crimes shows a pattern?"},{"metadata":{},"cell_type":"markdown","source":"We know the crimes have a kind of pattern and I want to know why is that. To simplify, I will only use 1 year data to do analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = Rawdata[(Rawdata['DATE'] > \"2016-07-01\") & (Rawdata['DATE'] < \"2017-08-01\")]\ntarget = target.sort_values(by=\"DATE\",ascending = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = createdf(\"Date\",target[\"DATE\"].value_counts().index,\"Count\",target[\"DATE\"].value_counts())\nt1 = t1.sort_values(by=\"Date\",ascending = True)\nlineplt(t1[\"Date\"],t1[\"Count\"],\"Date\",\"Count\",\"Crimes by Time(2016-07-01~2017-08-01)\",(15,8),80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(t1['Count'],mlag = 180,lag='AIC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target[\"DISTRICT\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The everyday crimes count is contributed by these district. So I want to see if some of these districts have a significant influence to the whole distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# target = target.dropna()\nfig,ax = plt.subplots(figsize =(15,40))\n# ax.xaxis.set_major_locator(ticker.MultipleLocator(5))\ni = 0\nfor dis in target[\"DISTRICT\"].unique():\n    if dis is not np.nan :\n        i += 1\n        da = target[target[\"DISTRICT\"] == dis]\n        d = createdf(\"Date\",da[\"DATE\"].value_counts().index,\"Count\",da[\"DATE\"].value_counts())\n        d = d.sort_values(by=\"Date\",ascending = True)\n        fig.add_subplot(12,1,i)\n        plt.plot(d[\"Date\"],d[\"Count\"])     \n        plt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n                wspace=None, hspace=0.4)\n        ax=plt.gca()\n        ax.xaxis.set_major_locator(ticker.MultipleLocator(50))\n        plt.title(dis,fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the plot of each district, it's really hard to identify the influence of each district.\n\nI plan to use the similarity of shapes to represent the influence. If the shape of the distributino of one District is very similar to the whole distribution, I will say that district has more influence to the whole distribution."},{"metadata":{},"cell_type":"markdown","source":"Because the ranges of the distribution are different, for exmaple the E18 is between 0 to 30 while D4 is between 0 to 60. In order to minimize the influence of scale, we need to standardize the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureScaling(arr):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    result = scaler.fit_transform(arr)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1[\"Count\"]=featureScaling(t1[\"Count\"].values.reshape(-1,1))\nlineplt(t1[\"Date\"],t1[\"Count\"],\"Date\",\"Count\",\"Crimes by Time(2016-07-01~2017-08-01)\",(15,8),80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to use Dynamic Time Warping (DTW) Algorithm to calculate the similarity. The algorithm will return a distance. That represent the distance between 2 time series. A higher distance means the 2 time series are not similar to each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"def dtw(seq1,seq2): #动态时间规整：形参为时间序列seq1,seq2 \n    m1=len(seq1)\n    m2=len(seq2)\n    #初始化距离矩阵\n    distance=np.zeros(shape=(m1,m2)) #m1行,m2列的距离矩阵\n    for i in range(m1):\n        for j in range(m2):\n            distance[i,j]=(seq1[i]-seq2[j])**2 #一维数组元素之间的欧式距离的平方    \n    #构建一个与矩阵d相同大小累积距离矩阵的D\n    D=np.zeros(shape=(m1,m2))\n    D[0,0]=distance[0,0] #第一个元素和距离矩阵保持一致\n    for i in range(1,m1): #累积距离矩阵的左边界\n        D[i,0]=distance[i,0]+D[i-1,0]\n    for j in range(1,m2):#累积距离矩阵的上边界\n        D[0,j]=distance[0,j]+D[0,j-1]\n    for i in range(1,m1):\n        for j in range(1,m2):\n            D[i,j]=distance[i,j]+np.min([D[i-1,j-1],D[i-1,j],D[i,j-1]])\n    return D[m1-1,m2-1] #函数返回值为最小动态规划路径","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dis in target[\"DISTRICT\"].unique():\n    if dis is not np.nan :\n        da = target[target[\"DISTRICT\"] == dis]\n        d = createdf(\"Date\",da[\"DATE\"].value_counts().index,\"Count\",da[\"DATE\"].value_counts())\n        d = d.sort_values(by=\"Date\",ascending = True)\n        d[\"Count\"]=featureScaling(d[\"Count\"].values.reshape(-1,1))\n        print(dis + ' distance: ' + str(dtw(t1[\"Count\"],d[\"Count\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results shows that D4, A1, B3, C11 have a relative low distance and B2 has a median distance. We can say the distribution of these districts could be relatively similar to the whole distribution.\n\nBut we need to see the proportion of each districts."},{"metadata":{"trusted":true},"cell_type":"code","source":"t2 = createdf(\"District\",target['DISTRICT'].value_counts(dropna=False).index,\"Count\",target['DISTRICT'].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t2[\"Count\"].sum()\nt2['Percent'] = t2[\"Count\"]/t2[\"Count\"].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Becasue we have too many districts, a pie chart might not be a good choice. So I choose to use squarify treemap."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,8))\nplot = squarify.plot(sizes = t2[\"Percent\"], # 指定绘图数据\n                     label = t2[\"District\"], # 指定标签\n                     alpha = 0.6, # 指定透明度\n                     value = t2[\"Percent\"].apply(lambda x: format(x, '.2%')) , # 添加数值标签\n                     edgecolor = 'white', # 设置边界框为白色\n                     linewidth =3 # 设置边框宽度为3\n                    )\nplot.set_title('Crimes by Districts',fontdict = {'fontsize':25})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's very interesting to see that D4, C11, B2, B3 and A1 all have more than 10% contribute to the whole crimes. Also the shape of the these districts are similar to the whole distribution. \n\nWe could say the variation of the wholee distribution is cause by the variation of these districts."},{"metadata":{"trusted":true},"cell_type":"code","source":"Rawdata.Lat.replace(-1, None, inplace=True)\nRawdata.Long.replace(-1, None, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart below shows the location of these districts on map"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,8))\nsns.scatterplot(x='Lat',\n               y='Long',\n                hue='DISTRICT',\n                alpha=0.01,\n               data=Rawdata)\nplt.legend(loc=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 Conclusion"},{"metadata":{},"cell_type":"markdown","source":"1. Does the frequency of crimes have any pattern?\n2. Is it possible to forecast the daily frequency of crimes? How?\n3. If the frequency of crime has some patterns, why is that?"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Q1\nThe frequency of crimes looks like a normal shape distribution. But it doesn’t pass the Shapiro-Wilk test and Kolmogorov-Smirnov test, so it's Significantly Different from a normal distribution.\n"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Q2\nIt's possible to forecast the daily frequency of crimes using ARIMA model, but due to the seasonality, the forecasting model is not perfect.\n\nAfter using SARIMA model, the forecasting looks better.\n\nAlso, we find most crimes happened on Friday and least crimes happened on Sunday."},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Q3\n\nThe distribution of the crimes is a \"Sin\" shape. By using Dynamic Time Warping (DTW) Algorithm, we find the shape of D4, A1, B3, C11, B2 is relatively similar to the shape of whole data and these districts each contributes more than 10% of the whole crimes. So we can say it could be the variation of these districts that cause the variation of the whole data."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}