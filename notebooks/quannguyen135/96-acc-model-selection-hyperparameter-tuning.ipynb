{"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.6.0","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python"}},"cells":[{"cell_type":"markdown","source":"# Model Selection and Hyperparameter Tuning\n\nThis notebook concerns the [Kaggle's _Human Activity Recognition with Smartphones_ dataset](https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones); it will guide you through the process of selecting the best out of a number of predicting models, and the subsequent process of hyperparameter tuning for that model.","metadata":{}},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","execution_count":null,"source":"import pandas as pd\nimport numpy as np\n\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df.head()","metadata":{},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"train_df.shape","metadata":{},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"X, y = train_df.iloc[:, 0:len(train_df.columns) - 1], train_df.iloc[:, -1]","metadata":{"collapsed":true},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"test_df = pd.read_csv('../input/test.csv')\nX_test, y_test = test_df.iloc[:, 0:len(test_df.columns) -1], test_df.iloc[:, -1]","metadata":{"collapsed":true},"outputs":[]},{"cell_type":"markdown","source":"## Predicting Models from scikit-learn","metadata":{}},{"cell_type":"code","execution_count":null,"source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom sklearn.metrics import accuracy_score # for evaluation\n\nclassifiers = [\n    DecisionTreeClassifier(),\n    KNeighborsClassifier(7), # because there are 6 different labels\n    SVC(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]\n\nnames = []\nscores = []\n\nfor clf in classifiers:\n    clf = clf.fit(X, y)\n    y_pred = clf.predict(X_test)\n    \n    names.append(clf.__class__.__name__)\n    scores.append(accuracy_score(y_pred, y_test))\n\nscore_df = pd.DataFrame({'Model': names, 'Score': scores}).set_index('Model')\nscore_df","metadata":{},"outputs":[]},{"cell_type":"markdown","source":"## Accuracy Visualization","metadata":{}},{"cell_type":"code","execution_count":null,"source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nax = score_df.plot.bar()\nax.set_xticklabels(score_df.index, rotation=45, fontsize=10)","metadata":{},"outputs":[]},{"cell_type":"markdown","source":"Here we see that Support Vector Machine achieves a significantly better score than any other model. We proceed to perform hyperparameter-tuning for SVC, specifically for parameters `kernel` and `C`.","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning for SVC with Grid Search","metadata":{}},{"cell_type":"code","execution_count":null,"source":"from sklearn.model_selection import GridSearchCV\n\nparameters = {\n    'kernel': ['linear', 'rbf'],\n    'C': [100, 20, 1, 0.1]\n}\n\nselector = GridSearchCV(SVC(), parameters, scoring='accuracy') # we only care about accuracy here\nselector.fit(X, y)\n\nprint('Best parameter set found:')\nprint(selector.best_params_)\nprint('Detailed grid scores:')\nmeans = selector.cv_results_['mean_test_score']\nstds = selector.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, selector.cv_results_['params']):\n    print('%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params))\n    print()","metadata":{},"outputs":[]},{"cell_type":"markdown","source":"We see that the parameter combination of `kernel = 'linear'` and `C = 1` gave the best result. We then use these parameters to perform prediction on the actual testing set.","metadata":{}},{"cell_type":"code","execution_count":null,"source":"clf = SVC(kernel='linear', C=1).fit(X, y)\ny_pred = clf.predict(X_test)\nprint('Accuracy score:', accuracy_score(y_test, y_pred))","metadata":{},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"","metadata":{"collapsed":true},"outputs":[]}]}