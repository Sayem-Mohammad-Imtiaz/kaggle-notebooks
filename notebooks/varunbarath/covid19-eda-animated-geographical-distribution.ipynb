{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Covid19 tweets. EDA. Visualization. Insides.</center></h1>\n\n<center><img src=\"https://ichef.bbci.co.uk/news/1024/cpsprodpb/031C/production/_112869700_gettyimages-1209519827-1.jpg\"></center>"},{"metadata":{},"cell_type":"markdown","source":"### Hello everyone! Here I am going to present some basic analysis of this dataset. We will create some plots based on existing features, do starting sentiment analysis (based on clustering). Also we will create world  map animation and prepare a lot of other interesting things! Let's start!\n\n<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation</center></h3>\n\n* [1. Dataset Quick Overview](#1)\n* [2. Data Visualization](#2)\n* [3. Additional features analysis](#3)\n* [4. Tweets text analysis](#4)\n* [5. Simple sentiment analysis](#5)\n* [6. Animation with geographical distribution of tweets](#6)\n\n\n#### If you are interested in Dynamic monitoring of the tweets, please check another one my kernel: https://www.kaggle.com/isaienkov/covid19-dynamic-in-time-and-space-of-the-tweets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom iso3166 import countries\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/covid19-tweets/covid19_tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:green; border:0; color:white'><center>1. Dataset Quick Overview</center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Let's do a first quick check of our dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see percent of NaNs for every column. We will visualize only columns with at least 1 missed value."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"missed = pd.DataFrame()\nmissed['column'] = df.columns\n\nmissed['percent'] = [round(100* df[col].isnull().sum() / len(df), 2) for col in df.columns]\nmissed = missed.sort_values('percent')\nmissed = missed[missed['percent']>0]\n\nfig = px.bar(\n    missed, \n    x='percent', \n    y=\"column\", \n    orientation='h', \n    title='Missed values percent for every column (percent > 0)', \n    height=400, \n    width=600\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:green; border:0; color:white'><center>2. Data Visualization</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"Let's see top 40 users by number of tweets."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ds = df['user_name'].value_counts().reset_index()\nds.columns = ['user_name', 'tweets_count']\nds = ds.sort_values(['tweets_count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"tweets_count\", \n    y=\"user_name\", \n    orientation='h', \n    title='Top 40 users by number of tweets', \n    width=800, \n    height=800\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.merge(df, ds, on='user_name')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see most popular users."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data = df.sort_values('user_followers', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_followers', 'tweets_count']]\ndata = data.sort_values('user_followers')\n\nfig = px.bar(\n    data.tail(40), \n    x=\"user_followers\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users by number of followers', \n    width=800, \n    height=800\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And most friendly users."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data = df.sort_values('user_friends', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_friends', 'tweets_count']]\ndata = data.sort_values('user_friends')\n\nfig = px.bar(\n    data.tail(40), \n    x=\"user_friends\", \n    y=\"user_name\", \n    color = 'tweets_count',\n    orientation='h', \n    title='Top 40 users by number of friends', \n    width=800, \n    height=800\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how coronavirus affect to new users creation."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['user_created'] = pd.to_datetime(df['user_created'])\ndf['year_created'] = df['user_created'].dt.year\ndata = df.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[data['year_created']>1970]\ndata = data['year_created'].value_counts().reset_index()\ndata.columns = ['year', 'number']\n\nfig = px.bar(\n    data, \n    x=\"year\", \n    y=\"number\", \n    orientation='v', \n    title='User created year by year', \n    width=800, \n    height=600\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from chart coronavirus increases the number of new twitter users."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see top 40 most popular locations by the number of tweets."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ds = df['user_location'].value_counts().reset_index()\nds.columns = ['user_location', 'count']\nds = ds[ds['user_location']!='NA']\nds = ds.sort_values(['count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"count\", \n    y=\"user_location\", \n    orientation='h', title='Top 40 user locations by number of tweets', \n    width=800, \n    height=800\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And also we can see the pie plot for the full picture about users locations."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def pie_count(data, field, percent_limit, title):\n    \n    data[field] = data[field].fillna('NA')\n    data = data[field].value_counts().to_frame()\n\n    total = data[field].sum()\n    data['percentage'] = 100 * data[field]/total    \n\n    percent_limit = percent_limit\n    otherdata = data[data['percentage'] < percent_limit] \n    others = otherdata['percentage'].sum()  \n    maindata = data[data['percentage'] >= percent_limit]\n\n    data = maindata\n    other_label = \"Others(<\" + str(percent_limit) + \"% each)\"\n    data.loc[other_label] = pd.Series({field:otherdata[field].sum()}) \n    \n    labels = data.index.tolist()   \n    datavals = data[field].tolist()\n    \n    trace=go.Pie(labels=labels,values=datavals)\n\n    layout = go.Layout(\n        title = title,\n        height=600,\n        width=600\n        )\n    \n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \npie_count(df, 'user_location', 0.5, 'Number of tweets per location')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to check last one categorical feature - `source`. Lets see top 40 sources by the number of tweets."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ds = df['source'].value_counts().reset_index()\nds.columns = ['source', 'count']\nds = ds.sort_values(['count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"count\", \n    y=\"source\", \n    orientation='h', \n    title='Top 40 user sources by number of tweets', \n    width=800, \n    height=800\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:green; border:0; color:white'><center>3. Additional features analysis<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Lets create new feature - `hashtags_count` that will show us how many hashtags in the current tweet."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['hashtags'] = df['hashtags'].fillna('[]')\ndf['hashtags_count'] = df['hashtags'].apply(lambda x: len(x.split(',')))\ndf.loc[df['hashtags'] == '[]', 'hashtags_count'] = 0\n\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And see the values for new created column."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['hashtags_count'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.scatter(\n    df, \n    x=df['hashtags_count'], \n    y=df['tweets_count'], \n    height=700,\n    width=700,\n    title='Total number of tweets for users and number of hashtags in every tweet'\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of new feature over the number of tweets is expected - a lot of tweets with few number of hashtags and few tweets with huge number of hashtags."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ds = df['hashtags_count'].value_counts().reset_index()\nds.columns = ['hashtags_count', 'count']\nds = ds.sort_values(['count'])\nds['hashtags_count'] = ds['hashtags_count'].astype(str) + ' tags'\n\nfig = px.bar(\n    ds, \n    x=\"count\", \n    y=\"hashtags_count\", \n    orientation='h', \n    title='Distribution of number of hashtags in tweets', \n    width=800, \n    height=600\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will see top 40 users that like to use hashtags a little bit more than others. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ds = df[df['tweets_count']>10]\nds = ds.groupby(['user_name', 'tweets_count'])['hashtags_count'].mean().reset_index()\nds.columns = ['user', 'tweets_count', 'mean_count']\nds = ds.sort_values(['mean_count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"mean_count\", \n    y=\"user\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with higher mean number of hashtags (at least 10 tweets per user)', \n    width=800, \n    height=800\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Just split day and time into separate columns"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date']) \ndf = df.sort_values(['date'])\ndf['day'] = df['date'].astype(str).str.split(' ', expand=True)[0]\ndf['time'] = df['date'].astype(str).str.split(' ', expand=True)[1]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of unique users per day"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ds = df.groupby(['day', 'user_name'])['hashtags_count'].count().reset_index()\nds = ds.groupby(['day'])['user_name'].count().reset_index()\nds.columns = ['day', 'number_of_users']\nds['day'] = ds['day'].astype(str) + ':00:00:00'\nfig = px.bar(\n    ds, \n    x='day', \n    y=\"number_of_users\", \n    orientation='v',\n    title='Number of unique users per day', \n    width=800, \n    height=800\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we are going to check how many tweets were for every day in our dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ds = df['day'].value_counts().reset_index()\nds.columns = ['day', 'count']\nds = ds.sort_values('count')\nds['day'] = ds['day'].astype(str) + ':00:00:00'\nfig = px.bar(\n    ds, \n    x='count', \n    y=\"day\", \n    orientation='h',\n    title='Tweets distribution over days present in dataset', \n    width=800, \n    height=800\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets do the same but for hours"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['hour'] = df['date'].dt.hour\nds = df['hour'].value_counts().reset_index()\nds.columns = ['hour', 'count']\nds['hour'] = 'Hour ' + ds['hour'].astype(str)\nfig = px.bar(\n    ds, \n    x=\"hour\", \n    y=\"count\", \n    orientation='v', \n    title='Tweets distribution over hours', \n    width=800\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets split hashtags into separate column."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def split_hashtags(x): \n    return str(x).replace('[', '').replace(']', '').split(',')\n\ntweets_df = df.copy()\ntweets_df['hashtag'] = tweets_df['hashtags'].apply(lambda row : split_hashtags(row))\ntweets_df = tweets_df.explode('hashtag')\ntweets_df['hashtag'] = tweets_df['hashtag'].astype(str).str.lower().str.replace(\"'\", '').str.replace(\" \", '')\ntweets_df.loc[tweets_df['hashtag']=='', 'hashtag'] = 'NO HASHTAG'\ntweets_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And show top 20 hashtags on tweets."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ds = tweets_df['hashtag'].value_counts().reset_index()\nds.columns = ['hashtag', 'count']\nds = ds.sort_values(['count'])\nfig = px.bar(\n    ds.tail(20), \n    x=\"count\", \n    y='hashtag', \n    orientation='h', \n    title='Top 20 hashtags', \n    width=800, \n    height=700\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we are going to calculate the length for every tweet in dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df['tweet_length'] = df['text'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.histogram(\n    df, \n    x=\"tweet_length\", \n    nbins=80, \n    title='Tweet length distribution', \n    width=800,\n    height=700\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ds = df[df['tweets_count']>=10]\nds = ds.groupby(['user_name', 'tweets_count'])['tweet_length'].mean().reset_index()\nds.columns = ['user_name', 'tweets_count', 'mean_length']\nds = ds.sort_values(['mean_length'])\nfig = px.bar(\n    ds.tail(40), \n    x=\"mean_length\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with the longest average length of tweet (at least 10 tweets)', \n    width=800, \n    height=800\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ds = df[df['tweets_count']>=10]\nds = ds.groupby(['user_name', 'tweets_count'])['tweet_length'].mean().reset_index()\nds.columns = ['user_name', 'tweets_count', 'mean_length']\nds = ds.sort_values(['mean_length'])\nfig = px.bar(\n    ds.head(40), \n    x=\"mean_length\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with the shortest average length of tweet (at least 10 tweets)', \n    width=800, \n    height=800\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style='background:green; border:0; color:white'><center>Tweets text analysis</center><h2>\n\n### Here we are going to check the `text` feature of the dataset.\n### Lets see general wordcloud for this column."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def build_wordcloud(df, title):\n    wordcloud = WordCloud(\n        background_color='gray', \n        stopwords=set(STOPWORDS), \n        max_words=50, \n        max_font_size=40, \n        random_state=666\n    ).generate(str(df))\n\n    fig = plt.figure(1, figsize=(14,14))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=16)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"build_wordcloud(df['text'], 'Prevalent words in tweets for all dataset')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets see world clouds for top 5 users."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df = df[df['user_name']=='GlobalPandemic.NET']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for GlobalPandemic.NET')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df = df[df['user_name']=='covidnews.ch']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for covidnews.ch')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df = df[df['user_name']=='Open Letters']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Open Letters')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df = df[df['user_name']=='Hindustan Times']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Hindustan Times')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = df[df['user_name']=='Blood Donors India']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Blood Donors India')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's also visualize WordCloud for user's description."},{"metadata":{"trusted":true},"cell_type":"code","source":"build_wordcloud(df['user_description'], 'Prevalent words in tweets for Blood Donors India')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h2 style='background:green; border:0; color:white'><center>Simple sentiment analysis</center><h2>\n\n### Lets do simple version of sentiment analysis. We just use Tfidf Vectorizer to get features and use Kmeans clustering algotithm to split data into 2 clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"vec = TfidfVectorizer(stop_words=\"english\")\nvec.fit(df['text'].values)\nfeatures = vec.transform(df['text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=2, random_state=0)\nkmeans.fit(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"res = kmeans.predict(features)\ndf['Cluster'] = res\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Cluster'] == 0].head(20)['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Cluster'] == 1].head(20)['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Number of samples for class 0: ', len(df[df['Cluster'] == 0]))\nprint('Number of samples for class 1: ', len(df[df['Cluster'] == 1]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster'] == 0]['text'], 'Wordcloud for cluster 0')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster'] == 1]['text'], 'Wordcloud for cluster 1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So we can see that cluster 0 contains more or less positive tweets, but cluster 1 contains tweets with information about new cases, reports and regions."},{"metadata":{},"cell_type":"markdown","source":"### Let's check more clusters for example 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=5, random_state=0)\nkmeans.fit(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = kmeans.predict(features)\ndf['Cluster5'] = res\ndf","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for i in range(5):\n    print('Number of samples for class ' + str(i) + ': ', len(df[df['Cluster5'] == i]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster5'] == 0]['text'], 'Wordcloud for cluster 0')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster5'] == 1]['text'], 'Wordcloud for cluster 1')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster5'] == 2]['text'], 'Wordcloud for cluster 2')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster5'] == 3]['text'], 'Wordcloud for cluster 3')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster5'] == 4]['text'], 'Wordcloud for cluster 4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h2 style='background:green; border:0; color:white'><center>Animation with geographical distribution of tweets</center><h2>"},{"metadata":{},"cell_type":"markdown","source":"### Here I am going to show approach how to use plotly world map to demonstrate geographical distribution of tweets."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df['location'] = df['user_location'].str.split(',', expand=True)[1].str.lstrip().str.rstrip()\nres = df.groupby(['day', 'location'])['text'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"country_dict = {}\nfor c in countries:\n    country_dict[c.name] = c.alpha3\n    \nres['alpha3'] = res['location']\nres = res.replace({\"alpha3\": country_dict})\n\ncountry_list = ['England', 'United States', 'United Kingdom', 'London', 'UK']\n\nres = res[\n    (res['alpha3'] == 'USA') | \n    (res['location'].isin(country_list)) | \n    (res['location'] != res['alpha3'])\n]\n\ngbr = ['England', 'UK', 'London', 'United Kingdom']\nus = ['United States', 'NY', 'CA', 'GA']\n\nres = res[res['location'].notnull()]\nres.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\nres.loc[res['location'].isin(us), 'alpha3'] = 'USA'\nres.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\nres.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\nplot = res.groupby(['day', 'location', 'alpha3'])['text'].sum().reset_index()\nplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.choropleth(\n    plot, \n    locations=\"alpha3\",\n    hover_name='location',\n    color=\"text\",\n    animation_frame='day',\n    projection=\"natural earth\",\n    color_continuous_scale=px.colors.sequential.Plasma,\n    title='Tweets from different countries for every day',\n    width=800, \n    height=600\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"res = df.groupby(['day', 'location', 'user_name'])['text'].count().reset_index()\nres = res[['day', 'location', 'user_name']]\nres['alpha3'] = res['location']\nres = res.replace({\"alpha3\": country_dict})\n\ncountry_list = ['England', 'United States', 'United Kingdom', 'London', 'UK']\n\nres = res[\n    (res['alpha3'] == 'USA') | \n    (res['location'].isin(country_list)) | \n    (res['location'] != res['alpha3'])\n]\n\ngbr = ['England', 'UK', 'London', 'United Kingdom']\nus = ['United States', 'NY', 'CA', 'GA']\n\nres = res[res['location'].notnull()]\nres.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\nres.loc[res['location'].isin(us), 'alpha3'] = 'USA'\nres.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\nres.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\nplot = res.groupby(['day', 'location', 'alpha3'])['user_name'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.choropleth(\n    plot, \n    locations=\"alpha3\",\n    hover_name='location',\n    color=\"user_name\",\n    animation_frame='day',\n    projection=\"natural earth\",\n    color_continuous_scale=px.colors.sequential.Plasma,\n    title='Numbers of active users for every day',\n    width=800, \n    height=600\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}