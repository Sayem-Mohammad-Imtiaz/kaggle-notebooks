{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# File loading\ndf  = pd.read_csv('../input/unigram_freq.csv')\nprint(df.shape)\ndf.dropna(axis=0,how='any')\nprint(df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines = [x for x in df['word'] if type(x) == type('a') ]\nprint(\"Line Count:\",len(lines))\nprint(lines[:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\nimport re\ndef process(sent):\n    sent=sent.lower()\n    sent=re.sub(r'[^0-9a-zA-Z ]','',sent)\n    sent=sent.replace('\\n','')\n    return sent    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines =[process(x) for x in lines]\ntemp = []\nfor line in lines:\n    temp+= [ x for x in line.split() ]\nlines = list(set(temp))\nprint(\"\\n\".join(lines[:4]))\nprint(\"Number of items:\",len(lines))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHAR INDEXING\nchar_set = list(\" abcdefghijklmnopqrstuvwxyz0123456789\")\nchar2int = { char_set[x]:x for x in range(len(char_set)) }\nint2char = { char2int[x]:x for x in char_set }\nprint(char2int)\nprint(int2char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = len(char_set)\ncodes = [\"\\t\",\"\\n\",'#']\nfor i in range(len(codes)):\n    code = codes[i]\n    char2int[code]=count\n    int2char[count]=code\n    count+=1\nprint(char2int)\nprint(int2char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n#thresh - 0 to 1\ndef gen_gibberish(line,thresh=0.2):\n    times = int(random.randrange(1,len(line)) * thresh)\n    '''\n    Types of replacement:\n        1.Delete random character.\n        2.Add random character.\n        3.Replace a character.\n        4.Combination?\n    '''\n    while times!=0:\n        # try to gen noise length times...\n        times-=1\n        val = random.randrange(0,10)\n        if val <= 5:\n            #get random index\n            val = random.randrange(0,10)\n            index = random.randrange(2,len(line))\n            if val <= 3 :\n                #delete character\n                line = line[:index]+line[index+1:]\n            else:\n                #add character\n                insert_index = random.randrange(0,len(char_set))\n                line = line[:index] + char_set[insert_index] + line[index:]\n        else:\n            index = random.randrange(0,len(char_set))\n            replace_index = random.randrange(2,len(line))\n            line = line[:replace_index] + char_set[index] + line[replace_index+1:]\n    return line\n\nsample = lines[5]\ngib = gen_gibberish(sample)\nprint(\"Original:\",sample)\nprint(\"Gibberish:\",gib)\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataset\ninput_texts = []\ntarget_texts = []\nREPEAT_FACTOR = 1\nSKIP = int(len(lines)*0.65)\n\nfor line in lines[SKIP:]:\n    if len(line)>10:\n        output_text = '\\t' + line + '\\n'\n        for _ in range(REPEAT_FACTOR):\n            input_text = gen_gibberish(line)\n            input_texts.append(input_text)\n            target_texts.append(output_text)\nprint(\"LEN OF SAMPLES:\",len(input_texts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_enc_len = max([len(x) for x in input_texts])\nmax_dec_len = max([len(x) for x in target_texts])\nprint(\"Max Enc Len:\",max_enc_len)\nprint(\"Max Dec Len:\",max_dec_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_samples = len(input_texts)\nencoder_input_data = np.zeros( (num_samples , max_enc_len , len(char_set)),dtype='float32' )\ndecoder_input_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\ndecoder_target_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\nprint(\"CREATED ZERO VECTORS\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filling in the enc,dec datas\nfor i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n    for t,char in enumerate(input_text):\n        encoder_input_data[ i , t , char2int[char] ] = 1\n    for t,char in enumerate(target_text):\n        decoder_input_data[ i, t , char2int[char] ] = 1\n        if t > 0 :\n            decoder_target_data[ i , t-1 , char2int[char] ] = 1\nprint(\"COMPLETED...\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input,LSTM,Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nepochs = 1000\nlatent_dim = 256\n\nnum_enc_tokens = len(char_set)\nnum_dec_tokens = len(char_set) + 2 # includes \\n \\t\nencoder_inputs = Input(shape=(None,num_enc_tokens))\nencoder = LSTM(latent_dim,return_state=True)\nencoder_outputs , state_h, state_c = encoder(encoder_inputs)\nencoder_states = [state_h,state_c]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_inputs = Input(shape=(None,num_dec_tokens))\ndecoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\ndecoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n\ndecoder_dense = Dense(num_dec_tokens, activation='softmax')\ndecoder_ouputs = decoder_dense(decoder_ouputs)\n\nmodel = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h=model.fit([encoder_input_data,decoder_input_data],decoder_target_data\n         ,epochs = epochs,\n          batch_size = batch_size,\n          validation_split = 0.2\n         )\nmodel.save('s2s.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(h.history['loss'])\nplt.title('Model Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model = Model(encoder_inputs,encoder_states)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\ndecoder_outputs,state_h,state_c = decoder_lstm(\n        decoder_inputs,initial_state = decoder_states_inputs\n)\ndecoder_states = [state_h,state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states\n)\nencoder_model.save('encoder.h5')\ndecoder_model.save('decoder.h5')\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_dec_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, char2int['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = int2char[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_dec_len):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_dec_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\nfor seq_index in range(10):\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Wrong sentence:', input_texts[seq_index])\n    print('Corrected sentence:', decoded_sentence)\n    print('Ground Truth:',target_texts[seq_index])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}