{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# imports\n# pandas \nimport pandas as pd\n# numpy\nimport numpy as np\n# matplotlib \nimport matplotlib.pyplot as plt\n#%matplotlib inline\nplt.style.use('ggplot')\n# seaborn\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncolumns=[\"id\",\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\"]\ndf= pd.read_csv('/kaggle/input/diamonds/diamonds.csv', names=columns,header=None)\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# info\nprint(\"\\n*** Structure ***\")\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary\nprint(\"\\n*** Summary ***\")\ndf.describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# head\nprint(\"\\n*** Head ***\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# the cut column\nfirst we are converting alpha to numeric data type for the ease of calculations<br>\nconverting via ***'map'***<br>\nmap can handle errors easily","metadata":{}},{"cell_type":"code","source":"print(\"\\n*** Cut ***\")\ncolName = 'cut'  \n# original data\nprint(\"*Original Count*\")\nprint(\"Null Values: \", df[colName].isnull().sum())\nprint(\"Zero Values: \", (df[colName]==0).sum())\nprint(\"Class Count\")\ndf.groupby([colName])['id'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaning data\nprint(\"*Categoric Data*\")\nprint(df[colName].unique())\n#assigning -1 for unknown values\ndf[colName] = df[colName].map({\"Fair\":0, \"Good\":1, \"Very Good\":2, \"Premium\":3, \"Ideal\":4, \"Unknown\":-1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaned data\nprint(\"*Cleaned Count*\")\nprint(\"Null Values: \", df[colName].isnull().sum())\nprint(\"Zero Values: \", (df[colName]==0).sum())\nprint(\"*Categoric Data*\")\nprint(df[colName].unique())\nprint(\"Class Count\")\ndf.groupby([colName])['id'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**check color**\n\n\nconvert alpha to numeric via cat codes\ncat code can handle nulls but not errors","metadata":{}},{"cell_type":"code","source":"print(\"\\n*** Color ***\")\ncolName = 'color'  \nprint(\"*Original Count*\")\nprint(\"Null Values: \", df[colName].isnull().sum())\nprint(\"Zero Values: \", (df[colName]==0).sum())\nprint(\"Categoric data\\n\",df[colName].unique())\nprint(\"Class Count\")\nprint(df.groupby([colName])['id'].count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean data\nprint(\"*Categoric Data*\")\nprint(df[colName].unique())\ndf[colName] = pd.Categorical(df[colName])\ndf[colName] = df[colName].cat.codes\nprint(\"Transformed into numeric values\\n\",df[colName].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaned data\nprint(\"*Cleaned Count*\")\nprint(\"Null Values: \", df[colName].isnull().sum())\n#here zero values doesn't mean invalid, it shows the actual value of F which is a categorical variable\nprint(\"Zero Values: \", (df[colName]==0).sum())\nprint(\"Class Count\")\nprint(df.groupby([colName])['id'].count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**check clarity**\n\n\nconvert alpha to numeric via label encoder\nit can't handle nulls & erros ...it needs requires valid data only","metadata":{}},{"cell_type":"code","source":"print(\"\\n*** Clarity ***\")\ncolName = 'clarity'\nprint(\"*Original Count*\")\nprint(\"Null Values: \", df[colName].isnull().sum())\nprint(\"Zero Values: \", (df[colName]==0).sum())\nprint(\"Class Count\")\nprint(df.groupby([colName])['id'].count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean data\nprint(\"*Categoric Data*\")\nprint(df[colName].unique())\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf[colName] = le.fit_transform(df[colName])\nprint(df[colName].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaned data\nprint(\"*Cleaned Count*\")\nprint(\"Null Values: \", df[colName].isnull().sum())\nprint(\"Zero Values: \", (df[colName]==0).sum())\nprint(\"Class Count\")\nprint(df.groupby([colName])['id'].count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# check outlier count","metadata":{}},{"cell_type":"code","source":"print('\\n*** Outlier Count ***')\ndef OutlierCount(df): \n    colNames = df.columns\n    strRetValue = \"\"\n    for colName in colNames:\n        #print(colName)\n        colValues = df[colName].values\n        #print(colValues)\n        outCount = colOutCount(colValues)\n        #print(outCount)\n        strRetValue = strRetValue + colName.ljust(15, ' ') + \"   \" + str(outCount) + \"\\n\"\n    return(strRetValue)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outliercount(data):\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    print(\"Outlier Count in each column\")\n    print(((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum())\n\noutliercount(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check variance\nprint('\\n*** Variance In Columns ***')\ndf.var()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check std dev \nprint('\\n*** StdDev In Columns ***')\ndf.std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check mean\nprint('\\n*** Mean In Columns ***')\ndf.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check zeros\nprint('\\n*** Columns With Zeros ***')\n(df==0).sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check nulls\nprint('\\n*** Columns With Nulls ***')\nprint(df.isnull().sum()) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check Carat\n# handle nulls if required\nprint(\"\\n*** Carat ***\")\ncolName = 'carat'\nprint(\"*Original Count*\")\nprint(\"Null Values: \", df[colName].isnull().sum())\nprint(\"Zero Values: \", (df[colName]==0).sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}