{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"white\")\nsns.set(style=\"ticks\", color_codes=True)\n%matplotlib inline\nfrom sklearn.model_selection import learning_curve, validation_curve, cross_val_score\n\n# Functions\ndef find_missing_data(df):# missing data\n  #missing data\n  total = df.isnull().sum().sort_values(ascending=False)\n  percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n  missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n  print (missing_data.head(20))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Load and explore data**"},{"metadata":{"trusted":true,"_uuid":"ba1812c3d0f5ded680e2ad26bb8bbcb75e425931"},"cell_type":"code","source":"housing = pd.read_csv(\"../input/housing.csv\")\nhousing.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a113466ad8e5a23eaf98f5867e35588c016c2b6"},"cell_type":"markdown","source":"Explore Data: missing values,  correlated features"},{"metadata":{"trusted":true,"_uuid":"2ed734eded207828532a08e6c47cd43ee49d8cd0"},"cell_type":"code","source":"find_missing_data(housing)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3fdaa0a19164a013dfeb5384c9f636bd3670204"},"cell_type":"markdown","source":"* Missing Data -- total bedrooms.\n** check to see if this feature is closely connected to another feature? Possibly total_rooms. It is possible that some might convert an extra bedroom into an office or gym >> so on average there might be some connection between total rooms and the number of bedrooms. \nA pairwise plot of possibly related features would give us a better idea."},{"metadata":{"trusted":true,"_uuid":"fc372be3bdbf9a66cb8107f476eb45fde66c625b"},"cell_type":"code","source":"## Fill missing data 'total_bedrooms'\ng = sns.pairplot(housing,\n                 x_vars=[\"total_bedrooms\"],\n                 y_vars=[\"total_rooms\",\"population\",\"households\",\"housing_median_age\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcda667d766924707ed322569b0cc23ac90dd1c7"},"cell_type":"markdown","source":"Interesting: \"total_rooms\",\"population\",\"households\" are linearly correlated with \"total_bedrooms\" & importantly with each other. >> Can be compressed down to one feature that capture most of the variance in this data. This is will help if Linear Regression is the way to go...\n** TakeHome: 'households' to linearly interp 'total_bedrooms' to fill missing data**"},{"metadata":{"trusted":true,"_uuid":"4ebb34f24d3a72eaae424c1f08dfcd76ea71c88c"},"cell_type":"code","source":"## Check relationship between features\ndf_train = housing.loc[:, housing.columns != 'ocean_proximity']\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, square=True,annot=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff0ea6b3e5eec5b27e08e50bd89b3dfece0ea787"},"cell_type":"markdown","source":"Hmm... the initial guess was correct **but** any realtor would tell you that the location determines the price range of the house and there seems to be no obvious relationship between 'latitude' and 'longitude' and the house price. Also, 'population' which should affect the price (albiet, more a complicated effect) also has a small effect. Intutively, these variables should give indications of whether the house is located in the city, suburb and county. If this information can be extracted from the geopositions, it would be a good start but definitely not enough ~ Detroit, for example, was backrupt at one point which would have had an effect on the housing market. Also, California is notoriously known to have rich neighbourhoods surrounded by poor neighbourhoods. Check out this link: http://storymaps.esri.com/stories/2016/wealth-divides/index.html"},{"metadata":{"trusted":true,"_uuid":"96ba0be6e837ee0211eb2dd21dc9b6657a7584c5"},"cell_type":"code","source":"plt.scatter(housing[\"median_income\"],housing[\"median_house_value\"])\nplt.xlabel(\"median income\");plt.ylabel(\"house value\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"411f0c3f6342e8cbdf0c493d8a971a5f464a3a3b"},"cell_type":"markdown","source":"There is definitely a slight linear correlation but many outliers. Can't delete any of the outlier information because it is possible that a retired person with little income could live in an expensive neighbourhood. A range of people of different incomes live in expensive neighbourhoods but very few \"rich\" people live in poor neighbourhoods.  **Take home: most of the people in a neighbourhood have similar median incomes.** "},{"metadata":{"_uuid":"6a4b8af824a5b1eb8c9b6adf12279bf49a1397da"},"cell_type":"markdown","source":"To do:\n1. fill missing \"total_bedrooms\" data\n2.  Compress \"total_rooms\",\"population\",\"households\" and \"total_bedrooms\" to a single feature\n3. Engineer a new feature that can increase the information from \"median_income\"  without letting the outliers overtly bias the threshold for node splits (Yes, I have decided to go with the Random Forest).\n\nIn essence, if we can provide the model a classification of \"Cheap\", \"Nominal\" and \"Expensive\" neighbourhoods, then the model take into account the other features to decide on the price.  "},{"metadata":{"trusted":true,"_uuid":"c5c0aa911e4ea6b6cf7ab7f4b51fc91fc65d1133"},"cell_type":"code","source":"# 1. Fill missing data\n## Get Linear dependency\nfrom sklearn.linear_model import LinearRegression\ndf_ss = housing.loc[:,['households','total_bedrooms']].dropna() ## \nX,y = np.log(df_ss[df_ss.loc[:, df_ss.columns != 'total_bedrooms'].columns]), \\\n          np.log(df_ss['total_bedrooms'])\n\n## Get score using original model\nlinreg = LinearRegression()\nlinreg.fit(X,y)\nscores = cross_val_score(linreg, X,y, cv=10)\nprint('CV accuracy (original): %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) # 95% correct, Not bad.\n# highest_score = np.mean(scores)\n# print intercept and coefficients\n# print linreg.intercept_ , linreg.coef_\n\n## select null rows:\nrowIX = housing[housing['total_bedrooms'].isnull()]\npredX = np.array(np.log(rowIX['households'])).reshape(-1,1)\nhousing.loc[rowIX.index.values, 'total_bedrooms'] = np.exp(linreg.predict(predX))\n\nprint (\"missing values?\", housing.isnull().sum().max()) #just checking that there's no missing data missing...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da98214155952e1f0a83499b0b5ee441edd7161a"},"cell_type":"code","source":"##2. Using PCA to combine features to 1 principal component that can account for max. variance in the features.\nfrom sklearn.decomposition import PCA\nhousing_features_pca = PCA(1)\nX_select  = housing.loc[:,['total_rooms','total_bedrooms', 'population','households']]\nhousing['rb/hp']=housing_features_pca.fit_transform(X_select) \nprint (housing_features_pca.explained_variance_ratio_) ## 1 Principal Component >> 95% var of data, Good enough.\n## New df with combined features: \nhousing_redu = housing.drop(columns=['total_rooms', 'total_bedrooms','population', 'households'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"076e927fb53b0e1562f23a0010fb0627d1a25481"},"cell_type":"code","source":"##3. Engineer new \"more informative\" features\ndef label_HousingPrice (row):\n    if row['median_income'] <= 3 :\n        return 'Cheap'\n    elif row['median_income'] <=6 :\n        return 'Nominal'\n    elif (row['median_income'] <=17):\n        return 'Expensive'\n#   else:\n#     return 'VeryExpensive'\n\nhousing_redu['PriceCatg'] = housing_redu.apply (lambda row: label_HousingPrice (row),axis=1)\nvar = 'Expensive'\n# housing_ss = housing_redu[['housing_median_age','rb/hp','median_income',\\\n#                            'PriceCatg','median_house_value']]\nhousing_ss = housing_redu.loc[housing_redu['PriceCatg'] == var]\n\n# sns.distplot(housing_ss.loc[housing_ss['median_house_value'] < 495e3]['median_house_value'])#, fit=stats.norm);\nsns.distplot(housing_ss['median_house_value'])#, fit=stats.norm);\nhousing_ss['median_house_value'].max()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eadd8ce99e4f7a5b11ff78c3a72afc0cbcff5dea"},"cell_type":"markdown","source":"Note: There is a \"very expensive\" neighbourhood ~ median_house_value > 470k. Possibly with high population?"},{"metadata":{"trusted":true,"_uuid":"fd221fb6eff70820da619e3a93c9833d0f3d0ac0"},"cell_type":"code","source":"### Find this \"very expensive\" neighbourhood (visually)\nplt.figure(figsize=(15,10))\nplt.scatter(housing_redu['longitude'],housing_redu['latitude'],c=housing_redu['median_house_value'],s=5,cmap='viridis')\nexpn = housing[housing_redu['PriceCatg'] == 'Expensive']\nvexpn = expn[expn['median_house_value']> 495e3]\nplt.scatter(vexpn['longitude'],vexpn['latitude'],c='r')\n# plt.colorbar()\nplt.xlabel('longitude')\nplt.ylabel('latitude')\nplt.title('House price wrt geo-coordinates')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43a6370f396925a6137cc0bfafc45d99b1420c8c"},"cell_type":"code","source":"vexpn.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3733cf262b7e4769622c264fd733ad9ab879873"},"cell_type":"code","source":"expn.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98d7458878beac74e991870d494ddbcdd13855b6"},"cell_type":"markdown","source":"About ~600 houses in the \"expensive\" neighbourhood are very expensive. Hopefully the decision tree can figure out why; No direct correlation to the feature variables is obvious. ** Something to come back to to improve predictions.**"},{"metadata":{"trusted":true,"_uuid":"7b1d73a09024ed745d468abebf14cebd11f66e04"},"cell_type":"code","source":"from sklearn.neighbors import KernelDensity\n\ndef getKDE(var):\n  housing_ss = housing_redu[housing_redu['PriceCatg'] == var]\n  X = housing_ss[['latitude','longitude']]\n  X *= np.pi / 180.  # Convert lat/long to radians\n  kde = KernelDensity(bandwidth=0.0001, metric='haversine',\n                        kernel='gaussian', algorithm='ball_tree')\n  kde.fit(X)\n  return kde\n\ncolumn_KDE =[]\nfor i,var in enumerate(housing_redu['PriceCatg'].unique()):\n  print (\" generating KDE of PriceCatg: \", var)\n  kde = getKDE(var)\n  column_KDE.append(var+'_KDE')\n  housing_redu[column_KDE[-1]] =  np.exp(kde.score_samples(housing_redu[['latitude','longitude']]* np.pi / 180.))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94fd09e1d14c3035fdf784bf52346449287ca760"},"cell_type":"markdown","source":"** Preprocess Data**"},{"metadata":{"trusted":true,"_uuid":"2112213d8135a772e6dfcafb5192297812b5b58b"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler,StandardScaler\n###OneHotEncoder: converting ocean_proximity to dummies\n\nhousing_onehot=pd.concat([pd.get_dummies(housing_redu['ocean_proximity'],drop_first=True),housing_redu],axis=1).drop('ocean_proximity',axis=1)\nhousing_onehot = housing_onehot.drop(columns=['PriceCatg',\n                                              'latitude','longitude','median_income'])#\n\nfrom sklearn.model_selection import train_test_split\nX = housing_onehot.drop('median_house_value',axis=1)\nX[column_KDE] = StandardScaler().fit_transform(X[column_KDE])\ny = housing_onehot['median_house_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n## Transform test and train data & rescale\n\nscalerMM = StandardScaler().fit(X_train)\nX_train = scalerMM.transform(X_train)\nX_test = scalerMM.transform(X_test)\n\n\nX.head() ## checking to make sure ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6ceaa467a4a031044abeea94b1c2cae5f0a8f33"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(max_depth=20, random_state=0,n_estimators=100,n_jobs=-1)\nregr.fit(X_train, y_train)\nscores = cross_val_score(regr, X_train, y_train, cv=10)\nprint('CV accuracy (original): %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\nhighest_score = np.mean(scores)\n\nprint (\"Test score:\", regr.score(X_test,y_test))\nregr_pred = regr.predict(X_test)\ntest_mse = np.mean(((regr_pred - y_test)**2))\ntest_rmse = np.sqrt(test_mse)\nprint ('final test rmse:', test_rmse) ## to beat (xgb): 41430 >> mine: 35271","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8eb25dbb02e488af25a200823b1072c848a85dbb"},"cell_type":"markdown","source":"The model is having a very hard time deciding on the \"expensive\" neighbourhoods and needs more help with this classification. Lets do some testing to see if this is true..."},{"metadata":{"trusted":true,"_uuid":"7620f424ff95ff4d7ffd32b95742726a26985972"},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.title('Feature Importance')\nsns.barplot(data={'importance':regr.feature_importances_,'feature':housing_onehot.columns[housing_onehot.columns!='median_house_value']},y='feature',x='importance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef8b5a1c7c48d746171fc3aa6f98c6bb6fb10e10"},"cell_type":"markdown","source":"So yes, classification of neighbourhoods apriori is important to get better results. Could this be overfitting the data?"},{"metadata":{"_uuid":"bd24f8834eb978503b3f1f37c0beee35ba477a5d"},"cell_type":"markdown","source":"**Learning Curve**"},{"metadata":{"trusted":true,"_uuid":"04b7a094f1b039b2704dd98c2c04becc738e26b2"},"cell_type":"code","source":"## Credit: https://www.kaggle.com/pmarcelino/data-analysis-and-feature-extraction-with-python\n# Plot learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n# Plot validation curve\ndef plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, cv)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(param_range, train_mean, color='r', marker='o', markersize=5, label='Training score')\n    plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='r')\n    plt.plot(param_range, test_mean, color='g', linestyle='--', marker='s', markersize=5, label='Validation score')\n    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n    plt.grid() \n    plt.xscale('log')\n    plt.legend(loc='best') \n    plt.xlabel('Parameter') \n    plt.ylabel('Score') \n    plt.ylim(ylim)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"269e91e478d0c290a8fd64152685bb47c3172933"},"cell_type":"code","source":"# Plot learning curves\nplot_learning_curve(regr, \"Toy Model\", X_train, \n                    y_train, ylim=(0.3, 1.01), cv=10, n_jobs=-1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79385ba4f77cad65a77ef0de7d645336af4dd023"},"cell_type":"code","source":"# Plot validation curve ## \ntitle = 'Validation Curve (Regression)'\nparam_name = 'n_estimators'\nparam_range = [500,1000,1500,2000] \ncv = 10\nplot_validation_curve(estimator=regr, title=title, X=X_train, y=y_train, \n                      param_name=param_name, ylim=(0.5, 1.01), param_range=param_range);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2827cfdb2a3f78777c1d19c988a00ba3a9f59618"},"cell_type":"markdown","source":"*To Do:  a more comprehensive hyperparameter gridsearch optimization*"},{"metadata":{"_uuid":"c567fad56b68a98dd5a97698d10f2f07e9a74641"},"cell_type":"markdown","source":"**Toy Model : What if we can classify the neighbourhoods a priori ? **"},{"metadata":{"trusted":true,"_uuid":"088991272112d3dc6f8db92648a4b75ed0b22147"},"cell_type":"code","source":"### This could be considered Engineering a Feature to find Price Category based on geospatial location\ndef label_HousingPrice (row):\n    if row['median_house_value'] <= 150e3 :\n        return 'Cheap'\n    elif row['median_house_value'] <=300e3 :\n        return 'Nominal'\n    elif row['median_house_value'] <=400e3 :\n        return 'Expensive'\n    else:\n        return 'VeryExpensive'\nhousing_play = housing_redu.copy()\nhousing_play['PriceCatg'] = housing_redu.apply (lambda row: label_HousingPrice (row),axis=1)\ndef PLAYgetKDE(var):\n    housing_ss = housing_play[housing_play['PriceCatg'] == var]\n    X = housing_ss[['latitude','longitude']]\n    X *= np.pi / 180.  # Convert lat/long to radians\n    kde = KernelDensity(bandwidth=0.0001, metric='haversine',\n                        kernel='gaussian', algorithm='ball_tree')\n    kde.fit(X)\n    return kde\nPLAYcolumn_KDE =[]\nfor i,var in enumerate(housing_play['PriceCatg'].unique()):\n    print (\" generating KDE of PriceCatg: \", var)\n    kde = PLAYgetKDE(var)\n    PLAYcolumn_KDE.append(var+'_KDE')\n    housing_play[PLAYcolumn_KDE[-1]] =  np.exp(kde.score_samples(housing_play[['latitude','longitude']]* np.pi / 180.))\n    \n### Model\nhousing_play=pd.concat([pd.get_dummies(housing_play['ocean_proximity'],drop_first=True),housing_play],axis=1).drop('ocean_proximity',axis=1)\nhousing_play = housing_play.drop(columns=['PriceCatg', 'latitude','longitude'])#\nX = housing_play.drop('median_house_value',axis=1)\nX[column_KDE] = StandardScaler().fit_transform(X[column_KDE])\ny = housing_play['median_house_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n## Transform test and train data & rescale\nscalerMM = StandardScaler().fit(X_train)\nX_train = scalerMM.transform(X_train)\nX_test = scalerMM.transform(X_test)\n\n###Fit\nregr = RandomForestRegressor(max_depth=20, random_state=0,n_estimators=100,n_jobs=-1)\nregr.fit(X_train, y_train)\nscores = cross_val_score(regr, X_train, y_train, cv=10)\nprint('CV accuracy (original): %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\nhighest_score = np.mean(scores)\n\nprint (\"Test score:\", regr.score(X_test,y_test))\nregr_pred = regr.predict(X_test)\ntest_mse = np.mean(((regr_pred - y_test)**2))\ntest_rmse = np.sqrt(test_mse)\nprint ('final test rmse:', test_rmse) ## to beat (xgb): 41430 ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f372afd99eab815b119d3cf9db031e05716ed2e8"},"cell_type":"markdown","source":"Okay, that definitely helps. Possibly overfitting since the KDE on the entire data and not specifically with the 'Training Data'. "},{"metadata":{"trusted":true,"_uuid":"36ceb3a88c2021ba03ad7040e2718b6c7255bc7d"},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.title('Feature Importance')\nsns.barplot(data={'importance':regr.feature_importances_,\n                  'feature':housing_play.columns[housing_play.columns!='median_house_value']},y='feature',x='importance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"182aaa8509084ff6e242ceaf2923c688fc72e62f"},"cell_type":"code","source":"# Plot learning curves\ntitle = \"Learning Curves (Regression)\"\nplot_learning_curve(regr, title, X_train, \n                    y_train, ylim=(0.8, 1.01), cv=10, n_jobs=-1);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c45e98edfc135bc536e7a43759407a85f93d347"},"cell_type":"markdown","source":"**Coming soon...**\nOnto geospatial data to classify neighbourhoods with both (latitude, longitude) >> proximity to city and median income. "},{"metadata":{"trusted":true,"_uuid":"26d879a5b7b7d7820f1a21f0f4ecc7ee6afec981"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}