{"cells":[{"metadata":{},"cell_type":"markdown","source":"Exploration of using tensorflow probability to create an uncertainty aware classifier\n\ninspired by and mostly copied from:\n\nhttps://medium.com/python-experiments/bayesian-cnn-model-on-mnist-data-using-tensorflow-probability-compared-to-cnn-82d56a298f45"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport warnings\nimport copy\n# warnings.simplefilter(action=\"ignore\")\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport matplotlib.pyplot as plt\nfrom matplotlib import figure \nfrom matplotlib.backends import backend_agg\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport seaborn as sns\nimport tensorflow as tf\nimport numpy as np\ntf.logging.set_verbosity(tf.logging.ERROR)\n# Dependency imports\nimport matplotlib\nimport tensorflow_probability as tfp\n#matplotlib.use(\"Agg\")\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SHAPE = [28, 28, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read in MNIST"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/'\n\n#mnist_onehot = input_data.read_data_sets(data_dir, one_hot=True)\nmnist_conv = input_data.read_data_sets(data_dir,reshape=False ,one_hot=False)\n#mnist_conv_onehot = input_data.read_data_sets(data_dir,reshape=False ,one_hot=True)\n\n# create a set of non-shuffled test images for reference\nfixed_val = np.zeros((1000,28,28,1))\nfor img_no in range(1000):\n    \n    fixed_val[img_no] = mnist_conv.validation.images[img_no].copy()\n\n# create a fixed set of images with random noise added, for a poor man's simulation of an adversarial attach\nperturbation_magnitude = 0.5\nnp.random.seed(42)\nperturbation = np.random.random((28,28,1))\nperturbed_val = np.zeros((1000, 28, 28, 1))\nfor img_no in range(1000):\n    perturbed_val[img_no] = mnist_conv.validation.images[img_no].copy() + perturbation_magnitude * perturbation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create some noisy images"},{"metadata":{"trusted":true},"cell_type":"code","source":"noisy_images = np.zeros((10000,28,28,1))\nfor i in range(10000):\n    \n    noisy_images[i] = np.random.random((28,28,1))   # Test data\n    \nplt.imshow(noisy_images[0].reshape((28,28)), cmap='gist_gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.001   #initial learning rate\nmax_step = 50000 #number of training steps to run\nbatch_size = 32 #batch size\nnum_monte_carlo = 500 #Network draws to compute predictive probabilities.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-Probability CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"images = tf.placeholder(tf.float32,shape=[None,28,28,1])\nlabels = tf.placeholder(tf.float32,shape=[None,])\nhold_prob = tf.placeholder(tf.float32)\n# define the model\nneural_net = tf.keras.Sequential([\n      tfp.layers.Convolution2DReparameterization(32, kernel_size=5,  padding=\"SAME\", activation=tf.nn.relu),\n      tf.keras.layers.MaxPooling2D(pool_size=[2, 2],  strides=[2, 2],  padding=\"SAME\"),\n      tfp.layers.Convolution2DReparameterization(64, kernel_size=5,  padding=\"SAME\",  activation=tf.nn.relu),\n      tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding=\"SAME\"),\n      tf.keras.layers.Flatten(),\n      tfp.layers.DenseFlipout(1024, activation=tf.nn.relu),\n      tf.keras.layers.Dropout(hold_prob),\n      tfp.layers.DenseFlipout(10)])\nlogits = neural_net(images)\n\n# Compute the -ELBO as the loss, averaged over the batch size.\nlabels_distribution = tfp.distributions.Categorical(logits=logits)\nneg_log_likelihood = -tf.reduce_mean(labels_distribution.log_prob(labels))\nkl = sum(neural_net.losses) / mnist_conv.train.num_examples\nelbo_loss = neg_log_likelihood + kl\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(elbo_loss)\n# Build metrics for evaluation. Predictions are formed from a single forward\n# pass of the probabilistic layers. They are cheap but noisy predictions.\npredictions = tf.argmax(logits, axis=1)\naccuracy, accuracy_update_op = tf.metrics.accuracy(labels=labels, predictions=predictions)\n\n# Extract weight posterior statistics for layers with weight distributions\n# for later visualization.\nnames = []\nqmeans = []\nqstds = []\nfor i, layer in enumerate(neural_net.layers):\n    try:\n        q = layer.kernel_posterior\n    except AttributeError:\n        continue\n    names.append(\"Layer {}\".format(i))\n    qmeans.append(q.mean())\n    qstds.append(q.stddev())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_image_and_prob(image_vals, image_indices, probs):\n    # utility to plot some images and the probability distributions\n    import pandas as pd\n    mean_probs = np.mean(probs, axis=0)\n    std_probs = np.std(probs, axis=0)\n    \n    for image_index in image_indices:\n        this_image = pd.DataFrame(probs[:,image_index,:]).melt()\n        fig, ax = plt.subplots(ncols=3, figsize=(8,4))\n        ax[0].imshow(image_vals[image_index][:,:,0], cmap='gist_gray');\n        sns.barplot(np.arange(10), mean_probs[image_index], ax=ax[1])\n        sns.boxplot('variable', 'value', data=this_image, ax=ax[2], fliersize=0)\n        for patch in ax[2].artists:\n            r, g, b, a = patch.get_facecolor()\n            patch.set_facecolor((r, g, b, .0))\n        sns.swarmplot('variable', 'value', data=this_image, ax=ax[2], alpha=0.25)\n        \n        ax[1].set_ylim([0,1])\n        ax[1].set_ylabel('Class \"Probability\"')\n        ax[1].set_xlabel('Class')\n        ax[2].set_xlabel('Class')\n        ax[1].set_ylabel('')\n        ax[2].set_ylim([0,1])\n        ax[0].set_title('Image #%i' % image_index)\n        fig.tight_layout()\n        display()\n        plt.show();\n        #print(mean_probs[image_index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Look at some questionable labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"noisy_data = tf.data.Dataset.from_tensor_slices((noisy_images))\nnoisy_data = noisy_data.batch(len(noisy_images))\nnoisy_data_iterator = noisy_data.make_one_shot_iterator()\nnoisy_data_vals = noisy_data_iterator.get_next()\n\nfixed_val_data = tf.data.Dataset.from_tensor_slices((fixed_val))\nfixed_val_data = fixed_val_data.batch(len(fixed_val))\nfixed_val_iterator = fixed_val_data.make_one_shot_iterator()\nfixed_val_vals = fixed_val_iterator.get_next()\n\ninit_op = tf.group(tf.global_variables_initializer(),\n                   tf.local_variables_initializer())\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    \n    for step in range(max_step):\n        images_b, labels_b = mnist_conv.train.next_batch(batch_size)\n        images_h, labels_h = mnist_conv.validation.next_batch(mnist_conv.validation.num_examples)\n            \n        _ = sess.run([train_op, accuracy_update_op], feed_dict={images: images_b, labels: labels_b, hold_prob:0.5})\n        if (step==0) | ((step + 1) % 500 == 0):\n            loss_value, accuracy_value = sess.run([elbo_loss, accuracy], feed_dict={images: images_b, labels: labels_b, hold_prob:0.5})\n            print(step + 1, loss_value, accuracy_value)\n    \n    # selection of intertesing mnist images\n    image_vals = sess.run(fixed_val_vals)\n    probs = np.asarray([sess.run((labels_distribution.probs),\n                                 feed_dict={images: image_vals, hold_prob:0.5}\n                                )\n                        for _ in range(num_monte_carlo)])\n    interesting_image_indices = [24, 29, 30, 48, 53, 54, 63, 67, 70, 80]\n    plot_image_and_prob(image_vals, interesting_image_indices, probs)\n\n    # find the worst case for noisy data and show\n    image_vals = sess.run(noisy_data_vals)\n    probs = np.asarray([sess.run((labels_distribution.probs),\n                                 feed_dict={images: image_vals, hold_prob:0.5}\n                                )\n                        for _ in range(num_monte_carlo)])\n    trouble_index = np.expand_dims(probs.mean(axis=0), axis=0)[0].max(axis=1).argmax()\n    plot_image_and_prob(image_vals, [trouble_index], probs)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### simulating an adversarial attach"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_image_and_prob_compare(image0_vals, image0_probs, image1_vals, image1_probs, image_indices):\n    import pandas as pd\n    mean_probs0 = np.mean(image0_probs, axis=0)\n    std_probs0 = np.std(image0_probs, axis=0)\n    mean_probs1 = np.mean(image1_probs, axis=0)\n    std_probs1 = np.std(image1_probs, axis=0)\n    \n    for image_index in image_indices:\n        \n        fig, ax = plt.subplots(ncols=6, figsize=(16,4))\n        this_image0 = pd.DataFrame(image0_probs[:,image_index,:]).melt()\n        ax[0].imshow(image0_vals[image_index][:,:,0], cmap='gist_gray');\n        sns.barplot(np.arange(10), mean_probs0[image_index], ax=ax[1])\n        sns.boxplot('variable', 'value', data=this_image0, ax=ax[2], fliersize=0)\n        for patch in ax[2].artists:\n            r, g, b, a = patch.get_facecolor()\n            patch.set_facecolor((r, g, b, .0))\n        sns.swarmplot('variable', 'value', data=this_image0, ax=ax[2], alpha=0.25)\n        \n        ax[1].set_ylim([0,1])\n        ax[1].set_ylabel('Class \"Probability\"')\n        ax[1].set_xlabel('Class')\n        ax[2].set_xlabel('Class')\n        ax[1].set_ylabel('')\n        ax[2].set_ylim([0,1])\n        ax[0].set_title('Image #%i' % image_index)\n        \n        this_image1 = pd.DataFrame(image1_probs[:,image_index,:]).melt()\n        ax[3].imshow(image1_vals[image_index][:,:,0], cmap='gist_gray');\n        sns.barplot(np.arange(10), mean_probs1[image_index], ax=ax[4])\n        sns.boxplot('variable', 'value', data=this_image1, ax=ax[5], fliersize=0)\n        for patch in ax[5].artists:\n            r, g, b, a = patch.get_facecolor()\n            patch.set_facecolor((r, g, b, .0))\n        sns.swarmplot('variable', 'value', data=this_image1, ax=ax[5], alpha=0.25)\n        \n        ax[4].set_ylim([0,1])\n        ax[4].set_ylabel('Class \"Probability\"')\n        ax[4].set_xlabel('Class')\n        ax[5].set_xlabel('Class')\n        ax[4].set_ylabel('')\n        ax[5].set_ylim([0,1])\n        ax[3].set_title('Image #%i' % image_index)\n        \n        fig.tight_layout()\n        display()\n        plt.show();\n\nfixed_val_data = tf.data.Dataset.from_tensor_slices((fixed_val))\nfixed_val_data = fixed_val_data.batch(len(fixed_val))\nfixed_val_iterator = fixed_val_data.make_one_shot_iterator()\nfixed_val_vals = fixed_val_iterator.get_next()\n\nperturbed_val_data = tf.data.Dataset.from_tensor_slices((perturbed_val))\nperturbed_val_data = perturbed_val_data.batch(len(perturbed_val))\nperturbed_val_iterator = perturbed_val_data.make_one_shot_iterator()\nperturbed_val_vals = perturbed_val_iterator.get_next()\n\ninit_op = tf.group(tf.global_variables_initializer(),\n                   tf.local_variables_initializer())\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    \n    for step in range(1000):#max_step):\n        images_b, labels_b = mnist_conv.train.next_batch(batch_size)\n        images_h, labels_h = mnist_conv.validation.next_batch(mnist_conv.validation.num_examples)\n            \n        _ = sess.run([train_op, accuracy_update_op], feed_dict={images: images_b, labels: labels_b, hold_prob:0.5})\n        if (step==0) | ((step + 1) % 500 == 0):\n            loss_value, accuracy_value = sess.run([elbo_loss, accuracy], feed_dict={images: images_b, labels: labels_b, hold_prob:0.5})\n            print(step + 1, loss_value, accuracy_value)\n            \n    original_vals = sess.run(fixed_val_vals)\n    perturbed_vals = sess.run(perturbed_val_vals)\n    \n    original_probs = np.asarray([sess.run((labels_distribution.probs),\n                                 feed_dict={images: original_vals, hold_prob:0.5}\n                                )\n                        for _ in range(num_monte_carlo)])\n    perturbed_probs = np.asarray([sess.run((labels_distribution.probs),\n                                 feed_dict={images: perturbed_vals, hold_prob:0.5}\n                                )\n                        for _ in range(num_monte_carlo)])\n    \n    # find a pathological case where adding noise drastically changes the classification\n    sensitive_index = ((np.expand_dims(original_probs.mean(axis=0), axis=0)[0].max(axis=1) < 0.4)*999. + # big number if no probability above threshold\n                       (np.expand_dims(perturbed_probs.mean(axis=0), axis=0)[0].max(axis=1) < 0.4)*999. + # big number if no probability above threshold\n                       np.abs(np.expand_dims(original_probs.mean(axis=0), axis=0)[0] * np.expand_dims(perturbed_probs.mean(axis=0), axis=0)[0]).sum(axis=1)).argmin()\n    \n    plot_image_and_prob_compare(original_vals, original_probs, perturbed_vals, perturbed_probs, [sensitive_index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}