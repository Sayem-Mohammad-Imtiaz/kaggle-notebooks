{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The importance of customer churn prediction\n\nIt is essential to predict future customer attrition rate as it can have a significant effect on businesses or organizations, and influence future expected revenue. In addition, customer churn prediction gives us help to determine the lack of customer service. With respect to machine learning algorithms and data analysis, we are able to identify and improve the factors being responsible to customer churn rate.","metadata":{}},{"cell_type":"markdown","source":"**Packages**","metadata":{}},{"cell_type":"code","source":"import time\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\n\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2021-09-01T08:47:44.535089Z","iopub.execute_input":"2021-09-01T08:47:44.535432Z","iopub.status.idle":"2021-09-01T08:48:21.809522Z","shell.execute_reply.started":"2021-09-01T08:47:44.535401Z","shell.execute_reply":"2021-09-01T08:48:21.808506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Set Classifier Model**\n\nThe first step is to create Classifier model.\nRegarding to using the different algorithms, we are going to create a model object that is a reusabale code.","metadata":{"id":"7alGMq2fZhpN"}},{"cell_type":"code","source":"class ClassifierModel():\n    def __init__(self, df, classifier, num_fold, parameters=None):\n        self.df = df\n        self.classifier = classifier\n        self.num_fold = num_fold\n        self.parameters = parameters\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = self.load_data()\n        self.model = self.train_model()\n\n    def load_data(self):\n        self.x = self.df.drop([\"Churn\", \"customerID\"], axis=1)\n        self.Y = self.df[\"Churn\"]\n\n        # Normalize data\n        scaler = MinMaxScaler()\n        scaler.fit(self.x)\n        self.X = scaler.transform(self.x)\n\n        # Separate train and test data\n        X_train, X_test, Y_train, Y_test = train_test_split(self.X, self.Y, test_size=0.1, random_state=42)\n\n        return X_train, X_test, Y_train, Y_test\n\n    def train_model(self):\n        start = time.time()\n        # Build the model\n        model = self.classifier(**self.parameters)\n\n        # Fit the model\n        model.fit(self.X_train, self.Y_train)\n\n        # Get feature importances\n        feature_importance = model.feature_importances_\n        # Set pandas series to see feature importance\n        model_importances = pd.Series(feature_importance,\n                                      index=self.x.columns.values)  # x is the first one before normalizing\n        print(model_importances)\n\n        end = time.time() - start\n        print(\"Elapsed time to tarin model = {} seconds\".format(end))\n\n        # Predict the model\n        predictions = model.predict(self.X_test)\n\n        return model\n\n    def evaluate_model(self):\n        cv = KFold(n_splits=self.num_fold, random_state=42, shuffle=True)\n        recall = cross_val_score(self.model, self.X, self.Y, cv=cv, scoring=\"recall\", n_jobs=-1)\n        precision = cross_val_score(self.model, self.X, self.Y, cv=cv, scoring=\"precision\", n_jobs=-1)\n        accuracy = cross_val_score(self.model, self.X, self.Y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n        f1 = cross_val_score(self.model, self.X, self.Y, cv=cv, scoring=\"f1_macro\", n_jobs=-1)\n        print(\"Accuracy in average = {}\".format(np.mean(accuracy)))\n        \n        # Display all metrics in a dataframe\n        metrics_df = pd.DataFrame([[accuracy, precision, recall, f1]], columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n        return metrics_df","metadata":{"id":"smLCtm_zw0tT","execution":{"iopub.status.busy":"2021-09-01T08:44:01.785764Z","iopub.execute_input":"2021-09-01T08:44:01.786105Z","iopub.status.idle":"2021-09-01T08:44:01.802839Z","shell.execute_reply.started":"2021-09-01T08:44:01.786069Z","shell.execute_reply":"2021-09-01T08:44:01.80195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Set the parameters**\n\nIn this stage, we config the training parameters for each ensemble model.","metadata":{"id":"taeBKv_SZoM-"}},{"cell_type":"code","source":"dataset_dir = \"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\ncategory_columns = [\"Contract\", \"gender\", \"Partner\",\"Dependents\", \"PhoneService\", \"MultipleLines\",\n                    \"InternetService\", \"OnlineSecurity\",\"OnlineBackup\", \"DeviceProtection\",\"TotalCharges\",\n                    \"TechSupport\",\"StreamingTV\", \"StreamingMovies\" , \"PaperlessBilling\", \"PaymentMethod\", \"Churn\"]\nNUM_FOLD = 5\n\nxgb_params = {\n    \"n_estimators\": 300,\n    \"learning_rate\": 0.001,\n    \"min_child_weight\": 1,\n    \"base_score\": 0.5,\n    \"gamma\": 0,\n    \"min_child_weight\": 1,\n    \"silent\" : 1,\n}\ngb_params = {\n    \"n_estimators\": 300,\n    \"learning_rate\" : 0.008,\n}\nada_params = {\n    \"n_estimators\": 400,\n    \"learning_rate\" : 0.01,\n}\nrf_params = {\n    \"n_estimators\": 300,\n    \"min_samples_leaf\" : 3,\n    \"max_features\" : \"sqrt\",\n}\ndt_params = {\n    \"min_samples_leaf\" : 2,\n}\net_params = {\n    \"n_estimators\": 300,\n    \"min_samples_leaf\" : 5,\n    \"min_samples_leaf\" : 2,\n    \"n_jobs\" : -1\n}","metadata":{"id":"60DR8r_zYkdT","execution":{"iopub.status.busy":"2021-09-01T08:48:57.290548Z","iopub.execute_input":"2021-09-01T08:48:57.290876Z","iopub.status.idle":"2021-09-01T08:48:57.298267Z","shell.execute_reply.started":"2021-09-01T08:48:57.290845Z","shell.execute_reply":"2021-09-01T08:48:57.297062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training Initializations**\n\nFirst, we load the csv file and then encode the categorical culomns.\nSecond, using the model object, we set the models, separately.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(dataset_dir)\n# Encode Categorical Columns\nlabelencoder = LabelEncoder()\ndata[category_columns] = data[category_columns].apply(labelencoder.fit_transform)\n\n\n# XGBoost Classifier\nxgb_clf = XGBClassifier\nxgb_model = ClassifierModel(df=data, classifier=xgb_clf, num_fold=NUM_FOLD, parameters=xgb_params)\nxgb_model.train_model()\n# Evaluation model\nxgb_metrics = xgb_model.evaluate_model() \nxgb_metrics.index = [\"XGBoost\"]\nresults = xgb_metrics\n\n# GBoost Classifier\ngb_clf = GradientBoostingClassifier\ngb_model = ClassifierModel(df=data, classifier=gb_clf, num_fold=NUM_FOLD, parameters=gb_params)\ngb_model.train_model()\n# Evaluation model\ngb_metrics = gb_model.evaluate_model() \ngb_metrics.index = [\"GBoost\"]\nresults = results.append(gb_metrics)\n\n# AdaBoost Classifier\nada_clf = AdaBoostClassifier\nada_model = ClassifierModel(df=data, classifier=ada_clf, num_fold=NUM_FOLD, parameters=ada_params)\nada_model.train_model()\n# Evaluation model\nada_metrics = ada_model.evaluate_model() \nada_metrics.index = [\"AdaBoost\"]\nresults = results.append(ada_metrics)\n\n# Random Forest Classifier\nrf_clf = RandomForestClassifier\nrf_model = ClassifierModel(df=data, classifier=rf_clf, num_fold=NUM_FOLD, parameters=rf_params)\nrf_model.train_model()\n# Evaluation model\nrf_metrics = rf_model.evaluate_model() \nrf_metrics.index = [\"Random Forest\"]\nresults = results.append(rf_metrics)\n\n# Decision Tree Classifier\ndt_clf = DecisionTreeClassifier\ndt_model = ClassifierModel(df=data, classifier=dt_clf, num_fold=NUM_FOLD, parameters=dt_params)\ndt_model.train_model()\n# Evaluation model\ndt_metrics = dt_model.evaluate_model() \ndt_metrics.index = [\"Decision Tree\"]\nresults = results.append(dt_metrics)\n\n# ExtraTreesClassifier\net_clf = ExtraTreesClassifier\net_model = ClassifierModel(df=data, classifier=et_clf, num_fold=NUM_FOLD, parameters=et_params)\net_model.train_model()\n# Evaluation model\net_metrics = et_model.evaluate_model() \net_metrics.index = [\"ExtraTrees\"]\nresults = results.append(et_metrics)\nprint(results.head())","metadata":{"id":"0D40qLAbYkfi","execution":{"iopub.status.busy":"2021-09-01T08:49:04.052798Z","iopub.execute_input":"2021-09-01T08:49:04.053166Z","iopub.status.idle":"2021-09-01T08:53:28.415504Z","shell.execute_reply.started":"2021-09-01T08:49:04.053137Z","shell.execute_reply":"2021-09-01T08:53:28.414499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"1ms15LSKYkmn"},"execution_count":null,"outputs":[]}]}