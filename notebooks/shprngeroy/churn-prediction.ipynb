{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install dython\nfrom dython.model_utils import metric_graph\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import KBinsDiscretizer\n#from sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import precision_score\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix\nimport shap\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T18:06:12.730665Z","iopub.execute_input":"2021-09-18T18:06:12.731781Z","iopub.status.idle":"2021-09-18T18:06:20.908783Z","shell.execute_reply.started":"2021-09-18T18:06:12.731713Z","shell.execute_reply":"2021-09-18T18:06:20.907905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%\ndf= pd.read_csv(r'../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:06:20.910306Z","iopub.execute_input":"2021-09-18T18:06:20.910542Z","iopub.status.idle":"2021-09-18T18:06:20.975732Z","shell.execute_reply.started":"2021-09-18T18:06:20.910512Z","shell.execute_reply":"2021-09-18T18:06:20.975011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom dython import nominal\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\na= nominal.associations(df,figsize=(30,30),mark_columns=True);\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:09:16.136662Z","iopub.execute_input":"2021-09-18T18:09:16.137119Z","iopub.status.idle":"2021-09-18T18:09:48.657886Z","shell.execute_reply.started":"2021-09-18T18:09:16.137087Z","shell.execute_reply":"2021-09-18T18:09:48.657029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols= df.select_dtypes('object').columns\nto_eliminate= ['customerID','TotalCharges' , 'Churn']\nnew_cols = [col for col in cols if col not in to_eliminate]\nnew_cols","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:17:21.566241Z","iopub.execute_input":"2021-09-18T18:17:21.566543Z","iopub.status.idle":"2021-09-18T18:17:21.576923Z","shell.execute_reply.started":"2021-09-18T18:17:21.566512Z","shell.execute_reply":"2021-09-18T18:17:21.576081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new= df.copy()\ndf_new['Churn']= df_new['Churn'].replace({'Yes':1, 'No':0}).astype('float')\nfor col in new_cols:\n    df_new.groupby(col)['Churn'].mean().plot(kind='bar')\n    plt.show();\n   ","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:24:18.905872Z","iopub.execute_input":"2021-09-18T18:24:18.906228Z","iopub.status.idle":"2021-09-18T18:24:21.707826Z","shell.execute_reply.started":"2021-09-18T18:24:18.906193Z","shell.execute_reply":"2021-09-18T18:24:21.70687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#%%\nclass MyOneHotEncoder(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, cat_cols=['tenure', 'MonthlyCharges', 'TotalCharges']):\n        super().__init__()\n        self.cat_cols= cat_cols  \n        self.hotcoder= ce.OneHotEncoder (cols=self.cat_cols, use_cat_names=True)\n        \n    def fit(self, X_train, y=None):\n        self.hotcoder.fit(X_train)\n\n        return self\n    \n    def transform(self, X):\n        X_new= self.hotcoder.transform(X)\n        self.new_cols=  X_new.columns\n        return X_new\n\n#%%\nclass RandomForestFiller (TransformerMixin, BaseEstimator):\n\n    def __init__ (self, max_depth= None):\n        super().__init__()\n        self.max_depth= max_depth\n        self.rfc= RandomForestRegressor(max_depth=self.max_depth)\n    \n    def fit(self, X, y=None):\n        X= X.replace(' ', np.nan).dropna(subset=['TotalCharges'])\n        y= X['TotalCharges'] # totalcharges\n        X= X.drop(['TotalCharges'], axis=1)\n        self.rfc.fit(X, y)\n        return self\n    \n    def transform (self,X):\n        X_copy= X.copy().drop(['TotalCharges'], axis=1)\n        filled_TotalCharges= self.rfc.predict(X_copy)\n        X_copy['TotalCharges']= X['TotalCharges'].replace(' ', np.nan)\n        X_copy['TotalCharges']= X_copy['TotalCharges'].combine_first(pd.Series(filled_TotalCharges, index= X_copy.index))\n        return X_copy\n\n\n#%%\n\nclass BinsEncoder(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, k=5):\n        super().__init__()\n        self.k= k\n        self.df_numeric_to_bins=KBinsDiscretizer(n_bins=self.k, encode='ordinal', strategy='quantile',)\n        \n        \n    def fit(self, X, y=None):\n        #cols = X.select_dtypes([np.number]).columns\n        self.cols=['tenure', 'MonthlyCharges', 'TotalCharges']\n        self.df_numeric_to_bins.fit(X[self.cols])\n               \n        return self\n    \n    def transform(self, X):\n        new_binned_cols = pd.DataFrame(self.df_numeric_to_bins.transform(X[self.cols]), columns= self.cols).set_index(X.index)\n        X[self.cols] = new_binned_cols.apply(lambda x: x.astype('str')+'_'+x.name)\n        return X\n    \n\nclass TheMightySelector (TransformerMixin, BaseEstimator):\n    \n    def __init__ (self, n= 10 ):\n        super().__init__()\n        self.n= n\n        self.chi2_selector = SelectKBest(k=self.n, score_func= chi2)\n        \n    def fit(self, X, y=None):\n        self.chi2_selector.fit(X, y)\n        return self  \n    \n    def transform (self,X):\n        self.k_cols= X.columns[self.chi2_selector.get_support(indices=False)]\n        X_new= X[self.k_cols]\n        #self.new= X_new\n        return X_new\n          \n#%%","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:06:20.977181Z","iopub.execute_input":"2021-09-18T18:06:20.977411Z","iopub.status.idle":"2021-09-18T18:06:20.999814Z","shell.execute_reply.started":"2021-09-18T18:06:20.977383Z","shell.execute_reply":"2021-09-18T18:06:20.998668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nX = df.drop(['Churn','customerID'], axis=1)\ny= df['Churn']\n\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size= 0.15, random_state= 12345)\n\ncols1= ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n                        'OnlineSecurity', 'OnlineBackup','DeviceProtection', 'TechSupport', 'StreamingTV',   'StreamingMovies',\n                       'Contract',  'PaperlessBilling', 'PaymentMethod']  \n\ncols2= ['tenure', 'MonthlyCharges', 'TotalCharges']\n\n\n#%%\n\nfrom sklearn.ensemble import VotingClassifier\n \nxgb= xgb.XGBClassifier()\n\nrfc= RandomForestClassifier()\n\n#lr= LogisticRegression()\n\nclassifiers= [(\"xgb\",xgb), (\"rfc\", rfc)]\n\nclf_voting= VotingClassifier(estimators=classifiers, voting='soft', weights=None)\n\n  \n#%%\n\n\nfinal_model = Pipeline(steps=[('ohe1',MyOneHotEncoder(cat_cols=cols1)), \n                              ('rff',RandomForestFiller() ), \n                              ('be', BinsEncoder()), \n                              ('ohe2',MyOneHotEncoder(cat_cols=cols2)),\n                              #('reduce_dim',SelectKBest(score_func= chi2)) ,\n                              ('reduce_dim',TheMightySelector ()),\n                              ('sampling', SMOTE()),\n                              ('voter', clf_voting)])\n\n\n#%%\nparams_store= final_model.get_params()\n\n\nparam_search = {\n  'rff__max_depth' : [2],\n  'be__k': [8],\n  #'kbs__n': [3,12,23],\n  'reduce_dim__n': [20,40],\n  'voter__xgb__max_depth': [6],\n  'voter__xgb__learning_rate': [0.004],\n  'voter__xgb__gamma'           : [0.1],\n  'voter__xgb__n_estimators': [125],\n  'voter__rfc__class_weight': ['balanced'],\n  'voter__rfc__max_depth':[4],\n  'voter__xgb__subsample':[0.1],\n  \"voter__xgb__min_child_weight\" : [1]\n  }\n\n\nscorer = {\n    'precision_score': make_scorer(precision_score, average = 'macro')\n}\n\n\ngsearch = GridSearchCV(estimator=final_model,  cv=3, refit='precision_score',\n                      scoring=scorer,   \n                      param_grid=param_search, verbose=2 ) #n_jobs=4\n\ngsearch.fit(X_train, y_train)\n\ny_test_pred= gsearch.best_estimator_.predict_proba(X_test)\npreds= gsearch.best_estimator_.predict(X_test)\n\n\ncv_report= pd.DataFrame(gsearch.cv_results_) # gives accuracy score \n\nfinal_results= pd.DataFrame(classification_report(y_test, preds, output_dict=True))\n\n# importances= pd.Series(gsearch.best_estimator_.named_steps[\"voter\"].feature_importances_ , index=gsearch.best_estimator_.named_steps['ohe'].x_new.columns).sort_values(ascending=False)\nconf_matrix= confusion_matrix(y_test ,preds)\n\n\n\nnew_cols= gsearch.best_estimator_.steps[4][1].k_cols\n\nfeauts= gsearch.best_estimator_.steps[6][1].estimators_[1].feature_importances_\n\npd.Series(feauts, index= new_cols).sort_values().idxmax()\n\n#%%\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:06:27.209192Z","iopub.execute_input":"2021-09-18T18:06:27.20948Z","iopub.status.idle":"2021-09-18T18:06:51.477775Z","shell.execute_reply.started":"2021-09-18T18:06:27.20945Z","shell.execute_reply":"2021-09-18T18:06:51.476126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmetric_graph(np.array(y_test=='Yes'), y_test_pred[:,1] ,metric='roc')\n\n\nfrom sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(np.array(y_test=='Yes'), y_test_pred[:,1], pos_label=1)\nprint(\"AUC=\" , metrics.auc(fpr, tpr))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:06:57.138762Z","iopub.execute_input":"2021-09-18T18:06:57.139083Z","iopub.status.idle":"2021-09-18T18:06:57.380602Z","shell.execute_reply.started":"2021-09-18T18:06:57.139051Z","shell.execute_reply":"2021-09-18T18:06:57.379476Z"},"trusted":true},"execution_count":null,"outputs":[]}]}