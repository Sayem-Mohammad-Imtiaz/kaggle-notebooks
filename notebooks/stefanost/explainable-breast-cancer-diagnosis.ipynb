{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Explainable Breast Cancer Diagnosis\n## via Logistic Regression and Decision Tree\n\nMost machine learning models are considered black boxes, but in high-stake situations, such as breast cancer diagnosis, we need to know how the classifier reaches its decisions. \n\nHere, we experiment with explainable techniques and models, not necessarily reaching for the highest accuracy possible (which might entail reverting to black box models), but exploring some options for interpretable machine learning and statistical analysis.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom skimage.io import imshow, imread\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df['diagnosis'] #output labels\ndf.drop(columns=['Unnamed: 32','id'],inplace=True) #one is useless, the other is Nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features are in three groups: 'mean', 'se', and 'worst'. We will make correlation heatmaps for each of these groups, erase redundant columns, then do a heatmap for the whole dataset and erase any final columns that may come up.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.iloc[:,1:11].corr(),annot=True,fmt='.1g');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.iloc[:,11:21].corr(),annot=True,fmt='.1g');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.iloc[:,21:].corr(),annot=True,fmt='.1g');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will erase one feature for every pair of features with correlation factor >=9 because it's as if we have the same column twice. Then we will make a correlation heatmap for the whole dataset, and erase a feature for each pair of features with correlation >=9.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['perimeter_mean','area_mean','compactness_mean','concave points_mean',\n                      'perimeter_se','area_se',\n                      'perimeter_worst','area_worst'],\n                       inplace=True)\nplt.figure(figsize=(14,10))\nsns.heatmap(df.iloc[:,1:].corr(), annot=True, fmt='.1g');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['radius_worst','texture_worst','concavity_worst','concave points_worst',\n                'texture_worst',], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#how many features does the new dataset have?\nprint ('The resulting dataset has',df.shape[1]-1, 'features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression for feature selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\ny=df['diagnosis']\nenc=LabelEncoder()\ny=enc.fit_transform(y.values)\nx=df.drop(columns='diagnosis').values\nx_tr,x_ts,y_tr,y_ts=train_test_split(x,y, random_state=7, test_size=0.2,stratify=y)\nsc=StandardScaler()\nx_tr_sc=sc.fit_transform(x_tr)\nx_ts_sc=sc.transform(x_ts)\n\n#after experiments we found this is best model\nlr=LogisticRegression(C=1.0, random_state=7)\nlr.fit(x_tr_sc,y_tr)\ny_pred=lr.predict(x_ts_sc)\nprint('coefs', lr.coef_)\nprint('accuracy', accuracy_score(y_ts,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#which are the most significant features, and how much they contribute\ncoefs=lr.coef_.reshape(18)\nfor ind in lr.coef_.argsort().reshape(18):\n    print(df.columns[ind+1])\n    print(coefs[ind])\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the ten most useful features\nab=np.abs(coefs)\ncols=df.columns[ab.argsort()[:-11:-1]+1]\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train a model with only the best features\n\nx=df[cols].values\nx_tr,x_ts,y_tr,y_ts=train_test_split(x,y, random_state=7, test_size=0.2,stratify=y)\nsc=StandardScaler()\nx_tr_sc=sc.fit_transform(x_tr)\nx_ts_sc=sc.transform(x_ts)\n\n\nlr=LogisticRegression(C=10.0, random_state=7)\nlr.fit(x_tr_sc,y_tr)\ny_pred=lr.predict(x_ts_sc)\nprint('coefs', lr.coef_)\nprint('accuracy', accuracy_score(y_ts,y_pred))\n\nfrom sklearn.metrics import confusion_matrix\n\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_ts,y_pred), annot=True, fmt='d');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that running logistic regression with only the selected features yields a slight increase in accuracy. Now we will confirm it visually, after inspecting the distributions of the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#make violin plots to visually evaluate features selected by lr\nmeans=df.iloc[:,1:7]\nses=df.iloc[:,7:15]\nworsts=df.iloc[:,15:]\n\nmeans_sc=(means-means.mean())/(means.std())\nses_sc=(ses-ses.mean())/(ses.std())\nworsts_sc=(worsts-worsts.mean())/(worsts.std())\n\nmeans_sc=pd.concat([df['diagnosis'],means_sc],axis=1)\nses_sc=pd.concat([df['diagnosis'],ses_sc],axis=1)\nworsts_sc=pd.concat([df['diagnosis'],worsts_sc],axis=1)\n\nmeans_sc=pd.melt(means_sc, id_vars='diagnosis',\n                 var_name='features',\n                 value_name='value')\nses_sc=pd.melt(ses_sc, id_vars='diagnosis',\n               var_name='features',\n               value_name='value')\nworsts_sc=pd.melt(worsts_sc, id_vars='diagnosis',\n                  var_name='features',\n                  value_name='value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(y='features',x='value', hue='diagnosis',\n               data=means_sc, split=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(y='features',x='value', hue='diagnosis',\n               data=ses_sc, split=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(y='features',x='value', hue='diagnosis', \n               data=worsts_sc, split=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the violin charts with the best features as selected by the logistic regression classifier we see that the first few of the chosen features are the ones that, in the violinplots have distributions that separate more the two classes. The rest don't seem too significant, but are still better than the features that weren't given strong coefficients by the logistic regression model. All in all, logistic regression seems to having given us the most significant features, the ones more helpful in determining the class.\n\nNow we will find the specific values in these features that determine the class. We will use Decision Tree.\nWe will also draw a graph, using the graphviz package, of the reasoning process the resulting tree goes through to reach its decisions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ntree=DecisionTreeClassifier(max_depth=4)\ntree.fit(x_tr,y_tr)\ny_pred=tree.predict(x_ts)\n\nprint('accuracy:',accuracy_score(y_pred,y_ts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_ts,y_pred), annot=True, fmt='d');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc.inverse_transform([0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create graph\nfrom sklearn.tree import export_graphviz\nimport graphviz\nfrom graphviz import Source\n\ngraph=Source(export_graphviz(tree,feature_names=df[cols].columns,\n                   class_names=['B','M'],rounded=True,proportion = False, filled=True,precision=2))\n\n\n\ndisplay(graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon comparison with the violin plots we see that the Decision Tree did pick good features and good values for these features. It's performance was obviously suboptimal, given its 88% accuracy. Also bear in mind, with Decision Trees there's always some randomness involved, and we may run a few trees to find the best.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We explored the potential of traditional machine learning for explainable classifications. In high-stake situations, like cancer diagnosis, we need some information on how classifiers make predictions. Our pipeline had four steps:\n\n1)We started by removing redundant features, identified through correlation analysis.\n\n2)Then we applied logistic regression to find how much each feature contributes to whether the tumor is benign or malignant. Based on the features selected by the model we picked the ten most significant.\n\n3)We confirmed they were the best features with two methods: a)we run a new logistic regression model and it yielded a slight increase in accuracy, and b) through visual inspection of violin plots we saw that the distributions of these feautures are distinct/separable for each class.\n\n4)Finally, we run a Decision Tree to find the values in these features that determine the class, and visualised this in a tree-graph image.\n\nFor each classification, now, the doctor can consult this graph to know how the system reached its decision. For high performance, the doctor should use the Logistic Regression classifier, which doesn't explain its decisions with such detail as the decision tree, but shows the coefficient with which each feature contributes to the classification, and reaches very high accuracy (97,36%).","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}