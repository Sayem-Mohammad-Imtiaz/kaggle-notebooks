{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Medical Cost Prediction __ Regression"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/insurance/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#distribution of charges\nsns.distplot(df['charges']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pairplot of numerical variables\nsns.pairplot(df[['age','bmi','children','charges']]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplots and violin plots for categorical variable distributions of charges\ndef dist(feature):\n    plt.figure(figsize=(12,4));\n    plt.subplot(1,2,1);\n    sns.boxplot( x=feature, y='charges', data=df);\n    plt.title('%s distribution_boxplot' %feature);\n    plt.subplot(1,2,2);\n    sns.violinplot(x=feature, y='charges', data=df);\n    plt.title('%s distribution_violinplot' %feature);\n\ndist('sex')\ndist('children')\ndist('smoker')\ndist('region')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly the 'smoker' attribute is important in predicting medical charges, with the 'children' attribute also bearing some slight significance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation heatmap of numerical variables\nsns.heatmap(df.corr(), annot=True, fmt='.2f');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding of categoricals\ndf['male']=pd.get_dummies(df['sex'],drop_first=True)\ndf['smoker_yes']=pd.get_dummies(df['smoker'],drop_first=True)\nregions=pd.get_dummies(df['region'], prefix='region', prefix_sep='_')\ndf=pd.concat([df,regions],axis=1)\ndf.drop(columns=['sex','smoker','region'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new correlation heatmap, all variables\nplt.figure(figsize=(8,5));\nsns.heatmap(df.corr(), annot=True, fmt='.1f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso,Ridge,ElasticNet,LinearRegression, RANSACRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessing: standardization, train_test split\n\ny=df['charges']\nx=df.drop(columns='charges')\n\nx_tr, x_ts, y_tr, y_ts= train_test_split(x, y, test_size=0.15, random_state=42)\n\nsc_x=StandardScaler()\nsc_y=StandardScaler()\nx_tr=sc_x.fit_transform(x_tr)\nx_ts=sc_x.transform(x_ts)\ny_tr=sc_y.fit_transform(y_tr[:,np.newaxis]).flatten()\ny_ts=sc_y.transform(y_ts[:,np.newaxis]).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Regression\nlr=LinearRegression()\nlr.fit(x_tr,y_tr)\ny_tr_pred=lr.predict(x_tr)\ny_ts_pred_lr=lr.predict(x_ts)\nprint('mse train:', mse(y_tr,y_tr_pred))\nprint('mse test:', mse(y_ts,y_ts_pred_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RANSAC Regression\nrnsc=RANSACRegressor(LinearRegression(),max_trials=100,min_samples=50,loss='absolute_loss',\n                    residual_threshold=5.0,random_state=42)\nrnsc.fit(x_tr,y_tr)\ny_tr_pred=rnsc.predict(x_tr)\ny_ts_pred_ransac=rnsc.predict(x_ts)\nprint('mse train:', mse(y_tr,y_tr_pred))\nprint('mse test:', mse(y_ts,y_ts_pred_ransac))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ridge Regression\nrdg=Ridge(alpha=0.00001)\nrdg.fit(x_tr,y_tr)\ny_tr_pred=rdg.predict(x_tr)\ny_ts_pred_rdg=rdg.predict(x_ts)\nprint('mse train:', mse(y_tr,y_tr_pred))\nprint('mse test:', mse(y_ts,y_ts_pred_rdg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've tried a few parameters for 'alpha', but it doesn't seem to be improving the performance. Let's try Lasso and ElasticNet, manually trying a few parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso Regression\nls=Lasso(alpha=0.00000000001)\nls.fit(x_tr,y_tr)\ny_tr_pred=ls.predict(x_tr)\ny_ts_pred_ls=ls.predict(x_ts)\nprint('mse train:', mse(y_tr,y_tr_pred))\nprint('mse test:', mse(y_ts,y_ts_pred_ls))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ElasticNet Regression\nlnt=ElasticNet(alpha=0.00000000000000001, l1_ratio=0.5)\nlnt.fit(x_tr,y_tr)\ny_tr_pred=lnt.predict(x_tr)\ny_ts_pred_el=lnt.predict(x_ts)\nprint('mse train:', mse(y_tr,y_tr_pred))\nprint('mse test:', mse(y_ts,y_ts_pred_el))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the above three aproaches, Ridge, Lasso, and ElasticNet regressors, seem to benefit from low 'alpha' values, but not enough to surpass the regular linear regression models.\n\nNow let's combine their predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"#stacked regressions\n\ny_ts_stacked=(y_ts_pred_lr + y_ts_pred_ransac + y_ts_pred_rdg + y_ts_pred_el +y_ts_pred_ls)/5\nprint(\"mse stacked:\", mse(y_ts,y_ts_stacked))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacking regressions doesn't improve on the results.\n\nNext, we try a neuralnet with keras"},{"metadata":{},"cell_type":"markdown","source":"#### Neural Network with Keras and Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import models\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nmodel=models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(x_tr.shape[1],)))\nmodel.add(layers.Dense(64,activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop=EarlyStopping(patience=5,verbose=1)\ncheck=ModelCheckpoint('DNN_linear_regression.h5', verbose=1, save_best_only=True)\nfit=model.fit(x_tr,y_tr, validation_split=0.1, batch_size=16, epochs=50,\n                 callbacks=[stop,check])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mse,test_mae=model.evaluate(x_ts,y_ts)\nprint('mse test:', test_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#second DNN\n\nmodel2=models.Sequential()\nmodel2.add(layers.Dense(32, activation='relu', input_shape=(x_tr.shape[1],)))\nmodel2.add(layers.Dense(32,activation='relu'))\nmodel2.add(layers.Dropout(0.5))\nmodel2.add(layers.Dense(1))\nmodel2.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop2=EarlyStopping(patience=5,verbose=1)\ncheck2=ModelCheckpoint('DNN2_linear_regression.h5', verbose=1, save_best_only=True)\nfit2=model2.fit(x_tr,y_tr, validation_split=0.1, batch_size=16, epochs=30,\n                 callbacks=[stop,check])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mse2,test_mae2=model2.evaluate(x_ts,y_ts)\nprint('mse test:', test_mse2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Polynomial Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures as poly\n\nlreg=LinearRegression()\nsq=poly(degree=2)\nx_tr_sq=sq.fit_transform(x_tr)\nx_ts_sq=sq.transform(x_ts)\nlreg.fit(x_tr_sq,y_tr)\nprint('polynomial 2nd degree mse train:' ,mse(y_tr, lreg.predict(x_tr_sq)))\nprint('polynomial 2nd degree mse test:' ,mse(y_ts, lreg.predict(x_ts_sq)))\nprint(\"\")\nprint('-----------')\nprint(\"\")\ncube=poly(degree=3)\nx_tr_cb=cube.fit_transform(x_tr)\nx_ts_cb=cube.transform(x_ts)\nlreg.fit(x_tr_cb,y_tr)\nprint('polynomial 3nd degree mse train:' ,mse(y_tr, lreg.predict(x_tr_cb)))\nprint('polynomial 3nd degree mse test:' ,mse(y_ts, lreg.predict(x_ts_cb)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 3rd degree polynomial fits the trainng data closer than the test data, ie it is overfitting, but the second degree polynomial seems to yield great results. We'll later use it to train a keras model."},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor as rfr\n\nforest=rfr(n_estimators=1000, criterion='mse', random_state=7)\nforest.fit(x_tr,y_tr)\ny_tr_pred=forest.predict(x_tr)\ny_ts_pred=forest.predict(x_ts)\n\nprint('mse train:', mse(y_tr,y_tr_pred))\nprint('mse test:', mse(y_ts,y_ts_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The random forest is massively overfitting, although it yields better results than the linear models on the normal data. "},{"metadata":{},"cell_type":"markdown","source":"#### Keras with 2nd-degree Polynomial"},{"metadata":{"trusted":true},"cell_type":"code","source":"model3=models.Sequential()\nmodel3.add(layers.Dense(64, activation='relu', input_shape=(x_tr_sq.shape[1],)))\nmodel3.add(layers.Dense(64,activation='relu'))\nmodel3.add(layers.Dropout(0.5))\nmodel3.add(layers.Dense(1))\nmodel3.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop3=EarlyStopping(patience=5,verbose=1)\ncheck3=ModelCheckpoint('DNN3_linear_regression.h5', verbose=1, save_best_only=True)\nfit3=model3.fit(x_tr_sq,y_tr, validation_split=0.1, batch_size=16, epochs=30,\n                 callbacks=[stop,check])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mse3,test_mae3=model3.evaluate(x_ts_sq,y_ts)\nprint('mse test:', test_mse3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}