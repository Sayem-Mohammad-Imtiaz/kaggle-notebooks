{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# 1.1 Call data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n# 1.2 Feature creation libraries\nfrom sklearn.random_projection import SparseRandomProjection as sr  # Projection features\nfrom sklearn.cluster import KMeans                    # Cluster features\nfrom sklearn.preprocessing import PolynomialFeatures  # Interaction features\n\n# 1.3 For feature selection\n# Ref: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif  # Selection criteria\n\n# 1.4 Data processing\n# 1.4.1 Scaling data in various manner\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n# 1.4.2 Transform categorical (integer) to dummy\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 1.5 Splitting data\nfrom sklearn.model_selection import train_test_split\n\n# 1.6 Decision tree modeling\n# http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n# http://scikit-learn.org/stable/modules/tree.html#tree\nfrom sklearn.tree import  DecisionTreeClassifier as dt\n\n# 1.7 RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\n# 1.8 Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1.9 Misc\nimport os, time, gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34d422562aaa8930291fe9fcbbb868bf8446de81"},"cell_type":"code","source":"# 2.1 Read  files\nhrt = pd.read_csv(\"../input/heart.csv\") #Loading of Data\n\n# 2.2 Look at data\nhrt.head(2)\nhrt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24a27085f14b27ab9aae25601151e253c85d3d1a"},"cell_type":"code","source":"\nhrt.dtypes.value_counts()   # All afeatures re integers except target\n\n\n# 2.3 Target classes are almost balanced\nhrt.target.value_counts()\n\nhrt.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"889b7251fef63773e7379772bd24b80315ef6a44"},"cell_type":"code","source":"# 3. split the data\nX_train, X_test, y_train, y_test = train_test_split(hrt.drop('target', 1), hrt['target'], test_size = 0.3, random_state=100)          \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2baea2476243df3680893295f2bf4a987a740bb1"},"cell_type":"code","source":"# Check the splits\nX_train.shape      \nX_test.shape      \ny_train.shape      \ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e9c3322cdff85c38916b0f94aefc4dab3c11cae"},"cell_type":"code","source":"# Check if there are Missing values? None\nX_train.isnull().sum().sum()  \nX_test.isnull().sum().sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4173212c2995236d5e25bf5484440e516d91c50a"},"cell_type":"code","source":"#  4. Feature 1: Row sums of features 1:13. More successful\n#                when data is binary.\n\nX_train['sum'] = X_train.sum(numeric_only = True, axis=1)  # numeric_only= None is default\nX_test['sum'] = X_test.sum(numeric_only = True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e2c83c11164a2215edd3fc312c6af638e205c1c"},"cell_type":"code","source":"# 4.1 Assume that value of '0' in a cell implies missing feature\n#     Transform train and test dataframes\n#     replacing '0' with NaN\n#     Use pd.replace()\ntmp_train = X_train.replace(0, np.nan)\ntmp_test = X_test.replace(0,np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b17ba3a27a02aee749a91ad79bc49585b064056"},"cell_type":"code","source":"# 4.2 Check if tmp_train is same as X_train or is a view\n#     of X_train? That is check if tmp_train is a deep-copy\n\ntmp_train is X_train                # False\n#tmp_train is train.values.base    # False\ntmp_train._is_view                # False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3993da05c851e1381a1e6e3d5091c7304631a1cd"},"cell_type":"code","source":"# 4.3 Check if 0 has been replaced by NaN\ntmp_train.head(1)\ntmp_test.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"871e50a37bc7c60a7ddf22cac965a7b1fbe86a58"},"cell_type":"code","source":"# 5. Feature 2 : For every row, how many features exist\n#                that is are non-zero/not NaN.\n#                Use pd.notna()\ntmp_train.notna().head(1)\nX_train[\"count_not0\"] = tmp_train.notna().sum(axis = 1)\nX_test['count_not0'] = tmp_test.notna().sum(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb6be3ca23af71437a750fcf8b12ea67b629bf35"},"cell_type":"code","source":"\n# 6. Similary create other statistical features\n#    Feature 3\n\nfeat = [ \"var\", \"median\", \"mean\", \"std\", \"max\", \"min\"]\nfor i in feat:\n    X_train[i] = tmp_train.aggregate(i,  axis =1)\n    X_test[i]  = tmp_test.aggregate(i,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"857d68debb40c12bb5bb8e4821a45b8dedcad722"},"cell_type":"code","source":"# 7 Delete not needed variables and release memory\ndel(tmp_train)\ndel(tmp_test)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48ef8563ae22e9fedbd6b82a92b1f82c5f8a4306"},"cell_type":"code","source":"# 7.1 So what do we have finally\nX_train.shape                \nX_train.head(1)\nX_test.shape    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"457ddb44908f2e12ffa16f1d6552785f7c7d42b7"},"cell_type":"code","source":"# 9.2. Store column names of our data somewhere\n\ncolNames = X_train.columns.values\ncolNames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f85c301d6f634263063a1bb1014a6c2fa56fc671"},"cell_type":"code","source":"############### Feature creation Using Random Projections ##################\n# 10. Random projection is a fast dimensionality reduction feature\n#     Also used to look at the structure of data\n\n# 11. Generate features using random projections\n#     First stack train and test data, one upon another\ntmp = pd.concat([X_train,X_test],\n                axis = 0,            # Stack one upon another (rbind)\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46dee567148c4b74e12efcd10692ad4fe5626955"},"cell_type":"code","source":"tmp.shape     # 303 X 21\n\ntmp=tmp.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e4566914267e4aca71b2e52b054f3f01f1cd942"},"cell_type":"code","source":"#  This decision, at present, is arbitrary\nNUM_OF_COM = 5\n\n# 12.1 Create an instance of class\nrp_instance = sr(n_components = NUM_OF_COM)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d36c1146551d5a1511151d568f539ff461ac38b2"},"cell_type":"code","source":"# 12.2 fit and transform the (original) dataset\n#      Random Projections with desired number\n#      of components are returned\n\n#rp = rp_instance.fit_transform(tmp[:, :])\nrp = rp_instance.fit_transform(tmp [:, :13])\n\n# 12.3 Look at some features\nrp[: 5, :  3]\n\n\n# 12.4 Create some column names for these columns\n#      We will use them at the end of this code\nrp_col_names = [\"r\" + str(i) for i in range(5)]\nrp_col_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dab32dac98be9507e906812e799579778b190ded"},"cell_type":"code","source":"# 13. Before clustering, scale data\n# 14.1 Create a StandardScaler instance\nse = StandardScaler()\n# 14.2 fit() and transform() in one step\ntmp = se.fit_transform(tmp)\n# 14.3\ntmp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dadc22cacafb75464981e0dca830dac9a7cc9e3"},"cell_type":"code","source":"# 15. Perform kmeans using 13 features.\n#     No of centroids is no of classes in the 'target'\ncenters = y_train.nunique()    # 2 unique classes\ncenters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b03137929cce400c81d1560cd358e7e3fa41ed9f"},"cell_type":"code","source":"# 16.1 Begin clustering\nstart = time.time()\n\n# 16.2 First create object to perform clustering\nkmeans = KMeans(n_clusters=centers, # How many\n                n_jobs = 2)         # Parallel jobs for n_init\n\n\n\n# 16.3 Next train the model on the original data only\nkmeans.fit(tmp[:, : 13])\n\nend = time.time()\n(end-start)/60.0      \n\n\n# 17 Get clusterlabel for each row (data-point)\nkmeans.labels_\nkmeans.labels_.size   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1749cd7ebf0e868dbea3861f78480fc6e4d68128"},"cell_type":"code","source":"# 18. Cluster labels are categorical. So convert them to dummy\n\n# 19.1 Create an instance of OneHotEncoder class\nohe = OneHotEncoder(sparse = False)\n\n# 19.2 Use ohe to learn data\n#      ohe.fit(kmeans.labels_)\nohe.fit(kmeans.labels_.reshape(-1,1))     # reshape(-1,1) recommended by fit()\n                                          # '-1' is a placeholder for actual\n# 19.3 Transform data now\ndummy_clusterlabels = ohe.transform(kmeans.labels_.reshape(-1,1))\ndummy_clusterlabels\ndummy_clusterlabels.shape    # 303 X 2 (as many as there are classes)\n\n\n# 19.4 We will use the following as names of new nine columns\n#      We need them at the end of this code\n\nk_means_names = [\"k\" + str(i) for i in range(2)]\nk_means_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d59775cc178b53513f0a8ad4853630a091ceb40a"},"cell_type":"code","source":"############################ Interaction features #######################\n# 20. Will require lots of memory if we take large number of features\n#     Best strategy is to consider only impt features\n\ndegree = 2\npoly = PolynomialFeatures(degree,                 # Degree 2\n                          interaction_only=True,  # Avoid e.g. square(a)\n                          include_bias = False    # No constant term\n                          )\n\n\n# 20.1 Consider only first 5 features\n#      fit and transform\ndf =  poly.fit_transform(tmp[:, : 5])\n\n\ndf.shape     # 303 X 10\n\n\n# 20.2 Generate some names for these 15 columns\npoly_names = [ \"poly\" + str(i)  for i in range(15)]\npoly_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6da1c4fb302317413bb1887ff31f022cb72a0e67"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0ae948b3984621f16997f8b4d3f44b6b266cd49"},"cell_type":"code","source":"################ concatenate all features now ##############################\n\n# 21 Append now all generated features together\n# 21 Append random projections, kmeans and polynomial features to tmp array\n\ntmp.shape          # 303 X 21\n\n#  21.1 If variable, 'dummy_clusterlabels', exists, stack kmeans generated\n#       columns also else not. 'vars()'' is an inbuilt function in python.\n#       All python variables are contained in vars().\n\nif ('dummy_clusterlabels' in vars()):               #\n    tmp = np.hstack([tmp,rp,dummy_clusterlabels, df])\nelse:\n    tmp = np.hstack([tmp,rp, df])      \n\n\ntmp.shape   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"814f3f178c3052123c53b98f7fe22735eea3900b"},"cell_type":"code","source":"# 21.1 Separate train and test\nX = tmp[: X_train.shape[0], : ]\nX.shape                             # 212 X 43 \n\n# 21.2\ntest = tmp[X_train.shape[0] :, : ]\ntest.shape                         # 91 X 43\n\n# 21.3 Delete tmp\ndel tmp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a30fcd96afc5dc27a4b4364a2c7135b0fc992bb4"},"cell_type":"code","source":"# 22. Split train into training and validation dataset\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y_train,\n                                                    test_size = 0.3)\n\n# 22.1\nX_train.shape    \nX_test.shape     \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c6c049eedf393c39438e6f4db1f718f287a1b4f"},"cell_type":"code","source":"# 23 Decision tree classification\n# 23.1 Create an instance of class\nclf = dt(min_samples_split = 5,\n         min_samples_leaf= 5\n        )\n\nstart = time.time()\n# 23.2 Fit/train the object on training data\n#      Build model\nclf = clf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60                    \n\n# 23.3 Use model to make predictions\nclasses = clf.predict(X_test)\n\n# 23.4 Check accuracy\n(classes == y_test).sum()/y_test.size   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56bf41b6ef1920651eaf48a389cc8da227633885"},"cell_type":"code","source":"# 24. Instantiate RandomForest classifier\nclf = rf(n_estimators=150)\n\n# 24.1 Fit/train the object on training data\n#      Build model\n\nstart = time.time()\nclf = clf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60\n\n# 24.2 Use model to make predictions\nclasses = clf.predict(X_test)\n# 24.3 Check accuracy\n(classes == y_test).sum()/y_test.size   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccc32148ea3f559d22b411f37f33c12f33d6c840"},"cell_type":"code","source":"# 25. Get feature importance\nclf.feature_importances_        # Column-wise feature importance\nclf.feature_importances_.size   # 43\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afab91f558167205c300b0f2c32b45500569837"},"cell_type":"code","source":"# 25.1 To our list of column names, append all other col names\n#      generated by random projection, kmeans (onehotencoding)\n#      and polynomial features\n#      But first check if kmeans was used to generate features\n\nif ('dummy_clusterlabels' in vars()):       # If dummy_clusterlabels labels are defined\n    colNames = list(colNames) + rp_col_names+ k_means_names + poly_names\nelse:\n    colNames = colNames = list(colNames) + rp_col_names +  poly_names     \n\n# 25.1.1 So how many columns?\nlen(colNames)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6fd378e19f53934e976feb4e33800192dbd992b"},"cell_type":"code","source":"# 25.2 Create a dataframe of feature importance and corresponding\n#      column names. Sort dataframe by importance of feature\nfeat_imp = pd.DataFrame({\n                   \"importance\": clf.feature_importances_ ,\n                   \"featureNames\" : colNames\n                  }\n                 ).sort_values(by = \"importance\", ascending=False)\n\n\nfeat_imp.shape                   # 43 X 2 ;\nfeat_imp.head(20)\n\n\n# 25.3 Plot feature importance for first 20 features\ng = sns.barplot(x = feat_imp.iloc[  : 20 ,  1] , y = feat_imp.iloc[ : 20, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7f650c7997e1b4d8ddfdf9b0691dc1db0d3e46d"},"cell_type":"code","source":"# 21   Select top 13 columns and get their indexes\n#      Note that in the selected list few kmeans\n#      columns also exist\nnewindex = feat_imp.index.values[:13]\nnewindex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c74cb599201eba02d4115294b5148e10485c99c"},"cell_type":"code","source":"# 22 Use these top 13 columns for classification\n# 22.1  Create classifier object\nclf = dt(min_samples_split = 5, min_samples_leaf= 5)\n# 22.2 Traion the object on data\nstart = time.time()\nclf = clf.fit(X_train[: , newindex], y_train)\nend = time.time()\n(end-start)/60                     # 1 minute\n\n\n# 22.3  Make prediction\nclasses = clf.predict(X_test[: , newindex])\n# 22.4 Accuracy?\n(classes == y_test).sum()/y_test.size","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}