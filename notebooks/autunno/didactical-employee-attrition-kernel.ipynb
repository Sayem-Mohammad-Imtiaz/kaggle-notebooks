{"cells":[{"metadata":{"_cell_guid":"90580aad-5f74-4c1c-bf2d-9bcb03346e4f","_uuid":"5d6f588ded4281be69dacd1ef72840432b03de12"},"cell_type":"markdown","source":"# IMB HR Analytics: Predicting employee attrition"},{"metadata":{"_cell_guid":"803939bd-8d42-4f9c-8037-a40d7d489175","_uuid":"33f435cf9e010b29627d5292a6865e64fb705076"},"cell_type":"markdown","source":"* Title: IBM HR Analytics Employee Attrition & Performance\n* Objective: Predict attrition of your valuable employees\n* Kaggle link: https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset\n* Inspired by: https://www.kaggle.com/arthurtok/employee-attrition-via-rf-gbm\n\n\nThe aim of this notebook is to show a step-by-step solution, explaining the basic concepts of data pre-processing, data analyzing, feature engineering and modeling.\n\n\n\n---\n\nIf you are just getting started with ML, I suggest you follow this guide and install __[Anaconda](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444)__. You should also remove any existing Python installation you have on your computer before proceeding.\n\nIf you are not familiar with Python and Data Science, but want to learn more, I suggest these courses:\n1. __[Basic Python course for data science](https://www.datacamp.com/courses/intro-to-python-for-data-science)__\n2. __[Machine Learning A-Z](https://www.udemy.com/machinelearning/learn/v4/overview)__\n3. __[Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning)__\n\nIf you are not familiar with using __[Jupyter Notebook](http://jupyter.org/)__, the following links may be useful:\n1. __[Jupyter notebook tutorial](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)__\n2. __[Collection of Jupyter notebook plugins](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)__ and __[Extension to control your Jupyter extensions](https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator)__\n4. __[Markdown for Jupyter notebooks cheatsheet](https://googleweblight.com/i?u=https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed&hl=pt-BR)__"},{"metadata":{"_cell_guid":"3bee4a11-28b0-4904-9e14-78e1cfd8d7b9","_uuid":"624c07627b1977d74b7ae28f3616f9e8634cc43d"},"cell_type":"markdown","source":"# How  this notebook is organized"},{"metadata":{"_cell_guid":"7f572cdb-648f-439d-aa16-8fa3cb2d7739","_uuid":"b01538b64cb1f323e8db99e842328117d6dd2c6c"},"cell_type":"markdown","source":"1. [Data pre-processing](#1.-Data-pre-processing)\n2. [Data analysis](#2.-Data-analysis)\n3. [Feature engineering](#3.-Feature-engineering)\n4. [Feature selection](#4.-Feature-selection)\n5. [Building various models](#5.-Building-various-models)\n6. [Result comparison and final considerations](#6.-Result-comparison-and-final-considerations)"},{"metadata":{"_cell_guid":"095b398c-86ed-4073-9827-5d4dd449bd81","_uuid":"63edff145f2922956c519262ff2e11ebf3ffdbf8"},"cell_type":"markdown","source":"# The problem"},{"metadata":{"_cell_guid":"8fd6eab1-df0a-47d0-aa77-91c54d2f879b","_uuid":"3aa26340aa546392f0cf2b05ec0f5ac0eb133182"},"cell_type":"markdown","source":"We have a very rich HR dataset with information ranging from education level to relationship satisfaction. The main data of interest is \"attrition\", which indicates whether an employee had any problems in the workplace. Its prediction is very valuable, as it can be used to reduce the number of occurrences and allow the company to better understand the source of conflict.\n\nAs the value we're trying to predict (attrition) is categorical (\"yes\" or \"no\"), we're dealing with a __[classification](https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/)__ problem. "},{"metadata":{"_cell_guid":"20114d5d-6e29-45d3-9bc7-4b78ca44342c","_uuid":"60145e58913bbdaaaf12fe4105d707b857b0e454"},"cell_type":"markdown","source":"# 1. Data pre-processing"},{"metadata":{"_cell_guid":"626d822b-4523-4b35-9d48-af9847d9decb","_uuid":"2a547de348172e9f71d1cf6268f1686bd72bc572"},"cell_type":"markdown","source":"We start off by importing all libraries we're gonna use up until the [Feature Engineering](#Feature-engineering) section:\n* numpy: array manipulation library\n* pandas: csvs importing, dataframe manipulation\n* seaborn: statistical data visualization\n* matplotlib.pyplot: graph rendering\n\n---\n\n* LabelEncoder: converts categorical data into numerical\n* train_test_split: helper method to split the dataset into training and test data\n* accuracy_score: helper method to show the % of correct predictions made by a classifier/regressor\n* cofusion_matrix: helper method to show to number of correct predictions for positive and negative cases\n\n---\n\n* RandomForestClassifier: random forest classifier implementation\n* SVC: support vector machine classifier implementation, applicable to higher dimensions computations\n* KNeighborsClassifier: nearest neighbors implementation\n* GaussianNB: naive bayes implementation\n* GridSearchCV: grid search implementation with k-fold cross-validation\n* StandardScaler: normalizes the numeric variables of a dataframe, specially needed for SVM models"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:44.539348Z","start_time":"2018-03-28T00:32:43.682115Z"},"collapsed":true,"_cell_guid":"1c45e6be-2ffb-4524-a1c8-a26172cabe5c","_uuid":"39eb4626a32195ab81bf30c2ba7f4df8f4a9ec0b","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-26T14:49:45.793735Z","start_time":"2018-03-26T14:49:45.790736Z"},"_cell_guid":"fca63482-f098-401c-aeca-1dcf19776b86","_uuid":"da5e3413045d495c6d7919c0117edf9cbd6db007"},"cell_type":"markdown","source":"### Importing the data"},{"metadata":{"_cell_guid":"6c624506-ad08-42d6-a555-0071e9d2e988","_uuid":"2f0d63d589d7a839c9ba32d48eb11e70f8b7a69b"},"cell_type":"markdown","source":"The next step is to import the csv using the pandas library, and take a look at quick peek at the data: "},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:44.572345Z","start_time":"2018-03-28T00:32:44.540337Z"},"collapsed":true,"_cell_guid":"8c6de01f-bd7a-4b67-b543-c0d206410d50","_uuid":"3a3dc22e7e9b6f37c55c034fa002d30f6aa340fb","scrolled":true,"trusted":false},"cell_type":"code","source":"dataset = pd.read_csv('../input/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:44.58236Z","start_time":"2018-03-28T00:32:44.573345Z"},"collapsed":true,"_cell_guid":"aa8cfd9a-2d4c-4b80-9682-05576c668dca","_uuid":"5154f2b752d05ae0ab6f7868687b0d34985bf0be","trusted":false},"cell_type":"code","source":"# number of entries\nprint(\"Number of entries: \" + str(len(dataset.index)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08ffe3a5-e29e-4081-be58-d45f94250482","_uuid":"3f099228f0b00e8d4c0c5e44a67cbef6517262df"},"cell_type":"markdown","source":"### Checking for empty values"},{"metadata":{"_cell_guid":"2ee3d230-b3a8-4d7a-8f4e-8dbdf400af15","_uuid":"fe6d6b1e40ba1ead0eaec34bfc9c8ab297b8f50b"},"cell_type":"markdown","source":"With that done, we need to clean up the dataset, dealing with missing values if necessary. The following command shows which columns have null values:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:44.60337Z","start_time":"2018-03-28T00:32:44.585348Z"},"collapsed":true,"_cell_guid":"4c3a4bbc-90b2-4374-8b83-1cf0ec85e09f","_uuid":"6b61065430854999321824fd60883363fd094377","trusted":false},"cell_type":"code","source":"dataset.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c810c32f-07ad-4e88-acc8-ba6fe3dd3411","_uuid":"7bd43844aac89c628b8921b2c0bc9d19b31738dc"},"cell_type":"markdown","source":"In this case, __there's no empty values__, which is great! However, if there were, we could evaluate what step to take next by counting how many empty values we have:\n\n<div class=\"alert alert-block alert-warning\">dataset.isnull().sum()</div>\n\nWhich can also be done column by column:\n\n> dataset[col].isnull().sum() \n\nThere are a few ways to deal with empty values, the most popular being removing rows with empty values: \n\n> dataset.dropna()\n\nThere are __[many approaches](https://machinelearningmastery.com/handle-missing-data-python/)__  to handle missing data, such as filling the gaps with the __[mean value](https://www.vocabulary.com/articles/chooseyourwords/mean-median-average/)__  of the column:\n\n> from sklearn.preprocessing import Imputer\n\n> imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n\n> imputer = imputer.fit(X[:, colStart:colEnd])\n\n> X[:, colStart:colEnd] = imputer.transform(X[:, colStart:colEnd])\n\nIt all depends on your data; in some cases using the median can lead to better results (specially when your data has __[outliers](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)__):\n\n> [1, 2, 3, 4, 50]\n\nThe median of the set above is 3, while the mean (or average) is 31."},{"metadata":{"_cell_guid":"9a700402-1c13-40ee-8515-2443bf23e248","_uuid":"c035aad576da5052bba40956aef47162d6dda8c5"},"cell_type":"markdown","source":"# 2. Data analysis"},{"metadata":{"_cell_guid":"4f35bb66-53ba-4541-8041-6cf9ea563ee7","_uuid":"e583529425698d8c67e7495d520f1a985719473a"},"cell_type":"markdown","source":"Before creating any Machine Learning models, one must first take a look at the data with the following questions in mind:\n* Are there strong relations between the features? Features that are too similar are redundant.\n* Do I need all the features? Could the model be simplified somehow?\n* Could some features be combined, or transformed somehow, to better represent the problem at hand? (e.g. with time variables, sometimes all you need is the hour or day, not the full date).\n\nMany times these questions can only be answered after running a model, and testing out some hypothesis. However, looking at some graphs may help this process.\n\nGiven infinite resources and time, *we could* look into all combinations. However, since there are many columns on this dataset, we're gonna choose only a few relations to analyze:\n* EducationField, MonthlyIncome\n* PerformanceRating, JobSatisfaction\n* NumCompaniesWorked, PerformanceRating\n* EducationField, JobSatisfaction\n* YearsWithCurrManager, JobSatisfaction\n* MonthlyRate, JobSatisfaction\n* DistanceFromHome, WorkLifeBalance\n* PerformanceRating, Overtime\n* WorkLifeBalance, Overtime\n* RelationshipSatisfaction, Overtime\n* MaritalStatus, YearsInCurrentRole\n* MartialStatus, YearsSinceLastPromotion\n\nWe start by defining a set of subplots (in this case, 5 by 3), and then plot the graphs.\n\nIt's important to use the right graph for the right data. __[Seaborn](https://seaborn.pydata.org/)__ library is specially fitting for data analysis because you can run it with categorical data. __[There's quite a few options to choose from](https://seaborn.pydata.org/tutorial/categorical.html)__, you may need to do some experimenting in order to find the best plot."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.215635Z","start_time":"2018-03-28T00:32:44.604354Z"},"collapsed":true,"_cell_guid":"3c850fa7-8ec0-4c27-ab50-7f2fd1ffc61e","_uuid":"3c44221ed55d0d80b883b7f1fbc35abf58f1d775","scrolled":false,"trusted":false},"cell_type":"code","source":"# Define a set of graphs, 3 by 5, usin the matplotlib library\nf, axes = plt.subplots(5, 3, figsize=(24, 36), sharex=False, sharey=False)\n\n# Define a few seaborn graphs, which for the most part only need the \"dataset\", the \"x and \"y\" axis and the position. \n# You can also show a third value and expand your analysis by setting the \"hue\" property.\nsns.swarmplot(x=\"EducationField\", y=\"MonthlyIncome\", data=dataset, hue=\"Gender\", ax=axes[0,0])\naxes[0,0].set( title = 'Monthly income against Educational Field')\n\nsns.pointplot(x=\"PerformanceRating\", y=\"JobSatisfaction\", data=dataset, hue=\"Gender\", ax=axes[0,1])\naxes[0,1].set( title = 'Job satisfaction against Performance Rating')\n\nsns.barplot(x=\"NumCompaniesWorked\", y=\"PerformanceRating\", data=dataset, ax=axes[0,2])\naxes[0,2].set( title = 'Number of companies worked against Performance rating')\n\nsns.barplot(x=\"JobSatisfaction\", y=\"EducationField\", data=dataset, ax=axes[1,0])\naxes[1,0].set( title = 'Educational Field against Job Satisfaction')\n\nsns.barplot(x=\"YearsWithCurrManager\", y=\"JobSatisfaction\", data=dataset, ax=axes[1,1])\naxes[1,1].set( title = 'Years with current Manager against Job Satisfaction')\n\nsns.pointplot(x=\"JobSatisfaction\", y=\"MonthlyRate\", data=dataset, ax=axes[1,2])\naxes[1,2].set( title = 'Job Satisfaction against Monthly rate')\n\nsns.barplot(x=\"WorkLifeBalance\", y=\"DistanceFromHome\", data=dataset, ax=axes[2,0])\naxes[2,0].set( title = 'Distance from home against Work life balance')\n\nsns.pointplot(x=\"OverTime\", y=\"WorkLifeBalance\", hue=\"Gender\", data=dataset, jitter=True, ax=axes[2,1])\naxes[2,1].set( title = 'Work life balance against Overtime')\n\nsns.pointplot(x=\"OverTime\", y=\"RelationshipSatisfaction\", hue=\"Gender\", data=dataset, ax=axes[2,2])\naxes[2,2].set( title = 'Overtime against Relationship satisfaction')\n\nsns.pointplot(x=\"MaritalStatus\", y=\"YearsInCurrentRole\", hue=\"Gender\", data=dataset, ax=axes[3,0])\naxes[3,0].set( title = 'Marital Status against Years in current role')\n\nsns.pointplot(x=\"Age\", y=\"YearsSinceLastPromotion\", hue=\"Gender\", data=dataset, ax=axes[3,1])\naxes[3,1].set( title = 'Age against Years since last promotion')\n\nsns.pointplot(x=\"OverTime\", y=\"PerformanceRating\", hue=\"Gender\", data=dataset, ax=axes[3,2])\naxes[3,2].set( title = 'Performance Rating against Overtime')\n\nsns.barplot(x=\"Gender\", y=\"PerformanceRating\", data=dataset, ax=axes[4,0])\naxes[4,0].set( title = 'Performance Rating against Gender')\n\nsns.barplot(x=\"Gender\", y=\"JobSatisfaction\", data=dataset, ax=axes[4,1])\naxes[4,1].set( title = 'Job satisfaction against Gender')\n\nsns.countplot(x=\"Attrition\", data=dataset, ax=axes[4,2])\naxes[4,2].set( title = 'Attrition distribution')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3565ad4c-45e0-4325-8d8c-3deeeb5e15ac","_uuid":"150c68a4109156eb07d1ec3c0ca1871f65fa6950"},"cell_type":"markdown","source":"We can make a few inferences from the data shown above:\n* MaritalStatus seems to indicate the same thing as Age in terms of promotions (and possibly other data), so maybe we can remove one of these.\n* Male RelationshipSatisfication and WorkLifeBalance are both highly affected by Overtime, maybe one of these could be dropped.\n* EducationalField and MonthlyIncome seems to be redundant, since there's a big correlation between them.\n* Overtime seems to affect different variables in different manners, also showing great variance between males and females.\n* Year 16 of YearsWithCurrentManager could be removed from the dataset, as it seems to be an outlier.\n* Only 16% of the employees had any kind of attrition in the workplace, which means that the distribution for our output variable is quite unbalanced.\n\nFun facts:\n* Men who do overtime are happier on their relationships, while Women are not affected much by it.\n* Women have better perform reviews than men when overtime was done."},{"metadata":{"_cell_guid":"ad4e7900-1755-44af-b115-9cbe95a50f80","collapsed":true,"_uuid":"dee479ba8e878016f1671c21afeb8775f8260f7d"},"cell_type":"markdown","source":"# 3. Feature engineering"},{"metadata":{"_cell_guid":"0093a17a-95b3-4adb-978a-79407c4b4339","_uuid":"aa85ec77382eb66835763e8b34ec90a3237fcfa7"},"cell_type":"markdown","source":"Feature Engineering is the art/science of representing data in the best way possible and involves a combination of intuition, field knowledge, experience and common sense. If you want to dig deeper into this topic, __[this guide](http://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/)__ provides a great overview with practical examples.\n\nFor this particular dataset, we have two main things to do before running a model:\n1. Decide what features we should keep.\n2. Transform categorical data into numerical.\n\n---\n\nOn the previous section, data analysis has shown that a few columns are redundant and could be removed, mainly: **MaritalStatus** and **EducationField**. However, since we don't know yet which is more important, we'll keep them for now, and re-evaluate after we build our first model. However, **EmployeeNumber** and **EmployeeCount** are evidently irrelevant features, so we can remove them. We could probably find many other columns to drop, but we can do that later by analyzing the results from our first ML model."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.221637Z","start_time":"2018-03-28T00:32:49.216651Z"},"collapsed":true,"_cell_guid":"9138868e-71a7-49ad-bc07-cf68e9543528","_uuid":"393e94f6ced31a1cef1164d2e0a7fcb6ab372f41","trusted":false},"cell_type":"code","source":"if 'EmployeeNumber' in dataset:\n    del dataset['EmployeeNumber']\n    \nif 'EmployeeCount' in dataset:\n    del dataset['EmployeeCount']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76c63a53-4eaf-45b9-8399-88d80e59bd97","_uuid":"2bf26bc64802aa712f107c5eb87953f779154752"},"cell_type":"markdown","source":"### Splitting the data"},{"metadata":{"_cell_guid":"784d3c77-06ef-4e3e-9921-725a8222daa3","_uuid":"c4ff6a9b3288391f3d577b9772e8c8498a714c65"},"cell_type":"markdown","source":"Before transforming the categorical data, we can separate the input features \"X\" from the output \"y\", which is needed for the model to understand what are the features (or inputs), and what value you are trying to predict (and doing this separation before the encoding is practical). The split is done through the **iloc** command, which is part of the pandas library:\n\n<div class=\"alert alert-block alert-warning\">dataset.iloc[rowStart:rowEnd, colStart:colEnd]</div>\n\nWhen the dataset is well organized, this is normally done as follows:\n\n<div class=\"alert alert-block alert-warning\">X = dataset.iloc[:, 0:N-1].values</div>\n\n<div class=\"alert alert-block alert-warning\">y = dataset.iloc[:, N].values</div>\n\nHowever, since the output variable is not located at the end, we're going to extract the features __[dataframe](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)__ by dropping the output variable:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.240653Z","start_time":"2018-03-28T00:32:49.222637Z"},"collapsed":true,"_cell_guid":"8ea8b029-2b6c-4df4-867c-abbd0ccc1fb5","_uuid":"1a3a9f357a5a89e05f33e5ce7a39c38ea33d35f4","trusted":false},"cell_type":"code","source":"features = dataset[dataset.columns.difference(['Attrition'])]\noutput = dataset.iloc[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3a01718-7b5d-4394-b238-0c0a3bcf704c","_uuid":"1a50431255b143a3fcda7d7209020568b27efeb7"},"cell_type":"markdown","source":"### Encoding"},{"metadata":{"_cell_guid":"f3f93b8d-c41d-456d-91f4-ead2ef274d98","_uuid":"9ae34864ffa7dec76711f2c36ec9a23da26dca3c"},"cell_type":"markdown","source":"As mentioned before, the categorical data must be converted into numbers for the Machine Learning model to work. This can be done through:\n* **LabelEncoder**: transform your textual column in a numeric continuous one (ranging from 1-N).\n> In MaritalStatus, value \"Single\", \"Married\" and \"Divorced\" could be represented as 0, 1 and 2, respectively\n\n\n* **OneHotEncoder**: transform your textual column in many binary ones (0-1). \n> In MaritalStatus, two separate columns could represent \"Single\" as 1 and 0, \"Married\" as 0 and 1 and \"Divorced as 0 and 0.\n\nAs rule of thumb, **OneHotEncoder** _is a much better approach to use_, mainly because it prevents models from making wrong assumptions. **LabelEncoder** can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], which could lead a model to infer that the average of dog and mouse is cat. However, some algorithms (like *decision trees* and *random forests*) can work with these categorical variables just fine, and LabelEncoder can be used to store values using less disk space.\n\nEven though our first model will be a *random forest* implementation, we're gonna  **hot encode** every categorical column, as it will allow us to build different models and remind us of the importance of doing proper encoding in most cases.\n\nThere are many ways of building your hot encoded feature map, the most popular ones being from __[sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)__ and __[pandas](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)__. In this notebook, I've choosen the pandas approach.\n\nFirst, we split the categorical columns from the numerical ones:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.262647Z","start_time":"2018-03-28T00:32:49.24266Z"},"collapsed":true,"_cell_guid":"d5228f76-4326-45ec-83ac-6dd8707a5ed2","_uuid":"9f9404169964b377cce161ee2c3979cf1bc5e193","trusted":false},"cell_type":"code","source":"categorical = []\nfor col, value in features.iteritems():\n    if value.dtype == 'object':\n        categorical.append(col)\n\n# Store the numerical columns in a list numerical\nnumerical = features.columns.difference(categorical)\n\nprint(categorical)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39f3328d-89f5-48dc-9b11-348c23555c1b","_uuid":"df60009648ece7e53edf7ea6881c31c498e6ed01"},"cell_type":"markdown","source":"The code above gave us two arrays, splitting the categorical from the numerical data.\n\nWith that information, we can now encode the categorical data into a binary representation. There are several ways of doing that, the most popular being __[OneHotEncode](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)__ from sklearn and __[get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)__ from pandas. Due to its simplicity, we are going to use the latter, with the **drop_first** parameter set to *true* (which help us avoiding the __[dummy variable trap](http://www.algosome.com/articles/dummy-variable-trap-regression.html)__):"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.298657Z","start_time":"2018-03-28T00:32:49.264648Z"},"collapsed":true,"_cell_guid":"97e38235-ba77-47dc-af88-2900793fb0b9","_uuid":"a87dafb2934480a83b8479d6fae8836832d473c8","trusted":false},"cell_type":"code","source":"# get the categorical dataframe, and one hot encode it\nfeatures_categorical = features[categorical]\nfeatures_categorical = pd.get_dummies(features_categorical, drop_first=True)\nfeatures_categorical.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6eca9f47-1c2a-4dce-bd44-acf5f0e66061","_uuid":"7f7a46ad2403729cb2a6bfe0c91ac00be0d09230"},"cell_type":"markdown","source":"<a id=\"feature_map\"></a>\n### Getting the feature map"},{"metadata":{"_cell_guid":"b63cafdd-d41d-4950-bee7-6716b0b9e403","_uuid":"edbfc9efec0c2ec9ab512e4ff4ef66534a17a58b"},"cell_type":"markdown","source":"We can now build our final feature map by getting all the numerical features concatenated with the newly generated columns:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.319663Z","start_time":"2018-03-28T00:32:49.299657Z"},"collapsed":true,"_cell_guid":"861d0d77-b298-4889-9c7f-5a7cd821835d","_uuid":"0ca8d34533cf5c690166a038b65dc6ec208bdea9","trusted":false},"cell_type":"code","source":"# get the numerical dataframe\nfeatures_numerical = features[numerical]\n\n# concatenate the features\nfeatures = pd.concat([features_numerical, features_categorical], axis=1)\n\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8b16618-62c7-4a8e-a638-272f6d109ef1","_uuid":"995fe48ea82a040148ef1357666fd3d102062d4d"},"cell_type":"markdown","source":"### Getting the output variable"},{"metadata":{"_cell_guid":"a078e90c-c466-4ee6-9f6b-156195148364","_uuid":"c22703ed1e812fcc19145c886ad529b32157adc5"},"cell_type":"markdown","source":"For the output variable (our output), we only need a simple label encoder"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.336684Z","start_time":"2018-03-28T00:32:49.321664Z"},"collapsed":true,"_cell_guid":"1eac2bc3-08d9-4d1e-838f-39d0a4113678","_uuid":"84a50780c5f37645993f05cb921110abc0a236e9","trusted":false},"cell_type":"code","source":"labelencoder = LabelEncoder()\noutput = labelencoder.fit_transform(output)\n\nprint(output)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"021a2bde-39f9-4c38-972e-b7a9152fd591","_uuid":"57c39e53422fd84a48d8a1b2e1df6f1b6d946ce9"},"cell_type":"markdown","source":"### Splitting the dataset"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-24T13:01:26.60415Z","start_time":"2018-03-24T13:01:26.597149Z"},"_cell_guid":"581d5c3f-b307-44ff-afdf-4908356afd12","_uuid":"394e5c1f11a2ddac2338049257eeae8ed3d85f04"},"cell_type":"markdown","source":"The last data pre-processing step is to split the __[training and test set](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)__. There's no hard rule on how to split the data, but it usually ranges between 70-90% for the training set (unless you are working with a *HUGE* dataset, in which case you may go with very different distributions, such as 30-70% (using more data on the test set).\n\nFor this problem, a standard distribution of 70/30% was chosen:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:49.354686Z","start_time":"2018-03-28T00:32:49.338668Z"},"collapsed":true,"_cell_guid":"911e0eca-598d-4646-9514-293867b68700","_uuid":"a51001b2991d517c9afefdd850ffa058ea0c8e13","trusted":false},"cell_type":"code","source":"features_train, features_test, attrition_train, attrition_test = train_test_split(features, output, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a4be249-50ab-484d-9cf0-9f234db7ed2f","_uuid":"b7f1bdcd1eb0223a056d07874a63677a39d68c60"},"cell_type":"markdown","source":"# 4. Feature selection"},{"metadata":{"_cell_guid":"992e3c46-0d4f-4772-b346-6c5874fc60d6","_uuid":"2aa048b39f8b6ea8f23e33040e4169bfa05e5cb9"},"cell_type":"markdown","source":"Feature selection is a very important step in machine learning model building. There are several tools that can help one pick the best features, among them we have the random forest model which provides as feedback the relative importance of each feature."},{"metadata":{"_cell_guid":"102c3db0-4bff-40cb-b229-e2a10f2422cf","_uuid":"7dd1fbd61f65408a4a8682963881535373a5555c"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"_cell_guid":"d97a681e-77b9-460c-b297-182e089d7184","_uuid":"6aa9c21189b45ba2cb8dabae43da88a81bd2a8ae"},"cell_type":"markdown","source":"__[Decision trees](https://www.lucidchart.com/pages/decision-tree)__ have been present on the AI field for a long time, being used on both __[expert systems](https://blog.leanix.net/en/artificial-intelligence-expert-systems)__ and __[machine learning models](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)__. On Machine Learning, a decision tree may be applied to both __[classification and regression](https://helloacm.com/a-short-introduction-classification-and-regression-trees/)__ problems. It has a pretty simple structure, where a sequence of questions with weights are answered in order to get a prediction:\n\n<img src=\"https://helloacm.com/wp-content/uploads/2016/03/decision-tree.jpg\" style=\"width: 50%\" alt=\"Decision tree missing :(\" title=\"Decision tree structure\" />"},{"metadata":{"_cell_guid":"2fb2f07e-dfaa-4ee0-89e1-16ac30653ebf","_uuid":"0e238e5052ea1d1075a4f1e47df588854a3c4d4c"},"cell_type":"markdown","source":"These models are pretty good if you have lots and lots of data (more specifically, it performs well for problems with many features/attributes), but they tend to  __[overfit](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)__ badly. The example below shows how a tree classifier can lead to very __[greedly separated data](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)__:\n<img src=\"https://i.imgur.com/LbwfSH7.png\" alt=\"Decision tree result visualization\" title=\"Decision tree\" />"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T01:26:07.017027Z","start_time":"2018-03-25T01:26:07.010016Z"},"_cell_guid":"cf559d2a-b25d-4fce-8153-7a76a3c663d5","_uuid":"b401121f1df223fd40dfafd630efc918da71522a"},"cell_type":"markdown","source":"This kind of behavior made them fall in disuse for ML models (although they are still popular for expert systems). The introduction of __[random forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)__, which is an __[ensemble](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)__ of decision trees, revitalized this model, as it's able to generalize better and is generally less prone to be affected by outliers. In short, it builds an *n* number of trees with different sub-sets of the data and features, which then are averaged at the end to provide the final prediction.\n\n<img src=\"https://cdn-images-1.medium.com/max/592/1*i0o8mjFfCn-uD79-F1Cqkw.png\" alt=\"Random forest missing\" title=\"Random forest\" />\n"},{"metadata":{"_cell_guid":"869668a6-53ac-495d-b475-49460dc10183","_uuid":"9724d5bcb609df419593dcfc779ea0d44d859ce8"},"cell_type":"markdown","source":"When working on a classification problem, it's almost always a great idea to start with __[sklearn RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)__; not only it's a pretty good classifier for many problems, but it also allows you evaluate what was the impact of each feature on the prediction. This is precious information, as one can build better models by removing undesirable features. There are other ways of figuring out which features are important to our model such as __[sklearn feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)__, but we won't experiment with it on this notebook."},{"metadata":{"_cell_guid":"65383a6f-8819-4327-923d-667de8bd8156","_uuid":"8aacc9984f4b5c85c66176b342c77ca2f4e4918b"},"cell_type":"markdown","source":"### Building the first model"},{"metadata":{"_cell_guid":"3fbd5918-0a04-461d-bb98-d6b453e20c5f","_uuid":"71f94f5d96e4c3e52bc8ea4bf2952cba04d4be84"},"cell_type":"markdown","source":"Since the main target of this first step is to take a closer look at the features, we won't fiddle much with the __[hyper-parameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)__ of the model besides specifying the use of 800 trees (n_estimators):"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:50.841055Z","start_time":"2018-03-28T00:32:49.355672Z"},"collapsed":true,"_cell_guid":"bc3b6f3c-f7a7-40c4-bdef-8c67459d7b3c","_uuid":"9849b131f7c463f5900c9f84c20b050e0b91cc25","trusted":false},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators = 800, criterion = 'entropy', random_state = 0)\nrandom_forest.fit(features_train, attrition_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-24T23:14:46.392069Z","start_time":"2018-03-24T23:14:46.389576Z"},"_cell_guid":"dafcef45-cb0a-48b5-8496-837410daab77","_uuid":"df191ee490ddaa2eccca90d74c6fe6a93c2e6e1a"},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"_cell_guid":"6a76636d-828d-4a3c-b7df-c0d9d0530cc4","_uuid":"fd885bf33f31cf5843d4ee9c2350990c8de4372d"},"cell_type":"markdown","source":"The random forest classifier was successfully built, which means we can now we can use it to predict employee attrition from datasets unknown to it, and see how well it performs. If things go sour and you get an underwhelming result, it could mean a few things:\n1. The model does not generalize well, and we need to either do more feature engineering, change the hyper-parameters or the model altogether.\n2. We don't have enough data.\n3. This problem can't be modeled with the known features.\n\nWe can only hope it's not (2) or (3).\n\n---\n\nWe stat by running the predict method on the test set and compare the results with the attrition test set:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:50.922076Z","start_time":"2018-03-28T00:32:50.842055Z"},"collapsed":true,"_cell_guid":"75d32c62-733d-47d7-baa4-9cacc422b4d9","_uuid":"abe8c6882362b2a18f05c906ffd900782f7e7f9f","trusted":false},"cell_type":"code","source":"# Get the prediction array, \nattrition_pred = random_forest.predict(features_test)\n\n# Get the accuracy %\nprint(\"Accuracy: \" + str(accuracy_score(attrition_test, attrition_pred) * 100) + \"%\") ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T16:01:17.319076Z","start_time":"2018-03-25T16:01:17.315084Z"},"_cell_guid":"48139301-e509-410d-a6c9-2b14cdb0eff5","_uuid":"5d6c05f35a6f02c502223f5781e463954e5360dd"},"cell_type":"markdown","source":"### Analysis"},{"metadata":{"_cell_guid":"1f3c6a4e-39db-459c-983a-d1e8033586fd","collapsed":true,"_uuid":"4d11f2f1783128dfb0d0605fd01de6b242417c83"},"cell_type":"markdown","source":"We got a training accuracy of approximately 86%, which is pretty good! However, results may be deceiving: in order to properly evaluate how well our model is performing, we need to know how many attrition cases it predicted correctly. This is specially important in this case, as our output variable is very unbalanced (only 14% of the cases are attrition).\n\nThe best way to do that is using a __[confusion matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)__. A confusion matrix is as big as your output variable's possible outcomes; in this case, it's a 2x2 matrix to represent prediction results for \"attrition\" and \"no attrition\" cases. The correct predictions are always shown on the diagonal of the table (from top left to bottom right):\n\n<img src=\"https://i.stack.imgur.com/KxrxP.png\" alt=\"Confusion Matrix img missing :(\" title=\"Confusion Matrix\" />"},{"metadata":{"_cell_guid":"a4caf972-3f8d-4b25-896b-aeed82875e1a","_uuid":"b6fed02a8353253ebda3ff2baaf315300950fe0e"},"cell_type":"markdown","source":"To build it, all you need is the sklearn confusion_matrix function, the test-set output value \"attrition_test\" which we obtained when [splitting the dataset](#Splitting-the-dataset) and the predicted output \"attrition_pred\", which we obtained when running the predict method on our classifier:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:51.006098Z","start_time":"2018-03-28T00:32:50.923076Z"},"collapsed":true,"_cell_guid":"fd2d4f81-811b-444f-8933-49fcc989870a","_uuid":"a9f46c040803006273023643abdeedbf9449ac0e","trusted":false},"cell_type":"code","source":"# Making the Confusion Matrix\nrf_cm = confusion_matrix(attrition_test, attrition_pred)\n\n# building a graph to show the confusion matrix results\nrf_cm_plot = pd.DataFrame(rf_cm, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                  columns = [i for i in {\"No attrition\", \"Attrition\"}])\nplt.figure(figsize = (6,5))\nsns.heatmap(rf_cm_plot, annot=True, vmin=5, vmax=90.5, cbar=False, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2b5725e-7ff9-43c9-9b49-1b9ae0412468","_uuid":"8a59be097c3d459573afadd6be4c032dd26c685b"},"cell_type":"markdown","source":"The results shown above show us that:\n\n1. We **correctly** predicted 369 entries as \"No Attrition\" and 11 entries as \"Attrition\"\n2. We **incorrectly** predicted __58 entries as \"No Attrition\"__ and 2 as \"Attrition\"\n\nAlthough our model had an accuracy of 86%, these results are not good. The main goal of our model is to __catch scenarios where employee attrition may occur__, in which we had an *abysmal* performance of approximately **17%** with the test set.\n\nWe can improve the situation a bit by applying known techniques such as cross-validation and re-sampling, which we will do on the [Testing New Models](#Testing-New-Models) section."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T00:58:59.923672Z","start_time":"2018-03-25T00:58:59.921168Z"},"collapsed":true,"_cell_guid":"3e5fd4f8-e400-4067-a8e6-7bd427be8589","_uuid":"2a194d94448b2b17b3cc8ed4868b65bb65fc15dd"},"cell_type":"markdown","source":"### Evaluating the weight of the features "},{"metadata":{"_cell_guid":"5cc6d767-93fa-4bb0-82a1-17f2cbbb0aff","_uuid":"49186f6edf7d35732defd3b1e7e00163e57b0137"},"cell_type":"markdown","source":"As we got a pretty underwhelming result, it's time to go back to the drawing board and see what can be improved. The first thing we need to look at is our features, as there are many benefits in reducing the amount of inputs:\n* Models are easier to understand and show better performance when we have less features\n* Some features may be redundant and represent the same thing as others (e.g. maybe age and marital status?), thus only being noise \n* Some features are not relevant to the model and may impact negatively on the prediction\n\nLuckily, the model we just built has a built-in functionality that allow us to analyze the weight of each feature as shown below:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:51.454213Z","start_time":"2018-03-28T00:32:51.007099Z"},"collapsed":true,"_cell_guid":"28b8bc6a-2692-446e-a48d-335395105144","_uuid":"f091cbec051b67503fdf0e923d9de1ed8f6ae835","scrolled":false,"trusted":false},"cell_type":"code","source":"def plot_feature_importances(importances, features):\n    # get the importance rating of each feature and sort it\n    indices = np.argsort(importances)\n\n    # make a plot with the feature importance\n    plt.figure(figsize=(12,14), dpi= 80, facecolor='w', edgecolor='k')\n    plt.grid()\n    plt.title('Feature Importances')\n    plt.barh(range(len(indices)), importances[indices], height=0.8, color='mediumvioletred', align='center')\n    plt.axvline(x=0.03)\n    plt.yticks(range(len(indices)), list(features))\n    plt.xlabel('Relative Importance')\n    plt.show()\n\nplot_feature_importances(random_forest.feature_importances_, features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T12:06:06.148119Z","start_time":"2018-03-25T12:05:53.293Z"},"collapsed":true,"_cell_guid":"561eca76-678d-4365-9ed5-2b8f42986f9c","_uuid":"6f695872c1bad24b6a3a8b4d32461ba0efed93a2"},"cell_type":"markdown","source":"The graph above is pretty simple to understand: the higher the relative importance is, the better. However, when looking at a feature importance map we need to keep a few things in mind. No model is able to properly evaluate the relative importance of features if they have high correlation, which our dataset seems to have a lot of. Also, categorical features which have many possible values also tend to be over-evaluated too.\n\nWith that in mind, where do we draw the line? There's no rule of thumb: we can make our own assumptions and run a few iterations to see what suit us best:\n\n1. First and foremost, **overtime** seems to be the most important feature, and any good model we build will have to include it.\n2. **Martial status** is also very important, and it's important to keep both **maried** and **single** features, as they also serve to indicate a divorced status, which may be an important factor.\n3. We get to a lot of **job roles** features with high impact, which are likely boosted in importance due to its variety. This is made more obvious due to the fact that **ALL** job roles are very similarly ranked in relative importance. We need to reduce the number of features and keep only what is crucial information; the easiest way to achieve that with the current data  is to remove the department information and keep the job roles. \n4. We should keep the gender; not only it had a good score but we also spotted a few interesting behaviors at the [data analysis section](#Data-Analysis).\n5. We should keep some of the top numerical features for now. Even though they didn't show high impact, it may be due to the boosted importance of categorical fields. We'll experiment with the most impactful ones: **YearsWithCurrManager**, **YearsSinceLastPromotion** and **YearsAtCompany**.\n\nEverything else will be cut for now, following the vertical blue line that was drawn in the plot above.\n"},{"metadata":{"_cell_guid":"5a7b9d3c-31cb-42c8-b195-f882dea12b47","_uuid":"9c1a62ba783b06a03f967546aa459abcfd25006d"},"cell_type":"markdown","source":"### Building the new feature dataframe"},{"metadata":{"_cell_guid":"f5fc30d4-e0cf-447d-8176-a6a96387372e","_uuid":"840a7677226d640121a7e678170f6ce96c5466bc"},"cell_type":"markdown","source":"In order to build our new feature map, we need to manipulate the features dataframe we built [previously](#feature_map) and *filter* only the desirable inputs:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:51.47322Z","start_time":"2018-03-28T00:32:51.455215Z"},"collapsed":true,"_cell_guid":"c2780d2b-6097-4ba1-ba64-6a071c3769f6","_uuid":"548622e34d865a3f01ae10a2b0999c712e347bd4","trusted":false},"cell_type":"code","source":"new_features = features.filter(['OverTime_Yes', 'MaritalStatus_Single', 'MaritalStatus_Married', 'JobRole_Sales Representative', 'JobRole_Sales Executive', 'JobRole_Research Scientist', 'JobRole_Research Director', 'JobRole_Manufacturing Director', 'JobRole_Manager', 'JobRole_Laboratory Technician', 'JobRole_Human Resources', 'Gender_Male', 'YearsWithCurrManager', 'YearsSinceLastPromotion', 'YearsAtCompany'])\nnew_features.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7832d158-d4d2-447e-8b99-7d7773575699","_uuid":"24ee928be01bc403070bbcc62b7626309dd65387"},"cell_type":"markdown","source":"The next step is to build a new train-step data split in order to train new models:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:51.488235Z","start_time":"2018-03-28T00:32:51.474219Z"},"collapsed":true,"_cell_guid":"e60570c5-a5af-423d-b783-6b7acc33a72c","_uuid":"c7fa8ae94e6b20285c25c4d4d773a73114e7a4fe","trusted":false},"cell_type":"code","source":"features_train_new, features_test_new, attrition_train_new, attrition_test_new = train_test_split(new_features, output, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"35739d2f-828e-4b50-bf6e-14467d35118b","_uuid":"ae06101e210a5f15e7307c1eb68f7af1a8e346c1"},"cell_type":"markdown","source":"And that's it, we have our new training/test data! We can now re-train our model and see what we get from it."},{"metadata":{"_cell_guid":"8a99e1b9-85e2-4b28-abcb-b78a42f9a843","_uuid":"8571302367dc032200bc863c81145d8611440f5c"},"cell_type":"markdown","source":"### Retraining the Random Forest classifier"},{"metadata":{"_cell_guid":"f02f5988-5bf0-440d-89fb-aaba003387de","_uuid":"69fdd1a3b41870e6d6d7850c0ac7d6c368782531"},"cell_type":"markdown","source":"With the the new training/test data available, we can re-visit the random forest we built and see if we get better results. We still won't focus much on the hyper-parameters, we just want to have a rough idea if we're going into the right direction."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:52.586523Z","start_time":"2018-03-28T00:32:51.489223Z"},"collapsed":true,"_cell_guid":"d977e9e1-a95a-4732-9941-ab5c0c1fa213","_uuid":"843045094972f9435ef8019c14b1ae620326b28d","trusted":false},"cell_type":"code","source":"# Build the new classifier\nrandom_forest_new = RandomForestClassifier(n_estimators = 800, criterion = 'entropy', random_state = 0)\nrandom_forest_new.fit(features_train_new, attrition_train_new)\n\n# Get the prediction array, \nattrition_pred_new = random_forest_new.predict(features_test_new)\n\n# Get the accuracy %\nprint(\"Accuracy: \" + str(accuracy_score(attrition_test_new, attrition_pred_new) * 100) + \"%\") ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c0967e8-515b-4ba8-905a-6d81bef00b9d","_uuid":"14db0fc77464e3841d1e77a36470d905734707f5"},"cell_type":"markdown","source":"From a first glance, our model actually got worst! Before we had a precision of 86%, and now only 81%. However, we should take a look at the confusion matrix before jumping into conclusions:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:52.785557Z","start_time":"2018-03-28T00:32:52.587506Z"},"collapsed":true,"_cell_guid":"5e87a340-4991-4e6f-8968-cf53282d06d9","_uuid":"2d34327b6b6d6b4042388be664d2717e1ad5cc36","trusted":false},"cell_type":"code","source":"# Define a set of graphs, 1 by 2, to show the previous and the new confusion matrix side by side\nf, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=False, sharey=False)\nplt.figure(figsize = (6,5))\n\n# building a graph to show the old confusion matrix results\nsns.heatmap(rf_cm_plot, annot=True, vmin=5, vmax=90.5, cbar=False, fmt='g', ax=axes[0])\naxes[0].set( title = 'Random Forest CM (All features)')\n\n# Making the new Confusion Matrix\nrf_cm_new = confusion_matrix(attrition_test_new, attrition_pred_new)\nrf_cm_new_plot = pd.DataFrame(rf_cm_new, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                  columns = [i for i in {\"No attrition\", \"Attrition\"}])\n\n# building a graph to show the new confusion matrix results\nsns.heatmap(rf_cm_new_plot, annot=True, vmin=10, vmax=90.5, cbar=False, fmt='g', ax=axes[1])\naxes[1].set( title = 'Random Forest CM (Selected features)')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c4db32a-ba61-410e-9b72-8a00d8e3d2ea","_uuid":"3ba4e4d99a56afbb0ba0a0f92b3b5c032419f281"},"cell_type":"markdown","source":"Note that we had a worst score in the \"no attrition\" identification, which means our model would flag employees as trouble makers in potential when they are not. This is, however, not as severe as failing to identify someone as a potential workplace attrition source. In that regard, we have effectively improved our model from **17%** to **25.7%** accuracy, but that's still far from ideal.\n\nWe can now take a new look at the feature relevant from our new feature sub-set:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:53.04341Z","start_time":"2018-03-28T00:32:52.786558Z"},"collapsed":true,"_cell_guid":"40d11127-59c2-4917-9c0f-ba7c0c8f5c30","_uuid":"3a419c249b7982a82e99308ea30f0749f70142b1","trusted":false},"cell_type":"code","source":"plot_feature_importances(random_forest_new.feature_importances_, new_features)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b3638a6-25c1-476c-8801-14941e1cd031","_uuid":"25a071af4b18f53d7db33174e3efb84d2fb80847"},"cell_type":"markdown","source":"Removing many features has shown that overtime was, perhaps, wrongly evaluated (or that we removed some key features that overtime required to show its impact). The plot above shows that employees that are stuck in the same position for a long time (company time, time since last promotion and time with current manager) are more likely to cause trouble, which makes sense; most people are not happy with the same routine for long periods of time.\n\nThe changes we made also show that, perhaps, the job role or the department aren't very important features, but this will be made clear when we build the new models. More work could be done here, and more hypothesis could be drawn; however, it's time to experiment with different models and fine-tune them, and see what we can get."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-27T23:29:33.92317Z","start_time":"2018-03-27T23:20:46.702Z"},"_cell_guid":"f114b8dc-af17-4da9-905c-5f168360fdf0","_uuid":"861e20a25bd273ab8fd9846db70d248629f276d9"},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"_cell_guid":"4e00f11c-9763-4661-b400-ccacebc10fac","_uuid":"0eb6a1b97d33ff02908fc1a56024e3d73c465428"},"cell_type":"markdown","source":"A few models we are going to build do not work well with __[un-normalized data](https://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/)__, so we're also going to generate scalled verions of the feature map:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:53.051417Z","start_time":"2018-03-28T00:32:53.044397Z"},"collapsed":true,"_cell_guid":"0339e235-da4c-49c0-a185-de04ce1ef962","_uuid":"d6a9dffcaccd514e894f38b99776bdbbe91a8492","trusted":false},"cell_type":"code","source":"sc = StandardScaler()\nfeatures_train_scaled = sc.fit_transform(features_train)\nfeatures_test_scaled = sc.transform(features_test)\nfeatures_train_new_scaled = sc.fit_transform(features_train_new)\nfeatures_test_new_scaled = sc.transform(features_test_new)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e2c9bbf-cb63-4d14-8e71-ba59f1586360","_uuid":"b68d9e44211d31fab3833514d07709b3a433ae89"},"cell_type":"markdown","source":"# 5. Building various models"},{"metadata":{"_cell_guid":"3e1c5d52-f6d2-4876-bdf1-2877977a30c4","_uuid":"72f585e42d2db71c198ddb648fe102ddf0a82ede"},"cell_type":"markdown","source":"On this section we'll be experimenting with many well-known classification models:\n* K-Nearest Neighbors (K-NN)\n* Kernel SVM\n* Naive Bayes\n* Random forest\n\n\nWe'll start by building the models with their default hyper-parameters, then we'll fine-tune each with __[grid search](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781783555130/6/ch06lvl1sec40/fine-tuning-machine-learning-models-via-grid-search)__ and compare the results.  "},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T17:14:09.838228Z","start_time":"2018-03-25T17:14:09.834228Z"},"_cell_guid":"1b0e157c-b0fe-44f2-ae17-843c2cc36861","_uuid":"8e92e3faf6b171fb766ec55514b1a8013811d292"},"cell_type":"markdown","source":"### Grid Search"},{"metadata":{"_cell_guid":"89b88d7c-e94c-4780-b3bf-db4a4492e48f","_uuid":"ba8d9656208359e7256755215cec266517e12280"},"cell_type":"markdown","source":"Grid search is a brute-force search algorithm used to find the best hyper-parameter combination for a given problem. The __[sklearn module](http://scikit-learn.org/stable/modules/grid_search.html)__ has a nice implementation of it with a built in __[K-fold cross-validation](https://www.cs.cmu.edu/~schneide/tut5/node42.html)__, which builds smaller models with \"K\" portions of the data set, better evaluating how well a model generalizes. In this study, we'll use K=5.\n\nHowever, we still need to pick what hyper-parameters we will play with and within what range, as it would be prohibitively expensive to attempt all possible combinations (and that's not how the algorithm works). From it, we expect to find what is the best combination of hyper-parameters for each model and, at the end, hopefully we'll end up with a model that has good performance for this problem.\n\nAs seen before, we have quite the unbalanced dataset, which means any model we build will achieve high accuracy while getting almost all \"attrition\" cases wrong. The reason for this is that a model may look at the data and decide that the best thing to do is to always predict “no attrition” and achieve high accuracy. __[There are several ways to avoid this](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)__, one of which is to apply smarter cross-validation scoring.\n\nThe chosen metric for this study is the **F1 score**, a weighted average of precision and recall. Recall is a metric of a model completeness, that is, the number of positive predictions divided by the number of positive class values in the test data. \n\nSince we'll be building quite a few grid search models with confusion matrix plots, it's better if we define some helper methods:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:53.291503Z","start_time":"2018-03-28T00:32:53.0534Z"},"collapsed":true,"_cell_guid":"6667c621-e923-41a8-9a95-300d308df907","_uuid":"6e5c73e796551a5c6c7e0f03617e25c0c4b73436","trusted":false},"cell_type":"code","source":"def plot_two_confusion_matrix(cm_1, title_1, cm_2, title_2):\n    # Define a set of graphs, 1 by 2, to show the previous and the new confusion matrix side by side\n    f, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=False, sharey=False)\n    plt.figure(figsize = (6,5))\n\n    # Builds the first CM plot\n    cm_1_plot = pd.DataFrame(cm_1, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                      columns = [i for i in {\"No attrition\", \"Attrition\"}])\n    sns.heatmap(cm_1_plot, annot=True, vmin=5, vmax=90.5, cbar=False, fmt='g', ax=axes[0])\n    axes[0].set(title = title_1)\n\n    # Builds the second CM plot\n    cm_2_plot = pd.DataFrame(cm_2, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                      columns = [i for i in {\"No attrition\", \"Attrition\"}])\n    sns.heatmap(cm_2_plot, annot=True, vmin=10, vmax=90.5, cbar=False, fmt='g', ax=axes[1])\n    axes[1].set( title = title_2)\n\n# We could have built a generic method to plot N number of confusion matrix, but we won't need it for this notebook\ndef plot_four_confusion_matrix(cm_1, title_1, cm_2, title_2, cm_3, title_3, cm_4, title_4):\n    # Define a set of graphs, 2 by 2, to show the previous and the new confusion matrix side by side\n    f, axes = plt.subplots(2, 2, figsize=(12, 12), sharex=False, sharey=False)\n    plt.figure(figsize = (6,5))\n\n    # Builds the first CM plot\n    cm_1_plot = pd.DataFrame(cm_1, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                      columns = [i for i in {\"No attrition\", \"Attrition\"}])\n    sns.heatmap(cm_1_plot, annot=True, vmin=5, vmax=90.5, cbar=False, fmt='g', ax=axes[0,0])\n    axes[0,0].set(title = title_1)\n\n    # Builds the second CM plot\n    cm_2_plot = pd.DataFrame(cm_2, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                      columns = [i for i in {\"No attrition\", \"Attrition\"}])\n    sns.heatmap(cm_2_plot, annot=True, vmin=10, vmax=90.5, cbar=False, fmt='g', ax=axes[0,1])\n    axes[0,1].set( title = title_2)\n\n    # Builds the third CM plot\n    cm_3_plot = pd.DataFrame(cm_3, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                      columns = [i for i in {\"No attrition\", \"Attrition\"}])\n    sns.heatmap(cm_3_plot, annot=True, vmin=10, vmax=90.5, cbar=False, fmt='g', ax=axes[1,0])\n    axes[1,0].set( title = title_3)\n\n    # Builds the fourth CM plot\n    cm_4_plot = pd.DataFrame(cm_4, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                      columns = [i for i in {\"No attrition\", \"Attrition\"}])\n    sns.heatmap(cm_4_plot, annot=True, vmin=10, vmax=90.5, cbar=False, fmt='g', ax=axes[1,1])\n    axes[1,1].set( title = title_4)\n    \n# Fit the classifier, get the prediction array and print the accuracy\ndef fit_and_pred_classifier(classifier, X_train, X_test, y_train, y_test):\n    # Fit the classifier to the training data\n    classifier.fit(X_train, y_train)\n\n    # Get the prediction array\n    y_pred = classifier.predict(X_test)\n    \n    # Get the accuracy %\n    print(\"Accuracy with selected features: \" + str(accuracy_score(y_test, y_pred) * 100) + \"%\") \n    \n    return y_pred\n\n# Run grid search, get the prediction array and print the accuracy and best combination\ndef fit_and_pred_grid_classifier(classifier, param_grid, X_train, X_test, y_train, y_test, scoring = \"f1\", folds = 5):\n    # Apply grid search with F1 Score to help balance the results (avoid bias on \"no attrition\")\n    grid_search = GridSearchCV(estimator = classifier, param_grid = param_grid, cv = folds, scoring = scoring, n_jobs = -1, verbose = 0)\n    grid_search.fit(X_train, y_train)\n    best_accuracy = grid_search.best_score_\n    best_parameters = grid_search.best_params_\n\n    # Get the prediction array\n    grid_search_pred = grid_search.predict(X_test)\n\n    # Print the accuracy and best parameter combination\n    print(scoring + \" score: \" + str(best_accuracy * 100) + \"%\")\n    print(\"Accuracy: \" + str(accuracy_score(y_test, grid_search_pred) * 100) + \"%\") \n    print(\"Best parameter combination: \" + str(best_parameters)) \n    \n    return grid_search_pred, grid_search_pred","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T17:11:15.77294Z","start_time":"2018-03-25T17:11:15.769939Z"},"_cell_guid":"90129dc7-bfef-4d72-8cc2-8ef822177b55","_uuid":"a70dbfe7ffa10f47fafee33f98cb72a99868997e"},"cell_type":"markdown","source":"### K-Nearest Neighbors (K-NN)"},{"metadata":{"_cell_guid":"318515c9-63f0-4089-9843-f6d21c2f0b3f","_uuid":"96c3ce31d063dffb73e50a890a101a5f0a89ecc8"},"cell_type":"markdown","source":"The K-Nearest Neighbors algorithm is __[non-parametric](http://www.statisticshowto.com/parametric-and-non-parametric-data/)__, that is, it doesn't make any assumptions on the underlying data-distribution. It operates by grouping elements together based on the K nearest neighbors, where K (e.g. K=5) and method used to calculate the distance (e.g. Euclidean) are configurable. Similarly to SVM, KNN does not work well with un-normalized data, so we'll use scaled features. The first model we're going to build will all the features, with K=5:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:53.347477Z","start_time":"2018-03-28T00:32:53.292462Z"},"collapsed":true,"_cell_guid":"305678d1-745e-471e-b23f-a4b1d12a1fd3","_uuid":"3a0880d66f33f1b81a742164b60baf8b133fd455","trusted":false},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n\n# Get the prediction array\nknn_pred = fit_and_pred_classifier(knn, features_train_scaled, features_test_scaled, attrition_train, attrition_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5a864ac9-88e4-4372-930f-74ba48c2c364","collapsed":true,"_uuid":"d9beb544e1d9113bea8b2399888063d063e35696"},"cell_type":"markdown","source":"Let's now try it out with the selected group of features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:53.364497Z","start_time":"2018-03-28T00:32:53.348477Z"},"collapsed":true,"_cell_guid":"356b5763-c8b0-446c-8481-1bfc53ca0d05","_uuid":"638779d0859ac0cc631544acf98bc476f5f5b6e1","trusted":false},"cell_type":"code","source":"knn_new = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n\n# Get the prediction array\nknn_pred_new = fit_and_pred_classifier(knn_new, features_train_new_scaled, features_test_new_scaled, attrition_train_new, attrition_test_new)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"328176b8-53f9-40d3-8fac-13db88674700","collapsed":true,"_uuid":"26bf52d3df8e40ef59c2ebb2ea17cf611f99af8a"},"cell_type":"markdown","source":"We apparently got better results with the selected group of features, but we can't be too sure just yet without analyzing the confusion matrix. Now that we built two simple K-NN models, it's time to apply grid search. These are the hyper-parameters we will cover:\n    \n* n_neighbors = number neighbors \n* weights = weight function used on prediction\n* algorithm = algorithm used to compute the nearest neighbor\n* metrics = the distance metric to use for the tree. \n* p = Power parameter for the Minkowski metric. p=1: manhattan_distance, p=2: euclidean_distance\n\nThe following range of values were chosen:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:32:53.372483Z","start_time":"2018-03-28T00:32:53.36548Z"},"collapsed":true,"_cell_guid":"aa8bcfcd-bf8e-4e37-af10-6f2eb49e3b13","_uuid":"43a68044267ff8e6b7e88e0cb311c112a070a135","trusted":false},"cell_type":"code","source":"param_grid = {\n    'n_neighbors': [1,2,4,5],\n    'weights': ['distance', 'uniform'],\n    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n    'metric': ['minkowski','euclidean','manhattan'], \n    'p': [1, 2]\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e4c5d89-4fc5-4949-9220-2196cbbd10d6","collapsed":true,"_uuid":"0c0b752f59e6ad3624304ef61be0d5a9fb21d10e"},"cell_type":"markdown","source":"With the grid parameters built, we can now perform grid search on our KNN models, starting with all the features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:01.716371Z","start_time":"2018-03-28T00:32:53.373483Z"},"collapsed":true,"_cell_guid":"ce2b73f0-f690-4436-ad48-931d6a2a4b87","_uuid":"2e48cb02cea84effec21e59a61c4b19ab1e5af14","trusted":false},"cell_type":"code","source":"# Run grid search, print the results and get the prediction array and model\nknn_grid_search_pred, knn_grid = fit_and_pred_grid_classifier(knn, param_grid, features_train_scaled, features_test_scaled, attrition_train, attrition_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b109c49-650c-4b7f-97aa-22c8b0a8a2b8","_uuid":"4e3295c791ea1f2525cb8a1daef7fa25be6c8977"},"cell_type":"markdown","source":"Now let's run grid search on a knn model built with the selected group of features:"},{"metadata":{"code_folding":[],"ExecuteTime":{"end_time":"2018-03-28T00:33:06.210031Z","start_time":"2018-03-28T00:33:01.717373Z"},"collapsed":true,"_cell_guid":"6e09b720-0542-4ac9-baeb-50b4d77f2cb8","_uuid":"269119669144da78876b74d32fae3ba7d5ab41e5","trusted":false},"cell_type":"code","source":"# Run grid search, print the results and get the prediction array and model\nknn_grid_search_pred_new, knn_grid_new = fit_and_pred_grid_classifier(knn_new, param_grid, features_train_new_scaled, features_test_new_scaled, attrition_train_new, attrition_test_new)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9d7d222-f695-4120-8dc4-d68eacdce3b9","_uuid":"5f0dac40385726a0f73ff8cd746c3051a70d2672"},"cell_type":"markdown","source":"We got slightly better results with the selected group of features, but our *F1 score* is still a bit low (41%). Let us now build the confusion matrix and analyze the results:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:06.471107Z","start_time":"2018-03-28T00:33:06.211032Z"},"collapsed":true,"_cell_guid":"06e36bee-e0f5-425c-953f-decc1db57809","_uuid":"9c8cf03c79db7e8bb02cca6e733b0f691cfd5257","trusted":false},"cell_type":"code","source":"# Build the Confusion Matrix with all features\nknn_cm = confusion_matrix(attrition_test, knn_pred)\n\n# Build the Confusion Matrix with the selected features\nknn_cm_new = confusion_matrix(attrition_test_new, knn_pred_new)\n\n# Build the Confusion Matrix with all features and grid search\nknn_grid_cm = confusion_matrix(attrition_test, knn_grid_search_pred)\n\n# Build the Confusion Matrix with the selected features and grid search\nknn_grid_cm_new = confusion_matrix(attrition_test_new, knn_grid_search_pred_new)\n\n# Plot the four Coufusion Matrix\nplot_four_confusion_matrix(knn_cm, 'K-NN (All features)', \n                           knn_cm_new, 'K-NN (Selected Features)',\n                           knn_grid_cm, 'K-NN (Grid Search + All features)', \n                           knn_grid_cm_new, 'K-NN (Grid Search + Selected Features)')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2f569f60-be24-42f9-939d-f9b0867369e6","_uuid":"d83ea5a70f3a7404bdf1b7cf63d1921447c14dfc"},"cell_type":"markdown","source":"K-NN performed very similarly to random forest, not showing much promise in terms of attrition prediction, while showing overall good accuracy. K-NN doesn't seem like a very promising model for this problem."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T17:11:15.77294Z","start_time":"2018-03-25T17:11:15.769939Z"},"_cell_guid":"143e6912-e018-4a2d-a900-2fe8720d1d9b","_uuid":"1a23c66f0c42209814753c8c116941fccc7d5432"},"cell_type":"markdown","source":"### Kernel SVM"},{"metadata":{"_cell_guid":"8b13787c-b57b-47e0-948f-6ffceb57e027","_uuid":"28985dbe3e821a4687d7e0b7f1aa058744a51aac"},"cell_type":"markdown","source":"Kernel SVM allows us to map the data to a higher dimension, allowing non-linear problems to be linearly separable (SVM). It's very popular due to its flexibility and performance.\n\nAs mentioned before, SVM does not work well with un-normalized data, so we're going to use scaled features. We start by building a very simple Kernel SVM model. For starters, I've chosen a Gaussian (rbf) kernel, which is a pretty good initial setup:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:06.523118Z","start_time":"2018-03-28T00:33:06.473106Z"},"collapsed":true,"_cell_guid":"89e81f6d-cf94-499c-9676-56a9ed675edc","_uuid":"dc9483d17f4c0835d243bf8435f4c90cd6250705","trusted":false},"cell_type":"code","source":"# Build the model and fit it to the training data\nsvc = SVC(kernel = 'rbf', random_state = 0)\n\n# Get the prediction array\nsvc_pred = fit_and_pred_classifier(svc, features_train_scaled, features_test_scaled, attrition_train, attrition_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"27e64a28-31ea-4bc4-9b5c-ae2b08b8052f","_uuid":"2c8e4c60960c31d9b355e9fa42bceec73cf1a9d7"},"cell_type":"markdown","source":"We got an accuracy of 87.3%, which is the highest we've seen so far! Let's do the same now for the selected group of features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:06.552126Z","start_time":"2018-03-28T00:33:06.524118Z"},"collapsed":true,"_cell_guid":"f6a8e389-0cf0-454a-a53b-9741d76aecf9","_uuid":"7b7390ab8cf24fa5b1401fd84e57d9d104ad53d7","trusted":false},"cell_type":"code","source":"# Build the model and fit it to the training data\nsvc_new = SVC(kernel = 'rbf', random_state = 0)\n\n# Get the prediction array\nsvc_pred_new = fit_and_pred_classifier(svc_new, features_train_new_scaled, features_test_new_scaled, attrition_train_new, attrition_test_new)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7874f292-7c93-4972-ab38-2dd87aa1a1c9","_uuid":"84ed6cb4257e7aa55e901cc48e69c253a2e8268b"},"cell_type":"markdown","source":"We got an accuracy of almost 84%, which at first glance seems worst than the Kernel SVM model with all the features, but we can't be too sure until looking at the Confusion Matrix. Now, let's see what we can achieve with a grid search, we are going to cover the following hyper-parameters:\n* kernel = Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable\n* gamma = Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n* C = Penalty parameter C of the error term.\n\nThe following range of values were chosen:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:06.560127Z","start_time":"2018-03-28T00:33:06.554127Z"},"collapsed":true,"_cell_guid":"c7315af6-835b-4455-97e1-d3d88808af09","_uuid":"fb1824e626108af455d23c0a0af409b636977d39","trusted":false},"cell_type":"code","source":"param_grid = [\n    {\n        'C': [1, 2, 4, 5], \n        'kernel': ['linear']\n    }, \n    {\n        'C': [10, 11, 12, 13, 14, 15], \n        'kernel': ['rbf', 'sigmoid'], \n        'gamma': [0.1, 0.2, 0.3, 0.4, 0.5]\n    },\n]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-26T14:15:54.726604Z","start_time":"2018-03-26T14:15:25.183Z"},"_cell_guid":"73f543fb-425a-484c-9a70-d1867d941860","_uuid":"23548bfc0448e203adaaecbb8e74e9d105980008"},"cell_type":"markdown","source":" With the grid parameters built, we can now try it out on our Kernel SVM classifier with all the features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:13.724196Z","start_time":"2018-03-28T00:33:06.561128Z"},"collapsed":true,"_cell_guid":"ffb1c747-0849-4094-ba49-4448140e2084","_uuid":"d93f6836f7d5fd3467d8d6ff8c5665ea20186ce5","trusted":false},"cell_type":"code","source":"# Run grid search, print the results and get the prediction array and model\nsvc_grid_search_pred, svc_grid = fit_and_pred_grid_classifier(svc, param_grid, features_train_scaled, features_test_scaled, attrition_train, attrition_test, scoring = \"recall\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"398903fe-eb76-4ac9-86cd-63209b9a957d","_uuid":"9d2f323a11cb4b83a26823db0edcd88ef64d3382"},"cell_type":"markdown","source":"Interestingly enough the best result out of 920 combinations was a simple line (while also being the best overall performance so far from all models we've built). KISS, I guess. We should now run grid search with the Kernel SVM model built with the selected group of features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:17.176161Z","start_time":"2018-03-28T00:33:13.726185Z"},"collapsed":true,"_cell_guid":"fe28d4ca-e350-48fa-96b4-02ee3d47d211","_uuid":"83e9a6ba9af29da7d6868cd7c4993736d92985eb","trusted":false},"cell_type":"code","source":"# Run grid search, print the results and get the prediction array and model\nsvc_grid_search_pred_new, svc_grid_new = fit_and_pred_grid_classifier(svc, param_grid, features_train_new_scaled, features_test_new_scaled, attrition_train_new, attrition_test_new, scoring = \"recall\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f997ce79-33ab-4c7a-b1a8-d011c5ec8ff3","_uuid":"741d96767bade01b5b3dd673b62feb77081d4ce3"},"cell_type":"markdown","source":"With an accuracy rating of 88.88% and 82.7%, Kernel SVM is showing good promise. Let us now compare the confusion matrix of all SVC models:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:17.439246Z","start_time":"2018-03-28T00:33:17.177162Z"},"collapsed":true,"_cell_guid":"0b8f3d2c-3462-4dcf-a45e-0f404899d208","_uuid":"d9e732eb9c0ae0f607979fc56bd32dcf96758025","trusted":false},"cell_type":"code","source":"# Build the Confusion Matrix with all features\nsvc_cm = confusion_matrix(attrition_test, svc_pred)\n\n# Build the Confusion Matrix with the selected features\nsvc_cm_new = confusion_matrix(attrition_test_new, svc_pred_new)\n\n# Build the Confusion Matrix with all features and grid search\nsvc_grid_cm = confusion_matrix(attrition_test, svc_grid_search_pred)\n\n# Build the Confusion Matrix with the selected features and grid search\nsvc_grid_cm_new = confusion_matrix(attrition_test_new, svc_grid_search_pred_new)\n\n# Plot the four Coufusion Matrix\nplot_four_confusion_matrix(svc_cm, 'Kernel SVM (All features)', \n                           svc_cm_new, 'Kernel SVM (Selected Features)',\n                           svc_grid_cm, 'Kernel SVM (Grid Search + All features)', \n                           svc_grid_cm_new, 'Kernel SVM (Grid Search + Selected Features)')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4b073f7-ed2f-494e-bb9b-eaf1528e6ef9","_uuid":"63701b78256cfc6245c4d54a118983708d614610"},"cell_type":"markdown","source":"Linear SVM with all the features got the best overall accuracy so far. However, it's still an underwhelming \"attrition\" prediction. That being said, it may still be useful to us when building our own ensemble."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T17:11:15.77294Z","start_time":"2018-03-25T17:11:15.769939Z"},"_cell_guid":"9e0a19cf-5952-45eb-bb17-dd25a1e0203c","_uuid":"f1e32a2580f19b212022350aeaf0422cad4c038f"},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"_cell_guid":"a12ebf8b-e02d-4b1b-ace6-0d528dff3a88","_uuid":"4ba058fa1afb34fb460954f395a2dfc6f61d225d"},"cell_type":"markdown","source":"Naive Bayes is a family of algorithms that all share a common principle: every feature is independent from each other. For example, a fruit may be considered an apple if it is red, round, and about 3″ in diameter. A Naive Bayes classifier considers each of these features to contribute independently to the prediction, regardless of any correlations between features. This is, of course, almost never true, which means that the applications of this model are very situational. However, it often gives good results, showing better results than other models that do not have their hyper-parameters fine-tuned.\n\nIt's a pretty simple model, and as such it doesn't benefit from grid search whatsoever. With that in mind, let's start by building a model with the all the features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:17.448248Z","start_time":"2018-03-28T00:33:17.440247Z"},"collapsed":true,"_cell_guid":"f7696057-4f17-4439-af32-a9214e19b189","_uuid":"a06cad8b4a9eb68c6e4e56acdf320a88b70da276","trusted":false},"cell_type":"code","source":"# Build the Naive Bayes classifier with all the features\nnaive_bayes = GaussianNB()\n\n# Fit the model and get the prediction array\nattrition_pred_nb = fit_and_pred_classifier(naive_bayes, features_train, features_test, attrition_train, attrition_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"301e9e4c-6db1-40be-b211-e80c5177623d","_uuid":"8ff8154e3b83583dd60cbf0a8600b1ebf5d7e25e"},"cell_type":"markdown","source":"The accuracy of the Naive Bayes model is just *ok* for this problem, which is expected since our features do not share the same statistical relevance, which is one of the base foundations for this model to perform well. We should now take a look at how it performs with the selected features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:17.464237Z","start_time":"2018-03-28T00:33:17.450233Z"},"collapsed":true,"_cell_guid":"cfb8ed20-f50e-49a0-8ac9-31f3032cb240","_uuid":"064ebeb8e34dc176ea3d6a353a4c5829b7c62197","trusted":false},"cell_type":"code","source":"# Build the Naive Bayes classifier with the selected features\nnaive_bayes_new = GaussianNB()\n\n# Fit the model and get the prediction array\nattrition_pred_nb_new = fit_and_pred_classifier(naive_bayes_new, features_train_new, features_test_new, attrition_train_new, attrition_test_new)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ada748c4-1ec3-4d30-9522-437e08bc647b","_uuid":"f4665060a30c2fff1d183835c226ab4084c42f74"},"cell_type":"markdown","source":"We got worst performance by using only the selected features, which is explained by the fact that we removed some statistically independent variables in favor of obtaining a cleaner model and improving our attrition prediction. Let's us now take a look at the confusion matrix:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:17.607283Z","start_time":"2018-03-28T00:33:17.465236Z"},"collapsed":true,"_cell_guid":"cfecee47-7211-4d77-b92a-2da021a77102","_uuid":"b9f5ed2e76cfbb8f779cd97f4c2196ecbbf7f4a6","trusted":false},"cell_type":"code","source":"# Build the Confusion Matrix with all features\nnb_cm = confusion_matrix(attrition_test, attrition_pred_nb)\n\n# Build the Confusion Matrix with the selected features\nnb_new_cm = confusion_matrix(attrition_test_new, attrition_pred_nb_new)\n\n# Plot all Confusion Matrix\nplot_two_confusion_matrix(nb_cm, 'Naive Bayes CM (All features)', nb_new_cm, 'Naive Bayes CM (Selected Features)')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"50887e58-d6f6-4d52-843d-31a2a0017b06","_uuid":"37acdabdb3204a700f805dc899eecc7df2a949ad"},"cell_type":"markdown","source":"Although Naive Bayes model accuracy isn't stellar when applied to this dataset, we actually got pretty good precision accuracy on attrition when using all the features: **76%**, aligning almost perfectly with its overall accuracy of **75%**.\n\nRunning the model with only the selected features had an even better score in that regard (**83%**), but it has shown too low of an overall prediction accuracy to be a strong contender."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T17:11:15.77294Z","start_time":"2018-03-25T17:11:15.769939Z"},"_cell_guid":"e46245b1-b916-4181-976f-642080986370","_uuid":"2b32b46088f66251ba8e03cc330a2b410f340aec"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"_cell_guid":"887797a2-f5a4-47f0-8ef4-404ae9c837bb","_uuid":"0282b623eec0c450d049ce9b25a7fe9b0d2fa5f5"},"cell_type":"markdown","source":"Since we have already built and tested the random forest classifier (with both the new set of features and all of them), we can now simply jump into the grid search and test out our results. These are the hyper-parameters we will cover:\n    \n* n_estimators = number of trees in the forest\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min number of data points placed in a node before the node is split\n* min_samples_leaf = min number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)\n\nThe following range of values were chosen:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:33:17.612278Z","start_time":"2018-03-28T00:33:17.608273Z"},"collapsed":true,"_cell_guid":"435ecbcc-f675-4a3a-bc98-25897abd003f","_uuid":"be15df3da5c8ccbb2d27162def5c2ef14230247e","trusted":false},"cell_type":"code","source":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [30, 40, 50],\n    'max_features': [6, 7, 8],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [8, 9, 10],\n    'n_estimators': [200, 300, 400, 500]\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39f856d8-9fcb-49df-be2e-465628dec415","_uuid":"4e1bc86b521915ec76d52a7a0f8955d22c5ea617"},"cell_type":"markdown","source":"With the grid parameters built, we can now try it out on our random forest classifier with all the features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:37:39.622954Z","start_time":"2018-03-28T00:33:17.614275Z"},"collapsed":true,"_cell_guid":"4fa04a0e-a4d4-462d-ae12-184d27615105","_uuid":"96231a45ce462e64233f83ac6288fee73f1d3aab","scrolled":true,"trusted":false},"cell_type":"code","source":"# Run grid search, print the results and get the prediction array and model\nrf_grid_search_pred, rf_grid = fit_and_pred_grid_classifier(random_forest, param_grid, features_train, features_test, attrition_train, attrition_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89cd270c-2a13-4b60-8038-9b25905c32d6","_uuid":"a46086488928760e59ce0045d98cdf8907fd64cc"},"cell_type":"markdown","source":"86,16% is not bad, but the F1 Score is underwhelming. Let's now try grid search with the random forest model built with a selected group of features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:11.003749Z","start_time":"2018-03-28T00:37:39.624944Z"},"collapsed":true,"_cell_guid":"b3e3e58d-1595-41c6-a70f-e48f47caef62","_uuid":"eaf81d18e2977f1ac5d213f649487093b72d668a","trusted":false},"cell_type":"code","source":"# Run grid search, print the results and get the prediction array and model\nrf_grid_search_pred_new, rf_grid_new = fit_and_pred_grid_classifier(random_forest, param_grid, features_train_new, features_test_new, attrition_train_new, attrition_test_new)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e35f1c45-d605-42be-b3d6-e40b748ddb30","_uuid":"41d454f315f69c3edb07d3d4c4d682c75e17fc0e"},"cell_type":"markdown","source":"With a select group of features we obtained 84.12% accuracy, slightly worst than the previous model, but with better F1 Score. In order to better evaluate these results, let's take a look at the confusion matrix:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:11.262935Z","start_time":"2018-03-28T00:41:11.005738Z"},"collapsed":true,"_cell_guid":"ec4ff41e-7086-41fb-aea1-e626f7cc97b0","_uuid":"04f2f983468d74e96a2d9b410d6806b1a6cc3ba0","trusted":false},"cell_type":"code","source":"# Build the Confusion Matrix with all features and grid search\nrf_grid_cm = confusion_matrix(attrition_test, rf_grid_search_pred)\n\n# Build the Confusion Matrix with the selected features and grid search\nrf_grid_cm_new = confusion_matrix(attrition_test_new, rf_grid_search_pred_new)\n\n# Plot the four Coufusion Matrix\nplot_four_confusion_matrix(rf_cm_plot, 'Random Forest CM (All features)', \n                           rf_cm_new_plot, 'Random Forest CM (Selected Features)',\n                           rf_grid_cm, 'Random Forest CM (Grid Search + All features)', \n                           rf_grid_cm_new, 'Random Forest CM (Grid Search + Selected Features)')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a10ae57-8e20-4f1c-91a6-174bbe812e74","_uuid":"e994d9d6ded5ba37dcc3622dc5c387bb07f6d047"},"cell_type":"markdown","source":"It's evident that the random forest model is not performing well for this problem, and in the end it's hard to choose the best one because they are all sub-par, despite having overall good accuracy. The best one is likely \"Random Forest CM (Selected Features)\", as even though it gets 8% of the \"no attrition\" predictions wrong (worst of all four), it has the best \"attrition\" prediction of the set."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-26T18:11:10.722231Z","start_time":"2018-03-26T18:10:00.373Z"},"_cell_guid":"f09418da-b951-41d5-8318-ffbfdd1cf31d","_uuid":"a520edc93d21dc4a262aede03347c998389f99b0"},"cell_type":"markdown","source":"### Custom-made ensemble"},{"metadata":{"_cell_guid":"d3547673-c4a9-49a5-bc05-64f8d5994785","_uuid":"8adf2a2e66883daa7ab75ff43c7dcab1ef5d4a21"},"cell_type":"markdown","source":"Even though a few models gave us quite good predictions (e.g. 88,88% overall accuracy with Kernel SVM by using all features, and Naive Bayes with the good attrition prediction), we could still go an extra mile and combine a few of the models implemented to make use of their strongest traits as a team. The combination of many models is called an ensemble. We'll attempt the following two combinations:\n* Emsemble 1:\n    1. Naive Bayes (**all features**) with 0.5 weight\n    2. K-NN (**grid search** + **all features**) with 0.1 weight\n    3. Random Forest (**grid search** + **all features**) with 0.3 weight\n    \n    \n* Emsemble 2:\n    1. Naive Bayes (**selected features**) with 0.5 weight\n    2. K-NN (**selected features**) with 0.2 weight\n    3. Random Forest (**grid search** + **selected** features) with 0.3 weight\n\nNaive Bayes was chosen as the base classifier (highest weight) due to its high accuracy on the attrition prediction. With that, we hope to create a model that can have high overall accuracy and also be a good model for predicting attrition.\n\nTo start, we need to build our classifiers (except by Naive Bayes, since it's already built) with the optimal parameters found by grid search, as it doesn't work with models built from grid search:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:11.274916Z","start_time":"2018-03-28T00:41:11.263912Z"},"collapsed":true,"_cell_guid":"9ac68aa7-c7f9-472a-8d25-ff73bbee2f2d","_uuid":"4314df2c7088b53cf3baab5b5e1a533d6a631f3d","trusted":false},"cell_type":"code","source":"# Ensemble 1 models\nrf_ens_1 = RandomForestClassifier(max_depth = 30, max_features = 7, min_samples_leaf = 2, min_samples_split = 9, n_estimators = 400, random_state = 0)\nkn_ens_1 = KNeighborsClassifier(algorithm = 'ball_tree', metric = 'minkowski', n_neighbors = 1, p = 1, weights = 'distance')\n\n# Ensemble 2 models\nrf_ens_2 = RandomForestClassifier(max_depth = 30, max_features = 7, min_samples_leaf = 1, min_samples_split = 9, n_estimators = 300, random_state = 0)\nkn_ens_2 = KNeighborsClassifier(algorithm = 'brute', metric = 'minkowski', n_neighbors = 5, p = 2, weights = 'distance')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fcc24f2f-1b6c-44ea-b81d-1d7498c2ce10","_uuid":"c861e72a20614d769184dbc37fb27aa623ad75c5"},"cell_type":"markdown","source":"With the models built, we need to now define our own methods for building an ensemble. This is needed because the default VotingClassifier function from sklearn does not accept different feature dataframes as input, which we need since Kernel SVM and KNN use feature scaling, while naive bayes and random forest do not. The biggest issue here is that while using normalized data do not affect random forest that much, it has a very negative impact on Naive Bayes when applied."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:11.291938Z","start_time":"2018-03-28T00:41:11.276917Z"},"collapsed":true,"_cell_guid":"fa62f6e9-dc9e-4b4f-a0f1-d677e5f2ff50","_uuid":"77d203d703880e6f004c8ccd5da08a0aa833b80b","trusted":false},"cell_type":"code","source":"# Helper method to fit multiple estimators at once\ndef fit_multiple_estimators(classifiers, X_list, y, sample_weights = None):\n    return  [clf.fit(X, y) if sample_weights is None else clf.fit(X, y, sample_weights) for clf, X in zip([clf for _, clf in classifiers], X_list)]\n\n# Helper method to act as an ensemble\ndef predict_from_multiple_estimator(estimators, X_list, weights = None):\n    # Predict 'soft' voting with probabilities\n    pred1 = np.asarray([clf.predict_proba(X) for clf, X in zip(estimators, X_list)])\n    pred2 = np.average(pred1, axis=0, weights=weights)\n    \n    # Return the prediction\n    return np.argmax(pred2, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60c80f84-012c-4fa9-a58a-f63a12996607","_uuid":"60a9239c0f9660b73f9c0d47a0ec963ea722f16f"},"cell_type":"markdown","source":"The first ensemble is built with models that use all features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:12.009105Z","start_time":"2018-03-28T00:41:11.292921Z"},"collapsed":true,"_cell_guid":"1a94979d-8f71-473f-aae2-10fc454fefd3","_uuid":"1b0bfafe46eeafdd4f62d52da92e14f50ebf8a9a","trusted":false},"cell_type":"code","source":"# Build the estimator list and the applicable feature list in correct order\nclassifiers = [('knn', kn_ens_1), ('rf', rf_ens_1), ('nb', naive_bayes)]\ntrain_list = [features_train_scaled, features_train, features_train]\ntest_list = [features_test_scaled, features_test, features_test]\n\n# Fit the ensemble\nfitted_estimators_1 = fit_multiple_estimators(classifiers, train_list, attrition_train)\n\n# Run a prediction for our ensemble\nensemble_1_pred = predict_from_multiple_estimator(fitted_estimators_1, test_list, weights=[1,2,5])\n\n# Get the accuracy %\nprint(\"Accuracy with selected features: \" + str(accuracy_score(attrition_test, ensemble_1_pred) * 100) + \"%\") ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d11df08e-be7d-4f8c-b37a-201a45ec4ea0","_uuid":"d7fee189ff83b28d37a5583e83b1c1ac61ba1191"},"cell_type":"markdown","source":"The second ensemble is built with models that use a selected group of features:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:12.544243Z","start_time":"2018-03-28T00:41:12.011106Z"},"collapsed":true,"_cell_guid":"98ad64ba-76bf-48f6-a475-3b0fc09beaf8","_uuid":"5ffca57272567277a2e7010ab2e29c12c65930ad","trusted":false},"cell_type":"code","source":"# Build the estimator list and the applicable feature list in correct order\nclassifiers = [('knn', kn_ens_2), ('rf', rf_ens_2), ('nb', naive_bayes)]\ntrain_list = [features_train_scaled, features_train, features_train]\ntest_list = [features_test_scaled, features_test, features_test]\n\n# Fit the ensemble\nfitted_estimators_2 = fit_multiple_estimators(classifiers, train_list, attrition_train)\n\n# Run a prediction for our ensemble\nensemble_2_pred = predict_from_multiple_estimator(fitted_estimators_2, test_list, weights=[2,3,5])\n\n# Get the accuracy %\nprint(\"Accuracy with selected features: \" + str(accuracy_score(attrition_test, ensemble_2_pred) * 100) + \"%\") ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fbc5666-6dd4-4a59-b63c-e1b54e928357","_uuid":"3a9146b83ab5b0ed5d06265effccabea3dd160c6"},"cell_type":"markdown","source":"From a first glance, both models had an average performance, and are apparently no better than the other models we build. Let's take a look at the confusion matrix to see exactly how well it's performing:"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:12.676844Z","start_time":"2018-03-28T00:41:12.546247Z"},"collapsed":true,"_cell_guid":"5db241d2-5ae5-4a6a-8bde-9f830d4cb896","_uuid":"df2addb9a08d912492a47d4df2171cc5a5c59a75","trusted":false},"cell_type":"code","source":"# Build the Confusion Matrix for ensemble 1\nensemble_1_cm = confusion_matrix(attrition_test, ensemble_1_pred)\n\n# Build the Confusion Matrix for ensemble 2\nensemble_2_cm = confusion_matrix(attrition_test_new, ensemble_2_pred)\n\n# Plot the four Coufusion Matrix\nplot_two_confusion_matrix(ensemble_1_cm, 'Ensemble 1 CM',  ensemble_2_cm, 'Ensemble 2 CM')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"431fec2f-4b1a-4fa3-a0d0-e6d1f1f1124f","_uuid":"9b7db96f96971b21fb8390e313ae7a95c53c72fd"},"cell_type":"markdown","source":"We ended up with two pretty similar models in the end, both showing acceptable accuracy (81% and 84% respectively) while having at least 50% attrition prediction, which is better than all models we built but Naive Bayes.\n\nFor the first ensemble I've chosen a bigger weight for Naive Bayes, which resulted in worst overall accuracy in order to score a little bit better on the attrition prediction."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-25T17:11:15.77294Z","start_time":"2018-03-25T17:11:15.769939Z"},"_cell_guid":"f61851de-a8c3-4f37-a691-b886738e8c0c","_uuid":"8e6fda01fcb2bcf74580bd70abd42ee09f2f515e"},"cell_type":"markdown","source":"# 6. Result comparison and final considerations"},{"metadata":{"_cell_guid":"9872b7a1-f918-4c78-8488-27b8942fef04","_uuid":"f24eb2227a4ecbf6e7aecffb63f77cc05708b8c5"},"cell_type":"markdown","source":"We've built a lot of models, some good, some bad. Out of all of them, four performed the best:\n* Linear SVM with all features: **88.88%** overall accuracy, **41%** attrition accuracy\n* Random Forest with selected features: **84%** overall accuracy, **28.5%** attrition accuracy\n* Naive Bayes with all features: **75%** accuracy, **65.7%** attrition accuracy\n* Ensemble 2, which is a combination of three models built with all features: **83.7%** accuracy, **50%** attrition accuracy"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-28T00:41:12.931921Z","start_time":"2018-03-28T00:41:12.677844Z"},"collapsed":true,"_cell_guid":"5b5d1296-3061-4796-933c-f5f9ea84ebdc","_uuid":"b47e4966da9ef24c9db7b92ed98a265e63a90381","trusted":false},"cell_type":"code","source":"plot_four_confusion_matrix(svc_grid_cm, 'Linear SVM (All features)', \n                           rf_grid_cm_new, 'Random Forest (Grid Search + Selected Features)',\n                           nb_cm, 'Naive Bayes (All features)', \n                           ensemble_2_cm, 'Ensemble 2')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4a9718b-03ae-4b8f-a82e-9ec9cee050fc","_uuid":"de8cc7dac17a684796b7799cc82177638042949a"},"cell_type":"markdown","source":"---\n\nTo help answer the question of \"what causes attrition at the workplace?\", we need to re-visit the relative importance we got from Random Forest and contextualize it to the results we obtained:\n\nWith all features:\n\n* Overtime\n* Martial status\n* Working with sales or research\n* Management positions.\n\nWith selected group of features:\n\n* Years at company\n* Years with same manager\n* Years since last promotion\n* Gender\n\nThis hypothesis can be validated through a set of plots:"},{"metadata":{"collapsed":true,"_uuid":"9ffb7cbf102e28a3fde12c987fac71b0f736cb7e","trusted":false},"cell_type":"code","source":"# Add the encoded attrition to the original dataset, to allow it being combined plotted with categorical features\nencoded_dataset = dataset\nencoded_dataset['Attrition'] = output\n\n# Define a set of graphs, 2 by 3, usin the matplotlib library\nf, axes = plt.subplots(2, 3, figsize=(24, 16), sharex=False, sharey=False)\n\nsns.boxplot(x=\"Attrition\", y=\"YearsAtCompany\", data=encoded_dataset, ax=axes[0,0])\naxes[0,0].set( title = 'Years at company against Attrition')\n\nsns.boxplot(x=\"Attrition\", y=\"YearsSinceLastPromotion\", data=encoded_dataset, ax=axes[0,1])\naxes[0,1].set( title = 'Years since last promotion against Attrition')\n\nsns.boxplot(x=\"Attrition\", y=\"YearsWithCurrManager\", data=encoded_dataset, ax=axes[0,2])\naxes[0,2].set( title = 'Years with current manager against Attrition')\n\nsns.violinplot(x=\"Attrition\", y=\"JobRole\", data=encoded_dataset, ax=axes[1,0])\naxes[1,0].set( title = 'Job role against against Attrition')\n\nsns.violinplot(x=\"Attrition\", y=\"Gender\", data=encoded_dataset, ax=axes[1,1])\naxes[1,1].set( title = 'Gender against against Attrition')\n\nsns.violinplot(x=\"Attrition\", y=\"OverTime\", data=encoded_dataset, ax=axes[1,2])\naxes[1,2].set( title = 'Overtime against against Attrition')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e03b080ffdb958ba3e307a4748056bdaa9f6f6e2"},"cell_type":"markdown","source":"The plot above shows that there's a strong relation between attrition and years with same manager, years since last promotion and years at the same company and doing overtime, but gender seems to have no real effect on it. Additionally, sales representative are also more prone to have work attrition."},{"metadata":{"_uuid":"882cf77ef44191e79c39d5ed7fbb1253bbf6af28"},"cell_type":"markdown","source":"We got mixed results with our models, but for the most cases those that were built with all features had better overall prediction accuracy, while having worst attrition accuracy. This mean we can trust that the features we selected are very good indicators of attrition, and one should keep an eye on employees who are stuck in the same situation for long periods of time, specially those that do overtime and/or work with sales. In short, variety is the spice of life, and dealing with people to meet a quota is stressful.\n\nWe're at the end of our journey. Several techniques were evaluated on this study, and there are a few conclusions that can be drawn from this analysis:\n\nThere's either a limit of how well attrition can be predicted with the known features, or feature selection can be improved in order to achieve better results. A future study could be done to focus on feature selection alone, re-running the algorithms built here.\nThe simplest model (Naive Bayes) is probably the most useful one.\nSo, what model to use? I would personally go with Naive Bayes (with all features): even though the overall accuracy is a bit underwhelming (75%), the attrition prediction is too valuable to pass. It's better raise false alarms (depending on how that is conducted) than to miss the change to identify attrition before it occurs."}],"metadata":{"varInspector":{"cols":{"lenType":16,"lenName":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_prefix":"del ","library":"var_list.py","delete_cmd_postfix":"","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_prefix":"rm(","library":"var_list.r","delete_cmd_postfix":") ","varRefreshCmd":"cat(var_dic_list()) "}},"window_display":false,"position":{"width":"800px","left":"144.641px","right":"20px","top":"97px","height":"325px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.6.5","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}