{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"_is_fork":false,"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","nbconvert_exporter":"python","mimetype":"text/x-python"},"_change_revision":0},"cells":[{"metadata":{"_uuid":"f820e501e86640c110f1483323b3a2c0976f8ee3","_cell_guid":"2622a368-95e2-0e19-b41e-26f6294b6dba"},"cell_type":"markdown","source":"**Import dependencies**"},{"metadata":{"_uuid":"98874e97ec1ca9186254563a6af3a38e90e9d032","collapsed":true,"_cell_guid":"6994fa23-50c9-cf3c-7820-b1738f41eade"},"execution_count":null,"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nimport time\nimport collections\nimport numpy as np\nimport random\nimport tensorflow as tf","outputs":[]},{"metadata":{"_uuid":"e3117724ab22548122b5548a4d7773faf8cfa74d","_cell_guid":"8af38afd-300c-23cb-8699-303b699bceb3"},"cell_type":"markdown","source":"**Word2Vec Model**"},{"metadata":{"_uuid":"a5dabb489114008054eaebe4b4e7c4854bdc3d31","collapsed":true,"_cell_guid":"ccf0fd83-eed2-dc90-47b5-a98e9f5124f3"},"execution_count":null,"cell_type":"code","source":"class VecModel:\n    \n    def __init__(self, batch_size, dimension_size, learning_rate, vocabulary_size):\n        \n        self.train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n        self.train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n        \n        # randomly generated initial value for each word dimension, between -1.0 to 1.0\n        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, dimension_size], -1.0, 1.0))\n        \n        # find train_inputs from embeddings\n        embed = tf.nn.embedding_lookup(embeddings, self.train_inputs)\n        \n        # estimation for not normalized dataset\n        self.nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, dimension_size], stddev = 1.0 / np.sqrt(dimension_size)))\n        \n        # each node have their own bias\n        self.nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n        \n        # calculate loss from nce, then calculate mean\n        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights = self.nce_weights, biases = self.nce_biases, labels = self.train_labels,\n                                                  inputs=embed, num_sampled = batch_size / 2, num_classes = vocabulary_size))\n        \n        #for a small neural network, for me, Adam works the best\n        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n        \n        # normalize the data by simply reduce sum\n        self.norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n        \n        # normalizing each embed\n        self.normalized_embeddings = embeddings / self.norm","outputs":[]},{"metadata":{"_uuid":"99b587ff7f6d2120488c92d4fac310819727fbd0","_cell_guid":"463ad619-1bd6-ec58-22c7-b4adffb0a71b"},"cell_type":"markdown","source":"**Function to read data set**"},{"metadata":{"_uuid":"1772e4990bbbdbec265ec6328661a105b67b557f","collapsed":true,"_cell_guid":"ce4757ed-84c8-4680-1e7c-b9cf465ae862"},"execution_count":null,"cell_type":"code","source":"def read_data(filename, train = True):\n    \n    import pandas as pd\n    dataset = pd.read_csv(filename)\n    \n    # 0 shape to get total of rows, 1 to get total of columns\n    rows = dataset.shape[0]\n    print (\"there are \", rows, \" total rows\\n\")\n    \n    if train:\n        # get value on last column only\n        label = dataset.ix[:, -1:].values\n        \n        # get all value except last column\n        concated = []\n        data = dataset.ix[:, 1:-1].values\n        \n        for i in range(data.shape[0]):\n            string = \"\"\n            \n            for k in range(data.shape[1]):\n                string += data[i][k] + \" \"\n            \n            concated.append(string)\n                \n    \n    # get second column and second last column\n    dataset = dataset.ix[:, 1:-1].values\n    string = []\n    for i in range(dataset.shape[0]):\n        for k in range(dataset.shape[1]):\n            string += dataset[i][k].split()\n            \n    string = list(set(string))\n    \n    if train:\n        return string, concated, label\n    else:\n        return string","outputs":[]},{"metadata":{"_uuid":"a1be172df2906cf86157d8cc5a3d3573ac63a3fa","_cell_guid":"cd3f531e-3de4-d68d-8fcf-81f9c659c7a4"},"cell_type":"markdown","source":"**Function to build our data set for Skip-gram model**"},{"metadata":{"_uuid":"abbdd8aa351b484e62c0ba4b7a9717f4d835c44d","collapsed":true,"_cell_guid":"a8b06e29-3faa-9629-aec9-736d694154d9"},"execution_count":null,"cell_type":"code","source":"def build_dataset(words, vocabulary_size):\n    count = []\n    # extend count\n    # -1 because first space used to place UNK keyword\n    # sorted decending order of words\n    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n\n    dictionary = dict()\n    for word, _ in count:\n        #simply add dictionary of word, used frequently placed top\n        dictionary[word] = len(dictionary)\n\n    data = []\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n\n        data.append(index)\n    \n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, reverse_dictionary","outputs":[]},{"metadata":{"_uuid":"7cca8d1b57d84007bfe47b5dbc35ac0dbd1185db","_cell_guid":"53f23b29-3684-be01-4221-9ce7d81cdea7"},"cell_type":"markdown","source":"**Function to generate skip-K gram**"},{"metadata":{"_uuid":"5bf54ad7de85b314a36984527575a63c9346db6c","collapsed":true,"_cell_guid":"977494a2-500b-1a30-4220-e7ba12043979"},"execution_count":null,"cell_type":"code","source":"def generate_batch_skipgram(words, batch_size, num_skips, skip_window):\n    data_index = 0\n    \n    #check batch_size able to convert into number of skip in skip-grams method\n    assert batch_size % num_skips == 0\n    \n    assert num_skips <= 2 * skip_window\n    \n    # create batch for model input\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1\n    \n    # a buffer to placed skip-grams sentence\n    buffer = collections.deque(maxlen=span)\n    \n    for i in range(span):\n        buffer.append(words[data_index])\n        data_index = (data_index + 1) % len(words)\n    \n    for i in range(batch_size // num_skips):\n        target = skip_window\n        targets_to_avoid = [skip_window]\n        \n        for j in range(num_skips):\n            \n            while target in targets_to_avoid:\n                # random a word from the sentence\n                # if random word still a word already chosen, simply keep looping\n                target = random.randint(0, span - 1)\n            \n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n        \n        buffer.append(words[data_index])\n        data_index = (data_index + 1) % len(words)\n    \n    data_index = (data_index + len(words) - span) % len(words)\n    return batch, labels","outputs":[]},{"metadata":{"_uuid":"04c6d26739b13f7ff47245b1c1802842eec7910a","_cell_guid":"5641ee88-b284-9c2f-a48b-4f927f13bb6c"},"cell_type":"markdown","source":"**Function to generate Vectors**"},{"metadata":{"_uuid":"aa2d9783d0b89776fc288badd77733bf7288f432","collapsed":true,"_cell_guid":"1428b19c-2cf8-fbf4-d387-3dd18415d093"},"execution_count":null,"cell_type":"code","source":"def generatevector(dimension, batch_size, skip_size, skip_window, num_skips, iteration, words_real):\n    \n    print (\"Data size: \", len(words_real))\n\n    data, _, dictionary = build_dataset(words_real, len(words_real))\n    \n    sess = tf.InteractiveSession()\n    print (\"Creating Word2Vec model..\")\n    \n    model = VecModel(batch_size, dimension, 0.1, len(dictionary))\n    sess.run(tf.global_variables_initializer())\n    \n    last_time = time.time()\n    \n    for step in range(iteration):\n        batch_inputs, batch_labels = generate_batch_skipgram(data, batch_size, num_skips, skip_window)\n        feed_dict = {model.train_inputs: batch_inputs, model.train_labels: batch_labels}\n        \n        _, loss = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\n        \n        if ((step + 1) % 100) == 0:\n            \n            new_time = time.time()\n            diff = new_time - last_time\n            last_time = new_time\n            print (\"batch: \", step + 1, \", loss: \", loss, \", speed: \", 100.0 / diff, \" batches / s\")\n            \n    return dictionary, model.normalized_embeddings.eval()","outputs":[]},{"metadata":{"_uuid":"6f2c2cc936fb47b3a813c916fe28253305e091ee","_cell_guid":"4e6a8d67-3c69-7966-fa61-74611acedaab"},"cell_type":"markdown","source":"**Our Neural Network model**"},{"metadata":{"_uuid":"2a56da31396d27989d67db61f6b7577a89b0cee2","collapsed":true,"_cell_guid":"a4b0991e-1a07-714d-d18b-7e5c8addddf3"},"execution_count":null,"cell_type":"code","source":"class Model:\n    \n    def __init__(self, activation, num_layers, size_layer, dimension, biased_node, learning_rate):\n        \n        if activation == 'relu':\n            self.activation = tf.nn.relu\n        elif activation == 'sigmoid':\n            self.activation = tf.nn.sigmoid\n        elif activation == 'tanh':\n            self.activation = tf.nn.tanh\n        else:\n            raise Exception(\"model type not supported\")\n        \n        self.X = tf.placeholder(\"float\", [None, dimension])\n        self.Y = tf.placeholder(\"float\", [None, 1])\n        \n        self.size_relay = tf.placeholder(tf.int32, None)\n        \n        self.input_layer = tf.Variable(tf.random_normal([dimension, size_layer]))\n    \n        if biased_node:\n            self.biased_input_layer = tf.Variable(tf.random_normal([size_layer]))\n            self.biased = []\n            for i in range(num_layers):\n                self.biased.append(tf.Variable(tf.random_normal([size_layer])))\n            \n        self.layers = []\n        for i in range(num_layers):\n            self.layers.append(tf.Variable(tf.random_normal([size_layer, size_layer])))\n\n        self.output_layer = tf.Variable(tf.random_normal([size_layer, 1]))\n        \n            \n        if biased_node:\n            self.first_l = self.activation(tf.add(tf.matmul(self.X[self.size_relay : self.size_relay + 1, :], self.input_layer), self.biased_input_layer))\n            \n            # prevent overfitting\n            self.first_l = tf.nn.dropout(self.first_l, 1.0)\n            \n            self.next_l = self.activation(tf.add(tf.matmul(self.first_l, self.layers[0]), self.biased[0]))\n                \n            for i in range(1, num_layers - 1):\n                self.next_l = self.activation(tf.add(tf.matmul(self.next_l, self.layers[i]), self.biased[i]))\n                    \n        else:\n            self.first_l = self.activation(tf.matmul(self.X[i : i + 1, :], self.input_layer))\n            \n            # prevent overfitting\n            self.first_l = tf.nn.dropout(self.first_l, 1.0)\n            \n            self.next_l = self.activation(tf.matmul(self.first_l, self.layers[0]))\n                \n            for i in range(1, num_layers - 1):\n                self.next_l = self.activation(tf.matmul(self.next_l, self.layers[i]))\n        \n        self.first_cost = tf.reduce_mean(tf.pow(tf.transpose(self.layers[0]) - self.next_l, 2))\n            \n        self.first_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self.first_cost)\n         \n        if biased_node:\n            self.first_l = self.activation(tf.add(tf.matmul(self.X[-1 : , :], self.input_layer), self.biased_input_layer))\n            \n            # prevent overfitting\n            self.first_l = tf.nn.dropout(self.first_l, 1.0)\n            \n            self.next_l = self.activation(tf.add(tf.matmul(self.first_l, self.layers[0]), self.biased[0]))\n            \n            for i in range(1, num_layers - 1):\n                self.next_l = self.activation(tf.add(tf.matmul(self.next_l, self.layers[i]), self.biased[i]))\n        \n        else:\n            self.first_l = self.activation(tf.matmul(self.X[-1 : , :], self.input_layer))\n            \n            # prevent overfitting\n            self.first_l = tf.nn.dropout(self.first_l, 0.5)\n            \n            self.next_l = self.activation(tf.matmul(self.first_l, self.layers[0]))\n            \n            for i in range(1, num_layers - 1):\n                self.next_l = self.activation(tf.matmul(self.next_l, self.layers[i]))\n        \n        # prevent overfitting\n        self.next_l = tf.nn.dropout(self.next_l, 1.0)\n        \n        self.b = tf.Variable(tf.random_normal([1], mean = 0.0, stddev = 0.1))\n        \n        self.last_l = tf.add(tf.matmul(self.next_l, self.output_layer), self.b)\n        \n        self.second_cost = tf.reduce_mean(tf.square(self.last_l - self.Y))\n        \n        # calculate penalty for high value in nodes\n        regularizers = tf.nn.l2_loss(self.input_layer) + sum(map(lambda x: tf.nn.l2_loss(x), self.layers)) + tf.nn.l2_loss(self.output_layer)\n        \n        # by simply cost the penalty value to prevent overfitting\n        self.second_cost = tf.reduce_mean(self.second_cost + 0.1 * regularizers)\n        \n        self.second_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self.second_cost)\n        \n        self.output = self.last_l\n        \n    \n    def step(self, sess, X, Y):\n    \n        out, _, cost, _, _ = sess.run([self.output, self.second_optimizer, self.second_cost, self.first_optimizer, self.first_cost], feed_dict={self.size_relay: X.shape[0] - 1, self.X: X, self.Y: Y})\n        \n        return out, cost","outputs":[]},{"metadata":{"_uuid":"bfbe692dc4a11a125b02c2a27ce4be8ba429c65b","_cell_guid":"5af38c54-5d6c-99c7-c2d4-5015f6d053f2"},"cell_type":"markdown","source":"**Our global variables**"},{"metadata":{"_uuid":"a7ab1e0785e294f2d6dc30c730d4910821acd29d","collapsed":true,"_cell_guid":"b6c49f36-0388-1c0b-3584-1bf705aac67f"},"execution_count":null,"cell_type":"code","source":"dataset = '../input/train.csv'\n\n# these global variables for vectors model\ndimension = 32\nskip_size = 8\nskip_window = 1\nnum_skips = 2\niteration_train_vectors = 1000\n\n# these global variables for NN\nnum_layers = 1\nsize_layer = 64\nlearning_rate = 0.05\nepoch = 50\nbiased_node = True\n\nsplit_data = 0.8\n\n# got sigmoid, relu, tanh\nactivation = 'tanh'\n\n# you can use word vectors or one-hot-encoder\nuse_word_vector = True","outputs":[]},{"metadata":{"_uuid":"76a693ef7262f0e94390725ad2e428711a52b2ac","_cell_guid":"f358fb6f-1b01-2a64-5b99-352855a3b87b"},"cell_type":"markdown","source":"**Start our main**"},{"metadata":{"_uuid":"e0edd651a0a390fd937e5bff744610d0e5f754d5","collapsed":true,"_cell_guid":"d81b0d0e-9e87-4049-b928-8e571493b027"},"execution_count":null,"cell_type":"code","source":"string, data, label = read_data(dataset, train = True)\n\nlabel = LabelEncoder().fit_transform(label)\n\nif use_word_vector:\n    vocab, vectors = generatevector(dimension, dimension, skip_size, skip_window, num_skips, iteration_train_vectors, string)\nelse:\n    vocab = list(set(string))\n    dimension = len(vocab)\n    print (\"dimension size: \", dimension)\n\nsess = tf.InteractiveSession()\n\ndata_train, data_test, label_train, label_test = train_test_split(data, label, test_size = split_data)\n\nmodel = Model(activation, num_layers, size_layer, dimension, biased_node, learning_rate)\n\nsess.run(tf.global_variables_initializer())","outputs":[]},{"metadata":{"_uuid":"1d63e5c037dc0ceccf31370b0d158adbe8a9ac4b","_cell_guid":"0c7e3124-83a0-2afa-e2d0-2ee678658016"},"cell_type":"markdown","source":"**Start our training**"},{"metadata":{"_uuid":"72abffa05d1b1ca31b4799b96afd8146f7943ffc","collapsed":true,"_cell_guid":"83279576-dadc-1089-184b-bb5af0502f4e"},"execution_count":null,"cell_type":"code","source":"for n in range(epoch):\n    total_cost = 0\n    total_accuracy = 0\n    last_time = 0.0\n    \n    for i in range(len(data_train)):\n        last_time = time.time()\n        data_ = data[i].split()\n        \n        if use_word_vector:\n            emb_data = np.zeros((len(data_), vectors.shape[1]), dtype = np.float32)\n            \n            for x in range(len(data_)):\n                found = False\n                for id_, word in vocab.items():\n                    \n                    if word == data_[x]:\n                        emb_data[x, :] = vectors[id_, :]\n                        found = True\n                        break\n                \n                if not found:\n                    print (\"not found any embedded words, recheck vectors pipelining, exiting..\")\n                    exit(0)\n            \n        else:\n            emb_data = np.zeros((len(data_), len(vocab)), dtype = np.float32)\n            \n            for k, text in enumerate(data[i].split()):\n                emb_data[k, vocab.index(text)] = 1.0\n    \n        X = np.array(emb_data)\n        \n        out, loss = model.step(sess, X, np.array([[label_train[i]]]))\n        if int(round(out[0][0])) == label_train[i]:\n            total_accuracy += 1\n        total_cost += loss\n        \n    \n    diff = time.time() - last_time\n    print (\"total accuracy during training: \", total_accuracy / (len(data_train) * 1.0))\n    print (\"batch: \", n + 1, \", loss: \", total_cost / len(data_train), \", speed: \", diff / len(data_train), \" s / epoch\")\n    total_cost = 0\n    total_accuracy = 0","outputs":[]},{"metadata":{"_uuid":"165404894e3f6742f3b1fa582b29d5bc40301d1e","_cell_guid":"25749ee5-6841-8607-98ed-2a6cbfc09928"},"cell_type":"markdown","source":"**Above only for testing in notebook. You can increase the iteration to get better accuracy**"}],"nbformat_minor":1,"nbformat":4}