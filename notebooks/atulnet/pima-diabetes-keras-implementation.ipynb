{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3126e70f-cab2-2a30-615a-4ca30b77f8ab"},"source":"# Pima Indians Diabetes DatabaseÂ¶\n\n## Using a Keras based neural network to predict diabetes\n\n\n- Atul Acharya\n\nThis notebook shows how to use a simple Keras based neural network for predicting diabetes. A few things implemented:\n\n- a 3-layer NN \n- model checkpointing / saving\n- plotting history"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86eb84b9-6e10-46cf-5088-598b12e7dd53"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ddb1465-2483-ae5e-8e5c-251d19e16c6a"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\n\nseed = 42\nnp.random.seed(seed)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19633fb6-3f7c-2935-105d-f35b233f9157"},"outputs":[],"source":"# load Pima dataset\npdata = pd.read_csv('../input/diabetes.csv')\npdata.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0d61adee-1014-d997-c12a-54bcd5910a6a"},"source":"Let's see what the dataset describes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7b8935f-6063-8608-9616-84b5a54b6ef9"},"outputs":[],"source":"pdata.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d099f09-4e80-f7e5-ccdb-7329c67464a2"},"source":"Looks like there are some 0-entries in the dataset. This may or may not be important."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52a26d38-3e4d-c5d3-11ae-be03b6eba259"},"outputs":[],"source":"# let's remove the 0-entries for these fields\n\nzero_fields = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n\ndef check_zero_entries(data, fields):\n    \"\"\" List number of 0-entries in each of the given fields\"\"\"\n    for field in fields:\n        print('field %s: num 0-entries: %d' % (field, len(data.loc[ data[field] == 0, field ])))\n\ncheck_zero_entries(pdata, zero_fields)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8cfd264-5eb6-8c6e-d2c6-465945f327fd"},"source":"[Thanks to **_ManasviKundalia_** for the interpretation.]\n\nAs one can see, there are several \"0\" entries, especially for SkinThickness and Insulin. Atleast some of them (e.g. **Insulin**) matter for diabetes predicition. \n\nWhat to do? \n\nLet's split into Train/Test datasets, and then add back the 0-entries by imputing them from the average.\nWe don't want to impute for the entire dataset at once, since this would affect the performance on the Test set.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"093556e8-a476-800d-ce04-197b6698aa2e"},"outputs":[],"source":"# First - split into Train/Test\nfrom sklearn.model_selection import train_test_split\n\nfeatures = list(pdata.columns.values)\nfeatures.remove('Outcome')\nprint(features)\nX = pdata[features]\ny = pdata['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f11787a2-4372-4684-61f2-cc42498dd83f"},"outputs":[],"source":"# lets fix the 0-entry for a field in the dataset with its mean value\ndef impute_zero_field(data, field):\n    nonzero_vals = data.loc[data[field] != 0, field]\n    avg = np.sum(nonzero_vals) / len(nonzero_vals)\n    k = len(data.loc[ data[field] == 0, field])   # num of 0-entries\n    data.loc[ data[field] == 0, field ] = avg\n    print('Field: %s; fixed %d entries with value: %.3f' % (field, k, avg))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"246dbd17-a705-b2c1-a632-d795a2b6b51b"},"outputs":[],"source":"# Fix it for Train dataset\nfor field in zero_fields:\n    impute_zero_field(X_train, field)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd7b63cc-8d85-31fa-9d00-307587bba4ee"},"outputs":[],"source":"# double check for the Train dataset\ncheck_zero_entries(X_train, zero_fields)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29ed960d-8a9f-bb3f-17d8-87a113e17eee"},"outputs":[],"source":"# Fix for Test dataset\nfor field in zero_fields:\n    impute_zero_field(X_test, field)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8b2c768-1be7-960f-78ac-224cacebb1ac"},"outputs":[],"source":"# double check for the Test dataset\ncheck_zero_entries(X_test, zero_fields)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b79bc368-922a-06c2-7321-8f443bbe8bc9"},"outputs":[],"source":"# Ensure that fieldnames aren't included\nX_train = X_train.values\ny_train = y_train.values\nX_test  = X_test.values\ny_test  = y_test.values"},{"cell_type":"markdown","metadata":{"_cell_guid":"bf86d9bb-459f-3441-a4ec-96debcd6fa2d"},"source":"### Neural Network model\n\nWe define a 3-layer NN model in Keras\n\n- First layer: 12 nodes, with RELU activation\n- 2nd layer:   8 nodes,  with RELU activation\n- 3rd layer:   output,   with sigmoid activation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ab854a9-4ff8-d20f-b6a3-39e9bb040ca8"},"outputs":[],"source":"NB_EPOCHS = 1000  # num of epochs to test for\nBATCH_SIZE = 16\n\n## Create our model\nmodel = Sequential()\n\n# 1st layer: input_dim=8, 12 nodes, RELU\nmodel.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n# 2nd layer: 8 nodes, RELU\nmodel.add(Dense(8, init='uniform', activation='relu'))\n# output layer: dim=1, activation sigmoid\nmodel.add(Dense(1, init='uniform', activation='sigmoid' ))\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy',   # since we are predicting 0/1\n             optimizer='adam',\n             metrics=['accuracy'])\n\n# checkpoint: store the best model\nckpt_model = 'pima-weights.best.hdf5'\ncheckpoint = ModelCheckpoint(ckpt_model, \n                            monitor='val_acc',\n                            verbose=1,\n                            save_best_only=True,\n                            mode='max')\ncallbacks_list = [checkpoint]\n\nprint('Starting training...')\n# train the model, store the results for plotting\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_test, y_test),\n                    nb_epoch=NB_EPOCHS,\n                    batch_size=BATCH_SIZE,\n                    callbacks=callbacks_list,\n                    verbose=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"817a2560-1d35-9c54-6300-bf97b13b3848"},"outputs":[],"source":"# Model accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b719352d-c431-dc4d-d1ca-1d177bb42951"},"outputs":[],"source":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af79c45e-f1b7-7b7f-b7b9-756b2a075224"},"outputs":[],"source":"# print final accuracy\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fba74d61-df6d-2a41-a316-7cc83ed78f96"},"source":"So we see several things:\n\n    - We get accuracy is  about **78%**, which is decent, not great\n    - after about 300 epochs, the model does not really improve.\n    - After about 500 epochs, the training loss starts to increase, which indicates overfitting\n\nA few things could be done to improve the results:\n\n- Different model architecture (num of nodes, etc)\n- Dropout\n- Adaptive learning rate\n\nAnything other suggestions for improvement? Thanks for taking a look. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ff78452-cdc7-16f5-2dbc-bb7e873dfe43"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}