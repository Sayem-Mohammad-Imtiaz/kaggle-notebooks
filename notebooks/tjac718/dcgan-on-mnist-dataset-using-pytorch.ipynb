{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Important Implementation Note\nI've defined 2 separate GAN's below: a basic GAN and a DCGAN. The following code is compatible with both networks. If you wish to switch between the different implementations, I've indicated whether to comment or uncomment code at the necessary locations. "},{"metadata":{},"cell_type":"markdown","source":"# Defining The Accelerator\nUsing a GPU if available, else a CPU. In this case, satisfactory performance is obtained with a CPU and training time is not too high."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading the training data\ntrain = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the Training Data\nFrom the original MNIST dataset, 60000 images are chosen for training. The training data is reshaped into 28x28 numpy array to represent the grayscale images. Finally, an important step of normalization follows to allow the GAN to converge better. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train.drop(labels = ['label'], axis = 1)\ntrain_data = train_data.values.reshape(-1, 28, 28)\ntrain_data = train_data/255.0\n\n#To create some space\ndel train ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just checking if the data type is as expected\nprint(isinstance(train_data, np.ndarray))\n\n#Checking the sanity of the shape of the training data\nprint(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Data\nRandomly plotting one of the training examples to catch a drift about the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(train_data[7], cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using DataLoader\nTo allow for efficient iteration over the different mini-batches using a hard coded batch size of 32."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting to torch tensor\ntrain_data = torch.Tensor(train_data)\n\n#Checking to confirm dtype as tensor\nprint(isinstance(train_data, torch.Tensor))\n\nrandom_seed = 1\nbatch_size = 32\ntrain_dl = DataLoader(train_data, batch_size, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Noise Generation\nThe input to the generator in a GAN is usually random noise. Here, I use torch.rand to generate random numbers between 0 and 1 and then rescale the numbers to fall between -1 and 1 so as to make the mean 0 for the uniform distribution to facilitate the training of the GAN."},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_noise_generator(batch_size, dim):\n    return torch.rand(batch_size, dim)*2 - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just checking the noise generator and plotting one of its outputs\na = random_noise_generator(64, 100)\nb = a[2]\nb = b.reshape(10, 10)\nb = b.numpy()\nplt.imshow(b, cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discriminator Model for Basic GAN\nThe discriminator model is fairly straightforward having a few fully connected layers having one small caveat, instead of the usual ReLU for activation, here, I've used a **leaky ReLU**. This is because, a leaky ReLU has a small gradient for negative values of input, allowing for a stronger flow of gradients from the discriminator to the generator. Also, some dropout layers have been added in intermediate stages to prevent overfitting. \nFurther, the final output is supposed to be between 0 and 1 (to predict if real or fake), however, the loss function used later on is **BCEWithLogitsLoss** (for numerical stability) which combines the sigmoid with the normal binary cross entropy function, so, no activation function is applied to the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unomment this if you want to run the Basic GAN instead of the DCGAN\n\n\"\"\"\nclass Discriminator(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size*4)\n        self.fc2 = nn.Linear(hidden_size*4, hidden_size*2)\n        self.fc3 = nn.Linear(hidden_size*2, hidden_size)\n        self.fc4 = nn.Linear(hidden_size, output_size)\n        \n        self.dropout = nn.Dropout(0.3)\n        \n    def forward(self, x):\n        x = x.view(-1, 28*28)\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        x = self.dropout(x)\n        \n        out = self.fc4(x)\n        return out\n\"\"\"        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discriminator Model for DCGAN\nThe discriminator model for a DCGAN can be implemented in many different ways, but it's recommended by the authors of the original paper on DCGAN's to have **strided convolutions** for downscaling, remove any fully connected layers, use batch normalization and the **leaky ReLU** for activation of all layers except sigmoid for the last one. I've added a few dropout layers additionally to prevent overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comment this if you want to run the Basic GAN instead of the DCGAN\n\nclass Discriminator(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv0 = nn.Conv2d(1, 32, kernel_size = 3, stride = 2, padding = 1)\n        #self.conv0_bn = nn.BatchNorm2d(32)\n        self.conv0_drop = nn.Dropout2d(0.25)\n        self.conv1 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n        #self.conv1_bn = nn.BatchNorm2d(64)\n        self.conv1_drop = nn.Dropout2d(0.25)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n        #self.conv2_bn = nn.BatchNorm2d(128)\n        self.conv2_drop = nn.Dropout2d(0.25)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n        #self.conv3_bn = nn.BatchNorm2d(256)\n        self.conv3_drop = nn.Dropout2d(0.25)\n        self.fc = nn.Linear(12544, 1)\n    \n    def forward(self, x):\n        x = x.view(-1, 1, 28, 28)\n        x = F.leaky_relu(self.conv0(x), 0.2)\n        #x = self.conv0_bn(x)\n        x = self.conv0_drop(x)\n        x = F.leaky_relu(self.conv1(x), 0.2)\n        #x = self.conv1_bn(x)\n        x = self.conv1_drop(x)\n        x = F.leaky_relu(self.conv2(x), 0.2)\n        #x = self.conv2_bn(x)\n        x = self.conv2_drop(x)\n        x = F.leaky_relu(self.conv3(x), 0.2)\n        #x = self.conv3_bn(x)\n        x = self.conv3_drop(x)\n        x = x.view(-1, self.num_flat_features(x))\n        x = self.fc(x)\n        \n        return x\n    \n    def num_flat_features(self, x):\n        size = x.size()[1:]\n        num_features = 1\n        for s in size:\n            num_features *= s\n        \n        return num_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generator Model for Basic GAN\nThe generator model is quite similar to the discriminator, with two small differences:\n1. Instead of the input being downscaled, it's upscaled to reach expected dimensions of MNIST dataset images.\n2. Final activation function **tanh** is applied to the output bringing its values between -1 and 1 for best performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment this if you want to run the Basic GAN instead of the DCGAN\n\n\"\"\"\nclass Generator(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size*2)\n        self.fc3 = nn.Linear(hidden_size*2, hidden_size*4)\n        self.fc4 = nn.Linear(hidden_size*4, output_size)\n        \n        self.dropout = nn.Dropout(0.3)\n        \n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        x = self.dropout(x)\n        \n        out = torch.tanh(self.fc4(x))\n        return out\n\"\"\"        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generator Model for DCGAN\nSimilar to the discrminator, there are many possible implementations but the authors of the original paper on DCGAN's recommend using **transposed convolutions** for upscaling, using batch normalization in intermediate stages and having a **ReLU** activation function after all layers except a tanh activation for the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comment this if you want to run the Basic GAN instead of the DCGAN\n\nclass Generator(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(100, 256*7*7)\n        self.trans_conv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n        #self.trans_conv1_bn = nn.BatchNorm2d(128)\n        self.trans_conv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 1, padding = 1)\n        #self.trans_conv2_bn = nn.BatchNorm2d(64)\n        self.trans_conv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 1, padding = 1)\n        #self.trans_conv3_bn = nn.BatchNorm2d(32)\n        self.trans_conv4 = nn.ConvTranspose2d(32, 1, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n    \n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(-1, 256, 7, 7)\n        x = F.relu(self.trans_conv1(x))\n        #x = self.trans_conv1_bn(x)\n        x = F.relu(self.trans_conv2(x))\n        #x = self.trans_conv2_bn(x)\n        x = F.relu(self.trans_conv3(x))\n        #x = self.trans_conv3_bn(x)\n        x = self.trans_conv4(x)\n        x = torch.tanh(x)\n        \n        return x        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting Model Hyperparameters for Basic GAN\nInput size to the generator is hard coded to be 100. Can be manipulated to check variations in performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment this if you want to run the Basic GAN instead of the DCGAN\n\n\"\"\"\n#For Discriminator\ninput_disc = 784\nhidden_disc = 32\noutput_disc = 1\n\n#For Generator\ninput_gen = 100\nhidden_gen = 32\noutput_gen = 784\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment this if you want to run the Basic GAN instead of the DCGAN\n\"\"\"\n#Creating instances of models\nD = Discriminator(input_disc, hidden_disc, output_disc)\nG = Generator(input_gen, hidden_gen, output_gen)\n\"\"\"\n\n#Comment the following 2 lines if you want to run the Basic GAN instead of the DCGAN\nD = Discriminator()\nG = Generator()\n\n#Sanity check of the model instances\nprint(D)\nprint(G)\n\n#Passing to the GPU\nD = D.to(device)\nG = G.to(device)\n\nD = D.float()\nG = G.float()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discriminator Loss\nDiscriminator loss is the resultant sum of two components:\n1. On passing real training set images through the discriminator, the output values are compared with 1 (for real images) and real loss is calculated.\n2. On passing random noise through the generator, fake images are created which are further passed through the discriminator. These outputs are compared with 0 (for fake images) and fake loss is calculated.\n\nI have defined relevant functions to calculate real loss, fake loss and the resultant sum of these two losses."},{"metadata":{"trusted":true},"cell_type":"code","source":"Loss = nn.BCEWithLogitsLoss()\ndef discriminator_real_loss(real_out):\n    real_label = torch.ones(real_out.size()[0], 1).to(device)\n    real_loss = Loss(real_out.squeeze(), real_label.squeeze())\n    return real_loss\n\ndef discriminator_fake_loss(fake_out):\n    fake_label = torch.zeros(fake_out.size()[0], 1).to(device)\n    fake_loss = Loss(fake_out.squeeze(), fake_label.squeeze())\n    return fake_loss\n\ndef discriminator_loss(real_out, fake_out):\n    real_loss = discriminator_real_loss(real_out)\n    fake_loss = discriminator_fake_loss(fake_out)\n    total_loss = (real_loss + fake_loss)\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generator Loss\nPassing random noise through the generator, we get fake images. On further passing these through the discriminator, we get their output labels. Since we want the generator to fool the discriminator, these output labels are compared with 1 (for supposedly real images) and loss is calculated."},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator_loss(gen_disc_out):\n    label = torch.ones(gen_disc_out.size()[0], 1).to(device)\n    gen_loss = Loss(gen_disc_out.squeeze(), label.squeeze())\n    return gen_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Optimizers\nIf going with the basic GAN:\nUsing Adam optimizers for both discriminator and generator. The learning rate is adjusted to 0.002, a slight deviation from the original paper, to allow for faster learning in this case.\n\nIf using the DCGAN architecture:\nUsing Adam optimizers for the generator and discriminator with a slight deviation from the usual settings, setting the learning rate to 0.0002 and $\\beta_{1}$ to be 0.5 according to the authors of the DCGAN paper for best performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important: If using Basic GAN instead of DCGAN, go for standard values lr = 0.001 and betas = (0.9, 0.999) \n\ndisc_opt = optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))\ngen_opt = optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the GAN\nThe GAN was trained with a mini-batch size of 32 for 25 epochs and samples of the generator output were printed every 5 epochs to monitor the progress of the generator."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(D, G, disc_opt, gen_opt, train_dl, batch_size = 32, epochs = 25, gen_input_size = 100):\n    \n    disc_losses = []\n    gen_losses = []\n    \n    #Having a fixed sample to monitor the progress of the generator\n    sample_size = 16\n    fixed_samples = random_noise_generator(sample_size, gen_input_size)\n    fixed_samples = fixed_samples.to(device)\n    \n    #Going into training mode\n    D.train()\n    G.train()\n    \n    for epoch in range(epochs + 1):\n        \n        disc_loss_total = 0\n        gen_loss_total = 0\n        gen_out = 0\n        \n        for train_x in train_dl:\n            \n            #Discriminator training\n            disc_opt.zero_grad()\n            \n            train_x = train_x*2 - 1          #Converting the real images to have values between -1 and 1\n            train_x = train_x.to(device)     #Passing to GPU\n            real_out = D(train_x.float())    \n            \n            disc_gen_in = random_noise_generator(batch_size, gen_input_size)\n            disc_gen_in = disc_gen_in.to(device)   #Passing to GPU\n            \n            disc_gen_out = G(disc_gen_in.float()).detach()  #Detaching to avoid training the generator\n            fake_out = D(disc_gen_out.float())\n            \n            disc_loss = discriminator_loss(real_out, fake_out)  #Loss calculation\n            disc_loss_total += disc_loss\n            disc_loss.backward()\n            disc_opt.step()  \n        \n            #Generator training\n            gen_opt.zero_grad()\n            \n            \n            gen_out = G(disc_gen_in.float())     #Feeding noise into the generator\n            gen_disc_out = D(gen_out.float())       #Passing into the discrminator\n            \n            gen_loss = generator_loss(gen_disc_out)  #Generator loss calculation\n            gen_loss_total += gen_loss\n            gen_loss.backward()\n            gen_opt.step()\n        \n        disc_losses.append(disc_loss_total)\n        gen_losses.append(gen_loss_total)\n        \n        #Plotting samples every 5 epochs\n        if epoch%5 == 0:\n            G.eval()                    #Going into eval mode to get sample images         \n            samples = G(fixed_samples.float())\n            G.train()                   #Going back into train mode\n            \n            fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n            for ax, img in zip(axes.flatten(), samples):\n               img = img.cpu().detach()\n               ax.xaxis.set_visible(False)\n               ax.yaxis.set_visible(False)\n               im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n        \n        \n        #Printing losses every epoch\n        print(\"Epoch \", epoch, \": Discriminator Loss = \", disc_loss_total/len(train_dl), \", Generator Loss = \", gen_loss_total/len(train_dl))    \n    \n    return disc_losses, gen_losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disc_losses, gen_losses = train(D, G, disc_opt, gen_opt, train_dl, batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Losses\nThe discriminator and generator losses are plotted as a function of the number of epochs to observe the trend and allow for easier debugging."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\ndisc_losses = np.array(disc_losses)\ngen_losses = np.array(gen_losses)\nplt.plot(disc_losses, label='Discriminator')\nplt.plot(gen_losses, label='Generator')\nplt.title(\"Training Losses\")\nplt.legend()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}