{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom matplotlib import pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n# data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading + Inspecting"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 13 columns, including the target so 12 features to work with. Enrolee_id holds no particular meaning, so we can discard it from the get-go. Most of our features are categorical."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Before proceeding, as usual let's check if there are any null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So,the numerical columns have no missing values in them, all NaN values are relegated to categorical columns."},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the target in detail. A , '1' indicates the employee made a career change, '0' indicating otherwise."},{"metadata":{},"cell_type":"markdown","source":"Clearly the dataset is imbalanced, people who don't pull off a career change outnumber those who do by almost 3 times. We will get back to this issue later."},{"metadata":{},"cell_type":"markdown","source":"# EDA + Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Let's plot some graphs and examing how our features relate to the target and each other. Starting with training_hours as it's a fairly straightforward & simple numerical column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.histplot(x=df['training_hours'],kde=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normally distributed with a high degree of skew and kurtosis. The mean is about 65 hours and the median, 47 hours."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness\" , df['training_hours'].skew())\nprint(\"Kurtosis\" , df['training_hours'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We apply log transformation to rectify this."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['training_hours'] = np.log(df['training_hours'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"High degree of skew, we will apply the log transformation to this column before passing it into a model."},{"metadata":{},"cell_type":"markdown","source":"Now, city_development_index is an interesting feature, is there any correlation between how developed a city is and people being more likely to switch careers?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['city_development_index'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the cities fall into the underdeveloped or extremely developed category, this is analogous to the real world. Let's see whether people living in super developed cities have a higher count of career switching."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_city = df.query('city_development_index>0.50')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_city['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So out of 4777 positive values, 4740 values are relegated to cities with more than a 0.50 value in the development_index. So it's safe to assume, people from more developed cities are more likely to switch careers. We will keep this feature as it is."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['gender'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the values unfortunately belong to one class :/, so it's probably not going to be a good feature, further exploration is unwarranted."},{"metadata":{},"cell_type":"markdown","source":"Now let's go onto Education Level, this intuitively seems like a good feature to have. Academically accomplished and studious employees might tend to chase new avenues."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['education_level'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 4 varying levels of education, with most employees being at the Graduate Level. It would be interesting to check whether employees with a higher education level than that would have a higher propensity for checking."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['education_level'],hue=df['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Doesn't seem like it, still an important feature to have as being \"Graduate\" or above still dramatically increases your chance of doing a career switch. "},{"metadata":{},"cell_type":"markdown","source":"Okay, let's move onto 'relevent expereience', seems important for obvious reasons."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['relevent_experience'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, overwhelming number of the employees have relevant experience. Let's see if having relevent experience could mean switching."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['relevent_experience'],hue=df['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Disappointingly, having relevent experience in the field is not a good indicator of whether an employee will switch. Neverthless, it will be a weak feature that can aid in prediction."},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the total   'experience'  an employee has."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['experience'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use a mapper to get rid of the special characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"experience_map = {\n    '<1'      :    0,\n    '1'       :    1, \n    '2'       :    2, \n    '3'       :    3, \n    '4'       :    4, \n    '5'       :    5,\n    '6'       :    6,\n    '7'       :    7,\n    '8'       :    8, \n    '9'       :    9, \n    '10'      :    10, \n    '11'      :    11,\n    '12'      :    12,\n    '13'      :    13, \n    '14'      :    14, \n    '15'      :    15, \n    '16'      :    16,\n    '17'      :    17,\n    '18'      :    18,\n    '19'      :    19, \n    '20'      :    20, \n    '>20'     :    21\n} \ndf['experience'] = df['experience'].map(experience_map)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.histplot(x=df['experience'],hue=df['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at last_new_job now, which is the difference in years b/w previous and current job."},{"metadata":{"trusted":true},"cell_type":"code","source":"last_new_job_map = {\n    'never'        :    0,\n    '1'            :    1, \n    '2'            :    2, \n    '3'            :    3, \n    '4'            :    4, \n    '>4'           :    5\n}\ndf['last_new_job'] = df['last_new_job'].map(last_new_job_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['last_new_job'],hue=df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Again, no discernible trend."},{"metadata":{},"cell_type":"markdown","source":"Now let's move onto company_size and type."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['company_size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Special characters again, so once again remove with a map"},{"metadata":{"trusted":true},"cell_type":"code","source":"company_size_map = {\n    '<10'          :    0,\n    '10/49'        :    1, \n    '100-500'      :    2, \n    '1000-4999'    :    3, \n    '10000+'       :    4, \n    '50-99'        :    5, \n    '500-999'      :    6, \n    '5000-9999'    :    7\n}\ndf['company_size'] = df['company_size'].map(company_size_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['company_size'],hue=df['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No clear trend again. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=df['education_level'],y=df['training_hours'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm, people with a Graduate and Masters degree tend to have higher number of training hours compared to the rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['major_discipline'],hue=df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"STEM employees dominate the dataset, so consequqntially they have the most employees who switch. Anyway, this tells us that most of time, an employee who switched has a STEM background."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(y=df['experience'],x=df['education_level'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Employees with only a primary education level sorely lack in experience, could be a good predictor for classifying into the '0' class, i.e no switch."},{"metadata":{},"cell_type":"markdown","source":"That should be enough EDA, let's plot a heatmap just in case we missed any correlations."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = df.corr()\nsns.heatmap(df_corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks, like we didn't!  We can proceed with cleaning and preprocessing."},{"metadata":{},"cell_type":"markdown","source":"# Cleaning + Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"First, let's seperate out our label from the dataframe and drop it.   We can also drop \"enrollee_id\" as it holds no useable information."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['target']\ndf.drop(columns=['enrollee_id','target'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's pull out the categorical columns in our dataframe, and apply Label Encoding on them."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = list(df.select_dtypes(include='object').columns)\nfor col in cat_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Writing a small helper function to check if there are any NULL values and calculate their proportion, column-wise."},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_cols = [i for i in df.columns if df[i].isna().any()]\ndef nan_calculator(df):\n    total = len(df)\n    for col in nan_cols:\n        null = df[col].isnull().sum()\n        print(f'Missing values: {null} of {total} in {col}')\nnan_calculator(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not many are missing, can be dealt with a basic imputer."},{"metadata":{},"cell_type":"markdown","source":"Let's use a simple imputer with median as our strategy."},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(df)\ndf = imp.transform(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"Using the classic, 'train_test_split' to split our training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state = 402)\nX_smote, Y_smote = smote.fit_resample(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember, we never had enough '1.0' target values. Using the Smote library to generate some synthetic samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_smote, Y_smote, test_size = 0.2 ,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Okay, we are ready to pass in our features to a model. Let's start with the simple Logisitc Regression, and try out increasingly complex models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nlr.score(X_val,y_val)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"68 percent, not bad for a simple model. Let's try a decision tree next."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\nclf.fit(X_train,y_train)\nclf.score(X_val,y_val)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clear improvement, to be expected as Decision Trees have a much higher model capacity. Trying Random Forests next, as they are an evolved version of Decision Trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=10, max_depth=None,\nmin_samples_split=2, random_state=0)\nrf.fit(X_train,y_train)\nrf.score(X_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost 85% now, again keeping in line with our expectations."},{"metadata":{},"cell_type":"markdown","source":"Finally, let's try out a Gradient Boosted Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_reg = XGBClassifier(max_depth=5)\nxgb_reg.fit(X_train,y_train)\nxgb_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A dip, probably overfitting. Let's try ExtraTreesClassifier next."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier()\netc.fit(X_train,y_train)\nprint(etc.score(X_val,y_val))\netc_pred = etc.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A 2 percent improvement over Random Forests, next we will try sci-kit learn's VotingClassifier to stack our best performing models and see if there is any improvement."},{"metadata":{"trusted":true},"cell_type":"code","source":"eclf1 = VotingClassifier(estimators=[\n('rf', rf),('etc',etc),('xgb',xgb_reg)], voting='hard')\neclf1 = eclf1.fit(X_train, y_train)\nprint(eclf1.score(X_val,y_val))\npred = eclf1.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stays about the same, so we will choose ExtraTreesClassifier as our model since it performs the best."},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = ['No career switch', \"Succesfull career transition\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val,etc_pred,target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad, 87 percent on precision and recall. Seems reliable. "},{"metadata":{},"cell_type":"markdown","source":"# Testing"},{"metadata":{},"cell_type":"markdown","source":"Now let's see how well our model performs well on the test set. First we have to apply whatever transformations we applied to our training set before splitting the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_test.csv')\ndf_test['training_hours'] = np.log(df_test['training_hours'])\nexperience_map = {\n    '<1'      :    0,\n    '1'       :    1, \n    '2'       :    2, \n    '3'       :    3, \n    '4'       :    4, \n    '5'       :    5,\n    '6'       :    6,\n    '7'       :    7,\n    '8'       :    8, \n    '9'       :    9, \n    '10'      :    10, \n    '11'      :    11,\n    '12'      :    12,\n    '13'      :    13, \n    '14'      :    14, \n    '15'      :    15, \n    '16'      :    16,\n    '17'      :    17,\n    '18'      :    18,\n    '19'      :    19, \n    '20'      :    20, \n    '>20'     :    21\n} \ndf_test['experience'] = df_test['experience'].map(experience_map)\nlast_new_job_map = {\n    'never'        :    0,\n    '1'            :    1, \n    '2'            :    2, \n    '3'            :    3, \n    '4'            :    4, \n    '>4'           :    5\n}\ndf_test['last_new_job'] = df_test['last_new_job'].map(last_new_job_map)\ncompany_size_map = {\n    '<10'          :    0,\n    '10/49'        :    1, \n    '100-500'      :    2, \n    '1000-4999'    :    3, \n    '10000+'       :    4, \n    '50-99'        :    5, \n    '500-999'      :    6, \n    '5000-9999'    :    7\n}\ndf_test['company_size'] = df_test['company_size'].map(company_size_map)\ndf_test.drop(columns=['enrollee_id'],inplace=True)\ncat_cols = list(df_test.select_dtypes(include='object').columns)\nfor col in cat_cols:\n    le = LabelEncoder()\n    df_test[col] = le.fit_transform(df_test[col])\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(df_test)\ndf_test = imp.transform(df_test)\nX_train_1,X_test_1,y_train_1,y_test_1 = train_test_split(df_test,y[0:2129],test_size=0.2)\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state = 402)\nX_smote, Y_smote = smote.fit_resample(X_train_1,y_train_1)\nX_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(X_smote, Y_smote, test_size = 0.2 ,random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etc.fit(X_train_1,y_train_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etc.score(X_val_1,y_val_1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etc_pred_test = etc.predict(X_val_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val_1,etc_pred_test,target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting almost the same level of performance as on our training set, this confirms the model has good generalisation capacity."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}