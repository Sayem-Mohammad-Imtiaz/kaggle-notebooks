{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Some information about the dataset\nThis dataset is composed of 6.6GB of data, including 8732 sound files of around 4 seconds(but less or equal) and 1 CSV file giving info about files where to find them in the dataset,their original file, their location in the original files, and of course their classes in ID but also with the name associated to it, also if the sound to detect is in foreground or background.\n\nThere are ten classes of sound and the aim of this project is to develop a classifier for this sound.\n\nThe ten classes are : airconditioner, carhorn, childrenplaying, dogbark, drilling, engineidling, gunshot, jackhammer, siren, street_music.\n\n## How I will do it ?\nSo the idea is to develop a classifier I decided to use features generating a 2D representation of the sound and applying CNN on the generated representation to classify the classes.\n\n\n\n\n**VERSION BIGNN :**\nThis is the version of the project using Transfer Learning on wide\\_resnet\\_50\\_2, using images from spectrogram resized","metadata":{}},{"cell_type":"markdown","source":"## Imports and define some paths\n","metadata":{}},{"cell_type":"code","source":"!pip install torchaudio==0.8.1\n%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n\n## Related to image processing\nimport cv2\n\n## Related to sound\nimport torchaudio\nimport torchaudio.transforms as T\nimport IPython.display as ipd\n\n## Related to deeplearning\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:04:47.169659Z","iopub.execute_input":"2021-09-23T12:04:47.170335Z","iopub.status.idle":"2021-09-23T12:06:32.353757Z","shell.execute_reply.started":"2021-09-23T12:04:47.170247Z","shell.execute_reply":"2021-09-23T12:06:32.352911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH_TO_FOLDER =\"\"\nCSV_FILENAME = \"UrbanSound8K.csv\"\nMOUNTING_LOCATION = \"../input/urbansound8k/\"\n\n# DF VARS\nFILENAME = \"slice_file_name\"\nFID = \"fsID\"\nSTART = \"start\"\nEND = \"end\"\nSL = \t\"salience\"\t\nFOLD = \"fold\"\nCLASSID = \"classID\"\nCLASS = \"class\"","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:32.355792Z","iopub.execute_input":"2021-09-23T12:06:32.356061Z","iopub.status.idle":"2021-09-23T12:06:32.36256Z","shell.execute_reply.started":"2021-09-23T12:06:32.356025Z","shell.execute_reply":"2021-09-23T12:06:32.36117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define some processing and visualization functions","metadata":{}},{"cell_type":"code","source":"def plot_spectrogram(waveform, title=None, ylabel='freq_bin', aspect='auto', axs = None):\n  if axs is None :\n    fig, axs = plt.subplots(1, 1)\n  axs.set_ylabel(ylabel)\n  axs.set_xlabel('frame')\n  amp = torchaudio.transforms.AmplitudeToDB('magnitude',top_db=80)\n  res = amp(np.abs(torch.stft(waveform,2048)))[0]\n  y = torch.sum(res.permute(2,0,1),axis =0)\n  im = axs.imshow(y)\n  plt.title(title or 'Spectrogram (db)')\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", axes = None):\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) / sample_rate\n  waveform = waveform[0]\n  if axes is None:\n    figure, axes = plt.subplots(1, 1)\n  plt.plot(time_axis, waveform, linewidth=1)\n  plt.title(title)\n\ndef load_sound_by_index(file_index, df):\n  sound_picked_fn = df[FILENAME][file_index]\n  waveform, sr = torchaudio.load(MOUNTING_LOCATION+PATH_TO_FOLDER+\"fold\"+ str(df[FOLD][file_index])+os.path.sep+sound_picked_fn) # load audio\n  audio_mono = torch.mean(waveform, dim=0, keepdim=True) # Convert sterio to mono\n  return audio_mono","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:32.363989Z","iopub.execute_input":"2021-09-23T12:06:32.364526Z","iopub.status.idle":"2021-09-23T12:06:32.376025Z","shell.execute_reply.started":"2021-09-23T12:06:32.364444Z","shell.execute_reply":"2021-09-23T12:06:32.375059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(MOUNTING_LOCATION+PATH_TO_FOLDER+CSV_FILENAME)\n#df = df.loc[df[\"fold\"]==1].reset_index(drop=True)  ## Select only mounting folder 1\nr,c = df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:32.378951Z","iopub.execute_input":"2021-09-23T12:06:32.379451Z","iopub.status.idle":"2021-09-23T12:06:32.418762Z","shell.execute_reply.started":"2021-09-23T12:06:32.379417Z","shell.execute_reply":"2021-09-23T12:06:32.418071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"print(f\"In this dataset there is {df.shape[0]} sounds\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:32.419866Z","iopub.execute_input":"2021-09-23T12:06:32.42022Z","iopub.status.idle":"2021-09-23T12:06:32.42584Z","shell.execute_reply.started":"2021-09-23T12:06:32.420172Z","shell.execute_reply":"2021-09-23T12:06:32.424923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df[CLASSID].values, bins=10,range=(0,9))\nplt.title('Classes histogram')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:32.42691Z","iopub.execute_input":"2021-09-23T12:06:32.427404Z","iopub.status.idle":"2021-09-23T12:06:32.686632Z","shell.execute_reply.started":"2021-09-23T12:06:32.427368Z","shell.execute_reply":"2021-09-23T12:06:32.685882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df[SL].values, bins=2,range=(1,2))\nplt.title('Salience (1: Foreground, 2: Background)')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:32.687824Z","iopub.execute_input":"2021-09-23T12:06:32.688077Z","iopub.status.idle":"2021-09-23T12:06:32.879382Z","shell.execute_reply.started":"2021-09-23T12:06:32.688045Z","shell.execute_reply":"2021-09-23T12:06:32.878729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There is {len(np.unique(df[FID].values))} differents sources of sound in this dataset.\")\n\nplt.hist(df[FID].values, bins=len(np.unique(df[FID].values)))\nplt.title('Sources histogram')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:32.880652Z","iopub.execute_input":"2021-09-23T12:06:32.880891Z","iopub.status.idle":"2021-09-23T12:06:35.62823Z","shell.execute_reply.started":"2021-09-23T12:06:32.880859Z","shell.execute_reply":"2021-09-23T12:06:35.627565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample pick","metadata":{}},{"cell_type":"code","source":"dfC1 = df[df[CLASSID]==1].reset_index(drop=True)\nindex = 20\nsound_picked_fn = dfC1[FILENAME][index]\nclass_name1 = dfC1[CLASS][index]\nwaveform, sr1 = torchaudio.load(MOUNTING_LOCATION+PATH_TO_FOLDER+\"fold\"+ str(dfC1[FOLD][index])+os.path.sep+sound_picked_fn) # load audio\naudio_mono1 = torch.mean(waveform, dim=0, keepdim=True) # Convert sterio to mono\n\ndfC2 = df[df[CLASSID]==2].reset_index(drop=True)\nsound_picked_fn = dfC2[FILENAME][0]\nclass_name2 = dfC2[CLASS][0]\nwaveform, sr2 = torchaudio.load(MOUNTING_LOCATION+PATH_TO_FOLDER+\"fold\"+ str(dfC2[FOLD][0])+os.path.sep+sound_picked_fn) # load audio\naudio_mono2 = torch.mean(waveform, dim=0, keepdim=True) # Convert sterio to mono\n\ndfC3 = df[df[CLASSID]==3].reset_index(drop=True)\nsound_picked_fn = dfC3[FILENAME][10]\nclass_name3 = dfC3[CLASS][10]\nwaveform, sr3 = torchaudio.load(MOUNTING_LOCATION+PATH_TO_FOLDER+\"fold\"+ str(dfC3[FOLD][10])+os.path.sep+sound_picked_fn) # load audio\naudio_mono3 = torch.mean(waveform, dim=0, keepdim=True) # Convert sterio to mono","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:35.629424Z","iopub.execute_input":"2021-09-23T12:06:35.629735Z","iopub.status.idle":"2021-09-23T12:06:35.835223Z","shell.execute_reply.started":"2021-09-23T12:06:35.629698Z","shell.execute_reply":"2021-09-23T12:06:35.834516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sfig, saxs = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n\nplt.sca(saxs[0])\nplot_waveform(audio_mono1.numpy(),sr1, title=\"Waveform sample \"+str(class_name1),axes=saxs[0])\n\nplt.sca(saxs[1])\nplot_waveform(audio_mono2.numpy(),sr2, title=\"Waveform sample \"+str(class_name2),axes=saxs[1])\n\nplt.sca(saxs[2])\nplot_waveform(audio_mono3.numpy(),sr3,title=\"Waveform sample \"+str(class_name3),axes=saxs[2])","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:35.837839Z","iopub.execute_input":"2021-09-23T12:06:35.838101Z","iopub.status.idle":"2021-09-23T12:06:40.470372Z","shell.execute_reply.started":"2021-09-23T12:06:35.838068Z","shell.execute_reply":"2021-09-23T12:06:40.469652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mfig, maxs = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\nplt.sca(maxs[0])\nplot_spectrogram(audio_mono1, title=\"Spectrogram sample \"+class_name1,axs=maxs[0])\n\nplt.sca(maxs[1])\nplot_spectrogram(audio_mono2, title=\"Spectrogram sample \"+class_name2,axs=maxs[1])\n\nplt.sca(maxs[2])\nplot_spectrogram(audio_mono3, title=\"Spectrogram sample \"+class_name3,axs=maxs[2])","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:40.474144Z","iopub.execute_input":"2021-09-23T12:06:40.476464Z","iopub.status.idle":"2021-09-23T12:06:41.133513Z","shell.execute_reply.started":"2021-09-23T12:06:40.476423Z","shell.execute_reply":"2021-09-23T12:06:41.132728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amp = torchaudio.transforms.AmplitudeToDB('magnitude',top_db=80)\ni1 = (torch.mean(torch.sum(amp(torch.abs(torch.stft(audio_mono1,799)[0])), axis=2).T, axis=0).reshape(20,20))\ni2 = (torch.mean(torch.sum(amp(torch.abs(torch.stft(audio_mono2,799)[0])), axis=2).T, axis=0).reshape(20,20))\ni3 = (torch.mean(torch.sum(amp(torch.abs(torch.stft(audio_mono3,799)[0])), axis=2).T, axis=0).reshape(20,20))\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\nplt.sca(axs[0])\nplt.imshow(i1)\nplt.title(\"Sample \"+class_name1)\nplt.sca(axs[1])\nplt.imshow(i2)\nplt.title(\"Sample \"+class_name2)\nplt.sca(axs[2])\nplt.imshow(i1)\nplt.title(\"Sample \"+class_name3)\nplt.suptitle('Compressed reshaped visu of spectrogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:41.134517Z","iopub.execute_input":"2021-09-23T12:06:41.134762Z","iopub.status.idle":"2021-09-23T12:06:41.576133Z","shell.execute_reply.started":"2021-09-23T12:06:41.134725Z","shell.execute_reply":"2021-09-23T12:06:41.57544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"arrOfImages = []\namp = torchaudio.transforms.AmplitudeToDB('magnitude',top_db=80)\n\ndef specAndResize(i):\n    return  cv2.resize(torch.sum(amp(torch.abs(torch.stft(load_sound_by_index(i, df),799)[0])), axis=2).numpy(),(100,100))\n\nfor i in range(0, df.shape[0]):\n  if (i%500 == 0):\n    print(\"Preprocessed percentage : \"+str(int((i/df.shape[0])*100)))\n  arrOfImages.append(specAndResize(i))\n  \narrOfImages = np.array(arrOfImages)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:06:41.577409Z","iopub.execute_input":"2021-09-23T12:06:41.577706Z","iopub.status.idle":"2021-09-23T12:07:16.048361Z","shell.execute_reply.started":"2021-09-23T12:06:41.57767Z","shell.execute_reply":"2021-09-23T12:07:16.047146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nclass SoundDataset(Dataset):\n  \n    def __init__(self, dataframe, arrayOfImages , transform=None):\n        \"\"\"\n        Args:\n            dataframe (Panda df): dataframe for sounds\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n            images (numpy array 2D) : Array of images (compressed spectrograms)\n        \"\"\"\n        self.sound_frame = dataframe\n        self.transform = transform\n        self.images = arrayOfImages\n\n    def __len__(self):\n        return len(self.sound_frame)\n\n    def filename(self, idx):\n      return os.path.join(MOUNTING_LOCATION+PATH_TO_FOLDER+\"fold\"+str(self.sound_frame[FOLD][idx])+os.path.sep,self.sound_frame[FILENAME][idx]) #self.sound_frame.iloc[idx, 5] self.sound_frame.iloc[idx, 0]\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        spec = self.images.squeeze()[idx]\n        image = np.repeat(spec[..., np.newaxis], 3, -1)\n        #image=spec\n        target = self.sound_frame[CLASSID][idx] #self.sound_frame.iloc[idx, 6]\n\n        #print(image.shape)\n        if self.transform:\n            image = self.transform(image)\n\n        return image, target\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Resize((100,100)), transforms.Normalize(np.mean(arrOfImages),np.std(arrOfImages))])\n\nds = SoundDataset(df,arrOfImages, transform=transform)\ntrain_set, val_test_set = torch.utils.data.random_split(ds, [round(r*0.6), r-round(r*0.6)])\ntest_set, val_set = torch.utils.data.random_split(val_test_set, [int((r-round(r*0.6))*0.5),int((r-round(r*0.6))*0.5)+1])#+1\ntrain_loader = DataLoader(train_set, batch_size=16,shuffle=True, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=16,shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_set, batch_size=16,shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:07:16.049653Z","iopub.status.idle":"2021-09-23T12:07:16.050072Z","shell.execute_reply.started":"2021-09-23T12:07:16.049845Z","shell.execute_reply":"2021-09-23T12:07:16.049868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.wide_resnet50_2(pretrained=True)\n\n#for param in model.parameters():\n#    param.requires_grad = False\n\nclassifier = nn.Sequential(OrderedDict([('fc1', nn.Dropout(0.4)),('fc2', nn.Linear(2048, 10)), ('sf', nn.LogSoftmax())]))\n    \nmodel.fc = classifier\n\ncriterion = nn.NLLLoss()\nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr = 0.0001, amsgrad=True)\n\nepochs = 20\nsteps = 0\nrunning_loss = 0\nloss_tab = []\nval_loss_tab = []\nval_acc_tab = []\nacc_tab = []\naccuracy = 0\nfor epoch in range(epochs):\n    for inputs, labels in train_loader:\n        steps += 1\n        # Move input and label tensors to the default device\n        inputs, labels = inputs.to(device), labels.to(device)\n \n        res = model(inputs)\n        loss = criterion(res, labels)\n        running_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()     \n    \n        top_probs, top_class = torch.topk(torch.exp(res), 1,dim=1)\n        valid_pred = top_class == labels.view(*top_class.shape)\n        count = torch.mean((valid_pred).type(torch.FloatTensor))\n        accuracy += count\n    val_loss = 0\n    val_accuracy = 0\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            res = model(inputs)\n            valR_loss = criterion(res, labels)\n\n            val_loss += valR_loss.item()\n            top_probs, top_class = torch.topk(torch.exp(res), 1,dim=1)\n            valid_pred = top_class == labels.view(*top_class.shape)\n            count = torch.mean((valid_pred).type(torch.FloatTensor))\n            val_accuracy += count\n            \n    print(f\"Epoch {epoch+1}/{epochs}.. \"\n          f\"Train loss: {running_loss/len(train_loader):.3f}.. \"\n          f\"Val loss: {val_loss/len(val_loader):.3f}.. \"\n          f\"Val accuracy: {val_accuracy/len(val_loader):.3f}\")\n    val_loss_tab.append(val_loss/len(val_loader))\n    loss_tab.append(val_loss/len(train_loader))\n    val_acc_tab.append(val_accuracy/len(val_loader))\n    acc_tab.append(accuracy/len(train_loader))\n    accuracy = 0\n    running_loss = 0\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:07:16.051584Z","iopub.status.idle":"2021-09-23T12:07:16.052054Z","shell.execute_reply.started":"2021-09-23T12:07:16.051815Z","shell.execute_reply":"2021-09-23T12:07:16.051838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss = 0\naccuracy = 0\nmodel.eval()\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        # TODO: validation and test_loss update\n        inputs, labels = inputs.to(device), labels.to(device)\n        res = model(inputs)\n        testR_loss = criterion(res, labels)\n\n        test_loss += testR_loss.item()\n        top_probs, top_class = torch.topk(torch.exp(res), 1,dim=1)\n\n        valid_pred = top_class == labels.view(*top_class.shape)\n        # TODO: accuracy update\n        count = torch.mean((valid_pred).type(torch.FloatTensor))\n        accuracy += count\n\nprint(f\"Test accuracy: {accuracy/len(test_loader):.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:07:16.053111Z","iopub.status.idle":"2021-09-23T12:07:16.053696Z","shell.execute_reply.started":"2021-09-23T12:07:16.053455Z","shell.execute_reply":"2021-09-23T12:07:16.053478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics\n","metadata":{}},{"cell_type":"code","source":"# loss curves\nx = np.linspace(0,len(loss_tab), len(val_loss_tab))\nplt.plot(x, loss_tab, label= \"Train loss\")\nplt.plot(x, val_loss_tab, label = \"Test loss\")\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracy curves\nx = np.linspace(0,len(acc_tab), len(val_acc_tab))\nplt.plot(x, acc_tab, label= \"Train acc\")\nplt.plot(x, val_acc_tab, label = \"Val acc\")\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nlabels_tab =[]\npred_tab =[]\nmodel_test = model.to(\"cpu\")\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels\n        res = model_test(inputs)\n        top_probs, top_class = torch.topk(torch.exp(res), 1,dim=1)\n        labels_tab.append(labels.clone().detach().to(\"cpu\"))\n        pred_tab.append(top_class.clone().detach().to(\"cpu\"))\n\n\nclasses = []\nfor values in range(0,10):\n  classes.append(df[df[CLASSID] == values][CLASS].reset_index()[CLASS][0])\n\nfrom sklearn.metrics import confusion_matrix\nlabels = np.concatenate([el.numpy() for el in labels_tab]).ravel()\n\npred = np.concatenate([el.numpy() for el in pred_tab]).ravel()\n\ncm = confusion_matrix(labels, pred)\n\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nplt.imshow(cm, interpolation='nearest')\nplt.title(\"Confusion matrix\")\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=90)\nplt.yticks(tick_marks, classes)\n\nfmt = '.2f' \nthresh = cm.max() / 2.\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:07:16.054978Z","iopub.status.idle":"2021-09-23T12:07:16.055841Z","shell.execute_reply.started":"2021-09-23T12:07:16.055542Z","shell.execute_reply":"2021-09-23T12:07:16.055566Z"},"trusted":true},"execution_count":null,"outputs":[]}]}