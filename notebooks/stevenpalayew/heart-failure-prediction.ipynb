{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T03:32:13.208588Z","iopub.execute_input":"2021-08-30T03:32:13.209027Z","iopub.status.idle":"2021-08-30T03:32:13.241883Z","shell.execute_reply.started":"2021-08-30T03:32:13.208933Z","shell.execute_reply":"2021-08-30T03:32:13.240813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<h1><center>Heart Failure Prediction</center></h1>**<h1>\n<center><img src=\"https://afmc.org/wp-content/uploads/2017/02/heartfailure.jpg\" width=\"600\"></center>","metadata":{}},{"cell_type":"markdown","source":"The dataset studied here has 12 features which can be used to predict mortality by heart failure (which is our label). More info about this dataset can be found here: https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\n\nOther notebooks that helped me prepare this one and which I would like to give credit to:\n\nhttps://www.kaggle.com/shivarajmishra/ann-vs-rest-endgame-for-heart-failure-prediction\n\nhttps://www.kaggle.com/karnikakapoor/heart-failure-prediction-ann\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport hypertools as hyp\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers import Dense, BatchNormalization, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras import callbacks\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:13.243383Z","iopub.execute_input":"2021-08-30T03:32:13.243739Z","iopub.status.idle":"2021-08-30T03:32:40.154708Z","shell.execute_reply.started":"2021-08-30T03:32:13.243706Z","shell.execute_reply":"2021-08-30T03:32:40.153546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Part 1: EDA (Data Quality Check, Exploration, Visualization)</h2>","metadata":{}},{"cell_type":"code","source":"# Load and take a peek at dataset\ndata = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:40.157974Z","iopub.execute_input":"2021-08-30T03:32:40.158502Z","iopub.status.idle":"2021-08-30T03:32:40.20539Z","shell.execute_reply.started":"2021-08-30T03:32:40.15842Z","shell.execute_reply":"2021-08-30T03:32:40.204288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:40.206914Z","iopub.execute_input":"2021-08-30T03:32:40.207204Z","iopub.status.idle":"2021-08-30T03:32:40.216118Z","shell.execute_reply.started":"2021-08-30T03:32:40.207176Z","shell.execute_reply":"2021-08-30T03:32:40.2154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate descriptive statistics\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:40.217055Z","iopub.execute_input":"2021-08-30T03:32:40.217498Z","iopub.status.idle":"2021-08-30T03:32:40.274151Z","shell.execute_reply.started":"2021-08-30T03:32:40.217445Z","shell.execute_reply":"2021-08-30T03:32:40.273146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check degree of class imbalance \ndata.groupby('DEATH_EVENT').size()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:40.275237Z","iopub.execute_input":"2021-08-30T03:32:40.275511Z","iopub.status.idle":"2021-08-30T03:32:40.286742Z","shell.execute_reply.started":"2021-08-30T03:32:40.275484Z","shell.execute_reply":"2021-08-30T03:32:40.285089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3D scatter for PCA using Hypertools\nlabels = data['DEATH_EVENT']\nhyp.plot(data, '.', hue=labels, reduce='PCA',legend=labels.unique().tolist())","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:40.288393Z","iopub.execute_input":"2021-08-30T03:32:40.288744Z","iopub.status.idle":"2021-08-30T03:32:41.032681Z","shell.execute_reply.started":"2021-08-30T03:32:40.288711Z","shell.execute_reply":"2021-08-30T03:32:41.03176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize based on (https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#standardizing)\nhyp.plot(data, '.', hue=labels, reduce='PCA',normalize='within', legend=labels.unique().tolist())","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:41.034668Z","iopub.execute_input":"2021-08-30T03:32:41.03495Z","iopub.status.idle":"2021-08-30T03:32:41.323669Z","shell.execute_reply.started":"2021-08-30T03:32:41.034923Z","shell.execute_reply":"2021-08-30T03:32:41.322662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ncorr=data.corr()\nsns.heatmap(corr)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:41.324994Z","iopub.execute_input":"2021-08-30T03:32:41.325355Z","iopub.status.idle":"2021-08-30T03:32:41.809836Z","shell.execute_reply.started":"2021-08-30T03:32:41.325305Z","shell.execute_reply":"2021-08-30T03:32:41.808773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Part 2: Data Preparation, Model Training and Assessment </h2> ","metadata":{}},{"cell_type":"code","source":"# Seperate data into features, target\ny = (data[\"DEATH_EVENT\"]).to_numpy()\nX_raw= (data.drop([\"DEATH_EVENT\"],axis=1))\n# Getting column names here to be used later\ncol_names=X_raw.columns.tolist()\nX_raw = X_raw.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:41.811236Z","iopub.execute_input":"2021-08-30T03:32:41.81155Z","iopub.status.idle":"2021-08-30T03:32:41.822549Z","shell.execute_reply.started":"2021-08-30T03:32:41.811517Z","shell.execute_reply":"2021-08-30T03:32:41.820728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train, Test Split. Then normalize features (note - we fit the transform on training data, then apply to train and test data in order to avoid data leakage)\nX_raw_train, X_raw_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=1) \n# Fit transform on training data\nscaler=StandardScaler()\nscaler.fit(X_raw_train)\n# Normalize train and test data by applying fitted transform\nX_train = scaler.transform(X_raw_train)\nX_test = scaler.transform(X_raw_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:41.824612Z","iopub.execute_input":"2021-08-30T03:32:41.824954Z","iopub.status.idle":"2021-08-30T03:32:41.835911Z","shell.execute_reply.started":"2021-08-30T03:32:41.824924Z","shell.execute_reply":"2021-08-30T03:32:41.83467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test decision tree on test data for baseline, training on rest of the data (don't need validation split because not tuning hyperparameters)\n# Train model\nDT_model = DecisionTreeClassifier(random_state=0)\nDT_model.fit(X_train, y_train)\n# Test model measuring accuracy and MCC, display results\nDT_MCC = matthews_corrcoef(y_test, DT_model.predict(X_test))\nDT_ACC = accuracy_score(y_test, DT_model.predict(X_test))\nprint(DT_MCC)\nprint(DT_ACC)\n\n\n# See whether normalizing our data had an impact (by training and testing on data that wasn't normalized)\nDT_model_raw = DecisionTreeClassifier(random_state=0)\nDT_model_raw.fit(X_raw_train, y_train)\nprint(matthews_corrcoef(y_test, DT_model_raw.predict(X_raw_test)))\nprint(accuracy_score(y_test, DT_model_raw.predict(X_raw_test)))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:41.837841Z","iopub.execute_input":"2021-08-30T03:32:41.83824Z","iopub.status.idle":"2021-08-30T03:32:41.863847Z","shell.execute_reply.started":"2021-08-30T03:32:41.838206Z","shell.execute_reply":"2021-08-30T03:32:41.862731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train ANN/NN (neural network). Use early stopping to mitigate overfitting. Use sklearn wrapper to be able to more easily investigate important features. \n# Note that sometimes training will get stuck at validation accuracy of around 0.7083. If this happens, re-run this cell. \n# Define early stopping behaviour\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, \n    patience=20, \n    restore_best_weights=True)\n\n# Function to create model \ndef create_model():\n    model = Sequential()\n    \n    model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n    model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model\n\n# Create model using Keras wrapper for sklearn\nmodel = KerasClassifier(build_fn=create_model)\n# Train model, display results of training\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 500, validation_split=0.2, callbacks=[early_stopping])\nval_accuracy = np.mean(history.history['val_accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_accuracy', val_accuracy*100))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:41.865134Z","iopub.execute_input":"2021-08-30T03:32:41.865408Z","iopub.status.idle":"2021-08-30T03:32:49.514824Z","shell.execute_reply.started":"2021-08-30T03:32:41.865381Z","shell.execute_reply":"2021-08-30T03:32:49.51334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test NN on test data\nNN_MCC = (matthews_corrcoef(y_test, model.predict(X_test)))\nNN_ACC = (accuracy_score(y_test, model.predict(X_test)))\n\nprint(NN_MCC)\nprint(NN_ACC)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:49.516435Z","iopub.execute_input":"2021-08-30T03:32:49.51692Z","iopub.status.idle":"2021-08-30T03:32:49.730159Z","shell.execute_reply.started":"2021-08-30T03:32:49.51687Z","shell.execute_reply":"2021-08-30T03:32:49.729271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A quick visualization comparing model performance. Save to PDF. \ndict_to_plot = {\"DT_MCC\": DT_MCC, \"DT_ACC\": DT_ACC, \"NN_MCC\": NN_MCC, \"NN_ACC\": NN_ACC}\ncolor_to_plot = ['black', 'blue']\nf=plt.figure()\nplt.bar(dict_to_plot.keys(), dict_to_plot.values(), color=color_to_plot)\nplt.title(\"Performance of DT (Decision Tree) and NN (Neural Network)\")\nf.savefig(\"Results.pdf\", bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:49.731364Z","iopub.execute_input":"2021-08-30T03:32:49.731853Z","iopub.status.idle":"2021-08-30T03:32:50.018156Z","shell.execute_reply.started":"2021-08-30T03:32:49.73181Z","shell.execute_reply":"2021-08-30T03:32:50.016596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Part 3: Potentially Important Features </h2>","metadata":{}},{"cell_type":"code","source":"# Having seen that performance is relatively good, and competitive with decision tree model, use neural network model to investigate potentially important features through finding permutation importances\nperm = PermutationImportance(model, random_state=1).fit(X_test,y_test)\neli5.show_weights(perm, feature_names = col_names)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:32:50.020862Z","iopub.execute_input":"2021-08-30T03:32:50.021267Z","iopub.status.idle":"2021-08-30T03:32:53.86188Z","shell.execute_reply.started":"2021-08-30T03:32:50.021232Z","shell.execute_reply":"2021-08-30T03:32:53.860496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Thank you for your time! Hope you learned something :) </h2>","metadata":{}}]}