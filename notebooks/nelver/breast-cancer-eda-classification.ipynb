{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nWe will begin with getting the basic idea about dataset (i.e., checking nulls, univariate distributions, distributions conditional on target variable). Then we will proceed by looking into which features are the most important at predicting the malign/benign cancer. To estimate feature importance we will use two techniques: t-test and random forest feature importance test. We will check whether estimations given by the aforementioned tests agree with each other. Finally, we will train some classification models to see how accurately we can predict whether the cancer is benign or malign. Let’s begin:"},{"metadata":{},"cell_type":"markdown","source":"# Import relevant libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport math\nfrom scipy import special #comb, factorial\nfrom keras import backend as K\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that `Unnamed: 32` only contain null values. We will remove the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 32'],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the distribution of our target variable, i.e., the distribution of  `diagnosis`:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalized distirbution of `diagnosis`"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['diagnosis'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we deal with disbalanced dataset, so when assesing the performance of our classification models, we will be using metrics other than accuracy.\n\nNow let's check the univariate distributions of our features (one can see that all of our features are numeric):"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Check the UNIVARIATE distributions\ncont_features = df.drop(['id','diagnosis'],axis=1).columns\nWIDTH = 16\nLENGTH = 40\n\nrows = math.ceil(len(cont_features)/3)\nfig, ax = plt.subplots(rows,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    ax[i].hist(df[feature],alpha=0.6)\n    ax[i].set_title(f'Distribution of a feature `{feature}`')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check the conditional distributions of each numeric feature (conditional on our taget variable, `diagnosis`):"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# BOX\ncont_features = df.drop(['id','diagnosis'],axis=1).columns\ncat_variable = 'diagnosis'\nWIDTH = 16\nLENGTH = 50\n\nrows = math.ceil(len(cont_features)/3)\nfig, ax = plt.subplots(rows,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i])\n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Visually, it seems that most of numeric features seem to be having a very decent predicting power of a target variable."},{"metadata":{},"cell_type":"markdown","source":"Now let's use more rigorous approache to assess the relation between each numeric feature and target variable. For that we will t-test for independence. We will do the following:\n1. Run t-test for independence (independence between each numeric feature and `diagnosis`)\n3. Check whether there are some features that **passed** independence test (we say that a numeric feature $X$ passed an independence test if we cannot reject the null hypothesis. The null hypothesis is: Numeric feature $X$ and categorical variable `diagnosis` are independent).\n4. Consider all the features that failed independence test (i.e., null hypothesis is rejected). For each test, rank the features (the smaller $p$-value, the higher the rank). Which features have the best predicting power (i.e., have the highest rank or the smallest $p$-value)?"},{"metadata":{},"cell_type":"markdown","source":"# The independent samples t-test"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from scipy.stats import ttest_ind\n\ncont_features = df.drop(['id','diagnosis'],axis=1).columns\n\nlabel = 'diagnosis'\ndic = {'Categorical': [],\n    'Numerical': [],\n    'p-value': [],\n    'p < 0.05': [],\n    't-statistic': []}\n\n\nassert df[label].unique().size == 2, 'Label must only contain two unique values!'\n\nfor feature in cont_features:\n    value_1 = df[label].unique()[0]\n    value_2 = df[label].unique()[1]\n    \n    a = df[df[label] == value_1][feature].values\n    b = df[df[label] == value_2][feature].values\n    \n    statistic, pval = ttest_ind(a,b)\n    \n    dic['Categorical'].append(label)\n    dic['Numerical'].append(feature)\n    dic['p-value'].append(pval)\n    dic['p < 0.05'].append(pval<0.05)\n    dic['t-statistic'].append(statistic)\n\n\nttest_df = pd.DataFrame(dic)\nttest_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many features passed independence test? (Feature $X$ has passed an independence test if and only if $p ≥ 0.05$)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ttest_df['p < 0.05'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we have only $5$ features that passed an independence test ($p$ value is larger than $0.05$). Let's have a look at the features that have passed the test"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Following features have passed t-test:\")\nprint([x for x in ttest_df[ttest_df['p < 0.05'] == False]['Numerical'].values])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fact that we failed to reject the null hypothesis for these 5 features imply that the features are not doing good job at discerning benign and malign cancer. Let's visualize the box plot for these features one more time."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"low_features = ttest_df[ttest_df['p < 0.05'] == False]['Numerical'].values\n\n\n# BOX\ncont_features = low_features\ncat_variable = 'diagnosis'\nWIDTH = 27\nLENGTH = 5\n\nrows = math.ceil(len(cont_features)/3)\nfig, ax = plt.subplots(1,5,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i],showfliers=False)\n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And indeed, we can see that the conditional distributions for each numeric feature are very similar.\n\nNow we move onto the features the failed independence test (i.e., $p$ value is smaller than $0.05$). We will rank each feature according to its $p$-value: the smaller the $p$-value, the higher the rank (e.g., the highest rank is $1$, the next highest rank is $2$ etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ttest_df = ttest_df.sort_values(by='p-value',ascending=True).reset_index().reset_index().drop('index',axis=1)\nttest_df.rename(columns={'level_0':'Rank'},inplace=True)\nttest_df['Rank'] += 1\nttest_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the conditional distributions of the top 6 features."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# BOX\ncont_features = ttest_df[ttest_df['Rank'] <= 6]['Numerical'].values\ncat_variable = 'diagnosis'\nWIDTH = 20\nLENGTH = 12\n\nrows = math.ceil(len(cont_features)/3)\nfig, ax = plt.subplots(2,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i],showfliers=False)\n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can compare the graphs of the top 6 features with the lowest p-value with those features that passed and independence test. Top 6 features have staggeringly different conditional distributions (signified by the fact that the yellow and blue boxes are very clearly separated), but for the features that passed an independence test, the conditional distributions are hardly distinguishable (i.e., the boxes are of rouhgly the same shape and are on roughly the same level)."},{"metadata":{},"cell_type":"markdown","source":"Now let's estimate feature importance using Random Forest, and let's compare top 6 features selected by t-test and top 6 features selected by random forest."},{"metadata":{},"cell_type":"markdown","source":"# Feature importance estimation via random forest"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X = df.drop(['id','diagnosis'],axis=1).copy()\ny = df['diagnosis'].copy()\n\n\nforest_clf = RandomForestClassifier(n_estimators=100)\nforest_clf.fit(X, y)\n\nimportances = forest_clf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(7,7))\nplt.bar(range(len(indices)),importances[indices])\nplt.xticks(range(len(indices)), indices)\nplt.title(\"Feature importance (Random Forest)\")\nplt.xlabel('Index of a feature')\nplt.ylabel('Feature importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top6_tt = set(ttest_df[ttest_df['Rank'] <= 6]['Numerical'].values)\ntop6_rf = set(np.array(X.iloc[:,indices[:6]].columns))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 6 features selected by t-test"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"top6_tt","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"Top 6 features selected by random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"top6_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that top 6 features selected by RF and t-test are identical, which implies that these 6 features are indeed good predictors of our target variable."},{"metadata":{},"cell_type":"markdown","source":"Now we will try to classify. Since we are only dealing with the numeric features and a lot of those features are (roughly) normally distributed, we will scale the features using `StandardScaler`. Our training will have 2 stages:\n\nStage 1. We use **all** numeric features present in the dataset.\n\nStage 2. We only use 6 top features.\n\nAfter training, we will compare the performance of our models based on the features used. Due to the fact that our target label is disbalanced, we will use macro f1 score to evaluate the performance of our models."},{"metadata":{},"cell_type":"markdown","source":"# Training:  All features"},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"X = df.drop(['id','diagnosis'],axis=1).copy()\ny = df['diagnosis'].copy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=11)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n\ndef f1_macro(clf,X_train=X_train,\n             y_train=y_train,\n             X_test=X_test,\n             y_test=y_test):\n    \n    clf.fit(X_train,y_train)\n    pred = clf.predict(X_test)\n    return f1_score(y_pred=pred,y_true=y_test,average='macro')\n\n\nmodels = {'GB':GaussianNB(),\n          'Logistic': LogisticRegression(random_state=11,max_iter=4000),\n          'SVM': SVC(), \n          'KNN': KNeighborsClassifier()}\n\nmodel_name = [x for x in models]\nf1_macro = [round(f1_macro(models[x]),2) for x in model_name]\n\n\n\ncat_features = model_name\n\ncount = np.array(f1_macro)\n\nto_sort = np.argsort(count)[::-1]\ncat_features = np.array(cat_features)[to_sort]\ncount = count[to_sort]\n\nplt.figure(figsize=(11,6))\ngraph = sns.barplot(cat_features,count)\nfor p in graph.patches:\n    graph.annotate(p.get_height(), (p.get_x()+0.4, p.get_height()),\n                   ha='center', va='bottom',\n                   color= 'black')\n\n\nplt.title(\"Performance of the models (all features)\")\nplt.xticks(rotation=45)\nplt.ylabel('f1 score (macro)')\nplt.xlabel('Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training:  Top 6 features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"top_6 = ['perimeter_worst', 'radius_worst', 'concave points_worst', 'area_worst',\n       'concave points_mean', 'perimeter_mean']\n\nX = df[top_6]\ny = df['diagnosis'].copy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=11)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n\ndef f1_macro(clf,X_train=X_train,\n             y_train=y_train,\n             X_test=X_test,\n             y_test=y_test):\n    \n    clf.fit(X_train,y_train)\n    pred = clf.predict(X_test)\n    return f1_score(y_pred=pred,y_true=y_test,average='macro')\n\n\nmodels = {'GB':GaussianNB(),\n          'Logistic': LogisticRegression(random_state=11,max_iter=400),\n          'SVM': SVC(), \n          'KNN': KNeighborsClassifier()}\n\nmodel_name = [x for x in models]\nf1_macro = [round(f1_macro(models[x]),2) for x in model_name]\n\n\n\ncat_features = model_name\n\ncount = np.array(f1_macro)\n\nto_sort = np.argsort(count)[::-1]\ncat_features = np.array(cat_features)[to_sort]\ncount = count[to_sort]\n\nplt.figure(figsize=(11,6))\ngraph = sns.barplot(cat_features,count)\nfor p in graph.patches:\n    graph.annotate(p.get_height(), (p.get_x()+0.4, p.get_height()),\n                   ha='center', va='bottom',\n                   color= 'black')\n\n\nplt.title(\"Performance of the models (top 6 features)\")\nplt.xticks(rotation=45)\nplt.ylabel('f1 score (macro)')\nplt.xlabel('Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that performance becomes better when we use only 6 features. We should also should note that we got very decent results even while using models with **default** hyperparameters."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}