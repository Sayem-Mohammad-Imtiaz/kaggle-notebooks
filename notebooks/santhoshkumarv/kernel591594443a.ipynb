{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\n#from sklearn.cross_validation import train_test_split\nfrom tensorflow.python.data import Dataset\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\n#Read the data from csv file\ndf = pd.read_csv('../input/covtype.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select predictors\nx = df[df.columns[:54]]\n#Target variable \ny = df.Cover_Type\n#Split data into train and test \nx_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.7, random_state =  90)\n'''As y variable is multi class categorical variable, hence using softmax as activation function and sparse-categorical cross entropy as loss function.'''\nmodel = keras.Sequential([\n keras.layers.Dense(64, activation=tf.nn.relu,                  \n input_shape=(x_train.shape[1],)),\n keras.layers.Dense(64, activation=tf.nn.relu),\n keras.layers.Dense(8, activation=  'softmax')\n ])\n\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nhistory1 = model.fit(\n x_train, y_train,\n epochs= 26, batch_size = 60,\n validation_data = (x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\ndf = pd.read_csv('../input/covtype.csv')\nx = df[df.columns[:55]]\ny = df.Cover_Type\nx_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.7, random_state =  90)\n#Select numerical columns which needs to be normalized\ntrain_norm = x_train[x_train.columns[0:10]]\ntest_norm = x_test[x_test.columns[0:10]]\n# Normalize Training Data \nstd_scale = preprocessing.StandardScaler().fit(train_norm)\nx_train_norm = std_scale.transform(train_norm)\n#Converting numpy array to dataframe\ntraining_norm_col = pd.DataFrame(x_train_norm, index=train_norm.index, columns=train_norm.columns) \nx_train.update(training_norm_col)\nprint (x_train.head())\n# Normalize Testing Data by using mean and SD of training set\nx_test_norm = std_scale.transform(test_norm)\ntesting_norm_col = pd.DataFrame(x_test_norm, index=test_norm.index, columns=test_norm.columns) \nx_test.update(testing_norm_col)\nprint (x_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n keras.layers.Dense(64, activation=tf.nn.relu,                  \n input_shape=(x_train.shape[1],)),\n keras.layers.Dropout(0.5),\n keras.layers.Dense(64, activation=tf.nn.relu),\n keras.layers.Dropout(0.5),\n keras.layers.Dense(8, activation=  'softmax')\n ])\n\nmodel.compile(optimizer=tf.train.AdamOptimizer (learning_rate=0.0001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nhistory2 = model.fit(\n x_train, y_train,\n epochs= 5, batch_size = 60,\n validation_data = (x_test, y_test))","execution_count":7,"outputs":[{"output_type":"stream","text":"Train on 406708 samples, validate on 174304 samples\nEpoch 1/5\n406708/406708 [==============================] - 23s 57us/sample - loss: 0.6497 - acc: 0.7639 - val_loss: 0.0887 - val_acc: 0.9721\nEpoch 2/5\n406708/406708 [==============================] - 23s 56us/sample - loss: 0.1370 - acc: 0.9549 - val_loss: 0.0335 - val_acc: 0.9893\nEpoch 3/5\n406708/406708 [==============================] - 24s 59us/sample - loss: 0.0685 - acc: 0.9770 - val_loss: 0.0169 - val_acc: 0.9929\nEpoch 4/5\n406708/406708 [==============================] - 24s 58us/sample - loss: 0.0441 - acc: 0.9850 - val_loss: 0.0102 - val_acc: 0.9970\nEpoch 5/5\n406708/406708 [==============================] - 23s 57us/sample - loss: 0.0327 - acc: 0.9891 - val_loss: 0.0068 - val_acc: 0.9982\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}