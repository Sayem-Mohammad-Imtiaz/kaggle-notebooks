{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://monkeylearn.com/static/6700dcab9bcc691104dd0d794f6e7ef4/Sentiment-analysis-of-Twitter-Social.png\" length=\"100px\" width=\"1000px\">"},{"metadata":{},"cell_type":"markdown","source":"# Twitter sentiment analysis - Extracting emotions through machine learning\n* Sentiment analysis is the automatic manner of analyzing textual content and sorting it into sentiments positive, negative, or neutral. Sentiment analysis is used to investigate opinions in Twitter data and it can assist organizations to analyse and understand about their brand. Twitter Sentiment Analysis is a stream of Social network analysis. This is also known as opinion mining"},{"metadata":{},"cell_type":"markdown","source":"* We import numpy, pandas, matplotlib, and seaborn for data analysis and visualization\n* We import wordcloud for EDA\n* We import re, string, nltk, spacy for tweets cleaning\n* We import sklearn and Xgboost for machine learning and metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# setting up the background style for the plots\nplt.style.use('fivethirtyeight')\n\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\n\nimport spacy\nnlp = spacy.load('en')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\nprint('Train dataset shape: ', train_df.shape)\nprint('Train dataset columns: ', train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selecting necessary features from train data\ntrain_df = train_df[['label','tweet']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Labbel Categories count\nsns.countplot(x= train_df['label'])\nplt.title('Label Counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* by the above plot we can understand that in our data most of the data labelled as 0 with means positive sentiment, on the other hand label = 1 means negative sentiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nsentences = train_df['tweet'].tolist()\nsentences_ss = \" \".join(sentences)\n\nplt.figure(figsize=(10,10))\nplt.imshow(WordCloud().generate(sentences_ss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* With the help of wordcloud we can find most repeated words easily from the whole dataset. By the above image we can come to a conclusion that in our data 'user' is the word which repeated more number of times. And we have some unknown symbols also in our data. So, befor proceeding with modelling, we should first clean the data."},{"metadata":{},"cell_type":"markdown","source":"### Word Frequency Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['tweet'].apply(len).plot(bins=100, kind = 'hist')\nplt.title('Length frequency of all tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above histogram shows us the length frequency of the each tweet from the dataset"},{"metadata":{},"cell_type":"markdown","source":"### Spliting dataset into input(X) and target(y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df.iloc[:,[1]]\ny = train_df.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning of Tweets\n\nIn this cleaning process we are going to do the following steps\n* removing @usernames \n* converting all words into lowercase\n* removing numbers\n* removing punctuations\n* removing URLs\n* tokanize each tweet -> Tokenization is a way of separating a piece of text (sentence) into smaller units called tokens.\n* remove while spaces\n* remove stop words -> “stop words” usually refers to the most common words in a language, like is, are, the, a, an, and so on\n* removing non alphabetics tokens\n* performing lemmatization -> remove inflectional endings only i.e. running, runner == run"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove @usernames\nX['tweet'] = X['tweet'].map(lambda x: re.sub('@\\S+', ' ', x))\n\n#all lower case\nX['tweet']=X['tweet'].map(lambda x: x.lower())\n\n# Remove numbers\nX['tweet'] = X['tweet'].map(lambda x: re.sub(r'\\d+', ' ', x))\n\n# Remove Punctuation\nX['tweet']  = X['tweet'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n\n# Remove urls\nurl_cleaner = \"https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\nX['tweet'] = X['tweet'].map(lambda x: re.sub(url_cleaner, ' ', x))\n\n# Remove white spaces\nX['tweet'] = X['tweet'].map(lambda x: x.strip())\n\n# Tokenizer\nX['tweet'] = X['tweet'].map(lambda x: word_tokenize(x))\n\n# Remove stop words\nstop_words = set(stopwords.words('english'))\nX['tweet'] = X['tweet'].map(lambda x: [w for w in x if not w in stop_words])\n\n# Remove non alphabetic tokens\nX['tweet'] = X['tweet'].map(lambda x: [w for w in x if w.isalpha()])\n\n# turning back to string\nX['tweet'] = X['tweet'].map(lambda x: ' '.join(x))\n\n#Lemmatizer\nX['tweet']=X[\"tweet\"].apply(lambda x: \" \".join([w.lemma_ for w in nlp(x)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n\ndocuments = [_text.split() for _text in X.tweet] \nw2v_model = gensim.models.word2vec.Word2Vec(size=200, \n                                            window=5, \n                                            min_count=3, \n                                            workers=6)\nw2v_model.build_vocab(documents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding Similar words"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.train(documents, total_examples=len(documents), epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test word embeddings\nw2v_model.most_similar(\"job\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corpus = []\n\nfor i in range(0, 31962):\n  review = re.sub('[^a-zA-Z]', ' ', X['tweet'][i])\n  review = review.split()\n  \n  ps = PorterStemmer()\n  \n  # stemming\n  review = [ps.stem(word) for word in review]\n  \n  # joining them back with space\n  review = ' '.join(review)\n  train_corpus.append(review)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CountVectorizer\n* CountVectorization is usedd to create a sparse matrix representing all the words in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(max_features = 3000)\nx = cv.fit_transform(train_corpus).toarray()\n\nprint(x.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.20, random_state = 46)\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_val = sc.transform(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred_dt = model.predict(x_val)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_val, y_val))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred_rf = model.predict(x_val)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred_xg = model.predict(x_val)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the f1 score for the validation set\nprint(\"f1 score :\", f1_score(y_val, y_pred_rf))\n\n# confusion matrix\ncm = confusion_matrix(y_val, y_pred_rf)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n* Twitter Sentiment analysis helps to monitor and analyse the emotions of human beings based on their tweets. \n* In this project work, Random forest classifier, Decission tree classifier, and XGBoost classifiers are used to perform Twitter sentiment analysis, out of these 3 algorithms Random Forest classifier works well and it prediction score is 95.13%.\n* nltk and Spacy has been used in this project to do the natural language processing job.\n* In future, we can extract data directly from twitter through api and we can analyze with the random forest.\n* We can also improve accuracy by doing prameter tuning using gridsearcv or randomizedsearchcv"},{"metadata":{},"cell_type":"markdown","source":"### Reference:\n\n* https://github.com/sharmaroshan/Twitter-Sentiment-Analysis/blob/master/Twitter_Sentiment.ipynb\n* https://www.kaggle.com/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk\n* https://towardsdatascience.com/twitter-sentiment-analysis-classification-using-nltk-python-fa912578614c\n* https://monkeylearn.com/blog/sentiment-analysis-of-twitter/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}