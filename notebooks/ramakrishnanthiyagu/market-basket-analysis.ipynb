{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#for performing MBA\nfrom mlxtend.frequent_patterns import apriori, association_rules","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading data\ndata=pd.read_csv('../input/groceries/groceries - groceries.csv')\nprint(f'Shape of data: {data.shape[0]} rows and {data.shape[1]} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check a glance of dataset\npd.set_option('max_columns', 35)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Top 20 frequently buying items by customer\nplt.rcParams['figure.figsize']=20,7\nsns.countplot(data=data, x=data['Item 1'],\n             order = data['Item 1'].value_counts().head(20).index,\n             palette='cool')\nplt.xticks(rotation=90)\nplt.xlabel('Product')\nplt.title('Top 20 frequently bought products')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#let's check popular products\nfrom wordcloud import WordCloud\n\nplt.rcParams['figure.figsize'] = (15, 15)\nwordcloud = WordCloud(width = 2000,  height = 1000, max_words = 20, colormap='cool').generate(str(data['Item 1']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Popular products',fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#let's create a list of list of transaction\nrecords = []\nfor i in range(0, len(data)):\n    records.append([str(data.values[i,j]) for j in range(1, data.values[i, 0]+1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's analyze the no. of items wrt each transaction\ncounts = [len(record) for record in records]\nprint(f'50 percent of the transactions are having items below or equal to {np.quantile(counts, .5)} only')\nprint(f'particular transaction having a maximum of {np.quantile(counts, 1)} items in it')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's encode the created list of list like onehot encode\n\nfrom mlxtend.preprocessing import TransactionEncoder\n\nte = TransactionEncoder()\nonehot = te.fit_transform(records)\nonehot = pd.DataFrame(onehot, columns = te.columns_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Shape of encoded onehot dataset\nprint(f'Shape of encoded data: {onehot.shape[0]} rows and {onehot.shape[1]} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Due to complexity we are reducing the no of items by selecting particular items alone\nonehot = onehot.loc[:, ['bottled beer', 'bottled water', 'brandy', 'brown bread', 'butter', 'syrup',\n                    'sweet spreads', 'beverages', 'berries', 'beef', 'bathroom cleaner', 'baking powder', 'bags',\n                    'baby food', 'baby cosmetics', 'Instant food products', 'tea', 'toilet cleaner', 'vinegar', 'waffles', 'whisky',\n                    'white bread', 'white wine', 'yogurt', 'zwieback', 'whole milk', 'whipped/sour cream', 'abrasive cleaner']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of encoded onehot dataset after reducing no. ofitems\nprint(f'Shape of encoded data: {onehot.shape[0]} rows and {onehot.shape[1]} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's generate the frequent itemset using apriori with minimum support of 0.01%\n#maximum item per transaction restricted to 3\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\nfrequent_itemsets=apriori(onehot, min_support=0.0001, use_colnames=True, max_len=3)\nfrequent_itemsets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generating Association rule with mlxtend's association_rules\nrules=association_rules(frequent_itemsets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rules.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Pruning the generated rules using multiple filters\n\ntargeted_rules = rules[rules['antecedents'] == {'baby food'}].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfiltered_rules = targeted_rules[(targeted_rules['confidence'] > 0.85) &\n                                (targeted_rules['lift'] > 1.00)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's analyse the 'baby foods' relevent consequents\n\nsupport_table = filtered_rules.pivot(index='consequents', columns='antecedents', values='lift')\nsns.heatmap(support_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rules['antecedent'] = rules['antecedents'].apply(lambda antecedent: list(antecedent)[0])\nrules['consequent'] = rules['consequents'].apply(lambda consequent: list(consequent)[0])\nrules['rule'] = rules.index\ncoords = rules[['antecedent','consequent','rule']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate parallel coordinates plot\n\nfrom pandas.plotting import parallel_coordinates\n\nplt.rcParams['figure.figsize']=10,15\nparallel_coordinates(coords,'rule', colormap = 'ocean')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}