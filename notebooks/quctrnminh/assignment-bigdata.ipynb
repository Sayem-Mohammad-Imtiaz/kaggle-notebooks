{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bài tập Dữ liệu lớn\n\n### Giảng viên hướng dẫn\nPGS.TS Thoại Nam\n\n\n### Thành viên nhóm:\n+ Trần Thế Huy (1770021)\n+ Trần Minh Quốc (1870322)\n+ Lê Văn Duẫn (1870387)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Tổng quan\n\nKhi tình hình dịch bệnh Covid-19 diễn biến phức tạp, số lượng tài liệu nghiên cứu về dịch bệnh cũng ngày càng nhiều, gây khó khăn cho việc tìm kiếm thông tin. Gom nhóm các tài liệu có liên quan nhằm giúp các nhà nghiên cứu dễ dàng truy xuất thông tin, tránh lãng phí thời gian công sức trở nên vô cùng cần thiết.\n\nPhương pháp giải quyết bài toán của nhóm sử dụng các thuật toán chính:\n+ Tiền xử lý dữ liệu bằng cách loại bỏ stopwords\n+ Vector hóa tài liệu bằng TF-IDF\n+ Giảm số chiều feature với PCA\n+ Gom cụm tài liệu với K-Means","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport json\nimport glob\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Meta","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv', dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Json\n+ Concatenate text segments in each json into a long string\n+ Extract features\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_json = glob.glob('../input/CORD-19-research-challenge/document_parses/pdf_json/*.json', recursive=True)\nlen(all_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FileReader:\n    def __init__(self, path):\n        with open(path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = '\\n'.join([part['text'] for part in content['abstract']])\n            self.body_text = '\\n'.join([part['text'] for part in content['body_text']])\n            \n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n    \n\nFileReader(all_json[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\n\nall_json = all_json[:20000]\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n        \n    dict_['abstract'].append(content.abstract)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    \n# df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove duplicates\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['body_text'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop nulls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_covid.sample(10000, random_state=0)\ndf.info()\ndel df_covid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Language Detection\n\nRemove documents that are not in English\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text\nfor ii in tqdm(range(0,len(df))):\n    # split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\n\nlanguages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['language'] = languages\nplt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\nplt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\nplt.title(\"Distribution of Languages in Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['language'] == 'en'] \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove stopwords\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the spacy bio parser\n\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NLP \nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_sci_lg  # model downloaded in previous step","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parser\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot(df['body_word_count'])\ndf['body_word_count'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['body_unique_words'])\ndf['body_unique_words'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df[\"processed_text\"] = df[\"body_text\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorization\n\nUse TF-IDF to turn a documents into a vector of importance of words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorize(text, maxx_features):\n    vectorizer = TfidfVectorizer(max_features=maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df['processed_text'].values\nX = vectorize(text, 2 ** 12)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA and K-Means\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95, random_state=42)\nX_reduced= pca.fit_transform(X.toarray())\nX_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\n\n# run kmeans with many different k\ndistortions = []\nK = range(2, 50)\nfor k in K:\n    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n    k_means.fit(X_reduced)\n    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n    #print('Found distortion for {} clusters'.format(k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_line = [K[0], K[-1]]\nY_line = [distortions[0], distortions[-1]]\n\n# Plot the elbow\nplt.plot(K, distortions, 'b-')\nplt.plot(X_line, Y_line, 'r')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}