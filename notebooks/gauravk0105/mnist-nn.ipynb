{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport keras\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras import optimizers\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom matplotlib import pyplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing mnist data\n\n# Training data\ndf_train = pd.read_csv('../input/mnist_train.csv')\n\n# Test data\ndf_test = pd.read_csv('../input/mnist_test.csv')\n\n# containing 60000 images each of shape (28*28*1) \n# row 1 -- [category, (28*28) i.e. image pixels in a entire row]\nprint(df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Validation split from training data\ndf_features = df_train.iloc[:, 1:785]     # pixels of image (28*28*1)\ndf_label = df_train.iloc[:, 0]            # label associated with the image\n\n# splitting training dataset into train and cross validation data\n# so that we can use training data to build several models with different different parameters (learning rate[alpha], adam optimization parameter[beta])\n# i.e. to tune model on different hyperparamter\n# and validation data to select the best model out of them which give better result on validation dataset.\nX_train, X_cv, y_train, y_cv = train_test_split(df_features, df_label, test_size = 0.2, random_state = 1212)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalization : Very Important (basically to speed up gradient descent an algorithm used to train model)\n#  generally speeds up learning and leads to faster convergence.\n# So in image best way to do normalization is to divide each pixel by 255 so that their standard deviation become zero  \nX_train = X_train.values.astype('float32')/255.\nX_cv = X_cv.values.astype('float32')/255.\n\n# applying normalization similarily on test data \nX_test = df_test.iloc[:, 1:785]\ny_test = df_test.iloc[:, 0]\nX_test = X_test.values.astype('float32')/255.\n\n\n# Convert labels to One Hot Encoded \n# in case of label 5 -- [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n# it will be helpful in case of vectorization approach which keras inbuilt uses and so helpful in doing vectorized calculation in one go\ny_train = keras.utils.to_categorical(y_train, 10)\ny_cv = keras.utils.to_categorical(y_cv, 10)\ny_test = keras.utils.to_categorical(y_test, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the shape\n\n# shape of training data\nprint (\"Shape of X_train: \", X_train.shape)\nprint (\"Shape of Y_train: \", y_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of validation data\n\nprint (\"Shape of X_cv: \", X_cv.shape)\nprint (\"Shape of Y_cv: \", y_cv.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of test data\n\nprint (\"Shape of X_test: \", X_test.shape)\nprint (\"Shape of Y_test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display image\n\nimage_X_train=X_train.reshape(48000, 28*28*1)\nplt.imshow(np.resize(image_X_train[4], (28, 28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(np.resize(image_X_train[331], (28, 28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(np.resize(image_X_train[2991], (28, 28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(np.resize(image_X_train[35547], (28, 28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(np.resize(image_X_train[6534], (28, 28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model : ADAM optimizater + 4 hidden + softmax output + learning rate (alpha=0.1) + dropout \n\n# Input Parameters\nn_input = 784 # number of features which is input layer\nn_hidden_1 = 300 # number of neuron in hidden layer 1 \nn_hidden_2 = 100 # number of neuron in hidden layer 2 \nn_hidden_3 = 100 # number of neuron in hidden layer 3\nn_hidden_4 = 200 # number of neuron in hidden layer 4\nnum_digits = 10  # number of node in output layer which is softmax layer\n\n# Insert Hyperparameters\nlearning_rate = 0.01  # hyperparameter in Gradient desent algorithm (represents the steps will take in order to converge)\ntraining_epochs = 30  # one iteration through training set\nbatch_size = 100  # dividing training data in batches for faster convergence\ndropout_factor = 0.3 # produce an effect of regularization (so it bascially shutdown few neurons in each of the hidden layer randomly for each of\n                     # the training example. So let consider for a patricular training sample t1, for that specific we are working with few number of neurons\n                     # meaning that we are having less number of parameter to train which led to finding out less complex hypothesis which will probably be\n                     # less prone to overfitting\n'''\n for a 1 training example\n we are having 784 features/nodes in input layer  \n                                                 [feature weights], [bias] \n hidden layer 1 -> 300 nodes, so # of weights are = (300*784 + 300*1)\n hidden layer 2 -> 100 nodes, so # of weights are = (100*300 + 100*1)\n hidden layer 3 -> 100 nodes, so # of weights are = (100*100 + 100*1)\n hidden layer 4 -> 200 nodes, so # of weights are = (200*100 + 200*1)\n \n output layer -> 10 nodes, so # of weights are = (10*200 + 10*1)\n'''            \n\n'''\nWhy relu?: because in sigmoid or in tanH activation function for higher value the derivative (slope) become zero and so learning becomes slow.\n'''\n\nInp = Input(shape=(784,))\nx = Dense(n_hidden_1, activation='relu',name = \"Hidden_Layer_1\")(Inp)\nx = Dropout(dropout_factor)(x)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dropout(dropout_factor)(x)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dropout(dropout_factor)(x)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\n\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)\n\nmodel = Model(Inp, output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size = batch_size, verbose=1, epochs = training_epochs, validation_data=(X_cv, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_input = 784 # number of features which is input layer\nn_hidden_1 = 300 # number of neuron in hidden layer 1 \nn_hidden_2 = 100 # number of neuron in hidden layer 2 \nn_hidden_3 = 100 # number of neuron in hidden layer 3\nn_hidden_4 = 200 # number of neuron in hidden layer 4\nnum_digits = 10  # number of node in output layer which is softmax layer\n\n# Insert Hyperparameters\nlearning_rate = 0.1 # hyperparameter in gradient desent algorithm i.e.  b0 = b0 - alpha*(derivative of error w.r.to b0) \n                # denotes how big the steps we are taking to reach towards the point of minima\ntraining_epochs = 30 # i.e. one iteration through training set \nbatch_size = 100  # dividing training data in batches for faster convergence\ndropout_factor = 0.3 # produce an effect of regularization\n\n# input layer (784 nodes)\nInp = Input(shape=(784,))\n\n# hidden layer 1 (300 nodes)  \nx = Dense(n_hidden_1, activation='relu',name = \"Hidden_Layer_1\")(Inp)\nx = Dropout(dropout_factor)(x)\n\n# hidden layer 1 (100 nodes)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dropout(dropout_factor)(x)\n\n# hidden layer 1 (100 nodes)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dropout(dropout_factor)(x)\n\n# hidden layer 1 (200 nodes)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\n\n# output layer (10 nodes)\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)\n\nmodel = Model(Inp, output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size = batch_size, verbose=1, epochs = training_epochs, validation_data=(X_cv, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n    with alpha = 0.01                                           with alpha = 0.1\n\nloss: 0.0328 - acc: 0.9901                                  loss: 0.0326 - acc: 0.9904\nval_loss: 0.0766 - val_acc: 0.9828                          val_loss: 0.0672 - val_acc: 0.9846\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot loss during training\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['acc'], label='train') \npyplot.plot(history.history['val_acc'], label='val') \npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(score, test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimage_idx = random.randint(1,10000)-1\nfirst_image = X_test[image_idx]\npixels = first_image.reshape((28, 28))\npyplot.imshow(pixels, cmap='gray')\npyplot.show()\n\n# Model predication output\npredictions = model.predict(X_test, batch_size=200)\nprint(\"Predication Output on test image: \", predictions[image_idx].argmax(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nModel Summary\n\nModel Used: model: ADAM optimizater + 4 hidden + softmax output + learning rate (alpha=0.1) + dropout\n\nwith Hyperparameter\n     alpha = 0.1, \n     batch size = 100\n     epochs: 30\n     dropout_factor = 0.3\n       \n\n     On Training Data: loss: 0.0326 - acc: 0.9904 \n     On validation Data: val_loss: 0.0672 - val_acc: 0.9846\n     On Test Data: loss: 0.0835998178840884 - acc: 0.9812\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}