{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import Libraries\nLet us import the required libraries and functions","metadata":{}},{"cell_type":"code","source":"from warnings import filterwarnings\nfilterwarnings('ignore')\nimport os\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 'Scikit-learn' (sklearn) emphasizes various regression, classification and clustering algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\n\n# 'Statsmodels' is used to build and analyze various statistical models\nimport statsmodels\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.compat import lzip\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n# 'SciPy' is used to perform scientific computations\nfrom scipy.stats import f_oneway\nfrom scipy.stats import jarque_bera\nfrom scipy import stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis and Preparation","metadata":{}},{"cell_type":"markdown","source":"### Data preparation is the process of cleaning and transforming raw data prior to building predictive models.\n\n##### Here we will analyze and prepare data to perform regression analysis:\n1. Check dimensions of the dataframe in terms of rows and columns\n2. Check the data types. Refer data definition to ensure your data types are correct\n3. If data types are not as per business context, change the data types as per requirement\n4. Study summary statistics\n5. Check for missing values\n6. Study correlation\n7. Perform feature engineering\n8. Detect outliers\n9. Recheck the correlation\n\n##### Note: It is an art to explore data and one will need more and more practice to gain expertise in this area.","metadata":{}},{"cell_type":"markdown","source":"### Understand the Dataset","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Summary Statistics\n","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Value","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"Total = df.isnull().sum().sort_values(ascending=False) \n\nPercent = (df.isnull().sum()*100/df.isnull().count()).sort_values(ascending=False)   \n\nmissing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])\n\n# print the missing data\nmissing_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize the Null Values\n","metadata":{}},{"cell_type":"code","source":"# set the figure size\nplt.figure(figsize=(15, 8))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(df.isnull(), cbar=False)\n\n# display the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the correlation\n","metadata":{}},{"cell_type":"code","source":"num_col = df.select_dtypes(include=np.number)\nnum_col.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = num_col.corr()\n\n# print the correlation matrix\ncorr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nsns.heatmap(corr, cmap='YlGnBu', vmax=1.0, vmin=-1.0,annot = True, annot_kws={\"size\": 15}, )\n\n# specify name of the plot using plt.title()\nplt.title('Correlation between numeric features')\n\n# display the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Linear Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nX = df.drop('alcohol', axis=1)\n\n# extract the target variable from the data set\ny = df['alcohol']\n\n# split data into train subset and test subset for predictor and target variables\n# random_state: the seed used by the random number generator\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression()\nmodel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred= model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nr2_train = r2_score(y_train, y_train_pred)\nr2_test = r2_score(y_test, y_test_pred)\n\nrmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint(r2_train, r2_test)\nprint(rmse_train, rmse_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discover Outliers\n","metadata":{}},{"cell_type":"markdown","source":"###### Importance of detecting an outlier\nAn outlier is an observation that appears to deviate distinctly from other observations in the data. If the outliers are not removed, the model accuracy may decrease.\n\n###### Recollect that one of the assumptions of Linear Regression is there should be no outliers present in the data","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the plot size\nplt.rcParams['figure.figsize']=(18,8)\n\n# create a boxplot for all numeric features\n# column: selects the specified columns\ndf.boxplot()\n# to display the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using IQR Method","metadata":{}},{"cell_type":"code","source":"Q1 = df.drop(['alcohol'], axis=1).quantile(0.25)\n\n# compute the first quartile using quantile(0.75)\n# use .drop() to drop the target variable \n# axis=1: specifies that the labels are dropped from the columns\nQ3 = df.drop(['alcohol'], axis=1).quantile(0.75)\n\n# calculate of interquartile range \nIQR = Q3 - Q1\n\n# print the IQR values for numeric variables\nprint(IQR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filter out the outlier values\n# ~ : selects all rows which do not satisfy the condition\n# |: bitwise operator OR in python\n# any() : returns whether any element is True over the columns\n# axis : \"1\" indicates columns should be altered (use \"0\" for 'index')\ndf = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rechecking after the Outlair detection\n","metadata":{}},{"cell_type":"code","source":"# set figure size \nplt.rcParams['figure.figsize']=(15,8)\n\n# recheck for outliers\n# column: selects the specifies columns\ndf.boxplot()\n# display only the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recheck the Correlation\n##### Recheck the correlation after treating outliers. An outlier might either decrease or increase a correlation coefficient, depending on where it is in relation to the other points","metadata":{}},{"cell_type":"code","source":"# generate the correlation matrix \ncorr =  df.corr()\n\n# print the correlation matrix\ncorr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(corr, cmap='YlGnBu', vmax=1.0, vmin=-1.0, annot = True, annot_kws={\"size\": 15})\n\n# specify name of the plot\nplt.title('Correlation between numeric features')\n\n# display the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression (OLS)","metadata":{}},{"cell_type":"code","source":"Xc=sm.add_constant(X)\nol = sm.OLS(y,Xc).fit()\n\n# print the summary output\nprint(ol.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VIF ","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\nvf=[vif(Xc.values,i) for i in range(Xc.shape[1])]\n\npd.DataFrame(vf,index=Xc.columns,columns=['vif'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n","metadata":{}},{"cell_type":"markdown","source":"### RFE -Recursive Feature Elimination","metadata":{}},{"cell_type":"code","source":"cols=list(X.columns)\n\n\nfor col in cols:\n    X[col+'_2']=X[col]**2\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nlir=LinearRegression()\n\nrfe = RFE(lir,n_features_to_select=10)\nrfe.fit(X,y)\n\npd.DataFrame(rfe.ranking_,index=X.columns,columns=['select']).sort_values(by='select')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nno_of_cols=12\ntrain_score=[]\ntest_score=[]\n\nfor n in range(no_of_cols):\n    lir = LinearRegression()\n    rfe = RFE(lir, n_features_to_select=n+1)\n    rfe.fit(X_train, y_train)\n    \n    score1 = rfe.score(X_train, y_train)\n    train_score.append(score1)\n\n    score2 = rfe.score(X_test, y_test)\n    test_score.append(score2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_score,'g')\nplt.plot(test_score,'r')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx=np.linspace(1,12,12)\nrf=pd.DataFrame(test_score,columns=['r-sq']).sort_values(by='r-sq',ascending=False)\nrf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RFE Cv","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lir=LinearRegression()\nrfecv=RFECV(lir,cv=3,scoring='r2')\nrfecv.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfecv.grid_scores_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1,23),rfecv.grid_scores_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf=pd.DataFrame(rfecv.grid_scores_,index=range(1,23),columns=['scores'])\nrf.sort_values(by='scores',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(rfecv.ranking_, index=X.columns, columns=['select']).sort_values(by='select')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['alcohol']\nX1 =df.drop(['alcohol','total sulfur dioxide'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n\nlir = LinearRegression(fit_intercept=True)\n\nlir.fit(X_train, y_train)\ny_train_pred = lir.predict(X_train)\n\nr2_Train = r2_score(y_train, y_train_pred)\nrmse_Train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n\nprint('r2-Train: ', r2_Train, 'rmse_Train: ', rmse_Train)\n\ny_test_pred = lir.predict(X_test)\n\nr2_Test = r2_score(y_test, y_test_pred)\nrmse_Test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint('r2-Test: ', r2_Test, 'rmse_Test: ', rmse_Test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forward Selection Approaches \n","metadata":{}},{"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\ny=df['alcohol']\nX=df.drop('alcohol',axis=1)\n\ncols=list(X.columns)\n\nfor col in cols:\n    X[col+'_2']=X[col]**2\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lir = LinearRegression()\nsfs1=sfs(lir,k_features=22,forward=True,scoring='r2',cv=3,verbose=2)\nsfs1=sfs1.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sf=pd.DataFrame(sfs1.subsets_).T\nsf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(sf.index,sf['avg_score'])\nplt.xlabel('number of features')\nplt.ylabel('r-square')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sf[sf['avg_score']==sf['avg_score'].max()]['feature_names']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sfs1=sfs(lir,k_features=13,forward=True,scoring='r2',cv=3)\nsfs1=sfs1.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features=list(sfs1.k_feature_names_)\nselected_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['alcohol']\nX1 = X[selected_features]\n\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n\nlir = LinearRegression(fit_intercept=True)\n\nlir.fit(X_train, y_train)\ny_train_pred = lir.predict(X_train)\n\nr2_Train = r2_score(y_train, y_train_pred)\nrmse_Train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n\nprint('r2-Train: ', r2_Train, 'rmse_Train: ', rmse_Train)\n\ny_test_pred = lir.predict(X_test)\n\nr2_Test = r2_score(y_test, y_test_pred)\nrmse_Test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint('r2-Test: ', r2_Test, 'rmse_Test: ', rmse_Test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regularition\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold,LeaveOneOut\nfrom sklearn.datasets import load_boston\nboston= load_boston()\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.linear_model import Lasso,LassoCV,Ridge,RidgeCV,ElasticNet,ElasticNetCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lassso regression\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nXs=ss.fit_transform(X)\nXs=pd.DataFrame(Xs,columns=X.columns)\n\nlasso=Lasso(alpha=0.1,max_iter=10000)\nlasso.fit(X,y)\n\npd.DataFrame(lasso.coef_,index=X.columns,columns=['coef'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=lasso.predict(Xs)\nr2_score(y,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Simulation to understand the impact of alpha on coeffecient","metadata":{}},{"cell_type":"code","source":"np.linspace(1,10,10)\n\nnp.logspace(-4,1,10)\n\nalphas=np.logspace(-3,-1,10)\ncoefs=[]\nfor a in alphas:\n    lasso=Lasso(alpha=a,max_iter=10000)\n    lasso.fit(Xs,y)\n    coefs.append(lasso.coef_)\n    \n    \nplt.figure(figsize=(10, 5))   \nplt.plot(alphas,coefs)\nplt.xlabel('alphas')\nplt.ylabel('coeffecients')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tunning to find out the best alpha","metadata":{}},{"cell_type":"code","source":"alphas=np.logspace(-3,0,50)\n\nlassocv=LassoCV(alphas=alphas,cv=3,max_iter=10000,random_state=5)\nlassocv.fit(Xs,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lassocv.alpha_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso=Lasso(alpha=lassocv.alpha_,max_iter=10000)\nlasso.fit(X,y)\npd.DataFrame(lasso.coef_,index=X.columns,columns=['coef'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=lasso.predict(Xs)\nr2_score(y,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rechecking The Linear OlS Model","metadata":{}},{"cell_type":"code","source":"Xc=sm.add_constant(X)\nol = sm.OLS(y,Xc).fit()\n\n# print the summary output\nprint(ol.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finally I got a better model as compair to the base model which I created .\n### R2 value now 76% with the above Linear Regression prcocess. which is a good fit model.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}