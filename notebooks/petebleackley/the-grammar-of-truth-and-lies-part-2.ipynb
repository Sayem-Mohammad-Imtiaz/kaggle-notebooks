{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport gensim\nimport pandas\nimport nltk.corpus\nimport nltk.sentiment\nimport sklearn.linear_model\nimport textblob\nimport random\nimport numpy\nimport sklearn.metrics\nimport sklearn.ensemble\nimport seaborn\nimport re\nimport collections\n\nsentence_splitter=re.compile(u\"\"\"[.?!]['\"]*\\s+\"\"\",re.UNICODE)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset gives me the opportunity to see if the fake news detection methods I investigated in [The Grammar of Truth and Lies](https://www.kaggle.com/petebleackley/the-grammar-of-truth-and-lies) are reproducible. In that notebook I used the grammatical structure of sentences, sentiment analysis and stop words to classify documents as real or fake news. Results were promissing, but the question remains as to whether the techniques used will work on another sample. As before we begin by extracting sentence structure features (concatenations of part of speech labels) from the documents, reducing the dimensionality with Latent Semantic Indexing, and classifying with Logistic Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentence_structure_features(document):\n    return ['_'.join((pos for (word,pos) in sentence.pos_tags))\n            for sentence in textblob.blob.TextBlob(document).sentences]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentenceStructureCorpus(object):\n    def __init__(self):\n        lies=pandas.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\n        n_lies=lies.shape[0]\n        self.vader=nltk.sentiment.vader.SentimentIntensityAnalyzer()\n        print(\"Converting Fake News corpus\")\n        self.data=[sentence_structure_features('{0}\\n{1}'.format(row['title'],row['text']))\n                   for (index,row) in lies.iterrows()]\n        sentiments=[self.analyse_sentiments('{0}\\n{1}'.format(row['title'],row['text']))\n                    for (index,row) in lies.iterrows()]\n        truth = pandas.read_csv('../input/fake-and-real-news-dataset/True.csv')\n        print('Converting Real News corpus')\n        self.data.extend([sentence_structure_features('{0}\\n{1}'.format(row['title'],row['text']))\n                          for (index,row) in truth.iterrows()])\n        sentiments.extend([self.analyse_sentiments('{0}\\n{1}'.format(row['title'],row['text']))\n                           for (index,row) in truth.iterrows()])\n        self.sentiments=numpy.array(sentiments)\n        self.N=len(self.data)\n        self.labels=numpy.ones(self.N)\n        self.labels[:n_lies]=0\n        self.test_sample=random.sample(range(self.N),self.N//10)\n        print(\"Creating dictionary\")\n        self.dictionary=gensim.corpora.dictionary.Dictionary(self.data)\n        \n    def __iter__(self):\n        return (self.dictionary.doc2bow(document) for document in self.data)\n                          \n    def analyse_sentiments(self,document):\n        valences=numpy.array([[sent['pos'],sent['neg'],sent['neu']]\n                             for sent in (self.vader.polarity_scores(sentence)\n                                          for sentence in sentence_splitter.split(document))])\n        return valences.sum(axis=0)\n    \n    def training_data(self):\n        return [self.dictionary.doc2bow(document) for (i,document) in enumerate(self.data)\n                if i not in self.test_sample]\n                \n    def training_labels(self):\n        return self.labels[[i for i in range(self.N) if i not in self.test_sample]]\n    \n    def training_sentiments(self):\n        return self.sentiments[[i for i in range(self.N) if i not in self.test_sample]]\n    \n    def test_sentiments(self):\n        return self.sentiments[self.test_sample]\n                \n    def test_data(self):\n        return [self.dictionary.doc2bow(self.data[i])\n                for i in self.test_sample]\n            \n    def test_labels(self):\n        return self.labels[self.test_sample]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ssf=SentenceStructureCorpus()\nprint(\"Training LSI\")\nlsi=gensim.models.lsimodel.LsiModel(ssf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pandas.Series(ssf.labels).value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `True` and `False` samples are almost balanced (there are slightly more `False` samples)"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors=gensim.matutils.corpus2dense(lsi[ssf.training_data()],lsi.num_topics).T\nclassifier=sklearn.linear_model.LogisticRegression(max_iter=200)\nprint(\"Training classifier\")\nclassifier.fit(vectors,ssf.training_labels())\nprint(\"Testing classifier\")\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           classifier.predict(gensim.matutils.corpus2dense(lsi[ssf.test_data()],\n                                                                                           lsi.num_topics).T))\nseaborn.heatmap(confusion,annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that almost all the `True` articles are classified as `True`, but that slightly more than half of the `False` articles are classified as `True`. This is qualitiatively similar to the results from the original results."},{"metadata":{"trusted":true},"cell_type":"code","source":"def precision(cm):\n    return cm[1,1]/cm[:,1].sum()\n\ndef recall(cm):\n    return cm[1,1]/cm[1].sum()\n\ndef accuracy(cm):\n    return (cm[0,0]+cm[1,1])/cm.sum()\n\ndef matthews(cm):\n    return (cm[0,0]*cm[1,1]-cm[1,0]*cm[0,1])/numpy.sqrt(cm[0].sum()*cm[1].sum()*cm[:,0].sum()*cm[:,1].sum())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matthews(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Precision is 60%, Recall is 94%, Accuracy is 66% and Matthew's Coefficient is 40%. Precision and Recall are very similar to the original dataset, but Accuracy and Matthew Coefficient are lower.\n\nNow let's use Sentiment Analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_classifier=sklearn.linear_model.LogisticRegression(max_iter=200)\nsentiment_classifier.fit(ssf.training_sentiments(),ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           sentiment_classifier.predict(ssf.test_sentiments()))\nseaborn.heatmap(confusion,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, Sentiment Analysis classifies more `False` articles correctly, but misclassifies more `True` articles. The loss of Recall is greater than in the original dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matthews(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Precision is slightly increased to 62% (not as great an increase as in the original dataset), Recall is reduced to 51% (as opposed to 86% in the original dataset), Accuracy is 62% (78% in the original dataset) and Matthews Coefficient is only 24%. Sentiment Analysis is therefore a less reliable signla in this dataset than in the original one.\n\nNow let's look at combining Sentence Structure Features with Sentiment Analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"enhanced_vectors=numpy.hstack([vectors,ssf.training_sentiments()])\ncombined_classifier=sklearn.linear_model.LogisticRegression(max_iter=200)\nprint(\"Training classifier\")\ncombined_classifier.fit(enhanced_vectors,ssf.training_labels())\nprint(\"Testing classifier\")\nenhanced_test_vectors=numpy.hstack([gensim.matutils.corpus2dense(lsi[ssf.test_data()],\n                                                                 lsi.num_topics).T,\n                                    ssf.test_sentiments()])\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           combined_classifier.predict(enhanced_test_vectors))\nseaborn.heatmap(confusion,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matthews(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Precision has increased to 67%, Recall to 74%, Accuracy to 70% and Matthews Coefficient is 41%. This is a better result than either feature set alone for Precision and Accuracy, equal to Sentence Structure Features alone on Matthews Coefficient, and intermediate between the two for Recall. However, it does not perform as well on any metric as it did on the original dataset.\n\nNow let us try Random Forest classifiers with each feature set."},{"metadata":{"trusted":true},"cell_type":"code","source":"forest0=sklearn.ensemble.RandomForestClassifier(n_estimators=100)\nforest0.fit(vectors,ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           forest0.predict(gensim.matutils.corpus2dense(lsi[ssf.test_data()],\n                                                                                           lsi.num_topics).T))\nseaborn.heatmap(confusion,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matthews(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Random Forest classifier does surprisingly well with Sentence Structure Features, achieving 97% precision, 94% Recall, 96% Accuracy and 92% Matthews Coefficient. This is much better than it performed on the original dataset. Note that in the discussion of this dataset, several people have reported surprisingly good results on this dataset with a variety of models."},{"metadata":{"trusted":true},"cell_type":"code","source":"forest1=sklearn.ensemble.RandomForestClassifier(n_estimators=100)\nforest1.fit(ssf.training_sentiments(),ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           forest1.predict(ssf.test_sentiments()))\nseaborn.heatmap(confusion,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matthews(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, Random Forests does not do as well with Sentiment Analysis alone - 73% Precision, 64% Recall, 72% Accuracy and 43% Matthew's Coefficient. This is similar to the way it performed on the original dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"forest2=sklearn.ensemble.RandomForestClassifier(n_estimators=100)\nforest2.fit(enhanced_vectors,ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           forest2.predict(enhanced_test_vectors))\nseaborn.heatmap(confusion,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matthews(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results for combining Sentence Structure Features with Sentiment Analysis with a Random Forest classifier are practically the same as for Sentence Structure Features alone.\n\nNow let us introduce Stopwords into our feature set, and classify with Logistic Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words(\"english\")\nstopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentence_structure_features(document):\n    blob = textblob.blob.TextBlob(document)\n    return ['_'.join((pos for (word,pos) in sentence.pos_tags))\n            for sentence in textblob.blob.TextBlob(document).sentences] +[word.lower() \n                                                                          for word in blob.words\n                                                                          if  word.lower() in stopwords]\nssf2 = SentenceStructureCorpus()\ntransform = gensim.models.LsiModel(ssf2,id2word=ssf2.dictionary)\ntraining_data = gensim.matutils.corpus2dense(transform[ssf2.training_data()],\n                                             transform.num_topics).T\ntest_data = gensim.matutils.corpus2dense(transform[ssf2.test_data()],\n                                         transform.num_topics).T\nclassifier = sklearn.linear_model.LogisticRegression(max_iter=200)\nclassifier.fit(training_data,ssf2.training_labels())\nconfusion = sklearn.metrics.confusion_matrix(ssf2.test_labels(),\n                                             classifier.predict(test_data))\nseaborn.heatmap(confusion,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matthews(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These results are close to those achieved with the original dataset using Sentence Structure Features and Stopwords.\n\nIn general we can see that the performance of these algorithms is broadly consistent between both datasets. The most surprising thing is how well Random Forests perform on this dataset when compared both to their performance on the original dataset and to the performance of other algorithms on this dataset."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}