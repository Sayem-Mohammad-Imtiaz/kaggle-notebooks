{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-05T18:16:28.249416Z","iopub.execute_input":"2021-09-05T18:16:28.249854Z","iopub.status.idle":"2021-09-05T18:16:28.269788Z","shell.execute_reply.started":"2021-09-05T18:16:28.249763Z","shell.execute_reply":"2021-09-05T18:16:28.26856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the initial libraries\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:17:24.185325Z","iopub.execute_input":"2021-09-05T18:17:24.185717Z","iopub.status.idle":"2021-09-05T18:17:24.18988Z","shell.execute_reply.started":"2021-09-05T18:17:24.185674Z","shell.execute_reply":"2021-09-05T18:17:24.188784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:17:40.935914Z","iopub.execute_input":"2021-09-05T18:17:40.936388Z","iopub.status.idle":"2021-09-05T18:17:41.041944Z","shell.execute_reply.started":"2021-09-05T18:17:40.936357Z","shell.execute_reply":"2021-09-05T18:17:41.040979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's see what the data looks like\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:17:54.871799Z","iopub.execute_input":"2021-09-05T18:17:54.872144Z","iopub.status.idle":"2021-09-05T18:17:54.906112Z","shell.execute_reply.started":"2021-09-05T18:17:54.872113Z","shell.execute_reply":"2021-09-05T18:17:54.905267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We notice we have columns 0,3,4,7, and 9 are numerical columns\n#and columns 1,2,5,6,8,10,11, and 12 are categorical columns\n#column 13 will be what we are trying to predict","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:18:27.936442Z","iopub.execute_input":"2021-09-05T18:18:27.936805Z","iopub.status.idle":"2021-09-05T18:18:27.940668Z","shell.execute_reply.started":"2021-09-05T18:18:27.936775Z","shell.execute_reply":"2021-09-05T18:18:27.939764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create numpy arrays\nX_numerical = dataset.iloc[:,[0,3,4,7,9]].values\nX_categorical = dataset.iloc[:,[1,2,5,6,8,10,11,12]]\ny = dataset.iloc[:,-1].values","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:19:07.167259Z","iopub.execute_input":"2021-09-05T18:19:07.167635Z","iopub.status.idle":"2021-09-05T18:19:07.177228Z","shell.execute_reply.started":"2021-09-05T18:19:07.167601Z","shell.execute_reply":"2021-09-05T18:19:07.175896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding X_categorical\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\norginalNumOfColsOfX_categorical = X_categorical.shape[1]\nfor i in range(X_categorical.shape[1]): \n    currNumOfColsOfX_categorical = X_categorical.shape[1]\n    indexOfColumnToEncode = currNumOfColsOfX_categorical - orginalNumOfColsOfX_categorical + i\n    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop='first'), [indexOfColumnToEncode])], remainder='passthrough', sparse_threshold=0)\n    X_categorical = np.array(ct.fit_transform(X_categorical)) ","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:19:24.383149Z","iopub.execute_input":"2021-09-05T18:19:24.383503Z","iopub.status.idle":"2021-09-05T18:19:25.281609Z","shell.execute_reply.started":"2021-09-05T18:19:24.383458Z","shell.execute_reply":"2021-09-05T18:19:25.280648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#join X_numerical and X_categorical into one array\nX = np.concatenate((X_numerical,X_categorical), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:19:35.081051Z","iopub.execute_input":"2021-09-05T18:19:35.081385Z","iopub.status.idle":"2021-09-05T18:19:35.086028Z","shell.execute_reply.started":"2021-09-05T18:19:35.081357Z","shell.execute_reply":"2021-09-05T18:19:35.084899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we see y is already binary encoded as it only contains 0s and 1s","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:19:44.951515Z","iopub.execute_input":"2021-09-05T18:19:44.951854Z","iopub.status.idle":"2021-09-05T18:19:44.955574Z","shell.execute_reply.started":"2021-09-05T18:19:44.951827Z","shell.execute_reply":"2021-09-05T18:19:44.954608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:19:58.508196Z","iopub.execute_input":"2021-09-05T18:19:58.508523Z","iopub.status.idle":"2021-09-05T18:19:58.559391Z","shell.execute_reply.started":"2021-09-05T18:19:58.508494Z","shell.execute_reply":"2021-09-05T18:19:58.558567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Scaling i.e Standardisation or Normalisation\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:20:08.406682Z","iopub.execute_input":"2021-09-05T18:20:08.407027Z","iopub.status.idle":"2021-09-05T18:20:08.413487Z","shell.execute_reply.started":"2021-09-05T18:20:08.406997Z","shell.execute_reply":"2021-09-05T18:20:08.412428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the ANN libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:20:17.655773Z","iopub.execute_input":"2021-09-05T18:20:17.656107Z","iopub.status.idle":"2021-09-05T18:20:23.372697Z","shell.execute_reply.started":"2021-09-05T18:20:17.656077Z","shell.execute_reply":"2021-09-05T18:20:23.371774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#After 50 or so hyperparameter tunings I found the following to be most effective, stable, and reliable","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:31:14.45482Z","iopub.execute_input":"2021-09-05T18:31:14.455165Z","iopub.status.idle":"2021-09-05T18:31:14.45838Z","shell.execute_reply.started":"2021-09-05T18:31:14.455135Z","shell.execute_reply":"2021-09-05T18:31:14.457697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = Sequential()\n# Adding first hidden layer\nclassifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.1))\n# Adding second hidden layer\nclassifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.1))\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:31:21.056696Z","iopub.execute_input":"2021-09-05T18:31:21.057296Z","iopub.status.idle":"2021-09-05T18:31:21.126816Z","shell.execute_reply.started":"2021-09-05T18:31:21.057261Z","shell.execute_reply":"2021-09-05T18:31:21.125857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the ANN to the Training set\ny_test = y_test.astype('float64') #to have the same type as the other sets\ny_train = y_train.astype('float64') #to have the same type as the other sets\nclassifierHistory = classifier.fit(X_train, y_train, batch_size = 64, epochs = 70, validation_data=(X_test,y_test)) \n","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:31:53.349165Z","iopub.execute_input":"2021-09-05T18:31:53.34954Z","iopub.status.idle":"2021-09-05T18:31:58.476981Z","shell.execute_reply.started":"2021-09-05T18:31:53.349503Z","shell.execute_reply":"2021-09-05T18:31:58.476102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#After several runs, this model either overfits or underfits by maximum 3% and the test accuracy ranges from 83%-90%\n#so call it 86.5 round down to 86% accurate\n#We see here we have a test accuracy of 90% but if we run the model again it will return another result ranging from 83%-90%","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:35:10.389841Z","iopub.execute_input":"2021-09-05T18:35:10.390325Z","iopub.status.idle":"2021-09-05T18:35:10.394236Z","shell.execute_reply.started":"2021-09-05T18:35:10.390294Z","shell.execute_reply":"2021-09-05T18:35:10.392994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Second model run to show accuracy range\n\nclassifier = Sequential()\nclassifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.1))\nclassifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.1))\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifierHistory = classifier.fit(X_train, y_train, batch_size = 64, epochs = 70, validation_data=(X_test,y_test)) ","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:36:47.729056Z","iopub.execute_input":"2021-09-05T18:36:47.729603Z","iopub.status.idle":"2021-09-05T18:36:52.128522Z","shell.execute_reply.started":"2021-09-05T18:36:47.729559Z","shell.execute_reply":"2021-09-05T18:36:52.127535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We have a test accuracy of 86.89% with an overfit of just over 2%. \n#The test accuracy is display that this model's average accuracy is 86%","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:39:22.156124Z","iopub.execute_input":"2021-09-05T18:39:22.156462Z","iopub.status.idle":"2021-09-05T18:39:22.160085Z","shell.execute_reply.started":"2021-09-05T18:39:22.156434Z","shell.execute_reply":"2021-09-05T18:39:22.158983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluating our classification model\nscores = classifier.evaluate(X_test, y_test) #it is important to keep in mind that the sigmoid function uses a 50% threshold\n#and for a real life scenario such as predicting heart disease prediction we would want to decrease or increase that threshold\n#depending on what the outcome means for the next steps to be taken\nprint('Accuracy: %.2f%%' % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:41:55.175135Z","iopub.execute_input":"2021-09-05T18:41:55.175735Z","iopub.status.idle":"2021-09-05T18:41:55.2308Z","shell.execute_reply.started":"2021-09-05T18:41:55.175686Z","shell.execute_reply":"2021-09-05T18:41:55.22961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualisation\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:42:11.031753Z","iopub.execute_input":"2021-09-05T18:42:11.032109Z","iopub.status.idle":"2021-09-05T18:42:11.036253Z","shell.execute_reply.started":"2021-09-05T18:42:11.032074Z","shell.execute_reply":"2021-09-05T18:42:11.035234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(classifierHistory.history['accuracy'])\nplt.plot(classifierHistory.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:42:20.902858Z","iopub.execute_input":"2021-09-05T18:42:20.903187Z","iopub.status.idle":"2021-09-05T18:42:21.094253Z","shell.execute_reply.started":"2021-09-05T18:42:20.90316Z","shell.execute_reply":"2021-09-05T18:42:21.093401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we see the train and test accuracy meet and stay within a 3% range the higher the epochs go.\n#epochs = 70 was found to optimally prevent a large overfitting or underfitting within this model\n#if there were more epochs the training accuracy would continue to increase \n#while the test accuracy would remain fairly constant, and this would cause either and large overfit or underfit.\n#for this run we see it prevented a large overfit","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:48:24.939775Z","iopub.execute_input":"2021-09-05T18:48:24.940116Z","iopub.status.idle":"2021-09-05T18:48:24.943773Z","shell.execute_reply.started":"2021-09-05T18:48:24.940086Z","shell.execute_reply":"2021-09-05T18:48:24.942993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(classifierHistory.history['loss'])\nplt.plot(classifierHistory.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:48:33.146177Z","iopub.execute_input":"2021-09-05T18:48:33.146557Z","iopub.status.idle":"2021-09-05T18:48:33.308693Z","shell.execute_reply.started":"2021-09-05T18:48:33.146521Z","shell.execute_reply":"2021-09-05T18:48:33.307997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The model loss visualisation further supports the findings above","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:49:16.274681Z","iopub.execute_input":"2021-09-05T18:49:16.275137Z","iopub.status.idle":"2021-09-05T18:49:16.278376Z","shell.execute_reply.started":"2021-09-05T18:49:16.275108Z","shell.execute_reply":"2021-09-05T18:49:16.277589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Conclusion\n#While an accuracy of 86% was achieved, the dataset only had 330 rows and hence is quite small.\n#Deep Learning is not suited for such little data\n#I suggest using a Machine Learning algorithm for this heart disease prediction","metadata":{"execution":{"iopub.status.busy":"2021-09-05T18:50:58.368238Z","iopub.execute_input":"2021-09-05T18:50:58.368727Z","iopub.status.idle":"2021-09-05T18:50:58.372171Z","shell.execute_reply.started":"2021-09-05T18:50:58.368695Z","shell.execute_reply":"2021-09-05T18:50:58.371215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}