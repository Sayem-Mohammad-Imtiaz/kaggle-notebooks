{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom keras import regularizers\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nimport matplotlib.pyplot as plt\nfrom math import sqrt, ceil\nfrom timeit import default_timer as timer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import UpSampling2D, Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape, GlobalAveragePooling2D,Dropout,SeparableConv2D, Activation\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import VGG16\nfrom keras.optimizers import SGD, Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport keras\nfrom keras.models import Model\nfrom tensorflow.keras.applications import EfficientNetB0\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wczytanie danych"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/traffic-signs-preprocessed/data2.pickle', 'rb') as f:\n    data = pickle.load(f, encoding='latin1')  # dictionary type\n\n# Preparing y_train and y_validation for using in Keras\ndata['y_train'] = to_categorical(data['y_train'], num_classes=43)\ndata['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n\n# Making channels come at the end\ndata['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\ndata['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\ndata['x_test'] = data['x_test'].transpose(0, 2, 3, 1)\n\n# Showing loaded data from file\nfor i, j in data.items():\n    if i == 'labels':\n        print(i + ':', len(j))\n    else: \n        print(i + ':', j.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Przykłady"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\n# Preparing function for ploting set of examples\n# As input it will take 4D tensor and convert it to the grid\n# Values will be scaled to the range [0, 255]\ndef convert_to_grid(x_input):\n    N, H, W, C = x_input.shape\n    grid_size = int(ceil(sqrt(N)))\n    grid_height = H * grid_size + 1 * (grid_size - 1)\n    grid_width = W * grid_size + 1 * (grid_size - 1)\n    grid = np.zeros((grid_height, grid_width, C)) + 255\n    next_idx = 0\n    y0, y1 = 0, H\n    for y in range(grid_size):\n        x0, x1 = 0, W\n        for x in range(grid_size):\n            if next_idx < N:\n                img = x_input[next_idx]\n                low, high = np.min(img), np.max(img)\n                grid[y0:y1, x0:x1] = 255.0 * (img - low) / (high - low)\n                next_idx += 1\n            x0 += W + 1\n            x1 += W + 1\n        y0 += H + 1\n        y1 += H + 1\n\n    return grid\n\n\n# Visualizing some examples of training data\nexamples = data['x_train'][:81, :, :, :]\nprint(examples.shape)  # (81, 32, 32, 3)\n\n# Plotting some examples\nfig = plt.figure()\ngrid = convert_to_grid(examples)\nplt.imshow(grid.astype('uint8'), cmap='gray')\nplt.axis('off')\nplt.gcf().set_size_inches(15, 15)\nplt.title('Some examples of training data', fontsize=18)\n\n# Showing the plot\nplt.show()\n\n# Saving the plot\nfig.savefig('training_examples.png')\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Funkcje generujące modele"},{"metadata":{},"cell_type":"markdown","source":"Zmniejszanie learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch, lr):\n    if epoch < 10:\n        print('lr = ',lr)\n        return lr\n    else:\n        print('lr = ',lr * tf.math.exp(-0.1))\n        return lr * tf.math.exp(-0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generuje początkowy model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PARAMS : epoki, ilość danych, learn_rate, filtr, ilość danych w batchu\ndef model1(epochs, val, kernel=3, batches = 5):\n    model1 = Sequential()\n    model1.add(Conv2D(32, kernel_size=kernel, padding='same', activation='relu', input_shape=(32, 32, 3)))\n    model1.add(MaxPool2D(pool_size=2))\n    model1.add(Flatten())\n    model1.add(Dense(500, activation='relu'))\n    model1.add(Dense(43, activation='softmax'))\n    model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\n    \n    h1 = model1.fit(data['x_train'][:val], data['y_train'][:val],\n              batch_size=batches, epochs = epochs,\n              validation_data = (data['x_validation'], data['y_validation']),\n              callbacks=[annealer], verbose=1)\n    \n    return h1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generuje model VGG (bez wag początkowych)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model2(epochs, val, batches = 32):\n    model = Sequential()\n\n    # The L2 regularization penalty is computed as: loss = l2 * reduce_sum(square(x))\n    \n    model.add(Conv2D(input_shape=(32,32,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    \n    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    \n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    \n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    \n    \n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    \n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n   \n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    \n    model.add(MaxPool2D(pool_size=(2, 2)))\n    \n\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    \n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    \n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    \n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(512,activation=\"relu\",kernel_regularizer=regularizers.l2(0.0005)))\n    model.add(BatchNormalization())\n    \n    model.add(Dropout(0.5))\n    #model.add(Dense(1000, activation=\"relu\"))\n    model.add(Dense(43, activation=\"softmax\"))\n    \n    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    \n    #annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\n    annealer = LearningRateScheduler(scheduler)\n    \n    h = model.fit(data['x_train'][:val], data['y_train'][:val],\n              batch_size= batches, epochs = epochs,\n              validation_data = (data['x_validation'], data['y_validation']),\n              callbacks=[annealer], verbose=1)\n    \n    return h\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generuje model VGG z wagami (imagenet)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model2_2(epochs, val, batches = 32):\n    \n    base = VGG16(weights='imagenet', include_top=False)\n    x = base.output\n    \n    x=GlobalAveragePooling2D()(x)\n    x=Dense(1024,activation='relu')(x)\n    x = Dropout(0.25)(x)\n    x=Dense(512,activation='relu')(x) \n    x = Dropout(0.25)(x)\n    \n    preds=Dense(43, activation='softmax')(x) #final layer with softmax activation\n    \n    model=Model(inputs=base.input,outputs=preds)\n    \n    #model.summary()\n    #for i,layer in enumerate(model.layers):\n    #    print(\"{}: {}\".format(i,layer))\n        \n    for layer in model.layers[:19]:\n        layer.trainable=False\n    for layer in model.layers[19:]:\n        layer.trainable=True\n        \n    epochs_cons = 50\n    learning_rate = 0.0005\n    decay_rate = learning_rate / epochs_cons\n    opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n    \n    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + 50))\n    \n    h = model.fit(data['x_train'][:val], data['y_train'][:val],\n              batch_size= batches, epochs = epochs,\n              validation_data = (data['x_validation'], data['y_validation']),\n              callbacks=[annealer], verbose=1)\n    \n    return h","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generuje model VGG z wagami (imagenet) - lambda/funkcja"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_vgg=model_vgg(20,6000, 0.005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_vgg(epochs, val,l_r = 0.001, schedule=True, batches = 32):\n    \n    base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n    \n    # wydobycie ostatniej warstwy 3 bloczka\n    last = base.get_layer('block3_pool').output\n    \n    x = Flatten()(last)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    pred = Dense(43, activation='softmax')(x)\n    \n    \n    model=Model(inputs=base.input,outputs=pred)\n    \n    #for layer in base.layers:\n    #   layer.trainable = False\n        \n    learning_rate = l_r\n    opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None)\n    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n\n    if schedule==True:\n        annealer = LearningRateScheduler(scheduler)\n        \n        h = model.fit(data['x_train'][:val], data['y_train'][:val],\n                      batch_size= batches, epochs = epochs,\n                      validation_data = (data['x_validation'], data['y_validation']),\n                      callbacks=[annealer], verbose=1)\n    # ------ 1e-3 = 0.001 <=> 0.001*0.95^(x+l_epok) -> lrs\n    #annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs/4))\n    \n    else:\n        annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + 40))\n        h = model.fit(data['x_train'][:val], data['y_train'][:val],\n                  batch_size= batches, epochs = epochs,\n                  validation_data = (data['x_validation'], data['y_validation']),\n                  callbacks=[annealer], verbose=1)\n    \n    return h","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EfficientNetB0 nie przyjmuje obrazków 32x32.**"},{"metadata":{},"cell_type":"markdown","source":"Generuje model EfficientNetB0 z wagami (imagenet)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def efficient(epochs, val,l_r = 0.001, schedule=True, batches = 32):\n    \n    size = (224, 224)\n    \n    \n    base = EfficientNetB0(include_top=False, weights='imagenet')\n    x=base.output\n    \n    x=GlobalAveragePooling2D()(x)\n    x=Dense(1280,activation='relu')(x)\n    x = Dropout(0.25)(x)\n    x=Dense(640,activation='relu')(x) \n    x = Dropout(0.25)(x)\n    preds=Dense(43, activation='softmax')(x) #final layer with softmax activation\n    \n    model=Model(inputs=base.input,outputs=preds)\n    \n    for layer in model.layers[:237]:\n        layer.trainable=False\n    for layer in model.layers[237:]:\n        layer.trainable=True\n        \n    opt = Adam(lr=l_r, beta_1=0.9, beta_2=0.999, epsilon=None)\n    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n    \n    #model=Model(inputs=base.input, outputs=outputs, name=\"EfficientNet\")\n    \n    #optimizer = Adam(learning_rate=l_r)\n    #model.compile(optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n    \n    #for layer in model.layers[:237]:\n    #    layer.trainable=False\n    #for layer in model.layers[237:]:\n    #    layer.trainable=True\n        \n    #opt = Adam(lr=l_r, beta_1=0.9, beta_2=0.999, epsilon=None)\n    #model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n        \n    if schedule==True:\n        annealer = LearningRateScheduler(scheduler)\n        \n        h = model.fit(data['x_train'][:val], data['y_train'][:val],\n                  batch_size= batches, epochs = epochs,\n                  validation_data = (data['x_validation'], data['y_validation']),\n                  callbacks=[annealer], verbose=1)\n   \n    else:\n        annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + 40))\n        h = model.fit(data['x_train'][:val], data['y_train'][:val],\n                  batch_size= batches, epochs = epochs,\n                  validation_data = (data['x_validation'], data['y_validation']),\n                  callbacks=[annealer], verbose=1)\n        \n    return h\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generuje model EfficientNetB0 z wagami (imagenet) bez zmiany learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"def efficient_cons(epochs, val,l_r = 0.001, batches = 32):\n    \n    base = EfficientNetB0(include_top=False, weights='imagenet')\n    x=base.output\n    \n    x=GlobalAveragePooling2D()(x)\n    x=Dense(1280,activation='relu')(x)\n    x = Dropout(0.25)(x)\n    x=Dense(640,activation='relu')(x) \n    x = Dropout(0.25)(x)\n    preds=Dense(43, activation='softmax')(x) #final layer with softmax activation\n    \n    model=Model(inputs=base.input,outputs=preds)\n    \n    for layer in model.layers[:237]:\n        layer.trainable=False\n    for layer in model.layers[237:]:\n        layer.trainable=True\n        \n    opt = Adam(lr=l_r, beta_1=0.9, beta_2=0.999, epsilon=None)\n    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n        \n    h = model.fit(data['x_train'][:val], data['y_train'][:val],\n                  batch_size= batches, epochs = epochs,\n                  validation_data = (data['x_validation'], data['y_validation']), \n                  verbose=1)\n        \n    return h\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FUNKCJA RYSOWANIA GRAFÓW"},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph(h):\n    %matplotlib inline\n    plt.rcParams['figure.figsize'] = (15.0, 5.0) # Setting default size of plots\n    plt.rcParams['image.interpolation'] = 'nearest'\n    plt.rcParams['font.family'] = 'Times New Roman'\n\n    fig = plt.figure()\n    plt.subplot(1,2,1)\n    plt.plot(h.history['accuracy'], '-o', linewidth=3.0)\n    plt.plot(h.history['val_accuracy'], '-o', linewidth=3.0)\n    plt.legend(['train', 'validation'], loc='upper left', fontsize='xx-large')\n    plt.xlabel('Epoch', fontsize=20)\n    plt.ylabel('Accuracy', fontsize=20)\n    plt.tick_params(labelsize=18)\n\n    plt.subplot(1,2,2)\n    plt.plot(h.history['val_loss'], '-o', linewidth=3.0)\n    plt.legend(['validation loss'], loc='upper left', fontsize='xx-large')\n    plt.xlabel('Epoch', fontsize=20)\n    plt.ylabel('Loss', fontsize=20)\n    plt.tick_params(labelsize=18)\n\n\n    # Showing the plot\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FUNKCJA PORÓWNYWANIA GRAFÓW"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef multi_graph(models, mat, name =\"\", sizeX = 20.0, sizeY = 15.0):\n    \n    plt.rcParams['figure.figsize'] = (sizeX, sizeY) # Setting default size of plots\n    plt.suptitle(name)\n    plt.subplot(2,2,1)\n    print(mat)\n    #print(len(models))\n    for i in range(len(models)):\n        plt.plot(models[i].history['accuracy'], '-o', linewidth=3.0, label='{0}e {1}d {2:.3f}a'.format(mat[0][i], mat[1][i], mat[3][i])) \n        \n        # dodać jeśli chcesz zmienić kernel : {2}k . ___ mat[2][i]\n        \n        #zapisywanie kolejnych elementow legendy\n        #A[i]='{0}e {1}d {2}k'.format(mat[0][i], mat[1][i], mat[2][i])\n    plt.legend(loc=4)\n    plt.xlabel('Epoch', fontsize=20)\n    plt.ylabel('Accuracy', fontsize=20)\n    plt.tick_params(labelsize=18)\n        \n    plt.subplot(2,2,2)\n    for i in range(len(models)):\n        plt.plot(models[i].history['val_accuracy'], '-o', linewidth=3.0, label='{0}e {1}d {2:.3f}a'.format(mat[0][i], mat[1][i], mat[3][i])) \n    plt.legend(loc=4)\n    plt.xlabel('Epoch', fontsize=20)\n    plt.ylabel('Validation accuracy', fontsize=20)\n    plt.tick_params(labelsize=18)\n    \n    plt.subplot(2,2,3)\n    for i in range(len(models)):\n        plt.plot(models[i].history['val_loss'], '-o', linewidth=3.0, label='{0}e {1}d {2:.3f}a'.format(mat[0][i], mat[1][i], mat[3][i]))  \n    plt.legend(loc=4)\n    plt.xlabel('Epoch', fontsize=20)\n    plt.ylabel('Validation loss', fontsize=20)\n    plt.tick_params(labelsize=18)\n    \n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FUNKCJA TWORZENIA MACIERZY DEF. MODEL (epoki, dane)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def matr(E, D, ALPH=0, isInt = True):\n    M = np.zeros((4,len(E)))\n    #M[0] -> epoki\n    #M[1] -> dane\n    #M[2] -> filtry\n\n    for i in range(len(E)):\n        M[0][i]=E[i]\n        M[1][i]=D[i]\n        #domyslny filtr 3x3\n        M[2][i]=3\n        #alpha - współczynnik uczenia\n        if ALPH!=0:\n            M[3][i]=ALPH[i]\n    print(M)\n\n    if isInt:\n        int_M=M.astype(int)\n        return int_M\n    else:\n        return M","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dla przykładu:"},{"metadata":{},"cell_type":"markdown","source":"# 1. Prosty model"},{"metadata":{},"cell_type":"markdown","source":"Parametry: \n* -> E[] = kolejne wartości epok\n* -> D[] = kolejne ilości danych trenujących"},{"metadata":{"trusted":true},"cell_type":"code","source":"E=[]\nD=[]\n# domyślnie\n# E=[10,10,20,20]\n# D=[10,1000,10,1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kod dodaje do tablicy TAB_MOD kolejne wytrenowane modele"},{"metadata":{"trusted":true},"cell_type":"code","source":"#E=[]\n#D=[]\n#LUB\nE=[10,10,20,20]\nD=[10,1000,10,1000]\nTAB_MOD=[]\n# iterator = 0\n\n#e = 10\n#d = 10\n\n#E.append(e)\n#D.append(d)\n\nfor i in range(len(E)):\n    print ('{} epok, {} danych uczących'.format(E[i], D[i]))\n    h_temp=model1(E[i], D[i])\n    TAB_MOD.append(h_temp)\n    print('<==============================================>')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stary kod\n#print ('{} epok, {} danych uczących'.format(E[iterator], D[iterator]))\n#h1 = model1(E[iterator], D[iterator])\n#TAB_MOD.append(h1)\n#iterator = iterator + 1\n#print ('<===========================>')\n\n#e = 10\n#d = 1000\n\n#E.append(e)\n#D.append(d)\n\n#print ('{} epok, {} danych uczących'.format(E[iterator], D[iterator]))\n#h2 = model1(E[iterator], D[iterator])\n#TAB_MOD.append(h2)\n#iterator = iterator + 1\n#print ('<===========================>')\n\n#e = 20\n#d = 10\n\n#E.append(e)\n#D.append(d)\n\n#print ('{} epok, {} danych uczących'.format(E[iterator], D[iterator]))\n#h3 = model1(E[iterator], D[iterator])\n#TAB_MOD.append(h3)\n#iterator = iterator + 1\n#print ('<===========================>')\n\n#e = 20\n#d = 1000\n\n#E.append(e)\n#D.append(d)\n\n#print ('{} epok, {} danych uczących'.format(E[iterator], D[iterator]))\n#h4 = model1(E[iterator], D[iterator])\n#TAB_MOD.append(h4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(E)):\n    graph(TAB_MOD[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Porównanie danych modeli na konkretnych wykresach"},{"metadata":{"trusted":true},"cell_type":"code","source":"#najpierw trzeba stworzyć macierz z danymi modeli\nM_model1 = matr(E, D)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Porównanie wszystkich modeli typu 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"#int_M=M.astype(int)\nmulti_graph(TAB_MOD, M_model1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Na wyższych wykresach widać, jak bardzo ilość danych ma znaczenie. Różnica też jest jednak spora (10 -> 1000). Jak widać, przygotowałem odpowiednie funkcje, więc jeśli by była potrzeba sprawdzenia dla innych warunków wystarczy zmienić parametry (model1(liczba epok, liczba danych, filtr)). Następnie można te modele porównać przy pomocy funkcji multi_graph(models, mat), do której przekazujemy tablicę modeli oraz ich parametrów, jako że nie mogłem zmienić nazw modelów. Trzeba sobie jakoś radzić. Oczywiście, e-ilość epok, d-ilość danych, k-szerokość konwolucji."},{"metadata":{},"cell_type":"markdown","source":"# 2. VGG(już nie 16) - zmodyfikowany VGG16 dopasowany do problemu"},{"metadata":{},"cell_type":"markdown","source":"Bez Transfer Learningu:"},{"metadata":{"trusted":true},"cell_type":"code","source":"E=[35,35]\nD=[6000,10000]\n#B=[20,50,20,50]\nTAB_MOD1=[]\n\nfor i in range(len(E)):\n    print ('{} epok, {} danych uczących'.format(E[i], D[i]))\n    h_temp=model2(E[i], D[i])\n    TAB_MOD1.append(h_temp)\n    print('<==============================================>')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Porównanie wyżej wytrenowanych modeli"},{"metadata":{"trusted":true},"cell_type":"code","source":"#najpierw trzeba stworzyć macierz z danymi modeli\n\nM_VGG = matr(E, D)\n\n\nmulti_graph(TAB_MOD1, M_VGG)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transfer learning:"},{"metadata":{},"cell_type":"markdown","source":"Trenowanie klasyfikatora:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#E=[100,100,200,200]\nE=[30, 30]\n#D=[6000,10000,6000,10000]\nD=[6000, 10000]\nTAB_MOD2=[]\n\nfor i in range(len(E)):\n    print ('{} epok, {} danych uczących (transfer learning)'.format(E[i], D[i]))\n    h_temp=model_vgg(E[i], D[i])\n    TAB_MOD2.append(h_temp)\n    print('<==============================================>')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Porównanie:"},{"metadata":{"trusted":true},"cell_type":"code","source":"M_VGG_T=matr(E, D)\n\nmulti_graph(TAB_MOD2, M_VGG_T, 'Trained + function scheduler')\nmulti_graph(TAB_MOD1, M_VGG, 'Not Trained')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL WYUCZONY VGG (zmiana liczb epok,stała liczba danych, inny scheduler, inne learning rate)"},{"metadata":{},"cell_type":"markdown","source":"Tablice z cechami modeli"},{"metadata":{"trusted":true},"cell_type":"code","source":"E=[30,30]\nD=[10000,10000]\nalpha=[0.001, 0.005]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TAB_MOD2_sc=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TAB_MOD2_lam=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Zakomentowałem z funkcją lambda, gdyż nie starczyłoby mi czasu na GPU"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Nauczanie modeli z schedulerem"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(E)):\n    print ('{} epok, {} danych uczących, {} -> startowy learning rate (transfer learning)'.format(E[i], D[i], alpha[i]))\n    h_temp=model_vgg(E[i], D[i], alpha[i], True)\n    TAB_MOD2_sc.append(h_temp)\n    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Przygotowanie do porównania - macierz cech:"},{"metadata":{"trusted":true},"cell_type":"code","source":"M_VGG_SC_L = matr(E, D, alpha, False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nauczanie modeli z schedulerem (lambda)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(E)):\n    print ('{} epok, {} danych uczących, {} -> startowy learning rate (transfer learning)'.format(E[i], D[i], alpha[i]))\n    h_temp=model_vgg(E[i], D[i], alpha[i], False)\n    TAB_MOD2_lam.append(h_temp)\n    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_graph(TAB_MOD2_sc, M_VGG_SC_L, 'Trained + scheduler')\nmulti_graph(TAB_MOD2_lam, M_VGG_SC_L, 'Trained + lambda scheduler')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Niżej próbowałem zmieniać funkcję scheduler, lecz to tylko pogorszało wyniki."},{"metadata":{"trusted":true},"cell_type":"code","source":"#def scheduler(epoch, lr):\n#    if epoch < 10:\n#        print(lr)\n#        return lr\n#    else:\n#    print(lr * tf.math.exp(-0.1))\n#    return lr * tf.math.exp(-0.1)\n    #if epoch > 15:\n    #    if lr<0.0005:\n    #        return lr*10\n    #    else:\n    #        return lr\n    #else:\n    #    return lr * tf.math.exp(-0.01)\n#    if epoch < 10:\n#        return lr\n#    if epoch > 20:\n#        if lr < 0.0035:\n#            return lr * 2\n#        else:\n#            return lr\n#    else:\n#        print(lr * tf.math.exp(-0.1))\n#        print(lr * tf.math.exp(-0.01))\n#        return lr * tf.math.exp(-0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Porównanie wszystkich modeli:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmulti_graph(TAB_MOD, M_model1, 'First model')\n\nmulti_graph(TAB_MOD1, M_VGG, 'Not Trained')\nmulti_graph(TAB_MOD2, M_VGG_T, 'Trained + lambda scheduler (const learning rate)')\n# - \n#multi_graph(TAB_MOD2_lam, M_VGG_SC_L, 'Trained + lambda scheduler')\nmulti_graph(TAB_MOD2_sc, M_VGG_SC_L, 'Trained + scheduler')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. EfficientNetB0"},{"metadata":{},"cell_type":"markdown","source":"Niestety nie udało mi się zmienić rozmiarów wejściowych danych by dopasować do rozdzielczości odpowiedniej dla systemu. "},{"metadata":{},"cell_type":"markdown","source":"Trenowanie i porównanie"},{"metadata":{"trusted":true},"cell_type":"code","source":"#E=[100, 100, 200, 200]\nE = [30, 30]\n#D=[10000, 10000, 10000, 10000]\nD=[10000, 10000]\n#A=[0.01, 0.05, 0.01, 0.05]\nA=[0.01, 0.05]\nTAB_MOD_B0_sc=[]\n#TAB_MOD_B0_lam=[]\nTAB_MOD_B0_const=[]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(E)):\n    print ('{} epok, {} danych uczących, {} -> startowy learning rate (transfer learning)'.format(E[i], D[i], alpha[i]))\n    h_temp=efficient(E[i], D[i], A[i], True)\n    TAB_MOD_B0_sc.append(h_temp)\n    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_B0 = matr(E, D, alpha, False)\nmulti_graph(TAB_MOD_B0_sc, M_B0, 'Trained + scheduler')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Zakomentowałem z lambdą, gdyż nie starczyło mi czasu GPU by to zapisać, jeszcze nie do końca rozumiem jak to tutaj działa wszystko"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for i in range(len(E)):\n#    print ('{} epok, {} danych uczących, {} -> startowy learning rate (transfer learning)'.format(E[i], D[i], alpha[i]))\n#    h_temp=efficient(E[i], D[i], A[i], False)\n#    TAB_MOD_B0_lam.append(h_temp)\n#    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#multi_graph(TAB_MOD_B0_lam, M_B0, 'Trained + lambda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TAB_MOD_B0_const=[]\nfor i in range(len(E)):\n    print ('{} epok, {} danych uczących, {} -> startowy learning rate (transfer learning)'.format(E[i], D[i], alpha[i]))\n    h_temp=efficient_cons(E[i], D[i], A[i])\n    TAB_MOD_B0_const.append(h_temp)\n    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_graph(TAB_MOD_B0_const, M_B0, 'B0: Trained lr const')\n#multi_graph(TAB_MOD_B0_lam, M_B0, 'B0: Trained + lambda')\nmulti_graph(TAB_MOD_B0_sc, M_B0, 'B0: Trained + scheduler')\n\nmulti_graph(TAB_MOD, M_model1, 'First model')\n\nmulti_graph(TAB_MOD1, M_VGG, 'VGG: Not Trained')\nmulti_graph(TAB_MOD2, M_VGG_T, 'VGG: Trained (const learning rate)')\n\nmulti_graph(TAB_MOD2_lam, M_VGG_SC_L, 'VGG: Trained + lambda scheduler')\nmulti_graph(TAB_MOD2_sc, M_VGG_SC_L, 'VGG: Trained + scheduler')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Widać, że modele VGG16 oraz EfficientNetB0 radzą sobie bardzo źle w porównaniu do początkowego. Też to, że pierwszy model jest najlepszy wynika z samego problemu. Nasze dane to są same znaki, do tego w formacie 32x32. Trudno dopasować parametry odpowiednio, by chociaż się zbliżyć. Dodatkowo sama nauka eksperymentów (różne parametry w modelach) trwają bardzo długo.\n# FUNKCJE\n*wyuczone*\n* efficient(liczba epok, liczba danych trenujących, learning rate, scheduler)\n* efficient_cons(liczba epok, liczba danych trenujących, learning rate, scheduler) - nie zmienia się learning rate w trakcie\n\n* model_vgg16(liczba epok, liczba danych trenujących, learning rate, scheduler) - wyuczony\n* model2(liczba epok, liczba danych trenujących) - od zera\n\n* model(liczba epok, liczba danych trenujących, kernel=3, zmniejszanie learning rate(T/F)) - od zera\n\n**Funkcje pomocnicze wykorzystane**\n* graph(tablica modeli) - rysuje zestaw grafów dla tablicy modeli\n* multi_graph(tablica modeli, macierz z hiperparametrami (parametry funkcji generujących modele) w celu rozróżnienia na legendzie), nazwa) - rysuje na każdym wykresie wszystkie modele z tablicy\n* matr(tablica z ilością epok, danych oraz ew. współczynników nauczania, czy same całkowite) - tworzy macierz przekazywaną do multi_graph"},{"metadata":{},"cell_type":"raw","source":"Struktury danych do tej pory:\n\nTAB_MOD_B0_const - tablica wszystkich modeli EfficientNetB0 bez zmian learning rate (ich cechy w M_B0)\nTAB_MOD_B0_lam - tablica wszystkich modeli EfficientNetB0 z schedulerem w formie wyrażenia lambda (ich cechy w M_B0)\nTAB_MOD_B0_sc - tablica wszystkich modeli EfficientNetB0 z schedulerem w formie funkcji (ich cechy w M_B0)\n\nTAB_MOD2_sc - tablica wytrenowanych modeli VGG z schedulerem w formie funkcji (ich cechy w M_VGG_SC_L)\nTAB_MOD2_lam - tablica wytrenowanych modeli VGG z schedulerem w formie wyrażenia lambda (ich cechy w M_VGG_SC_L)\n\nTAB_MOD - tablica modeli podstawowych (ich cechy w M_model1)"},{"metadata":{},"cell_type":"markdown","source":"# 4. Własny model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PARAMS : epoki, ilość danych, scheduler, ilość danych w batchu\ndef model_test(epochs, val, l_r = 0.005, schedule = False, lower = True, batches = 32):\n    \n    model = Sequential()\n    \n    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(32,32,3)))\n    model.add(Conv2D(32,(3,3), activation='relu'))\n    model.add(MaxPool2D(pool_size=2))\n    model.add(Dropout(0.25))\n              \n    model.add(Conv2D(63, 3, padding='same', activation='relu'))\n    model.add(Conv2D(64,(3,3), activation='relu'))\n    model.add(MaxPool2D(pool_size=2))\n    model.add(Dropout(0.25))\n              \n    model.add(Flatten())\n    \n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n              \n    model.add(Dense(43, activation='softmax'))\n    \n    #model.summary()\n    \n    learning_rate = l_r\n    opt = Adam(lr=learning_rate)\n    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n    \n    if lower==True:\n        annealer = LearningRateScheduler(scheduler)\n\n        h1 = model.fit(data['x_train'][:val], data['y_train'][:val],\n                        batch_size=batches, epochs = epochs,\n                        validation_data = (data['x_validation'], data['y_validation']),\n                        callbacks=[annealer], verbose=1)\n    else:\n        h1 = model.fit(data['x_train'][:val], data['y_train'][:val],\n                        batch_size=batches, epochs = epochs,\n                        validation_data = (data['x_validation'], data['y_validation']),\n                        verbose=1)\n    \n    return h1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#E=[50,   50,   50,    50,    100,  100,  100,   100]\n#D=[6000, 6000, 10000, 10000, 6000, 6000, 10000, 10000 ]\n#A=[0.01, 0.05, 0.01,  0.05,  0.01, 0.05, 0.01,  0.05 ]\nE=[30, 30]\nD=[10000,10000]\nA=[0.01, 0.05]\nM_MY=matr(E,D,A, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TAB_MY_MOD_SC=[]\n#TAB_MY_MOD_LAM=[]\nTAB_MY_MOD_CONST=[]\n\nTAB_1MOD=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TAB_MY_MOD_LAM=[]\n#for i in range(len(E)):\n#    print ('{} epok, {} danych uczących, {} -> learning rate'.format(E[i], D[i], A[i]))\n#    h_temp=model_test(E[i], D[i], A[i])\n#    TAB_MY_MOD_LAM.append(h_temp)\n#    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#multi_graph(TAB_MY_MOD_LAM, M_MY, 'Mój model - scheduler lambda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(E)):\n    print ('{} epok, {} danych uczących, {} -> learning rate'.format(E[i], D[i], A[i]))\n    h_temp=model_test(E[i], D[i], A[i], True)\n    TAB_MY_MOD_SC.append(h_temp)\n    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_graph(TAB_MY_MOD_SC, M_MY, 'Mój model - scheduler funkcja')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(E)):\n    print ('{} epok, {} danych uczących, {} -> learning rate'.format(E[i], D[i], A[i]))\n    h_temp=model_test(E[i], D[i], A[i], True, False)\n    TAB_MY_MOD_CONST.append(h_temp)\n    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_graph(TAB_MY_MOD_CONST, M_MY, 'Mój model - brak zmiany parametru nauczania')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podstawowy model nie miał możliwości zmiany learning rate, co można dodać łatwo w opcjach optymalizatora. Lecz nie będę go poprawiał, skoro chciałem napisać coś lepszego."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(E)):\n    print ('{} epok, {} danych uczących'.format(E[i], D[i]))\n    h_temp=model_test(E[i], D[i])\n    TAB_1MOD.append(h_temp)\n    print('<==============================================>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_MY1=matr(E,D,0, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_graph(TAB_1MOD, M_MY1, 'Początkowy model')\nmulti_graph(TAB_MY_MOD_SC, M_MY, 'Mój model - scheduler funkcja')\nmulti_graph(TAB_MY_MOD_CONST, M_MY, 'Mój model - brak zmiany parametru nauczania')\n\nmulti_graph(TAB_MOD_B0_const, M_B0, 'B0: Trained lr const')\nmulti_graph(TAB_MOD_B0_lam, M_B0, 'B0: Trained + lambda')\nmulti_graph(TAB_MOD_B0_sc, M_B0, 'B0: Trained + scheduler')\n\nmulti_graph(TAB_MOD, M_model1, 'First model')\n\nmulti_graph(TAB_MOD1, M_VGG, 'VGG: Not Trained')\nmulti_graph(TAB_MOD2, M_VGG_T, 'VGG: Trained (const learning rate)')\n\nmulti_graph(TAB_MOD2_lam, M_VGG_SC_L, 'VGG: Trained + lambda scheduler')\nmulti_graph(TAB_MOD2_sc, M_VGG_SC_L, 'VGG: Trained + scheduler')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Udało się. Mój autorski model pokonał ten pierwotny. Validation loss spada o wiele szybciej, do tego każdy model jest całkiem dobry, a z tych przykładowych to tylko kilka wybranych mogłoby się równiać względem Validation loss. Inne miary są nie do dogonienia przez pierwotny model. Na wykresach pierwotnego modelu dodatkowo są powtórzone badania podwójnie (widać, że są dwa takie same podpisy na legendzie - przykładowo niebieski i żółty to są dwie próby/dwa modele o takich samych parametrach)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}