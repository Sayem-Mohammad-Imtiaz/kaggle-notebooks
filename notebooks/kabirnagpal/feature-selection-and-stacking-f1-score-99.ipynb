{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning Models for Human Activity Recognition\n\nThe Dataset used has a lot of features which calls for a tonne of preprocessing.  \nHowever the aim of the notebook is to compare ML models for unprocessed data and try to increase score.  \nWe'll also learn about a feature selection method which can be done to increase score for some or decrease time.  \nIt's basically a tradeoff between time and score.  \n\nEDA for the same has been well demonstrated in [this](https://www.kaggle.com/abheeshthmishra/eda-of-human-activity-recognition) notebook.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import *\nfrom sklearn.tree import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.linear_model import *\nfrom sklearn.naive_bayes import *\nfrom sklearn.svm import *\nfrom sklearn.neighbors import *\nfrom sklearn.tree import *\nfrom sklearn.metrics import *\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Train Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/human-activity-recognition-with-smartphones/train.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# % of Different categories\nAs the percentage is roughly equal, hence we can consider it to a balanced dataset.  \nHowever we'll still use F1-score for comparisons","metadata":{}},{"cell_type":"code","source":"df['Activity'].groupby(df['Activity']).count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity = df['Activity'].groupby(df['Activity']).count().index\nactivity_data = df['Activity'].groupby(df['Activity']).count().values\ncolors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#8c564b\",\"#a4d321\"]\nplt.pie(activity_data, labels=activity,  colors=colors , autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title(\"% of Different categories\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the number of null values","metadata":{}},{"cell_type":"code","source":"print(df.isna().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df.drop(['Activity'],axis=1)\ny = df['Activity']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Models","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = [\n    KNeighborsClassifier(5),\n    SVC(kernel=\"rbf\"),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GaussianNB(),\n    RidgeClassifier(),\n    LogisticRegression(max_iter=200)\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f_score(X_train, X_test, y_train, y_test):\n    for clf in classifiers:\n        s = time.time()\n        clf.fit(X_train,y_train)\n        y_pred = clf.predict(X_test)\n        f = f1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")\n        e = time.time()\n        print(f\"Score: {round(f,3)} \\t Time(in secs): {round(e-s,3)} \\t Classifier: {clf.__class__.__name__}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# F1-Score\n\nRecall = TruePositives / (TruePositives + FalseNegatives)\n\nPrecision = TruePositives / (TruePositives + FalsePositives)\n\nF1 = 2 (precision recall) / (precision + recall)","metadata":{}},{"cell_type":"markdown","source":"### Accuracy for train data","metadata":{}},{"cell_type":"code","source":"f_score(X_train, X_test, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Above score achieved is after splitting train data and not test data**","metadata":{}},{"cell_type":"markdown","source":"### Accuracy for test data","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/human-activity-recognition-with-smartphones/test.csv\")\ndf_test_x = df_test.drop(['Activity'],axis=1)\ndf_test_y = df_test['Activity']\nf_score(x, df_test_x, y, df_test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking Classifier\nStacking classifier build a new classifier.  \nTo learn more refer [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)","metadata":{}},{"cell_type":"code","source":"estimators = [\n        ('RFC' ,RandomForestClassifier(n_estimators=500, random_state = 42)),\n        ('KNC', KNeighborsClassifier(5)),\n        ('DTC', DecisionTreeClassifier()),\n        ('SVC', SVC(kernel=\"rbf\")),\n        ('RC',  RidgeClassifier()),\n]\n\nclf = StackingClassifier(\n    estimators=estimators, \n    final_estimator=GradientBoostingClassifier()\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy for train data","metadata":{}},{"cell_type":"code","source":"s = time.time()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy for test data","metadata":{}},{"cell_type":"code","source":"s = time.time()\nclf.fit(x,y)\ny_pred = clf.predict(df_test_x)\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=df_test_y,y_pred=y_pred,average=\"macro\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the stacking classifier does a great work of boosting accuracy to **99+** for train data and **96+** for test data.  \nHowever it consumes a lot of time.  ","metadata":{}},{"cell_type":"markdown","source":"# Trying reducing number of features\nRandom forest classifier determines importance of variables.  \nThis can be used to filter most important features.  \nYou may also use Logistic regression.  \nTo understand simply: Logistic regression determines linear coeffecients.  \nCoeffecients with higher magnitudes have a greater impact on `Y` than others.","metadata":{}},{"cell_type":"code","source":"sel = SelectFromModel(RandomForestClassifier())\nsel.fit(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = x.columns[(sel.get_support())]\nprint(len(features))\nfeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence Random Forest find these 125 features as important","metadata":{}},{"cell_type":"code","source":"X1 = x.filter(items=features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training on reduced dataset","metadata":{}},{"cell_type":"markdown","source":"### Accuracy for train data","metadata":{}},{"cell_type":"code","source":"f_score(X_train, X_test, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy for test data","metadata":{}},{"cell_type":"code","source":"f_score(X1, df_test_x.filter(items=features), y, df_test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train with reduced Dataset with Stacking Classifier","metadata":{}},{"cell_type":"markdown","source":"### Accuracy for train data","metadata":{}},{"cell_type":"code","source":"s = time.time()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy for test data","metadata":{}},{"cell_type":"code","source":"s = time.time()\nclf.fit(X1,y)\ny_pred = clf.predict(df_test_x.filter(items=features))\ne = time.time()\nprint(f\"time consumed: {round(e-s,3)}\")\nf1_score(y_true=df_test_y,y_pred=y_pred,average=\"macro\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After Feature selection we get maximum train score of **98+** and test score of **93+**","metadata":{}},{"cell_type":"markdown","source":"### The tradeoff between score and time\nIn most cases the models will be trained prior and deployed with just the weights, however in situations with on device processing like a smartphone we need to decide what we want.  \nStacking almost always boosts your accuracy as explained in case above, it does comes at the cost of extra training time.  \nI hope this notebook helped you.  \n**Happy Learning**","metadata":{}}]}