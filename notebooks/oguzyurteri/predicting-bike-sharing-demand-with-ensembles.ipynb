{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Bike Sharing Demand with Ensembles","metadata":{}},{"cell_type":"markdown","source":"Detailed explanation for a similar project can be found in my article in the following link:\n\nhttps://www.linkedin.com/pulse/predicting-bike-sharing-demand-ensemble-methods-oğuz-can-yurteri/","metadata":{}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"Bike sharing system is a modern way of bike rentals in which bikes are available for individuals to borrow a bike from a dock and return it at another belonging to the same system. Membership, rental and return processes are generally automatic and digital. By the half of 2018, there are   ap-proximately 1600 bike sharing programs with 18,2 million bikes in over 1000 cities.\n\nApart from their positive effect on traffic, environmental and health issues, bike sharing systems are attractive due to the data they generate for research purposes. The departure and arrival time, location and duration of usage are explicitly recorded in these systems. This data could be used to measure the mobility and detect the important events of a city.\n\nThe goal of this project is to predict the number of hourly bike usage based on environmental and seasonal conditions. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom numpy import median\nfrom numpy import mean\n\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.metrics import mean_squared_log_error as msle \nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.inspection import permutation_importance\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv('/kaggle/input/bike-sharing-dataset/hour.csv')\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:33:43.735888Z","iopub.execute_input":"2021-07-17T11:33:43.736346Z","iopub.status.idle":"2021-07-17T11:33:43.817069Z","shell.execute_reply.started":"2021-07-17T11:33:43.736304Z","shell.execute_reply":"2021-07-17T11:33:43.816273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a better understanding and improving readability, the names of the attributes are changed as following:\n\n- instant: instant\n- dteday: date\n- season: season\n- yr: year\n- mnth: month\n- hr: hour\n- holiday: holiday\n- weekday: weekday\n- workingday: workingday\n- weathersit: weathersit\n- temp: temp\n- atemp: feel_temp\n- hum: humidity\n- windspeed: windspeed\n- casual: casual\n- registered: registered\n- cnt: bike_use (Target variable)\n\nThe target variable which is predicted is bike_use. The differentiation between the casual and registered users is not in the scope of the project, that’s why they are removed from the dataset. Besides, instant attribute is just an index, so it is also removed.\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T09:10:09.626762Z","iopub.execute_input":"2021-07-17T09:10:09.627353Z","iopub.status.idle":"2021-07-17T09:10:09.648309Z","shell.execute_reply.started":"2021-07-17T09:10:09.627315Z","shell.execute_reply":"2021-07-17T09:10:09.646969Z"}}},{"cell_type":"code","source":"data.columns = ['instant','date','season','year','month','hour','holiday','weekday','workingday','weathersit','temp','feel_temp','humidity','windspeed','casual','registered','bike_use']\n\ndata.drop(columns='instant',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:33:46.997988Z","iopub.execute_input":"2021-07-17T11:33:46.998411Z","iopub.status.idle":"2021-07-17T11:33:47.010037Z","shell.execute_reply.started":"2021-07-17T11:33:46.998377Z","shell.execute_reply":"2021-07-17T11:33:47.008756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"There are 17379 instances in the dataset which means that some dates and hours are missing in two years. However, there is not any blank row or column. \n\nIt can be seen that the number of instances is balanced among the seasons, years, months, hours and days as expected. For this reason, missing dates and hours are not an issue because they are random, most probably due to the problems in data acquisition.\n\nThe number of holidays and non-working days are less than the number of non-holidays and working days, and the number of the days with weather situation 1(clear, few clouds, partly cloudy) are the most as expected.","metadata":{}},{"cell_type":"code","source":"cat_var = ['season','year','month','hour']\n\nfor var in cat_var:\n    sns.countplot(var,data=data) \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:15.121963Z","iopub.execute_input":"2021-07-17T10:00:15.122712Z","iopub.status.idle":"2021-07-17T10:00:15.837389Z","shell.execute_reply.started":"2021-07-17T10:00:15.12266Z","shell.execute_reply":"2021-07-17T10:00:15.83611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_var2 = ['holiday', 'weekday','workingday', 'weathersit']\n\nfor var in cat_var2:\n    sns.countplot(var,data=data) \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:15.839233Z","iopub.execute_input":"2021-07-17T10:00:15.839564Z","iopub.status.idle":"2021-07-17T10:00:16.60465Z","shell.execute_reply.started":"2021-07-17T10:00:15.839523Z","shell.execute_reply":"2021-07-17T10:00:16.603523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions of continuous variables temp and feel_temp are close to normal distribution, however the distribution of continuous variable windspeed is right-skewed and humidity is left-skewed. ","metadata":{}},{"cell_type":"code","source":"cont_var = ['temp', 'feel_temp','humidity', 'windspeed']\n\nfor var in cont_var:\n    sns.distplot(data.loc[:,var],kde=False) \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:16.605956Z","iopub.execute_input":"2021-07-17T10:00:16.606258Z","iopub.status.idle":"2021-07-17T10:00:17.489656Z","shell.execute_reply.started":"2021-07-17T10:00:16.606229Z","shell.execute_reply":"2021-07-17T10:00:17.488173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the target variable, bike_use, is right-skewed and its value varies between 0-1000.","metadata":{}},{"cell_type":"code","source":"sns.distplot(data.bike_use,kde=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:17.491249Z","iopub.execute_input":"2021-07-17T10:00:17.491588Z","iopub.status.idle":"2021-07-17T10:00:17.762154Z","shell.execute_reply.started":"2021-07-17T10:00:17.491555Z","shell.execute_reply":"2021-07-17T10:00:17.760869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation map shows that people tend to use bike less in bad weather conditions and high humidity, but more in high temperatures. Temp, feel_temp and humidity variables should be used as inputs in modelling. Also, there is a strong positive correlation between temp and feel_temp attributes. ","metadata":{}},{"cell_type":"code","source":"corrmat = data.iloc[:,[8,9,10,11,12,15]].corr()\n\nsns.heatmap(corrmat, vmax=0.9,square=True, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:17.763459Z","iopub.execute_input":"2021-07-17T10:00:17.763786Z","iopub.status.idle":"2021-07-17T10:00:18.264764Z","shell.execute_reply.started":"2021-07-17T10:00:17.763755Z","shell.execute_reply":"2021-07-17T10:00:18.263615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The seasonal effect is too high in bike usage and also, there are different demand levels among the months in the same season. That’s why, both season and month should be used as inputs in the model. \n\nAlthough both season and month is nominal categorical variables, only season attribute will be one-hot encoded since adding 12 extra attributes will increase the sparsity too much which does not worth for low variation between the months in the same season. \n\nThe summation of bike usage in 2012 is higher than 2011 which means that both the number of users and de-mand of the system increased by time, so the year attribute should also be used as input in the model. \n\nThe bike usage is at maximum especially in rush hours. By looking at the relationship between bike_use and hour closer, it can be seen that the hours 7, 8, 9, 17, 18, 19, and 20 have different characteristics in working days. To represent this characteristic better, a new feature called is_towork could be defined (1: hour is 7 or 8 or 9 or 17 or 18 or 19 and workingday is 1, 0: otherwise) to determine whether this hour is rush hour or not. The hour attribute could be one-hot encoded instead, however it will increase the sparsity and dimension of the data too much by add-ing 24 extra attribute which is not preferred. So, the original hour attribute and the new is_towork attribute will be used together as inputs. ","metadata":{}},{"cell_type":"code","source":"cat_var = ['season','year','month','hour']\n\nfor var in cat_var:\n    sns.barplot(x=var,y='bike_use',data=data,estimator=sum)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:18.267245Z","iopub.execute_input":"2021-07-17T10:00:18.267608Z","iopub.status.idle":"2021-07-17T10:00:47.739856Z","shell.execute_reply.started":"2021-07-17T10:00:18.267575Z","shell.execute_reply":"2021-07-17T10:00:47.738458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'hour'\ndata1 = pd.concat([data['bike_use'], data[var]],axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='hour', y=\"bike_use\", hue='workingday', data=data)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:47.741369Z","iopub.execute_input":"2021-07-17T10:00:47.741741Z","iopub.status.idle":"2021-07-17T10:00:49.124974Z","shell.execute_reply.started":"2021-07-17T10:00:47.741709Z","shell.execute_reply":"2021-07-17T10:00:49.123718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_var2 = ['holiday', 'weekday','workingday', 'weathersit']\n\nfor var in cat_var2:\n    sns.barplot(x=var,y='bike_use',data=data,estimator=mean)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:49.126255Z","iopub.execute_input":"2021-07-17T10:00:49.126555Z","iopub.status.idle":"2021-07-17T10:00:50.856571Z","shell.execute_reply.started":"2021-07-17T10:00:49.126526Z","shell.execute_reply":"2021-07-17T10:00:50.855569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'weekday'\ndata1 = pd.concat([data['bike_use'], data[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"bike_use\", data=data1)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:00:50.857975Z","iopub.execute_input":"2021-07-17T10:00:50.858569Z","iopub.status.idle":"2021-07-17T10:00:51.125302Z","shell.execute_reply.started":"2021-07-17T10:00:50.858523Z","shell.execute_reply":"2021-07-17T10:00:51.123957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(columns=['date','casual','registered'],inplace=True)\n\ndata['is_weekend'] = [1 if i==0 else 1 if i==6 else 0 for i in data.weekday]\ndata['is_towork'] = [1 if (data.loc[i,'hour']==7 or data.loc[i,'hour']==8 or data.loc[i,'hour']==9 or data.loc[i,'hour']==17 or data.loc[i,'hour']==18 or data.loc[i,'hour']==19 or data.loc[i,'hour']==20) and (data.loc[i,'workingday']==1) else 0 for i in data.index]\n\ndata = pd.get_dummies(data, columns=[\"season\"],prefix='season_is')","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:33:53.880583Z","iopub.execute_input":"2021-07-17T11:33:53.881178Z","iopub.status.idle":"2021-07-17T11:33:55.451213Z","shell.execute_reply.started":"2021-07-17T11:33:53.881141Z","shell.execute_reply":"2021-07-17T11:33:55.450008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the exploratory data analysis, the following attributes will be used as inputs of the regres-sion model to predict bike_use:\n\n- year \n- month \n- hour\n- holiday \n- weekday \n- workingday\n- weathersit \n- temp \n- feel_temp \n- humidity \n- windspeed \n- bike_use\n- is_weekend \n- is_towork \n- season_is_1 (one-hot encoded season variable)\n- season_is_2 (one-hot encoded season variable)\n- season_is_3 (one-hot encoded season variable)\n- season_is_4 (one-hot encoded season variable)","metadata":{}},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"markdown","source":"The dataset is divided into training (80% of the data) and test (20% of the data) dataset. Both of them represents the overall attribute features (covers all seasons, months, days, hours etc.). \n\nIn order to predict hourly bike usage, two decision tree based ensemble learning methods, Random Forest Regression and Extreme Gradient Boosting (XGBoost) Regression, are used separately and the results are compared.\n\nEnsemble learning methods combine several machine learning algorithms to improve results. Ran-dom Forest Regression uses bootstrap aggregating to ensemble different decision trees. Many de-cision trees are formed in parallel with different training data subsets. The different training subsets are built by sampling the training dataset with replacement (bootstrap), also different subset of fea-tures can be used in each tree. The prediction is calculated by averaging the results of the decision trees (aggregating). XGBoost uses boosting to ensemble different decision trees. Many decision trees are formed sequentially to learn from the errors (residuals) from its predecessor decision tree. \n\nMean absolute error (MAE) and mean squared logarithmic error (MSLE) are selected as evaluation metrics. Mean absolute error shows how many bikes in average is deviated from the actual demand. Mean squared logarithmic error is selected since it penalizes underestimates more than overestimates. Supplying insufficient bikes especially in rush hours could lead to problems such as customer dissatisfaction and losing customers, that’s why underestimating is more harmful than overestimating. ","metadata":{}},{"cell_type":"code","source":"x = data.drop(columns=['bike_use'])\ny = data.bike_use\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:33:58.374539Z","iopub.execute_input":"2021-07-17T11:33:58.374999Z","iopub.status.idle":"2021-07-17T11:33:58.396967Z","shell.execute_reply.started":"2021-07-17T11:33:58.374961Z","shell.execute_reply":"2021-07-17T11:33:58.395823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grid search cross validation method with 5-fold is applied to training dataset to find the optimum number of decision trees (estimators) and maximum depth for each tree for Random Forest Regression and XGBoost Regression based on mean absolute error. ","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:09:35.79384Z","iopub.execute_input":"2021-07-17T10:09:35.794362Z","iopub.status.idle":"2021-07-17T10:09:35.802511Z","shell.execute_reply.started":"2021-07-17T10:09:35.794317Z","shell.execute_reply":"2021-07-17T10:09:35.80111Z"}}},{"cell_type":"code","source":"model = xgb.XGBRegressor()\n\nn_estimators = [1000,2000,3000]\nmax_depth = [3,6]\n\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_mean_absolute_error\")\n\ngrid_result = grid_search.fit(x_train,y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:12:05.692379Z","iopub.execute_input":"2021-07-17T10:12:05.692901Z","iopub.status.idle":"2021-07-17T10:17:45.041097Z","shell.execute_reply.started":"2021-07-17T10:12:05.692859Z","shell.execute_reply":"2021-07-17T10:17:45.039776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestRegressor()\n\nn_estimators = [500,1000,1500]\nmax_depth = [3,6]\n\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_mean_absolute_error\")\n\ngrid_result = grid_search.fit(x_train,y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:30:31.774592Z","iopub.execute_input":"2021-07-17T10:30:31.774984Z","iopub.status.idle":"2021-07-17T10:37:36.72293Z","shell.execute_reply.started":"2021-07-17T10:30:31.774952Z","shell.execute_reply":"2021-07-17T10:37:36.721847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost is better in terms of MAE, on the other hand Random Forest is slightly better in terms of MSLE on the test dataset.","metadata":{}},{"cell_type":"code","source":"rf_reg = RandomForestRegressor(n_estimators=1000, max_depth=6)\n\nrf_reg.fit(x_train,y_train)\n\npred_test = rf_reg.predict(x_test)\n\nprint(\"mae test: \",mae(y_test,pred_test))\nprint(\"msle test: \",msle(y_test,pred_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T10:57:06.514095Z","iopub.execute_input":"2021-07-17T10:57:06.514465Z","iopub.status.idle":"2021-07-17T10:57:27.857776Z","shell.execute_reply.started":"2021-07-17T10:57:06.514428Z","shell.execute_reply":"2021-07-17T10:57:27.856342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_reg = xgb.XGBRegressor(n_estimators=1000, max_depth=6)\n\nxgb_reg.fit(x_train,y_train)\n\npred_test = xgb_reg.predict(x_test)\n\npred_test = [0 if i<0 else i for i in pred_test]\n\nprint(\"mae test: \",mae(y_test,pred_test))\nprint(\"msle test: \",msle(y_test,pred_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:34:02.25294Z","iopub.execute_input":"2021-07-17T11:34:02.253468Z","iopub.status.idle":"2021-07-17T11:34:11.084225Z","shell.execute_reply.started":"2021-07-17T11:34:02.253394Z","shell.execute_reply":"2021-07-17T11:34:11.08278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature importance from mean impurity decrease and permutation importance of XGBoost can be seen below. \n\nMean impurity decrease feature importance method measures the feature importance based on how much each feature contributes to decreasing the weighted impurity in each split of trees in terms of variance. \n\nPermutation feature importance method measures the feature importance by re-shuffling each feature in test dataset and observing how much the model performance decreases. \n\nBoth methods illustrate that hour, is_towork temp, year, humidity and workingday are important features to predict bike demand. The assumption made for is_towork proves itself with its importance in the model. The assumption made for is_weekend is also an important feature in terms of permutation importance, however the assumption made for it is relatively less valid than is_towork.","metadata":{}},{"cell_type":"code","source":"# Mean impurity decrease feature importance\n\nfeat_importances = pd.Series(xgb_reg.feature_importances_, index=x_train.columns)\nfeat_importances.sort_values(ascending=True,inplace=True)\nfeat_importances.plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:34:13.45048Z","iopub.execute_input":"2021-07-17T11:34:13.45085Z","iopub.status.idle":"2021-07-17T11:34:14.755099Z","shell.execute_reply.started":"2021-07-17T11:34:13.450819Z","shell.execute_reply":"2021-07-17T11:34:14.753975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Permutation feature importance\n\nresult = permutation_importance(xgb_reg, x_test, y_test, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=x_test.columns[sorted_idx])\nfig.tight_layout()\nplt.show()    ","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:34:46.256866Z","iopub.execute_input":"2021-07-17T11:34:46.257295Z","iopub.status.idle":"2021-07-17T11:35:01.832151Z","shell.execute_reply.started":"2021-07-17T11:34:46.257262Z","shell.execute_reply":"2021-07-17T11:35:01.830703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Residuals resulted from XGBoost can be seen below. \n\nThe residuals are distributed symmetrically around 0, but not normal which means that there are trends which are not represented by the model. Especially, there are some residuals with high magnitude. Investigating some specific dates and hours with high deviation shows that there are specific events happened on that days such as holidays, earthquakes, festivals which cannot be taken as inputs to the model, that’s why cannot be represented by the model. \n\nAlthough it causes a decrease in the performance, this situation could allow us to use the data and the model for validation of anomaly or event detection algorithms as well.","metadata":{}},{"cell_type":"code","source":"res = y_test - pred_test\n\nsns.distplot(res,kde=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T11:35:44.479971Z","iopub.execute_input":"2021-07-17T11:35:44.480428Z","iopub.status.idle":"2021-07-17T11:35:44.741792Z","shell.execute_reply.started":"2021-07-17T11:35:44.48038Z","shell.execute_reply":"2021-07-17T11:35:44.741055Z"},"trusted":true},"execution_count":null,"outputs":[]}]}