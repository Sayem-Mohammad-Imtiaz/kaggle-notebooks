{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport missingno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom collections import Counter\n\n# set default plotting parameter\nsns.set_theme(\"paper\", \"darkgrid\")\nplt.rc(\"axes\", titlesize=18)\nplt.rc(\"axes\", titleweight=\"bold\")\nplt.rc(\"axes\", labelsize=13)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load & explore data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze missing values","metadata":{}},{"cell_type":"markdown","source":"## Show missing values using missingno","metadata":{}},{"cell_type":"code","source":"missingno.matrix(df, figsize=(8,4), fontsize=12);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns \"director\", \"cast\" and \"country\" containing the most missing values. We can use a heatmap visualization to get an understanding of the correlation between missing values in different columns.","metadata":{}},{"cell_type":"code","source":"missingno.heatmap(df, figsize=(8,4), fontsize=12);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest correlation can be found between `country` and `director`, meaning that if we find a `NaN` value in `country` then it's more likely to find a `director` `NaN` as well.\nThis correlation includes suggestions which columns are the most promising to drop `NaN` values.","metadata":{}},{"cell_type":"markdown","source":"# Handle missing values","metadata":{}},{"cell_type":"markdown","source":"We can simply drop the rows where \"date_added\" contains missing values, which results in the following DataFrame.","metadata":{}},{"cell_type":"code","source":"# reset index fit on the shape\ndf = df.dropna(subset=[\"date_added\"]).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the remaining columns are of type string (object) we're able to fill missing values with the keyword \"missing\". Consequently there are no missing values anymore.","metadata":{}},{"cell_type":"code","source":"# fill missing values in director column\ndf[\"director\"] = df.director.fillna(\"missing\")\ndf[\"cast\"] = df.cast.fillna(\"missing\")\ndf[\"country\"] = df.country.fillna(\"missing\")\ndf[\"rating\"] = df.rating.fillna(\"missing\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\n1. split `date_added` into month, date and year","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Split date_added into month, date and year","metadata":{}},{"cell_type":"code","source":"# added_*: extract day/month/year from date_added\n# duration_unit: extract unit from duration as \"Season\" or \"Minute\"\n# duration_quality: extract numerical value from duration irrespective \n#                   of duration_unit the given duration_unit\ndf = df.assign(\n    added_month=pd.to_datetime(df.date_added).dt.month,\n    added_date=pd.to_datetime(df.date_added).dt.day,\n    added_year=pd.to_datetime(df.date_added).dt.year,\n    duration_unit=np.where(df.duration.str.contains(\"Season\"), \"Season\", \"Minute\"),\n    duration_quantity=lambda x: x.duration.str.split(\" \", expand=True)[0],\n).drop(\"date_added\", axis=1).astype({\"duration_quantity\":\"int\"})\n\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking that we didn't created any missing values leads to","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nAt first we compare the number of movies agains the number of tv shows.\n","metadata":{}},{"cell_type":"code","source":"\ndf.type.value_counts().plot(\n    kind=\"bar\",\n    width=0.5, \n    color=\"red\",\n    title=\"Type frequency on netflix \\n dataset\",\n    ylabel=\"Frequency\", \n    xlabel=\"Type Category\",\n    rot=0\n);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graph we see that there are twice as many movies as tv shows on Netflix.\n\nFurthermore we want to know which are the most common directors in our dataset.","metadata":{}},{"cell_type":"code","source":"df.director.value_counts().drop(labels=[\"missing\"]).head(20).plot(\n    kind=\"bar\", \n    color=\"red\",\n    figsize=(8,4),\n    rot=80,\n    title=\"Director frequency on netflix dataset\",\n    width=0.5, \n    xlabel=\"Director\",\n    ylabel=\"Frequency\",\n);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Answering that question, we find a winner with `Ra√∫l Campos` and `Jan Suter` but there is no obvious leading role since the top 5 directors are more or less in the same range.\n\nSince movies and tv shows are pretty imbalanced we apply the same analysis separately on movies and tv shows which leads to the following.","metadata":{}},{"cell_type":"code","source":"df_movie = df.query(\"type == 'Movie'\")\ndf_tvshow = df.query(\"type == 'TV Show'\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,8))\ndf_movie.director.value_counts()[1:20].plot(kind=\"bar\", color=\"red\", ax=ax1, ylim=(0, 20))\ndf_tvshow.director.value_counts()[1:20].plot(kind=\"bar\", color=\"grey\", ax=ax2, ylim=(0, 20))\nax1.set_title(\"Movies\", fontsize=24, fontweight=\"bold\")\nax2.set_title(\"TV Shows\", fontsize=24, fontweight=\"bold\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the tv show data is less relevant regarding the director frequency since most of them can be found only once.\n\n### Which are the most frequent actors in our dataset?\nSimilar to the directors frequency we can ask for the **actors frequency**.","metadata":{}},{"cell_type":"code","source":"# extract all actors from \"cast\"\ndata = [x.split(\",\") for x in df.cast.to_list()]\ndata = list(itertools.chain(*data))\n\n# get data histogram\nelement = Counter(data)\n\n# cast element data into DataFrame\ndf_cast = (pd.DataFrame.from_dict(element, orient=\"index\", columns=[\"actor_freq\"])\n           .reset_index().\n           rename(columns={\"index\":\"cast\"})\n           .sort_values(by=\"actor_freq\", ascending=False)\n           .query(\"cast != 'missing'\")\n           .reset_index(drop=True)\n)\n\n# plot values\ndf_cast.head(20).plot(\n    kind=\"bar\", \n    x=\"cast\", \n    y=\"actor_freq\", \n    color=\"red\",\n    figsize=(8,4),\n    width=0.7, \n    title=\"Actor Frequency\",\n    xlabel=\"Actor\",\n    ylabel=\"Frequency\",\n    rot=80,\n);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From which country can we find the most dataset entries?","metadata":{}},{"cell_type":"code","source":"# get frequencies of all filming spot contries\nspot_categories = [x.split(\",\") for x in df.country.to_list()]\nspot_categories = list(itertools.chain(*spot_categories))\n\n# get data histogram\nspot_freq = Counter(spot_categories)\n\n# cast histogram to DataFrame\nspot_df = (pd.DataFrame.from_dict(spot_freq, orient=\"index\", columns=[\"spot_frequency\"])\n           .reset_index()\n           .rename(columns={\"index\":\"country\"})\n           .sort_values(by=\"spot_frequency\", ascending=False)\n           .query(\"country != 'missing'\")\n           .reset_index(drop=True)\n          )\n\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,4))\n\n# plot DataFrame histogram\nspot_df.head(20).plot(\n    kind=\"bar\", \n    x=\"country\", \n    y=\"spot_frequency\",\n    ax=ax1,\n    color=\"red\",\n    #figsize=(8,4),\n    width=0.8,\n    xlabel=\"Country\",\n    ylabel=\"Frequency\",\n    title=\"Netflix location-added frequency\",\n    rot=70)\n\nspot_df.assign(spot_frequency=lambda x: x.spot_frequency / spot_df.spot_frequency.sum()).head(20).plot(\n    kind=\"bar\", \n    x=\"country\", \n    y=\"spot_frequency\",\n    ax=ax2,\n    color=\"red\",\n    #figsize=(8,4),\n    width=0.8,\n    xlabel=\"Country\",\n    ylabel=\"% Frequency\",\n    ylim=(0,1),\n    title=\"Netflix location-added percentage\",\n    rot=70);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obvously, the United States are included overproporionally.","metadata":{}},{"cell_type":"markdown","source":"### Are there months where more shows or movies are added as normally?","metadata":{}},{"cell_type":"code","source":"df.added_month.value_counts().plot(\n    kind=\"bar\", \n    x=\"month\", \n    y=\"frequency\", \n    color=\"red\", \n    figsize=(7,4),\n    rot=0,\n    title=\"Netflix monthly added frequency\",\n    width=0.6,\n    xlabel=\"Month\",\n    ylabel=\"Frequency\",\n    ylim=(0,1000),\n);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We find a slightly sign that netflix adds more movies / shows when reaching the end of the year (including january as well). This can be interpreted with the motivation that netflix tries to be more attractive selling their subscription as a christmas present. This interpretation could conform with the local peak at march and may where a netflix subscription may be an easter present as well.","metadata":{}},{"cell_type":"markdown","source":"## How does the release frequency of movies / TV shows change over time?","metadata":{}},{"cell_type":"code","source":"release_timeline = df.groupby('type')['release_year'].value_counts()\n\nfig, (ax1, ax2) = plt.subplots(\n    1, \n    2, \n    figsize=(16, 6), \n    subplot_kw={\n        \"ylim\": (0, release_timeline.max() + 50),\n    })\nrelease_timeline[\"Movie\"].head(20).plot(\n    kind=\"bar\",\n    ax=ax1,\n    color=\"red\",\n    rot=50,\n    title=\"Movie release frequency\", \n    xlabel=\"Release Year\", \n    ylabel=\"Frequency\",\n)\nrelease_timeline[\"TV Show\"].head(20).plot(\n    kind=\"bar\", \n    ax=ax2,\n    color=\"red\",\n    rot=50,\n    title=\"TV Show release frequency\", \n    xlabel=\"Release Year\", \n    ylabel=\"Frequency\",\n);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like netflix' *focus lies currently on tv shows*, although definitely more movies were added in the recent history. \n\nThe *decreasing movie numbers* can be partially interpreted with the pandemic occurence of Covid and the corresponding problems for directors to produce new movies. But we can see the first decreasing numbers in 2018, long befor Covid was a problem. \n\nKeeping that in mind, it is more likely that netflix tries to change his *focus from movies towards creating and adding more and more tv shows*, because this is what customer will bind to the company and leads to keeping subscriptions alive from the perspective of their customers.","metadata":{}},{"cell_type":"markdown","source":"### Which categories are the most represented in our dataset?","metadata":{}},{"cell_type":"code","source":"data = [x.split(\",\") for x in df.listed_in.to_list()]\ndata = list(itertools.chain(*data))\n\nlisted_count = Counter(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create DataFrame from listed_in frequencies using listed_dic\nlisted_in_freq = (pd.DataFrame\n    .from_dict(listed_count, orient=\"index\", columns=[\"frequency\"])\n    .sort_values(by=[\"frequency\"], ascending=False)\n)\n\n# plot frequencies\nlisted_in_freq.head(50).plot(\n    kind=\"bar\",\n    color=\"red\",\n    figsize=(18, 6),\n    rot=82,\n    title=\"Category Frequency of netflix movies / shows\",\n    width=0.7,\n    xlabel=\"Movie/Show Category\",\n    ylabel=\"Frequency\",\n);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesingly, `International Movies`, `Dramas` and `Comedies` are the top 3 categories, followed by `Documentaries`.","metadata":{}},{"cell_type":"markdown","source":"### How many seasons are most frequent in our tv show dataset?","metadata":{}},{"cell_type":"code","source":"# filter only Season entries and count values\nseason_freq = df.query(\"duration_unit == 'Season'\").duration_quantity.value_counts()\n\n# use sum aggregation for count values below a specific threshold \nthreshold = 30\nseason_freq_restagg = season_freq[season_freq >= threshold] \nseason_freq_restagg[\"Rest\"] = season_freq[season_freq < threshold].sum()\n\n# create chart display lables\nseason_label = [\"1 Season\"] + [str(season_int) + \" Seasons\" for season_int in season_freq_restagg.index[1:-1]] + [\"7+ Seasons\"]\n\nseason_freq_restagg.plot(\n    kind=\"pie\", \n    autopct='%1.2f%%',\n    figsize=(10,8),\n    labels=season_label,\n    pctdistance=0.7,\n    startangle=180,\n    title=\"Season frequency at Netflix dataset\",\n    ylabel=\"\",\n    \n);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is another surprising insight. Two of three netflix tv shows consist of only one season. Only one of three tv shows consist of more or equal than two seasons. We can conclude that tv shows like `Game of thrones` is one of the most successful productions of netflix but at the same time it is a quite rare show (<1.25%).","metadata":{}},{"cell_type":"markdown","source":"### How is the movie duration distributed?","metadata":{}},{"cell_type":"code","source":"# create binsizes\nbins = [*range(0,316,15)]\n\ndf.query(\"duration_unit == 'Minute'\").duration_quantity.plot(\n    kind=\"hist\", \n    bins=bins, \n    color=\"red\",\n    figsize=(10,6),\n    title=\"Netflix movie duration histogram\",\n)\nplt.xticks(bins);\nplt.xlabel(\"Minutes\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we expected, the most movies will end after 90-105 minutes.","metadata":{}},{"cell_type":"markdown","source":"### Just a final view on netflix ratings","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(figsize=(12,5))\nsns.countplot(\n    x=\"rating\", \n    data=df, \n    order=df.rating.value_counts().index.to_list(),\n)\nax.set_title(\"Most common ratings on Netflix dataset\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"count\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a Recommender System","metadata":{}},{"cell_type":"markdown","source":"## First Approach: Tf-idf\n\n### Theoretical background\nThis approach uses the *scikit-learn* learn library. The term **_Tf-idf_** stands for **_term frequency-inverse document frequency_**, which designates a way to create a metric which aims to score the importance of a specific word for a given document.\n\nIn general, one will consider not just one document but multiple documents $d_i$ in a so called corpus $C=\\{d_i | 1 \\leq i \\leq n\\}$. There are vaious ways to calculate Tf-idf (see Wikipedias article on [Tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition) for details. **NOTE**: we omit the idf-parameter $D$ which counts the documents whose contain the term $t$ for the sake of simplicity).\n\nWe add some insights from the Tfidf function:\n* $\\text{tfidf}(t,d)=0$ means that $t$ is not necessary for the document $d$\n* if a term $t$ is not contained in a document $d$ (term-frequency is $0$), then $\\text{tfidf}(t,d)=0$\n* if a term $t$ occurs in every document of the corpus $C$ (inverse-document-frequency is $0$), then $\\text{tfidf}(t,d)=0$ as well\n* $\\text{tfidf}(t,d)=0$, if $t$ occurs in every document (e.g. common words like \"the\")\n* $\\text{tfidf} \\geq 0$\n\n\nIt's important to understand that Tf-idf is a metric $\\text{tfidf}(t,d)$ for one term $t$ in a given document $d$. Once being aware of that thing we can go on calculating all values $\\text{tfidf}(t_i,d_j)$ for all the documents $d_j \\in C$ and where $t_i$ for $1 \\leq i \\leq m$ were all the different terms (words) occurring in all document in $C$.\n\nGiven a random term $\\bar{t}$ in one of the documents $d_i$ and another random document $\\bar{d}$ in the corpus $C$, then the _Tf-idf_ metric itself can be evaluated on $\\bar{t}$ and $\\bar{d}$ given by $\\text{tfidf}(\\bar{t}, \\bar{d})$. \n\nThe interpretation of the resulting score answers the question on **how important is the term $\\bar{t}$ with respect to the document $\\bar{d}$**. A zero value can be interpreted as the term $\\bar{t}$ is not important for the document $\\bar{d}$ beacuse that document does not contain that term. With an increasing _Tf-idf_ score the importance increases as well.\n\nWith that understanding we can arrange our scores as follows:\n\n$$\n\\begin{pmatrix}\n\\text{tfidf}(t_1, d_1) & \\text{tfidf}(t_2, d_1) & \\cdots & \\text{tfidf}(t_n, d_1)\\\\ \n\\text{tfidf}(t_1, d_2) & \\text{tfidf}(t_2, d_2) & & \\\\ \n& & \\ddots & \\vdots\\\\\n\\text{tfidf}(t_1, d_m) & & \\cdots & \\text{tfidf}(t_n, d_m)\\\\\n\\end{pmatrix} =: \\left(x_1, \\cdots , x_n\\right) \\in \\mathbf{R}^{m \\times n}\n$$\n\nNormalizing these vectors $x_i$ leads to the following representation\n\n$$\n\\left(\\frac{x_1}{\\lVert x_1 \\rVert}, \\cdots , \\frac{x_n}{\\lVert x_n \\rVert} \\right) =: X \\in \\mathbf{R}^{m \\times n}\n$$\n\nUnderstanding this matrix as a bunch of vectors, we can calculate the similarity between these vectors using the **cosine similarity** (see [here](https://www.machinelearningplus.com/nlp/cosine-similarity/#2whatiscosinesimilarityandwhyisitadvantageous) for a vivid interpretation), which is defined as \n$$\\text{cossim}(X, X) = X X^T =: Y_{cos} \\in \\mathbf{R}^{m \\times m}$$.\n\n#### **How can we interpret these values in our similarity matrix $Y_{cos}$?**\n\nTo answer that question we take a look at any arbitrary position $y_{ij} \\in Y_{cos}$ with $1 \\leq i,j \\leq m$. Analyzing the above calculation we see that we're comparing the importance of all terms $t_k$ for the document $d_i$ with the importance of the same term for document $d_j$ by multiplying both importance scores pairwise:\n\n$$\\begin{align} y_{ij} & = \\frac{x_i x_j^{T}}{\\lVert x_i \\rVert \\lVert x_j \\rVert} \\\\\n            & = \\frac{1}{\\lVert x_i \\rVert \\lVert x_j \\rVert}\\begin{pmatrix} \\text{tfidf}(t_1, d_i) & \\cdots & \\text{tfidf}(t_m, d_i) \\end{pmatrix} * \\begin{pmatrix} \\text{tfidf}(t_1, d_j) \\\\ \\vdots \\\\ \\text{tfidf}(t_m, d_j) \\end{pmatrix} \\\\\n            & = \\frac{1}{\\lVert x_i \\rVert \\lVert x_j \\rVert} \\sum^{m}_{k=1} \\text{tfidf}(t_k, d_i)*\\text{tfidf}(t_k, d_j)  \\quad \\text{with} \\quad x_i, x_j \\in X\n            \\end{align}$$\n\nGeneral speaking, we can say that **$y_{ij}$ compares the importance scores of two documents $d_{i}$ and $d_{j}$ ($1\\leq i,j \\leq m$) with respect to ALL given terms $t_k$** ($1\\leq k \\leq n$). If we now follow the hypthesis that similar documents are more likely to contain similar important terms, then we got our final interpretation model which can be sumarized as \n\n>**the higher a score $y_{ij}$ is, the more similar are the referring documents**.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nnetflix_df = df.copy()\n\n# init vectorizer object\ntfidf = TfidfVectorizer(stop_words=\"english\")\n\n# create tfidf matrix (X in above description)\nX = tfidf.fit_transform(netflix_df.description)\n\nprint(\"Shape of Tfidf matrix X:\\t\", X.shape)\nprint(\"Tfidf matrix X:\")\nprint(X.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our Tfidf matrx $X$ consists of $7777$ rows as expected which cooinces with the number of documents in our corpus (netflix DataFrame). We can also conclude, that these documents generate a word pool of 17893 words (excluding stop words), whose importance is measured (using $\\text{tfidf}(t, D)$)  on all documents separately.\n\nNext, we use the cosine similarity as mentioned above.","metadata":{}},{"cell_type":"code","source":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(X, X)\n\n# create index lookup on all netflix titles\nindices = pd.Series(netflix_df.index, index=netflix_df['title']).drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How will we get from our cosine matrix to our recommendation?\nThe idea is currently straight forward. To receive a similar recommendation for any given title wich is contained in our DataFrame we just have to look for the referring index in our DataFrame. Since we concluded that any entry $y_{ij}$ in our cosine matrix $Y_{cos}$ indicates the similarity score wich we derived from comparing two documents $d_i$ and $d_j$, it is now clear that for a given document $d_i$ we just have to look for _that_ document $\\bar{d}_j$ that generates the highest score $\\bar{y}_{ij}$ with fixed parameter $i$.\n\nThat's basically all we will do next.","metadata":{}},{"cell_type":"code","source":"def get_tfidf_recommendation(\n    title: str, indices: pd.Series, cosine_sim:np.ndarray) -> pd.DataFrame:\n    \"\"\"Generate similarity recommendations for a particular title based on tfidf.\n        \n    Args:\n        title (str): title to find recommendations for (must be included in netflix_df)\n        indices (pd.Series): Series maps netflix_df titles to corresponding indices\n        cosine_matrix (np.ndarray): cosine matrix derived from TfidfVectorizer appplied \n                                    on netflix_df.description\n    \"\"\"\n    # title index lookup \n    idx = indices[title]\n    \n    # create netflix DataFrame with similarity scores\n    scores = pd.DataFrame(\n        data={\n            \"score\":cosine_sim[idx],\n            \"title\":netflix_df[\"title\"],\n            \"description\":netflix_df[\"description\"],\n        }, \n        index=range(len(cosine_sim[idx]))\n    ).sort_values(by=\"score\", ascending=False)\n    \n    return scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's test it!","metadata":{}},{"cell_type":"code","source":"get_tfidf_recommendation(\"ZZ TOP: THAT LITTLE OL' BAND FROM TEXAS\", indices, cosine_sim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Second approach: using pre-trained word2vec model from Google","metadata":{}},{"cell_type":"markdown","source":"### First let's normalize the text fragments using *str.lower* and *nltk.tokenize*\nFirst using the method *str.lower* will modify strings to contain only lower-case characters.\nAfter that we split a lower-cased sentence in a list containing all the single words from our sentence by using *nltk.tokenize*.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\ndf_netflix = (\n    df.filter([\"show_id\", \"title\", \"listed_in\", \"description\"]).assign(\n        title_list=lambda x: x.title.str.lower().apply(word_tokenize),\n        listed_in=lambda x: x.listed_in.str.lower().apply(word_tokenize),\n        description=lambda x: x.description.str.lower().apply(word_tokenize),\n    )\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing stopwords and punctuations\nStopwords are common words used in every language which specify mostly nouns better. The provide us with a better understanding of the context but are not very significant words. Let's say you read the word \n\n> \"book\"\n\nThen you will know that text is probably about books or literatur. Now, a common *stopword* would be `\"his\"` and reading\n\n> \"his book\"\n\nwould let you know that your text is still about a book and someone who owns that book. But if you're mostly interested in classifying that sentence as literatur all your focus should stay on the word \"book\". Consequently, we can remove all these common stopwords from our list. Therefore we may want to use *nltk.corpus.stopwords*.\n\nWith a similar argumentation, we can remove punctuations as well, using *string.punctuation*.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom string import punctuation\nfrom typing import List\nstopword_list = stopwords.words(\"english\") + list(punctuation)\n\ndef remove_stopwords(list_input: List[str]) -> List[str]:\n    \"\"\"Filter stopwords from list.\"\"\"\n    return list(filter(lambda word: word not in stopword_list, list_input))\n\ndef remove_punctuation(input_list: List[str]) -> List[str]:\n    \"\"\"Remove punctionations from words in list.\n    \n    str.maketrans creates a dict mapping with characters to interchange, \n    whereby the third parameter the characters describes which should be removed, \n    e.g.\n        str.maketrans(\"abc\",\"123\",\".\") -> {97: 49, 98: 50, 99: 51, 46: None} \n        meaning, \"a\"->1, \"b\"->2, \"c\"->3 and remove \".\"\n    \"\"\"\n    return [word.translate(str.maketrans('', '', punctuation)) for word in input_list]\n\ndef remove_empty_char(input_list: List[str]) -> List[str]:\n    \"\"\"Remove empty strings (strings with len==0).\"\"\"\n    return list(filter(lambda x: len(x)>0, input_list))\n\n# remove stopwords, punctuations and empty strings\ndf_netflix = df_netflix.assign(\n    title_list=lambda df: df.filter([\"title_list\"]).applymap(lambda x: remove_stopwords(x)),\n    listed_in=lambda df: df.filter([\"listed_in\"]).applymap(lambda x: remove_stopwords(x)),\n    description=lambda df: df.filter([\"description\"]).applymap(lambda x: remove_stopwords(x)),\n    description_pt1=lambda df: df.filter([\"description\"]).applymap(lambda x: remove_punctuation(x)),\n    description_pt2=lambda df: df.filter([\"description_pt1\"]).applymap(lambda x: remove_empty_char(x)),\n).drop(columns=[\"description\", \"description_pt1\"]).rename(columns={\"description_pt2\": \"description\"})","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop duplicates using sets\nIt's common practice to remove duplicates using first set and casting them afterwards back to lists.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true}},{"cell_type":"code","source":"# remove duplicates\ndf_netflix = df_netflix.assign(\n    title_list=lambda df: df.title_list.apply(lambda x: list(set(x))),\n    listed_in=lambda df: df.listed_in.apply(lambda x: list(set(x))),\n    description=lambda df: df.description.apply(lambda x: list(set(x))),\n)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download pretrained word2vec model from Google\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n!gunzip GoogleNews-vectors-negative300.bin.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\n\nwv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix_netflix_vocab = []\nfor list_ in df_netflix.to_numpy():\n    list_[2] = [word for word in list_[2] if word in wv.vocab]\n    list_[3] = [word for word in list_[3] if word in wv.vocab]\n    list_[4] = [word for word in list_[4] if word in wv.vocab]\n    matrix_netflix_vocab.append(list_)\ndf_netflix_vocab = pd.DataFrame(matrix_netflix_vocab, columns=df_netflix.columns)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_netflix_vocab","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_netflix","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef recommendation(title):\n    matrix_netflix_title_vocab = []\n    for list_ in df_netflix[df_netflix['title'] == title].to_numpy():\n        list_[2] = [word for word in list_[2] if word in wv.vocab]\n        list_[3] = [word for word in list_[3] if word in wv.vocab]\n        list_[4] = [word for word in list_[4] if word in wv.vocab]\n        matrix_netflix_title_vocab.append(list_)\n\n    matrix_similarity = []\n    pbar = tqdm(matrix_netflix_vocab)\n    for list1 in pbar:\n        for list2 in matrix_netflix_title_vocab:\n            score_catg = wv.n_similarity(list1[2], list2[2])\n            score_desc = wv.n_similarity(list1[3], list2[3])\n            try:\n                score_title = wv.n_similarity(list1[4], list2[4])/2\n            except:\n                score_title = 0\n            if ((list1[1] != list2[1]) & (score_catg > 0.85)):\n                matrix_similarity.append([list1[1], list2[1], score_title, score_catg, score_desc])\n        pbar.update()\n    pbar.close()\n    df_netflix_similarity = pd.DataFrame(matrix_similarity, columns = ['recommendation','title','score_title', 'score_category', 'score_description'])\n    df_netflix_similarity['final_score'] = df_netflix_similarity['score_title'] + df_netflix_similarity['score_category'] + df_netflix_similarity['score_description']\n    return (df_netflix_similarity.sort_values(by=['final_score', 'score_category', 'score_description', 'score_title'], ascending=False).head(10))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommendation('Game of thrones')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommendation('Friends')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}