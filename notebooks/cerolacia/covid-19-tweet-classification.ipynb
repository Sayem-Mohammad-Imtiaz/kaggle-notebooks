{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 TWEET SENTIMENT CLASSIFICATION"},{"metadata":{},"cell_type":"markdown","source":"![](https://viterbischool.usc.edu/wp-content/uploads/2020/03/COVID_TWITTER1200X600.jpg)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Location'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=train['Sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(a, x=['Positive','Negative','Neutral','Extremely Positive','Extremely Negative'], y='Sentiment',color=['Positive','Negative','Neutral','Extremely Positive','Extremely Negative'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=train['Location'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(t,y='Location', color=t.index)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time=train['TweetAt'].to_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train['Sentiment'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sentiment']=train['Sentiment'].map({'Positive':0,'Negative':1,'Neutral':2,'Extremely Positive':3,\n                                           'Extremely Negative':4})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['OriginalTweet']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['OriginalTweet'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\n\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleaner(tweet):\n    \n    # remove urls\n    tweet = re.sub(r'http\\S+', ' ', tweet)\n    \n    # remove html tags\n    tweet = re.sub(r'<.*?>',' ', tweet)\n    \n    # remove digits\n    tweet = re.sub(r'\\d+',' ', tweet)\n    \n    # remove hashtags\n    tweet = re.sub(r'#\\w+',' ', tweet)\n    \n    # remove mentions\n    tweet = re.sub(r'@\\w+',' ', tweet)\n    \n    #removing stop words\n    tweet = tweet.split()\n    tweet = \" \".join([word for word in tweet if not word in stop_words])\n    \n    return  tweet\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstop_words = stopwords.words('english')\ntrain['OriginalTweet']=train['OriginalTweet'].apply(lambda x:x.lower())\ntrain['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain_cleaned = train['OriginalTweet'].apply(cleaner)\ntrain_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['OriginalTweet']=train_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = ' '.join([text for text in train_cleaned])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://c.tenor.com/RMYmjb3OW44AAAAj/emotional-emotion.gif)"},{"metadata":{},"cell_type":"markdown","source":"Positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"Positive =' '.join([text for text in train['OriginalTweet'][train['Sentiment'] == 0]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Positive)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative"},{"metadata":{"trusted":true},"cell_type":"code","source":"Negative =' '.join([text for text in train['OriginalTweet'][train['Sentiment'] == 1]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Negative)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neutral"},{"metadata":{"trusted":true},"cell_type":"code","source":"Neutral =' '.join([text for text in train['OriginalTweet'][train['Sentiment'] == 2]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Neutral)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extremely Positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"Extremely_Positive =' '.join([text for text in train['OriginalTweet'][train['Sentiment'] == 3]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Extremely_Positive)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extremely Negative"},{"metadata":{"trusted":true},"cell_type":"code","source":"Extremely_Negative =' '.join([text for text in train['OriginalTweet'][train['Sentiment'] == 4]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Extremely_Negative)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TweetAt'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as Layers\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport seaborn as sns\n\n\nimport numpy as np \nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizer class in tensorflow allow us to vectorize \n# a text corpus into sequence of integers\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_cleaned)\n\nX = tokenizer.texts_to_sequences(train_cleaned)\n\nvocab_size = len(tokenizer.word_index)+1\n\n\n\nprint(\"Vocabulary size: {}\".format(vocab_size))\nprint(\"\\nExample:\\n\")\nprint(\"Sentence:\\n{}\".format(train_cleaned[6]))\nprint(\"\\nAfter tokenizing :\\n{}\".format(X[6]))\n\nX = pad_sequences(X, padding='post')\nprint(\"\\nAfter padding :\\n{}\".format(X[6]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding = {'Extremely Negative': 0,\n            'Negative': 0,\n            'Neutral': 1,\n            'Positive':2,\n            'Extremely Positive': 2\n           }\n\nlabels = ['Negative', 'Neutral', 'Positive']\n           \ny.replace(encoding, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\n\n# hyper parameters\nEPOCHS = 2\nBATCH_SIZE = 32\nembedding_dim = 16\nunits = 256\n\nmodel = tf.keras.Sequential([\n    Layers.Embedding(vocab_size, embedding_dim, input_length=X.shape[1]),\n    Layers.Bidirectional(Layers.LSTM(units,return_sequences=True)),\n    Layers.GlobalMaxPool1D(),\n    Layers.Dropout(0.4),\n    Layers.Dense(64, activation=\"relu\"),\n    Layers.Dropout(0.4),\n    Layers.Dense(3)\n])\n\n\nmodel.compile(loss=SparseCategoricalCrossentropy(from_logits=True),\n              optimizer='adam',metrics=['accuracy']\n             )\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note:-Here I have used Bidirectional LSTM.\nBidirectional LSTMs are an extension to typical LSTMs that can enhance performance of the model on sequence classification problems. Where all time steps of the input sequence are available, Bi-LSTMs train two LSTMs instead of one LSTMs on the input sequence.\n![](https://www.i2tutorials.com/wp-content/media/2019/05/Deep-Dive-into-Bidirectional-LSTM-i2tutorials.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X, y, epochs=EPOCHS, validation_split=0.12, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(\n    history.history, y=['accuracy', 'val_accuracy'],\n    labels={'index': 'epoch', 'value': 'accuracy'}\n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(\n    history.history, y=['loss', 'val_loss'],\n    labels={'index': 'epoch', 'value': 'loss'}\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test['OriginalTweet'].copy()\ny_test = test['Sentiment'].copy()\n\nX_test = X_test.apply(cleaner)\n\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_test = pad_sequences(X_test, padding='post')\n\ny_test.replace(encoding, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc = model.evaluate(X_test,y_test,verbose=0)\nprint('Test loss: {}'.format(loss))\nprint('Test Accuracy: {}'.format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = confusion_matrix(y_test, pred)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in labels],\n    columns = [i for i in labels]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, pred, target_names=labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span class='h1'>Please Upvote if you liked the work</span>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile notebook_style.py\nfrom IPython.core.display import display,HTML\n\ndef notebook_style(span_h_font='Ewert',\n                   span_h_color='#ff3636',\n                   prompt_font='Ewert',\n                   prompt_in_color='#ff3636',\n                   prompt_out_color='#3636ff',\n                   warn_font='Roboto',\n                   warn_color='#ff36ff',\n                   out_font='Roboto',\n                   out_color='#3636ff',\n                   background_out_color='whitesmoke',\n                   span_text_shadow=True):\n    style_str=\"\"\"<style>\"\"\"+\\\n    \"\"\"@import 'https://fonts.googleapis.com/css?family=\"\"\"+\\\n    prompt_font+\"\"\"|\"\"\"+out_font+\"\"\"|\"\"\"+warn_font+\"\"\"'; \"\"\"\n    if span_text_shadow==True:\n        style_str+=\"\"\"span {color:black; \"\"\"+\\\n                   \"\"\"text-shadow:4px 4px 4px #aaa;}\"\"\"\n    style_str+=\"\"\"div.alert {text-shadow:4px 4px 4px #aaa;}\"\"\"\n    style_str+=\"\"\"span.h1,span.h2,span.h3,\"\"\"+\\\n               \"\"\"span.h4,span.h5,span.h6 \"\"\"+\\\n               \"\"\"{color:\"\"\"+span_h_color+\"\"\"; \"\"\"+\\\n               \"\"\"font-family:\"\"\"+span_h_font+\"\"\";} \"\"\"\n    style_str+=\"\"\"div.warn {background-color:\"\"\"+\\\n                            background_out_color+\"\"\"; color:\"\"\"+\\\n               warn_color+\"\"\"; font-size:110%; \"\"\"+\\\n               \"\"\"font-family:\"\"\"+warn_font+\"\"\";} \"\"\"\n    for el in [\"\"\"div.output_area pre\"\"\",\n               \"\"\"div.output_stderr pre\"\"\"\n               \"\"\"div.output_subarea\"\"\",\n               \"\"\"div.output_html\"\"\",\n               \"\"\"div.output_stderr\"\"\"]:\n        style_str+=el+\"\"\"{background-color:\"\"\"+\\\n                   background_out_color+\"\"\"; color:\"\"\"+\\\n                   out_color+\"\"\"; font-size:110%; \"\"\"+\\\n                   \"\"\"font-family:\"\"\"+out_font+\"\"\";} \"\"\"\n    style_str+=\"\"\"div.input_prompt {color:\"\"\"+\\\n               prompt_in_color+\"\"\"; \"\"\"+\\\n               \"\"\"font-family:\"\"\"+prompt_font+\"\"\";} \"\"\"\n    style_str+=\"\"\"div.output_prompt {color:\"\"\"+\\\n               prompt_out_color+\"\"\"; \"\"\"+\\\n               \"\"\"font-family:\"\"\"+prompt_font+\"\"\";} \"\"\"\n    style_str+=\"\"\".cm-s-ipython span.cm-comment {color:darkslategray;} \"\"\"\n    style_str+=\"\"\".cm-s-ipython span.cm-def {color:#3636ff;} \"\"\"\n    style_str+=\"\"\".cm-s-ipython span.cm-operator {color:#ff36ff;} \"\"\"\n    style_str+=\"\"\".cm-s-ipython span.cm-keyword {color:darkgreen;} \"\"\"\n    style_str+=\"\"\".cm-s-ipython span.cm-string {color:#ff3636;} \"\"\"\n    style_str+=\"\"\"</style>\"\"\"\n    display(HTML(style_str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%run notebook_style.py\n\n# default options\nnotebook_style(span_h_font='Ewert',\n               span_h_color='#ff3636',\n               prompt_font='Ewert',\n               prompt_in_color='#ff3636',\n               prompt_out_color='#3636ff',\n               warn_font='Roboto',\n               warn_color='#ff36ff',\n               out_font='Roboto',\n               out_color='#3636ff',\n               background_out_color='whitesmoke',\n               span_text_shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have learned this kind of styling from this notebook:\nhttps://www.kaggle.com/olgabelitskaya/styling-recipes\nThis notebook by **Olga Belitskaya** is truely awesome!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}