{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nimport matplotlib.pyplot as plt#visualization\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ntweets = pd.read_csv(\"../input/clinton-trump-tweets/tweets.csv\")\ntweets = tweets[[ 'handle', 'text', 'is_retweet', 'original_author', \n                 'time', 'lang', 'retweet_count', 'favorite_count']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's have a look as general view."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['lang'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data manipulation\n\n### Editing language names. As you see we have 3 main language but \"und\" is Undetermined then lets collect them as called \"Other\". And convert to date format and extract hour."},{"metadata":{"trusted":true},"cell_type":"code","source":"def language(df) :\n    if df[\"lang\"] == \"en\" :\n        return \"English\"\n    elif df[\"lang\"] == \"es\" :\n        return \"Spanish\"\n    else :\n        return \"Other\"\n\ntweets[\"lang\"] = tweets.apply(lambda tweets:language(tweets),axis = 1)\n\n\n# datetime convert\nfrom datetime import datetime\ndate_format = \"%Y-%m-%dT%H:%M:%S\" \ntweets[\"time\"]   = pd.to_datetime(tweets[\"time\"],format = date_format)\ntweets[\"hour\"]   = pd.DatetimeIndex(tweets[\"time\"]).hour\ntweets[\"month\"]  = pd.DatetimeIndex(tweets[\"time\"]).month\ntweets[\"day\"]    = pd.DatetimeIndex(tweets[\"time\"]).day\ntweets[\"month_f\"]  = tweets[\"month\"].map({1:\"JAN\",2:\"FEB\",3:\"MAR\",\n                                        4:\"APR\",5:\"MAY\",6:\"JUN\",\n                                        7:\"JUL\",8:\"AUG\",9:\"SEP\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets have a look"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['lang'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seperate 'handle' and 'text' values then make DataFrame using concate."},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.concat([tweets.handle, tweets.text], axis=1)       # data icinde handle ve text kisimini ayirip concate edip DF yaptik. cunku diger kisimlara ihtiyacimiz olmayacak","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Null values of rows are throwing up."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(axis=0, inplace=True)                  # bos gozlem yerlerinin satirlarini sildik","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check what we have in data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.handle.value_counts()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overall Tweets and Retweets visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total number of tweets by both of the twitter handles\nsns.countplot(x='handle', data = tweets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Analysis of the number of tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of tweets by the months\nmonthly_tweets = tweets.groupby(['month', 'handle']).size().unstack()\nmonthly_tweets.plot(title='Monthly Tweet Counts', colormap='copper')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trump tweets without retweets\ntweets_trump   = (tweets[(tweets[\"handle\"] == \"realDonaldTrump\") &\n                         (tweets[\"is_retweet\"] == False)].reset_index()\n                  .drop(columns = [\"index\"],axis = 1))\n\n#trump tweets with retweets\ntweets_trump_retweets   = (tweets[(tweets[\"handle\"] == \"realDonaldTrump\") &\n                                  (tweets[\"is_retweet\"] == True)].reset_index()\n                                  .drop(columns = [\"index\"],axis = 1))\n\n#hillary tweets without retweets\ntweets_hillary  = (tweets[(tweets[\"handle\"] == \"HillaryClinton\") &\n                            (tweets[\"is_retweet\"] == False)].reset_index()\n                              .drop(columns = [\"index\"],axis = 1))\n\n#hillary tweets with retweets\ntweets_hillary_retweets  = (tweets[(tweets[\"handle\"] == \"HillaryClinton\") &\n                            (tweets[\"is_retweet\"] == True)].reset_index()\n                              .drop(columns = [\"index\"],axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\n\nplt.figure(figsize = (13,6))\nplt.subplot(121)\ntweets[tweets[\"handle\"] ==\n       \"realDonaldTrump\"][\"is_retweet\"].value_counts().plot.pie(autopct = \"%1.0f%%\",\n                                                                wedgeprops = {\"linewidth\" : 1,\n                                                                              \"edgecolor\" : \"k\"},\n                                                                shadow = True,fontsize = 13,\n                                                                explode = [.1,0.09],\n                                                                startangle = 20,\n                                                                colors = [\"#ff0026\",\"#fbff00\"]\n                                                               )\nplt.ylabel(\"\")\nplt.title(\"Percentage of Trump retweets\")\n\n\nplt.subplot(122)\ntweets[tweets[\"handle\"] ==\n       \"HillaryClinton\"][\"is_retweet\"].value_counts().plot.pie(autopct = \"%1.0f%%\",\n                                                                wedgeprops = {\"linewidth\" : 1,\n                                                                              \"edgecolor\" : \"k\"},\n                                                                shadow = True,fontsize = 13,\n                                                                explode = [.09,0],\n                                                                startangle = 60,\n                                                                colors = [\"#0095ff\",\"#fbff00\"]\n                                                               )\nplt.ylabel(\"\")\nplt.title(\"Percentage of Hillary retweets\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Regular expression for delete or cleaning of some expression. We are cleaning just for 5.row. If it is cleaned then we will use all data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# regular expression yapalim. Burada gulucuk ve benzeri ifadeleri silmek icin\n\nimport re\n\nfirst_text = data.text[5]                   # butun datadan once 5.siradaki datanin temizlenmesini yapalim bakalim temizlenmis ise butun dataya uygulamaya calisacagiz\ntext = re.sub(\"[^a-zA-Z]\",\" \", first_text)\ntext = text.lower()                        # ilk ve son haline gore description icindeki elemanlari kucultmesini istedik ve yazdirirsak kuculdugunu gorebiliriz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, lets download nltk for using 'stopwords' and 'punkt'. These are extracting some words like 'an', 'the',..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk         # nltk nin download kiti bulunmaktadir. Bisey ifade etmeyen kelimeleri(Stop words ler ornegin 'an' veya 'the') cikarmamiz gerekiyor. Eger cikarmazsak 'an' veya 'the' kelimeleri enfazla kullanilanlar olarak gorunmus olacak bunlar bizim icin bisey ifade etmiyor\nnltk.download(\"stopwords\")          # hazir olarak bulunan stopwordslari indirip uygulayabiliriz\nnltk.download(\"punkt\")\n\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting words with tokenize modul from nltk."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = nltk.word_tokenize(text)       # tokenize ettik yani kelimeleri birbirinden ayirdik. ayirmak icin split kullandik cunku mesela \"doesn't\" vy \"shouldn't\" kelimesini \"does\" ve \"not\" olarak ayiramiyor bunun icin tokenize kullandik\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning all useless words with stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = [ word for word in text if not word in set(stopwords.words(\"english\"))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check..."},{"metadata":{"trusted":true},"cell_type":"code","source":"text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pure version of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"# kelimeleri sade haline indirgemek icin\n\nimport nltk as nlp\nnltk.download('wordnet')\n\nlemma = nlp.WordNetLemmatizer()\ntext = [lemma.lemmatize(word) for word in text]\n\ntext = \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We cleaned 5.row in data. Now, we are doing for all data and add a list."},{"metadata":{"trusted":true},"cell_type":"code","source":"# simdi BUTUN VERI dekilere bunu uygulamak icin for dongusu olusturalim:\n\ntext_list = []\nfor text in data.text:\n    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n    text= text.lower()\n    text= nltk.word_tokenize(text)\n    text = [ word for word in text if not word in set(stopwords.words(\"english\"))]\n    lemma = nlp.WordNetLemmatizer()\n    text = [lemma.lemmatize(word) for word in text]\n    text = \" \".join(text)\n    text_list.append(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's see every words in the list."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If you want to how much words to determine in the words list, we are using CountVectorizer(for bag of words)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# bag of words: kac kelime kullanmak istiyorsak kendimiz belirliyoruz. Duygu kelimelerini verip analizini yapacagiz\n\nfrom sklearn.feature_extraction.text import CountVectorizer        # bag of words olusturmak icin \n\nmax_features = 5000               # max ... kadar kelimeye baksin\n\n# Simdi modelimizi olusturalim\ncount_vectorizer = CountVectorizer(max_features=max_features, stop_words=\"english\")\nsparce_matrix = count_vectorizer.fit_transform(text_list).toarray()                     # modelimizi fitleyip array donusturduk\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most used words determining number of tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{max_features} most used words:\\n\\n{count_vectorizer.get_feature_names()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tweets month by month"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nsns.countplot(x = \"month_f\",hue = \"handle\",palette = [\"#ff0026\",\"#0095ff\"],\n              data = tweets.sort_values(by = \"month\",ascending = True),\n             linewidth = 1,edgecolor = \"k\"*tweets_trump[\"month\"].nunique())\nplt.grid(True)\nplt.title(\"Tweets month by month in 2016 election\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorize the text column into Positive and Negative sentiments using TextBlob\n\n### Sentiment Analysis on the entiere dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\nbloblist_desc = list()                                  # butun tweet ler listeleniyor\n\ndf_tweet_descr_str=tweets['text'].astype(str)           # text ler ayiklaniyor\n\nfor row in df_tweet_descr_str:\n    blob = TextBlob(row)\n    bloblist_desc.append((row,blob.sentiment.polarity, blob.sentiment.subjectivity))\n    df_tweet_polarity_desc = pd.DataFrame(bloblist_desc, columns = ['sentence','sentiment','polarity'])\n \ndef f(df_tweet_polarity_desc):\n    if df_tweet_polarity_desc['sentiment'] > 0:\n        val = \"Positive\"\n    elif df_tweet_polarity_desc['sentiment'] == 0:\n        val = \"Neutral\"\n    else:\n        val = \"Negative\"\n    return val\n\ndf_tweet_polarity_desc['Sentiment_Type'] = df_tweet_polarity_desc.apply(f, axis=1)\n\nplt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.countplot(x=\"Sentiment_Type\", data=df_tweet_polarity_desc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n* If this tutorial is not enough you can check NLP for Beginners prepared by \n    - https://www.kaggle.com/mogady/kickstarter-s-nlp-anlaysis\n* After this tutorial, my aim is to prepare 'kernel' which is connected to Recommendation System 'The Movies Dataset' data.\n* If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n* Thank you for your suggestion and votes ;)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}