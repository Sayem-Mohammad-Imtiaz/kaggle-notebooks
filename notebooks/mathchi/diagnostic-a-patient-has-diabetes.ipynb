{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Diabets \n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n\n\n### Content\n\nSeveral constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n- Pregnancies: Number of times pregnant \n- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n- BloodPressure: Diastolic blood pressure (mm Hg) \n- SkinThickness: Triceps skin fold thickness (mm) \n- Insulin: 2-Hour serum insulin (mu U/ml) \n- BMI: Body mass index (weight in kg/(height in m)^2) \n- DiabetesPedigreeFunction: Diabetes pedigree function \n- Age: Age (years) \n- Outcome: Class variable (0 or 1)\n\n\n#### Relevant Information:\n      Several constraints were placed on the selection of these instances from\n      a larger database.  In particular, all patients here are females at\n      least 21 years old of Pima Indian heritage.  ADAP is an adaptive learning\n      routine that generates and executes digital analogs of perceptron-like\n      devices.  It is a unique algorithm; see the paper for details.\n\n#### Number of Instances: 768\n\n#### Number of Attributes: 8 plus class \n\n#### For Each Attribute: (all numeric-valued)\n   1. Number of times pregnant\n   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n   3. Diastolic blood pressure (mm Hg)\n   4. Triceps skin fold thickness (mm)\n   5. 2-Hour serum insulin (mu U/ml)\n   6. Body mass index (weight in kg/(height in m)^2)\n   7. Diabetes pedigree function\n   8. Age (years)\n   9. Class variable (0 or 1)\n\n#### Missing Attribute Values: Yes\n\n#### Class Distribution: (class value 1 is interpreted as \"tested positive for\n   diabetes\")\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Installing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);  # to display all columns and rows\npd.set_option('display.float_format', lambda x: '%.2f' % x) # The number of numbers that will be shown after the comma.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. EDA (Exploratory of Data Analysis)\n### 2.1. Data Preperation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the dataset\ndf = pd.read_csv(\"../input/diabetes-data-set/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe([0.10,0.25,0.50,0.75,0.90,0.95,0.99]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Outcome\"].value_counts()*100/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Outcome.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Age\"].hist(edgecolor = \"black\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Missing Value Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0, np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.bar(df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def carp(x,y):\n    \n    z = x*y\n    \n    return z\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carp(4,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The missing values will be filled with the median values of each variable.\n\ndef median_target(var):   \n    \n    temp = df[df[var].notnull()]\n    \n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    \n    return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.columns\n\ncolumns = columns.drop(\"Outcome\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_target('Glucose')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The values to be given for incomplete observations are given the median value of people who are not sick and the median values of people who are sick.\n\ncolumns = df.columns\n\ncolumns = columns.drop(\"Outcome\")\n\nfor col in columns:\n    \n    df.loc[(df['Outcome'] == 0 ) & (df[col].isnull()), col] = median_target(col)[col][0]\n    df.loc[(df['Outcome'] == 1 ) & (df[col].isnull()), col] = median_target(col)[col][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df['Outcome'] == 0 ) & (df[\"Pregnancies\"].isnull()), \"Pregnancies\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[(df['Outcome'] == 0 ) & (df[\"BloodPressure\"].isnull())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Outliers Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df[\"BloodPressure\"].quantile(0.25)\nQ3 = df[\"BloodPressure\"].quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[(df[\"BloodPressure\"] > upper)].any(axis=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in df:\n    print(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in df:\n    \n    Q1 = df[feature].quantile(0.05)\n    Q3 = df[feature].quantile(0.95)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5*IQR\n    upper = Q3 + 1.5*IQR\n    \n    if df[(df[feature] > upper)].any(axis=None):\n        print(feature,\"yes\")\n    else:\n        print(feature, \"no\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# According to BMI, some ranges were determined and categorical variables were assigned.\nNewBMI = pd.Series([\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\n\ndf[\"NewBMI\"] = NewBMI\n\ndf.loc[df[\"BMI\"] < 18.5, \"NewBMI\"] = NewBMI[0]\n\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"BMI\"] <= 24.9), \"NewBMI\"] = NewBMI[1]\ndf.loc[(df[\"BMI\"] > 24.9) & (df[\"BMI\"] <= 29.9), \"NewBMI\"] = NewBMI[2]\ndf.loc[(df[\"BMI\"] > 29.9) & (df[\"BMI\"] <= 34.9), \"NewBMI\"] = NewBMI[3]\ndf.loc[(df[\"BMI\"] > 34.9) & (df[\"BMI\"] <= 39.9), \"NewBMI\"] = NewBMI[4]\ndf.loc[df[\"BMI\"] > 39.9 ,\"NewBMI\"] = NewBMI[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_insulin(row):\n    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"NewInsulinScore\"] = df.apply(set_insulin, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.drop(\"NewInsulinScore\", inplace = True, axis = 1)\n#df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some intervals were determined according to the glucose variable and these were assigned categorical variables.\nNewGlucose = pd.Series([\"Low\", \"Normal\", \"Overweight\", \"Secret\", \"High\"], dtype = \"category\")\n\ndf[\"NewGlucose\"] = NewGlucose\n\ndf.loc[df[\"Glucose\"] <= 70, \"NewGlucose\"] = NewGlucose[0]\n\ndf.loc[(df[\"Glucose\"] > 70) & (df[\"Glucose\"] <= 99), \"NewGlucose\"] = NewGlucose[1]\n\ndf.loc[(df[\"Glucose\"] > 99) & (df[\"Glucose\"] <= 126), \"NewGlucose\"] = NewGlucose[2]\n\ndf.loc[df[\"Glucose\"] > 126 ,\"NewGlucose\"] = NewGlucose[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. One-hot Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns =[\"NewBMI\",\"NewInsulinScore\", \"NewGlucose\"], drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Feature Standartization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"Outcome\"]\nX = df.drop([\"Outcome\",'NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret'], axis = 1)\ncols = X.columns\nindex = X.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\ntransformer = RobustScaler().fit(X)\nX = transformer.transform(X)\nX = pd.DataFrame(X, columns = cols, index = index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([X, categorical_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression(random_state = 12345)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier(random_state = 12345)))\nmodels.append(('RF', RandomForestClassifier(random_state = 12345)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 12345)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 12345)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345)))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 12345)\n        \n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Model Optimizasyonu (Model Tunning)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 9.1. Random Forests Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_params = {\"n_estimators\" :[100,200,500,1000], \n             \"max_features\": [3,5,7], \n             \"min_samples_split\": [2,5,10,30],\n            \"max_depth\": [3,5,8,None]}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(random_state = 12345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_cv = GridSearchCV(rf_model, \n                    rf_params,\n                    cv = 10,\n                    n_jobs = -1,\n                    verbose = 2).fit(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.1.1. RF Final Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned = RandomForestClassifier(**gs_cv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned = rf_tuned.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(rf_tuned, X, y, cv = 10).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(rf_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.2. LightGBM Model Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(random_state = 12345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {\"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.5],\n              \"n_estimators\": [500, 1000, 1500],\n              \"max_depth\":[3,5,8]}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_cv = GridSearchCV(lgbm, \n                     lgbm_params, \n                     cv = 10, \n                     n_jobs = -1, \n                     verbose = 2).fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.2.1 LightGBM Final Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(lgbm_tuned, X, y, cv = 10).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.3. XGBoost Model Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = GradientBoostingClassifier(random_state = 12345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {\n    \"learning_rate\": [0.01, 0.1, 0.2, 1],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 10),\n    \"max_depth\":[3,5,8],\n    \"subsample\":[0.5, 0.9, 1.0],\n    \"n_estimators\": [100,1000]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.3.1. XGBoost Final Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_tuned = GradientBoostingClassifier(**xgb_cv_model.best_params_).fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(xgb_tuned, X, y, cv = 10).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(xgb_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"## 10. Comparison of Final Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\n\nmodels.append(('RF', RandomForestClassifier(random_state = 12345, max_depth = 8, max_features = 7, min_samples_split = 2, n_estimators = 500)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 12345, learning_rate = 0.1, max_depth = 5, min_samples_split = 0.1, n_estimators = 100, subsample = 1.0)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345, learning_rate = 0.01,  max_depth = 3, n_estimators = 1000)))\n\n# evaluate each model in turn\nresults = []\nnames = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 12345)\n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"## 11. Conclusion\n\nThe aim of this study was to create classification models for the diabetes data set and to predict whether a person is sick by establishing models and to obtain maximum validation scores in the established models. The work done is as follows:\n\n1) Diabetes Data Set read.\n\n2) With Exploratory Data Analysis; The data set's structural data were checked.\nThe types of variables in the dataset were examined. Size information of the dataset was accessed. The 0 values in the data set are missing values. Primarily these 0 values were replaced with NaN values. Descriptive statistics of the data set were examined.\n\n3) Data Preprocessing section;\ndf for: The NaN values missing observations were filled with the median values of whether each variable was sick or not. The outliers were determined by LOF and dropped. The X variables were standardized with the rubost method..\n\n4) During Model Building;\nLogistic Regression, KNN, SVM, CART, Random Forests, XGBoost, LightGBM like using machine learning models Cross Validation Score were calculated. Later Random Forests, XGBoost, LightGBM hyperparameter optimizations optimized to increase Cross Validation value.\n\n5) Result;\nThe model created as a result of LightBM hyperparameter optimization became the model with the highest Cross Validation Score value. (0.89)\n\n\n\nNote:\n\n* After this notebook, my aim is to prepare 'kernel' which is 'not clear' data set.\n\n* If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n\n* Thank you for your suggestion and votes ;)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}