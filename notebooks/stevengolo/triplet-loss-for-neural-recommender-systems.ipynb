{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Triplet Loss for Implicit Feedback Neural Recommender Systems\n\nThis notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found [here](https://github.com/m2dsupsdlclass/lectures-labs). \n\n**Goals**\n\n* Demonstrate how it is possible to build a bi-linear recommender system only using positive feedback data.\n* Train deeper architectures following the same design principles.\n\nThis notebook is inspired by Maciej Kula's [Recommendations in Keras using triplet loss](https://github.com/maciejkula/triplet_recommendations_keras). Contrary to the Maciej Kula's work, we won't use the [Bayesian Personalized Ranking](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf) loss but instead will introduce the more common margin-based comparator.\n\n**Dataset used**\n\n* Anime Recommendations Database from Kaggle [link](https://www.kaggle.com/CooperUnion/anime-recommendations-database)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load libraries\nimport umap\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport tensorflow as tf\n\nfrom collections import deque\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import (Concatenate, Dense, Dot, Dropout,\n                                     Embedding, Flatten, Input, Lambda)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and preprocess the data\n\n### Ratings file\n\nAfter loading the data, each line of the dataframe contains:\n * user_id - non identifiable randomly generated user id.\n * anime_id - the anime that this user has rated.\n * rating - rating out of $10$ this user has assigned ($-1$ if the user watched it but did not assign a rating)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load and preprocess rating files\ndf_raw = pd.read_csv('../input/anime-recommendations-database/rating.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of the ratings data: {df_raw.shape}.\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Anime metadata file\n\nThe anime metadata file contains the following metadata: \n * anime_id - myanimelist.net's unique id identifying an anime.\n * name - full name of the anime.\n * genre - comma separated list of genres for this anime.\n * type - movie, TV, OVA, etc.\n * episodes - how many episodes in this show ($1$ if it's a movie).\n * rating - average rating out of $10$ for this anime.\n * members - number of community members that are in this anime's group."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load metadata file\nmetadata = pd.read_csv('../input/anime-recommendations-database/anime.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of the metadata: {metadata.shape}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merge ratings and metadata\n\nLet's enrich the raw ratings with the collected items metadata by merging the two dataframes on `anime_id`."},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings = df_raw.merge(metadata.loc[:, ['name', 'anime_id', 'type', 'episodes']], left_on='anime_id', right_on='anime_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of the complete data: {ratings.shape}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing\n\nTo understand well the distribution of the data, the following statistics are computed:\n* the number of users\n* the number of items\n* the rating distribution\n* the popularity of each anime"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of unique users: {ratings['user_id'].unique().size}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of unique animes: {ratings['anime_id'].unique().size}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of the ratings\nx, height = np.unique(ratings['rating'], return_counts=True)\n\nfig, ax = plt.subplots()\nax.bar(x, height, align='center')\nax.set(xticks=np.arange(-1, 11), xlim=[-1.5, 10.5])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's compute the popularity of each anime, defined as the number of ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of ratings for each movie\npopularity = ratings.groupby('anime_id').size().reset_index(name='popularity')\nmetadata = metadata.merge(popularity, left_on='anime_id', right_on='anime_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Speed-up the computation\n\nIn order to speed up the computation, we will subset the dataset using three criteria:\n* Remove the $-1$ ratings (people who watch the anime but without giving a rate).\n* Get only TV shows (because I like TV show).\n* Get the most popular ones (more than $5000$ ratings)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get most popular anime id and TV shows\nmetadata_5000 = metadata.loc[(metadata['popularity'] > 5000) & (metadata['type'] == 'TV')]\n# Remove -1 ratings and user id less than 10000\nratings = ratings[(ratings['rating'] > -1) & (ratings['user_id'] < 10000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean id\n\nAdd a new column to metadata_5000 in order to clean up id of the anime."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe for anime_id\nmetadata_5000 = metadata_5000.assign(new_anime_id=pd.Series(np.arange(metadata_5000.shape[0])).values)\nmetadata_5000_indexed = metadata_5000.set_index('new_anime_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the dataframe\nratings = ratings.merge(metadata_5000.loc[:, ['anime_id', 'new_anime_id', 'popularity']], left_on='anime_id', right_on='anime_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe for user_id\nuser = pd.DataFrame({'user_id': np.unique(ratings['user_id'])})\nuser = user.assign(new_user_id=pd.Series(np.arange(user.shape[0])).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the dataframe\nratings = ratings.merge(user, left_on='user_id', right_on='user_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of the rating dataset: {ratings.shape}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_USER_ID = ratings['new_user_id'].max()\nMAX_ITEM_ID = ratings['new_anime_id'].max()\n\nN_USERS = MAX_USER_ID + 1\nN_ITEMS = MAX_ITEM_ID + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of users: {N_USERS} / Number of animes: {N_ITEMS}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Later in the analysis, we will assume that this popularity does not come from the ratings themselves but from an external metadata, *e.g.* box office numbers in the month after the release in movie theaters.\n\n### Split the dataset into train/test sets\n\nLet's split the enriched data in a train/test split to make it possible to do predictive modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n\nuser_id_train = np.array(train['new_user_id'])\nanime_id_train = np.array(train['new_anime_id'])\nratings_train = np.array(train['rating'])\n\nuser_id_test = np.array(test['new_user_id'])\nanime_id_test = np.array(test['new_anime_id'])\nratings_test = np.array(test['rating'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implicit feedback data\n\nConsider the ratings greater or equal than 8 as positive feedback and ignore the rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pos = train.query('rating > 7')\ntest_pos = test.query('rating > 7')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because the median rating is around $8$, this cut will remove approximately half of the ratings from the datasets."},{"metadata":{},"cell_type":"markdown","source":"## The Triplet Loss\n\nThe following section demonstrates how to build a low-rank quadratic interaction model between users and anime. The similarity score between a user and an anime is defined by the unormlized dot products of their respective embeddings. The matching scores can be used to rank items to recommend to a specific user. \n\nTraining of the model parameters is achieved by randomly sampling negative animes not seen by a pre-selected anchor user. We want the model embedding matrices to be such that the similarity between the user vector and the negative vector is smaller than the similarity between the user vector and the positive item vector. Furthermore, we use a margin to turther move appart the negative from the anchor user.\n\nHere is the architecture of such a triplet architecture. The triplet name comes from the fact that the loss to optimize is defined for the triple `(anchor_user, positive_item, negative_item)`:\n![](https://raw.githubusercontent.com/m2dsupsdlclass/lectures-labs/3cb7df3a75b144b4cb812bc6eacec8e27daa5214/labs/03_neural_recsys/images/rec_archi_implicit_2.svg)\n\nWe call this model a triplet model with bi-linear interactions because the similarity between a user and an anime is captured by a dot product of the first level embedding vectors. This is therefore not a deep architecture."},{"metadata":{"trusted":true},"cell_type":"code","source":"def identity_loss(y_true, y_pred):\n    \"\"\"Ignore y_true and return the mean of y_pred.\n    This is a hack to work-around the design of the Keras API that is\n    not really suited to train networks with a triplet loss by default.\n    \"\"\"\n    return tf.reduce_mean(y_pred)\n\n\nclass MarginLoss(layers.Layer):\n    \"\"\"Define the loss for the triple architecture\n    \n    Parameters\n    ----------\n    margin: float, default=1.\n        Define a margin (alpha)\n    \"\"\"\n    def __init__(self, margin=1.):\n        super().__init__()\n        self.margin = margin\n        \n    def call(self, inputs):\n        pos_pair_similarity = inputs[0]\n        neg_pair_similarity = inputs[1]\n        \n        diff = neg_pair_similarity - pos_pair_similarity\n        return tf.maximum(diff + self.margin, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the actual code that builds the model(s) with shared weights. Note that here we use the cosine similarity instead of unormalized dot products (both seems to yield comparable results).\nThe triplet model is used to train the weights of the companion similarity model. The triplet model takes one user, one positive anime (relative to the selected user) and one negative item and is trained with comparator loss. The similarity model takes one user and one anime as input and return compatibility score (*aka* the match score)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TripletModel(Model):\n    \"\"\"Define the triplet model architecture\n    \n    Parameters\n    ----------\n    embedding_size: integer\n        Size the embedding vector\n    n_users: integer\n        Number of user in the dataset\n    n_items: integer\n        Number of item in the dataset\n    l2_reg: float or None\n        Quantity of regularization\n    margin: float\n        Margin for the loss\n        \n    Arguments\n    ---------\n    margin: float\n        Margin for the loss\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    flatten: Flatten\n        Flatten layer\n    dot: Dot\n        Dot layer\n    margin_loss: MarginLoss\n        Loss layer\n    \"\"\"\n    def __init__(self, n_users, n_items, embedding_size=64, l2_reg=None, margin=1.):\n        super().__init__(name='TripletModel')\n        \n        # Define hyperparameters\n        self.margin = margin\n        l2_reg = None if l2_reg == 0 else l2(l2_reg)\n        \n        # Define Embedding layers\n        self.user_embedding = Embedding(output_dim=embedding_size,\n                                        input_dim=n_users,\n                                        input_length=1,\n                                        input_shape=(1,),\n                                        name='user_embedding',\n                                        embeddings_regularizer=l2_reg)\n        # The following embedding parameters will be shared to encode\n        # both the positive and negative items.\n        self.item_embedding = Embedding(output_dim=embedding_size,\n                                        input_dim=n_items,\n                                        input_length=1,\n                                        name='item_embedding',\n                                        embeddings_regularizer=l2_reg)\n        \n        # The two following layers are without parameters, and can\n        # therefore be used for oth potisitve and negative items.\n        self.flatten = Flatten()\n        self.dot = Dot(axes=1, normalize=True)\n        \n        # Define the loss\n        self.margin_loss = MarginLoss(margin)\n        \n    def call(self, inputs, training=False, y=None, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with three elements\n            First element corresponds to the users\n            Second element corresponds to the positive items\n            Third element correponds to the negative items\n        \"\"\"\n        user_input = inputs[0]\n        item_pos_input = inputs[1]\n        item_neg_input = inputs[2]\n        \n        # Create embeddings\n        user_embedding = self.user_embedding(user_input)\n        user_embedding = self.flatten(user_embedding)\n        \n        item_pos_embedding = self.item_embedding(item_pos_input)\n        item_pos_embedding = self.flatten(item_pos_embedding)\n\n        item_neg_embedding = self.item_embedding(item_neg_input)\n        item_neg_embedding = self.flatten(item_neg_embedding)\n        \n        # Similarity computation betweeitem_neg_embeddings\n        pos_similarity = self.dot([user_embedding, item_pos_embedding])\n        neg_similarity = self.dot([user_embedding, item_neg_embedding])\n\n        return self.margin_loss([pos_similarity, neg_similarity])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nEMBEDDING_SIZE = 64\nL2_REG = 1e-6\n\n# Define a triplet model\ntriplet_model = TripletModel(N_USERS, N_ITEMS, EMBEDDING_SIZE, L2_REG)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MatchModel(Model):\n    \"\"\"Define the triplet model architecture\n    \n    Parameters\n    ----------\n    user_layer: Embedding\n        User layer from TripletModel\n    item_layer: Embedding\n        Item layer from TripletModel\n        \n    Arguments\n    ---------\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    flatten: Flatten\n        Flatten layer\n    dot: Dot\n        Dot layer\n    \"\"\"\n    def __init__(self, user_layer, item_layer):\n        super().__init__(name='MathcModel')\n\n        # Reuse the layer from the triplet model\n        self.user_embedding = user_layer\n        self.item_embedding = item_layer\n        \n        self.flatten = Flatten()\n        self.dot = Dot(axes=1, normalize=True)\n        \n    def call(self, inputs, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with three elements\n            First element corresponds to the users\n            Second element corresponds to the positive items\n        \"\"\"\n        user_input = inputs[0]\n        item_pos_input = inputs[1]\n        \n        # Create embeddings\n        user_embedding = self.user_embedding(user_input)\n        user_embedding = self.flatten(user_embedding)\n        \n        item_pos_embedding = self.item_embedding(item_pos_input)\n        item_pos_embedding = self.flatten(item_pos_embedding)\n                \n        # Similarity computation between embeddings\n        pos_similarity = self.dot([user_embedding, item_pos_embedding])\n        \n        return pos_similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a match model\nmatch_model = MatchModel(triplet_model.user_embedding, triplet_model.item_embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note the `triplet_model` and `match_model` have as much parameters, they share both user and anime embeddings. Their only difference is that the latter doesn't compute the negative similarity.\n\n### Quality of ranked recommendations\n\nNow that we have a randomly initialized model, we can start computing random recommendations. To assess their quality, we do the following for each user:\n* compute matching scores for animes (except the ones that the user has already seen in the training set);\n* compare to the positive feedback actually collected on the test set using the ROC AUC ranking metric;\n* average ROC AUC scores accross users to get the average performance of the recommender model on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_roc_auc(model, data_train, data_test):\n    \"\"\"Compute the ROC AUC for each user and average over users.\n    \n    Parameters\n    ----------\n    model: MatchModel\n        A MatchModel to train\n    data_train: numpy array\n        Train set\n    data_test: numpy array\n        Test set\n        \n    Return\n    ------\n    Average ROC AUC scores across users\n    \"\"\"\n    max_user_id = max(data_train['new_user_id'].max(),\n                      data_test['new_user_id'].max())\n    max_anime_id = max(data_train['new_anime_id'].max(),\n                       data_test['new_anime_id'].max())\n    \n    user_auc_scores = []\n    for user_id in range(1, max_user_id + 1):\n        pos_item_train = data_train[data_train['new_user_id'] == user_id]\n        pos_item_test = data_test[data_test['new_user_id'] == user_id]\n        \n        # Consider all the items already seen in the training set\n        all_item_idx = np.arange(1, max_anime_id + 1)\n        items_to_rank = np.setdiff1d(all_item_idx,\n                                     pos_item_train['new_anime_id'].values)\n        \n        # Ground truth: return 1 for each item positively present in\n        # the test set and 0 otherwise\n        expected = np.in1d(items_to_rank,\n                           pos_item_test['new_anime_id'].values)\n        \n        # At least one positive test value to rank\n        if np.sum(expected) >= 1:\n            repeated_user_id = np.empty_like(items_to_rank)\n            repeated_user_id.fill(user_id)\n            \n            # Make prediction\n            predicted = model.predict([repeated_user_id, items_to_rank], batch_size=4096)\n            \n            # Compute AUC scores\n            user_auc_scores.append(roc_auc_score(expected, predicted))\n        \n    return sum(user_auc_scores) / len(user_auc_scores)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By default, the model should make predictions to rank the items in random order. The **ROC AUC score** is a ranking score that represents the **expected value of correctly ordering uniformly sampled pairs of recommendations**. A random (untrained) model should yield $0.50$ ROC AUC on average."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\nprint(f'Average ROC AUC on the untrained model: {average_roc_auc(match_model, train_pos, test_pos)}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Triplet Model\n\nLet's now fit the parameters of the model by sampling triplets: for each user, select a anime in the positive feedback set of that user and randomly sample another anime to serve as negative item. \n\nNote that this sampling scheme could be improved by removing items that are marked as postive in the data to remove some label noise. In practice, this does not seem to be a problem though."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_triplets(pos_data, max_item_id, random_seed=42):\n    \"\"\"Sample negative items ar random\n    \n    Parameters\n    ----------\n    pos_data: pd.DataFrame\n        Dataframe of positive items\n    max_item_id: integer\n        Number of items in the complete dataframe\n    random_seed: integer, default=42\n        Random number generation\n    \n    Return\n    ------\n    A list with entries user_ids, pos_items_ids and neg_items_ids\n    \"\"\"\n    rng = np.random.RandomState(random_seed)\n    \n    user_ids = pos_data['new_user_id'].values.astype('int64')\n    pos_item_ids = pos_data['new_anime_id'].values.astype('int64')\n    neg_item_ids = rng.randint(low=1, \n                               high=max_item_id + 1, \n                               size=len(user_ids), dtype='int64')\n    return [user_ids, pos_item_ids, neg_item_ids]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nN_EPOCHS = 10\nBATCH_SIZE = 64\n\n# We plug the identity loss and a fake target variable ignored by \n# the model to be able to use the Keras API to train the model.\nfake_y = np.ones_like(train_pos[\"new_user_id\"], dtype='int64')\n    \ntriplet_model.compile(loss=identity_loss, optimizer='adam')\n    \nfor i in range(N_EPOCHS):\n    # Sample new negative items to build different triplets at each epoch\n    triplet_inputs = sample_triplets(train_pos, MAX_ITEM_ID, random_seed=i)\n        \n    # Fit the model incrementally by doing a single pass over the sampled triplets\n    triplet_model.fit(x=triplet_inputs, y=fake_y,\n                      shuffle=True, batch_size=BATCH_SIZE, epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the convergence of the model. Ideally, we should prepare a\n# validation set and compute this at each epoch but this is too slow.\ntest_auc = average_roc_auc(match_model, train_pos, test_pos)\nprint(f'Average ROC AUC on the trained model: {test_auc}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print summary of triplet model\ntriplet_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print summary of match model\nmatch_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both models have exactly the same number of parameters, namely the parameters of the two embeddings:\n* user embedding: $n_{users} \\times embedding_{dim}$\n* item embedding: $n_{items} \\times embedding_{dim}$\n\nThe triplet model uses the same item embedding twice, once to compute the positive similarity and the other time to compute the negative similarity. However, because two nodes in the computation graph share the same instance of the item embedding layer, the item embedding weight matrix is shared by the two branches of the graph and therefore the total number of parameters for each model is in both cases:\n$$n_{users} \\times embedding_{dim} + n_{items} \\times embedding_{dim}$$"},{"metadata":{},"cell_type":"markdown","source":"## Training a Deep Matching Model on Implicit Feedback\n\nInstead of using hard-coded cosine to predict the match of a `(user_id, item_id)` pair, we can instead specify a deep neural network based on parametrisation of the similarity. The parameters of that matching model are also trained with the margin comparator loss.\n![](https://github.com/m2dsupsdlclass/lectures-labs/raw/3cb7df3a75b144b4cb812bc6eacec8e27daa5214/labs/03_neural_recsys/images/rec_archi_implicit_1.svg)"},{"metadata":{},"cell_type":"markdown","source":"**Goals**\n* Implement a `(deep_match_model, deep_triplet_model)` pair of models for the architecture described in the schema. The last layer of the embedded Multi Layer Perceptron outputs a single scalar that encoded the similarity between a user and a candidate item.\n* Evaluate the resulting model by computing the per-usage average ROC AUC score on the test feedback data:\n    * Check that the AUC ROC score is close to $0.5$ for a randomly initialized model.\n   \n**Hints**\n* It is possible to reuse the code to create embeddings from the previous model definition.\n* The concatenation between user and the positive item embedding can be obtained with the `Concatenate` layer:\n        concat = Concatenate()\n        positive_embeddings_pair = concat([user_embedding, positive_item_embedding])\n        negative_embeddings_pair = concat([user_embedding, negative_item_embedding])\n* Those embedding pairs should be fed to a shared MLP instance to compute the similarity scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MLP(layers.Layer):\n    \"\"\"Define the MLP layer for the triplet architecture\n    \n    Parameters\n    ----------\n    n_hidden: Integer, default=1\n        Number of hidden layer\n    hidden_size: list of size `n_hidden`\n        Output size of the hidden layer\n    p_dropout: float, default=0.\n        Probability for the Dropout layer\n    l2_reg: float, default=None\n        Regularizer\n        \n    Argument\n    --------\n    layers: list of Layer\n        The different layers used in the MLP\n    \"\"\"\n    def __init__(self, n_hidden=1, hidden_size=[64], p_dropout=0., l2_reg=None):\n        super().__init__()\n        \n        self.layers = [Dropout(p_dropout)]\n        \n        for i in range(n_hidden):\n            self.layers.append(Dense(hidden_size[i], \n                                     activation='relu', \n                                     kernel_regularizer=l2_reg))\n            self.layers.append(Dropout(p_dropout))\n        \n        self.layers.append(Dense(1, \n                                 activation='relu', \n                                 kernel_regularizer=l2_reg))\n        \n    def call(self, x, training=False):\n        for layer in self.layers:\n            if isinstance(layer, Dropout):\n                x = layer(x, training=training)\n            else:\n                x = layer(x)\n        return x\n    \n    \nclass DeepTripletModel(Model):\n    \"\"\"Define the triplet model architecture\n    \n    Parameters\n    ----------\n    embedding_size_user: integer\n        Size of the embedding vector for the user\n    embedding_size_item: integer\n        Size of the embedding vector for the item\n    n_users: integer\n        Number of user in the dataset\n    n_items: integer\n        Number of item in the dataset\n    n_hidden: Integer, default=1\n        Number of hidden layer\n    hidden_size: list of size `n_hidden`\n        Output size of the hidden layer\n    l2_reg: float or None\n        Quantity of regularization\n    margin: float\n        Margin for the loss\n    p_dropout: float, default=0.\n        Probability for the Dropout layer\n        \n    Arguments\n    ---------\n    margin: float\n        Margin for the loss\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    flatten: Flatten\n        Flatten layer\n    concat: Concetenate\n        Concatenate layer\n    mlp: MLP\n        MLP layer\n    margin_loss: MarginLoss\n        Loss layer\n    \"\"\"\n    def __init__(self, n_users, n_items, \n                 embedding_size_user=64, embedding_size_item=64, \n                 n_hidden=1, hidden_size=[64], \n                 l2_reg=None, margin=1., p_dropout=0.):\n        super().__init__(name='TripletModel')\n        \n        # Define hyperparameters\n        self.margin = margin\n        l2_reg = None if l2_reg == 0 else l2(l2_reg)\n        \n        # Define Embedding layers\n        self.user_embedding = Embedding(output_dim=embedding_size_user,\n                                        input_dim=n_users,\n                                        input_length=1,\n                                        input_shape=(1,),\n                                        name='user_embedding',\n                                        embeddings_regularizer=l2_reg)\n        # The following embedding parameters will be shared to encode\n        # both the positive and negative items.\n        self.item_embedding = Embedding(output_dim=embedding_size_item,\n                                        input_dim=n_items,\n                                        input_length=1,\n                                        name='item_embedding',\n                                        embeddings_regularizer=l2_reg)\n        \n        # The two following layers are without parameters, and can\n        # therefore be used for oth potisitve and negative items.\n        self.flatten = Flatten()\n        self.concat = Concatenate()\n        \n        # Define the MLP\n        self.mlp = MLP(n_hidden, hidden_size, p_dropout, l2_reg)\n        \n        # Define the loss\n        self.margin_loss = MarginLoss(margin)\n        \n    def call(self, inputs, training=False, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with three elements\n            First element corresponds to the users\n            Second element corresponds to the positive items\n            Third element correponds to the negative items\n        \"\"\"\n        user_input = inputs[0]\n        item_pos_input = inputs[1]\n        item_neg_input = inputs[2]\n        \n        # Create embeddings\n        user_embedding = self.user_embedding(user_input)\n        user_embedding = self.flatten(user_embedding)\n        \n        item_pos_embedding = self.item_embedding(item_pos_input)\n        item_pos_embedding = self.flatten(item_pos_embedding)\n\n        item_neg_embedding = self.item_embedding(item_neg_input)\n        item_neg_embedding = self.flatten(item_neg_embedding)\n        \n        # Concatenate embeddings\n        pos_embeddings_pair = self.concat([user_embedding, item_pos_embedding])\n        neg_embeddings_pair = self.concat([user_embedding, item_neg_embedding])\n        \n        # Pass trough the MLP\n        pos_similarity = self.mlp(pos_embeddings_pair)\n        neg_similarity = self.mlp(neg_embeddings_pair)\n        \n        return self.margin_loss([pos_similarity, neg_similarity])\n\n    \nclass DeepMatchModel(Model):\n    \"\"\"Define the triplet model architecture\n    \n    Parameters\n    ----------\n    user_layer: Embedding\n        User layer from TripletModel\n    item_layer: Embedding\n        Item layer from TripletModel\n    mlp: MLP\n        MLP layer from TripletModel\n\n    Arguments\n    ---------\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    mlp: MLP\n        MLP layer\n    flatten: Flatten\n        Flatten layer\n    concat: Concatenate\n        Concatenate layer\n    \"\"\"\n    def __init__(self, user_layer, item_layer, mlp):\n        super().__init__(name='MatchModel')\n\n        # Reuse the layer from the triplet model\n        self.user_embedding = user_layer\n        self.item_embedding = item_layer\n        self.mlp = mlp\n        \n        self.flatten = Flatten()\n        self.concat = Concatenate()\n        \n    def call(self, inputs, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with three elements\n            First element corresponds to the users\n            Second element corresponds to the positive items\n        \"\"\"\n        user_input = inputs[0]\n        item_pos_input = inputs[1]\n        \n        # Create embeddings\n        user_embedding = self.user_embedding(user_input)\n        user_embedding = self.flatten(user_embedding)\n        \n        item_pos_embedding = self.item_embedding(item_pos_input)\n        item_pos_embedding = self.flatten(item_pos_embedding)\n        \n        pos_embeddings_pair = self.concat([user_embedding, item_pos_embedding])\n        \n        # Similarity computation between embeddings\n        pos_similarity = self.mlp(pos_embeddings_pair)\n        \n        return pos_similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define and train the model\nHYPER_PARAM = dict( \n    embedding_size_user=32, \n    embedding_size_item=64, \n    n_hidden=1, \n    hidden_size=[128], \n    l2_reg=0., \n    margin=0.5, \n    p_dropout=0.1)\n\ndeep_triplet_model = DeepTripletModel(N_USERS, N_ITEMS, **HYPER_PARAM)\ndeep_match_model = DeepMatchModel(deep_triplet_model.user_embedding, \n                                  deep_triplet_model.item_embedding, \n                                  deep_triplet_model.mlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Average ROC AUC on the untrained model: {average_roc_auc(deep_match_model, train_pos, test_pos)}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nN_EPOCHS = 20\n\n# We plug the identity loss and a fake target variable ignored by \n# the model to be able to use the Keras API to train the model.\nfake_y = np.ones_like(train_pos[\"new_user_id\"], dtype='int64')\n    \ndeep_triplet_model.compile(loss=identity_loss, optimizer='adam')\n    \nfor i in range(N_EPOCHS):\n    # Sample new negative items to build different triplets at each epoch\n    triplet_inputs = sample_triplets(train_pos, MAX_ITEM_ID, random_seed=i)\n        \n    # Fit the model incrementally by doing a single pass over the sampled triplets\n    deep_triplet_model.fit(x=triplet_inputs, y=fake_y,\n                      shuffle=True, batch_size=BATCH_SIZE, epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the convergence of the model. Ideally, we should prepare a\n# validation set and compute this at each epoch but this is too slow.\ntest_auc = average_roc_auc(deep_match_model, train_pos, test_pos)\nprint(f'Average ROC AUC on the trained model: {test_auc}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_triplet_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_match_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both models have again exactly the same number of parameters, namely the parameters of the two embeddings:\n- user embedding: $n_{users} \\times user_{dim}$\n- item embedding: $n_{items} \\times item_{dim}$\n\nand the parameters of the MLP model used to compute the similarity score of an `(user, item)` pair:\n- first hidden layer weights: $(user_{dim} + item_{dim}) \\times hidden_{size}$\n- first hidden biases: $hidden_{size}$\n- extra hidden layers weights: $hidden_{size} \\times hidden_{size}$\n- extra hidden layers biases: $hidden_{size}$\n- output layer weights: $hidden_{size} \\times 1$\n- output layer biases: $1$\n\nThe triplet model uses the same item embedding layer twice and the same MLP instance twice: once to compute the positive similarity and the other time to compute the negative similarity. However because those two lanes in the computation graph share the same instances for the item embedding layer and for the MLP, their parameters are shared."},{"metadata":{},"cell_type":"markdown","source":"## Possible Extensions\n\nWe may implement any of the following ideas.\n\n### Leverage User and Item metadata\n\nAs we did for the Explicit Feedback model, it's also possible to extend our models to take additional user and item metadata as side information when computing the match score.\n\n### Better Ranking Metrics\n\nIn this notebook, we evaluated the quality of the ranked recommendations using the ROC AUC metric. This score reflect the ability of the model to correctly rank any pair of items (sampled uniformly at random among all possible items).\n\nIn practice, recommender systems will only display a few recommendations to the user (typically 1 to 10). It is typically more informative to use an evaluation metric that characterize the quality of the top ranked items and attribute less or no importance to items that are not good recommendations for a specific users. Popular ranking metrics therefore include the *Precision at k* and the *Mean Average Precision*.\n\n### Hard Negatives Sampling\n\nIn this experiment, we sampled negative items uniformly at random. However, after training the model for a while, it is possible that the vast majority of sampled negatives have a similarity already much lower than the positive pair and that the margin comparator loss sets the majority of the gradients to zero effectively wasting a lot of computation.\n\nGiven the current state of the recsys model we could sample harder negatives with a larger likelihood to train the model better closer to its decision boundary. This strategy is implemented in the WARP loss [1].\n\nThe main drawback of hard negative sampling is increasing the risk of sever overfitting if a significant fraction of the labels are noisy.\n\n### Factorization Machines\n\nA very popular recommender systems model is called Factorization Machines [2][3]. They two use low rank vector representations of the inputs but they do not use a cosine similarity or a neural network to model user/item compatibility.\n\nIt is be possible to adapt our previous code written with Keras to replace the cosine sims / MLP with the low rank FM quadratic interactions by reading through [this gentle introduction](http://tech.nextroll.com/blog/data-science/2015/08/25/factorization-machines.html).\n\nIf you choose to do so, you can compare the quality of the predictions with those obtained by the [pywFM project](https://github.com/jfloff/pywFM) which provides a Python wrapper for the [official libFM C++ implementation](http://www.libfm.org/). Maciej Kula also maintains a lighfm that implements an efficient and well documented variant in Cython and Python.\n\n### References:\n\n[1] Wsabie: Scaling Up To Large Vocabulary Image Annotation\nJason Weston, Samy Bengio, Nicolas Usunier, 2011\nhttps://research.google.com/pubs/pub37180.html\n\n[2] Factorization Machines, Steffen Rendle, 2010\nhttps://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf\n\n[3] Factorization Machines with libFM, Steffen Rendle, 2012\nin ACM Trans. Intell. Syst. Technol., 3(3), May.\nhttp://doi.acm.org/10.1145/2168752.2168771\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}