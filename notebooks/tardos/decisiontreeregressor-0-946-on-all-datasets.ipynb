{"cells":[{"metadata":{"id":"ky2oahUSxu_Q","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max.columns', None)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"VqZqDodSxniv","outputId":"24ee5ab4-b59e-414e-cccc-4357248c50de","trusted":true},"cell_type":"code","source":"# Import du dataset\n# from google.colab import files\n# uploaded = files.upload()\n\n# Local\n# This data is composed of multiple datasets for each brand of car\n# We firstly need to get all those datsets in one plca befor deciding of merging them all together\ndata_path= '/kaggle/input/used-car-dataset-ford-and-mercedes/'\ndatasets_dict= {}\n\nfor data in os.listdir(data_path):\n    if 'unclean' not in data:\n        datasets_dict[data.replace(\".csv\", \"\")] = pd.read_csv(f'{data_path}/{data}')\n\nprint(datasets_dict.keys())","execution_count":null,"outputs":[]},{"metadata":{"id":"fFD6eGOUzGF0","outputId":"954aa838-52c9-431b-9ec6-5d20f2ceb791","trusted":true},"cell_type":"code","source":"# We will then check if all the datatsets have the sames columns\ndata_set_columns= [dataset.columns for dataset in datasets_dict.values()]\n[print(sorted(set(columns))) for columns in data_set_columns]","execution_count":null,"outputs":[]},{"metadata":{"id":"rZFLy6L00X0u","trusted":true},"cell_type":"code","source":"# We need to add a column \"manufacturer\" on each dataset to keep track of it once they're all merged\nmanufacturers_dict={\n    \"focus\": \"ford\",\n    \"cclass\": \"mercedes\",\n    \"hyundi\": \"hyundai\",\n    \"merc\": \"mercedes\",\n    \"vw\": \"Volkswagen\"\n}\nfor manufacturer, dataset in datasets_dict.items():\n    if manufacturer in manufacturers_dict.keys():\n      dataset[\"Manufacturer\"]= manufacturers_dict[manufacturer]\n    else:\n      dataset[\"Manufacturer\"]= manufacturer","execution_count":null,"outputs":[]},{"metadata":{"id":"wD17pfLGyH0E"},"cell_type":"markdown","source":"# Merging all datasets\n- We will use the append() method to do so\n    - Some columns describe the same variables but are not named the same way\n    - We will therefore have to complete one of the column with the values of the other column"},{"metadata":{"id":"pVXjkO3ryFzR","trusted":true},"cell_type":"code","source":"# We merge all datasets together\nfull_df= pd.concat(\n    list(datasets_dict.values()),\n    ignore_index= True)","execution_count":null,"outputs":[]},{"metadata":{"id":"oQNZYWyKyWJO","outputId":"c3c38b44-4b96-4e2b-9e2b-77550622028c","trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"n9d2yMzoyY__","outputId":"c56565db-30a6-4187-afe2-62d9fb54ff67","trusted":true},"cell_type":"code","source":"# Datset columns\nfull_df.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"-Wos3hMinu5C","outputId":"2d6d5e46-7f67-4731-a622-4e1e5ed4ad7b","trusted":true},"cell_type":"code","source":"# Infos\nfull_df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"aPVi-J1b-Npu"},"cell_type":"markdown","source":"# EDA\n- **Target variable**: \"price\"\n- **Shape**: (118150, 17)\n- **Missing values**:\n  - Clearly the tax(£) with **more than 90% of missing values**  \n- **Values types**"},{"metadata":{"id":"-jsOce1FzVnj","outputId":"a5f46000-5506-4570-d1e2-59ff0fd5e834","trusted":true},"cell_type":"code","source":"# We will work on a copy of this data set\ndf= full_df.copy()\nprint(f\"full_df: {full_df.shape}, Copy: {df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"18WazG3DtcNP","outputId":"49c4f719-88dc-4528-ac49-67fab56d857e","trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"9joyMTFntvpQ"},"cell_type":"markdown","source":"- We do not see any high coeficient of variance"},{"metadata":{"id":"Yy5Hee2Utm7Y","outputId":"f4a510dd-04b7-43a0-da74-83a3854f59d7","trusted":true},"cell_type":"code","source":"skew = df.describe().T\nskew['coef']=skew['std']/skew['mean']\nskew","execution_count":null,"outputs":[]},{"metadata":{"id":"y3noX2nw_VKt"},"cell_type":"markdown","source":"## Missing values"},{"metadata":{"id":"kdwy6oAFz4tm","outputId":"df1ee812-d145-4f55-ae64-341a5a06b8c8","trusted":true},"cell_type":"code","source":"# Heatmap to visualize the empty columns\nplt.figure(figsize=(15,10))\nsns.heatmap(df.isnull())","execution_count":null,"outputs":[]},{"metadata":{"id":"RGpfCc3N_fPK","outputId":"b46aa257-169d-4a6b-b54e-4afc11682c19","trusted":true},"cell_type":"code","source":"# Percentage of missing values per column\n(full_df.isnull().sum()/full_df.shape[0] * 100).sort_values(ascending= False)","execution_count":null,"outputs":[]},{"metadata":{"id":"wgnNnt8pz7E9","outputId":"43366a93-bb38-4b85-d304-cf12fa68357b","trusted":true},"cell_type":"code","source":"# Analysis of columns with more than 90% missing values\nmissing_cols= df.columns[full_df.isnull().sum()/full_df.shape[0] > 0.90]\nmissing_cols","execution_count":null,"outputs":[]},{"metadata":{"id":"aGVssJHhNZCB"},"cell_type":"markdown","source":"### Filling useful columns, deleting the useless ones\n"},{"metadata":{"id":"WupkUvsqHIkO","trusted":true},"cell_type":"code","source":"# We fillna() the tax column withe the tax(£) columns\ndf[\"tax\"]= df[\"tax\"].fillna(df[\"tax(£)\"])\n\n# We then drop it\ndf.drop(\"tax(£)\", inplace=True, axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"id":"x1bfJoQZSWVw","outputId":"a6e7f728-246c-4147-d2bb-9a4b82706417","trusted":true},"cell_type":"code","source":"df.isnull().sum()/df.shape[0] *100","execution_count":null,"outputs":[]},{"metadata":{"id":"L1dbrMGmShqS","outputId":"19fda12e-527d-4514-bb95-b4340a0ecaa7","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"zBwnN1QQDlUq"},"cell_type":"markdown","source":"### Drop NaN values"},{"metadata":{"id":"ksCLCZSADpd1","outputId":"e031f68c-3b9e-4c3b-838e-01efec72f5d4","trusted":true},"cell_type":"code","source":"df.isnull().sum()/df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"wuv793YiDxn-","outputId":"ae83912d-072d-4216-86c0-5c8e2d5de4cf","trusted":true},"cell_type":"code","source":"# We can drop the Nan values as they represent only 8% of the datframe\nprint(df.shape)\ndf= df.dropna()\nprint(df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1LnoMfDFTN3Z"},"cell_type":"markdown","source":"## Numerical Values"},{"metadata":{"id":"-PTi0PUJTL48","outputId":"f8f7e212-96a9-41ee-e5a8-a0f3733d012a","trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"0j1h9sItZmT5","outputId":"683b3995-ee25-4e75-d2d4-a3ffc1c4ae7f","trusted":true},"cell_type":"code","source":"# List of numerical columns\ndf_numerical= [col for col in df.columns if df[col].dtype != 'object']\ndf_numerical","execution_count":null,"outputs":[]},{"metadata":{"id":"hUv5PdsgZ1Ic","outputId":"4ca2427e-225b-4e20-f0c5-630e8389b8f9","trusted":true},"cell_type":"code","source":"\"\"\"Distribution of numerical values\"\"\"\n\nfig, ax = plt.subplots(3, 2, \n                       figsize=(10, 14))\ncol= 1\nfor i in df[df_numerical].columns:\n    plt.subplot(4, 2, col)\n    sns.distplot(df[i], color='blue')\n    col=col+1\n    plt.xlabel(i, fontsize=12)\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"-UiDOwJLa2sx","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"VXMEHmNcWWm-"},"cell_type":"markdown","source":"### Target: price\n- Our target 'price' is an object column, we will transform it to int\n- **Distribution study**:\n  - *Positive skewness factor*: verified by mean > median > mode\n    -  We therefore know that we have some outliers with high weight, we keep it in mind just in case\n    "},{"metadata":{"id":"8e_eemTXTY0t","outputId":"5237338a-8b98-4a0f-db50-9a2bb1d84090","trusted":true},"cell_type":"code","source":"# We will transform ou data frame to take values only where price is not null\ndf= df.dropna(subset=['price'])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"a4CEM-4naf9J","trusted":true},"cell_type":"code","source":"# We convert price column \nprice_col= df.price.astype(str)\nprice_col= price_col.str.replace('[£\\,]','').astype(float)","execution_count":null,"outputs":[]},{"metadata":{"id":"gM_JbR77amsm","outputId":"89f0fe60-3344-43ac-f6e6-cb9c1da75db0","trusted":true},"cell_type":"code","source":"# Target variable distribution\nsns.distplot(price_col)","execution_count":null,"outputs":[]},{"metadata":{"id":"zIGk21vVR5wZ","outputId":"d2e1b421-ddbd-4f7f-a9df-c97797aa3e01","trusted":true},"cell_type":"code","source":"# Stastical informations abour target\nprint(f\"Median: {price_col.median()}\\nMean: {price_col.mean()}\\nMode: {price_col.mode()}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"xpaEuurQScQN","outputId":"c2139e59-1ac5-4f3d-ae2d-6cb8ba8b3f54","trusted":true},"cell_type":"code","source":"# Clearer view of our target\nsns.boxplot(price_col)","execution_count":null,"outputs":[]},{"metadata":{"id":"XxYDXSMuWmVi","trusted":true},"cell_type":"code","source":"# finally we replace the column in our dataset\ndf.price= price_col","execution_count":null,"outputs":[]},{"metadata":{"id":"1WTX5PS7WtNN","outputId":"39210f7d-7a62-43f2-d901-959f1f7ded9e","trusted":true},"cell_type":"code","source":"# Check of the type once again\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"qW7CA5hwLy8D"},"cell_type":"markdown","source":"- Automatic and semi-auto worth more money than manual transmission"},{"metadata":{"id":"g9Uj70AsLqj9","outputId":"82dc9727-06ac-4fb7-c9bc-76e6ce2ee488","trusted":true},"cell_type":"code","source":"# Price by transmission type\nsns.barplot(x = df[\"transmission\"], y = df[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"vy39FGTUMS_6"},"cell_type":"markdown","source":"- Mercedes/audi/BMW are the manufacturer which worth the most money"},{"metadata":{"id":"-4xB__qHMC35","outputId":"6cbee1c1-4ae3-4e26-b79a-e382990288c2","trusted":true},"cell_type":"code","source":"# Price by manufacturer\nsns.barplot(x = df[\"Manufacturer\"], y = df[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"ya8zxJXLNRJd"},"cell_type":"markdown","source":"- The most recent cars worth the most money\n- We can also see that cars old enought to be considered as **collection cars (here 1970) worth also good money**"},{"metadata":{"id":"BqQPVzlINI1P","outputId":"83ba554d-c795-4f72-c548-e74da57aa34d","trusted":true},"cell_type":"code","source":"# Price by year\nplt.figure(figsize=(15,5),facecolor='w') \nsns.barplot(x = df[\"year\"], y = df[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"SeBoq9kLeskw"},"cell_type":"markdown","source":"### Mileage\n- No NaN values\n- Obvious negative corrrelation between price and mileage\n  - will show a correlation heatmap later in the study\n"},{"metadata":{"id":"MfA8OTqkr6Ln","outputId":"5d01e6d6-21e4-4696-83c5-d9fc8f485316","trusted":true},"cell_type":"code","source":"df.mileage.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"QoB1P7UHfWEO","outputId":"7323dd40-62de-46b8-888a-92345611fa98","trusted":true},"cell_type":"code","source":"sns.pairplot(df[['mileage', 'price']])","execution_count":null,"outputs":[]},{"metadata":{"id":"-KYRvZbRrI1d"},"cell_type":"markdown","source":"### Engine Size\n\n"},{"metadata":{"id":"Qvs0_ptUc7oV","outputId":"7866b816-65db-4411-96eb-50a38eac08be","trusted":true},"cell_type":"code","source":"df.engineSize.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"2lkVLvwrsplO","outputId":"bb145d8c-2bef-4e2a-bdb9-13134e87bf2c","trusted":true},"cell_type":"code","source":"sns.distplot(df.engineSize)","execution_count":null,"outputs":[]},{"metadata":{"id":"Gfr_V_5XwK5D"},"cell_type":"markdown","source":"### Year\n- No NaN values\n- Value with year =2060\n  - Delete this row"},{"metadata":{"id":"j_MatyCMwNbh","outputId":"6b93d6b0-015b-47d3-dc39-e2010b3802d4","trusted":true},"cell_type":"code","source":"df.year.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"IyOGD7sCyIIb","outputId":"580acebf-d1f7-4b32-f4d8-01ab866c30ef","trusted":true},"cell_type":"code","source":"sns.boxplot(df.year)","execution_count":null,"outputs":[]},{"metadata":{"id":"8o__BfeGy3i9","outputId":"c75eb9e9-7459-4655-e79f-be19f580556a","trusted":true},"cell_type":"code","source":"# Value at 2060 outlier\ndf.year.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"id":"MdT-thKpzMCI","trusted":true},"cell_type":"code","source":"# Deleting this row\ndf= df[df.year<=2021]","execution_count":null,"outputs":[]},{"metadata":{"id":"9kbyBJWp0BRL","outputId":"3ce3f645-95b4-4127-d5c4-79fabab31dcf","trusted":true},"cell_type":"code","source":"df.year.max()","execution_count":null,"outputs":[]},{"metadata":{"id":"_8dmecUILfXP","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"D-ghjBooY1yi"},"cell_type":"markdown","source":"## Categorical values\n"},{"metadata":{"id":"nO99k1iZX8J2"},"cell_type":"markdown","source":"#### Models\n- No NaN values in this column\n- We can observe the most present cars model for each manufacturer\n- The most present model in the dataset"},{"metadata":{"id":"CktYQN1-Yjv4","outputId":"363bf50e-1162-4709-b26f-f313007652b2","trusted":true},"cell_type":"code","source":"# null values\ndf.model.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"35_wiDI0YV6o","outputId":"90c2f9c0-c878-48e6-f3de-159f16b3e9ab","trusted":true},"cell_type":"code","source":"# Number of models\nlen(df.model.unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"YiGO9--EVnL7","outputId":"70901c39-b841-4510-b7bb-620293071624","trusted":true},"cell_type":"code","source":"# Population of each model for a given manufacturer\nplt.figure(figsize=(7,11))\ndf[\"model\"].hist(by= df.Manufacturer, figsize= (15,11))","execution_count":null,"outputs":[]},{"metadata":{"id":"lv2HF3yJZByn","outputId":"c168fd57-f1ab-41db-feea-53209b8f61fb","trusted":true},"cell_type":"code","source":"# Most represented model in dataset\ndf.model.value_counts(normalize= True)*100","execution_count":null,"outputs":[]},{"metadata":{"id":"y2JtM_TIcbsC"},"cell_type":"markdown","source":"#### Transmission\n- No NaN values\n- Distribution of each kind of transmission\n- Most of the transmissions are manual\n"},{"metadata":{"id":"-WIfFp5DeiXZ","outputId":"325d1984-68be-4aac-9874-01af59647087","trusted":true},"cell_type":"code","source":"df.transmission.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"WLlFa09XebMg","outputId":"cf6110ff-ddf7-4161-d845-f0d1050f6d44","trusted":true},"cell_type":"code","source":"df.transmission.value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"id":"XsqkB5aBXv9g","outputId":"d9cb0b04-dfc2-4442-92b1-f0179bd4c666","trusted":true},"cell_type":"code","source":"sns.countplot(df.transmission)","execution_count":null,"outputs":[]},{"metadata":{"id":"IFLxkJVKiqJU"},"cell_type":"markdown","source":"#### Fuel type\n- No NaN values\n- Different kind of fuel type in the dataset\n- Most present fuel type is Petrol"},{"metadata":{"id":"pndFydnWp6gq","outputId":"0372c87d-93e1-40a9-c16b-91835fc49702","trusted":true},"cell_type":"code","source":"df[\"fuelType\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"KdAusrgBhqU2","outputId":"7380e6c3-914e-4407-8e00-0f41355fd5e1","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,7))\nplt.xticks(rotation=90)\nsns.countplot(df['fuelType'])","execution_count":null,"outputs":[]},{"metadata":{"id":"w3figlYWWMDU","outputId":"e35a1df8-2f3f-487b-b296-a5d59f07ee4b","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"8nrlWbI8EUBt"},"cell_type":"markdown","source":"## Correlations\n- Important correlations:\n  - year/mileage: -0.74\n  - year/price: +0.49\n  - price/engineSize: +0.64\n  - tax/mpg: -0.45\n  - price/mileage: -0.42"},{"metadata":{"id":"vnxIs3xCEYOI","outputId":"8ab9bbec-d990-4350-e1ea-a6074bcc4395","trusted":true},"cell_type":"code","source":"# Correlation heatmap\nsns.heatmap(df.corr(),\n            annot= True,\n            square= True,\n            linewidth=1, linecolor='w')","execution_count":null,"outputs":[]},{"metadata":{"id":"OqiHGwBEFDec","outputId":"818f2ea1-dcb4-4eef-e593-c2955d010999","trusted":true},"cell_type":"code","source":"# Pair plots\nsns.pairplot(df.sample(frac= 0.8))","execution_count":null,"outputs":[]},{"metadata":{"id":"rz7F_sNJOy0D"},"cell_type":"markdown","source":"## Features creation\n- We will create:\n  - \"Country\" feature based on the manufacturer's country\n  - \"Age\" feature based on the age of the car, easily computable\n- We will remove the year column to avoid correlation"},{"metadata":{"id":"b6xaWk3NPQ_b","outputId":"c1d21e34-b045-45be-d10c-5109d28287d6","trusted":true},"cell_type":"code","source":"# Country column\nmap_country={\n    'bmw':'germany', \n    'mercedes':'germany', \n    'audi':'germany', \n    'vauxhall': 'USA', \n    'ford': 'USA', \n    'toyota':'japan', \n    'hyundai':'south_korea',\n    'Volkswagen':'germany', \n    'skoda': 'czech'\n}\n\ndf[\"country\"]= df[\"Manufacturer\"].map(map_country)\ndf[[\"Manufacturer\", \"country\"]].head(3)","execution_count":null,"outputs":[]},{"metadata":{"id":"y5JqrYJEPVCG","outputId":"62155796-8ad1-4b05-ce48-27dd464a5d63","trusted":true},"cell_type":"code","source":"# Age column\ndf[\"age\"]= abs(df[\"year\"]-2021)\ndf[[\"year\", \"age\"]].head(3)","execution_count":null,"outputs":[]},{"metadata":{"id":"bmNO0RglQ4Bn","outputId":"6221d9f1-4e20-4ce1-ff06-270a02a076b0","trusted":true},"cell_type":"code","source":"# Droping the year column\ndf.drop(\"year\", axis=1, inplace=True)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"Im7uMWw0Q8Zn"},"cell_type":"markdown","source":"# Data pre-processing"},{"metadata":{"id":"VKIi7F34gTFi","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"id":"j65a8prvfP2s","outputId":"add5e3f3-b911-49f0-f388-71a3c54882a7","trusted":true},"cell_type":"code","source":"# Categorical columns\ndf_categorical = [col for col in df.columns if df[col].dtype =='object']\nprint(df_categorical)\n\n# Numerical columns\ndf_numerical = [col for col in df.columns if df[col].dtype !='object']\nprint(df_numerical)","execution_count":null,"outputs":[]},{"metadata":{"id":"Fe4ZQvXRfFya"},"cell_type":"markdown","source":"## One Hot encoder\n- In order to have only numerical values, we need to encode our categorical data\n- We will choose pandas.get_dummies over OHE from sklearn since it will keep column names more recognizable"},{"metadata":{"id":"z69Fh8bmft6E","outputId":"b187b6fc-5b77-4464-a312-277671f47bf5","trusted":true},"cell_type":"code","source":"# One Hot encoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse= False, drop='first')\ntest = ohe.fit_transform(df[df_categorical])\ntest","execution_count":null,"outputs":[]},{"metadata":{"id":"3XRkCliWhZZW","outputId":"2ce82ec7-afd2-4bf6-8a8a-7aca3c08e140","trusted":true},"cell_type":"code","source":"# Pandas dummies\ndf_expended= pd.get_dummies(df)\ndf_expended.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"0Lgyk6urDV-9","outputId":"719d29a3-03f0-4abd-bec2-ba941fe49639","trusted":true},"cell_type":"code","source":"df_expended.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"ByYTy4PVVHzY"},"cell_type":"markdown","source":"## Standardisation\n- We will then standardize all the variables in the data set\n- The standard score of a sample x is calculated as:\n\n```\n    z = (x - u) / s\n```\n\n  - u --> is the mean of the training samples or zero if with_mean=False\n  - s --> is the standard deviation of the training samples or one if with_std=False."},{"metadata":{"id":"MF54CPVeT21_","outputId":"25d181d9-8fc8-4bae-b6b2-d83b08d172b3","trusted":true},"cell_type":"code","source":"# Standard scaler whithout standardisation\nfrom sklearn.preprocessing import RobustScaler\nstder= StandardScaler(with_std= False)\n\ndf_expended_std = stder.fit_transform(df_expended)\ndf_expended_std = pd.DataFrame(df_expended_std, columns = df_expended.columns)\nprint(df_expended_std.shape)\ndf_expended_std.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ggf-tF7QHs3R"},"cell_type":"markdown","source":"## Splitting sets"},{"metadata":{"id":"1N5_o0erw6k7","outputId":"ad1cfa29-848e-4ccb-9580-20a6bce3cf5f","trusted":true},"cell_type":"code","source":"# Features\nX= df_expended_std.drop(\"price\", axis=1)\n\n# Target\ny= df_expended_std[\"price\"]\n\nprint(f\"X: {X.shape}\\ny: {y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"_-xf9QRvw7_S","trusted":true},"cell_type":"code","source":"# We creat our test and train sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"mYniK9oi2R8M","outputId":"3da087de-aadd-4e70-a0d8-aca5adb82dcc","trusted":true},"cell_type":"code","source":"size={\n    \"x_test\": X_test.shape,\n    \"x_train\": X_train.shape,\n    \"y_test\": y_test.shape,\n    \"y_train\": y_train.shape\n}\nprint(size)","execution_count":null,"outputs":[]},{"metadata":{"id":"ALq8w318JvaG"},"cell_type":"markdown","source":"# **Model**\n\n- For this regression study, we need to predic the price of a car based on all the different given parameters, in order to do so we will use a linear regression model.\n- We have:\n  - Feature engineered (kind of) by creating two variables, age and country of the manufacturer.\n  - Pre processed our data: One Hot Encoder, Standardisation\n- We will:\n  - Select the best features with SelectKBest: Trying multiple sets of features and choose a suitable number of features."},{"metadata":{"id":"p3VgKvpCQIIf"},"cell_type":"markdown","source":"## Selecting best features\n- We have 223 features after the OHE with pd.get_dummies(), **I will therefore use the SelectKbest()** from sklearn in order to select the best features to apply regression.\n  - **SelectKBest** will do for us an univariate feature selection with a scoring based on f_regression. It will select the k best features based of the scoring of each one against the data set\n- I will **select from 4 to 223 features (k) on f_regression** in order to see the most revelant features in the dataset.\n- **We will select 158 features**"},{"metadata":{"id":"thOeuEbJJg9l","outputId":"09e9ae9c-4ee7-45b3-fb21-eea35a8e3501","trusted":true},"cell_type":"code","source":"column_names = df_expended_std.drop('price', axis= 1).columns\n\nno_of_features = []\nr_squared_train = []\nr_squared_test = []\n\n# We iterate over a range of 4, 223 for the number of best features\nfor k in range(4, 224, 2):\n    selector = SelectKBest(f_regression, \n                           k = k)\n    \n    # Our transformed sets with the k-best features\n    X_train_transformed = selector.fit_transform(X_train, y_train)\n    X_test_transformed = selector.transform(X_test)\n\n    # We train a basic regression model on those transformed sets\n    regressor = LinearRegression()\n    regressor.fit(X_train_transformed, y_train)\n\n\n    no_of_features.append(k)\n    r_squared_train.append(regressor.score(X_train_transformed, y_train))\n    r_squared_test.append(regressor.score(X_test_transformed, y_test))\n    \nsns.lineplot(x = no_of_features, y = r_squared_train, legend = 'full')\nsns.lineplot(x = no_of_features, y = r_squared_test, legend = 'full')","execution_count":null,"outputs":[]},{"metadata":{"id":"yA2O2DE-w6aV","outputId":"bed745ea-64be-4d23-8878-80c7b382ac74","trusted":true},"cell_type":"code","source":"# We can see that the curve stabilizes around ~160 variables\n# We will inspect more closely\n\nmax_test_score= max(r_squared_test)\nindex_max= r_squared_test.index(max_test_score)\nprint(f\"Best score is obtained for {no_of_features[index_max]} features --> score: {max_test_score}\") ","execution_count":null,"outputs":[]},{"metadata":{"id":"IuKT8R2v2rl9"},"cell_type":"markdown","source":"- 158 features seems a good choice as it's shows the first score of at least 0.85"},{"metadata":{"id":"MJEzwPMqz_XV","trusted":true},"cell_type":"code","source":"# We will see with less variables\nfor n_features in range(50, 224,2):\n  index_reasonable= no_of_features.index(n_features)\n  score= r_squared_test[index_reasonable]\n  print(\"--\",n_features, score)","execution_count":null,"outputs":[]},{"metadata":{"id":"heRtKq5g067t","outputId":"866d5799-9638-4b07-c7bb-344f23a535de","trusted":true},"cell_type":"code","source":"sns.lineplot(x = no_of_features[20:], y = r_squared_train[20:], legend = 'full')\nsns.lineplot(x = no_of_features[20:], y = r_squared_test[20:], legend = 'full')","execution_count":null,"outputs":[]},{"metadata":{"id":"SVvnrjgv2Tid","trusted":true},"cell_type":"code","source":"# We selected 158 columns, let see them\nselector = SelectKBest(f_regression, k = 158)\n\n# Transformed sets\nX_train_transformed = selector.fit_transform(X_train, y_train)\nX_test_transformed = selector.transform(X_test)\n\n# Names of best features\nkbest_features = list(column_names[selector.get_support()])","execution_count":null,"outputs":[]},{"metadata":{"id":"eawwTd8bNAWw"},"cell_type":"markdown","source":"## Trying different models\n- We will choose the DecisionTreeRegressor as it gives us a R^2 of 0.92"},{"metadata":{"id":"rEdt7QwpJyMm","trusted":true},"cell_type":"code","source":"# Function to try our different models later\ndef test_regressor_model(models_list, X_train_transformed, X_test_transformed, y_train, y_test):\n  \"\"\"\n  - models_list: list of tuple, \n    - tuple[0] = model to test, \n    - tuple[1] = model_name\n  \"\"\"\n  for model in models_list:\n    model[0].fit(X_train_transformed, y_train)\n\n    y_pred= model[0].predict(X_test_transformed)\n    score= model[0].score(X_test_transformed, y_test)\n    print(f\"{model[1]:-<50}{score}\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tUW4UqX3N9K1","outputId":"2a7deaef-db57-4b29-bebe-21f092522c8b","trusted":true},"cell_type":"code","source":"# Creation of the list of models we want to test\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge, ElasticNet, Lasso\n\nmodels_list= [(DecisionTreeRegressor(), \"DecisionTreeRegressor\"),\n              (Ridge(), \"Ridge regression\"),\n              (ElasticNet(), \"Elastic Net\"),\n              (Lasso(), \"Lasso\")]\n\n# Scoring              \ntest_regressor_model(models_list, X_train_transformed, X_test_transformed, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"kjMrGpF8_DxD"},"cell_type":"markdown","source":"# Model optimisation\n- We will try to optimise as much as possible our selected model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nparams= {'min_samples_split': range(2, 15)}\ngrid= GridSearchCV(DecisionTreeRegressor(), params, n_jobs=15)\n\ngrid.fit(X_train_transformed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ny_pred= grid.predict(X_test_transformed)\n\nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}