{"cells":[{"metadata":{"_uuid":"17b2b5de772a089d21031530a60a49513027c940"},"cell_type":"markdown","source":"# **Telco Customer Churn - Comparing Classification Algorithms**"},{"metadata":{"_uuid":"6c16a8f04cea1d0a870c23fd90e5a6e6ae12a519"},"cell_type":"markdown","source":"### One of the biggest challenges in the telecom service industry is to retain customers. Our main aim here is to create a model to forecast if, based on custumer's profile, he or she will churn or not in the future.\n\n### We begin by loading the most relevant packages. After data reading, we discharge the column customer ID, since it will be useless to us.  "},{"metadata":{"trusted":true,"_uuid":"5bbe4e9a68dbfcccc8ffc289374859e0eba50441"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import sample\nimport numpy as np\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32eaf079db933025ec4acda614afcbaad4b47d0d"},"cell_type":"code","source":"data = pd.read_csv('../input/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndata.drop('customerID', axis='columns', inplace=True)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79e098fb1574e2c9cdbb547f5f454797552bb653"},"cell_type":"markdown","source":"### The column TotalCharges was read as strings, because some of the rows the data is non-numerical. We will transform it into numerical data and discharge some few non-numerical rows. We also create a list with the name of the categorical columns."},{"metadata":{"trusted":true,"_uuid":"38f3822948d82cd7ec75ede2e0c4a09cc7290fe9"},"cell_type":"code","source":"data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\nprint(data.shape)\ndata.dropna(inplace=True)\nprint(data.shape)\n\ncolumns = list(data.columns)\ncolumns.remove('MonthlyCharges')\ncolumns.remove('tenure')\ncolumns.remove('TotalCharges')\nprint(columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"598a233138dcbf4ed38fc14275066dc9e99e3297"},"cell_type":"markdown","source":"### We can do some basic exploratory analysis by plotting the categorical features, dividing them in Churn and No Churn. Here we can already make some good assumptions about which kind of customer can churn."},{"metadata":{"trusted":true,"_uuid":"240ea49d3981bf846ba7f8b351661ad3d40a6070"},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\", font_scale=1.75)\nfor column in sample(columns, 5):\n    plt.figure(figsize=(8,8))\n    sns.countplot(x=column, hue='Churn', data=data)\n    plt.xticks(rotation='45')\n    plt.margins(0.2)\n    plt.show()\n    print('\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0cbd7b1a27a9b75cfc2e624f6d095fd0d5fb386"},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.distplot(data.loc[data['Churn']=='No', 'MonthlyCharges'], label='Churn: No')\nsns.distplot(data.loc[data['Churn']=='Yes', 'MonthlyCharges'], label='Churn: Yes')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(8,8))\nsns.distplot(data.loc[data['Churn']=='No', 'tenure'], label='Churn: No')\nsns.distplot(data.loc[data['Churn']=='Yes', 'tenure'], label='Churn: Yes')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(8,8))\nsns.distplot(data.loc[data['Churn']=='No', 'TotalCharges'], label='Churn: No')\nsns.distplot(data.loc[data['Churn']=='Yes', 'TotalCharges'], label='Churn: Yes')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fca5074a01fe8a70fa8feb6d18abcfa9fa8e82db"},"cell_type":"markdown","source":"# Classification"},{"metadata":{"_uuid":"d061c1bca8d06d6ce39bbe5167716d89973173f4"},"cell_type":"markdown","source":"### First we load the relevant packages, and create the function that will present us the results of our models tests."},{"metadata":{"trusted":true,"_uuid":"2678e721e3ee085908206fd9b160b2983ac3a7fa"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc1f0664d229f396e221e6ef1d2f3a3d40f2dd87"},"cell_type":"code","source":"def full_report(classifier, X_test, y_test, class_labels):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.metrics import classification_report, confusion_matrix\n    from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n    \n    print('Best train score (accuracy) {0:.2f}'.format(clf.best_score_))\n    print('Best parameters {}\\n'.format(clf.best_params_))\n\n    y_pred = clf.predict(X_test)\n    y_proba = clf.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n    \n    test_accuracy = accuracy_score(y_test,y_pred)\n    test_f1_score = f1_score(y_test, y_pred)\n    test_roc_auc = roc_auc_score(y_test, y_proba)\n    \n    print('Test Accuracy: {0:.2f}'.format(test_accuracy))\n    print('Test f1 score: {0:.2f}'.format(test_f1_score))\n    print('Test ROC-AUC: {0:.2f}\\n'.format(test_roc_auc))\n\n    print(classification_report(y_test,y_pred, target_names=class_labels))\n\n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    \n    sns.set(style=\"darkgrid\")\n    sns.set_context(\"notebook\", font_scale=1.75)\n    \n    cnf_matrix = 100*(cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis])\n\n    plt.figure(figsize=(8,8))\n    plt.title('Confusion matrix (%)')\n    sns.heatmap(cnf_matrix, \n                annot=True,\n                cmap='RdYlBu',\n                xticklabels=class_labels, \n                yticklabels=class_labels)\n    plt.xlabel('Predicted')\n    plt.ylabel('Observed')\n    plt.tight_layout()\n    plt.show()\n\n    plt.figure(figsize=(8,8))\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=2, label='ROC Curve (area = %0.2f)' % roc_auc_score(y_test, y_proba))\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.tight_layout()\n    plt.show()\n    \n    return [test_accuracy, test_f1_score, test_roc_auc]\n\ndef FeaturesImportance(importances, labels):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    \n    labels = labels[importances > 0]\n    importances = importances[importances > 0]\n    \n    importances = list(importances)\n    labels = list(labels)\n    \n    ordered_labels =[]\n    ordered_importances = []\n    \n    for _ in range(len(importances)):\n        i_max = importances.index(max(importances))\n        ordered_labels.append(labels[i_max])\n        ordered_importances.append(importances[i_max])\n        importances.pop(i_max)\n        labels.pop(i_max)\n\n    plt.figure(figsize=(8,8))\n    plt.title('Features Importance')\n    sns.barplot(x=ordered_importances, y=ordered_labels)\n    plt.xlabel('Relative importance')\n    plt.ylabel('Feature')\n    #plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01694aacfa8b30a54f57022e05fc0e4a0d9f53c9"},"cell_type":"markdown","source":"### The categorical data will be encoded into numbers."},{"metadata":{"trusted":true,"_uuid":"588156e4d4c728b684196edd0c71fa95a0ceec45"},"cell_type":"code","source":"X = data[columns[:-1]]\n\nle = LabelEncoder()\ny = le.fit_transform(data['Churn'])\n\ndummie_columns = []\nfor column in columns[:-1]:\n    if (len(np.unique(data[column]))==2):\n        le = LabelEncoder()\n        X[column] = le.fit_transform(X[column])\n    else:\n        dummie_columns.append(column)\n\nprint('Dummie columns: {}'.format(dummie_columns))\n\nX = pd.get_dummies(X, columns=dummie_columns)\n\nX['MonthlyCharges'] = data['MonthlyCharges']\nX['tenure'] = data['tenure']\nX['TotalCharges'] = data['TotalCharges']\nX[['MonthlyCharges', 'tenure', 'TotalCharges']] = StandardScaler().fit_transform(data[['MonthlyCharges', 'tenure', 'TotalCharges']])\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c416d43e47631b05e099ba15d121b85f37738d6"},"cell_type":"markdown","source":"### And divided into train and test sets."},{"metadata":{"trusted":true,"_uuid":"6120c01a6c70911668cf05df7980c1554898b048"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68addd037fb511ca257c8d8537b33ee0ee03bca5"},"cell_type":"code","source":"summary = {}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530b6579af7033d60adb31a02073e342e5fcdb1e"},"cell_type":"markdown","source":"### Next, we will train our models and look for the best hyper-parameters of each one and test the best model found. In the end, we will present a summary of all results. For the test, we will use as metrics the accuracy, the f1 score, the ROC area under the curve and the confusion matrix, because you know, picture is worth a thousand words.\n\n### The algorithms used here are: Logistic Regression, Support Vector, Stochastic Gradient Descent, Nearest Neighbors, Gaussian Naive Bayes, Bernoulli Naive Bayes Decision Tree, Random Forest, Extra Trees and finally Gradient Boosting."},{"metadata":{"_uuid":"3f6fdef5b3212dc49a7cffaf87f696d48dcae224"},"cell_type":"markdown","source":"## **Logistic Regression**"},{"metadata":{"trusted":true,"_uuid":"efcdc65dde57d01ab8ac3491939d57b407d69aa8"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nparams = {\n    'penalty':['l1','l2'],\n    'tol':[1e-6,1e-5,1e-4,1e-3,1e-2],\n    'C':[10,1,0.1,0.01,0.001],  \n}\n\nlog_reg = LogisticRegression(class_weight='balanced',\n                            random_state=42)\n\nclf = GridSearchCV(log_reg, param_grid=params, \n                   cv=5, scoring='accuracy', \n                   verbose=1, n_jobs=-1)\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f050e1ee3489edffe5942f9754da768bd1ceb7b2"},"cell_type":"code","source":"summary['LogisticRegression'] = full_report(clf, X_test, y_test, ['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75bfaaadf48a7af1e766de6520dcaf5811a48d2d"},"cell_type":"markdown","source":"## **Stochastic Gradient Descent**"},{"metadata":{"trusted":true,"_uuid":"a2a87d512bb641512e7992c9925bc94cc071c35e"},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nparams = {\n    'loss':['hinge', 'log',  'perceptron'],\n    'penalty': [None, 'l2', 'l1', 'elasticnet'],\n    'alpha':10.0**-np.arange(1,5),\n    'tol':10.0**-np.arange(1,5),\n    'eta0':10.0**-np.arange(1,5),\n    'learning_rate':['constant', 'optimal', 'invscaling'],\n}\n\nsgd_clf = SGDClassifier(class_weight='balanced', \n              n_jobs=-1, random_state=42)\n\nclf = GridSearchCV(sgd_clf, param_grid=params, \n                   cv=5, scoring='accuracy', \n                   verbose=1, n_jobs=-1)\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a76f370e0ad5c217294a625c78706a289168502"},"cell_type":"code","source":"summary['StochasticGradientDescent'] = full_report(clf, X_test, y_test, ['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c1b84020dd7e361e80f7b3fa3df39635a1a5f7c"},"cell_type":"markdown","source":"## **Decision Tree Classifier**"},{"metadata":{"trusted":true,"_uuid":"84ad4bdb1229a3d669aff428b845a25e609a63c1"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nparams = {\n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':list(range(1,11)),\n    'min_samples_split':list(range(2,11)),\n    'min_samples_leaf':list(range(1,11)),\n    'max_features':['auto','sqrt','log2',None],\n    \n}\n\ndt_clf = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n\nclf = GridSearchCV(dt_clf, param_grid=params, \n                   cv=5, scoring='accuracy', \n                   verbose=1, n_jobs=-1)\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03b11931c8c6c8db3ee6322b215340af9b5773ab"},"cell_type":"code","source":"summary['DecisionTreeClassifier'] = full_report(clf, X_test, y_test, ['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31ee8565058ecdc6ba85763dbeb42e605a18d80c"},"cell_type":"code","source":"FeaturesImportance(clf.best_estimator_.feature_importances_, X.columns) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de9db78e4cfcdf7fb060703b232e45d194105f2d"},"cell_type":"markdown","source":"## **Random Forest Classifier**"},{"metadata":{"trusted":true,"_uuid":"2dd48bfca2c9da267d76d4fd4fdb683959884c08"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nparams = {\n    'n_estimators':[10]+list(range(50,301,50)),\n    'criterion':['gini','entropy'],\n    'max_features':['auto','sqrt','log2',None],\n    'max_depth':list(range(1,4)),\n    'min_samples_split':list(range(2,5)),\n    'min_samples_leaf':list(range(1,4)),\n}\n\nrf_clf = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced')\n\nclf = GridSearchCV(rf_clf, param_grid=params, \n                   cv=5, scoring='accuracy', \n                   verbose=1, n_jobs=-1)\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8da18afac620a187afcc4dd94827408d0821d212"},"cell_type":"code","source":"summary['RandomForestClassifier'] = full_report(clf, X_test, y_test, ['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a75190c301853f8b16c34d9df4635f096ab24240"},"cell_type":"code","source":"FeaturesImportance(clf.best_estimator_.feature_importances_, X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77c30797a9b9634d65dc3b938a5c71fc5748e163"},"cell_type":"code","source":"summary = pd.DataFrame.from_dict(summary, orient='index')#, columns=['Accuracy', 'f1 score', 'ROC area'])\nsummary.rename(columns ={0:'Accuracy', 1:'f1 score', 2:'ROC area'}, inplace=True)\nprint(summary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28274fbfed6f560332a6554440696c4d9b636fab"},"cell_type":"markdown","source":"# **Conclusion**"},{"metadata":{"_uuid":"b42166fcd8b8276b2bd1bd82f626a1d1614e40b1"},"cell_type":"markdown","source":"### As can be seen, with the present data the models developed here are fairly good. The main problem here, as in many other real cases, is the lack o balance between the classes, which in this case means that there are almost 3 times more samples in the No Churn class.\n\n### Even thought, some simple models, that run faster then others, like logistic regression and decision trees could restore the balance and could maybe be improved with more data. On the other hand, some models were better in one of the classes. One could use each of them depending on the objectives.\n\n### In general, all the models could be improved with a bigger, balanced data set. More features could also be helpful."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}