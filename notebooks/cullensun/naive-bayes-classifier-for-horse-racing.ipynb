{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Classification with Naive Bayes Classifier for Hong Kong Horse Racing\nThis is a differnt differnt model but do the same task as [my another notebook with Deep Neural Network](https://www.kaggle.com/cullensun/deep-learning-model-for-horse-racing). Naive Bayes Classifer is based on statistical classification. Let's see if it's better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import packages","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read inputs\nHere, I am going to select some features that I think useful. I will also join runs.csv and races.csv because they are related and each includes some features for the classification.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"races = pd.read_csv(r\"../input/hkracing/races.csv\", delimiter=\",\", header=0, index_col='race_id')\nraces_data = races[['venue', 'race_no', 'config', 'surface', 'distance', 'going', 'horse_ratings', 'race_class']]\nruns = pd.read_csv(r\"../input/hkracing/runs.csv\", delimiter=\",\", header=0)\nruns_data = runs[['race_id', 'result', 'won', 'horse_age', 'horse_country', 'horse_type', 'horse_rating',\n                  'declared_weight', 'actual_weight', 'draw', 'win_odds']] \ndata = runs_data.join(races_data, on='race_id')\n# drop race_id after join because it's not a feature\ndata = data.drop(columns=['race_id'])\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data prepocessing\n- Drop rows with NaN\n- Use different encoders for ordinal and nomimal columns.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove rows with NaN\nprint(data[data.isnull().any(axis=1)])\nprint('data shape before drop NaN rows', data.shape)\ndata.dropna(inplace=True)\ndata.reset_index(drop=True, inplace=True)\nprint('data shape after drop NaN rows', data.shape)\n\n# encode ordinal columns: config, going, horse_ratings\nencoder = preprocessing.OrdinalEncoder()\ndata['config'] = encoder.fit_transform(data['config'].values.reshape(-1, 1))\ndata['going'] = encoder.fit_transform(data['going'].values.reshape(-1, 1))\ndata['horse_ratings'] = encoder.fit_transform(data['horse_ratings'].values.reshape(-1, 1))\n\n# encode labels\nlb_encoder = preprocessing.LabelEncoder()\ndata['horse_country'] = lb_encoder.fit_transform(data['horse_country'])\ndata['horse_type'] = lb_encoder.fit_transform(data['horse_type'])\ndata['venue'] = lb_encoder.fit_transform(data['venue'])\n\nprint(data.dtypes)\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection\nFeature selection is so important. When I tried to reduce/add some columns for fitting, I found that dropping some feature will cause big change to the performance. Then I found this [article](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e) and learned to find best features. To my surprise, win_odds is such an important feature. \n\nAfter found the important features, I simply use them as fitting data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature selection\n# result and won are outputs, the rest are inputs\nX = data.drop(columns=['result', 'won'])\ny = data['won']\n\n# apply SelectKBest class to extract top 10 best features\nbest_features = SelectKBest(score_func=chi2, k=10)\nfit = best_features.fit(X, y)\ndf_scores = pd.DataFrame(fit.scores_)\ndf_columns = pd.DataFrame(X.columns)\n# concat two dataframes for better visualization \nfeature_scores = pd.concat([df_columns, df_scores], axis=1)\nfeature_scores.columns = ['features', 'score']  \nprint(feature_scores.nlargest(10, 'score')) \n\n# choose the top 10 features only\nX = data[['win_odds', 'draw', 'declared_weight', 'actual_weight', 'horse_rating', \n          'horse_country', 'venue', 'race_no', 'horse_ratings', 'race_class']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting with Naive Bayes Classifier \nHere, I use kFold cross validation, and calculate the precision. Luckily, it's just 3 lines of code with help of sklearn packages.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB()\nscores = cross_val_score(mnb, X, y, cv=10, scoring='precision')\naverage_precision = sum(scores) / len(scores) \nprint(f'MultinomialNB average precision: {average_precision}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion \n\nI chose `precision` as an evaluation parameter because it's our interest to bet on win.\n\n$ precision = \\frac{TP}{TP + FP} $\n\nIf `precision = 0.1`, it means we make bet for winning horse 10 times and only once is correct. Generally speaking, the model generalize poorly.  \n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}