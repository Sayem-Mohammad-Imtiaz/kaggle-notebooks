{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9d1134c5-bf60-f61e-0aa3-882600b2f6c3"},"source":"Comparing different classifiers to predict the type of breast cancer "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"deffb92c-a7ac-bf19-a54a-73a8acaf8b7b"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a7735d8-9708-a67d-3f37-a6b9a64c9c7a"},"outputs":[],"source":"#First import all the libraries needed\n\nimport numpy as np #for linear algebra\nimport pandas as pd #for chopping, processing\nimport csv #for opening csv files\n%matplotlib inline \nimport matplotlib.pyplot as plt #for plotting the graphs\nfrom sklearn.linear_model import LogisticRegression #for logistic regression\nfrom sklearn.pipeline import Pipeline #to assemble steps for cross validation\nfrom sklearn.preprocessing import PolynomialFeatures #for all the polynomial features\nfrom sklearn import svm #for Support Vector Machines\nfrom sklearn.neighbors import NearestNeighbors #for nearest neighbor classifier\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier #for decision tree classifier\nfrom sklearn.naive_bayes import GaussianNB  #for naive bayes classifier\nfrom scipy import stats #for statistical info\nfrom sklearn.model_selection import train_test_split # to split the data in train and test\nfrom sklearn.model_selection import KFold # for cross validation\nfrom sklearn.grid_search import GridSearchCV  # for tuning parameters\nfrom sklearn.neighbors import KNeighborsClassifier  #for k-neighbor classifier\nfrom sklearn import metrics  # for checking the accuracy \nfrom time import time"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3b2da9b-1ebc-2888-4de8-e44ff54294b2"},"outputs":[],"source":"#load data\ndata = pd.read_csv(\"../input/data.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4241320b-cedc-5124-612e-81c5fe04e7cf"},"outputs":[],"source":"#to know the number of cases and the number of variables\ndata.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"193cc47f-f66f-4e1c-c1cc-526e94b68536"},"outputs":[],"source":"#Description of the dataset\n\n#how many cases are included in the dataset\nlength = len(data)\n#how many features are in the dataset\nfeatures = data.shape[1]-1\n\n# Number of malignant cases\nmalignant = len(data[data['diagnosis']=='M'])\n\n#Number of benign cases\nbenign = len(data[data['diagnosis']=='B'])\n\n#Rate of malignant tumors over all cases\nrate = (float(malignant)/(length))*100\n\nprint (\"There are \"+ str(len(data))+\" cases in this dataset\")\nprint (\"There are {}\".format(features)+\" features in this dataset\")\nprint (\"There are {}\".format(malignant)+\" cases diagnosed as malignant tumor\")\nprint (\"There are {}\".format(benign)+\" cases diagnosed as benign tumor\")\nprint (\"The percentage of malignant cases is: {:.4f}%\".format(rate))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0d90959-0eab-86cc-f100-1c7350a6eb73"},"outputs":[],"source":"data.diagnosis.unique()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7dde4fb-41cd-c830-e63f-53529001bdef"},"outputs":[],"source":"#drop ID because we do not need the ID number \n\ndata.drop('id',axis=1,inplace=True)\n#check that dropped\ndata.head(1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60ed9586-827c-2c9c-0068-15288ec6a27f"},"outputs":[],"source":"# Extract feature columns where everything but the diagnosis is included.\n# I am separating all the features that are helpful in determining the diagnosis\nfeatures = list(data.columns[1:30])\nprint (features)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db9cc1ed-b48b-6fca-7e6a-4e2381e98d3a"},"outputs":[],"source":"\n#Our target is predicting the diagnosis in benign or malignant, so we need\n#to extract this one as the dependent variable - the variable we will predict\ntarget = data.columns[0:1]\nprint (target)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"230841d7-a0fc-3fa9-2ad1-8e91d745f501"},"outputs":[],"source":"#Now we need to separate the data into feature data and target data\nX = data[features] #our features that we will use to predict Y\nY = data[target] #our dependent variable, the one we are trying to predict from X"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a897ab5e-8873-593d-3c41-6fd9a2d253f7"},"outputs":[],"source":"# X should have 29 variables and 569 cases\nX.shape\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf56c037-b4ef-f581-58c9-72e6aac2d7ca"},"outputs":[],"source":"# Y should have 1 variable - just the diagnosis and 569 cases\nY.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2ff4465-8140-efca-7808-e7ecb46aa80e"},"outputs":[],"source":"# Show the feature information by printing the first row\n# Show the traget information by also printing the first row\nprint (\"\\nFeature values:\")\nprint (X.head(1))\nprint (\"\\nTarget values:\")\nprint (Y.head(1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a34c4098-b342-610b-7bc3-90542f40debf"},"outputs":[],"source":"df=pd.DataFrame(data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4f4c775-a4bb-239a-66f8-5a54169b3444"},"outputs":[],"source":"#Research shows that any variables that are highly correlated\n#should be removed from further analysis. But, PCA takes care of multicollinearity, so maybe \n#I identify them which ones there are and let PCA to do its job.  \n#Just in case let's see how two highly correlated variables look like\n#using prettyplots\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1)\nfor i in range(1):\n    x=df['perimeter_mean']\n    y=df['area_worst']\n    ax.scatter(x,y, label=str(i))\n#ax.legend()\nax.set_title('Correlation of perimeter_mean and area_worst with correlation .99 or r-square= .81')\nfig.savefig('scatter.png')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d264261f-e988-8c11-04d9-68a3d3501277"},"outputs":[],"source":"#Let's visualize another set of variables that are not correlated as highly as the first ones\n#These have a correlation coefficient of .75 which means an r-squared score of approximately .49\nfig, ax = plt.subplots(1)\nfor i in range(1):\n    x=df['concavity_mean']\n    y=df['compactness_worst']\n    ax.scatter(x,y, label=str(i))\n#ax.legend()\nax.set_title('Correlation of the mean of concavity and worst compactness')\nfig.savefig('scatter.png')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b0789c5-8f6a-b308-88b4-32b0e1b0955b"},"outputs":[],"source":"def preprocess_features(X):\n    \n    # Initialize new output DataFrame\n    output = pd.DataFrame(index = X.index)\n\n    # Investigate each feature column for the data\n    for col, col_data in X.iteritems():\n        \n        # If data type is non-numeric, replace all M/B malignant/benign values with 1/0\n        if col_data.dtype == object:\n            col_data = col_data.replace(['M', 'B'], [1, 0])\n \n        # Collect the revised columns\n        output = output.join(col_data)\n    \n    return output\n\nX = preprocess_features(X)\nY = preprocess_features(Y)\nprint (\"Processed feature columns ({} total features):\\n{}\".format(len(X.columns), list(X.columns)))\nprint (\"Target columns ({} total features):\\n{}\".format(len(Y.columns), list(Y.columns)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aab72dce-15c5-9412-9319-f1de8841dccc"},"outputs":[],"source":"# import cross_validation to split the train and testing\nfrom sklearn.cross_validation import train_test_split\n# Set the number of training points\nnr_train = 300\n# Set the number of testing points\nnr_test = X.shape[0] - nr_train\n\n# Shuffle and split the dataset into the number of training and testing points above\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=nr_test, random_state=40)\n\n# Show the results of the split\nprint (\"Training set has {} samples.\".format(X_train.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test.shape[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19477643-3d0d-81fe-3cc8-1ef997a6cc3d"},"outputs":[],"source":"from sklearn.metrics import f1_score\ndef train_classifier(clf, X_train, Y_train):\n    ''' Fits a classifier to the training data. '''\n    \n    # Start the clock, train the classifier, then stop the clock\n    start = time()\n    clf.fit(X_train, Y_train)\n    end = time()\n    \n    # Print the results\n    print (\"Trained model in {:.4f} seconds\".format(end - start))\n\n    \ndef predict_labels(clf, features, target):\n    ''' Makes predictions using a fit classifier based on F1 score. '''\n    \n    # Start the clock, make predictions, then stop the clock\n    start = time()\n    Y_pred = clf.predict(features)\n    end = time()\n    \n    # Print and return results\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return f1_score(target.values, Y_pred, pos_label=1)\n\n\ndef train_predict(clf, X_train, Y_train, X_test, Y_test):\n    ''' Train and predict using a classifer based on F1 score. '''\n    \n    # Indicate the classifier and the training set size\n    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n    \n    # Train the classifier\n    train_classifier(clf, X_train, Y_train)\n    \n    # Print the results of prediction for both training and testing\n    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, Y_train)))\n    print (\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, Y_test)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5aca3ed1-8224-4274-ab77-558bb88f7483"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a864a89-0525-35dd-60bb-d5a093254b90"},"outputs":[],"source":"clf_A = KNeighborsClassifier()\nclf_B = DecisionTreeClassifier(random_state=0)\nclf_C = SVC()\nclf_D = GaussianNB()\nclf_E = RandomForestClassifier(n_estimators=10)\nclf_G = AdaBoostClassifier()\nclf_H = QuadraticDiscriminantAnalysis()\nclf_I = MLPClassifier(alpha=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7db49590-7942-98b7-5d41-2452eafeae10"},"outputs":[],"source":"X_train_100 = X_train[:100]\nY_train_100 = Y_train[:100]\n\nX_train_200 = X_train[:200]\nY_train_200 = Y_train[:200]\n\nX_train_300 = X_train[:300]\nY_train_300 = Y_train[:300]\n\nX_train_300 = X_train[:400]\nY_train_300 = Y_train[:400]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"456a1b2e-f85c-ccf1-98df-09bbcc489265"},"outputs":[],"source":"for clf in [clf_A, clf_B, clf_C, clf_D, clf_E, clf_G, clf_H, clf_I]:\n    for size in [300, 400]:\n        train_predict(clf, X_train[:size], Y_train[:size], X_test, Y_test)\n        print ('/n')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"485419cf-7ca5-e7b3-6c65-9f6f18fd8afa"},"outputs":[],"source":"#additional tuning of the best classifiers\nclf_D = GaussianNB()\nclf_G = AdaBoostClassifier(algorithm = 'SAMME')\nclf_H = QuadraticDiscriminantAnalysis(reg_param = 0.001, store_covariances=True, tol = 0.01)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b1fee94-87f3-0dea-56fc-cb0017149d67"},"outputs":[],"source":"#trying with only 300 and 400 sampling sizes\nX_train_300 = X_train[:300]\nY_train_300 = Y_train[:300]\n\nX_train_300 = X_train[:400]\nY_train_300 = Y_train[:400]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8bcb49e-29f3-37ab-ed2b-27f4ff253858"},"outputs":[],"source":"\nfor clf in [clf_D, clf_G, clf_H]:\n    for size in [300, 400]:\n        train_predict(clf, X_train[:size], Y_train[:size], X_test, Y_test)\n        print ('/n')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e505fa3c-59e1-1ee6-d681-f1567380b853"},"outputs":[],"source":"from itertools import cycle\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn import metrics\nimport pandas as pd\nfrom ggplot import *"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74820a80-f4d9-a6cf-fb6a-fb092c74026a"},"outputs":[],"source":"# ROC curve for Naive Bayes\npreds = clf_D.predict_proba(X_test)[:,1]\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\ndf = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\nggplot(df, aes(x='fpr', y='tpr')) +\\\n    geom_line() +\\\n    geom_abline(linetype='dashed')+\\\n    ggtitle (\"ROC for Naive Bayes has an area under the curve of \" + str(metrics.auc(fpr,tpr)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fb45e54-b708-25cb-b9c8-e6e25eeb8e30"},"outputs":[],"source":"# ROC curve for AdaBoost\npreds = clf_G.predict_proba(X_test)[:,1]\nfprA, tprA, _ = metrics.roc_curve(Y_test, preds)\n\ndf = pd.DataFrame(dict(fprA=fprA, tprA=tprA))\nggplot(df, aes(x='fprA', y='tprA')) +\\\n    geom_line() +\\\n    geom_abline(linetype='dashed')+\\\n    ggtitle (\"ROC for AdaBoost has an area under the curve of \" + str(metrics.auc(fprA,tprA)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd8d68d3-6c3a-b854-4307-1b3cb9fcecdc"},"outputs":[],"source":"# ROC curve for QDA\npreds = clf_H.predict_proba(X_test)[:,1]\nfprH, tprH, _ = metrics.roc_curve(Y_test, preds)\n\ndf = pd.DataFrame(dict(fprH=fprH, tprH=tprH))\nggplot(df, aes(x='fprH', y='tprH')) +\\\n    geom_line() +\\\n    geom_abline(linetype='dashed')+\\\n    ggtitle (\"ROC for QDA has an area under the curve of \" + str(metrics.auc(fprH,tprH)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a65e09a-9a0a-9dd8-6936-3857bd5d5f6f"},"outputs":[],"source":"# ROC curve for K-Neighbors\npreds = clf_A.predict_proba(X_test)[:,1]\nfprN, tprN, _ = metrics.roc_curve(Y_test, preds)\n\ndf = pd.DataFrame(dict(fprN=fprN, tprN=tprN))\nggplot(df, aes(x='fprN', y='tprN')) +\\\n    geom_line() +\\\n    geom_abline(linetype='dashed')+\\\n    ggtitle (\"ROC for K-Neighbors has an area under the curve of \" + str(metrics.auc(fprN,tprN)))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de8f4e9f-c11d-e7e1-ee0b-6f174c722386"},"outputs":[],"source":"# ROC curve for Decision trees\npreds = clf_B.predict_proba(X_test)[:,1]\nfprD, tprD, _ = metrics.roc_curve(Y_test, preds)\n\ndf = pd.DataFrame(dict(fprD=fprD, tprD=tprD))\nggplot(df, aes(x='fprD', y='tprD')) +\\\n    geom_line() +\\\n    geom_abline(linetype='dashed')+\\\n    ggtitle (\"ROC for Decision Trees has an area under the curve of \" + str(metrics.auc(fprD,tprD)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2413dabf-0642-bf9c-9944-089485f4d05f"},"outputs":[],"source":"# Set the number of training points much smaller than before\nnr_train = 50\n# Set the number of testing points\nnr_test = X.shape[0] - nr_train\n\n# Shuffle and split the dataset into the number of training and testing points above\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=nr_test, random_state=40)\n\n# Show the results of the split\nprint (\"Training set has {} samples.\".format(X_train.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test.shape[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"751d6a6e-ecc0-0803-d2bf-e4a7da8dabd1"},"outputs":[],"source":"for clf in [clf_D, clf_G, clf_H]:\n    for size in [50]:\n        train_predict(clf, X_train[:size], Y_train[:size], X_test, Y_test)\n        print ('/n')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5a9850f-9dc8-e223-3289-a725bbc73df8"},"outputs":[],"source":"Even for a small size of training set, these three perform very well with this data.  We need to remember that these dataset has no missing values at all and very few outliers. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a81c8c92-0872-706b-8aba-5c5256cdb358"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}