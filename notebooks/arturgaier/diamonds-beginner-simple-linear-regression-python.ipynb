{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Diamonds Beginner Simple Linear Regressoin with Python**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# the model I will use is Linear Regression (Method : minimum sum of the squared deviations), with R^2 as Model-Fitness. \n# Linear Regression is usefull for predicting Prices, Weahter ect.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import your relevant libraries\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df= pd.read_csv(\"../input/diamonds/diamonds.csv\", index_col= 0) #erase the index so shuffling the data later will be possible\ndf.head()\n# lets take a quick look on my dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# never hurts to have a quick overview of the datatypes which we are handling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as we can see, there are categorical feartures in this dataset, lets find out what the contain:\ndf[\"cut\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"clarity\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"color\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fortunately, the data desciption on kaggle gives us an insight of what these values acutally mean\n# cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n# clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n# color diamond colour, from J (worst) to D (best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#in this machine learning tutorial we want to make linear regressions which need arbitrary values->\n#so we make a dicionary!\ncut_dict = {\"Fair\":1, \"Good\":2, \"Very Good\":3, \"Premium\":4, \"Ideal\":5}\ncolor_dict = {'E':6, 'I':2, 'J':1, 'H':3, 'F':5, 'G':4, 'D':7}\nclarity_dict = {\"I3\":1, \"I2\": 2, \"I1\":3, \"SI2\":4, \"SI1\":5, \"VS2\":6, \"VS1\":7, \"VVS2\":8, \"VVS1\":9, \"IF\":10, \"FL\":11}\n\n#than we transfer our dictionaries into our dataframe\ndf[\"cut\"] = df[\"cut\"].map(cut_dict)\ndf[\"color\"] = df[\"color\"].map(color_dict)\ndf[\"clarity\"] = df[\"clarity\"].map(clarity_dict)\n#ofcourse you must keep in mind that this will not be 100% correct as I have no real idea that for example..\n#.. \"Good\" is twice the value of \"Fair\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#did it work?\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the columns x,y and z are also described on kaggle\ndf.rename(columns = {\"depth\":\"DepthPc\",\"x\":\"lenght\", \"y\":\"widht\", \"z\":\"depth\"}, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr(method = \"pearson\")[\"price\"].sort_values(ascending = False)\n# lets look out for highly correlated features to our target \"price\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we see that we have a strong linear relationship between the target and the following 4 features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize = (12,8))\nsns.heatmap(df.corr(method = \"pearson\"), cmap=\"Greens\")\n# a heatmap is a brilliant figure to visulize multicollinearity, lets keep carat, lenght, width and depth in mind","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in this first run we won´t eliminate these highly correlated features \n# I make a full run with all features\ny = df[\"price\"]\nx = df.drop(columns = {\"price\"})\n\nimport sklearn \ndf = sklearn.utils.shuffle(df)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\nprint(model.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model-Fitness of 0.9069 is pretty hight, but let´s not forget the high correlation bewteen our four features from above\n# for this, we eliminate the features \"lenght\", widht\", \"depth\"\ny = df[\"price\"]\nx = df.drop(columns = {\"price\", \"lenght\", \"widht\", \"depth\"})\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\nmodel2 = LinearRegression()\nmodel2.fit(x_train, y_train)\nprint(model2.score(x_test, y_test))\n\n# Note, that R-squared will always increase as you add more features to the model, even if they are unrelated to the target\n# Selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the accuracy above is pretty good! So why did I eliminate these features?\n# with the dimensions of each diamond by the features \"lenght\",\"widht\" and \"depth\"..\n#..and under the asumption that the density of diamonds is stabil..\n# I asume that \"lenght\",\"widht\" and \"depth\" has little more information as if we alrady use \"carat\" ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if we want a more precise model we can try an eliminate outliers in our target variable (another aproach might be normalizing with functions which are resilient to those)\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (12,8))\nax = sns.boxplot(y = train[\"SalePrice\"], data = train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# approach to eleminate outliers:\nupper_quartile = np.percentile(df[\"price\"], 75)\nlower_quartile = np.percentile(df[\"price\"], 25)\niqr = upper_quartile - lower_quartile\nupper_whisker = (upper_quartile + 1.5*iqr)\nlower_whisker = (lower_quartile - 1.5*iqr)\ndf = df[(lower_whisker < df[\"price\"]) & (df[\"price\"] < upper_whisker)]\ndf.info()\n\ny = df[\"price\"]\nx = df.drop(columns = {\"price\", \"lenght\", \"widht\", \"depth\"})\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle=False)\nmodel3 = LinearRegression()\nmodel3.fit(x_train, y_train)\nprint(model3.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a slightly lesser coefficient of determination is a bit suprising\n# Maybe I destroyed valuable information with erasing outliers\nprice_predict = model3.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets compare our predictions for price with the actual value\n# the overall result is quite good BUT..\n# .. a closer look reveals that our model sometimes predicted a negative price which is of course not reasonable\nresult = pd.DataFrame({\"prediction\": price_predict.flatten(), \"actual_price\": y_test})\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# thanks for reading, I hope you enjoyed it\n# if you have annotations or suggestions on improving my work I would gladly hear your opinion!","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}