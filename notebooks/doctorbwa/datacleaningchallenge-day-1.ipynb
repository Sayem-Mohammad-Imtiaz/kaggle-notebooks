{"cells":[{"metadata":{"_cell_guid":"b91a74ba-85f4-486e-b5f9-d0898f0626bf","_uuid":"6ac53f18b4f4ec0fc44348cedb5d1c319fa127c0"},"cell_type":"markdown","source":"### All days of the challange:\n\n* [Day 1: Handling missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values)\n* [Day 2: Scaling and normalization](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data)\n* [Day 3: Parsing dates](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/)\n* [Day 4: Character encodings](https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/)\n* [Day 5: Inconsistent Data Entry](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/)\n___\nWelcome to day 1 of the 5-Day Data Challenge! Today, we're going to be looking at how to deal with missing values. To get started, click the blue \"Fork Notebook\" button in the upper, right hand corner. This will create a private copy of this notebook that you can edit and play with. Once you're finished with the exercises, you can choose to make your notebook public to share with others. :)\n\n> **Your turn!** As we work through this notebook, you'll see some notebook cells (a block of either code or text) that has \"Your Turn!\" written in it. These are exercises for you to do to help cement your understanding of the concepts we're talking about. Once you've written the code to answer a specific question, you can run the code by clicking inside the cell (box with code in it) with the code you want to run and then hit CTRL + ENTER (CMD + ENTER on a Mac). You can also click in a cell and then click on the right \"play\" arrow to the left of the code. If you want to run all the code in your notebook, you can use the double, \"fast forward\" arrows at the bottom of the notebook editor.\n\nHere's what we're going to do today:\n\n* [Take a first look at the data](#Take-a-first-look-at-the-data)\n* [See how many missing data points we have](#See-how-many-missing-data-points-we-have)\n* [Figure out why the data is missing](#Figure-out-why-the-data-is-missing)\n* [Drop missing values](#Drop-missing-values)\n* [Filling in missing values](#Filling-in-missing-values)\n\nLet's get started!"},{"metadata":{"_cell_guid":"5cd5061f-ae30-4837-a53b-690ffd5c5830","_uuid":"9d82bf13584b8e682962fbb96131f2447d741679"},"cell_type":"markdown","source":"# Take a first look at the data\n________\n\nThe first thing we'll need to do is load in the libraries and datasets we'll be using. For today, I'll be using a dataset of events that occured in American Football games for demonstration, and you'll be using a dataset of building permits issued in San Francisco.\n\n> **Important!** Make sure you run this cell yourself or the rest of your code won't work!"},{"metadata":{"trusted":true,"_uuid":"19ea48874bf8c443d1d961b89eda978516dc30f5"},"cell_type":"code","source":"!ls -ltr ../input","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"135a7804-b5f5-40aa-8657-4a15774e3666","_uuid":"835cbe0834b935fb0fd40c75b9c39454836f4d5f","trusted":true},"cell_type":"code","source":"# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# read in all our data\nnfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")\nsf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")\n\n# set seed for reproducibility\nnp.random.seed(0) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09b58d03-d34d-497a-b298-12a0ae962e3d","_uuid":"53c84bf86149ac41b237633a1a79d6130d6a2cd4"},"cell_type":"markdown","source":"The first thing I do when I get a new dataset is take a look at some of it. This lets me see that it all read in correctly and get an idea of what's going on with the data. In this case, I'm looking to see if I see any missing values, which will be reprsented with `NaN` or `None`."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# look at a few rows of the nfl_data file. I can see a handful of missing data already!\nnfl_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"604ac3a4-b1d9-4264-b312-4bbeecdeec00","_uuid":"03ce3b4afe87d98f777172c2c7be066a66a0b237"},"cell_type":"markdown","source":"Yep, it looks like there's some missing values. What about in the sf_permits dataset?"},{"metadata":{"_cell_guid":"8dca377c-95be-40ec-87dc-61a8fca750e2","_uuid":"e389495bb2e5d27ab632d5f3648ca1f912c94706","trusted":true},"cell_type":"code","source":"# your turn! Look at a couple of rows from the sf_permits dataset. Do you notice any missing data?\n\n# your code goes here :)\nsf_permits.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33656c2b-a74e-4b76-9af2-d7ecd518577b","_uuid":"400b025f618cc76a39fec2537193f28ba1e49168"},"cell_type":"markdown","source":"# See how many missing data points we have\n___\n\nOk, now we know that we do have some missing values. Let's see how many we have in each column. "},{"metadata":{"_cell_guid":"a69ac02d-197b-487b-a38f-2f853d208eed","_uuid":"6dc0e32180c4a3bba003e7886faf126d93affadf","trusted":true},"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = nfl_data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6659a104ba22d4acdfda19c25f63cd5c676a399"},"cell_type":"code","source":"missing_values_count.sort_values(ascending=False)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84455c7e-6c63-4e08-a7b4-7520a61072f9","_uuid":"054ba8782a7b0555336eddb90c985fb638beac4d"},"cell_type":"markdown","source":"That seems like a lot! It might be helpful to see what percentage of the values in our dataset were missing to give us a better sense of the scale of this problem:"},{"metadata":{"_uuid":"c79b0c03209432cd8b953d0b23705cb5bed22a7a"},"cell_type":"markdown","source":"\n**drbwa:** Arrays in numpy are used to represent matrices. As such, each row is expected to have the same length. Otherwise, the numpy array degenerates to an array of lists.\n\nRunning np.product(numpyarray.shape) gives you the total number of cells."},{"metadata":{"trusted":true,"_uuid":"c305337ae5d043390b299a5d17335a2bff09b131"},"cell_type":"code","source":"a1 = np.array([2, 4, 6])\nprint(f\"the product of all array elements: {np.product(a1)}\")\na2 = np.array([[1, 2, 3], [4, 5, 6]])\nprint(f\"shape of array a2: {a2.shape}\")\nprint(f\"the product of the diemensions of the 2-d array a2: {np.product(a2.shape)}\")\nprint(\"this is the total number of elements or cells in this 2-d array\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb77dd56-192e-48be-8181-2082985dd5a2","_uuid":"d6e65ba197893f29d9dce0b0cd1c75017b60db09","trusted":true},"cell_type":"code","source":"# how many total missing values do we have?\ntotal_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\nnfl_missing_percent = (total_missing/total_cells) * 100\nprint(f\"nfl_missing_percent: {nfl_missing_percent:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31daa324-9215-4930-985c-01dee717b6b8","_uuid":"3331fa42efa16f3db2e8e196411f351c5f8309f5"},"cell_type":"markdown","source":"Wow, almost a quarter of the cells in this dataset are empty! In the next step, we're going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them."},{"metadata":{"_cell_guid":"f20a9474-41ee-4ecd-a2f4-1ab147fc8655","_uuid":"64487760aa1afaaa8b8a4d1f95206773759db101","trusted":true},"cell_type":"code","source":"# your turn! Find out what percent of the sf_permits dataset is missing\n\n# get the number of missing data points per column\nsf_missing_values_count = sf_permits.isnull().sum()\n#sf_missing_values_count\n\n# look at the # of missing points in the first ten columns\nsf_missing_values_count[0:10]\n\n# total number of cells in sf permits dataset\nsf_total_cells = np.product(sf_permits.shape)\n\n# total number of cells with missing values in sf permits dataset\nsf_total_missing = sf_missing_values_count.sum()\n\n# percent of data missing in sf permits dataset\nsf_missing_percent = (sf_total_missing / sf_total_cells) * 100\nprint(f\"sf_missing_percent: {sf_missing_percent:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62b9f021-5b80-43e2-bf60-8e0d5e22d572","_uuid":"032a618abb98a28e60ab84376cf21402178f995d"},"cell_type":"markdown","source":"# Figure out why the data is missing\n____\n \nThis is the point at which we get into the part of data science that I like to call \"data intution\", by which I mean \"really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis\". It can be a frustrating part of data science, especially if you're newer to the field and don't have a lot of experience. For dealing with missing values, you'll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:\n\n> **Is this value missing becuase it wasn't recorded or becuase it dosen't exist?**\n\nIf a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called \"imputation\" and we'll learn how to do it next! :)\n\nLet's work through an example. Looking at the number of missing values in the nfl_data dataframe, I notice that the column `TimesSec` has a lot of missing values in it: "},{"metadata":{"_cell_guid":"77739e82-8d32-4374-84bf-a924b6065168","_uuid":"b65aea6046964806e44422c057bce8bd7f8e59d5","trusted":true},"cell_type":"code","source":"# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b17f4c9-dcab-4857-82f9-a2534e804c91","_uuid":"5cff158285ab37a89b80dcc35d5c690cdb42d3a4"},"cell_type":"markdown","source":"By looking at [the documentation](https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016), I can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because they were not recorded, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.\n\nOn the other hand, there are other fields, like `PenalizedTeam` that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn't make sense to say *which* team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like \"neither\" and use that to replace the NA's.\n\n> **Tip:** This is a great place to read over the dataset documentation if you haven't already! If you're working with a dataset that you've gotten from another person, you can also try reaching out to them to get more information.\n\nIf you're doing very careful data analysis, this is the point at which you'd look at each column individually to figure out the best strategy for filling those missing values. For the rest of this notebook, we'll cover some \"quick and dirty\" techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data.\n\n## Your turn!\n\n* Look at the columns `Street Number Suffix` and `Zipcode` from the `sf_permits` datasets. Both of these contain missing values. Which, if either, of these are missing because they don't exist? Which, if either, are missing because they weren't recorded?"},{"metadata":{"trusted":true,"_uuid":"31bc52c2a57be3207136cdef564cacabb25f6155"},"cell_type":"code","source":"sf_missing_values_count[['Street Number Suffix', 'Zipcode']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86c05501c0069f543f319104e185490e313015d2"},"cell_type":"markdown","source":"**drbwa:** Every street has a zip code (I think). So if we find an entry without a zip code, we can assume that this is due to a data entry / capture problem. It would make sense to try and ammend the missing information (e.g., look up zip code for a given address).\n\nOn the other hand, not every street number does have a suffix (e.g. '44A', '44B'). If we encounter an entry without a Street Number Suffix, it may be due to this address not having a suffix."},{"metadata":{"_cell_guid":"ea022b62-6419-47e7-973e-c3e707e2795f","_uuid":"3f72f46f2464c7cd12f5eb2a752746ce1cd0b5a7"},"cell_type":"markdown","source":"# Drop missing values\n___\n\nIf you're in a hurry or don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: I don't generally recommend this approch for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)  \n\nIf you're sure you want to drop rows with missing values, pandas does have a handy function, `dropna()` to help you do this. Let's try it out on our NFL dataset!"},{"metadata":{"_cell_guid":"ad0ac9a2-2854-4bb7-8886-8eee7fad8756","_uuid":"ad8ef7825ba9bce3472a47d7c5242a4522f14065","trusted":true},"cell_type":"code","source":"# remove all the rows that contain a missing value\nnfl_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e0545655-3d37-448b-ae56-2c7707cd805d","_uuid":"33eb849e076d2a4d0c409f58d78b5f303879b1b3"},"cell_type":"markdown","source":"Oh dear, it looks like that's removed all our data! 😱 This is because every row in our dataset had at least one missing value. We might have better luck removing all the *columns* that have at least one missing value instead."},{"metadata":{"_cell_guid":"97709ad4-f7b8-4cd0-8911-56e14db904ae","_uuid":"87c569672854fe23e1ee9376ef3115ba4712cbf5","trusted":true},"cell_type":"code","source":"# remove all columns with at least one missing value\ncolumns_with_na_dropped = nfl_data.dropna(axis=1)\ncolumns_with_na_dropped.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e51b19b-c44d-4487-8417-725d2b911739","_uuid":"e60a092d2799851aa725eadf28b197022a6b127f","trusted":true},"cell_type":"code","source":"# just how much data did we lose?\nprint(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f417c614-f77f-45eb-b16b-fdc0be936502","_uuid":"bac84fee4ca849e54839c716c43dddfbb559954b"},"cell_type":"markdown","source":"We've lost quite a bit of data, but at this point we have successfully removed all the `NaN`'s from our data. "},{"metadata":{"_cell_guid":"0fe94654-7dad-4e8d-bbbb-e65e2bb2f767","_uuid":"8207912f74712835283f7e1b30dad0471ee2e1fc","trusted":true},"cell_type":"code","source":"# Your turn! Try removing all the rows from the sf_permits dataset that contain missing values. How many are left?\nsf_permits_dropped_rows_with_nans = sf_permits.dropna()\nsf_permits_dropped_rows_with_nans","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ec643e1-abba-4683-b794-a1924e657501","_uuid":"f804c0448b18b6d411ddf8452d15abba8292fffa","trusted":true},"cell_type":"code","source":"# Now try removing all the columns with empty values. Now how much of your data is left?\nsf_permits_dropped_cols_with_nans = sf_permits.dropna(axis=1)\nprint(f\"Number of columns in original dataset: {sf_permits.shape[1]}\")\nprint(f\"Number of columns after dropping colums with nas: {sf_permits_dropped_cols_with_nans.shape[1]}\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1dbe153d-7b30-4ad8-80ad-a4c7fb53928e","_uuid":"eb1ef8d47d9ebed77c3d21eca24708708ed4d45f"},"cell_type":"markdown","source":"# Filling in missing values automatically\n_____\n\nAnother option is to try and fill in the missing values. For this next bit, I'm getting a small sub-section of the NFL data so that it will print well."},{"metadata":{"_cell_guid":"76fd83fb-a6d9-4c03-8c94-a111ee529881","_uuid":"e0944282c73a63513d5345689ddd6a9da0fc8547","trusted":true},"cell_type":"code","source":"# get a small subset of the NFL dataset\nsubset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\nsubset_nfl_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"527c8703-4b29-459d-af7d-5505da36016b","_uuid":"f8cfe916904af3265d8ecc4f791f9f62e34ff458"},"cell_type":"markdown","source":"We can use the Panda's fillna() function to fill in missing values in a dataframe for us. One option we have is to specify what we want the `NaN` values to be replaced with. Here, I'm saying that I would like to replace all the `NaN` values with 0."},{"metadata":{"_cell_guid":"c01ed989-8901-43c8-afa3-6ca36605dfb5","_uuid":"77eac530ce398b8c13eb7886f86bce48fd997f34","trusted":true},"cell_type":"code","source":"# replace all NA's with 0\nsubset_nfl_data.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1103b725-c823-4f40-9bda-e97997856339","_uuid":"bec603202c6bfaae7a49b4a4042f37019ad1d801"},"cell_type":"markdown","source":"I could also be a bit more savvy and replace missing values with whatever value comes directly after it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)"},{"metadata":{"_cell_guid":"90ddac9b-ee20-492e-b437-0519c97ca317","_uuid":"afba99aa7897539e9a0af77dce03daab94d0ca68","trusted":true},"cell_type":"code","source":"# replace all NA's the value that comes directly after it in the same column, \n# then replace all the reamining na's with 0\nsubset_nfl_data.fillna(method = 'bfill', axis=0).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"980e5d67-7e9c-41a3-b17e-51d87e9da9cf","_uuid":"1f8ac8b52f2933612e315f06a53185e164e6c5bc"},"cell_type":"markdown","source":"Filling in missing values is also known as \"imputation\", and you can find more exercises on it [in this lesson, also linked under the \"More practice!\" section](https://www.kaggle.com/dansbecker/handling-missing-values). First, however, why don't you try replacing some of the missing values in the sf_permit dataset?"},{"metadata":{"_cell_guid":"da426397-7e17-40ce-a0d4-ca6d39e47498","_uuid":"f7d403c19eaf31ee0a4e04b9e1119eda96a9f95c","trusted":true},"cell_type":"code","source":"# Your turn! Try replacing all the NaN's in the sf_permits data with the one that\n# comes directly after it and then replacing any remaining NaN's with 0\nsf_permits.fillna(method=\"bfill\", axis=0).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4f37fce-4d08-409e-bbbd-6a26c3bbc6ee","_uuid":"52b0af56e3c77db96056e9acd785f8f435f7caf5"},"cell_type":"markdown","source":"And that's it for today! If you have any questions, be sure to post them in the comments below or [on the forums](https://www.kaggle.com/questions-and-answers). \n\nRemember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the \"Commit & Run\" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also let's you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the \"Commit & Run\" button) and setting the \"Visibility\" dropdown to \"Public\".\n\n# More practice!\n___\n\nIf you're looking for more practice handling missing values, check out these extra-credit\\* exercises:\n\n* [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values): In this notebook Dan shows you several approaches to imputing missing data using scikit-learn's imputer. \n* Look back at the `Zipcode` column in the `sf_permits` dataset, which has some missing values. How would you go about figuring out what the actual zipcode of each address should be? (You might try using another dataset. You can search for datasets about San Fransisco on the [Datasets listing](https://www.kaggle.com/datasets).) \n\n\\* no actual credit is given for completing the challenge, you just learn how to clean data real good :P"},{"metadata":{"_uuid":"c701cbec3630e748e0a96fa9359b7c09fefc9a09"},"cell_type":"markdown","source":"# My (drbwa's) attempt at fixing some of the missing zip codes\nI can think of two approaches to reduce the number of null zip codes in the `sf_permits` dataset.\n\nUse a Web service API to look up zip codes\nThere are Web services that allow you to look up zip codes according to a street address or latitude and longitude coordinates. This should result in fairly accurate data and is simple to implement.\n\nUse another dataset that contains (some of) the missing zip codes\nUse another dataset that contains (most of) the missing data. You need to find some way to match on relevant columns in both datasets. It is likely that performing the matching correctly will require some wizardry due to different formats being used, etc. The accuracy depends not only on the quality of supplemental dataset, but also on the accuracy with which you can match relevant rows in both datasets."},{"metadata":{"_uuid":"a8c37d97c41b058bb7b65c79ee4ebae78a48c920"},"cell_type":"markdown","source":"## Idea #1: Use a Web service API to look up zip codes\nThe idea is to use a Web service that can return a zip code given a partial address (state, city, street, and so on). As it turns out, there are plenty Web services that sell programmatic access to address data including zip codes. And all of them charge for lookups beyond a small rate of lookups per day.\n\nNeed a plan B."},{"metadata":{"_uuid":"b1fa6e6ceee311971f29a100410cf167c72095cc"},"cell_type":"markdown","source":"## Idea #2: Use another dataset to find missing zip codes\nPlan B is to use another dataset to find (most of) the missing zip codes.\n\nI found a dataset on data.gov about San Francisco Fire Department (SFFD) service calls. It looks like this would require a lot of work and still leaves plenty of opportunities to introduce inaccuracies that are difficult to quantify and control. First, the sffd_service_calls.address attribute describes the mid-block of a reported incident. It may thus give a zip code that is different from the one that would match the street number of the same street in sf_permits. Second, if the way I go about matching is too permissive, we might match the wrong streets. Third, it does not help that I do not really know San Francisco and thus have no intuition that would allow me to come up with more promising approaches or be able to spot errors in the results easily.\n\nAnother dataset that looks more promising is the latest address data for San Francisco from openaddresses.io: [San Francisco](https://s3.amazonaws.com/data.openaddresses.io/runs/506567/us/ca/san_francisco.zip). Need to decide whether it makes more sense to match on longitude / latitude or on street name and number. For that, we check which of the two is more complete."},{"metadata":{"trusted":true,"_uuid":"b31cd11e7f2dc1b575a76ca0eeff0edb0cdd447c"},"cell_type":"code","source":"oa_sf_df = pd.read_csv('../input/openaddress-san-francisco/san_francisco/us/ca/san_francisco.csv')\noa_sf_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f64d59680d170c446913af736afdd41951029b83"},"cell_type":"markdown","source":"Let's check how many longitude / latitude and street number / name columns have missing values in both datasets"},{"metadata":{"trusted":true,"_uuid":"f5f1fe49849a5843a1ebf23f4b35b1e64e770ab3"},"cell_type":"code","source":"# count of null values by column\noa_missing_values_count = oa_sf_df.isnull().sum()\npermits_missing_values_count = sf_permits.isnull().sum()\n\nprint(f\"OpenAddress number of rows with null longitude: {oa_missing_values_count['LON']}\")\nprint(f\"OpenAddress number of rows with null latitude: {oa_missing_values_count['LAT']}\")\nprint(f\"OpenAddress number of rows with null street number: {oa_missing_values_count['NUMBER']}\")\nprint(f\"OpenAddress number of rows with null street name: {oa_missing_values_count['STREET']}\")\nprint(\"*\" * 3)\n# Location contains a tuple longitude and latitude values\nprint(f\"sfpermits number of rows with null location: {permits_missing_values_count['Location']}\")\nprint(f\"sfpermits number of rows with null street number: {permits_missing_values_count['Street Number']}\")\nprint(f\"sfpermits number of rows with null street name: {permits_missing_values_count['Street Name']}\")\nprint(f\"sfpermits number of rows with null street name suffix: {permits_missing_values_count['Street Suffix']}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17bc5893e71c1dd0d5c55dd52abfbc9994bc1a3a"},"cell_type":"markdown","source":"It appears that in this case we have more complete data for street names and numbers than for coordinates.\n\nWe note that the street number suffix in the OpenAddress dataset is part of the street number. For example:"},{"metadata":{"trusted":true,"_uuid":"e05f2117e85ce9a03456ed5261bcc46afbf78d0d"},"cell_type":"code","source":"oa_sf_df[oa_sf_df[\"NUMBER\"].str.contains('A')].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a64aa68ae75c74c423ab22171bcb3182cfb61852"},"cell_type":"markdown","source":"A little experiment to figure out how I might approach 'merging' the missing data from one data frame into another one."},{"metadata":{"trusted":true,"_uuid":"a25c33d68375b485218355260c98309d21e4fe2a"},"cell_type":"code","source":"streets = pd.Series(['BUSH ST', 'BUSH ST', 'SUTTER ST', 'PACIFIC AVE'])\nnumbers = pd.Series(['100', '200', '60', '80'])\nzipcodes = pd.Series(['4339320', np.NaN, np.NaN, np.NaN])\na = {'Street': streets, 'Number': numbers, 'Zipcode': zipcodes}\na_df = pd.DataFrame.from_dict(a)\na_df['Street-Number'] = a_df['Street'] + '-' + a_df['Number']\n\nstreets2 = pd.Series(['CLAY ST', 'BUSH ST', 'BERRY ST', 'SUTTER ST', 'PACIFIC AVE'])\nnumbers2 = pd.Series(['20', '100', '40', '60', '80'])\nzipcodes2 = pd.Series(['40549', '4339320', '40545', '60213', '12345'])\nirrelevant = pd.Series(['A', 'B', 'C', 'D', 'E'])\nb = {'Street': streets2, 'Number': numbers2, 'Zipcode': zipcodes2, 'Superfluous': irrelevant}\nb_df = pd.DataFrame.from_dict(b)\nb_df['Street-Number'] = b_df['Street'] + '-' + b_df['Number']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6137b45dc0aabeae06d2f4a498e5b708fe637d0b"},"cell_type":"code","source":"print(a_df)\nprint(\"\\n\")\nprint(b_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a3056d2f1c52d4bfbe3b5f2f740f5c2de828a7c"},"cell_type":"code","source":"a_df['Zipcode'] = a_df['Zipcode'].fillna(a_df['Street-Number'].map(b_df.set_index('Street-Number')['Zipcode']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e4c3939254b1ad6dfbf3d8d84e9c6eeb1b9d182"},"cell_type":"code","source":"print(a_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d2ce422144853690460043e1e45345415ea06f4"},"cell_type":"markdown","source":"This approach seems to do the trick. Now, let's apply this to the actual datasets.\n\nLet us examine step-by-step how our approach of filling the missing zipcode values in one DataFrame with the values from another DataFrame works.\n\nFirst, we note that the formats of street names in the permits and in OA data frames do not match. The former has separate columns for street names and street suffix (e.g., St, Av). The latter combines these into one column whose values are in uppercase and uses 'AVE' instead 'Av' to denote avenues. In addition, the street number suffix is a separate column in sf_permits, but it is simply part of the NUMBER column in the OA data frame. We will deal with this last difference later.\n\nFor now, we add a STREET column to the sf_permits dataframe that is a concatenation of its Street Name and Street Suffix columns, converts all characters to uppercase and replaces occurrences of 'Av' with 'AVE'."},{"metadata":{"trusted":true,"_uuid":"3e2d20278322ce607cf683dcb387747bd23a5bfe"},"cell_type":"code","source":"# Let's first load the sf_permits DF from scratch to overwrite any changes\nsf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77fed40a45dd52570155d10b560f2ee9c525949c"},"cell_type":"code","source":"# Number and proportion of missing Zipcode values before...\n# get the number of missing data points in the Zipcode column of the sf_permits data frame\nsf_missing_zipcodes_count_before = sf_permits['Zipcode'].isnull().sum()\n# total number of Zipcode rows\nsf_total_zipcodes = len(sf_permits['Zipcode'])\n# percent of missing zipcodes data in sf permits dataset\nsf_missing_zipcodes_percent_before = (sf_missing_zipcodes_count_before / sf_total_zipcodes) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8948ca7f40318ae3c47b416b0bbd1ffd57c8fe3e"},"cell_type":"code","source":"# Add the new STREET column to the sf_permits data frame as explained above.\nsf_permits['STREET'] = (sf_permits['Street Name'] + ' ' + sf_permits['Street Suffix']).str.upper().str.replace('AV', 'AVE')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c127de8ebfc96ea84704e6ed846b1b09788f81bf"},"cell_type":"markdown","source":"Second, we create a new data frame based on the oa_address data frame that contains only the columns we are interested in for this problem."},{"metadata":{"trusted":true,"_uuid":"08da5539562a47bee8c15fc5cd935bb47c423b90"},"cell_type":"code","source":"oa_update_df = pd.DataFrame()\noa_update_df[['STREET', 'NUMBER', 'POSTCODE']] = oa_sf_df[['STREET', 'NUMBER', 'POSTCODE']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"004358033633ec46de68c5df63aefe02c2756727"},"cell_type":"markdown","source":"Third, we create a column called STREET-NUMBER in both data frames that is a concatentation of the street name and number separated by a hyphen. In the sf_permits data frame, we append any non-NaN values from its Street Number Suffix column. In the oa_update_df data frame, the number suffix is already part of its NUMBER column.\n\nI will explain shortly how we make use of this column to match rows in both data frames as needed to get the missing zip codes, but basically I am going to use the STREET-NUMBER column as a temporary index. As there are likely to be duplicate entries in this column (i.e. same street name and number) and as all of those should have the same postcode, we only keep unique values around (for oa_update_df which will sport the temporary index)."},{"metadata":{"trusted":true,"_uuid":"c4e159ac051dc74299997ab94d4bca6189f82d81"},"cell_type":"code","source":"sf_permits['STREET-NUMBER'] = sf_permits['STREET'] + '-' + sf_permits['Street Number'].astype(str) + sf_permits['Street Number Suffix'].str.upper().fillna('')\noa_update_df['STREET-NUMBER'] = oa_update_df['STREET'] + '-' + oa_update_df['NUMBER']\noa_update_df.drop_duplicates('STREET-NUMBER', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d77e8deff72a22df860730de474dd11a186498d0"},"cell_type":"markdown","source":"Fourth, it is time for some magic to get the missing values from the oa_update data frame to update the correct rows in the sf_permits data frame. This involves a couple of Pandas methods."},{"metadata":{"_uuid":"c031c7fbf51a390b27ad64af18ef928f76b48232"},"cell_type":"markdown","source":"[Series.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html) takes either a value (e.g., a scalar such as the number 0) or a dict or Series or DataFrame of values to use to fill the missing values in the Series upon which it is invoked. We are going to make use of this to fill the missing zip code values in the `sf_permits` data frame. However, there is a slight complication as `fillna` will by default map values based on matching indices. The indices in our two data frames do not really match in any meaningful way."},{"metadata":{"_uuid":"0d69416120cb9187ef76d310f590e41153b96e0a"},"cell_type":"markdown","source":"In order to overcome this issue, we will make use of [Series.map](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html). \n\nThe `map` function creates a mapping from the index values of the first Series onto the values of the second Series and it does so by matching the values of the first Series to the index values of the second Series. For example:"},{"metadata":{"trusted":true,"_uuid":"550dc3b13cf813d209e95be56a8624d02207381f"},"cell_type":"code","source":"x = pd.Series([1,2,3], index=['one', 'two', 'three'])\ny = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])\nx.map(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd9fdd3caf2e00386000f933bb98ed4b521a43d7"},"cell_type":"markdown","source":"If one or more of the values in x do not match the index in y, then the map method does not find a corresponding mapping:"},{"metadata":{"trusted":true,"_uuid":"27d10d547ca29d6c84aa9054f4c4e555f52e4c72"},"cell_type":"code","source":"x = pd.Series([0,2,3], index=['one', 'two', 'three'])\ny = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])\nx.map(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83ed2355e3beaa5f7b192c5348c9d885b02616f5"},"cell_type":"markdown","source":"Another example to illustrate how map works:"},{"metadata":{"trusted":true,"_uuid":"a60ee2b6ea29bd6281a2a80cfc495f3acdcbdc25"},"cell_type":"code","source":"x = pd.Series([3,2,1], index=['one', 'two', 'three'])\ny = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])\nx.map(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"722647c32ad5be2c47313000fe451f8fc1c98745"},"cell_type":"markdown","source":"We want to fill the NaNs in the `Zipcode` column in `sf_permits` with the values from the `POSTCODE` column in the `oa_update_df` data frame. We do so by making use of two tricks. The first trick is to use the `map` method to supply values to `fillna`. How do we parameterise `map` to give us the right `Zipcode` values? The second trick consists of asking `map` to match the `sf_permits.Street-Number` column values to a temporary index we set on `oa_update_df` consisting of the `Street-Number` values. And for each row that matches (value from series 1 matches index values of series 2), we want `map` to use the `oa_update_df.POSTCODE` value. That is, map is going to match rows from both data frames on `STREET-NUMBER` (column to temporary index) and use the corresponding `POSTCODE` values from matching rows in `oa_update_df` to set the `Zipcode` column in `sf_permits`.\n\nThere is another way to explain this. We remind ourselves that the `STREET-NUMBER` columns in both data frames are a concatenation of street names, numbers and suffixes. So what we are doing here is to ask Pandas to match the street names, numbers and suffixes in both data frames and use the postcode values from the second data frame for matching rows.\n\nHere is what all of this logic looks like in code. Magic."},{"metadata":{"trusted":true,"_uuid":"f9414891a08f327f1db593c2c429fc1c204edbed"},"cell_type":"code","source":"sf_permits['Zipcode'] = sf_permits['Zipcode'].fillna(sf_permits['STREET-NUMBER'].map(oa_update_df.set_index('STREET-NUMBER')['POSTCODE']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77365369468adb035115c0797b61102c0ec97240"},"cell_type":"markdown","source":"Next, let's check to see, if a street address that had a NaN value before, contains a zip code after this operation."},{"metadata":{"trusted":true,"_uuid":"ff3e38cb02e6027a93e115494666bf802e3e1a31"},"cell_type":"code","source":"sf_permits[(sf_permits['Street Name'] == 'Washington') & (sf_permits['Street Number'] == 3191)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47871ba74aa8a06379cbc426fcd111a81610ec64"},"cell_type":"markdown","source":"Finally, let us compare the before and after statistics of how many zip codes are missing."},{"metadata":{"trusted":true,"_uuid":"44604459f45911aab611a58261d5657573dc5e5e"},"cell_type":"code","source":"# Number and proportion of missing Zipcode values after...\n# get the number of missing data points in the Zipcode column of the sf_permits data frame\nsf_missing_zipcodes_count_after = sf_permits['Zipcode'].isnull().sum()\n# percent of missing zipcodes data in sf permits dataset\nsf_missing_zipcodes_percent_after = (sf_missing_zipcodes_count_after / sf_total_zipcodes) * 100\n\nprint(\"BEFORE\")\nprint(f\"Zipcode rows: {len(sf_permits.Zipcode)}\")\nprint(f\"Missing zipcodes count: {sf_missing_zipcodes_count_before}\")\nprint(f\"Missing zipcodes %: {sf_missing_zipcodes_percent_before:.2f}%\")\nprint(\"*\" * 3)\nprint(\"AFTER\")\nprint(f\"Zipcode rows: {len(sf_permits.Zipcode)}\")\nprint(f\"Missing zipcodes count: {sf_missing_zipcodes_count_after}\")\nprint(f\"Missing zipcodes %: {sf_missing_zipcodes_percent_after:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcdf7a9f286572e4964900166762a7d92bd068b1"},"cell_type":"markdown","source":"We would need to run some additional tests to ensure that we did not introduce too many errors and that we end up with high quality, useful data, but the situation of missing values looks much improved."},{"metadata":{"_uuid":"80c9591a108d551ee6becaa9c4bf96d098eb3282"},"cell_type":"markdown","source":"*The End*"},{"metadata":{"trusted":true,"_uuid":"1f6fee70e3c65e70c02ca190d8739f916a8feacb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}