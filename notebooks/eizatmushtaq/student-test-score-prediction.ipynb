{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Introduction***\n\nWith the advent of technology and sophistication of database management resources, recently there has been interest in educational databases containing a variety of valuable information which could help less sucessful students improve their academic performance and  help academic institutions optimize their resources to improve overall wellbeing of their students. The objective of the task is to predict post test scores of students using given set of features.","metadata":{"execution":{"iopub.status.busy":"2021-09-10T23:04:27.393348Z","iopub.execute_input":"2021-09-10T23:04:27.393784Z","iopub.status.idle":"2021-09-10T23:04:27.400427Z","shell.execute_reply.started":"2021-09-10T23:04:27.393746Z","shell.execute_reply":"2021-09-10T23:04:27.39881Z"}}},{"cell_type":"markdown","source":"### Importing libraries and dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\npd.options.plotting.backend = \"plotly\"\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nfrom plotly.figure_factory import create_distplot\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,explained_variance_score\nfrom xgboost import XGBRegressor\n\n#tensorflow packages for bayesian NN's\n#from tensorflow import keras\n#from tensorflow.keras import layers\n#import tensorflow_probability as tfp\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#read the dataset\ndata = pd.read_csv(\"/kaggle/input/predict-test-scores-of-students/test_scores.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-12T03:21:45.908007Z","iopub.execute_input":"2021-09-12T03:21:45.908578Z","iopub.status.idle":"2021-09-12T03:21:45.935247Z","shell.execute_reply.started":"2021-09-12T03:21:45.908542Z","shell.execute_reply":"2021-09-12T03:21:45.933689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print shape of data\nprint(\"The data contains\",data.shape[0],\"rows and\",data.shape[1], \"columns\\n\")\n#print head of data\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-12T02:40:55.293563Z","iopub.execute_input":"2021-09-12T02:40:55.294065Z","iopub.status.idle":"2021-09-12T02:40:55.331494Z","shell.execute_reply.started":"2021-09-12T02:40:55.294022Z","shell.execute_reply":"2021-09-12T02:40:55.330547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Exploratory Data Analysis***\n\n","metadata":{}},{"cell_type":"code","source":"#print dataframe information\ndata.info()\n\nprint(\"-\"*50)\n\n#checking if any null values in the dataset\nprint(\"Any null values in the data:\",data.isna().sum().any())\n\nprint(\"-\"*50)\n\n# Exploring  unique values of categorical variables of data\nfor col_name in data:\n    if data[col_name].dtype == \"object\":\n        print(col_name,\":\",data[col_name].unique())","metadata":{"execution":{"iopub.status.busy":"2021-09-12T02:37:39.379921Z","iopub.execute_input":"2021-09-12T02:37:39.380313Z","iopub.status.idle":"2021-09-12T02:37:39.41186Z","shell.execute_reply.started":"2021-09-12T02:37:39.380274Z","shell.execute_reply":"2021-09-12T02:37:39.410312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data contains 8 categorical variables (Dtype-object) and 3 continous features (type-float64).Target variable - posttest.","metadata":{}},{"cell_type":"code","source":"# Initialize figure with subplots\nfig = make_subplots(\n    rows=3, cols=2, subplot_titles=(\"School Setting distribution\", \"School Type Distribution\",'Teaching method distribution','Gender distribution','Lunch distribution','Relationship Between Post-test and Pre-test')\n)\n\n# Add traces\nfig.add_trace(go.Bar(x=data['school_setting'].value_counts().index.values, y=data['school_setting'].value_counts().values/np.sum(data['school_setting'].value_counts())), row=1, col=1)\nfig.add_trace(go.Bar(x=data['school_type'].value_counts().index.values, y=data['school_type'].value_counts().values/np.sum(data['school_type'].value_counts())), row=1, col=2)\nfig.add_trace(go.Bar(x=data['teaching_method'].value_counts().index.values, y=data['teaching_method'].value_counts().values/np.sum(data['teaching_method'].value_counts())), row=2, col=1)\nfig.add_trace(go.Bar(x=data['gender'].value_counts().index.values, y=data['gender'].value_counts().values/np.sum(data['gender'].value_counts())), row=2, col=2)\nfig.add_trace(go.Bar(x=data['lunch'].value_counts().index.values, y=data['lunch'].value_counts().values/np.sum(data['lunch'].value_counts())), row=3, col=1)\nfig.add_trace(go.Scatter(x=data['pretest'], y=data['posttest'], mode='markers'), row=3, col=2)\n\n# Update xaxis properties\nfig.update_xaxes(title_text=\"School Setting\", row=1, col=1)\nfig.update_xaxes(title_text=\"School Type\", row=1, col=2)\nfig.update_xaxes(title_text=\"Teaching Method\", row=2, col=1)\nfig.update_xaxes(title_text=\"Gender\", row=2, col=2)\nfig.update_xaxes(title_text=\"lunch\", row=3, col=1)\nfig.update_xaxes(title_text=\"Pretest\", row=3, col=2)\n\n\n# Update yaxis properties\nfig.update_yaxes(title_text=\"Percent of total\",tickformat = ',.0%',row=1, col=1)\nfig.update_yaxes(title_text=\"Percent of total\",tickformat = ',.0%', row=1, col=2)\nfig.update_yaxes(title_text=\"Percent of total\",tickformat = ',.0%', row=2, col=1)\nfig.update_yaxes(title_text=\"Percent of total\",tickformat = ',.0%', row=2, col=2)\nfig.update_yaxes(title_text=\"Percent of total\",tickformat = ',.0%', row=3, col=1)\nfig.update_yaxes(title_text=\"Posttest\",row=3, col=2)\n\n\n# Update title and height\nfig.update_layout(title_text=\"Ploting distribution of features\", height=700)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T02:37:42.548274Z","iopub.execute_input":"2021-09-12T02:37:42.548694Z","iopub.status.idle":"2021-09-12T02:37:42.719087Z","shell.execute_reply.started":"2021-09-12T02:37:42.548662Z","shell.execute_reply":"2021-09-12T02:37:42.717821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows that their is bias in this dataset. \n1. Distributions of students in school settings, school type, teaching methods and lunch distributions is unbalanced.\n2. The relationship between pre test and post test is highly correlated. We can further check it within the setting of different features.\n\nUnbalanced data may cause lower model predictions for some groups.","metadata":{}},{"cell_type":"code","source":"# Group data together\nhist_data = [data['pretest'], data['posttest']]\n\ngroup_labels = ['pretest', 'posttest']\n\n# Create distplot with custom bin_size\nfig_distributions = ff.create_distplot(hist_data, group_labels, bin_size=.5, show_rug=False)\nfig_distributions.update_layout(title_text=\"Distribution of posttest and pretest\", height=500)\nfig_distributions.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T02:37:47.684627Z","iopub.execute_input":"2021-09-12T02:37:47.685022Z","iopub.status.idle":"2021-09-12T02:37:47.829969Z","shell.execute_reply.started":"2021-09-12T02:37:47.684986Z","shell.execute_reply":"2021-09-12T02:37:47.828894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of pretest and posttest is fairly normal, we don't see any skewness.","metadata":{}},{"cell_type":"code","source":"def create_distplot(column_name):\n    #testing target variable against school setting\n    fig = ff.create_distplot([data['posttest'][data[column_name] == c].values for c in data[column_name].unique()],\n    data[column_name].unique(),\n    show_hist=True,\n    show_rug=False,bin_size=.5\n        )\n    title_text_1 = \"Distribution of post-test by \" + column_name\n    fig.update_layout(title_text= title_text_1, height=500)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T02:37:51.076274Z","iopub.execute_input":"2021-09-12T02:37:51.076691Z","iopub.status.idle":"2021-09-12T02:37:51.083506Z","shell.execute_reply.started":"2021-09-12T02:37:51.076653Z","shell.execute_reply":"2021-09-12T02:37:51.082222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create distribution plots by target variable-postestscore\nfor col_name in data:\n    if data[col_name].dtype == \"object\" and col_name not in ['student_id','classroom']:\n        create_distplot(col_name)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-12T02:37:53.515318Z","iopub.execute_input":"2021-09-12T02:37:53.5158Z","iopub.status.idle":"2021-09-12T02:37:54.283148Z","shell.execute_reply.started":"2021-09-12T02:37:53.515761Z","shell.execute_reply":"2021-09-12T02:37:54.281988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#examining values in continous features to detect any outliers\nfig = go.Figure()\n\nfor col_name in data:\n    if data[col_name].dtype == \"float64\":\n        fig.add_trace(go.Box(y=data[col_name].values, name=data[col_name].name))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T02:38:00.311836Z","iopub.execute_input":"2021-09-12T02:38:00.312226Z","iopub.status.idle":"2021-09-12T02:38:00.36073Z","shell.execute_reply.started":"2021-09-12T02:38:00.31219Z","shell.execute_reply":"2021-09-12T02:38:00.359944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary EDA \n\n* The above distribution plots shows the correlation between each categorical features vs post-test.\n* We can see some overall top performing schools such as \"UKPGS\" has significant better pretest results than others, also school \"GOOBU\" has significant worst postest results than others. This indicates that school might be a factor in the performance of a student.\n* Suburban schools are covering more on the higher tests results and Urban are covering more on the lower test results. \n* Distributions from lunch shows that students without previlage to free lunch have higher test results and students with free lunch might have lower test results which is counterintuitive to this  study https://www.theatlantic.com/education/archive/2017/03/do-healthy-lunches-improve-student-test-scores/520272/ i.e. students have nutrition they need throughout the day to learn, therefore perform better in tests.\n* No significant differences between postest scores of different genders\n","metadata":{}},{"cell_type":"markdown","source":"# ***Data preparation and Feature Transformation***","metadata":{"execution":{"iopub.status.busy":"2021-09-11T21:24:32.794222Z","iopub.execute_input":"2021-09-11T21:24:32.794651Z","iopub.status.idle":"2021-09-11T21:24:32.799752Z","shell.execute_reply.started":"2021-09-11T21:24:32.794614Z","shell.execute_reply":"2021-09-11T21:24:32.798256Z"}}},{"cell_type":"code","source":"#dropping variables with no significance to data \nX = data.drop(['posttest','student_id'], axis=1)\ny = data['posttest']\n\n#listing cateogorical and number features for pipeline\ncategorical_features = list(X.select_dtypes(include=['object']))\nnumeric_features = list(X.select_dtypes(include=['float64']))\n\n#specificing preprocessing steps in pipeline\nnumeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\npreprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),('cat', categorical_transformer, categorical_features)])\n\n#spliting dataset in test and train\nx_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.30, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:07:30.997098Z","iopub.execute_input":"2021-09-12T04:07:30.997505Z","iopub.status.idle":"2021-09-12T04:07:31.013856Z","shell.execute_reply.started":"2021-09-12T04:07:30.997472Z","shell.execute_reply":"2021-09-12T04:07:31.012891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_evaluation(y_test, y_predicted):\n    '''Show model score accuracy, with MAE, MSE and explained variance score'''\n    \n    MAE = mean_absolute_error(y_test, y_predicted)\n    MSE = mean_squared_error(y_test, y_predicted)\n    r2 = r2_score(y_test, y_predicted)\n    exp_var_score = explained_variance_score(y_test, y_predicted)\n    \n    print(f\"Mean absolute error: {round(MAE, 3)}\\nMean squared error: {round(MSE, 3)}\\nR2: {round(r2, 3)}\\nExplained Variance Score:{(exp_var_score)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:07:34.027081Z","iopub.execute_input":"2021-09-12T04:07:34.027437Z","iopub.status.idle":"2021-09-12T04:07:34.032285Z","shell.execute_reply.started":"2021-09-12T04:07:34.027404Z","shell.execute_reply":"2021-09-12T04:07:34.031462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scatter plot to check overfitting\ndef check_overfitting_scatter(y_test,y_predict,model):\n    fig = px.scatter(x = y_test, y = y_hat, trendline = 'ols',labels=dict(x='True scores', y='Predicted scores'),title=model)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:07:36.236942Z","iopub.execute_input":"2021-09-12T04:07:36.237416Z","iopub.status.idle":"2021-09-12T04:07:36.242498Z","shell.execute_reply.started":"2021-09-12T04:07:36.237385Z","shell.execute_reply":"2021-09-12T04:07:36.241081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression - Baseline Model","metadata":{"execution":{"iopub.status.busy":"2021-09-11T21:23:08.162228Z","iopub.execute_input":"2021-09-11T21:23:08.162683Z","iopub.status.idle":"2021-09-11T21:23:08.169163Z","shell.execute_reply.started":"2021-09-11T21:23:08.162649Z","shell.execute_reply":"2021-09-11T21:23:08.167798Z"}}},{"cell_type":"code","source":"#starting with simple linear regression model, easy to understand and implement\nLinear_regr = Pipeline(steps=[('preprocessor', preprocessor),('regr', LinearRegression())])\nLinear_regr.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T03:43:41.257291Z","iopub.execute_input":"2021-09-12T03:43:41.257958Z","iopub.status.idle":"2021-09-12T03:43:41.319593Z","shell.execute_reply.started":"2021-09-12T03:43:41.257906Z","shell.execute_reply":"2021-09-12T03:43:41.318635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model score:\",Linear_regr.score(x_train, y_train))\n#predict testing dataset\ny_hat = Linear_regr.predict(x_test)\n\n#Model evaluation\nmodel_evaluation(y_test,y_hat)\n\n#plotting test vs predicted\ncheck_overfitting_scatter(y_test,y_hat,'Linear Regression: Plotting predicted vs true scores')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:08:01.888554Z","iopub.execute_input":"2021-09-12T04:08:01.888884Z","iopub.status.idle":"2021-09-12T04:08:01.995718Z","shell.execute_reply.started":"2021-09-12T04:08:01.888856Z","shell.execute_reply":"2021-09-12T04:08:01.994606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tree Based Model - Random Forest","metadata":{}},{"cell_type":"code","source":"#random forest with gridsearch and cross validation\nparam_grid_rf = [{\n    'ranfr__n_estimators': [500, 700,1000],\n    'ranfr__criterion': ['mse','mae'],\n    'ranfr__max_depth': [5, 8, 12],\n    'ranfr__max_features': ['sqrt'],\n    'ranfr__min_samples_leaf':[0.001, 0.003]\n}]\n\nrandom_f_regr = Pipeline(steps=[('preprocessor', preprocessor),('ranfr',RandomForestRegressor(random_state = 42))])\n\n#grid search with cross val\nrandom_f_cv = GridSearchCV(random_f_regr, param_grid = param_grid_rf, cv = 3, n_jobs = 1)\nrandom_f_cv.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:08:28.774543Z","iopub.execute_input":"2021-09-12T04:08:28.774879Z","iopub.status.idle":"2021-09-12T04:17:42.844364Z","shell.execute_reply.started":"2021-09-12T04:08:28.774849Z","shell.execute_reply":"2021-09-12T04:17:42.843182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best Parameter Set: ',format(random_f_cv.best_params_))\nprint('Best Estimator Training Score: ',format(random_f_cv.best_score_))\n\nbest_est_ramdom_f_cv = random_f_cv.best_estimator_\n\ny_hat = best_est_ramdom_f_cv.predict(x_test)\n\n#Model evaluation\nmodel_evaluation(y_test,y_hat)\n#plotting test vs predicted\ncheck_overfitting_scatter(y_test,y_hat,'Random Forrest Regression: Plotting predicted vs true scores')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:18:11.187346Z","iopub.execute_input":"2021-09-12T04:18:11.187706Z","iopub.status.idle":"2021-09-12T04:18:11.460101Z","shell.execute_reply.started":"2021-09-12T04:18:11.187671Z","shell.execute_reply":"2021-09-12T04:18:11.459282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest did not outperform linear regression. It was highly resource intensive and did not let me experiment with more hyperparameters. ","metadata":{}},{"cell_type":"markdown","source":"# XGBOOST","metadata":{}},{"cell_type":"code","source":"#starting with simple linear regression model, easy to understand and implement\nxgboost = Pipeline(steps=[('preprocessor', preprocessor),('xgboost', XGBRegressor(verbosity=0))])\nxgboost.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:18:17.659545Z","iopub.execute_input":"2021-09-12T04:18:17.659908Z","iopub.status.idle":"2021-09-12T04:18:17.839573Z","shell.execute_reply.started":"2021-09-12T04:18:17.659874Z","shell.execute_reply":"2021-09-12T04:18:17.838539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model score:\",xgboost.score(x_train, y_train))\n\n#predict testing dataset\ny_hat = xgboost.predict(x_test)\n#Model evaluation\nmodel_evaluation(y_test,y_hat)\n#plotting test vs predicted\ncheck_overfitting_scatter(y_test,y_hat,'Xgboost: Plotting predicted vs true scores')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:18:22.269011Z","iopub.execute_input":"2021-09-12T04:18:22.269473Z","iopub.status.idle":"2021-09-12T04:18:22.379243Z","shell.execute_reply.started":"2021-09-12T04:18:22.269427Z","shell.execute_reply":"2021-09-12T04:18:22.378217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though xgboost is one of the widely used algorithms in the kaggle competitions, in this task, xgboost seems to overfit traning data and underperforms when compared to linear regression","metadata":{}},{"cell_type":"markdown","source":"# Conclusions\n\nAll models performed well and the scores were within the acceptable ranges of accuracy. However, Linear regression performed the best with MAE ~ 2.2. \n\nIf I were to spend more time on this assignment, I would try more algorithms such as bayesian neural networks, Adaboost etc. and further evaluate results from linear regression to check which subsets are underperforming. Test data against Lasso, Ridge, and ElasticNet extensions of linear equation, with an additional penalty parameter that aims to minimize complexity and/or reduce the number of features used in the final model.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}