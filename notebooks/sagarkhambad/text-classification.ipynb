{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Coronavirus Tweets NLP - Text Classifiation\nThis notebook aims at building at text classification engine from the content of Coronavirus Tweets NLP - Text Classifiation dataset that contains around 41157 reviews. Basically, the engine works as follows: after user has provided with tweet, the engine cleans the data and tries to classify the tweet as positive, negative or neutral."},{"metadata":{},"cell_type":"markdown","source":"The Notebook is organised as follows.\n\n**1.Text Cleaning**\n\n* Removing the URLS \n* Removing HTML tags\n* Removing Numbers/Digits\n* Removing Punctuations\n* Removing Mentions\n* Removing Hash\n* Removing extra spaces\n\n**2.Converting Text to Numerical Vector**  \n* TF-IDF\n\n**3.Modeling**\n* MultinomialNB\n* Random Forest\n* SGD Classifier\n* XGBoost\n\n**4.Conclusion**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_dataset = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding=\"latin\")\ntest_dataset = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding=\"latin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_dataset.shape)\nprint(test_dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_dataset.columns)\nprint(test_dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset[\"Sentiment\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, there are five classes:'Neutral, Positive, Extremely Negative, Negative,Extremely Positive'.<br>\nExtremely Negative & Negative is encoded as 0.<br>\nExtremely Positive & Positive is encoded as 2.<br>\nNeutral is encoded as 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"def classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"2\"\n    elif x == \"Extremely Negative\":\n        return \"0\"\n    elif x == \"Negative\":\n        return \"0\"\n    elif x ==  \"Positive\":\n        return \"2\"\n    else:\n        return \"1\"\n    \n\ntrain_dataset['class']=train_dataset['Sentiment'].apply(lambda x:classes_def(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset[\"class\"].value_counts(normalize= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nSTOPWORDS = set(stopwords.words('english'))\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nfrom tqdm import tqdm\npreprocessed_tweets = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(train_dataset['OriginalTweet'].values):\n    sentance = re.sub(r'https?://\\S+|www\\.\\S+', r'', sentance) # remove URLS\n    sentance = re.sub(r'<.*?>', r'', sentance) # remove HTML\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(r'\\d+', '', sentance).strip() # remove number\n    sentance = re.sub(r\"[^\\w\\s\\d]\",\"\", sentance) # remove pnctuations\n    sentance = re.sub(r'@\\w+','', sentance) # remove mentions\n    sentance = re.sub(r'#\\w+','', sentance) # remove hash\n    sentance = re.sub(r\"\\s+\",\" \", sentance).strip() # remove space\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    \n    sentance = ' '.join([e.lower() for e in sentance.split() if e.lower() not in STOPWORDS])\n    preprocessed_tweets.append(sentance.strip())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(preprocessed_tweets)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(preprocessed_tweets)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_tf_idf\ny = train_dataset[\"class\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(X.tocsr(), y, test_size= 0.33, stratify=y,  random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"# MultinomialNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_params ={'alpha':[10**x for x in range(-4,4)]}\nalpha_log = [math.log(x,10) for x in grid_params[\"alpha\"]]\n\nMultinomialNB_model = GridSearchCV(MultinomialNB(),grid_params,\n                     scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nMultinomialNB_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame.from_dict(MultinomialNB_model.cv_results_)\nresults = results.sort_values(['param_alpha'])\n\nplt.plot(alpha_log, results[\"mean_train_score\"], label='Train Accuracy')\nplt.plot(alpha_log, results[\"mean_test_score\"].values, label='CV Accuracy')\n\nplt.scatter(alpha_log, results[\"mean_train_score\"].values, label='Train Accuracy points')\nplt.scatter(alpha_log, results[\"mean_test_score\"].values, label='CV Accuracy points')\n\nplt.legend()\nplt.xlabel(\"Alpha: hyperparameter\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\nprint(MultinomialNB_model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MultinomialNB_model = MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\nMultinomialNB_model.fit(X_train,y_train)\n\ny_pred = MultinomialNB_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results, we can see that the recall is very low for class 2.<br>\nRecall = True Positive/(True Positive + True Negative).<br>\nWhich implies, **74% tweets** indicating **Covid-19 Positive** but model predicting as **Negative** or **Netrual**."},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [1,5,10,50]\nn_estimators = [5,10,100,500]\ngrid_params ={'max_depth':max_depth,'n_estimators':n_estimators}\n\nRandomFoest_model = GridSearchCV(RandomForestClassifier(class_weight = 'balanced'), grid_params,\n                  scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nRandomFoest_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(RandomFoest_model.cv_results_)\nprint(RandomFoest_model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib\n\nmax_depth = [1,1,1,1,5,5,5,5,10,10,10,10,50,50,50,50]\nn_estimators = [5,10,100,500,5,10,100,500,5,10,100,500,5,10,100,500]\nmean_train_score = list(results[\"mean_train_score\"].values)\nmean_test_score = list(results[\"mean_test_score\"].values)\n\nfig = matplotlib.pyplot.figure(figsize=(12,6))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(max_depth, n_estimators, mean_train_score, c='r', marker='o')\nax.scatter(max_depth, n_estimators, mean_test_score, c='b', marker='o')\n\nax.set_xlabel('max_depth ')\nax.set_ylabel('n_estimators')\nax.set_zlabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RandomFoest_model = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n            criterion='gini', max_depth=50, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n            verbose=0, warm_start=False)\nRandomFoest_model.fit(X_train,y_train)\n\ny_pred = RandomFoest_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results, we can see that recall is imporved but precision is low for class 2.<br>\nPrecision = True Positive/(True Positive + False Positive).<br>\nWhich implies, **51% tweets** indicating **Covid-19 Negative** but model predicting as **Positive** or **Netrual**.<br>\nRecall and Precision is descent for class 1 and 2."},{"metadata":{},"cell_type":"markdown","source":"# SGD Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10**x for x in range(-4,4)]\npenalty = [\"l1\",\"l2\"]\ngrid_params ={'alpha':alpha,'penalty':penalty}\nalpha_log = [math.log(x,10) for x in grid_params[\"alpha\"]]\n\nSGDClassifier_model = GridSearchCV(SGDClassifier(class_weight= 'balanced'), grid_params,\n                     scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nSGDClassifier_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(SGDClassifier_model.cv_results_)\nresults = results.sort_values(['param_alpha'])\n\nprint(SGDClassifier_model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SGDClassifier_model = SGDClassifier(class_weight='balanced', penalty='l1')\nSGDClassifier_model.fit(X_train,y_train)\n\ny_pred = SGDClassifier_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results, we can see that recall is imporved but precision is low for class 2.<br>\nPrecision = True Positive/(True Positive + False Positive).<br>\nWhich implies, **42% tweets** indicating **Covid-19 Negative** but model predicting as **Positive** or **Netrual**.<br>\nRecall and Precision is good for class 1 and 2."},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = [0.0001, 0.001, 0.01, 0.1]\nmax_depth = [1,3,5,7]\nn_estimators = [5,10,100,500]\ngrid_params ={'max_depth':max_depth,'n_estimators':n_estimators, 'learning_rate':learning_rate}\n\nXGBoost_model = GridSearchCV(XGBClassifier(), grid_params,\n                      scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nXGBoost_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(XGBoost_model.cv_results_)\nprint(XGBoost_model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBoost_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=7, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)\nXGBoost_model.fit(X_train,y_train)\n\ny_pred = XGBoost_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results, we can see that recall & Precision is imporved and descent for all classes."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"* Precision, Recall and F1 score  for XGBoost model is descent enough than other models. So will go with XGBoost Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}