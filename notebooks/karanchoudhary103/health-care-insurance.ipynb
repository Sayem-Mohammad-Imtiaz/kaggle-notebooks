{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"TASK TO BE COMPLETED\nInstructions\n1. Perform Data Pre-processing and Data Visualization on your data set to develop a thorough understanding of the data. (5)\n\n2. Apply k-means clustering technique after identifying the best k value. (5)\n\n3. Apply hierarchical clustering on the dataset and display the dendrogram. (5)\n\n4. Apply any two performance measures and give your analysis. (5)\n\n// Please note make your Jupyter notebook/Spyder user friendly by adding comments whereever required.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#HEALTH CARE DATASET","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#About the dataset\nage-numeric variable with value 18 and above 18\nsex:gender male or female 0 = Female 1 = Male\nbmi(body mass index)=weight(kg)/height(m)^2  To check about the fitness of the person\nchildren - Numeric variable\nsmoker: Whelther a person smokes or not   , 0 = No smoker, 1 = Smoker\nsex/gender: male or female, 0 = Female ,1 = Male\nregion: four cardinal directions are  north, east, south, and west.\n        Northeast (NE), Southeast (SE), Southwest (SW), and Northwest (NW)\ncharges:amount you have to pay in float value(0.4%f)\nAbout the notebook \n1.About the dataset , preprocessing  and analysis about the dataset.\n2.K-means (Centroid Based) Clustering Algorithm.\n3.Hierarchical ( Divisive and Agglomerative ) clustering.\n4.performance measures about the methods.'''","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#IMPORTING LIBRAIES","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#healthcare insurance dataset\ndataset=pd.read_csv(\"insurance.csv\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# clustering is done on numerical dataset so we are converting categorical value of sexand smoker value to the numerical values\nwe will change the sex(female/male) to 0 and 1.\nwe will change the value's of smoker yes or not to  0 and 1\nwe can perform this function by label encoding also and by replace method.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#changing the attribute sex/gender categorical to numerical values\ndataset['sex'].replace('female',0,inplace = True)\ndataset['sex'].replace('male',1,inplace = True)\n#from sklearn import preprocessing\n# label_encoder object knows how to understand word labels. \n#label_encoder = preprocessing.LabelEncoder()\n# Encode labels in column 'sex'. \n#dataset['sex']= label_encoder.fit_transform(dataset[‘sex']) \n#print(dataset.head())'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#changing the attribute smoker categorical to numerical values\ndataset['smoker'].replace('yes',1,inplace = True)\ndataset['smoker'].replace('no',0,inplace = True)\n\n###from sklearn import preprocessing\n###label_encoder = preprocessing.LabelEncoder()\n# Encode labels in column 'smoker'. \n#dataset['smoker']= label_encoder.fit_transform(dataset[‘smoker']) \n#print(dataset.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# now dataframe is int format for sex and smoker\ndataset.dtypes[::]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#This command will tells about the nullvalues in the dataset and memory usage.\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#describe() function computes a summary of statistics pertaining to the DataFrame columns. \n#This function gives the mean, std and IQR values.\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset = pd.DataFrame(np.random.rand(10, 5), columns=['age','sex','bmi','children','smoker'])\ndataset.plot.box(grid='True')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#now comes the correlation where we are finding the relation between coloumns.\ndataset.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''import seaborn as sn\nsn.heatmap(dataset.corr(), annot=True)\nplt.show()\n'''\nimport numpy as np\nimport seaborn as sns\ncorr = dataset.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 12))\ncmap = sns.light_palette(\"black\",n_colors=6)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#data visulization\nplt.figure(figsize=(20,7))\ndataset.groupby('children')['charges'].max().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,7))\ndataset.groupby('smoker')['charges'].max().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,7))#male and 0 for female\ndataset.groupby('sex')['charges'].max().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.scatter('age','charges',data=dataset)\nplt.xlabel('AGE')\nplt.ylabel('CHARGES')\nplt.title(\"Scatter plot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.scatter('region','charges',data=dataset)\nplt.xlabel('children')\nplt.ylabel('charges')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,20))\nax = sns.lmplot(x = 'age', y = 'charges', data=dataset, hue='smoker')\nax = sns.lmplot(x = 'children', y = 'charges', data=dataset, hue='smoker')\nax = sns.lmplot(x = 'bmi', y = 'charges', data=dataset, hue='smoker')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#now we can conclude that the value on which the dependent variable matters by sorting them. \n#charges is our dependent variable of the dataset\ndataset.corr()['charges'].sort_values()  #In increasing order wrt charges","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From this we can conclude that the smoker has to pay more for the heath care insurance charges than non smoker. .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#KMEANS CLUSTERING\n#This is the most popular method of clustering. It uses Euclidean distance between clusters in each iteration to decide a data point should belong to which cluster, and proceed accordingly. To decide how many no. of clusters to consider, we can employ several methods. The basic and most widely used method is Elbow Curve.\n#Method-1: Plotting Elbow Curve\n#In this curve, wherever we observe a \"knee\" like bent, we can take that number as the ideal no. of clusters to consider in K-Means algorithm.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X=dataset[[\"age\",\"bmi\",\"smoker\",\"children\"]]\n#Plotting Elbow Curve\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn import metrics\n#fitting into the model\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,10))\n\nvisualizer.fit(X)    \nprint(visualizer.show())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, along Y-axis, \"distortion\" is defined as \"the sum of the squared differences between the observations and the corresponding centroid\". It is same as WCSS (Within-Cluster-Sum-of-Squares).\nLet's see the centroids of the clusters. Afterwards, we will fit our scaled data into a K-Means model having 2 clusters, and then label each data point (each record) to one of these 2 clusters.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#Fitting data into K-Means model with 2 clusters\nX=dataset[[\"age\",\"bmi\",\"smoker\",\"children\",\"sex\"]]\nkm=KMeans(n_clusters=2,random_state=42)\nkm.fit(X)\nprint(\"Cluster centers for first   -->\",km.cluster_centers_[0])\nprint(\"Cluster centers for second  -->\",km.cluster_centers_[1])\nprint(\"clustering labels are :\",km.labels_[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#count for total clsuters \npd.Series(km.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'''We see, the higher no. of records belong to the first cluster than second cluster.\nNow, we are interested to check how good is our K-Means clustering model. Silhouette Coefficient is one such metric to check that. The Silhouette Coefficient is calculated using:\n->the mean intra-cluster distance ( a ) for each sample\n->the mean nearest-cluster distance ( b ) for each sample\n->The Silhouette Coefficient for a sample is (b - a) / max(a, b)'''","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# calculate SC for K=2 through K=10 bcz w have n_smaples-1\nk_range = range(2, 10)\nX=dataset[[\"age\",\"bmi\",\"smoker\",\"children\",\"sex\"]]\nscores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=42)\n    km.fit(X)\n    scores.append(metrics.silhouette_score(X, km.labels_))\nprint(\"Score from 2 to 10 -> \\n\",scores[::])\nprint(\"Highest value in silhouette score is of cluster 2 --> \",scores[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X=dataset[[\"age\",\"bmi\",\"smoker\",\"children\"]]\n#Plotting Elbow Curve\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn import metrics\n#fitting into the model\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(2,10),metric='silhouette')\nprint(visualizer)\nvisualizer.fit(X)    \nprint(visualizer.show())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter=100, n_init=10)\ny_kmeans = kmeans.fit_predict(X)\n\nprint(\"kmeans inertia for cluster 2 which is min. ->\",kmeans.inertia_)\n#lower the intertia better the cluster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe the highest silhouette score with no. of clusters 2. In elbow method , from Elbow Curve, we got to see the \"knee\" like bent at no. of clusters 2. \nSo we will do further analysis to choose the ideal no. of clusters for our dataset.\n#For further anlaysis we will use another method.\nDavies-Bouldin Score is defined as the average similarity measure of each cluster with its most similar cluster, \nwhere similarity is the ratio of within-cluster distances to between-cluster distances.\nThus, clusters which are farther apart and less dispersed will result in a better score.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import davies_bouldin_score\ndb= {}\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000,random_state=42).fit(X)\n    clusters = kmeans.labels_\n    db[k] = davies_bouldin_score(X,clusters)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.\nThe minimum score is zero, with lower values indicating better clustering.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"The values of \\n\",db)\n#Plotting Davies-Bouldin Scores\nplt.figure(figsize=(12,6))\nplt.plot(list(db.keys()), list(db.values()))\nplt.xlabel(\"Number of cluster\", fontsize=12)\nplt.ylabel(\"Davies-Bouldin values\", fontsize=12)\nplt.title(\"Davies-Bouldin Scores vs No. of Clusters\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Hierarchical Clustering\n\nThere are two types of hierarchical clustering: Divisive and Agglomerative.\nIn divisive (top-down) clustering method, all observations are assigned to a single cluster and then that cluster is partitioned to two least similar clusters, and then those two clusters are partitioned again to multiple clusters, and thus the process go on. \nIn agglomerative (bottom-up), the opposite approach is followed. Here, the ideal no. of clusters is decided by dendrogram.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#hierarchical clustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendrogram=sch.dendrogram(sch.linkage(X,method='ward'))\nplt.title(\"Dendogram\")\nplt.xlabel('Customers')\nplt.ylabel('Euclidean Distances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nhc=AgglomerativeClustering(n_clusters=2,affinity='euclidean',linkage='ward')\ny_hc=hc.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# As we in datset we have morethan 2 colums we cant plot in it 2D plane so we are using\n'''plt.scatter(X[y_hc == 0, 0],X[y_hc == 0, 1],s=100,c='red',label='Cluster1')\nplt.scatter(X[y_hc == 1, 0],X[y_hc == 1, 1],s=100,c='cyan',label='Cluster2')\nplt.title('Clusters')\nplt.xlabel('Coord 1')\nplt.ylabel('Coord 2')\nplt.legend()\nplt.show()''''''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import seaborn as sns\ncmap = sns.cubehelix_palette(as_cmap=True, rot=.3, light=0.6)\ng = sns.clustermap(X[0:10], cmap=cmap, linewidths=.5)\ng = sns.clustermap(X[0:200], cmap=cmap, linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"THANKS!!!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}