{"cells":[{"metadata":{"_uuid":"2f9abad977af30040cf32ce2515cd77530f8414b"},"cell_type":"markdown","source":"**1. Define the Problem**\n\nWe want to use this dataset to try some beginner models for ML. My idea is to show if we can predict the type, the region or the price with the given data and simple models. Over time, I might add a time series to the kernel.\n\nThis kernel is mainly a project for me to check what I already learned. The best way to learn something is to explain it to someone else ;) \n\nThe procedure is based on the tutorial by https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n\n"},{"metadata":{"_uuid":"8679fb07afe26bd7748d39666598444f687ffc85"},"cell_type":"markdown","source":"**2. Preparation**\n\n**2.1. Get the Data**\n\nFirst I import some important libraries as Pandas, Numpy,... Then I'll load the dataset into a pandas dataframe.\nYou can immediately see that the date cannot be used in this format. Therefore I will split it up into \"Day\", \"Month\" and \"Year\". There's already a \"year\" column because of this I'll check if it's identical to the new \"Year\" column and then drop it. "},{"metadata":{"trusted":true,"_uuid":"d4ed6fb8f67b5c924cd049b4db149b111a599920"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5c5d972fc6fbc8e3c2f0bd43192fd2d0d14dc67"},"cell_type":"code","source":"df = pd.read_csv(\"../input/avocado.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"114ca5a77550e102688de522c4cc5973eb834bec"},"cell_type":"code","source":"# Split Date into D,M,Y\ndate = df[\"Date\"].apply(lambda x: x.split(\"-\"))\ndf[\"Date_Year\"] = date.apply(lambda date: date[0]).astype(int)\ndf[\"Date_Month\"] = date.apply(lambda date: date[1]).astype(int)\ndf[\"Date_Day\"] = date.apply(lambda date: date[2]).astype(int)\ndf.drop(\"Date\", axis=1, inplace = True)\n\n#Check if year and Date_Year are matching\nmatch = df[\"Date_Year\"] == df[\"year\"]\nfor i in match:\n    if i == False:\n        print(\"No match\")\ndf.drop(\"year\", axis=1, inplace = True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"058a8e1e83321fc63b4905ab99be099b7180b82d"},"cell_type":"markdown","source":"**2.2. Get an Overview**\n\nNow we will meet the data. First we look at the length of the dataframe and then we see if the \"Unnamed\" column reflects the index. It doesn't seem to do this, so I will drop it first.\n\nWith almost 18000 entries it seems to be a rather small dataset. We should at least keep this in mind for the next steps."},{"metadata":{"trusted":true,"_uuid":"16d4cecba1a196c35adeafee9c97e6079360c6ba"},"cell_type":"code","source":"#Get an Overview\nprint(df[\"Unnamed: 0\"].max())\nprint(df.index.max())\n\ndf.drop(\"Unnamed: 0\", axis=1, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1180f4f298ff2e8c73c0d0bd51a01ec8f2d47a0"},"cell_type":"markdown","source":"Now I look at the values of the nominal variables and get the metric data described. \n\nFirst, I notice that there are many different regions. It could be helpful to reduce the dimension of this variable. For example we could categorize the region into North, West, South, East. \n\nA look at the statistics shows us that there are sometimes large ranges between Min and Max values. Therefore, we should at least check these variables for outliers. In addition, you can see that the distribution of various variables is skewed and that we should take a closer look at them.\n"},{"metadata":{"trusted":true,"_uuid":"027dfd43bf043c378972af8ccdb15942660063e5"},"cell_type":"code","source":"print(df.type.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d46789e524d7f3a119fde4d116e72590a19df32"},"cell_type":"code","source":"print(df.region.unique())\n#Maybe cluster to N, W, S, E","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"b47f2cf83f8341ca0110664bffcad0b35500f702"},"cell_type":"code","source":"df.describe()\n#Check for Outliers (Min/Max)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7375d2dde39316150aff9f5ac0730a24963d1552"},"cell_type":"markdown","source":"**2.3. 4C's: Completing, Correcting, Creating, and Converting**\n\n**2.3.1. Cheking for Missing Values**\n\nNow we have a rough overview over the individual variables. In the next step I'll check the data for missing values. We don't have any missing values, which is why we're quick with the step \"Completing\"."},{"metadata":{"trusted":true,"_uuid":"6dd6ac7832557686eb755f309467b06d75515492"},"cell_type":"code","source":"print('Columns with NAN:\\n', df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dd6c3942864a6c1aabc43d69fb2f2196a63496d"},"cell_type":"markdown","source":"**2.3.2. Looking for Outliers**\n\nIn this step, we will have a look for univariate outliers. \nThere are some outliers that appear on the far right of the box plot. Also on the scatter plots you can see some outliers far away from the other values. We need to check this further."},{"metadata":{"trusted":true,"_uuid":"7390fe99505db5c41b6c345d90dbd361b9570eeb"},"cell_type":"code","source":"sns.boxplot(x=df['AveragePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcc8e2e6ce8dc07d6aff5aaba8013aa51d26a04f"},"cell_type":"code","source":"sns.boxplot(x=df['Total Volume'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff497bb3269a03024f1316c92a666253ba8ca03b"},"cell_type":"code","source":"sns.boxplot(x=df['4046'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ec8292059c9cc44e7a3e0974b112170443d04fe5"},"cell_type":"code","source":"sns.boxplot(x=df['4225'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"9bff266e0c77a75c68abecfc514db95ef12c78ec"},"cell_type":"code","source":"sns.boxplot(x=df['4770'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6a71146d3440b39a42e6a88f4a9a06026b77956"},"cell_type":"code","source":"sns.boxplot(x=df['Total Bags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"422be260505999093bbe61fac148aa29366c68a2"},"cell_type":"code","source":"sns.boxplot(x=df['Small Bags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc70978ce6d7d7ce50e205c91e50c14030044577"},"cell_type":"code","source":"sns.boxplot(x=df['Large Bags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82a85938d5fc6e6b9cc2ad5670e15b375847e310"},"cell_type":"code","source":"sns.boxplot(x=df['XLarge Bags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd034b93e02c920f773dbb947207ac27b595a1a8"},"cell_type":"code","source":"sns.boxplot(x=df['Date_Year'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1675db6efc57f97466b661fa7f9cd2741759c7f6"},"cell_type":"code","source":"sns.boxplot(x=df['Date_Month'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e091586e5f08bcd8b92bd37c7d6ba0f843642a6c"},"cell_type":"code","source":"sns.boxplot(x=df['Date_Day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f0eaa0ec115c845136626d38ad4741b76b04bfd"},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90faffdb4f05ed2c881ded68ad7f2c0b29d3f3d4"},"cell_type":"markdown","source":"Now I calculate the Z score. I choose a distance of 3. All values outside this distance of [-3, 3] will be defined as outlier and then removed from the dataset.\n\nSince the quartiles of some variables are zero, I will not use the IQR, otherwise too many rows will be removed.\n\nYou can now check the boxplots again and you will see that the outliers disappear."},{"metadata":{"trusted":true,"_uuid":"011ed8f4e175490a3ccf404668f46a7bd908eff0"},"cell_type":"code","source":"#Delete Outliers with ZScore\nnot_to_check = [\"type\",\"region\"]\n\nfrom scipy import stats\n \nz = np.abs(stats.zscore(df.drop(not_to_check, axis=1)))\nthreshold = 3\nrow_to_drop = np.where(z > threshold)[0]\ndf = df.drop(df.index[row_to_drop])\n\nprint(len(row_to_drop), \"Outliers removed\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24ff70921fe10cf1837b097054cecfc63d7bd379"},"cell_type":"markdown","source":"**2.3 Explore the Data**\n\nNow that the data has been cleaned up, we take a closer look at the individual variables to see what we can do with them.\n\nAs you can see there is a lot of skewness. "},{"metadata":{"trusted":true,"_uuid":"c83eeedcc13bd6664e030044e76898cef6bf3efa"},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f82b50e01bba727cddb0c172559da7fff7ba478e"},"cell_type":"code","source":"#Heatmap\n\ncorr = df.corr()\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"debbe92a6cc3f4b2140521743775e1f964bf609b"},"cell_type":"markdown","source":"To improve the skewness we will logarythmize the variables in the hope to get closer to a normal distribution.\n\nYou can also see a strong correlation between \"Total Bags\" and \"Small Bags\", which is obvious as \"Total Bags\" is a sum of the other columns. The "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4cc2b860396f067ab99f942c23010e2b8ff0e946"},"cell_type":"code","source":"input_list = [\"Total Volume\", \"4046\", \"4225\", \"4770\", \"Total Bags\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\"]\n\ndef log_var(input_var):\n    df[input_var] = np.log(1 + df[input_var])\n\nlog_var(input_list)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d7d0579147a18a603c18b03b2b63d23648d4695"},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3979374719c84d14c648d30d351bf817cbba69e"},"cell_type":"markdown","source":"**3. Preprocessing ** \n\nIn this section I will show some approaches to data preparation. However, not every step is suitable, depending on what you want to test. I try to note when you should not use the step. For example, it's not useful to divide the region into dummies if you want to predict them later as dependent variables.\n\n**3.1. Encode Nominal Variables**\n\nAs a first step we code the type of cultivation ('type') numerically. No information is lost, which is why we perform these steps first. \n\nWe also drop the day because in a simple model I assume that the day has no influence if you look at the whole year. But with a more advanced model, it might be interesting to look at the days of the week. Maybe more avocados are bought on Mondays."},{"metadata":{"trusted":true,"_uuid":"fac5a04ad70780b40554ed06a50d0d7e1654a97e"},"cell_type":"code","source":"df[\"type\"] = df[\"type\"].replace(\"conventional\",0).replace(\"organic\",1)\ndf.drop(\"Date_Day\", axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87f18ab04bff08bb1313a76f33ee3f4f512ee92"},"cell_type":"markdown","source":"**3.2. Create Dummies**\n\nFor the variable 'region' we use dummies to better pass them to the model. We could also code the variable numerically as we did before. \n\nIf you use your model to predict the region, you should not use dummy variables. Therefore we make a copy of the dataframe to use it later without dummies for predicting the region."},{"metadata":{"trusted":true,"_uuid":"d8b4d7680eeb100ad0e43fd370417b3fe8afd4c8"},"cell_type":"code","source":"df_region = df\ndf = pd.get_dummies(df, columns = [\"region\"])\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed7aa24eb0d987e50548c6ed76a3fd28f461b650"},"cell_type":"markdown","source":"**3.3. Group Data**\n\nA third approach to dealing with data is to group it. We use a very simple approach and divide the price into high and low in relation to the mean. This is necessary if, for example, we want to determine the price by a logistic regression. Therefore we make a copy of the dataframe to use it later."},{"metadata":{"trusted":true,"_uuid":"fa115fc4e7692b7faa9145a9e9293803a59926ab"},"cell_type":"code","source":"df_price = df\n\nmean = df_price[\"AveragePrice\"].mean()\n\ndf_price.loc[df_price[\"AveragePrice\"] > mean, 'Price'] = 1\ndf_price.loc[df_price[\"AveragePrice\"] < mean, 'Price'] = 0\n\ndf_price = df_price.drop(\"AveragePrice\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fca3e7983fb131584959c3c6fb156b682220580e"},"cell_type":"markdown","source":"**4. Model**\n\n**4.1. Logistic Regression**\n\nNow we can use a first model. For the beginning I use a logistic regression to predict the type of cultivation. To do this we drop this property for x. Then we divide the dataset into a train and a test part and perform the regression and let us output the score to get a first estimation about the model."},{"metadata":{"trusted":true,"_uuid":"622e111853cc7d3fddaf02bc0331ad35edc80d0a"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nx = df.drop([\"type\"], axis=1)\ny = df[\"type\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(x_train, y_train)\nprint(model.score(x_test, y_test))\nprint(model.score(x_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df968841f84de82ba526c9800e9fa1d6a5e35779"},"cell_type":"markdown","source":"The results for both datasets are very similar and very high. But what if there are very few organically grown avocados in the dataset and we therefore make such a good statement? Imagine there would only be 1 'organic' in a set of 100 avocados. Then a simple estimate would be 99% accurate. Let's check that out.\n\nTake a look at the ROC Curve. We have a perfect right angle and thus a very high hit rate with no false positive. The recall curve shows that we have a high precision and high recall. The sample is so we can say that our model works very well for this variable. \n"},{"metadata":{"trusted":true,"_uuid":"708e60df944ff4d26aab3ea5265388183eae769c"},"cell_type":"code","source":"df[\"type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc4f4eb053f32604c79be8b2ed7d4c083310f330"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, precision_recall_curve\n\n#ROC Curve\nfpr_model, tpr_model, thresholds_model = roc_curve(y_test, model.predict_proba(x_test)[:,1])\nplt.plot(fpr_model, tpr_model)\nplt.xlabel(\"P(FP)\")\nplt.ylabel(\"P(TP)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aa5adf3a775bb80e5d269a6cf1023d278a176aa"},"cell_type":"code","source":"#Recall Curve\nprecision_model, recall_model, thresholds_model = precision_recall_curve(y_test, model.predict_proba(x_test)[:,1])\nplt.plot(precision_model, recall_model)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1d3ef6065407e80b85cdc87ba4ad7caaf8b0071"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"b4aaf0d7b434c4077ca4a179e142af0ed9dc2c08"},"cell_type":"markdown","source":"**4.2. KNN & RandomForest**\n\n**4.2.1. Model**\n\nThat was easy. Let's have a look at a little bit more difficult prediction. Now we want to predict the region.\n\nTo do this, we first use logistic regression. As you can see, this does not produce good results in this case. \nSo we try some other models. For example, we use the KNN for a next neighbor classifier and Random Forest for a decision tree."},{"metadata":{"trusted":true,"_uuid":"8133f47ee3ac3b07406dd986aab700ad2c323a28"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nx = df_region.drop([\"region\"], axis=1)\ny = df_region[\"region\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(x_train, y_train)\nprint(model.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28169d5ec55dd6031fe4c9bb345d5dde6c60e9cf"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nx = df_region.drop([\"region\"], axis=1)\ny = df_region[\"region\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn = knn.fit(x_train, y_train)\n\nprint(knn.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e8be2e1b187156a49adf5a39ad832ced45c65b0"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nx = df_region.drop([\"region\"], axis=1)\ny = df_region[\"region\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\nrfc = RandomForestClassifier(criterion = \"entropy\", n_estimators = 300)\nrfc.fit(x_train, y_train)\n\nprint(rfc.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2e21883bfa7e7af2728159e0277799fd06e0a3a"},"cell_type":"markdown","source":"**4.2.2. Cross-Validation**\n\nNow we already have significantly better results. If you now play with the parameters, you will see that the results can be improved/worsened. \n\nLet's check if the results are similar with a cross validation. We do not use cross-validation for the RFC, since this is already covered by the procedure itself."},{"metadata":{"trusted":true,"_uuid":"1c25a3d2cedbe49953917b0fc6cdf3b3cdb0ea58"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\n\nx = df_region.drop([\"region\"], axis=1)\ny = df_region[\"region\"]\n\nscores = cross_val_score(LogisticRegression(solver=\"liblinear\"), x, y, cv = RepeatedKFold(n_repeats = 2))\n\nprint(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db2cd766ef404f136d6a2a7a1141a1e29713cf9d"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\n\nx = df_region.drop([\"region\"], axis=1)\ny = df_region[\"region\"]\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn = knn.fit(x,y)\nscores = cross_val_score(knn, x, y, cv = RepeatedKFold(n_repeats = 2))\n\nprint(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7bc88e2744e1153bdc09c175f4077e6c8d63956"},"cell_type":"markdown","source":"**4.3. The whole model**\n\nNow we want to apply all the methods shown together to the Age variable and try to predict whether the price is low or high.\n\nFirst let's test some models."},{"metadata":{"trusted":true,"_uuid":"69484284ec7d9b8ac283113efaf54acb74708655"},"cell_type":"code","source":"df_price.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3360339b0889e33ece6804ed292b924a4342a5a5"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(x_train, y_train)\nprint(model.score(x_test, y_test))\nprint(model.score(x_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47fe40db1058649ab1e161d4240351c2df90f6ec"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn = knn.fit(x_train, y_train)\n\nprint(knn.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d563d226c5295aa77368e3894c0c1be4c2a88ae9"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\nrfc = RandomForestClassifier(criterion = \"entropy\", n_estimators = 300)\nrfc.fit(x_train, y_train)\n\nprint(rfc.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ee842cadc9ea7852592db18faf7d20ae9742fdf"},"cell_type":"markdown","source":"**4.3.2. Validate**\n\nThe cross validation and the curves look pretty good for KNN and RFC. So we can assume that our model works well. Of course the accuracy can be increased a bit. "},{"metadata":{"trusted":true,"_uuid":"904530f2be6fdd9d3689b552f0401945353df350"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nscores = cross_val_score(LogisticRegression(solver=\"liblinear\"), x, y, cv = RepeatedKFold(n_repeats = 2))\n\nprint(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65e82fcfcbafa8898761878467ace16d89dec072"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn = knn.fit(x,y)\nscores = cross_val_score(knn, x, y, cv = RepeatedKFold(n_repeats = 2))\n\nprint(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90cc5c8594a025aef3c17cdc1506dd10d535bcf6"},"cell_type":"code","source":"df_price[\"Price\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ba99e0d9472341ebb73e0f8ead3e50dd4282121"},"cell_type":"code","source":"#ROC Curve\nfpr_model, tpr_model, thresholds_model = roc_curve(y_test, model.predict_proba(x_test)[:,1])\nplt.plot(fpr_model, tpr_model, label = \"LogisticRegression\")\n\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, knn.predict_proba(x_test)[:,1])\nplt.plot(fpr_knn, tpr_knn, label = \"KNN\")\n\nfpr_rfc, tpr_rfc, thresholds_rfc = roc_curve(y_test, rfc.predict_proba(x_test)[:,1])\nplt.plot(fpr_rfc, tpr_rfc, label = \"RFC\")\nplt.xlabel(\"P(FP)\")\nplt.ylabel(\"P(TP)\")\nplt.legend(loc = \"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1b351ea07e9d9eb10c6715a3524b274c1150660e"},"cell_type":"code","source":"#Recall Curve\nprecision_model, recall_model, thresholds_model = precision_recall_curve(y_test, model.predict_proba(x_test)[:,1])\nplt.plot(precision_model, recall_model, label = \"LogisticRegression\")\n\nprecision_knn, recall_knn, thresholds_knn = precision_recall_curve(y_test, knn.predict_proba(x_test)[:,1])\nplt.plot(precision_knn, recall_knn, label = \"KNN\")\n\nprecision_rfc, recall_rfc, thresholds_rfc = precision_recall_curve(y_test, rfc.predict_proba(x_test)[:,1])\nplt.plot(precision_rfc, recall_rfc, label = \"RFC\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b036818cea518b0b33c91790cbbe10effc42cfe"},"cell_type":"markdown","source":"**4.3.3. Validation Curve**\n\nNow we take a closer look at the validation curve for the KNN method. We see that we have an overfitting in a few neighbours and on the right side we come into an underfitting area. Thus we are with the selected n_neighbors = 5 already strongly at the border to the overfitting."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f0b0b58c69478d8d91f7bf42b178473d07fc796b"},"cell_type":"code","source":"#Validation Curve\nfrom sklearn.model_selection import validation_curve\nparam_range = np.array([40, 30, 20, 15, 10, 8, 7, 6, 5, 4, 3, 2, 1])\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\ntrain_scores, test_scores = validation_curve(\n    KNeighborsClassifier(), \n    x,\n    y,\n    param_name = \"n_neighbors\",\n    param_range=param_range)\n\nplt.plot(param_range, np.mean(train_scores, axis = 1))\nplt.plot(param_range, np.mean(test_scores, axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec591fc6c08787c8f87a55a11c34db89db4a9ab4"},"cell_type":"markdown","source":"**4.3.4. Learning Curve**\n\nWe can also have the Learning Curve drawn. This shows us whether it would be possible to improve the model with a larger data set. It can be seen, however, that none of the curves really converges further, so a larger data set does not bring much improvement in this case."},{"metadata":{"trusted":true,"_uuid":"c4f3093244d59e720950a24d43986a87204ba7b3"},"cell_type":"code","source":"#Learning Curve\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.utils import shuffle\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx, y = shuffle(x, y)\n\ntrain_sizes_abs, train_scores, test_scores = learning_curve(LogisticRegression(solver=\"liblinear\"), x, y)\nplt.plot(train_sizes_abs, np.mean(train_scores, axis = 1))\nplt.plot(train_sizes_abs, np.mean(test_scores, axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98f8c6579df0e40d3baae7af3d0fb0c13cf72be5"},"cell_type":"code","source":"x = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx, y = shuffle(x, y)\n\ntrain_sizes_abs, train_scores, test_scores = learning_curve(KNeighborsClassifier(n_neighbors=5), x, y)\nplt.plot(train_sizes_abs, np.mean(train_scores, axis = 1))\nplt.plot(train_sizes_abs, np.mean(test_scores, axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d06931d15405126b5d482af23d650768f4030a26"},"cell_type":"code","source":"x = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx, y = shuffle(x, y)\n\ntrain_sizes_abs, train_scores, test_scores = learning_curve(RandomForestClassifier(criterion = \"entropy\", n_estimators = 300), x, y)\nplt.plot(train_sizes_abs, np.mean(train_scores, axis = 1))\nplt.plot(train_sizes_abs, np.mean(test_scores, axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57891aa45a6e4360acb732bcc93049d08145d6de"},"cell_type":"markdown","source":"**5. Further Actions**\n\nNow we have already been able to make a quite good prediction. But how can we improve the model apart from feature engineering? \n\n**5.1. Hyperparameter Tuning ** \n\nOne possibility would be to adjust the parameters of the model. We could use GridSearchCV to compare different parameters and select the best one. I will now do this using the KNN classifier as an example.\n\nAs you can see we get the best possible value for the desired parameter and also the score that matches this value. You can also see that this value, if we now think back to the curves, is in the area where no overfitting or underfitting occurs. "},{"metadata":{"trusted":true,"_uuid":"1f7af41d39d0f793ce8234929109be419ab080fb"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y)\n\n# Choose the type of classifier. \nclf = KNeighborsClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_neighbors': [1,3,5,7,9,11,13,15], \n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(x_train, y_train)\n\n# Set the clf to the best combination of parameters\nprint(grid_obj.best_estimator_)\nprint(grid_obj.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02e9a5e60e19813b4017338ddc653bb4ea172663"},"cell_type":"markdown","source":"**5.2. Compare Different Models**\n"},{"metadata":{"trusted":true,"_uuid":"a9e7adeda8b2f22f167471f3d822e8fff0a540c0"},"cell_type":"code","source":"models = [\"Logistic Regression\", \"Random Forest Classifier\", \"KNeighbors Classifier\"]\nscores = []\npredictions_list = []\n\nx = df_price.drop([\"Price\"], axis=1)\ny = df_price[\"Price\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y)\n\nlr = LogisticRegression()\nrfc = RandomForestClassifier(criterion = \"entropy\", n_estimators = 300)\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nLogistic_Regression = lr.fit(x_train,y_train)\nRFC = rfc.fit(x_train, y_train)\nKNN = knn.fit(x_train,y_train)\n\nscores.append(Logistic_Regression.score(x_train, y_train))\nscores.append(RFC.score(x, y))\nscores.append(KNN.score(x_train, y_train))\n\npredictions_LR = Logistic_Regression.predict(x_test)\npredictions_list.append(accuracy_score(y_test, predictions_LR))\npredictions_RFC = RFC.predict(x_test)\npredictions_list.append(accuracy_score(y_test, predictions_RFC))\npredictions_KNN = KNN.predict(x_test)\npredictions_list.append(accuracy_score(y_test, predictions_KNN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7de4124b41ad7937847bba086c70cf66da7343b0"},"cell_type":"code","source":"sns.set_color_codes(\"muted\")\nsns.barplot(x=predictions_list, y=models, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8211f12ad0453c13a9152540babbe8f31c750ffc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}