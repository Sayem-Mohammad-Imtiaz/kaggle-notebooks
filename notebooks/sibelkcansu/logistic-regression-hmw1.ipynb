{"cells":[{"metadata":{"_uuid":"a713fe5cb9a7cd558d3ca22f50f5852abcf242e4"},"cell_type":"markdown","source":"# INTRODUCTION\n\nIn this kernel, we will apply Logistic Regression procedure to \"Gender Recognition by Voice Data\"\n1. [Read Data](#1)\n1. [Logistic Regression](#2)\n    1. [Determine Values](#3)\n    1. [Train Test Split](#4)\n    1. [Forward Backward Propagation](#5)\n    1. [Prediction](#6)\n    1. [Logistic Regression Algorithm](#7)\n    1. [Logistic Regression with sklearn Library](#8)\n1. [Conclusion](#9) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f4dc5cfe5aabd9a03b37806c544f9d6a3c4bd68"},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# Read Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/voice.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b966c09eed78c7ffe08aa0595218f987ba2ffba"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b51dbf51d860735477a525f93b6dec661cdb52a9"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8fdd1aa90830a8f081c638041dbb9a0bd047ced"},"cell_type":"code","source":"data.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"617d07b97b2b5353f10f305ad8b40bbb51ee24a5"},"cell_type":"markdown","source":"Let's classify male and female as male=1 and female=0. "},{"metadata":{"trusted":true,"_uuid":"845988bdf7e3b9617e8fe027c33ac10c7409f465"},"cell_type":"code","source":"data.label=[  1 if i==\"male\" else 0 for i in data.label]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4255ae868542ea9212448a989d8e8b69b4d67bcb"},"cell_type":"markdown","source":"Now, label is a binary output, and our data is convenient for Logistic Regression."},{"metadata":{"trusted":true,"_uuid":"29b97243f7e2ebf11c00dee461bdba9a08adea42"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bd54ff846c154f18fe47d1b07e479aca0d47c69"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2fd414fd4e73418e87c7aaf56ed40623a602a65"},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# Logistic Regression\n\nLogistic Regression is a classification algortihm. It is the simplest deep learning (neural network). \n\nFirst of all we want to train our data. So , we will use Computation Graph. Here are the components of Computation Graph.\n* parameters: weights and bias(w and b )\n* weights: coefficents of values of  each feature \n* z = ((w)^T)*x + b  or we can write  z = b + p1*w1 + p2*w2 + ... + p20*w20 for our data\n* p1, p2,..., p20: values of each feature in data (this will be meaningful after train test split method !)\n* y_head = sigmoid(z)\n    * Sigmoid function (which is called as activation function) makes z between 0 and 1 so that is a probabilitic result. \n    * Mathematical equation of sigmoid function is   $f(x)=\\displaystyle \\frac{1}{1+\\mathbb{e}^{-x}}$.\n"},{"metadata":{"_uuid":"29c71979c98d470ceee7cd994c94989d1f4e5f84"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# Determine Values\nFirst of all, we will determine x and y values for Logistic Regression."},{"metadata":{"trusted":true,"_uuid":"72eaf6c9beb40b01d5ed1be1fc0238ef9311be02"},"cell_type":"code","source":"y=data.label.values\nx_data=data.drop([\"label\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"540cee2b9ecbf592eff2de01204b9c2a84a86454"},"cell_type":"markdown","source":"Let's check what is y and x_data.\n* y is our output\n* the values in x_data will be coefficients of weights. "},{"metadata":{"trusted":true,"_uuid":"aa5f68b326166faee6ad326ede85cddaf6973d92"},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0323308ded9be7b95a5392a81a60bc10e550f669"},"cell_type":"code","source":"x_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa01bdd7ea9dcd2a1e427ba141f94a8f3af8c030"},"cell_type":"markdown","source":"To get an appropriate model we need to normalize the values in x_data."},{"metadata":{"trusted":true,"_uuid":"2d8cac21bfdce13d08b101cefea853d8d1e738ef"},"cell_type":"code","source":"# normalization =(a-min(a))/(max(a)-min(a))\n\nx=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfd5d6582c2fc329389e63f1d01d70fee7808e84"},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8963be0adc261d365179c458a10071e64ee99d36"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# Train Test Split\nWe want to train our data by Linear Regression. But after getting our model, we need another data to test our model. So we will use **train_test_split**\nto control the acurracy of our model.\n* train_test_split says that take 80% of data to get the model and use 20% of data to control the model."},{"metadata":{"trusted":true,"_uuid":"c17bec10240f3af551c11906cb5086e0e40faf66"},"cell_type":"code","source":"# create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n\n# our features must be row in our matrix.\n\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\n\nprint(\"x_train: \", x_train.shape)\nprint(\"x_test: \", x_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98e2aa82a00a8b7bd4a22cbfacf8573c4b9a6ea7"},"cell_type":"markdown","source":"Now, we split our data with train_test_split and we will use x_train and y_train  for Linear Regression Model. \n* We will define the functions which we'll use in Linear Regression. \n*  First, we will define initial weights, initial bias and sigmoid function."},{"metadata":{"trusted":true,"_uuid":"6d5866615ab56dede4747cb8102bc5b95416a5d4"},"cell_type":"code","source":"# lets initialize parameters\n# So what we need is dimension, that is, the number of features as a parameter for our initialize method(def)\n# dimension=20\n#initial weights=0.01, initial bias=0\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\n\n#sigmoid function\n\ndef sigmoid(z):\n    \n    y_head=1/(1+np.exp(-z))\n    return y_head\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93e634b68af256b0e7d52a5e50295a2381cd83d3"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# Forward Backward Propagation\n\n**Forward propagation** is the all steps from features (x_train) to cost.\n*  z = ((w)^T)*x + b  or we can write  z = b + p1*w1 + p2*w2 + ... + p20*w20\n* Then compute y_head=sigmoid(z)\n* Calculate loss(error) function= $-(1-y).\\log(1-\\widetilde{y})-y.\\log(\\widetilde{y})$; ( actually we are finding y_head for each column in x_train matrix.)\n    * We are using loss function to decide whether our prediction is correct or not.    \n* Cost function=Summation of all loss functions.\n\n**Backward propagation** means that we are updating parameters in terms of the value of Cost Funciton. So we will use y_head that we found in forward propagation.\n\n*  Updating: There is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with  α (learning rate). Then update weight. \n    * w = w - learning_rate * gradients[\"derivative_bias\"]\n* We will do the same thing for bias. \n     *     i.e.  Take derivative of bias according to weight and bias. Then multiply it with  α (learning rate). Then update bias.\n         * b = b - learning_rate * gradients[\"derivative_bias\"]        \n\n"},{"metadata":{"trusted":true,"_uuid":"cedddfe1f7d962d6e25b9c646a23c9ba49cf37d1"},"cell_type":"code","source":"# forward backward propagation\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss=-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost=(np.sum(loss))/x_train.shape[1] # x_train.shape[1] is for scaling\n    \n    #backward propagation\n    # In backward propagation we will use y_head that found in forward propagation\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1] is for scaling\n    derivative_bias=np.sum(y_head-y_train)/x_train.shape[1]                   # x_train.shape[1] is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"621ef2902927a3f5063c975fb8ac6c5bdcf216c3"},"cell_type":"markdown","source":"When updating parameters, we need to choose wisely learning rate. Learning rate should be neither too big nor too small.\n* Here, number_of_iterations and learning_rate are called as hyperparameter. That is, we need to set the values by hand. "},{"metadata":{"trusted":true,"_uuid":"96da3d71e1faf7df1a5e5bc1988c62b7140cb606"},"cell_type":"code","source":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72cefce8dacf2bfaf51fb96df34bc32148067e45"},"cell_type":"markdown","source":"<a id=\"6\"></a>\n# Prediction\nUp to here, we do:\n* prepare our data for LR\n* parameters: weights and bias\n* initialize parameters\n* sigmoid fuction\n* loss function\n* Cost function\n* updating parameters\n* Now let's predict.  In prediction step we have x_test as input "},{"metadata":{"trusted":true,"_uuid":"92a4d6dcc9d018d25b3ebc9200f36852f4526444"},"cell_type":"code","source":"#prediction\ndef predict(w,b,x_test):\n    # x_test is an input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1f6a30332fd8dbe53ba320d982077e0f0e5d613"},"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Logistic Regression Algorithm\nWe make prediction.  Let's define logistic_regression function with learning_rate = 1, num_iterations = 100"},{"metadata":{"trusted":true,"_uuid":"5516a8d80993c0550df9e7b0c688dd68dc905d7e"},"cell_type":"code","source":"#Logistic Regression\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 20\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print train/test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12725fed2b92007575b407d122f10a3e226d25f5"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9124174a7a33016558bf801574ba66ea2dddb34"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63329fcdc34d566c2343d2aee71e12a60844aa9e"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 2, num_iterations = 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ac0ff58263692c30f56cbbea0e88e0a66d2b409"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1dc190296f326dba2291b22e7d40a0dec7f7773"},"cell_type":"markdown","source":"As we see, we have the best result when  learning_rate = 3, num_iterations = 300 or  learning_rate = 2, num_iterations = 500."},{"metadata":{"_uuid":"de6553eef51adf825201da51d1a63324aa485ab7"},"cell_type":"markdown","source":"<a id=\"8\"></a>\n# Logistic Regression with sklearn Library\nAlso, we can use sklearn library to make Linear Regression."},{"metadata":{"trusted":true,"_uuid":"1c11e0019b251a69566d2d07665b3a2df44f4088"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\n\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abd4cf266563e6379d708c36582b755daf92da5d"},"cell_type":"markdown","source":"<a id=\"9\"></a>\n# Conclusion\n\nIf we write all the functions we need, then we get the best result for accuracy as  \n* test accuracy: 97.94952681388013 % when  learning_rate = 2, num_iterations = 500.\n* But if we use sklearn libray for Linear Regression our test accuracy is 0.9810725552050473."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}