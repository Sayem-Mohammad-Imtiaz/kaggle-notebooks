{"cells":[{"metadata":{},"cell_type":"markdown","source":"Our objective is to evaluate the diagnosis based on data. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/data.csv')\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that the data is very clean, with no Null values, and all being floats (except diagnosis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\ndata['diagnosis'] = data['diagnosis'].map({'M':1, 'B':0})\ny = pd.DataFrame(data = data['diagnosis'])   \nlist = ['Unnamed: 32','id','diagnosis']\nx = data.drop(list,axis = 1 )\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To begin, we shall look at the correlation of the features (i.e. exclduing that of Diagnosis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The idea is that we should remove features that are correlated, as it will lower the number of features to work with. Once we have done so, we can then compare the correlation of the remaining features against that of diagnosis."},{"metadata":{"trusted":true},"cell_type":"code","source":"rem = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\ndata_new = x.drop(rem, axis=1)\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(data_new.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, we further filter out the features, leaving only those that have sufficiently strong correlation with that of diagnosis."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new['diagnosis'] = y\ncorrmat = data_new.corr().abs()\ntop_corr_features = corrmat.index[abs(corrmat[\"diagnosis\"])>0.1]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(data_new[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With that in mind, we can recreate the new data based on the required columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"req = ['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean', 'symmetry_mean', 'area_se', 'concavity_se',\n      'smoothness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst']\ndata_new2 = data_new[req]\ndata_new = data_new.drop(['diagnosis'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check if the data agrees with what was selected, we can do a simple test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(data_new2, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can start modelling. We will be stacking and mixing the models for this test run."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final = pd.read_csv('../input/data.csv')\ny_final = data_final['diagnosis'].map({'M':0, 'B':1})\ndata_final = data_final[req]\ndata_final.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_final = data_final\nx_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.2, random_state=42)\nx_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To optimize the code, we will use the Randomized Search CV to help."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\ncriterion =['mse', 'friedman_mse', 'mae']\nsplitter =['best', 'random']\nmax_depth = [None, 1, 11, 21, 31]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nmax_features = ['auto', 'sqrt', 'log2']\n\nrandom_grid = {'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'criterion' :criterion,\n              'splitter' : splitter}\n\nprint(random_grid)\n\nmodel_test = DecisionTreeRegressor(random_state=43)    \nmodel_random = RandomizedSearchCV(estimator = model_test, param_distributions = random_grid, n_iter = 100, cv = 3, random_state = 42, n_jobs = -1)\nmodel_random.fit(x_train, y_train)\nmodel_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf = RandomForestClassifier(random_state=43, n_estimators = 20, min_samples_split=2, min_samples_leaf=2,\n                               max_features='sqrt', max_depth=21,bootstrap=False)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nprint(r2_score(y_test,clf_rf.predict(x_test)))\nprint(mean_squared_error(y_test, clf_rf.predict(x_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\netc_test = ExtraTreesClassifier(random_state=43, n_estimators=300, min_samples_split=2,min_samples_leaf=1,\n                               max_features='sqrt',max_depth=31, bootstrap=True)\netc_test = etc_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,etc_test.predict(x_test)))\nprint(mean_squared_error(y_test, etc_test.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\netr_test = ExtraTreesRegressor(random_state=43,n_estimators=100, min_samples_split=2, min_samples_leaf=1,\n                               max_features='auto',max_depth = None,bootstrap=False)\netr_test = etr_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,etr_test.predict(x_test)))\nprint(mean_squared_error(y_test, etr_test.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr_test = RandomForestRegressor(random_state=43,n_estimators=200,min_samples_split=2,min_samples_leaf=1,\n                                max_features='log2',max_depth=11, bootstrap=False)\nrfr_test = rfr_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,rfr_test.predict(x_test)))\nprint(mean_squared_error(y_test, rfr_test.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtc_test = DecisionTreeClassifier(random_state=43,min_samples_leaf=8)\ndtc_test = dtc_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,dtc_test.predict(x_test)))\nprint(mean_squared_error(y_test, dtc_test.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndtr_test = DecisionTreeRegressor(random_state=43,splitter='best',min_samples_split=2,min_samples_leaf=8,\n                                max_features='auto', max_depth=11, criterion='mse')\ndtr_test = dtr_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,dtr_test.predict(x_test)))\nprint(mean_squared_error(y_test, dtr_test.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\ngbr = ensemble.GradientBoostingRegressor(n_estimators = 400, max_depth = 7, min_samples_split = 8,\n          learning_rate = 0.1, loss = 'ls')\ngbr = gbr.fit(x_train,y_train)\n\nprint(r2_score(y_test,gbr.predict(x_test)))\nprint(mean_squared_error(y_test, gbr.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After testing and confirming the different models, we can join them all together using Stacking CV Regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.regressor import StackingCVRegressor\nrfc = RandomForestClassifier(random_state=43, n_estimators = 20, min_samples_split=2, min_samples_leaf=2,\n                               max_features='sqrt', max_depth=21,bootstrap=False)   \netc = ExtraTreesClassifier(random_state=43, n_estimators=300, min_samples_split=2,min_samples_leaf=1,\n                               max_features='sqrt',max_depth=31, bootstrap=True)\netr = ExtraTreesRegressor(random_state=43,n_estimators=100, min_samples_split=2, min_samples_leaf=1,\n                               max_features='auto',max_depth = None,bootstrap=False)\nrfr = RandomForestRegressor(random_state=43,n_estimators=200,min_samples_split=2,min_samples_leaf=1,\n                                max_features='log2',max_depth=11, bootstrap=False)\ndtc = DecisionTreeClassifier(random_state=43,min_samples_leaf=8)\ndtr = DecisionTreeRegressor(random_state=43,splitter='best',min_samples_split=2,min_samples_leaf=8,\n                                max_features='auto', max_depth=11, criterion='mse')\ngbr = ensemble.GradientBoostingRegressor(n_estimators = 400, max_depth = 7, min_samples_split = 8,\n          learning_rate = 0.1, loss = 'ls')\nstack_gen = StackingCVRegressor(regressors=(rfc, etr, rfr, dtc, dtr),\n                                meta_regressor=dtr,\n                                use_features_in_secondary=True)\n\nstack_gen_model = stack_gen.fit(x_train,y_train)\n\nprint(r2_score(y_test,stack_gen_model.predict(x_test)))\nprint(mean_squared_error(y_test, stack_gen_model.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, we can blend the models together to gain a prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"def blend_models_predict(X):\n    return ((0.05 * etc.predict(X)) + \\\n            (0.05 * gbr.predict(X)) + \\\n            (0.1 * rfc.predict(X)) + \\\n            (0.1 * rfr.predict(X)) + \\\n            (0.1 * dtc.predict(X)) + \\\n            (0.2 * dtr.predict(X)) + \\\n            (0.1 * etr.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))\n\netc_model = etc.fit(x_train,y_train)\ngbr_model = gbr.fit(x_train,y_train)\nrfc_model = rfc.fit(x_train,y_train)\nrfr_model = rfr.fit(x_train,y_train)\ndtc_model = dtc.fit(x_train,y_train)\ndtr_model = dtr.fit(x_train,y_train)\netr_model = etr.fit(x_train,y_train)\n\nprint(r2_score(y_test,blend_models_predict(x_test)))\nprint(mean_squared_error(y_test, blend_models_predict(x_test)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}