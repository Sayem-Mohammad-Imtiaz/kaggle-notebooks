{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pandas ","metadata":{}},{"cell_type":"markdown","source":"As a data scientist we will not be creating the datasets, instead we will be reading data from the different types of files. Hence we will practice the pandas library on actual dataset.","metadata":{}},{"cell_type":"markdown","source":"## Importig the numpy and pandas","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Reading data from csv file . \n\n1. pd.read_csv(r\"filepath\")\n2. pandas can read many types of data including excel, sql, clipboard, table , text etc. \n3. use r\" \" to avoid the error or use double // as escape character ","metadata":{}},{"cell_type":"code","source":"# pd.read function - \n\ndf = pd.read_csv(r\"../input/test-file/tested.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### head(*) and tail(*)<br>\n\nHead shows first five rows of dataset and tail last five rows<br>\nwe can use integer value as argument to show specific number of lines <br>\nWhy we use head and tail methods, to check the data columns and values and to get idea about dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2) # shows 2 lines","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next we want to look at the shape of data, how many rows and columns we have in dataset","metadata":{}},{"cell_type":"code","source":"# shape shows rows and columns of the dataset, here we have 891 row ( we call it observations)\n# and 12 columns ( we call it features/variables in data science )\n\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We also want to know the data type of the columns right","metadata":{}},{"cell_type":"code","source":"# dtypes - return data type of columns,\n# info() - we can get more information using info() than dtypes\n\nprint(df.dtypes)\nprint('\\n')\nprint(df.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quickly Checking columns and index","metadata":{}},{"cell_type":"code","source":"# columns - checking column names \n# index - to check index of datasets\n\nprint(df.columns)\nprint('\\n')\nprint(df.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### To get information like mean min max standard deviation and percentiles. Use following function","metadata":{}},{"cell_type":"code","source":"# describe() - information about the numerical columns\n\nprint(df.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accessing rows or columns of the dataset","metadata":{}},{"cell_type":"markdown","source":"### Accessing columns","metadata":{}},{"cell_type":"code","source":"# Single columns - there are two methods to access the columns by . and by ['']\n\n# note if columns has space then . method won't work. And it confuses us if its column or method\n# hence we will be using [\" \"] methods throughout the file. Once you get confirtable with it you can use . method as you wish. \n\n# by . method - lets call it dot method\nprint(df.Survived)\n\n# by [\"\"] method lets call it name method\nprint(df['Survived'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aceesing multiple columns - by passing list of desired columns in the square brackets\n# Sequence of columns will be as you passed them in list of columns\n\ndf[['Survived','PassengerId']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are two more ways to access the columns and are oftern used to get the chunk of dataset<br>\n\n<b>loc method</b>\n\niloc[] - index location, index based location<br>\nloc[] - location , name based location <br>\nLOC uses row and index sequence to access the cut of original dataframe<br>\nboth rows or columns can be range, single or list<br>","metadata":{}},{"cell_type":"code","source":"# using loc method to get all rows and specific columns\n\nprint('1. all rows and col - survived ')\nprint(r\"Code :- df.loc[ : , 'Survived']\") \n\nprint(df.loc[ : , 'Survived']) # focus here\n\nprint('\\n2. all rows and column from Sex to Ticket ')\nprint(r\"Code :- df.loc[ : , 'Sex': 'Ticket']\")\n\nprint(df.loc[ : , 'Sex': 'Ticket'])  # focus here\n\nprint('\\n3. all rows and specific columns pass as list Pclass, Fare and Name')\nprint(r\"Code :- df.loc[ : , ['Pclass','Fare','Name']]\")\n\nprint(df.loc[ : , ['Pclass','Fare','Name']])  # focus here\n\n# as our dataset have the index in number from 0 to 890 we can use number in loc otherwise we need to provide the index name\n\nprint('\\n4. all rows till index named 100 ( 100th  included ) and col - survived ')\nprint(r\"Code :- df.loc[:100 , 'Survived']\") \n\nprint(df.loc[:100 , 'Survived'])   # focus here\n\nprint('\\n5. all rows from 10 to 100 and column from Sex to Ticket')\nprint(r\"Code :- df.loc[10 : 100, 'Sex': 'Ticket']\")\n\nprint(df.loc[10 : 100, 'Sex': 'Ticket'])  # focus here\n\nprint('\\n6. rows 12,100,890 and specific columns pass as list Pclass, Fare and Name')\nprint(r\"Code :- df.loc[[12,100,890] , ['Pclass','Fare','Name']]\")\n\nprint(df.loc[[12,100,890] ,columns = ['Pclass','Fare','Name']])  # focus here","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>iloc method</b><br>\n\niloc method is similar to the loc method but instead of using name of index or columns it uses the index of it <br>\nPlease note our dataset already have the index as 0 to 890 hence the row index might be look similar to loc but <br>\nits different as in loc we used it as name not index here we are using it as index <br>","metadata":{}},{"cell_type":"code","source":"# using loc method to get all rows and specific columns\n# note that iloc method excludes the last index passed\n\nprint('\\n1. all rows till index till 100 ( 100th  excluded ) and col - indexed at 1')\nprint(r\"Code :- df.iloc[:100 , 1]\") \n\nprint(df.iloc[:100 , 1])   # focus here\n\nprint('\\n2. all rows from 10 to 100 and column from index 4 to 10 excluded 10')\nprint(r\"Code :- df.iloc[10 : 100, 4 : 10 ]\")\n\nprint(df.iloc[10 : 100, 4 : 10 ])  # focus here\n\nprint('\\n3. rows 12,100,890 and specific columns pass as list of index for Pclass, Fare and Name')\nprint(r\"Code :- df.iloc[ [12,100,890] , [2,9,3] ]\")\n\nprint(df.iloc[[12,100,800],[2,9,3]])  # focus here","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conditional Slicing","metadata":{}},{"cell_type":"code","source":"# getting all the rows of dataset with Sex as female \n\ndf[df['Sex'] == 'female'].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting specific columns with condition Sex == female \n\ndf[df['Sex'] == 'female'][[\"Fare\",'Age','Name']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting all the columns with age < 18  and sex as male to check get all info about the male kids onboard\n\ndf[(df['Age'] < 18) & (df['Sex'] =='male')].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### accessing the elements in the columns\n\nBased on conditions and index","metadata":{}},{"cell_type":"code","source":"# we can access the elements in the columns by adding one more square bracket next to it.\n\n# we can eighter pass index in the next square bracket or\nprint(df['Age'][10:20])\n\n# we can pass condition. the condition can be related to other columns in df as well. \nprint(df['Age'][df['Sex'] =='male'])\n\n# other way is to use loc method\n# here after df.loc we used the condition and then pass the column name we want values from.\nprint(df.loc[df['Age'] > 18 , 'Fare']) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding columns to dataset","metadata":{}},{"cell_type":"markdown","source":"We won't be learning adding rows , as we will most probably adding the columns ( new features ) not rows ( observations). ","metadata":{}},{"cell_type":"code","source":"# quickly checking features we have  in data set with \ndf.columns ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding the same value to all the rows of new columns \n# we have added the new column 'Adult' with 0 to all rows.\n\ndf['Adult'] = 0\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will correct the elements in Adult columns. \n# if the age of person is above 18 he is adult mean true and True in python is 1\n# using condition to filter the elements in columns with age \n\ndf.loc[df['Age'] > 18, 'Adult'] = 1\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### renaming the column names ","metadata":{}},{"cell_type":"code","source":"# rename() function - as argument use columns = {} dictionary as key old names and values new names\n\ndf.rename(columns = {'SibSp' : 'Siblings'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring some methods of dataframes","metadata":{}},{"cell_type":"code","source":"# unique - applied on columns to check unique elements in the column\ndf['Embarked'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nunique - number of unique elements in the column\ndf['Sex'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# value_counts() - return count of each unique element in colums\ndf['Pclass'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set_index(\"colname\") - set the column we want as index of dataset\n# inplace = True - to make changes to original dataset\ndf.set_index('PassengerId', inplace = True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reset_index() - reset the index to original form 0 - 890\ndf.reset_index(inplace = True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort_values(by = 'colname') - sort the dataset based on column\ndf.sort_values(by = 'Fare', ascending = True).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# isnull() return True or False value of dataset , we use isnull().sum() to get sum of all nan values columns wise\ndf.isnull().sum()  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similarly we also have notnull()\ndf.notnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropna() - drop all the na values columns wise or row wise depending on aixs\n# here axis 0 is row and 1 is column, default axis is row\ndf.dropna()          # to drop all rows with even one nan value \ndf.dropna(axis = 1)  # to drop all columns with even only one nan values \ndf.dropna(how = 'all') # only drop the row if all the element in row are nan\ndf.dropna(how = 'any') # drop the row if any element is nan ( its default)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fillna() - filling na values with specific value it  can be anything.\n\ndf['Embarked'].fillna(value = \"S\", inplace = True)\ndf.isna().sum() # checking if empbarked has null values anymore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fillna() filling na values with some meaningful value. like mean , median , mode etc. \n\ndf['Age'].fillna(value = df['Age'].mean(), inplace = True)\ndf.isna().sum()  # checking if age column has nan value now","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Groupby function","metadata":{}},{"cell_type":"code","source":"# grouping the dataset based on features and performing operations on them.\n\ngender = df.groupby('Sex') # created a group based on Gender\n\nprint(gender.sum()) # performating operation on group object\nprint('\\n')\nprint(gender.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# another example of groupby with multilevel groups, with gender and adult or not. \n\ngroup_gender_adult = df.groupby(['Sex','Adult'])\n\ngroup_gender_adult.sum()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map() - apply the function element wise\n\ndf['Age'].map(lambda x : x/2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply() - apply the function columns wise \n\ndf['Fare'].apply(lambda x : x+x*0.12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if columns are numerical or not using applymap()\n\ndf.applymap(np.isreal).sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making pivot table using pandas\n\npd.pivot_table(df,values = 'Age', index = 'Sex', columns = 'Survived', aggfunc= 'count')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some of the Series function applied on the Dataframe columns  ","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idxmax() - return the index of the maximum value in the column\nind = df['Fare'].idxmax()\nprint(ind)\nprint(df.loc[ind,'Fare'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idxmin() - return the index of the minimum value in the column\nind2 = df['Age'].idxmin()\nprint(ind2)\nprint(df.loc[ind2,'Age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlargest() - return largest numbers\ndf['Age'].nlargest() # default return top 5 \ndf['Age'].nlargest(3) # with argument number of output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nsmallest() - return smallest numbers\ndf['Fare'].nsmallest() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exporting the files in pandas\n\nProvide the full path to store at specific location otherwise it will be stored at the Current directory","metadata":{}},{"cell_type":"code","source":"# To csv file\ndf.to_csv('Titanic_output.csv', index = False)\n\n# to excel file\ndf.to_excel('Titanic_output.xlsx', sheet_name = 'new')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}