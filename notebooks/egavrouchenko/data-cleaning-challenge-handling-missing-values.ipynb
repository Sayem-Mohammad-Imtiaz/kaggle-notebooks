{"cells":[{"metadata":{"_cell_guid":"b91a74ba-85f4-486e-b5f9-d0898f0626bf","_uuid":"6ac53f18b4f4ec0fc44348cedb5d1c319fa127c0"},"cell_type":"markdown","source":"### All days of the challange:\n\n* [Day 1: Handling missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values)\n* [Day 2: Scaling and normalization](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data)\n* [Day 3: Parsing dates](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/)\n* [Day 4: Character encodings](https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/)\n* [Day 5: Inconsistent Data Entry](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/)\n\nHere's what we're going to do today:\n\n* [Take a first look at the data](#Take-a-first-look-at-the-data)\n* [See how many missing data points we have](#See-how-many-missing-data-points-we-have)\n* [Figure out why the data is missing](#Figure-out-why-the-data-is-missing)\n* [Drop missing values](#Drop-missing-values)\n* [Filling in missing values](#Filling-in-missing-values)\n\nLet's get started!"},{"metadata":{"_cell_guid":"135a7804-b5f5-40aa-8657-4a15774e3666","_uuid":"835cbe0834b935fb0fd40c75b9c39454836f4d5f","trusted":true,"collapsed":true},"cell_type":"code","source":"# modules we'll use\nimport pandas as pd\nimport numpy as np\nimport math\n# read in all our data\nsf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")\nusa_zipcodes= pd.read_csv(\"../input/usa-zip-codes-to-locations/US Zip Codes from 2013 Government Data.csv\")\n# set seed for reproducibility\nnp.random.seed(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b42981e20863b3885ee5e0219efecaf6e8a93b","collapsed":true},"cell_type":"code","source":"from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b24e0d35239faaff1a6c2f32f93e669e8323d63","collapsed":true},"cell_type":"code","source":"\n#lat, long \ndef haversine(lat1,long1, lat2,long2):\n    radius = 6371 # km\n    dlat = math.radians(lat2-lat1)\n    dlon = math.radians(long2-long1)\n    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n    d = radius * c\n    return d\n\ndef findZipcodeForLocation (lat1, long1, zipcodes):\n    minDistance =10^400\n    zipCode=0\n    for i in range(len(zipcodes)):\n        currDistance = haversine(lat1,long1, zipcodes.iloc[i,1], zipcodes.iloc[i,2])\n        if currDistance < minDistance : \n            minDistance = currDistance\n            zipCode = zipcodes.iloc[i,0]\n    return int(round(zipCode))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cab7c9c09c5c5b67328c35d253df521aec79bd7","collapsed":true},"cell_type":"code","source":"sf_zipcode = pd.read_csv(\"../input/sf-zipcodes-limited/SFZ.csv\")\nsf_zipcode.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ea8f18028c5e2f2c93f077d233f5cfbdca2f34d","collapsed":true},"cell_type":"code","source":"print (\"Number of rows in sf zipcodes: %d \\n\" % sf_zipcode.shape[0] )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09b58d03-d34d-497a-b298-12a0ae962e3d","_uuid":"53c84bf86149ac41b237633a1a79d6130d6a2cd4"},"cell_type":"markdown","source":"The first thing I do when I get a new dataset is take a look at some of it. This lets me see that it all read in correctly and get an idea of what's going on with the data. In this case, I'm looking to see if I see any missing values, which will be reprsented with `NaN` or `None`."},{"metadata":{"_cell_guid":"604ac3a4-b1d9-4264-b312-4bbeecdeec00","_uuid":"03ce3b4afe87d98f777172c2c7be066a66a0b237"},"cell_type":"markdown","source":"Yep, it looks like there's some missing values. What about in the sf_permits dataset?"},{"metadata":{"_cell_guid":"8dca377c-95be-40ec-87dc-61a8fca750e2","_uuid":"e389495bb2e5d27ab632d5f3648ca1f912c94706","trusted":true,"collapsed":true},"cell_type":"code","source":"# your turn! Look at a couple of rows from the sf_permits dataset. Do you notice any missing data?\nsf_permits.sample(5)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31daa324-9215-4930-985c-01dee717b6b8","_uuid":"3331fa42efa16f3db2e8e196411f351c5f8309f5"},"cell_type":"markdown","source":"Wow, almost a quarter of the cells in this dataset are empty! In the next step, we're going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them."},{"metadata":{"_cell_guid":"f20a9474-41ee-4ecd-a2f4-1ab147fc8655","_uuid":"64487760aa1afaaa8b8a4d1f95206773759db101","trusted":true,"collapsed":true},"cell_type":"code","source":"# your turn! Find out what percent of the sf_permits dataset is missing\nsf_missing_values_count = sf_permits.isnull().sum()\n# look at the # of missing points in the first ten columns\nsf_missing_values_count[0:10]\nsf_total_cells = np.product(sf_permits.shape)\nsf_total_missing = sf_missing_values_count.sum()\n\n# percent of data that is missing\n(sf_total_missing/sf_total_cells) * 100","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62b9f021-5b80-43e2-bf60-8e0d5e22d572","_uuid":"032a618abb98a28e60ab84376cf21402178f995d"},"cell_type":"markdown","source":"# Figure out why the data is missing\n\n> **Is this value missing becuase it wasn't recorded or becuase it dosen't exist?**\n\nIf a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called \"imputation\" and we'll learn how to do it next! :)\n\nLet's work through an example. Looking at the number of missing values in the nfl_data dataframe, I notice that the column `TimesSec` has a lot of missing values in it: "},{"metadata":{"_cell_guid":"f417c614-f77f-45eb-b16b-fdc0be936502","_uuid":"bac84fee4ca849e54839c716c43dddfbb559954b"},"cell_type":"markdown","source":"We've lost quite a bit of data, but at this point we have successfully removed all the `NaN`'s from our data. "},{"metadata":{"_cell_guid":"1dbe153d-7b30-4ad8-80ad-a4c7fb53928e","_uuid":"eb1ef8d47d9ebed77c3d21eca24708708ed4d45f"},"cell_type":"markdown","source":"# Filling in missing values automatically\n_____\n\nAnother option is to try and fill in the missing values. For this next bit, I'm getting a small sub-section of the NFL data so that it will print well."},{"metadata":{"_cell_guid":"1103b725-c823-4f40-9bda-e97997856339","_uuid":"bec603202c6bfaae7a49b4a4042f37019ad1d801"},"cell_type":"markdown","source":"I could also be a bit more savvy and replace missing values with whatever value comes directly after it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)"},{"metadata":{"_cell_guid":"980e5d67-7e9c-41a3-b17e-51d87e9da9cf","_uuid":"1f8ac8b52f2933612e315f06a53185e164e6c5bc"},"cell_type":"markdown","source":"Filling in missing values is also known as \"imputation\", and you can find more exercises on it [in this lesson, also linked under the \"More practice!\" section](https://www.kaggle.com/dansbecker/handling-missing-values). First, however, why don't you try replacing some of the missing values in the sf_permit dataset?"},{"metadata":{"trusted":true,"_uuid":"8b04dc0f776fafc771cc1d33b4f444debdc999ee","collapsed":true},"cell_type":"code","source":"sf_permits.loc[:, 'Neighborhoods - Analysis Boundaries':'Zipcode'].sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98b1eb6894a8a30cf6d9cb0d6015d1cf4f37d435","collapsed":true},"cell_type":"code","source":"sf_permits.rename(columns={'Neighborhoods - Analysis Boundaries': 'Neighborhood'}, inplace=True)\nsf_nhoods = sf_permits['Neighborhood'].unique()\n#sf_nhoods.sort()\nsf_nhoods","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4be3d7ae650b9fa92540c0eadee2bd3150556fb1","collapsed":true},"cell_type":"code","source":"\nsf_zipcode.drop([\"Unnamed: 2\",\"Unnamed: 3\"], axis=1, inplace=True)\nsf_zipcode.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e351847afe5f0943e2b4c1b80271c228f3d73cb","collapsed":true},"cell_type":"code","source":"sfz= sf_zipcode['Neighborhood'].unique()\nsfz.sort()\nsfz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c960ff380eb63de3beda3b83cd7ec049df964fed","collapsed":true},"cell_type":"code","source":"sf_zipcode.Neighborhood = sf_zipcode.Neighborhood.replace(\"\\xa0\", \"\", regex=True)\nsfzz = sf_zipcode.Neighborhood.unique()\nsfzz.sort()\nsfzz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd101b3f19c81e914200db18efea5191a3f4b294","collapsed":true},"cell_type":"code","source":"sf_permits.Neighborhood.isna().sum()\nsf_permits.Zipcode.isna().sum()\nsf_permits_nn=sf_permits.query('Neighborhood.isnull() and Zipcode.isnull()', engine='python')\nsf_permits_nn.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"980aee1a5d979e17b690dfa7376d31e3458d9fdd","collapsed":true},"cell_type":"code","source":"sf_permits[['LAT','LNG']] = sf_permits.Location.str.split(',', expand = True)\nsf_permits.LAT= sf_permits.LAT.str.replace('(','')\nsf_permits.LNG= sf_permits.LNG.str.replace(')','')\nsf_permits.LNG.sample(10)\nsf_permits.LAT.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b88ec87f78cd5542a829b7c9defbdcc573be5f04","collapsed":true},"cell_type":"code","source":"sf_permits.Zipcode.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95d002964f80346a85150097a75869614a2dacc2","scrolled":true,"collapsed":true},"cell_type":"code","source":"sfz_unique= pd.DataFrame(sf_zipcode.Zipcode.unique())\nsfz_unique.columns =['Zipcode']\nsfz_unique.shape[0]\n#sfz_unique.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea8b2d1f54502231df5dfd098b810cf269725b5c","collapsed":true},"cell_type":"code","source":"sfzz_unique =pd.DataFrame(sf_permits.Zipcode.unique())\nsfzz_unique.columns =['Zipcode']\nsfzz_unique.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"def994d022531ee788211192c2eee9995006c927","collapsed":true},"cell_type":"code","source":"#ca_zipcodes=usa_zipcodes[(usa_zipcodes['ZIP']>=90001) & (usa_zipcodes['ZIP'] <=96162)]\ncaf_zipcodes = pd.merge(usa_zipcodes,sfz_unique, left_on =['ZIP'], right_on=['Zipcode'],how='inner')\ncaf_zipcodes.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"457d609979213c9f9014af1dbe0053f6794c011a","collapsed":true},"cell_type":"code","source":"ca_zipcodes.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bd32716114a06c8479836efc9935615d34bf30c","collapsed":true},"cell_type":"code","source":"# Find the missing zip codes from location column (now split by LAT and LNG) using \n# USA Zip code dataset filtered by California zip codes\nsf_permits.sample(5)\nsf_permits['Zipcode'] = sf_permits.where(sf_permits['Zipcode'].isna()).apply(lambda row: findZipcodeForLocation(row['LAT'],row['LNG'],ca_zipcodes), axis=1)\nsf_permits.Zipcode.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da426397-7e17-40ce-a0d4-ca6d39e47498","_uuid":"f7d403c19eaf31ee0a4e04b9e1119eda96a9f95c","trusted":true,"collapsed":true},"cell_type":"code","source":"\nsff_permits = sf_permits.fillna(method = 'bfill', axis=0).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b347414ee0b541bc919357493c01165e1b0060e0","collapsed":true},"cell_type":"code","source":"n_m_values_count = sff_permits.isnull().sum()\nn_cols = sff_permits.shape[1]\nn_m_values_count[0:n_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90f9d499748909c8ba7dc35e9970c31f56d02669","collapsed":true},"cell_type":"code","source":"zsf_missing_values_count = sff_permits.isnull().sum()\n# look at the # of missing points in the first ten columns\nzsf_total_cells = np.product(sf_permits.shape)\nzsf_total_missing = zsf_missing_values_count.sum()\n# percent of data that is missing\n(zsf_total_missing/zsf_total_cells) * 100","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4f37fce-4d08-409e-bbbd-6a26c3bbc6ee","_uuid":"52b0af56e3c77db96056e9acd785f8f435f7caf5"},"cell_type":"markdown","source":"And that's it for today! If you have any questions, be sure to post them in the comments below or [on the forums](https://www.kaggle.com/questions-and-answers). \n\nRemember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the \"Commit & Run\" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also let's you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the \"Commit & Run\" button) and setting the \"Visibility\" dropdown to \"Public\".\n\n# More practice!\n___\n\nIf you're looking for more practice handling missing values, check out these extra-credit\\* exercises:\n\n* [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values): In this notebook Dan shows you several approaches to imputing missing data using scikit-learn's imputer. \n* Look back at the `Zipcode` column in the `sf_permits` dataset, which has some missing values. How would you go about figuring out what the actual zipcode of each address should be? (You might try using another dataset. You can search for datasets about San Fransisco on the [Datasets listing](https://www.kaggle.com/datasets).) \n\n\\* no actual credit is given for completing the challenge, you just learn how to clean data real good :P"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}