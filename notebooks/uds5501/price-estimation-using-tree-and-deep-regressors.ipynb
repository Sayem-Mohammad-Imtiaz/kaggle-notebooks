{"cells":[{"metadata":{"_uuid":"1e924bf72a42f73483330fec577233d661ef6c9d"},"cell_type":"markdown","source":"# Price Estimations Using Multiple Regressors\n\nWelcome to the price estimation kernel by Uddeshya Singh\n\n![](https://aia.es/wp-content/uploads/2012/10/price_estimation.jpg)\n\nI will be covering:\n* Basic EDA\n* Feature Engineering\n* Deep Learning Modelling\n* Benchmarking Scheme\n\nAll for the **Total Price** of a particular customer"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input\"))\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\nsns.set_palette('muted')\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df = pd.read_csv('../input/WA_Fn-UseC_-Telco-Customer-Churn.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e3df0b33981dd84f58269b731e19375775fbe7b"},"cell_type":"markdown","source":"# The Dataset!"},{"metadata":{"trusted":true,"_uuid":"b96872f321fe87cf90968e3b386e61e212e95a3e","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ee5a86e7f921b178bfc4edb296e82f143cdd9b0"},"cell_type":"markdown","source":"# Exploratory Data Analysis\nLet's go through various visualisations to have a deeper grasp at what our data really represents!"},{"metadata":{"trusted":true,"_uuid":"af70f50642b16e2f8f02e84db3e530906af4dd8d","_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"sns.countplot(x = 'SeniorCitizen', data = df, hue = 'gender')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c667f5cf36c0ba7eee7f3212736f3ca2a5ef4f9"},"cell_type":"markdown","source":"As one might guess, 0 represents that the customer is not a senior citizen . Hence, one may infer that only about 14% of the customers are actually 60+. A pretty classic scenario in current generation."},{"metadata":{"trusted":true,"_uuid":"8ca00b3d0d2d906f2a9f843c16a74948a64e84a4","collapsed":true},"cell_type":"code","source":"sns.countplot(x = 'Contract', data = df, hue = 'gender')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2e2fed9011d1346ca07935db2c9380c5f78d84e"},"cell_type":"markdown","source":"Another inference that might be drawn from the Contract's count plot that **Month-to-Month** plans are best served and preferred among the consumers. To attract retentivity, one may think about offering **Free Subscriptions and premium support** for the first month as a trial!"},{"metadata":{"trusted":true,"_uuid":"b5bfa7af8c8036d1e925413e8299941e3ed7e07e","collapsed":true},"cell_type":"code","source":"sns.countplot(x = 'DeviceProtection', data = df, hue = 'gender')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7db4da053a379fc85d6e481f5a85c6602d0a5ca9"},"cell_type":"markdown","source":"Looking at the scenario, not many customers prefer Device Protection If offered. But for the sake of business, the company may leverage into offering a wider internet access and there maybe a **30-40%** chance of getting a new preemium customer who may opt for Device Protection. Similar scenarios can be scene in Tech Support too!"},{"metadata":{"_uuid":"3c5ed339818e97587e115c3bf3005ae3054ef3fe"},"cell_type":"markdown","source":"# Feature Engineering\n\nNow let's move on to the feature engineering section. The motive of this part is to make sure that we convert our features in algorithm processable quantities.\n\nWhat I will be doing is mostly applying* One Hot Encodings in the columns with >2 unique values* and explicitly change **Gender** Column to match the 0 and 1 categories."},{"metadata":{"trusted":true,"_uuid":"7d2b9762da5db86c6b482865400c55ad6119bad8","collapsed":true},"cell_type":"code","source":"# Categorizing Male and Female in 1s and 0s\ndef gender_labels(element):\n    if element == 'Male':\n        return 0\n    elif element == 'Female':\n        return 1\n# Making a new column in the dataframe\ndf['GenderLabel'] = df['gender'].apply(gender_labels)\n\n#Dropping the original gender column\ndf.drop(['gender'] ,axis = 1, inplace=True)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d0a3e5baf440615a3fbce1b4b4eaa85abcb1ed4"},"cell_type":"markdown","source":"With the step one of feature engineering done, let's move on to code our utility to put **binary labels** on the columns with unique values of **Yes, No and No Internet**!"},{"metadata":{"trusted":true,"_uuid":"0cb17b4425fa386cf0d80f89bdd867946fe40067","collapsed":true},"cell_type":"code","source":"# Now, to relable the columns which have just \"Yes\" and \"No\" as their entries!\nlistOfColumns = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'TechSupport', 'Churn', 'StreamingMovies', 'StreamingTV', 'DeviceProtection', 'PaperlessBilling']\n\n# The Labelling Function\ndef Labelizer(input_value):\n    '''Returns 1 for a Yes and a 0 for any other No'''\n    if input_value == 'Yes':\n        return 1\n    else:\n        return 0\n    \nfor i in listOfColumns:\n    newCol = i+'_label'\n    df[newCol] = df[i].apply(Labelizer)\n\ndf.drop(listOfColumns, axis = 1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75ce82e45e52a1b13bab9ce186cf5e73f036ba89","collapsed":true},"cell_type":"markdown","source":"With the binary operations all done, its time to fix our remaining columns with **One Hot Encoding** utility"},{"metadata":{"trusted":true,"_uuid":"6ce7f133ce0f827c5bc8775b0334254b4e2cbdd7","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"list_nonBinary = ['Contract', 'PaymentMethod', 'InternetService']\nfor i in list_nonBinary:\n    df = pd.concat([df, pd.get_dummies(df[i])], axis = 1)\n    df.drop([i], axis = 1, inplace=True)\n\n#print(\"Post feature Engineering, the columns are as follows : \", df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5dc54764db3755b28526738e61346c6e3e3a99f"},"cell_type":"markdown","source":"## The Total Charges Dilemma\nOne thing which I shoud mention here is that I couldn't really fix the total charges columns to the datatype of **float** by the convention *.astype*.\nSo I confirmed to the maneover mentioned below to get through that errors!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2dc15c72149bda11e064255ca68b579762bad18c"},"cell_type":"code","source":"df['TotalChargesNew'] = df['tenure']*df['MonthlyCharges']\ndf.drop(['MonthlyCharges', 'TotalCharges'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"751e8aa0da4dca561f5795fc25fd9045d86ca683","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4972fdbb98c7f01d540bc80ea969bef45cd64579","collapsed":true},"cell_type":"code","source":"#Dropping the Customer ID for obvious reasons\ndf.drop(['customerID'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"196fdfae9486d83df57fc6c1b072d8fa752d7b54"},"cell_type":"markdown","source":"# Simple Train Test split"},{"metadata":{"trusted":true,"_uuid":"5d7ac337dd6a81ac0bd19baf1a696b1b2cf5f78c","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = df['TotalChargesNew']\nX = df.drop(['TotalChargesNew'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec4e2d543a4d323f8c1d5d81bda6456fe6e479ba"},"cell_type":"markdown","source":"# Model Making\n\nI will be using the following models to checkout the estimation performance!\n1. Decision Tree Regressor\n2. XGBoost Regressor\n3. Linear Regressor\n4. Gradient Boost Regressor\n5. A Naive Deep Learning Model"},{"metadata":{"_uuid":"7b24278cd68244269c4c21a2028b7bb6d0910fe8"},"cell_type":"markdown","source":"# Decision Tree Regressor"},{"metadata":{"trusted":true,"_uuid":"50335cb0afdad7643a430cc8240fcc3c7d8be7ff","collapsed":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nmyRegressor = DecisionTreeRegressor(criterion='mse')\nmyRegressor.fit(X_train, y_train)\nprediction = myRegressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"88c0c84277462f92398793747dfd271c841397c5"},"cell_type":"markdown","source":"Checking out our predicitons along side by making a simple dataset "},{"metadata":{"trusted":true,"_uuid":"781f52852003247c59a57f3f0b9df6be7b6ca4d7","collapsed":true},"cell_type":"code","source":"final_df_Decision = pd.DataFrame({'Predictions':prediction, 'True' : y_test})\nfinal_df_Decision.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"98c36a216f4b0cb18588a9eba03652ab86119cdd"},"cell_type":"markdown","source":"# The Curves matter\nAn excellent curve fitting result of first 250 test values and seeing how our model fares against them"},{"metadata":{"trusted":true,"_uuid":"fc9b507ba72dae19e7f137ce20014b50a4f14be3","collapsed":true},"cell_type":"code","source":"Prediction_Line = go.Scatter(\n    x = [i for i in range(250)],\n    y = prediction[:250]\n)\nActual_Line = go.Scatter(\n    x = [i for i in range(250)],\n    y = y_test.values[:250]\n)\n\ndata = [Prediction_Line, Actual_Line]\niplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3631956e731727e2887674fe6703c15c9e929039","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nprint(\"Decision Tree metrics are about accurate to %.2f dollars (+ and -)\"% (mean_squared_error(prediction, y_test)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a37362d91d4cb2cd66754f70e48acd8829bbfd8"},"cell_type":"markdown","source":"# Linear Regression"},{"metadata":{"trusted":true,"_uuid":"9a542c24ff09c86c3a1fa7fb981030cd237d8a6d","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nclf2 = LinearRegression()\nclf2.fit(X_train, y_train)\npreds2 = clf2.predict(X_test)\n\nprint(\"RMSE Score for Linear Regressor is %.2f\"%(mean_squared_error(y_test, preds2))**0.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4368d7befa135f89e6e4aef626af3e28b476d6ab","collapsed":true},"cell_type":"markdown","source":"# Gradient Boost Regressor"},{"metadata":{"trusted":true,"_uuid":"41862f3a4096efebfc22cc62cfd2b2078720ef16","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nclf3 = GradientBoostingRegressor()\nclf3.fit(X_train, y_train)\npreds3 = clf3.predict(X_test)\n\nprint(\"RMSE Score for Gradient Boost Regressor is %.2f\"%(mean_squared_error(y_test, preds3))**0.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cac683c21b82e7d390c94874bfc9e150a1473e57"},"cell_type":"markdown","source":"# XGBoost Regressor"},{"metadata":{"trusted":true,"_uuid":"753c3dbc0c84695ed977ac072601b34c03fae8f1","collapsed":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nclf4 = XGBRegressor()\nclf4.fit(X_train, y_train)\npreds4 = clf4.predict(X_test)\n\nprint(\"RMSE Score of XGBoost Regressor is %.2f\"%(mean_squared_error(y_test, preds4))**0.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"037064aa11fe8fa3481acc4067ccba28857bdab1"},"cell_type":"markdown","source":"# DeepLearning Model Design!\n\nThe following is the model design of our naive model which we will be testing our dataset upon"},{"metadata":{"trusted":true,"_uuid":"84c69c6476d2021fc55142fd21e284dfcb2a360f","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import (Dense, Dropout, BatchNormalization)\n\nmodel = Sequential()\nmodel.add(Dense(25, input_dim = 25, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.9))\n\nmodel.add(Dense(12, activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(6, activation='relu'))\nmodel.add(Dropout(0.9))\n\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss= 'mse')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3850df4eb9b8d44b18f475d851c409fb9e6c5f3","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"model.summary()\nmodel.lr = 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5e2fc4c77722c4f183cd8f928b3990374eba1b0","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=1000, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a095ab2b379124dd6e9c4b1a9cdee838f3c5c85e"},"cell_type":"markdown","source":"# Benchmarks\n\nHave a look at the benchmark performances and decide for yourselves, which is the best regressor and whom you are going to opt!"},{"metadata":{"trusted":true,"_uuid":"c38b8443aa74b9e34ca21632544d57538a8c8ce8","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"benchmarks = pd.DataFrame({\"Naive Deep Learning Model\" : (sum(history.history['loss'])/len(history.history['loss']))**0.5,\n                          \"XG Boost Regressor\" : mean_squared_error(y_test, preds4)**0.5,\n                          \"Gradiant Boost Regressor\" : mean_squared_error(y_test, preds3)**0.5,\n                          \"Decision Tree Regressor\" : mean_squared_error(y_test, prediction)**0.5,\n                          \"Linear Regressor\" : mean_squared_error(y_test, preds2)**0.5\n                          }, index = range(1)).T\nbenchmarks.columns=['RMSE']\nbenchmarks['Regressor'] = benchmarks.index\nbenchmarks.index = range(5)\nbenchmarks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1af0ebb3d07bf5f407a42640e191fa00d38eb628","collapsed":true},"cell_type":"markdown","source":"# Note : The RMSE score is almost equivalent to the actual tolerance of the prediction.\n\n## As you may have noticed by now, XG Boost and Gradient Boost comes out on top with an approximate error of 111 dollars!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cc3fcd666641df7b0d0eef6403daacf8b00306ee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}