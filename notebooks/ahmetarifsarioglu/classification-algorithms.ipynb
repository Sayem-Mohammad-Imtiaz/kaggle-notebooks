{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel has been prepared for practicing classification algorithms with the data of \"biomechanical features of orthopedic patients\"."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EXPLORATORY DATA ANALYSIS**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-NEAREST NEIGHBOURS (KNN) CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nx,y = data.loc[:,data.columns!='class',],data.loc[:,'class']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\"With KNN (k=3) accuracy is: \", knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model complexity\nneig = np.arange(1,25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    train_accuracy.append(knn.score(x_train,y_train))\n    test_accuracy.append(knn.score(x_test,y_test))\n# Plot\nplt.figure(figsize=(13,8))\nplt.plot(neig, test_accuracy, label = \"Testing Accuracy\")\nplt.plot(neig, train_accuracy, label = \"Training Accuracy\")\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K={}\". format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SUPPORT VECTOR MACHINE CLASSIFICATION (SVM)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"class\"] = [1 if each ==\"Abnormal\" else 0 for each in data[\"class\"]]\ny = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nx = ((x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of svm algo: \", svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NAIVE-BAYES CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"test score with naive bayes: \", nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DECISION TREE CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"test score with decision tree: \", dt.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RANDOM FOREST CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(x_train,y_train)\nprint(\"test score with random forest classification: \", rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgbc = XGBClassifier()\nxgbc.fit(x_train, y_train)\nprint(\"test score with XGBoost: \", xgbc.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CatBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\ncbc = CatBoostClassifier()\ncbc.fit(x_train, y_train)\nprint(\"test score with CatBoost: \", cbc.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LOGISTIC REGRESSION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test score with logistic regression: \", lr.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ARTIFICIAL NEURAL NETWORK (ANN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = x.values   # ann needs array as input\nprint(type(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=3, activation='relu')) \nann.add(tf.keras.layers.Dense(units=3, activation='relu'))  \nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nann.fit(X_train, y_train, batch_size = 32, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONFUSION MATRIX**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We get the highest accuracy with Random Forest Classification which is %87.\n#Lets evaluate Random Forest prediction with confusion matrix\nfrom sklearn.ensemble import RandomForestClassifier \nrf = RandomForestClassifier(n_estimators=100, random_state=1) \nrf.fit(x_train,y_train) \nprint(\"test score with random forest classification: \", rf.score(x_test,y_test))\n\ny_pred = rf.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix visualiation\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We made 6 wrong predictions for both abnormal and normal samples in data with Random Forest Classification."},{"metadata":{},"cell_type":"markdown","source":"**Applying k-Fold Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = rf, X = x_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying Grid Search to find the best model and the best parameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators':[50, 100, 200, 500], 'criterion': ['gini', 'entropy']}]\ngrid_search = GridSearchCV(estimator = rf, \n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(x_train, y_train)\nbest_accuracy = grid_search.best_score_   \nbest_parameters = grid_search.best_params_  \nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters: \", best_parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we apply apply grid-search with 10-fold cross validation, we get the best result if we choose gini as criterion and 200 as n_estimators.So we manage to reach an accuracy of 87.56%"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}