{"cells":[{"metadata":{},"cell_type":"markdown","source":"# one-year-industrial-component-degradation\n\nDataset:  https://www.kaggle.com/inIT-OWL/one-year-industrial-component-degradation"},{"metadata":{},"cell_type":"markdown","source":"### Note:\n**I'm new to Kaggle kernels, this notebook was created localy on jupyter then adapted for the kaggle environment, but alot of things got left out, such as how all the data was put in one dataframe, so here's my github repo for the original notebook**\n\nhttps://github.com/Iuryck/Machine_Degradation"},{"metadata":{},"cell_type":"markdown","source":"        Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nfrom sklearn.svm import OneClassSVM\nfrom numpy.random import seed\nfrom keras.layers import Input, Dropout\nfrom keras.layers.core import Dense \nfrom keras.models import Model, Sequential, load_model\nfrom keras import regularizers\nfrom keras.models import model_from_json\nfrom scipy.special import softmax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Context\n##### This dataset contains the machine data of a degrading component recorded over the duration of 12 month total. It was initiated in the European research and innovation project IMPROVE.\n\n# Content\n###### The Vega shrink-wrapper from OCME is deployed in large production lines in the food and beverage industry. The machine groups loose bottles or cans into set package sizes, wraps them in plastic film and then heat-shrinks the plastic film to combine them into a package. The plastic film is fed into the machine from large spools and is then cut to the length needed to wrap the film around a pack of goods. The cutting assembly is an important component of the machine to meet the high availability target. Therefore, the blade needs to be set-up and maintained properly. Furthermore, the blade can not be inspected visually during operation due to the blade being enclosed in a metal housing and its fast rotation speed. Monitoring the cutting blades degradation will increase the machines reliability and reduce unexpected downtime caused by failed cuts.\n\n\n\n#### The 519 files in the dataset are of the format MM-DDTHHMMSS_NUM_modeX.csv, where MM is the month ranging from 1-12 (not calendar month), DD is the day of the month, HHMMSS is the start time of day of recording, NUM is the sample number and X is a mode ranging from 1-8. Each file is a ~8 second sample with a time resolution of 4ms that totals 2048 time-samples for every file."},{"metadata":{},"cell_type":"markdown","source":"#  DATA\n\n### --pCut::Motor_Torque -> Torque in nM\n\n### --pCut::CTRL_Position_controller::Lag_error -> Represent the instantaneous position error between the set-point from the path generator and the real current encoder position of the motor\n\n### --pCut::CTRL_Position_controller::Actual_position -> Cutting blade position in mm\n\n### --pCut::CTRL_Position_controller::Actual_speed -> Speed of the cutting blade\n\n### --pSvolFilm::CTRL_Position_controller::Actual_position -> Plastic film unwinder position in mm\n\n### --pSvolFilm::CTRL_Position_controller::Actual_speed -> Speed of the plastic film unwinder\n\n### --pSvolFilm::CTRL_Position_controller::Lag_error -> Represent the instantaneous position error between the set-point from the path generator and the real current encoder position of the motor\n\n### --pSpintor::VAX_speed -> VAX measurement of performance\n"},{"metadata":{},"cell_type":"markdown","source":"**First things first, we have info on the samples that aren't in our sample csv files. Also, all our samples are separated in different files. So I compiled it all together to get a big dataframe with everything we need as the One_year_compiled.csv.**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\nmain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's look for correlation between data. As you will see, the only things that look correlated is the Motor Torque, Blade Speed and the Blade Lag Error, also the VAX speed and wrapper speed. Excluding of course, month and sample_number, and all self correlations on the diagonal.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#heatmap of correlations from -1 to 1\nsns.heatmap(main_df.corr(), vmin= -1, vmax = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets drop some columns and flip our motor torque column values by multiplying it by -1. Just for some more visual understanding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n#Dropping nonsense columns for this proposal                                 (axis=1) = columns\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n#Flipping column values\nmain_df['pCut::Motor_Torque'] = main_df['pCut::Motor_Torque'] *-1\n#Heatmap\nsns.heatmap(main_df.corr(), vmin= -1, vmax = 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treating non-numerical data\n\n**In our dataset, the modes for the machine may influence in data patterns, so we need to transform that data from string to numerical classes. This function will do just that.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_non_numeric(df):\n    # Values in each column for each column\n    columns = df.columns.values\n    \n    for column in columns:\n        \n        # Dictionary with each numerical value for each text\n        text_digit_vals = {}\n        \n        # Receives text to convert to a number\n        def convert_to_int (val):\n            \n            # Returns respective numerical value for class\n            return text_digit_vals[val]\n        \n        # If values in columns are not float or int\n        if df[column].dtype !=np.int64 and df[column].dtype != np.float64:\n            \n            # Gets values form current column\n            column_contents = df[column].values.tolist()\n            \n            # Gets unique values from current column\n            unique_elements = set(column_contents)\n            \n            # Classification starts at 0\n            x=0\n            \n            for unique in unique_elements:\n                \n                # Adds the class value for the text in dictionary, if it's not there\n                if unique not in text_digit_vals:\n                    text_digit_vals[unique] = x\n                    x+=1\n            \n            # Maps the numerical values to the text values in columns \n            df[column] = list(map(convert_to_int, df[column]))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Approach\n\n**A few algorithms will be tested to see if we can get some info on the machine. We will use OneClass SVM and KMeans with 1 cluster to try clustering. After that we will try an Autoencoder to try and reproduce data based on the machines healthy condition.**\n\n**In all 3 cases we will grab a slice of the first rows and consider it as the Healthy State of the machine, then feed it to the algorithms. After that we will give the algorithms the entirety of the dataset and see how they perform on the rest of the data. Deviations, low scores, and high losses will be considered as anomalies to be studied.**"},{"metadata":{},"cell_type":"markdown","source":"# OneClass SVM approach\n\n**OneClass SVM is used for outlier detection, it tries to find 2 classes in the data, the \"normal\" class and the outliers. We will use the SVM to try and find outliers and anomalies.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grabbing the entire dataset\nmain_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n#Dropping columns with unwanted/irrelevant info for the algorithm\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n#Transforming modes into classified data\nmain_df = handle_non_numeric(main_df)\n\n#Passing our dataframe as our features\nX = main_df\n\n#Defining preprocessor for the data\nscaler = preprocessing.MinMaxScaler()\n#Preprocessing\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\n\n\n#Scaling\nX = preprocessing.scale(X)\n#Splitting the feature data for training data. First 200.000 rows.\nX_train = X[:200000]\n\n\n#Creating a fitting OneClass SVM\nocsvm = OneClassSVM(nu=0.25, gamma=0.05)\nocsvm.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predicting and classifying the dataset in anomalies and non-anomalies, then passing it to a dataframe.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf=main_df.copy()\ndf['anomaly'] = pd.Series(ocsvm.predict(X))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Saving Dataframe**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving Dataframe.\ndf.to_csv('Labled_df.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading into dataframe**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading into dataframe\ndf = pd.read_csv('../input/created/Labled_df.csv', index_col=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing anomalies**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting labled groups\nscat_1 = df.groupby('anomaly').get_group(1)\nscat_0 = df.groupby('anomaly').get_group(-1)\n\n# Plot size\nplt.subplots(figsize=(15,7))\n\n# Plot group 1 -labeled, color green, point size 1\nplt.plot(scat_1.index,scat_1['pCut::Motor_Torque'], 'g.', markersize=1)\n\n# Plot group -1 -labeled, color red, point size 1\nplt.plot(scat_0.index, scat_0['pCut::Motor_Torque'],'r.', markersize=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing scores for the whole dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a dataframe for the score of each data sample\nscore = pd.DataFrame()\n#Returning scores for the dataset\nscore['score'] = ocsvm.score_samples(X)\n\n#Plot size\nplt.subplots(figsize=(15,7))\n#Plotting\nscore['score'].plot()\n#Saving score dataframe\nscore.to_csv('SVM_Score.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inverted score moving mean**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,7))\n\n\n((score['score'].rolling(20000).mean())*-1).plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scat plot to see the score through the noise**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,7))\nplt.plot(score.index, score['score'],'r.', markersize=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KMeans approach\n\n**Kmeans approach will do the same thing as the OC-SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#------ Preparing features for training and future prediction -----\nmain_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\nmain_df = handle_non_numeric(main_df)\nX = main_df\n\nscaler = preprocessing.MinMaxScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\n\n\n\nX = preprocessing.scale(X)\n#-------------------------------------------------------------------\n\n\n#Percentage of the data that will be considered healthy condition\ntrain_percentage = 0.15\n#Integer value for the slice that will be considered healthy condition\ntrain_size = int(len(main_df.index)*train_percentage)\n#Grabbing slice for training data\nX_train = X[:train_size]\n\n\n#Defining KMeans with 1 cluster\nkmeans = KMeans(n_clusters=1)\n#Fitting the algorithm\nkmeans.fit(X_train)\n\n#Creating a copy of the main dataset\nk_anomaly = main_df.copy()\n\n#Dataframe now will receive the distance of each data sample from the cluster\nk_anomaly = pd.DataFrame(kmeans.transform(X))\n\n#Saving cluster distane into csv file\nk_anomaly.to_csv('KM_Distance.csv')\n\n#Plot\nplt.subplots(figsize=(15,7))\n\nplt.plot(k_anomaly.index, k_anomaly[0], 'g', markersize=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AutoEncoder approach\n\n**AutoEncodersare Neural nets that expandes and compresses data into higher and lower dimensions, then tries to recreate the data. The idea is that the autoencoder will understand the relation between the features and from that, it will recreate the exact data it was given.**\n\n**We will feed the algorithm the healthy state of the machine. As it tries to rebuild the rest of the data as the healthy state, reconstruction loss, difference between predicted machine data and real machine data, will be considered \"unhealthy\" state.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------- Preparing data for training --------------------------- \nmain_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\nmain_df = handle_non_numeric(main_df)\nX = main_df\n\nscaler = preprocessing.MinMaxScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\n\n\n\nX = preprocessing.scale(X)\n\n\ntrain_percentage = 0.15\ntrain_size = int(len(main_df.index)*train_percentage)\n\nX_train = X[:train_size]\n#----------------------------------------------------------------------------------\n\n\n\n#Seed for random batch validation and training\nseed(10)\n\n\n#Elu activatoin function\nact_func = 'elu'\n\n# Input layer\nmodel=Sequential()\n\n# First hidden layer, connected to input vector X. \nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform',\n                kernel_regularizer=regularizers.l2(0.0),\n                input_shape=(X_train.shape[1],)\n               )\n         )\n# Second hidden layer\nmodel.add(Dense(10,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n# Thrid hidden layer\nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\n# Input layer\nmodel.add(Dense(X_train.shape[1],\n                kernel_initializer='glorot_uniform'))\n\n# Loss function and Optimizer choice\nmodel.compile(loss='mse',optimizer='adam')\n\n# Train model for 50 epochs, batch size of 200 \nNUM_EPOCHS=50\nBATCH_SIZE=200\n\n#Grabbing validation and training loss over epochs\nhistory=model.fit(np.array(X_train),np.array(X_train),\n                  batch_size=BATCH_SIZE, \n                  epochs=NUM_EPOCHS,\n                  validation_split=0.1,\n                  verbose = 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting Validation loss and Training loss over the epochs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,7))\n\nplt.plot(history.history['loss'],'b',label='Training loss')\nplt.plot(history.history['val_loss'],'r',label='Validation loss')\nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Loss, [mse]')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will feed the algortihm the same training data, and make it try to reconstruct data. We will then see the distribution of the loss over the train data, further on we will use this distribution to determine some Thresholds.**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Reconstructing train data\nX_pred = model.predict(np.array(X_train))\n\n#Creating dataframe for reconstructed data\nX_pred = pd.DataFrame(X_pred,columns=main_df.columns)\nX_pred.index = pd.DataFrame(X_train).index\n\n#Dataframe to get the difference of predicted data and real data. \nscored = pd.DataFrame(index=pd.DataFrame(X_train).index)\n#Returning the mean of the loss for each column\nscored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n\n#plot\nplt.subplots(figsize=(15,7))\nsns.distplot(scored['Loss_mae'],\n             bins = 15, \n             kde= True,\n            color = 'blue');\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now to do the same thing but with all our data to see the loss over time, this will give us interesting data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Reconstructing full data\nX_pred = model.predict(np.array(X))\nX_pred = pd.DataFrame(X_pred,columns=main_df.columns)\nX_pred.index = pd.DataFrame(X).index\n\n#Returning mean of the losses for each column and putting it in a dataframe\nscored = pd.DataFrame(index=pd.DataFrame(X).index)\nscored['Loss_mae'] = np.mean(np.abs(X_pred-X), axis = 1)\n\n#Plot size\nplt.subplots(figsize=(15,7))\n\n\n#Saving dataframe\nscored.to_csv('AutoEncoder_loss.csv')\n\n#Plot\nplt.plot(scored['Loss_mae'],'b',label='Prediction Loss')\n\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\nplt.ylabel('Loss, [mse]')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Scatter plot for each algorithm scoring, to see through noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot size\nplt.subplots(figsize=(15,7))\n#Reading loss csv file\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n#Plot\nplt.plot(enc_loss.index,enc_loss['Loss_mae'], 'g.', markersize=1,label=\"AutoEncoder Loss\")\n#Labels and legends\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\n#Show plot\nplt.show()\n\nplt.subplots(figsize=(15,7))\nk_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nplt.plot(k_anomaly.index,k_anomaly['0'], 'g.', markersize=1,label=\"KM cluster Distance\")\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\nplt.show()\n\nplt.subplots(figsize=(15,7))\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nplt.plot(score.index,score['score'], 'g.', markersize=1,label=\"OCSVM score\")\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting each algorithm scoring together, with OCSVM flipped over 0.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot size\nplt.subplots(figsize=(15,7))\n\n#Reading each socring csv file\nk_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\n#Scaling data for vizualization\nk_distance = k_anomaly/k_anomaly.max()\nsvm_score = (score/score.max())*-1\n\nplt.plot(enc_loss.index,enc_loss['Loss_mae'], label=\"AutoEncoder Loss\")\nplt.plot(svm_score.index, svm_score['score'],label=\"OCSVM score\")\nplt.plot(k_distance.index,k_distance['0'], label=\"Kmeans Euclidean Dist\")\n\n\n\nplt.gca().legend(('AutoEncoder Loss','OCSVM score * -1','Kmeans Euclidean Dist'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looking for correlation between the algorithms**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading score files\nk_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\n# Dataframe to see correlation\ncorr = pd.DataFrame()\n\n#Passing score data to corr dataframe \ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\n#Seeing correlation\ncorr.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scatter plot with movving mean**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#---- Reading data and passing it to dataframe again ----- \n\nk_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n#---------------------------------------------------------\n\n\n#Plot size\nplt.subplots(figsize=(15,7))\n\n#Scatter plot of SVM score\nplt.plot(corr.index, corr['SVM_score'], 'g.', markersize=1, label = 'OCSVM_score')\n#Plotting moving mean of 1000 data points\nplt.plot(corr.index, corr['SVM_score'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\n#Legend\nplt.legend(loc='upper right')\n#Show\nplt.show()\n\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr.index, corr['KM_cluster_distance'], 'g.', markersize=1, label = 'KM_cluster_distance')\nplt.plot(corr.index, corr['KM_cluster_distance'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\n\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr.index, corr['AutoEnc_loss'], 'g.', markersize=1, label = 'AutoEnc_loss')\nplt.plot(corr.index, corr['AutoEnc_loss'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\n\n\nplt.legend(loc='upper right')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loss Distribution over training data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\n\n\n#Plot size\nplt.subplots(figsize=(10,7))\n#Hist plot of first 160.000 rows, 15 bins\nsns.distplot(corr['SVM_score'].head(160000), bins=15)\n#Show\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['KM_cluster_distance'].head(160000),bins=15)\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['AutoEnc_loss'].head(160000),bins=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loss distribution over entire dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\n\n\n\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['SVM_score'], bins=15)\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['KM_cluster_distance'],bins=15)\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['AutoEnc_loss'],bins=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will use the info on the Training Loss distribution to determine some thresholds for the graphs. Moving means will also be plotted.**\n\n**_Upper threshold = Highest values of training loss distribution_**\n\n**_Lower threshold = Lowest values of training loss distribution_**\n\n**_Highes density = Mode of training loss distribution_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\n\n\n#Creating an array for the thresholds to be plotted over the entire dataset\nlower_threshold = np.full((corr['SVM_score'].size, 1), 0)\nupper_threshold = np.full((corr['SVM_score'].size, 1), 18000)\nhigh_density_threshold = np.full((corr['SVM_score'].size, 1), 13250)\n\n#Plot size\nplt.subplots(figsize=(15,7))\n\n#Score Plot\nplt.plot(corr.index, corr['SVM_score'], 'k', markersize=1, label = 'OCSVM_score')\n#Moving mean plot\nplt.plot(corr.index, corr['SVM_score'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\n#Threshold plots\nplt.plot(corr.index, lower_threshold, label='Lower Threshold')\nplt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\nplt.plot(corr.index, high_density_threshold, label = 'Highest Density')\nplt.legend(loc='upper right')\n#Show\nplt.show()\n\n\nlower_threshold = np.full((corr['KM_cluster_distance'].size, 1), 1.2)\nupper_threshold = np.full((corr['KM_cluster_distance'].size, 1), 17.5)\nhigh_density_threshold = np.full((corr['KM_cluster_distance'].size, 1), 2.5)\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr.index, corr['KM_cluster_distance'], 'k', markersize=1, label = 'KM_cluster_distance')\nplt.plot(corr.index, corr['KM_cluster_distance'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.plot(corr.index, lower_threshold, label='Lower Threshold')\nplt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\nplt.plot(corr.index, high_density_threshold, label = 'Highest Density')\nplt.legend(loc='upper right')\nplt.show()\n\n\n\nlower_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0)\nupper_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.1)\nhigh_density_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.05)\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr.index, corr['AutoEnc_loss'], 'k', markersize=1, label = 'AutoEnc_loss')\nplt.plot(corr.index, corr['AutoEnc_loss'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.plot(corr.index, lower_threshold, label='Lower Threshold')\nplt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\nplt.plot(corr.index, high_density_threshold, label = 'Highest Density')\nplt.legend(loc='upper right')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scatter plot of the algorithm scores vs each other**"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\n\n\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr['KM_cluster_distance'],corr['SVM_score'],'b.',markersize=1 )\nplt.xlabel('KM')\nplt.ylabel('SVM')\nplt.show()\n\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr['AutoEnc_loss'],corr['SVM_score'],'b.' ,markersize=1 )\nplt.xlabel('Encoder')\nplt.ylabel('SVM')\nplt.show()\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr['AutoEnc_loss'],corr['KM_cluster_distance'],'b.' ,markersize=1 )\nplt.xlabel('Encoder')\nplt.ylabel('KM')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Continuing with Autoencoder\n\n**Though there is some visual anomaly simularity between the algorithms, the clustering algorithms give us much noise and not much to work on. On the other hand the autoencoder has a almost certain run to failure point. We can´t conclude with absolute certainty that components were changed after the highest loss peak, but it is much possible.**\n\n**Furthermore, we will now analyze the loss of the encoder by month, with the thresholds.**  "},{"metadata":{"trusted":true},"cell_type":"code","source":"k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nmain_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n\n\n\n#Passing encoder loss to main dataframe, to make it easier to separate by month\nmain_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n\n#Getting list of months\nmonths = main_df['month'].dropna().unique()\n\n#Looping through every month\nfor month in months:\n    #Grabbing the slice of the dataframe for each month \n    month_df = main_df.groupby('month').get_group(month)\n    \n    \n    # Array Thresholds\n    upper_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.1)\n    high_density_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.05)\n\n    #Plot\n    plt.subplots(figsize=(15,7))\n    plt.plot(month_df.index, month_df['AutoEnc_loss'], label=f'AutoEnc_loss month_{month}')\n    plt.plot(month_df.index, upper_threshold, label = 'Upper Threshold')\n    plt.plot(month_df.index, high_density_threshold, label = 'Highest Density')\n    plt.legend(loc='upper right')\n    plt.ylim(0,1.3)\n    \n    plt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will see Loss distribution by month**"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nmain_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n\n\nmain_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n\n\n\n\n\nmonths = main_df['month'].dropna().unique()\n\nfor month in months:\n    month_df = main_df.groupby('month').get_group(month)\n    \n    \n    \n    plt.subplots(figsize=(15,7))\n    sns.distplot((month_df['AutoEnc_loss']), bins=15).set_title(f'Month {month} Loss Distribution')\n    #X axis limits\n    plt.xlim([-1.2,1.2])\n    plt.show()\n\n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**An interesting way to see the anomaly is the tail of the distribution. As we could see in the plot by month, month 4 has the highest anomaly point and counts. It's also obervable the influence of that in the distribution of the loss. Maybe by looking at the Kurtosis of each month we can get more info.**\n\n**Kurtosis will tell us about the shape of the distribution. High kurtosis means that alot of data points have the same value, and that the tails or the standard deviation is realy small or non existent (in our case, many datapoints close to 0 means a nice condition of the machine). Low Kurtosis means we have lots of spread out datapoints, which gives the distribution wide fat tails, almost the size of it´s peak.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\nscore = pd.read_csv('../input/created/SVM_Score.csv')\nenc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nmain_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n\n\nmain_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n\n\nmonths = main_df['month'].dropna().unique()\n\nfor month in months:\n    month_df = main_df.groupby('month').get_group(month)\n    kurt = (month_df['AutoEnc_loss']).kurtosis()\n    print(f'Month {month} kurtosis = {kurt}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So months with low kurtosis are the months with more anomalies, which can tell us a little about the condition of the machine.**\n\n**We won't be moving on with this distribution analyses approach, for I personaly don't know much about distribution analyses. _But it maybe a very interesting approach to analyze the data_.** "},{"metadata":{},"cell_type":"markdown","source":"# Sensor detection\n\n**Now that we know where the machine has a problem, we will try to find which component/sensor is causing this disturbance in the autoencoder. For that we will train it again, and get it´s predictions and losses for each column to see which one has the highest contribution to the total loss.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\nmain_df = handle_non_numeric(main_df)\nX = main_df\n\nscaler = preprocessing.MinMaxScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\n\n\n\nX = preprocessing.scale(X)\n\n\ntrain_percentage = 0.15\ntrain_size = int(len(main_df.index)*train_percentage)\n\nX_train = X[:train_size]\n\nseed(10)\n\nact_func = 'elu'\n\n# Input layer:\nmodel=Sequential()\n# First hidden layer, connected to input vector X. \nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform',\n                kernel_regularizer=regularizers.l2(0.0),\n                input_shape=(X_train.shape[1],)\n               )\n         )\n\nmodel.add(Dense(10,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\nmodel.add(Dense(X_train.shape[1],\n                kernel_initializer='glorot_uniform'))\n\nmodel.compile(loss='mse',optimizer='adam')\n\n# Train model for 100 epochs, batch size of 10: \nNUM_EPOCHS=50\nBATCH_SIZE=200\n\nhistory=model.fit(np.array(X_train),np.array(X_train),\n                  batch_size=BATCH_SIZE, \n                  epochs=NUM_EPOCHS,\n                  validation_split=0.1,\n                  verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting and passing prediction to dataframe\nX_pred = model.predict(np.array(X))\nX_pred = pd.DataFrame(X_pred,columns=main_df.columns)\nX_pred.index = pd.DataFrame(main_df).index\n\n#Passing X from an array to a dataframe\nX = pd.DataFrame(X,columns=main_df.columns)\nX.index = pd.DataFrame(main_df).index\n\n#Dataframe where all the loss per columns will go\nloss_df = pd.DataFrame()\n\n#Dropping mode as it can't logically contribute to degredation\nmain_df.drop('mode',axis=1, inplace=True)\n\n#Iterating through columns\nfor column in main_df.columns:\n    #Getting the loss of the prediction for that column\n    loss_df[f'{column}'] = (X_pred[f'{column}'] - X[f'{column}']).abs()\n     \n    #Plotting the loss\n    plt.subplots(figsize=(15,7))\n    plt.plot(loss_df.index, loss_df[f'{column}'], label=f'{column} loss')\n    plt.legend(loc='upper right')\n    \n    plt.show()\n\n#Saving loss Dataframe\nloss_df.to_csv('AutoEncoder_loss_p_column.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will apply Softmax function to each row so we can get the percentage that each colunm contributes to the total loss. As for the sum of each row will give us 1.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sftmax_df = pd.read_csv('../input/created/AutoEncoder_loss_p_column.csv', index_col=0)\nsftmax_df = softmax(sftmax_df, axis=1)\nsftmax_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the percentage of each columns contribution to total loss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in sftmax_df.columns:\n    \n\n    plt.subplots(figsize=(15,7))\n    plt.plot(sftmax_df.index, sftmax_df[f'{column}'], label=f'{column} loss')\n    plt.legend(loc='upper right')\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we can plot a stack plot to better visualize the contribution of each column to the total loss. As you will see the Blades position contributes very much to the total loss on that peak we saw. We will look in closer to that slice.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,7))\n\n#Labels for stackbar plot\ndf_label = ['Torque', 'Cut lag','Cut speed','Cut position','Film position','Film speed','Film lag','VAX']\n\n#Stackbar plot\nplt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n              sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pSpintor::VAX_speed'],\n             labels = df_label)\n\nplt.legend(loc='upper center', ncol=8)\n\nplt.ylim(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,7))\n\ndf_label = ['Torque', 'Cut lag','Cut speed','Cut position','Film position','Film speed','Film lag','VAX']\n\n#Grabbing the slice where the larger anomaly is\nsftmax_df = sftmax_df[400000:600000]\n\nplt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n              sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pSpintor::VAX_speed'],\n             labels = df_label)\n\nplt.legend(loc='upper center', ncol=8)\n\nplt.ylim(0,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looks like the Lag Error for the blade also gives a big slice of the total loss. The possible explanation here is that the blade is worn, and for that, it's starting to deviate from the path the machine tries to trace for the blade when cutting the film.**\n\n**Now we will look into the distribution of the contribution of each column to teh total loss, Just to get a better grasp of which sensors are giving higher loss.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in sftmax_df.columns:\n    \n\n    plt.subplots(figsize=(15,7))\n    \n    sns.distplot(( sftmax_df[f'{column}']), bins=15).set_title(f'Contribution Distribution')\n    plt.xlim(0,1)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion \n\n**With all the info gathered we could tell where and when the machine suffered massive degradation. We could also tell which of the measurments contributes more to the loss on the whole year of the machine, which tells us these components made need more attention. Percentile thresholds, Distribution anomaly analyses, SVMs and much more other methods can be used for the detection of component wear over time with the info given here.**\n\n**A problem with this method is the need to preprocess and scale data entirely to be then given to the algorithm. A way to overcome this problem is to use this dataset or a slice of it and combine it with a new slice of data for system health analyses. And for each new slice of data that needs analyzing, we remove the previous added slice, and keep this dataset intact.** "},{"metadata":{},"cell_type":"markdown","source":"# References\n\n**-- https://www.sciencedirect.com/science/article/pii/S221282711830307X**\n\n**-- https://towardsdatascience.com/machine-learning-for-anomaly-detection-and-condition-monitoring-d4614e7de770**\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}