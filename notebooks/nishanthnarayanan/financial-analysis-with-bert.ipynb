{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing  import LabelEncoder\nle = LabelEncoder()\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport missingno as miss\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.offline import iplot\nimport cufflinks as cf\ncf.go_offline()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv\", encoding='latin-1', header = None)\ndf.columns = [\"Sentiment\", \"News Headline\"]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include = \"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss.bar(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for duplicates\n\nlen(df[df.duplicated()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop_duplicates()\nprint(df.head())\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of Characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nr_of_char'] = df['News Headline'].str.len()\ndf['nr_of_char'] = df['nr_of_char'] / df['nr_of_char'].max()\ndf[['Sentiment', 'nr_of_char']].pivot(columns = 'Sentiment', values = 'nr_of_char').iplot(kind = 'box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nr_of_words'] = df['News Headline'].str.split().str.len()\ndf['nr_of_words'] = df['nr_of_words'] / df['nr_of_char'].max()\ndf[['Sentiment', 'nr_of_words']].pivot(columns = 'Sentiment', values = 'nr_of_words').iplot(kind = 'box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of unique words"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nr_of_unique_words'] = df['News Headline'].apply(lambda x: len(set(x.split())))\ndf['nr_of_unique_words'] = df['nr_of_unique_words'] / df['nr_of_unique_words'].max()\ndf[['Sentiment', 'nr_of_unique_words']].pivot(columns = 'Sentiment', values = 'nr_of_unique_words').iplot(kind='box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of punctuation marks"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nr_of_punctuation'] = df['News Headline'].str.split(r\"\\?|,|\\.|\\!|\\\"|'\").str.len()\ndf['nr_of_punctuation'] = df['nr_of_punctuation'] / df['nr_of_punctuation'].max()\ndf[['Sentiment', 'nr_of_punctuation']].pivot(columns = 'Sentiment', values = 'nr_of_punctuation').iplot(kind = 'box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ndf['nr_of_stopwords'] = df['News Headline'].str.split().apply(lambda x: len(set(x) & stop_words))\ndf['nr_of_stopwords'] = df['nr_of_stopwords'] / df['nr_of_stopwords'].max()\ndf[['Sentiment', 'nr_of_stopwords']].pivot(columns = 'Sentiment', values = 'nr_of_stopwords').iplot(kind = 'box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr().iplot(kind='heatmap',colorscale=\"YlGnBu\",title=\"Feature Correlation Matrix\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.insert(0, 'Id', range(1, 1 + len(df))) #defining custom Id column\ndef show_donut_plot(col): #donut plot function\n    \n    rating_data = df.groupby(col)[['Id']].count().head(10)\n    plt.figure(figsize = (12, 8))\n    plt.pie(rating_data[['Id']], autopct = '%1.0f%%', startangle = 140, pctdistance = 1.1, shadow = True)\n\n    # create a center circle for more aesthetics to make it better\n    gap = plt.Circle((0, 0), 0.5, fc = 'white')\n    fig = plt.gcf()\n    fig.gca().add_artist(gap)\n    \n    plt.axis('equal')\n    \n    cols = []\n    for index, row in rating_data.iterrows():\n        cols.append(index)\n    plt.legend(cols)\n    \n    plt.title('Donut Plot: Reviews \\n', loc='center')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_donut_plot('Sentiment')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the text column"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport spacy\nnlp = spacy.load('en')\n\ndef normalize(msg):\n    \n    msg = re.sub('[^A-Za-z]+', ' ', msg) #remove special character and intergers\n    doc = nlp(msg)\n    res=[]\n    for token in doc:\n        if(token.is_stop or token.is_punct or token.is_currency or token.is_space or len(token.text) <= 2): #word filteration\n            pass\n        else:\n            res.append(token.lemma_.lower())\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"News Headline\"] = df[\"News Headline\"].apply(normalize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_collection = Counter([item for sublist in df['News Headline'] for item in sublist])\nfreq_word_df = pd.DataFrame(words_collection.most_common(20))\nfreq_word_df.columns = ['frequently_used_word','count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(freq_word_df, x=\"frequently_used_word\", y=\"count\", color=\"count\", title = 'Frequently used words - Scatter plot')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"News Headline\"] = df[\"News Headline\"].apply(lambda x : \" \".join(x))\ndf = df[[\"News Headline\", \"Sentiment\"]]\ndf[\"Sentiment\"] = le.fit_transform(df[\"Sentiment\"])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's start with BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers\n!pip install simpletransformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rename = {\"News Headline\": \"text\", \"Sentiment\": \"labels\"}\ndf.rename(columns = rename, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x_y = df.sample(frac = 0.75, random_state = 42)\ntest_x_y = pd.concat([df, train_x_y]).drop_duplicates(keep=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_x_y.shape)\nprint(test_x_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\n\nmodel_args = ClassificationArgs()\nmodel_args.train_batch_size = 2\nmodel_args.gradient_accumulation_steps = 8\nmodel_args.learning_rate = 3e-5\nmodel_args.num_train_epochs = 1\n\nmodel_bert = ClassificationModel(\"bert\", \"bert-base-uncased\", num_labels=3, args=model_args, use_cuda=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bert.train_model(train_x_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_bert, out_bert = model_bert.predict(test_x_y['text'].values)\n\nacc_bert = accuracy_score(test_x_y['labels'].to_numpy(), pred_bert)\nf1_bert = f1_score(test_x_y['labels'].to_numpy(), pred_bert, average='micro')\n\nprint(\"Accuracy Score -\",acc_bert)\nprint(\"F1 Score - \", f1_bert)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(pred_bert, test_x_y['labels'].to_numpy())\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(3,3)\nsns.heatmap(cm, annot=labels, fmt=\"\", cmap='Blues')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(10,5))\nsns.regplot(x=pred_bert, y=test_x_y['labels'].to_numpy(),marker=\"*\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}