{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom pylab import rcParams\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:53:38.857277Z","iopub.execute_input":"2021-06-14T17:53:38.857638Z","iopub.status.idle":"2021-06-14T17:53:38.86776Z","shell.execute_reply.started":"2021-06-14T17:53:38.857594Z","shell.execute_reply":"2021-06-14T17:53:38.866383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/predict-test-scores-of-students/test_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T12:07:06.53643Z","iopub.execute_input":"2021-06-14T12:07:06.536839Z","iopub.status.idle":"2021-06-14T12:07:06.568185Z","shell.execute_reply.started":"2021-06-14T12:07:06.536805Z","shell.execute_reply":"2021-06-14T12:07:06.567012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T12:07:09.316929Z","iopub.execute_input":"2021-06-14T12:07:09.3173Z","iopub.status.idle":"2021-06-14T12:07:09.352856Z","shell.execute_reply.started":"2021-06-14T12:07:09.317249Z","shell.execute_reply":"2021-06-14T12:07:09.351824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T12:08:26.538863Z","iopub.execute_input":"2021-06-14T12:08:26.539243Z","iopub.status.idle":"2021-06-14T12:08:26.601214Z","shell.execute_reply.started":"2021-06-14T12:08:26.53921Z","shell.execute_reply":"2021-06-14T12:08:26.600178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like there is no null values since the count value of every columns are the same. Just in case, ","metadata":{}},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T12:09:15.942865Z","iopub.execute_input":"2021-06-14T12:09:15.943261Z","iopub.status.idle":"2021-06-14T12:09:15.954733Z","shell.execute_reply.started":"2021-06-14T12:09:15.943229Z","shell.execute_reply":"2021-06-14T12:09:15.953579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T12:23:13.147353Z","iopub.execute_input":"2021-06-14T12:23:13.147719Z","iopub.status.idle":"2021-06-14T12:23:13.168176Z","shell.execute_reply.started":"2021-06-14T12:23:13.147681Z","shell.execute_reply":"2021-06-14T12:23:13.167215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Visualization**","metadata":{}},{"cell_type":"markdown","source":"* *School*","metadata":{}},{"cell_type":"markdown","source":"> Shows how many students they have for each School within this data","metadata":{}},{"cell_type":"code","source":"data['school'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:18:20.84469Z","iopub.execute_input":"2021-06-14T13:18:20.84505Z","iopub.status.idle":"2021-06-14T13:18:20.853143Z","shell.execute_reply.started":"2021-06-14T13:18:20.845018Z","shell.execute_reply":"2021-06-14T13:18:20.852471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The Average Posttest school for each school","metadata":{}},{"cell_type":"code","source":"scores = data.groupby('school')['posttest'].mean()\nplt.figure(figsize=(24,8))\nsns.barplot(x=data['school'].unique(), y=scores.values)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:53:27.913711Z","iopub.execute_input":"2021-06-14T15:53:27.914052Z","iopub.status.idle":"2021-06-14T15:53:28.245746Z","shell.execute_reply.started":"2021-06-14T15:53:27.914023Z","shell.execute_reply":"2021-06-14T15:53:28.244063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The minimum number of students who go to certain school was 41 and I thought it was a fair number that would not hurt the purpose of average posttest score. <br> <br>It clearly shows that each school has it's own unique value of posttest scores. School can be a factor that affects the posttest prediction <br><br> Planning to do 'One-hot encoding' for this feature","metadata":{}},{"cell_type":"markdown","source":"* *School Setting & School Type Visualization*","metadata":{}},{"cell_type":"markdown","source":"> Ratio","metadata":{}},{"cell_type":"code","source":"setting = data.groupby('school_setting')['school_type'].value_counts().to_frame()\nsetting.columns = ['Count']\ndf1 = setting.reset_index(level=[0,1])\nfig = px.sunburst(df1, path=['school_setting', 'school_type'], values='Count', color_discrete_sequence=px.colors.sequential.Blackbody,\n                 height=400, width=500)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:35:20.819529Z","iopub.execute_input":"2021-06-14T13:35:20.81994Z","iopub.status.idle":"2021-06-14T13:35:20.917045Z","shell.execute_reply.started":"2021-06-14T13:35:20.819908Z","shell.execute_reply":"2021-06-14T13:35:20.915082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* *Other Categorical Features Visualization (Average posttest scores)*","metadata":{}},{"cell_type":"markdown","source":"> I thought the type of Classroom does not affect the posttest prediction, so I excluded from the categorical features list","metadata":{}},{"cell_type":"code","source":"slist = ['school_setting', 'school_type', 'teaching_method', 'gender', 'lunch']","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:54:06.152366Z","iopub.execute_input":"2021-06-14T14:54:06.152777Z","iopub.status.idle":"2021-06-14T14:54:06.157496Z","shell.execute_reply.started":"2021-06-14T14:54:06.152742Z","shell.execute_reply":"2021-06-14T14:54:06.1562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2,3,figsize=(15,10))\n\ndef graph(x, i):\n    sns.barplot(x=data[x].unique(), y=data.groupby(x)['posttest'].mean().values, ax=i, palette='nipy_spectral')\n    i.set_title(x+' (Average Posttest Score)', fontsize=11, fontdict={\"fontweight\": \"bold\"})\n    \n    for p in i.patches:\n        text = str(int(p.get_height()))\n        i.annotate(text, (p.get_x()+p.get_width()/2, p.get_height()+1),\n                   ha=\"center\", va='center', fontsize=10, fontweight=\"bold\")\n\ngraph('school_setting', ax[0,0])\ngraph('school_type', ax[0,1])\ngraph('teaching_method', ax[0,2])\ngraph('gender', ax[1,0])\ngraph('lunch', ax[1,1])\nfig.delaxes(ax= ax[1,2]) ","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:59:24.696386Z","iopub.execute_input":"2021-06-14T14:59:24.696762Z","iopub.status.idle":"2021-06-14T14:59:25.527172Z","shell.execute_reply.started":"2021-06-14T14:59:24.69673Z","shell.execute_reply":"2021-06-14T14:59:25.526121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Except for the 'gender' feature, the Average Posttest score is different for each column values, so I think these features are useful for posttest prediction","metadata":{}},{"cell_type":"markdown","source":"* *Number of Students in Class*","metadata":{}},{"cell_type":"code","source":"sns.lineplot(x=data.groupby('n_student')['posttest'].mean().index, \n             y=data.groupby('n_student')['posttest'].mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:43:37.492263Z","iopub.execute_input":"2021-06-14T15:43:37.492641Z","iopub.status.idle":"2021-06-14T15:43:37.65805Z","shell.execute_reply.started":"2021-06-14T15:43:37.492604Z","shell.execute_reply":"2021-06-14T15:43:37.657119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> It is interesting to see that the number of students in the class is inversely proportional to the average posttest score. We can imply from this graph that students in less number of students in class tends to get higher posttest grade than that of more number of students in class since they can easily communicate with a professor, get individual help or feedback from their assignments and have a tight relationship with a professor.","metadata":{}},{"cell_type":"markdown","source":"* *Classroom Visualization*","metadata":{}},{"cell_type":"code","source":"cl_scores = data.groupby('classroom')['posttest'].mean()\nplt.figure(figsize=(24,8))\nsns.barplot(x=data['classroom'].unique(), y=cl_scores.values)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:53:51.759727Z","iopub.execute_input":"2021-06-14T15:53:51.760062Z","iopub.status.idle":"2021-06-14T15:53:53.385297Z","shell.execute_reply.started":"2021-06-14T15:53:51.760032Z","shell.execute_reply":"2021-06-14T15:53:53.384355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> It looks like the type of classroom affects the posttest score, I might need to include this feature to predict the posttest score. <br> <br> Since this feature has high cardinality, I am planning to imply TargetEncoder on this feature which deals with categorical feature with high cardinality.","metadata":{}},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom numpy import asarray\nfrom sklearn.preprocessing import StandardScaler\nfrom category_encoders import TargetEncoder","metadata":{"execution":{"iopub.status.busy":"2021-06-14T16:04:23.657332Z","iopub.execute_input":"2021-06-14T16:04:23.65771Z","iopub.status.idle":"2021-06-14T16:04:23.931558Z","shell.execute_reply.started":"2021-06-14T16:04:23.65768Z","shell.execute_reply":"2021-06-14T16:04:23.930713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Dropping **'student_id'**, and **'gender'** features. **'student_id'** works as an index and **'gender'** feature does not affect the posttest grade as we can see it from the graph above.","metadata":{}},{"cell_type":"code","source":"X = data.drop(['student_id', 'gender', 'posttest'], axis=1)\ny = data['posttest']\ndisplay(X.head())\ndisplay(y.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-14T16:40:03.29734Z","iopub.execute_input":"2021-06-14T16:40:03.297705Z","iopub.status.idle":"2021-06-14T16:40:03.318566Z","shell.execute_reply.started":"2021-06-14T16:40:03.297674Z","shell.execute_reply":"2021-06-14T16:40:03.317554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* *Frequency Encoding ('School', 'Classroom' features)*","metadata":{}},{"cell_type":"code","source":"f_list = ['school', 'classroom']\nfor x in f_list:\n    encoding = X.groupby(x).size()\n    encoding = encoding/len(X)\n    X[x] = X[x].map(encoding)\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T16:40:04.366152Z","iopub.execute_input":"2021-06-14T16:40:04.366548Z","iopub.status.idle":"2021-06-14T16:40:04.392447Z","shell.execute_reply.started":"2021-06-14T16:40:04.366502Z","shell.execute_reply":"2021-06-14T16:40:04.391483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* *Binary Encoding('school_setting', 'school_type', 'teaching_method', 'lunch')*","metadata":{}},{"cell_type":"code","source":"b_list = ['school_setting', 'school_type', 'teaching_method', 'lunch']\nb_encoder = preprocessing.LabelBinarizer()\nfor x in b_list:\n    a = b_encoder.fit_transform(X[x])\n    X[x] = a\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T16:40:05.131339Z","iopub.execute_input":"2021-06-14T16:40:05.13173Z","iopub.status.idle":"2021-06-14T16:40:05.183036Z","shell.execute_reply.started":"2021-06-14T16:40:05.131697Z","shell.execute_reply":"2021-06-14T16:40:05.182194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* *Standard Scaling*","metadata":{}},{"cell_type":"code","source":"scale_col = ['n_student', 'pretest']\nfor l in scale_col:\n    sc = StandardScaler()\n    scaled_X = sc.fit_transform(asarray(X[l]).reshape(-1,1))\n    X[l] = scaled_X\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T16:41:59.737364Z","iopub.execute_input":"2021-06-14T16:41:59.737743Z","iopub.status.idle":"2021-06-14T16:41:59.756137Z","shell.execute_reply.started":"2021-06-14T16:41:59.737711Z","shell.execute_reply":"2021-06-14T16:41:59.755113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Machine Learning**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression, SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, roc_curve, roc_auc_score, r2_score, mean_squared_error\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:08:38.923836Z","iopub.execute_input":"2021-06-14T17:08:38.924221Z","iopub.status.idle":"2021-06-14T17:08:38.931001Z","shell.execute_reply.started":"2021-06-14T17:08:38.92419Z","shell.execute_reply":"2021-06-14T17:08:38.929609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:08:44.016698Z","iopub.execute_input":"2021-06-14T17:08:44.017034Z","iopub.status.idle":"2021-06-14T17:08:44.025412Z","shell.execute_reply.started":"2021-06-14T17:08:44.017Z","shell.execute_reply":"2021-06-14T17:08:44.024599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = LogisticRegression(max_iter=1000)\nmodel1.fit(X_train, y_train)\npred1 = model1.predict(X_test)\nacc1 = r2_score(y_test, pred1)\nmae1 = mean_absolute_error(y_test, pred1)\nmse1 = mean_squared_error(y_test, pred1)\nprint('Accuracy: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(acc1*100, mae1, mse1))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:08:44.223638Z","iopub.execute_input":"2021-06-14T17:08:44.224146Z","iopub.status.idle":"2021-06-14T17:08:45.262543Z","shell.execute_reply.started":"2021-06-14T17:08:44.224107Z","shell.execute_reply":"2021-06-14T17:08:45.261418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = KNeighborsRegressor()\nmodel2.fit(X_train, y_train)\npred2 = model2.predict(X_test)\nacc2 = r2_score(y_test, pred2)\nmae2 = mean_absolute_error(y_test, pred2)\nmse2 = mean_squared_error(y_test, pred2)\nprint('Accuracy: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(acc2*100, mae2, mse2))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:09:14.808807Z","iopub.execute_input":"2021-06-14T17:09:14.809162Z","iopub.status.idle":"2021-06-14T17:09:14.828781Z","shell.execute_reply.started":"2021-06-14T17:09:14.809133Z","shell.execute_reply":"2021-06-14T17:09:14.827538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3= DecisionTreeRegressor(max_depth=10, min_samples_leaf=15)\nmodel3.fit(X_train, y_train)\npred3 = model3.predict(X_test)\nacc3 = r2_score(y_test, pred3)\nmae3 = mean_absolute_error(y_test, pred3)\nmse3 = mean_squared_error(y_test, pred3)\nprint('Accuracy: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(acc3*100, mae3, mse3))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:09:46.895277Z","iopub.execute_input":"2021-06-14T17:09:46.895676Z","iopub.status.idle":"2021-06-14T17:09:46.910928Z","shell.execute_reply.started":"2021-06-14T17:09:46.895641Z","shell.execute_reply":"2021-06-14T17:09:46.909989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model4 = RandomForestRegressor()\nmodel4.fit(X_train, y_train)\npred4 = model4.predict(X_test)\nacc4 = r2_score(y_test, pred4)\nmae4 = mean_absolute_error(y_test, pred4)\nmse4 = mean_squared_error(y_test, pred4)\nprint('Accuracy: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(acc4*100, mae4, mse4))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:10:32.167048Z","iopub.execute_input":"2021-06-14T17:10:32.167433Z","iopub.status.idle":"2021-06-14T17:10:32.550731Z","shell.execute_reply.started":"2021-06-14T17:10:32.167401Z","shell.execute_reply":"2021-06-14T17:10:32.549582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model5 = SVC()\nmodel5.fit(X_train, y_train)\npred5 = model6.predict(X_test)\nacc5 = r2_score(y_test, pred5)\nmae5 = mean_absolute_error(y_test, pred5)\nmse5 = mean_squared_error(y_test, pred5)\nprint('Accuracy: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(acc5*100, mae5, mse5))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:11:37.463357Z","iopub.execute_input":"2021-06-14T17:11:37.463734Z","iopub.status.idle":"2021-06-14T17:11:37.805765Z","shell.execute_reply.started":"2021-06-14T17:11:37.463702Z","shell.execute_reply":"2021-06-14T17:11:37.804672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model6 = XGBRegressor()\nmodel6.fit(X_train, y_train)\npred6 = model6.predict(X_test)\nacc6 = r2_score(y_test, pred6)\nmae6 = mean_absolute_error(y_test, pred6)\nmse6 = mean_squared_error(y_test, pred6)\nprint('Accuracy: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(acc6*100, mae6, mse6))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:12:39.631826Z","iopub.execute_input":"2021-06-14T17:12:39.632159Z","iopub.status.idle":"2021-06-14T17:12:39.784679Z","shell.execute_reply.started":"2021-06-14T17:12:39.632131Z","shell.execute_reply":"2021-06-14T17:12:39.783315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Comparison Table**","metadata":{}},{"cell_type":"code","source":"acc_table = pd.DataFrame({'Model': ['Logistic Regression',\n                                   'KNN',\n                                   'Decision Tree',\n                                   'Random Forest Tree',\n                                   'SVC',\n                                   'XGB'],\n                         'Accuracy Score': [acc1,\n                                           acc2,\n                                           acc3,\n                                           acc4,\n                                           acc5,\n                                           acc6]})\nacc_table = acc_table.sort_values(by='Accuracy Score', ascending=False)\nacc_table.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:14:36.129141Z","iopub.execute_input":"2021-06-14T17:14:36.129582Z","iopub.status.idle":"2021-06-14T17:14:36.151184Z","shell.execute_reply.started":"2021-06-14T17:14:36.129542Z","shell.execute_reply":"2021-06-14T17:14:36.149351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_table = pd.DataFrame({'Model': ['Logistic Regression',\n                                   'KNN',\n                                   'Decision Tree',\n                                   'Random Forest Tree',\n                                   'SVC',\n                                   'XGB'],\n                         'MSE': [mse1,\n                                           mse2,\n                                           mse3,\n                                           mse4,\n                                           mse5,\n                                           mse6],\n                           'MAE': [mae1,\n                                           mae2,\n                                           mae3,\n                                           mae4,\n                                           mae5,\n                                           mae6]})\nerror_table = error_table.sort_values(by='MSE', ascending=True)\nerror_table.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:16:34.323939Z","iopub.execute_input":"2021-06-14T17:16:34.324287Z","iopub.status.idle":"2021-06-14T17:16:34.343301Z","shell.execute_reply.started":"2021-06-14T17:16:34.324258Z","shell.execute_reply":"2021-06-14T17:16:34.342567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Best Model Parameter Tuning (KNN)**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nknn = KNeighborsClassifier()\n\nspace = dict()\nspace['n_neighbors'] = [4,5,6,7,8,10]\nspace['weights'] = ['uniform', 'distance']\nspace['leaf_size'] = [10,20,30,40,50]\nspace['algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute']\n\nsearch = GridSearchCV(knn, space, scoring='r2', n_jobs=-1, cv=cv)\nresult = search.fit(X_train,y_train)\nprint('Best Score: %s' %result.best_score_)\nprint('Best HyperParameters: %s' %result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:20:59.056944Z","iopub.execute_input":"2021-06-14T17:20:59.057281Z","iopub.status.idle":"2021-06-14T17:21:40.035058Z","shell.execute_reply.started":"2021-06-14T17:20:59.057253Z","shell.execute_reply":"2021-06-14T17:21:40.03425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_model = KNeighborsRegressor(leaf_size=45, n_neighbors=8, n_jobs=-1, p=1)\nt_model.fit(X_train, y_train)\nt_pred = t_model.predict(X_test)\nt_acc = r2_score(y_test, t_pred)\nt_mae = mean_absolute_error(y_test, t_pred)\nt_mse = mean_squared_error(y_test, t_pred)\nprint('Accuracy: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(t_acc*100, t_mae, t_mse))\nprint('Improvement \\nAcc: {:.2f} \\nMAE: {:.2f} \\nMSE: {:.2f}'.format(t_acc*100-acc3*100, t_mae - mae3, t_mse - mse3))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:30:23.632044Z","iopub.execute_input":"2021-06-14T17:30:23.632387Z","iopub.status.idle":"2021-06-14T17:30:23.750053Z","shell.execute_reply.started":"2021-06-14T17:30:23.632355Z","shell.execute_reply":"2021-06-14T17:30:23.749237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> By changing parameters with the best parameters I got from the GridSearchCV, my model did not improve so I decided to change parameters on my own just in case. I do not know why my GridSearchCV did not work...","metadata":{}},{"cell_type":"markdown","source":"# **KNN Visualization**","metadata":{}},{"cell_type":"markdown","source":"> Since KNN cannot be visualized in multidimension, we need to find out two features that are correlated the most <br>\n\n> Feature to Feature Correlations - Higher value indicates simillarity of both two features. Therefore, the less value the better.\n\n> Feature to Outcome Correlations - Higher value indicates the importance of feature","metadata":{}},{"cell_type":"code","source":"h_data = X.join(data['posttest'].to_frame())\nplt.figure(figsize=(15,8))\nmask = np.triu(np.ones_like(h_data.corr(), dtype=np.bool))\nsns.heatmap(data=h_data.corr(),annot=True,cmap='BrBG',mask=mask)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:38:26.192462Z","iopub.execute_input":"2021-06-14T17:38:26.192862Z","iopub.status.idle":"2021-06-14T17:38:26.725798Z","shell.execute_reply.started":"2021-06-14T17:38:26.192831Z","shell.execute_reply":"2021-06-14T17:38:26.724587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Chose feature lunch and pretest because eventhough feature to feature correlation is -0.62 which is pretty high, both of their feature to outcome correlations are decently high (-0.6, 0.95)","metadata":{}},{"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nfrom sklearn.metrics import accuracy_score, classification_report\n# filter warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef accuracy(k, ls, nj, p, X_train, y_train, X_test, y_test):\n    # instantiate learning model and fit data\n    knn = KNeighborsRegressor(n_neighbors=k, leaf_size=ls, n_jobs=nj, p=p)    \n    knn.fit(X_train, y_train)\n\n    # predict the response\n    pred = knn.predict(X_test)\n\n    # evaluate and return  accuracy\n    return r2_score(y_test, pred)\n\ndef classify_and_plot(X, y):\n    ''' \n    split data, fit, classify, plot and evaluate results \n    '''\n    # split data into training and testing set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n    # init vars\n    n_neighbors = 8\n    leaf_size=45\n    n_jobs=-1\n    p=1\n    h           = .02  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold  = ListedColormap(['#FF0000', '#0000FF'])\n\n    rcParams['figure.figsize'] = 5, 5\n        \n    clf = KNeighborsClassifier(n_neighbors, leaf_size=leaf_size, n_jobs=n_jobs, p=p)\n    clf.fit(X_train, y_train)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n        # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    fig = plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n        # Plot also the training points, x-axis = 'Glucose', y-axis = \"BMI\"\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)   \n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"0/1 outcome classification (k = %i)\" % (n_neighbors))\n    plt.show()\n\n        # evaluate\n    y_expected  = y_test\n    y_predicted = clf.predict(X_test)\n\n        # print results\n    print('----------------------------------------------------------------------')\n    print('Accuracy = {:.2f}'.format(accuracy(n_neighbors, leaf_size, n_jobs, p, X_train, y_train, X_test, y_test)*100))\n    print('----------------------------------------------------------------------')\n\n# we only take the best two features and prepare them for the KNN classifier\nrows_nbr = 2133 # data.shape[0]\nX_prime  = np.array(h_data.iloc[:rows_nbr, [6,7]])\nX        = X_prime # preprocessing.scale(X_prime)\ny        = np.array(h_data.iloc[:rows_nbr, 8])\n\n# classify, evaluate and plot results\nclassify_and_plot(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:57:53.165385Z","iopub.execute_input":"2021-06-14T17:57:53.165882Z","iopub.status.idle":"2021-06-14T17:57:55.89987Z","shell.execute_reply.started":"2021-06-14T17:57:53.16585Z","shell.execute_reply":"2021-06-14T17:57:55.898784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> With only two features (lunch, pretest), the accuracy for KNN model was around 90% which is pretty high","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}