{"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"28857dbf-8329-4083-864c-6683d5952820","_uuid":"d4be6529348c06f82202347863121a94bca8a4f6"},"source":"# Mushroom Classification with Categorical Data\n\n### Explicit Goals:\n1. Select a model that is most equipped to predict the status of a mushroom based on the provided 22 features as either edible or poisonous.\n2. Determine the effectiveness of LabelEncoder as a preprocessing method vs. get_dummies.\n\n### Personal Goals:\n1. Test various quality of life functions that provide a first glance at the effectiveness of various classification modeling methods.\n2. Maintain a strong level of documentation and clarity throughout the kernel.\n3. Embrace the Kaggle notebook platform as a way to organize my code as well as display controlled experiments.\n4. Continue to get used to the platform and tackling a project from start-to-finish.","cell_type":"markdown"},{"metadata":{"_cell_guid":"87a56361-0862-491a-8107-892848f9b01b","_uuid":"29e1ee36b93beab04ff8d741091fcf21ca28fb67"},"source":"# Packages Used","cell_type":"markdown"},{"metadata":{"_cell_guid":"4d4b03e2-7111-4842-8da4-db6c94cefcfa","collapsed":true,"_uuid":"c67caae5b04eef0fe177cdbace12b75724872d20"},"outputs":[],"execution_count":null,"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_curve, auc, confusion_matrix\nfrom astropy.table import Table, Column\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\n%matplotlib inline"},{"metadata":{"_cell_guid":"63a222d2-4bca-46fc-a3df-da951f9030b1","_uuid":"231ca61936f59b55c937a4e62014a6f2248260e4"},"source":"# Importing and Processing the Data","cell_type":"markdown"},{"metadata":{"_kg_hide-output":true,"_cell_guid":"d25a404a-4345-48cf-b626-7c766869d89b","scrolled":true,"_uuid":"324fa315c96d23a8202419746887ef2a846cbefc"},"outputs":[],"execution_count":null,"cell_type":"code","source":"df = pd.read_csv(\"../input/mushrooms.csv\")\nprint(df.shape)\ndf.head()"},{"metadata":{"_cell_guid":"d6197af1-3f5f-4d2e-af83-dcbc69549ccd","_uuid":"2cf00c45b821ac959e11e8c9d9c2bec14e674521"},"source":"It is important to note that every feature included in this dataset is categorical, which provides its own set of challenges. In order to fit models using these features, they must be converted into a useable, numeric form. I will be converting them into both integers (as is used by the most popular kernel for this dataset) and into indicator variables (a more classic statistical approach).","cell_type":"markdown"},{"metadata":{"_cell_guid":"06b1ec37-90ee-4ea9-a4f7-3e165277ed4d","scrolled":true,"_uuid":"1ea96b58466a82ddaac4430c60062f0de58cef6f"},"outputs":[],"execution_count":null,"cell_type":"code","source":"df['class'].value_counts()"},{"metadata":{"_cell_guid":"20ed2917-e5ce-4af2-8b46-86d615c88a2c","_uuid":"68ede86b19adba68afd84b986928745415d2e0fa"},"source":"While the data is technically imbalanced towards edible mushrooms, it isnt by a significant amount. Throughout the course of this analysis I will not only be tracking accuracy score, but also recall and precision scores.","cell_type":"markdown"},{"metadata":{"_kg_hide-output":true,"_cell_guid":"133b6ebe-d9e7-4efe-8e0e-84e0c7947284","_uuid":"6e326938c65c98ba7fe7dd609946803c88b2a891"},"outputs":[],"execution_count":null,"cell_type":"code","source":"df.isnull().sum().sum()"},{"metadata":{"_cell_guid":"000f9cd7-72dd-45b1-b9a1-6ffa4fe9eadd","_uuid":"da5fccbdacdec20424f48e2ff9b519644022dfde"},"source":"### Conversion of categorical features into indicator variables (get_dummies)","cell_type":"markdown"},{"metadata":{"_cell_guid":"ce5e7dfe-552f-43b8-9fd3-530f3121ce0f","_uuid":"f063aea182c37eebada43719996ee6a5f5964707"},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_dum = pd.DataFrame()\nfor col in df.columns:\n    dum = pd.get_dummies(df[col])\n    for dcol in dum.columns:\n        name = col +\"_\"+ dcol\n        df_dum[name] = dum[dcol]\nprint(df_dum.shape)"},{"metadata":{"_cell_guid":"cabd2ffe-f20e-4873-b22b-9ade4bbe84cd","_uuid":"2724b95dd6a4f68b49d873f1c5388870f163a328"},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_dum.head()"},{"metadata":{"_cell_guid":"34688ce8-a635-4cd1-92d7-b58aac4bd877","_uuid":"7d721fdcbbe5ee217b9ecdf6969cba78dc6a0f24"},"source":"### Conversion of categorical features into integer variables","cell_type":"markdown"},{"metadata":{"_cell_guid":"3707bad0-d510-4461-8c2c-0fc63e5bc6cf","_uuid":"4834ee5cb5c0e6aee00ddf23583ec3895eab70ae"},"source":"I'm not happy with the practice of using integers as a numerical standby for categorical features. This method implies some amount of heirarchy beteen categories, which is most dangerous in models like logistic regression in which the results would depend upon the mapping of the categories to integers. This method does require less columns of data to be used for modeling, which is a distinct advantage for larger datasets.","cell_type":"markdown"},{"metadata":{"_cell_guid":"e4e4b555-dbda-468d-ae7a-be0ecb014c8a","scrolled":true,"_uuid":"d475198a3113e52bb02f2d99b03a28a106c0ddd2"},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_int = pd.DataFrame()\nle = LabelEncoder()\nfor col in df.columns:\n    df_int[col] = le.fit_transform( df[col])\ndf_int.head()"},{"metadata":{"_cell_guid":"ed9adea5-49c3-489a-b34e-6fe6d7a8504f","collapsed":true,"_uuid":"341a8a32da40900c6519bdace195f39557c1a782"},"outputs":[],"execution_count":null,"cell_type":"code","source":"X = df.iloc[:,1:]\ny = df.iloc[:,0]\nXd = df_dum.iloc[:,2:]\nyd = df_dum.iloc[:,0:2]\nXi = df_int.iloc[:,1:]\nyi = df_int.iloc[:,0]\nscalerd = StandardScaler()\nXds = scalerd.fit_transform(Xd)\nscaleri = StandardScaler()\nXis = scaleri.fit_transform(Xi)"},{"metadata":{"_cell_guid":"438bf04b-2cc8-4784-a076-b54a6fefc0fd","_uuid":"b22442a16e2f658297e55c65d5a63c569ce0f7a4"},"source":"## Summary of Preprocess:\nDataframes:\n* Categorical Features : df\n* Dummy Features : df_dum\n* Integer Features : df_int\n\nI/O versions:\n* Categorical Features : X\n* Categorical Response: y\n* Dummy Features : Xd\n* Dummy Response : yd\n* Integer Features: Xi\n* Integer Response: yi where y == 'p'\n* Scaled, Dummy Features : Xds, scalerd\n* Scaled, Integer Features: Xis, scaleri","cell_type":"markdown"},{"metadata":{"_cell_guid":"e09313a8-9d2e-4f79-ad51-f7151e3fc948","_uuid":"f6e75b613cfb55698f9d319267a0f7c865b4c0f3"},"source":"# Initial Model Fitting","cell_type":"markdown"},{"metadata":{"_cell_guid":"e88dc7a2-7d72-4634-8e60-e2e8354a0dcb","_uuid":"b05f20eeb9bb5f462327b4678c123bb50718a51e"},"source":"### Definition of my quality of life functions:\nAs I am prone to using very similar processes to compare these models not just in this kernel but in other analyses that I perform, I developed these funciton to keep myself organized and manage my cross-validation and shuffling in a systematic way. While it is a bit dense to go through before the analysis is even performed, in my own work I would stick these in their own .py file and import them (while making small edits as necessary depending on the problem).","cell_type":"markdown"},{"metadata":{"_cell_guid":"843b3bd3-8409-4d9a-aca7-ef3de17bae26","_uuid":"25e98d2c74a2a3d38cf4f5f1d9a35df70c33d58d","_kg_hide-input":false,"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def d_method( X, y, model, random_state = 0, k = 5 ):\n    # Fits a categorical model and outputs a cross-validation result of:\n    # Accuracy, Recall, Precision, and the model thats fit last.\n    # The data is train/test split and shuffled systematically\n    kf = ShuffleSplit( n_splits = k )\n    ac = np.zeros( k ); re = np.zeros( k ); pr = np.zeros( k )\n    i = 0\n    for train_index, test_index in kf.split(X):\n        t_model = clone(model)\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        t_model.fit( X_train, y_train )\n        y_pred = t_model.predict( X_test )\n        ac[i] = accuracy_score( y_test, y_pred )\n        re[i] = recall_score( y_test, y_pred )\n        pr[i] = precision_score( y_test, y_pred )\n        i = i+1\n    return( ac, re, pr, t_model )"},{"metadata":{"_cell_guid":"3bef677c-c523-4829-a1e8-b942b26a5726","collapsed":true,"_uuid":"64b82e1389cee9498ecbb45a2fb8631c5f76300f"},"outputs":[],"execution_count":null,"cell_type":"code","source":"def d_conf( X_test, y_test, t_model, p=0.5 ):\n    # Creates a confusion matrix\n    # Using a test set of data and a trained model\n    ypro = t_model.predict_proba(X_test)\n    yp = ypro[:,1] >= p\n    cm = confusion_matrix(y_test, yp)\n    return(cm)"},{"metadata":{"_cell_guid":"401cc502-cd31-43e7-9f7f-dae62c660d2b","collapsed":true,"_uuid":"fd20c10aab164a7a00fd4de6a6cb6a3684d039b6"},"outputs":[],"execution_count":null,"cell_type":"code","source":"def d_conf_l( X_test, y_test, t_model):\n    # The same as d_conf only removing the theshold of class selection\n    yp = t_model.predict(X_test)\n    cm = confusion_matrix(y_test, yp)\n    return(cm)"},{"metadata":{"_cell_guid":"cc5d1fe6-8684-4f6f-965c-e21c7261e641","collapsed":true,"_uuid":"90754e502c1652c6f5a6884f3a3436ddc71ff463"},"outputs":[],"execution_count":null,"cell_type":"code","source":"def d_roc( X_test, y_test, t_model, points=100):\n    # Generates a plot that describes the changes in\n    #   Accuracy, recall, precision scores as the threshold\n    #   For classification changes.\n    #   Using a test set of data and a trained model\n    ac_p=np.zeros(points); re_p=np.zeros(points); pr_p=np.zeros(points); \n    for i in np.arange(points):\n        ypro = t_model.predict_proba(X_test)\n        yp = ypro[:,1] >= (i/points)\n        ac_p[i] = accuracy_score( y_test, yp )\n        re_p[i] = recall_score( y_test, yp )\n        pr_p[i] = precision_score( y_test, yp )\n    t = np.arange(points)/points\n    plt.plot(t, ac_p, label='accuracy score')\n    plt.plot(t, re_p, label = 'recall score')\n    plt.plot(t, pr_p, label = 'precision score')\n    plt.legend()\n    plt.show()"},{"metadata":{"_cell_guid":"1d9a8353-f884-4639-a439-8fa62da3b15a","collapsed":true,"_uuid":"08acdf31c9edb01f0f6ff1eb62633f7af00a3057"},"outputs":[],"execution_count":null,"cell_type":"code","source":"def d_summary(X, y, model, random_state = 0,k=5,X_test=None,y_test=None,p=0.5,points=100):\n    # Runs the d_method, d_conf, and d_roc on a model and dataset.\n    ac, re, pr, t_model = d_method( X, y, model )\n    print( \"Accuracy Score =  \", ac, \" Mean = \", ac.mean() )\n    print( \"Recall Score =    \", re, \" Mean = \", re.mean() )\n    print( \"Precision Score = \", pr, \" Mean = \", pr.mean() )\n    if X_test is None:\n        X_test = X\n    if y_test is None:\n        y_test = y\n    cm = d_conf( X_test, y_test, t_model, p)\n    print(\"Confusion Matrix:\")\n    print(cm)\n    d_roc( X_test, y_test, t_model, points)"},{"metadata":{"_cell_guid":"4a206b8b-5dd9-4b0c-b9b3-788a4be2afbf","_uuid":"7d27d5eab76d444a902e7fddfd8dbfac61da1301"},"source":"## Logistic Regression","cell_type":"markdown"},{"metadata":{"_cell_guid":"a97a75fa-faea-44e2-99e5-46ef0ba7d5fe","_uuid":"9b57e50e36ee187b3221a50f29749db75111f84f"},"outputs":[],"execution_count":null,"cell_type":"code","source":"d_summary( Xds, yi, LogisticRegression() )"},{"metadata":{"_cell_guid":"c1f16b08-74a2-40f8-947a-5140d06137c2","_uuid":"eb3e69e95883c1afa693407f342bbd8e23160cde"},"outputs":[],"execution_count":null,"cell_type":"code","source":"d_summary(Xis, yi, LogisticRegression() )"},{"metadata":{"_cell_guid":"f902a36d-0f3e-4c13-bcf4-95f06dc84e3d","_uuid":"4a47683f02a072130c4e77aa4788b1e905eedb65"},"source":"Well... this is unexpected to say the least. I want to make it perfectly clear that the 100% accuracy score does NOT apply to an independent validation set that is put aside, but rather the total X and y put into the last kfold model that was fit in a particular cross-validation, so there is some amount of overfitting, however this is meant as an at-a-glance estimation.\n\nHowever this is a fairly significant result from just logistic regression. To confirm this, I'll just run a couple logistic regressions but use a basic train_test_split to output the confusion matrix.","cell_type":"markdown"},{"metadata":{"_cell_guid":"9bf39374-a375-438b-b96b-136ea86758b3","_uuid":"b149c16eee27e029d587f4315c3363e6bdcafb5f"},"outputs":[],"execution_count":null,"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(Xis, yi, test_size=0.2)\nd_summary( Xis, yi, LogisticRegression(),X_test= X_test,y_test=y_test)"},{"metadata":{"_cell_guid":"ca1fbc7a-85ab-4c28-8de5-2113024eccc3","_uuid":"19bf355b7d51ac8b4170097e6331f4fcaee9b120"},"outputs":[],"execution_count":null,"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(Xds, yi, test_size=0.2)\nd_summary( Xds, yi, LogisticRegression(),X_test= X_test,y_test=y_test)"},{"metadata":{"_cell_guid":"9745bd8f-e927-4376-aaf4-8c40619ebe91","_uuid":"07dfd61b149a2a951117a692ac412be83434ae99"},"source":"This confirms the initial results above. While this isnt definitive proof that dummy variables are superior to integer variables for categorical variables as a whole, it certainly appears to be for this data generating system.\n\nSince its a bit awkward to just do logistic regression and be done with it, I'll do some hyper-parameter tuning since I've been wanting to build a function to handle that for later.","cell_type":"markdown"},{"metadata":{"_cell_guid":"05ed0245-6672-418c-9322-352a75fb409a","collapsed":true,"_uuid":"4506a41d53a9c0b44f1a3f8a291200757ecc0d30"},"outputs":[],"execution_count":null,"cell_type":"code","source":"def d_table( X, y, model, A, parameter):\n    # Returns accuracy scores for models performed with specfied hyper-parameters.\n    #   Take care to describe the parameter as a string\n    print( 'alpha\\t\\tAccuracy\\tRecall\\t\\tPrecision')\n    for alpha in A:\n        l_model = clone(model)\n        eval(\"l_model.set_params(\" + parameter + \"=\" + str(alpha) + \")\")\n        ac, re, pr, t_model = d_method( Xds, yi, l_model )\n        print( alpha,'\\t\\t%0.4f\\t\\t%0.4f\\t\\t%0.4f' % (ac.mean(), re.mean(), pr.mean() ) )"},{"metadata":{"_cell_guid":"6aa6ba94-0c7c-4895-b081-72d64135cb00","_uuid":"649c4b0e03b16f213eedb647650e80feb4f7d052"},"outputs":[],"execution_count":null,"cell_type":"code","source":"reg_strength = [0.01, 0.05, 0.1, 0.5,1,5, 10,50,100]\nprint('Indicator Variables')\nd_table( Xds, yi, LogisticRegression(penalty=\"l1\"), reg_strength, 'C' )\nprint('Integer Variables')\nd_table( Xis, yi, LogisticRegression(penalty=\"l1\"), reg_strength, 'C' )"},{"metadata":{"_cell_guid":"77bedf85-b430-4c2c-bcc3-008b0d35db39","_uuid":"196d1925b0d693737c629b1b25f7e7ae8f7c0285"},"source":"Not particularly helpful information, but I felt pretty cool using the eval workaround for setting parameters in the model mid-loop.\n\nLet's look at some more models since I still have more kaggle run-time to kill.","cell_type":"markdown"},{"metadata":{"_cell_guid":"5521356c-9280-4610-8f67-1fe7112758e6","_uuid":"639a11d1c08c9912baca9f457ddf5a3729bad677"},"source":"## Decision Tree Classifier","cell_type":"markdown"},{"metadata":{"_cell_guid":"aba69038-51ec-45a5-bcb1-28ef9a544383","_uuid":"0d1ee35820d62dd41f1547a742802eced1792bcd"},"outputs":[],"execution_count":null,"cell_type":"code","source":"print(\"Indicator / Dummy\")\nd_summary( Xds, yi, DecisionTreeClassifier())\nprint(\"Integer / LabelEncoder\")\nd_summary( Xis, yi, DecisionTreeClassifier())"},{"metadata":{"_cell_guid":"e363100c-189f-492a-84de-6d78ef755b81","_uuid":"b57bffd8848bdcb51680b7c28f5ec50650e31035"},"outputs":[],"execution_count":null,"cell_type":"code","source":"min_samples = np.arange(10) + 3\nprint(\"Indicator / Dummy\")\nd_table( Xds, yi, DecisionTreeClassifier(), min_samples, 'min_samples_split')\nprint(\"Integer / LabelEncoder\")\nd_table( Xis, yi, DecisionTreeClassifier(), min_samples, 'min_samples_split')"},{"metadata":{"_cell_guid":"4b73fc73-066a-48b3-91d5-6ff208cde055","_uuid":"be281090e9cf78e7e3672c915681e45f494d6a6b"},"outputs":[],"execution_count":null,"cell_type":"code","source":"print( \"Dummy / Indicator\")\nd_summary( Xds, yi, RandomForestClassifier())\nprint( \"LabelEncoder / Integer\")\nd_summary( Xis, yi, RandomForestClassifier())"},{"metadata":{"_cell_guid":"801f3dc6-7096-4942-b4af-ee52e34ca29b","_uuid":"405799b7abd08343cdd8ae82f02293c0ea268211"},"outputs":[],"execution_count":null,"cell_type":"code","source":"max_features = np.arange(10) + 3\nprint(\"Indicator / Dummy\")\nd_table( Xds, yi, DecisionTreeClassifier(), max_features, 'max_features')\nprint(\"Integer / LabelEncoder\")\nd_table( Xis, yi, DecisionTreeClassifier(), max_features, 'max_features')"},{"metadata":{"_cell_guid":"eeeae4cc-c2d4-420c-97cb-0c6090e4b38f","collapsed":true,"_uuid":"fc0d0bb37d774c8de0a7a0e515fb989068bce5b7"},"outputs":[],"execution_count":null,"cell_type":"code","source":""},{"metadata":{"_cell_guid":"15854d68-9204-4d4b-99d1-b959cca5dcbb","_uuid":"c261ae617c6a952c8aa7cd58546157c17560055e"},"outputs":[],"execution_count":null,"cell_type":"code","source":"print( \"Dummy / Indicator\")\nac, re, pr, t_model = d_method( Xds, yi, SVC())\ncm = d_conf_l(Xds, yi, t_model)\nprint( \"Accuracy Score =  \", ac, \" Mean = \", ac.mean() )\nprint( \"Recall Score =    \", re, \" Mean = \", re.mean() )\nprint( \"Precision Score = \", pr, \" Mean = \", pr.mean() )\nprint(cm)\nprint( \"LabelEncoder / Integer\")\nac, re, pr, t_model = d_method( Xis, yi, SVC())\ncm = d_conf_l(Xis, yi, t_model)\nprint( \"Accuracy Score =  \", ac, \" Mean = \", ac.mean() )\nprint( \"Recall Score =    \", re, \" Mean = \", re.mean() )\nprint( \"Precision Score = \", pr, \" Mean = \", pr.mean() )\nprint(cm)"},{"metadata":{"_cell_guid":"b6574461-5769-4173-8249-c32138cee7ec","_uuid":"4bda579450b8a5ccefd753a449e5f6bde3d6815e"},"outputs":[],"execution_count":null,"cell_type":"code","source":"print( \"Dummy / Indicator\")\nac, re, pr, t_model = d_method( Xds, yi, KNeighborsClassifier())\ncm = d_conf_l(Xds, yi, t_model)\nprint( \"Accuracy Score =  \", ac, \" Mean = \", ac.mean() )\nprint( \"Recall Score =    \", re, \" Mean = \", re.mean() )\nprint( \"Precision Score = \", pr, \" Mean = \", pr.mean() )\nprint(cm)\nprint( \"LabelEncoder / Integer\")\nac, re, pr, t_model = d_method( Xis, yi, KNeighborsClassifier())\ncm = d_conf_l(Xis, yi, t_model)\nprint( \"Accuracy Score =  \", ac, \" Mean = \", ac.mean() )\nprint( \"Recall Score =    \", re, \" Mean = \", re.mean() )\nprint( \"Precision Score = \", pr, \" Mean = \", pr.mean() )\nprint(cm)"},{"metadata":{"_cell_guid":"890d8435-d241-41bb-b45a-53b0d064a79b","_uuid":"7f83f09873848ae60437aa24b3d1eef8e777fcb0"},"outputs":[],"execution_count":null,"cell_type":"code","source":"print( \"Dummy / Indicator\")\nd_summary( Xds, yi, GaussianNB())\nprint( \"LabelEncoder / Integer\")\nd_summary( Xis, yi, GaussianNB())"},{"metadata":{"_cell_guid":"3e647095-2209-4515-84b0-d0abe66c37f5","collapsed":true,"_uuid":"1e62ce8c849691c9df210c7d4cac3c54ea04769d"},"source":"# Summary\n","cell_type":"markdown"},{"metadata":{"_cell_guid":"c15e2e1c-a636-4f7b-8c4f-0e9e23fbbfcb","collapsed":true,"_uuid":"4b4801355cbf6c8c5b42882a755ec9f7f0589ce8"},"source":"While this dataset reached very high acuracy scores with minimal effort, the dummy variables performed slightly better than the integer variables. ","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":""}]}