{"cells":[{"metadata":{},"cell_type":"markdown","source":"#The Bert model fine tuning code is adapted from the huggingface and beautiful article and video shared by \nhttps://mccormickml.com/2019/07/22/BERT-fine-tuning/\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"CJALWEXbS_iS","colab_type":"code","colab":{}},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"cyYVUk3qS_ie","colab_type":"code","colab":{}},"cell_type":"code","source":"\nann_df=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Annotations.csv\")\nposts_df=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Posts.csv\")\ncats_df=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Categories.csv\")\nNews_df=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Newspaper_Staff.csv\")\narticles_df=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Articles.csv\")\ncvsplit_df=pd.read_csv(\"/kaggle/input/10k-german-news-articles/CrossValSplit.csv\")\nconsol_df=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Annotations_consolidated.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"QdIPOmi1S_ik","colab_type":"code","outputId":"4d294acd-db9e-49eb-ae30-2800f4ea2dcc","colab":{"base_uri":"https://localhost:8080/","height":191}},"cell_type":"code","source":"ann_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Ecxk5JhCS_ip","colab_type":"code","outputId":"c14e99e8-8c30-4221-d202-4930d715db4e","colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":"posts_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"YP0yGfXPS_ix","colab_type":"code","outputId":"ff6e4207-849e-488d-ecd4-93ce0ca53a09","colab":{"base_uri":"https://localhost:8080/","height":191}},"cell_type":"code","source":"cats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"tzdXS-icS_i1","colab_type":"code","outputId":"37ecfcf0-89b7-4e1d-ab0a-7c717eb2a071","colab":{"base_uri":"https://localhost:8080/","height":191}},"cell_type":"code","source":"News_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"NnvBy2MfS_i8","colab_type":"code","outputId":"09548188-6422-4327-e4db-0ee139cb2b5e","colab":{"base_uri":"https://localhost:8080/","height":191}},"cell_type":"code","source":"articles_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"AKhYgZ40S_jE","colab_type":"code","outputId":"12840ba5-281e-41c5-a43c-5016f1ac0ad7","colab":{"base_uri":"https://localhost:8080/","height":191}},"cell_type":"code","source":"cvsplit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"D1lHD8EWS_jI","colab_type":"code","outputId":"b9fbc378-3df3-4185-db8e-19d43e0c31a8","colab":{"base_uri":"https://localhost:8080/","height":191}},"cell_type":"code","source":"consol_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"GniUxcRVS_jL","colab_type":"code","outputId":"325af70c-6867-4f0c-fbd1-2cc95125a763","colab":{"base_uri":"https://localhost:8080/","height":66}},"cell_type":"code","source":"ann_df['Category'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"X3-QYqTWS_jQ","colab_type":"code","outputId":"215604a5-87aa-41e9-ab17-52e282e6986f","colab":{"base_uri":"https://localhost:8080/","height":163}},"cell_type":"code","source":"ann_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"WJlC7Af_S_jV","colab_type":"code","outputId":"2565b708-d4e6-4e73-e48f-632fcbab4c6b","colab":{"base_uri":"https://localhost:8080/","height":180}},"cell_type":"code","source":"articles_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"SKEzkrfTS_ja","colab_type":"code","outputId":"7cc8d208-80b6-4e0e-dcbc-84d17bb740f1","colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":"articles_df[\"Body\"][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"wijBuSVeS_jd","colab_type":"code","outputId":"d7220a40-e9ba-485e-e7d5-47b9fef0b780","colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":"posts_df[\"Body\"][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"y1isBMtoS_jh","colab_type":"code","outputId":"6d375dfd-846c-4ea3-e8a6-2eab346148c0","colab":{"base_uri":"https://localhost:8080/","height":261}},"cell_type":"code","source":"posts_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"TRZdp_eIS_jm","colab_type":"code","outputId":"36995cb8-c69b-48af-c091-dbca0bcdf45a","colab":{"base_uri":"https://localhost:8080/","height":147}},"cell_type":"code","source":"consol_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"cKVlIFE0S_jt","colab_type":"code","outputId":"ef33dfd6-0082-4163-af93-02cc15d3bc8c","colab":{"base_uri":"https://localhost:8080/","height":76}},"cell_type":"code","source":"posts_df[posts_df[\"ID_Post\"]==3326]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"LB19fbLgS_jw","colab_type":"code","colab":{}},"cell_type":"code","source":"result_Df = pd.merge(posts_df, consol_df, on=\"ID_Post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"vcVORIxOS_j0","colab_type":"code","outputId":"a02bff17-7bc2-4730-cc20-07aa0a6fccd9","colab":{"base_uri":"https://localhost:8080/","height":310}},"cell_type":"code","source":"print(result_Df.info())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"h3alFCscS_j3","colab_type":"code","outputId":"a3b79a6b-1830-4655-a63d-603aa733d5a9","colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":"result_Df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"IIsHz23vS_j6","colab_type":"code","colab":{}},"cell_type":"code","source":"tr_Df = pd.merge(result_Df, articles_df, on=\"ID_Article\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Li0LAhlZS_kC","colab_type":"code","outputId":"fbcd6173-07af-4464-dd96-00bde8a7c337","colab":{"base_uri":"https://localhost:8080/","height":358}},"cell_type":"code","source":"tr_Df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"hUBxLnbCS_kG","colab_type":"code","outputId":"82514341-3566-4071-930e-9a16034e6bd8","colab":{"base_uri":"https://localhost:8080/","height":255}},"cell_type":"code","source":"tr_Df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Uga0KUJvS_kK","colab_type":"code","outputId":"4c13bcc1-ca2b-47fb-cf6d-5b6666e54865","colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":"tr_Df['Body_y'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"IeLxe4vtS_kN","colab_type":"code","outputId":"b0fa5971-9242-4dc8-96eb-43787adbd1c9","colab":{"base_uri":"https://localhost:8080/","height":66}},"cell_type":"code","source":"tr_Df[\"Category\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"VJni7VZyS_kS","colab_type":"code","outputId":"17ff5f61-6a73-4ed3-e35d-874eac0418e3","colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":"tr_Df[\"Value\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"_oUAJBR0S_kY","colab_type":"text"},"cell_type":"markdown","source":"I'm assuming the x values are \"Body_y\" column and The predictors as \"Value\" column"},{"metadata":{"trusted":true,"id":"Z4tJkoLyS_kZ","colab_type":"code","colab":{}},"cell_type":"code","source":"import re\ndef clean_text(sent):\n  \n  sentence = sent.lower()\n  S=re.sub('\"','',sentence)\n  S = re.sub(r'[^\\w]', ' ', S)\n  S = re.sub(\"\\s\\s+\", \" \", S)\n\n  return S\ntr_Df['Body_y']=tr_Df['Body_y'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"U2ZUKG9hS_kc","colab_type":"code","outputId":"aa893e96-1096-4d7b-f42d-29c78ca02600","colab":{"base_uri":"https://localhost:8080/","height":82}},"cell_type":"code","source":"tr_Df['Body_y'][0:3]","execution_count":null,"outputs":[]},{"metadata":{"id":"S0L7jyQbs9HF","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":" from sklearn.model_selection import train_test_split\n X_train, X_test, y_train, y_test = train_test_split( tr_Df['Body_y'], tr_Df['Value'], test_size=0.20, random_state=42,stratify=tr_Df['Category'])","execution_count":null,"outputs":[]},{"metadata":{"id":"uuV_0siTuD4b","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train=pd.DataFrame(pd.concat([X_train,y_train],axis=1))\ntest=pd.DataFrame(pd.concat([X_test,y_test],axis=1))","execution_count":null,"outputs":[]},{"metadata":{"id":"cK_dmR_iubyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"c078360a-4211-4d11-ca4a-e6186759a7ea","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"4y8lJaS4fWrZ","colab_type":"code","outputId":"f1052f47-606f-45e2-c61f-fe841346b0d6","colab":{"base_uri":"https://localhost:8080/","height":638},"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_5AtZJ4uS_kn","colab_type":"code","outputId":"273d20a0-a4f9-409e-e964-8007e5e4f2db","colab":{"base_uri":"https://localhost:8080/","height":61}},"cell_type":"code","source":"from tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"id":"pU_6pbR7f8J_","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def gen_id_mask(data):\n  sms = data.Body_y.values\n\n  # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n  sms = [\"[CLS] \" + sm + \" [SEP]\" for sm in sms]\n  labels = data.Value.values\n\n  tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n\n  tokenized_texts = [tokenizer.tokenize(sm) for sm in sms]\n  \n\n  MAX_LEN = 256\n  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n  attention_masks = []\n\n  # Create a mask of 1s for each token followed by 0s for padding\n  for seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n\n  print('success')\n  return (input_ids,labels,attention_masks)","execution_count":null,"outputs":[]},{"metadata":{"id":"wmOlLV3Eflau","colab_type":"code","outputId":"96f0f33b-8e7c-491a-817e-474a50a8e477","colab":{"base_uri":"https://localhost:8080/","height":50},"trusted":true},"cell_type":"code","source":"train_input_ids,train_labels,train_input_masks=gen_id_mask(train)\ntest_input_ids,test_labels,test_input_masks=gen_id_mask(test)","execution_count":null,"outputs":[]},{"metadata":{"id":"wVv8VBjXonms","colab_type":"code","outputId":"c910457c-be6a-4327-c3c1-2bd42dd71218","colab":{"base_uri":"https://localhost:8080/","height":488},"trusted":true},"cell_type":"code","source":"train_input_ids[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"IAlj5ODTrFHN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5dc70340bbed432f9ac0915fd382a58b","0cd5636c674247258c55ba876e64c66d","d1211283f694447c9c01892bc7332229","77ad6ecd31414dec873faec5ac284906","7e42ca706e424594a6775d00ea9c7baa","e1ff0ff4634d4a43bd95495ae21dbb8c","4c56c8ff320549afabe63851979ff2c7","b5a4a3fa88fc418d82f54bbb42b0688d","b6d06a99d2e441bdbeca9184705bc940","aa3e619049a14b8bb1bcfa07b46064dd","e73104c931a34a53b50b36ffb5a7cef5","10f23617014d44b7b4e74a02fb124f2b","33ebbd6ea4694607914286d82d2fd6bf","cc4200a4af144d75bf151be33b5afcfa","e14b34286049409eb6d8adf2a117ab91","e630ed404b2b49ea9caff3494870a67e"]},"outputId":"b3fec84a-12b1-4428-fdf5-5479e62cd1b4","trusted":true},"cell_type":"code","source":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif torch.cuda.is_available():    \n\n    # use the GPU.    \n    device = torch.device(\"cuda\")\n    print('GPU Device name is :', torch.cuda.get_device_name(0))\nmodel_bert = BertForSequenceClassification.from_pretrained(\n    \"dbmdz/bert-base-german-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel_bert.cuda()","execution_count":null,"outputs":[]},{"metadata":{"id":"AOISkznhr19L","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","execution_count":null,"outputs":[]},{"metadata":{"id":"cEDrd8Qt2Ria","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"c4307bf8-2ef3-4137-d1cb-6ff719f162cf","trusted":true},"cell_type":"code","source":"\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#Converting the inputs for model into torch tensors\ntrain_inputs = torch.tensor(train_input_ids)\nvalidation_inputs = torch.tensor(test_input_ids)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(test_labels)\ntrain_masks = torch.tensor(train_input_masks)\nvalidation_masks = torch.tensor(test_input_masks)\n\n\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 16\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ws79vxqz6wFm","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom tqdm import tqdm, trange\n\nimport io\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4S9RawJEhY8T","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"id":"BFjtjCo72XYv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"295e9078-89b4-4118-e16d-18a9ce8042c0","trusted":true},"cell_type":"code","source":"import random\nfrom transformers import get_linear_schedule_with_warmup\n\n\n\n#Code is adapted from following path (run_glue.py)\n#https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n\n\n# Prepare optimizer and schedule (linear warmup and decay)\n# no_decay = ['bias', 'LayerNorm.weight']\n# optimizer_grouped_parameters = [\n#         {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n#         {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n#         ]\n\n# optimizer = AdamW(optimizer_grouped_parameters, lr=2e-3)\nparam_optimizer = list(model_bert.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\n\noptimizer = AdamW(optimizer_grouped_parameters,lr=1e-5)\n# Number of training epochs (authors recommend between 2 and 4)\n\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * 10 #The value 10 is no of epochs\n\n# Create the learning rate scheduler.\n# scheduler = get_linear_schedule_with_warmup(optimizer, \n#                                             num_warmup_steps = 0, # Default value in run_glue.py\n#                                             num_training_steps = total_steps)\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss and accuracies after each epoch so we can plot them.\nloss_values = []\nTrain_Master_accuracy=[]\nValidation_Master_accuracy=[]\nepochs=2\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model_bert.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model_bert.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        # The documentation for this `model` function is here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n        outputs = model_bert(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        #scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss / len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n    \n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ====================================================\n    #               Validation and Training Accuracies\n    # ====================================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model_bert.eval()\n\n    # Tracking variables \n    train_loss, train_accuracy = 0, 0\n    nb_train_steps, nb_train_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in train_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            outputs = model_bert(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_train_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        train_accuracy += tmp_train_accuracy\n\n        # Track the number of batches\n        nb_train_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Train Accuracy: {0:.2f}\".format(train_accuracy/nb_train_steps))\n    Train_Master_accuracy.append(train_accuracy/nb_train_steps)\n    print(\"  Accuracy evaluation on training took: {:}\".format(format_time(time.time() - t0)))\n\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            outputs = model_bert(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n\n        # Track the number of batches\n        nb_eval_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Validation Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n    Validation_Master_accuracy.append(eval_accuracy/nb_eval_steps)\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))","execution_count":null,"outputs":[]},{"metadata":{"id":"bDAuW0u9FY3A","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"GERman_10K_news_Classifier.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"5dc70340bbed432f9ac0915fd382a58b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0cd5636c674247258c55ba876e64c66d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d1211283f694447c9c01892bc7332229","IPY_MODEL_77ad6ecd31414dec873faec5ac284906"]}},"0cd5636c674247258c55ba876e64c66d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d1211283f694447c9c01892bc7332229":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e42ca706e424594a6775d00ea9c7baa","_dom_classes":[],"description":"Downloading","_model_name":"IntProgressModel","bar_style":"success","max":614,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":614,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1ff0ff4634d4a43bd95495ae21dbb8c"}},"77ad6ecd31414dec873faec5ac284906":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4c56c8ff320549afabe63851979ff2c7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 614/614 [00:00&lt;00:00, 22.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5a4a3fa88fc418d82f54bbb42b0688d"}},"7e42ca706e424594a6775d00ea9c7baa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e1ff0ff4634d4a43bd95495ae21dbb8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4c56c8ff320549afabe63851979ff2c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b5a4a3fa88fc418d82f54bbb42b0688d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b6d06a99d2e441bdbeca9184705bc940":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_aa3e619049a14b8bb1bcfa07b46064dd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e73104c931a34a53b50b36ffb5a7cef5","IPY_MODEL_10f23617014d44b7b4e74a02fb124f2b"]}},"aa3e619049a14b8bb1bcfa07b46064dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e73104c931a34a53b50b36ffb5a7cef5":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_33ebbd6ea4694607914286d82d2fd6bf","_dom_classes":[],"description":"Downloading","_model_name":"IntProgressModel","bar_style":"success","max":442252390,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442252390,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc4200a4af144d75bf151be33b5afcfa"}},"10f23617014d44b7b4e74a02fb124f2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e14b34286049409eb6d8adf2a117ab91","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 442M/442M [00:06&lt;00:00, 71.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e630ed404b2b49ea9caff3494870a67e"}},"33ebbd6ea4694607914286d82d2fd6bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cc4200a4af144d75bf151be33b5afcfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e14b34286049409eb6d8adf2a117ab91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e630ed404b2b49ea9caff3494870a67e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}