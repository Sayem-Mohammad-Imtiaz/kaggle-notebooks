{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importamos \nLas librerias necesarias para hacer los modelos que utilizaremos luego, ademas de las métricas para evaluar los resultados.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":false,"scrolled":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn import tree\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input\nAbrimos el dataset con la información suministrada por kaggle","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv');\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Primero definimos categorias para armar el input después","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CATEGORIES = {\n    'class': ['e', 'p'],\n    'cap-shape': ['x', 'b', 'c', 'k', 's', 'f'], \n    'cap-surface': ['f', 'g', 'y', 's'], \n    'cap-color': ['n', 'b', 'c', 'g','r', 'p', 'u', 'e', 'w', 'y'], \n    'bruises': ['t', 'f'], \n    'odor': ['a', 'l', 'c', 'y', 'f', 'm', 'n', 'p', 's'],\n    'gill-attachment': ['a', 'd', 'f', 'n'], \n    'gill-spacing': ['c', 'w', 'd'], \n    'gill-size': ['b', 'n'],\n    'gill-color': ['k', 'n', 'b', 'h', 'g', 'r', 'o', 'p', 'u', 'e', 'w', 'y'],\n    'stalk-shape': ['e', 't'], \n    'stalk-root': ['b', 'c', 'u', 'e', 'z', 'r', '?'], \n    'stalk-surface-above-ring': ['f', 'y', 'k', 's'],\n    'stalk-surface-below-ring': ['f', 'y', 'k', 's'],\n    'stalk-color-above-ring': ['b', 'n', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n    'stalk-color-below-ring': ['b', 'n', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n    'veil-type': ['p', 'u'], \n    'veil-color': ['n', 'o', 'w', 'y'], \n    'ring-number': ['o', 'n', 't'],\n    'ring-type': ['c', 'e', 'f', 'l', 'n', 'p', 's', 'z'], \n    'spore-print-color': ['k', 'n', 'b', 'h', 'r', 'o', 'u', 'w', 'y'], \n    'population': ['a', 'c', 'n', 's', 'v', 'y'], \n    'habitat': ['g', 'l', 'm', 'p', 'u', 'w', 'd']  \n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convertimos los datos de *string* a *enteros* segun los posibles valores que pueden tomar","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"    \ndf = data\nfor feature, vocab in CATEGORIES.items():\n    df[feature] = df[feature].apply(lambda x: vocab.index(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separamos los datos por un lado en test y train y por el otro en datos y labels (X e y)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, 1:23].values\ny = df.iloc[:, 0].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=40)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Red Neuronal\n\nCreamos el modelo que consistirá de el input (convertido a numeros anteriormente), una gran capa densa de 128 neuronas y una capa de salida con dos neuronas que corresponderan a cada una de las clases que se puede asignar a los hongos.\n\nEsta red es la primera planteada y no fue necesaria cambiarla (como suele ser el caso) para que se ajuste mejor al dataset. Se utiliza relu como función de activación y 20 ciclos de entrenamiento (de los cuales en realidad solo eran necesarios 10 aproximadamente).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(22,)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(2),\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora generamos las metricas con ayuda de sklearn dado que la libreria de Keras no tiene soporte nativo para el F1 score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(X_test).argmax(axis=-1)\nmetrics = precision_recall_fscore_support(y_test, y_predict, average='binary')\nprint('Precision :', metrics[0])\nprint('Recall :', metrics[1])\nprint('Medida F1 :', metrics[2])\nprint('Error cuadratico medio: ', mean_squared_error(y_test, y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forrest\nAhora utilizando la misma información usamos el clasificador random forrest, como comentario importante hay que destacar la simplicidad de este clasificador no solo para hacer el codigo sino tambien para elegir hyperparametros.\nTambién se eligió un unico arbol como estimador dada la simplicidad del problema (y veremos que no hacen falta más).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a random forest Classifier. By convention, clf means 'Classifier'\nclf = RandomForestClassifier(n_jobs=2, random_state=0, n_estimators=1)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indice_tree = 0 # Which tree we want to plot\nfig, axes = plt.subplots(figsize = (20,20), dpi=800)\ntree.plot_tree(clf.estimators_[indice_tree],\n               feature_names = list(CATEGORIES.keys()), \n               class_names=['edible', 'poisonous'],\n               filled = True,\n               rounded = True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_forrest_predict = clf.predict(X_test)\nmetrics = precision_recall_fscore_support(y_test, y_forrest_predict, average='binary')\nprint('Precision :', metrics[0])\nprint('Recall :', metrics[1])\nprint('Medida F1 :', metrics[2])\nprint('Error cuadratico medio: ', mean_squared_error(y_test, y_forrest_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusiones\n\nSe puede observar que las predicciones dieron 100% correctas al igual que el clasificador de la red neuronal. En general veriamos resultados mejores en este ultimo clasificador pero debido a la facilidad del dataset no fue el caso. Esta facilidad viene dada no solo en cuanto a la predictibilidad sino tambien la limpieza del dataset y su muy buena documentación que tampoco suele ser el caso mas normal.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}