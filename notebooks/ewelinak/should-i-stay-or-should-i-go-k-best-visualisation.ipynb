{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom IPython.display import Image\nimport os\n!ls ../input/\nImage('../input/bank-picture/bank_churns.jpg')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The core of this kernel is to find features which have the highest impact on the target variable in the Credit Card Customers dataset. In this kernel I will present the simple visualisations method and K-Best features selection algorithm.\n\n# Table of content\n* [First glance at the dataset](#First_glance)\n* [Visualizations for numerical features](#Num_vis)\n* [Visualizations for categorical features](#Cat_vis)\n* [Data preprocessing](#data_pre)\n* [Logistic Regression](#Log_reg)\n* [KBest features selection](#Kb_sel)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"First_glance\"></a>\n# First glance at the dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# Importing libraires\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import product\nfrom scipy import stats\nfrom scipy.stats import ttest_ind\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set(style=\"ticks\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dataset and removing variables releted tp Naive Baissian Classifier\ndf = pd.read_csv(\"../input/credit-card-customers/BankChurners.csv\")\ndf = df.drop(df.columns[[21, 22]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get a glance at the Bank Churn Dataset. After removing variables related to Naive Bayesian Classifier, 21 variables left. Most of them are integers or floats. Attrition Flag is a dependent variable - it tells if the customer has churned from the bank or not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"Num_vis\"></a>\n# Visualizations for numerical features\n\nLet’s visualise the distribution for each of the numeric variables. To visualise particular variables I will use distplot function from the seaborn package nested in a for loop."},{"metadata":{"trusted":true},"cell_type":"code","source":"# In the first step I create a new dataframe including only numeric variables\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnewdf = df.select_dtypes(include=numerics)\nnewdf2 = newdf.loc[:, newdf.columns != 'CLIENTNUM']\n\n# Now I'm slicing the column with Attrition_Flag variable. On the plot I' d like to present the distribution for existing\n# and churned customers separately. \n\ndf_AttiredFlag = pd.DataFrame(df['Attrition_Flag'])\n\n# Joining newdf2 and df_AttiredFlag (with inner join)\ndf_joined = pd.concat([newdf2, df_AttiredFlag], axis = 1, join = 'inner')\n\n# Plotting multiple variables in the for loop\n\ncol = 0\nfor col in df_joined[['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']]:\n    fig, axis2 = plt.subplots(figsize=(15,5))\n    sns.distplot(df_joined[df_joined['Attrition_Flag'] == 'Attrited Customer'][col], bins = 20,  label = 'Existing customers',  color = 'skyblue')\n    sns.distplot(df_joined[df_joined['Attrition_Flag'] == 'Existing Customer'][col], bins = 20,  label = 'Existing customers',  color = 'red')\n   \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on analysis of the plots created for numerical variables we can conclude that the most influential features are:\n* Contracts_Count_12_mon\n* Total_Rev_Balance\n* Total_Amt_Chng_Q4_Q1\n* Total_Trans_Amt\n* Total_Trans_Ct\n* Total_Ct_Chng_Q4_Q1\n* Avg_Utilization_Ratio\n\nIn the next part of this kernel we will confirm or reject the hypothesis that factors mentioned above have the highest impact on the final result."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"Cat_vis\"></a>\n# Visualizations for categorical features"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# In the first step I create a new dataframe with only categorical variables\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnewdf_non_num = df.select_dtypes(exclude=numerics)\nnon_numeric_df = pd.DataFrame(newdf_non_num.loc[:, newdf_non_num.columns != 'CLIENTNUM'])\n\n# Creating a list with labels for levels of particular variable\nlist_labels = []\nfor col in non_numeric_df[[ \"Gender\", \"Education_Level\", \"Marital_Status\", \"Income_Category\", \"Card_Category\"]]:   \n    list_labels.append(non_numeric_df[col].value_counts().index)\n    print(list_labels)\n    \nlists = []\nfor x in range (5):\n    lists.append([i.split()[0] for i in list_labels[x]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating plots nested in a for loop\n\ncol = 0\n#def non_num_plot (self):\nfor col, i in zip(non_numeric_df[['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']], range(5)):\n        fig, ax = plt.subplots(1,2, dpi = 100, figsize=(15,15))\n        ax[0].pie(non_numeric_df[non_numeric_df['Attrition_Flag'] == 'Attrited Customer'][col].value_counts(), labels = lists[i], autopct='%.2f%%')\n        ax[0].set_title(\"Attrited Customers\")\n        ax[1].pie(non_numeric_df[non_numeric_df['Attrition_Flag'] == 'Existing Customer'][col].value_counts(), labels = lists[i], autopct='%.2f%%')\n        ax[1].set_title(\"Existing Customers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no significant differences between plots presenting churns and existing clients categorical characteristics."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"data_pre\"></a>\n# Data preprocessing\nFor improving the accuracy of the model it is indicated to remove from the dataset highly correlated features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing correlations between particular features\ncorrelations_frame = df.corr()\nplt.subplots(figsize=(20, 10))\nsns.heatmap(correlations_frame, annot = True, cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove Client Id and the most correlated features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data preprocessing\ndf_prepared = df.drop([\"CLIENTNUM\", 'Total_Amt_Chng_Q4_Q1','Total_Trans_Amt','Avg_Open_To_Buy' , 'Avg_Utilization_Ratio'], axis = 1)\ndf_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next step we will encode categorical features as dummies variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding dummies varaibles\ndf_encoded = pd.get_dummies(df_prepared, columns=['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category'])\ndf_encoded = df_encoded.drop(['Attrition_Flag_Existing Customer'], axis =1)\ndf_encoded.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"Log_reg\"></a>\n# Logistic regression\n\nThe idea is to create a simple logistic regression model and check which variable has an impact on the final result."},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# Importing libraires\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.metrics import classification_report\n\n\n# Splitting data into train and test\nX = df_encoded.loc[:, df_encoded.columns != 'Attrition_Flag_Attrited Customer']\nY = df_encoded['Attrition_Flag_Attrited Customer']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n\n\n# Creating logistic regression model\nlog_reg = LogisticRegression()\nmodel1 = log_reg.fit(X_train, Y_train)\ny_predicted = log_reg.predict(X_test)\n\nprint(classification_report(Y_test, y_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"Kb_sel\"></a>\n# K-best features selection "},{"metadata":{},"cell_type":"markdown","source":"To check which variables are the most influential for the final result we will use Select KBest function.\n\nFind out more about Select KBest function: https://www.kaggle.com/jepsds/feature-selection-using-selectkbest?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features selection with KBest\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nselected_KBest = SelectKBest(f_classif, k=5).fit(X_train, Y_train)\nselected_KBest_df = pd.DataFrame({'Features': list(X_train.columns),\n                                'Scores': selected_KBest.scores_})\nselected_KBest_df.sort_values(by='Scores', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the “Select KBest” algorithm five features that have the highest impact on the target variables are:\n* Total_Trans_Ct\n* Total_Ct_Chng_Q4_Q1\n* Total_Revolving_Balance\n* Contacts_Count_12_Mon\n* Months_Inactive_12_Mon,\n\nResults given by the KBest features selection algorithm are consistent with what we observed on the basis of the plots."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}