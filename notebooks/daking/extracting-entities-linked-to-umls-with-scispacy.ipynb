{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Dict, Iterable, Tuple\n\nimport os\nimport json\n\nfrom tqdm import tqdm\n\nimport spacy\nfrom spacy.tokens import Span\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_nlp = spacy.load('en_core_sci_sm')\n\n# Add the abbreviation pipe to the spacy pipeline.\nabbreviation_pipe = AbbreviationDetector(full_nlp)\nfull_nlp.add_pipe(abbreviation_pipe)\n\n# Add the entity linking pipe to the spacy pipeline\nlinker = UmlsEntityLinker(resolve_abbreviations=True, filter_for_definitions=False)\nfull_nlp.add_pipe(linker)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"CORD_DATA_PATH = os.path.join(\"/kaggle\", \"input\", \"CORD-19-research-challenge\")\nNONCOMM_USE_PATH = os.path.join(CORD_DATA_PATH, \"noncomm_use_subset\", \"noncomm_use_subset\")\nCOMM_USE_PATH = os.path.join(CORD_DATA_PATH, \"comm_use_subset\", \"comm_use_subset\")\nPMC_CUSTOM_LICENSE_PATH = os.path.join(CORD_DATA_PATH, \"custom_license\", \"custom_license\")\nBIORXIV_MEDRXIV_PATH = os.path.join(CORD_DATA_PATH, \"biorxiv_medrxiv\", \"biorxiv_medrxiv\")\n\ndef load_json_files_lazy(directory_path: str) -> Iterable[Dict]:\n    \"\"\"Load the json files from a directory lazily\"\"\"\n    loaded_files = []\n    for filename in os.listdir(directory_path):\n        full_path = os.path.join(directory_path, filename)\n        with open(full_path) as _json_file:\n            loaded_file = json.load(_json_file)\n            yield loaded_file\n\nnoncomm_use_loaded_files = load_json_files_lazy(NONCOMM_USE_PATH)\ncomm_use_loaded_files = load_json_files_lazy(COMM_USE_PATH)\npmc_custom_license_loaded_files = load_json_files_lazy(PMC_CUSTOM_LICENSE_PATH)\nbiorxiv_medrxiv_loaded_files = load_json_files_lazy(BIORXIV_MEDRXIV_PATH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def build_doc_with_entities(full_text: str, abstract_text: List, body_text: List):\n    \"\"\"Build a doc using mention spans from the input file, but the scispacy linker\"\"\"\n    with full_nlp.disable_pipes(['UmlsEntityLinker', 'ner']):\n        doc = full_nlp(full_text)\n        \n    entities = []\n    character_offset = 0\n    for paragraph in abstract_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        for entity in paragraph['entity_spans']:\n            entity_start = character_offset + entity['start']\n            entity_end = character_offset + entity['end']\n            entity_span = doc.char_span(entity_start, entity_end)\n\n            # just skip for now if the character span does not align\n            if entity_span is not None:\n                entities.append(entity_span)\n\n        character_offset += len(paragraph_text) + 1\n\n    for paragraph in body_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        for entity in paragraph['entity_spans']:\n            entity_start = character_offset + entity['start']\n            entity_end = character_offset + entity['end']\n            entity_span = doc.char_span(entity_start, entity_end)\n\n            # just skip for now if the character span does not align\n            if entity_span is not None:\n                entities.append(entity_span)\n\n        character_offset += len(paragraph_text) + 1\n    \n    new_entity_spans = [Span(doc, entity_span.start, entity_span.end, label=\"Entity\") for entity_span in entities]\n    doc.ents = new_entity_spans\n    doc = linker(doc)\n    \n    return doc\n\ndef add_entities_to_file(input_json: Dict, use_existing_mentions: bool = False) -> Dict:\n    \"\"\"Copies the input json and adds the linked entities to it. \n       If you want to use entity annotations already present in the input json file,\n       set the use_existing_mentions flag, otherwise scispacy's base model will be used for NER\"\"\"\n    body_text = input_json[\"body_text\"]\n    abstract_text = input_json['abstract']\n    \n    paragraph_char_spans = []\n    char_span_index = 0\n    paragraph_index = 0\n    full_text = \"\"\n    for paragraph in abstract_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        full_text += paragraph_text + \" \"\n        paragraph_char_spans.append((\"abstract\", paragraph_index, char_span_index, char_span_index + len(paragraph_text)))\n        char_span_index = char_span_index + len(paragraph_text) + 1\n        paragraph_index += 1\n    \n    paragraph_index = 0\n    for paragraph in body_text:\n        paragraph_text = paragraph[\"text\"].strip()\n        full_text += paragraph_text + \" \"\n        paragraph_char_spans.append((\"body_text\", paragraph_index, char_span_index, char_span_index + len(paragraph_text)))\n        char_span_index = char_span_index + len(paragraph_text) + 1\n        paragraph_index += 1\n    \n    full_text = full_text[:-1]\n    \n    if not use_existing_mentions:\n        doc = full_nlp(full_text)\n    else:\n        doc = build_doc_with_entities(full_text, abstract_text, body_text)\n    \n    input_copy = input_json.copy()\n    for i, (paragraph, (section, paragraph_index, start_char, end_char)) in enumerate(zip(abstract_text + body_text, paragraph_char_spans)):\n        entities = []\n        paragraph_span = doc.char_span(start_char, end_char)\n        for mention_span in paragraph_span.ents:\n            linked_cuis_and_scores = mention_span._.umls_ents\n            # the definition, aliases, and type can be accessed via linker.umls.cui_to_entity[cui]\n            entity = {}\n            entity['start'] = mention_span.start_char - paragraph_span.start_char\n            entity['end'] = mention_span.end_char - paragraph_span.start_char\n            entity['text'] = mention_span.text\n            \n            # could filter out specific UMLS types here, if desired\n            entity['links'] = [(cui, linker.umls.cui_to_entity[cui].types[0], score) for (cui, score) in linked_cuis_and_scores]\n            entities.append(entity)\n\n        input_copy[section][paragraph_index]['entity_spans'] = entities\n    return input_copy\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_json_file(directory_path: str, file_name: str, output_json: Dict):\n    \"\"\"Write a json file out\"\"\"\n    with open(os.path.join(directory_path, file_name), 'w') as _json_file:\n        json.dump(output_json, _json_file, indent=4)\n\ndef write_subset_directory_with_entities(directory_path: str, inputs: List[Dict], num_files_to_process: int = 0):\n    \"\"\"Write the transformed jsons for a full subset directory\"\"\"\n    if not os.path.exists(directory_path):\n        os.makedirs(directory_path, exist_ok=True)\n\n    for i, file in tqdm(enumerate(inputs), desc=f\"Processing {directory_path}\"):\n        if i >= num_files_to_process:\n            break\n        new_json = add_entities_to_file(file)\n        write_json_file(directory_path, file['paper_id'] + '.json', new_json)\n\nKAGGLE_OUTPUT_DIRECTORY = os.path.join(\"/kaggle\", \"working\")\nCORD_OUTPUT_DIRECTORY = os.path.join(KAGGLE_OUTPUT_DIRECTORY, \"CORD-19-with-entities\")\nCOMM_USE_SUBSET_OUTPUT_DIRECTORY = os.path.join(CORD_OUTPUT_DIRECTORY, \"comm_use_subset\", \"comm_use_subset\")\nNONCOMM_USE_SUBSET_OUTPUT_DIRECTORY = os.path.join(CORD_OUTPUT_DIRECTORY, \"noncomm_use_subset\", \"noncomm_use_subset\")\nCUSTOM_LICENSE_OUTPUT_DIRECTORY = os.path.join(CORD_OUTPUT_DIRECTORY, \"custom_license\", \"custom_license\")\nBIORXIV_MEDRXIV_OUTPUT_DIRECTORY = os.path.join(CORD_OUTPUT_DIRECTORY, \"biorxiv_medrxiv\", \"biorxiv_medrxiv\")\n\n# with open(os.path.join(COMM_USE_PATH, '039ca136c69c998e9a2677259d9de2941a13304a.json')) as _json_file:\n#     loaded = json.load(_json_file)\n\n# with_ents = add_entities_to_file(loaded)\n\n# with_ents_2 = add_entities_to_file(with_ents, True)\n\n# with open(os.path.join(COMM_USE_SUBSET_OUTPUT_DIRECTORY, 'test.json'), 'w') as _json_file:\n#     json.dump(with_ents_2, _json_file)\n\n# we will just process a few from each subset since this is a notebook\nwrite_subset_directory_with_entities(COMM_USE_SUBSET_OUTPUT_DIRECTORY, comm_use_loaded_files, 10)\nwrite_subset_directory_with_entities(NONCOMM_USE_SUBSET_OUTPUT_DIRECTORY, noncomm_use_loaded_files, 10)\nwrite_subset_directory_with_entities(CUSTOM_LICENSE_OUTPUT_DIRECTORY, pmc_custom_license_loaded_files, 10)\nwrite_subset_directory_with_entities(BIORXIV_MEDRXIV_OUTPUT_DIRECTORY, biorxiv_medrxiv_loaded_files, 10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can look at some of the output\n# You'll note immediately that you may want to write some extra rules for extracting novel coronavirus, because it is not yet present in the UMLS knowledge base\n# One useful thing to play around with here is filtering the linked entities based on your use case and the UMLS type tree, as types higher up on the tree indicate\n# more general entities\nwith open(os.path.join(BIORXIV_MEDRXIV_OUTPUT_DIRECTORY, list(os.listdir(BIORXIV_MEDRXIV_OUTPUT_DIRECTORY))[0])) as _json_file:\n    loaded_file = json.load(_json_file)\n\nbody = loaded_file['body_text']\nfirst_paragraph = body[0]\n\nprint(first_paragraph['text'])\nprint()\nfor entity in first_paragraph['entity_spans']:\n    top_link = entity['links'][0] if len(entity['links']) > 0 else None\n    mention_text = entity['text']\n    print(f\"Mention: {mention_text}\")\n    print(linker.umls.cui_to_entity[top_link[0]] if top_link else \"No links passed the threshold\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}