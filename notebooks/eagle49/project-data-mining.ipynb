{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport time\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/berlin-airbnb-data/reviews_summary.csv')\n\nprint(\"The dataset has {} rows and {} columns.\".format(*df.shape))\n\nprint(\"It contains {} duplicates.\".format(df.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The dataset has {} rows and {} columns.\".format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.1. Dealing with Missing Values\n<a id='2.1. Dealing with Missing Values'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**language Detection **"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from langdetect import detect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def language_detection(text):\n    try:\n        return detect(text)\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf['language'] = df['comments'].apply(language_detection)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.language.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df.language.value_counts(normalize=True).head(6).sort_values().plot(kind='barh', figsize=(9,5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_eng = df[(df['language']=='en')]\ndf_de  = df[(df['language']=='de')]\ndf_fr  = df[(df['language']=='fr')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vizualisation WordCloud**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom PIL import Image\n\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud(wordcloud, language):\n    plt.figure(figsize=(12, 10))\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis(\"off\")\n    plt.title(language + ' Comments\\n', fontsize=18, fontweight='bold')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**English WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=None, max_words=200, background_color=\"lightgrey\", \n                      width=3000, height=2000,\n                      stopwords=stopwords.words('english')).generate(str(df_eng.comments.values))\n\nplot_wordcloud(wordcloud, 'English')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**German WordCloud**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=None, max_words=150, background_color=\"powderblue\",\n                      width=3000, height=2000,\n                      stopwords=stopwords.words('german')).generate(str(df_de.comments.values))\n\nplot_wordcloud(wordcloud, 'German')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**French WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=200, max_words=150, background_color=\"lightgoldenrodyellow\",\n                      #width=1600, height=800,\n                      width=3000, height=2000,\n                      stopwords=stopwords.words('french')).generate(str(df_fr.comments.values))\n\nplot_wordcloud(wordcloud, 'French')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sentiment Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign it to another name to make it easier to use\nanalyzer = SentimentIntensityAnalyzer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting only the negative score\ndef negative_score(text):\n    negative_value = analyzer.polarity_scores(text)['neg']\n    return negative_value\n\n# getting only the neutral score\ndef neutral_score(text):\n    neutral_value = analyzer.polarity_scores(text)['neu']\n    return neutral_value\n\n# getting only the positive score\ndef positive_score(text):\n    positive_value = analyzer.polarity_scores(text)['pos']\n    return positive_value\n\n# getting only the compound score\ndef compound_score(text):\n    compound_value = analyzer.polarity_scores(text)['compound']\n    return compound_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndf_eng['sentiment_neg'] = df_eng['comments'].apply(negative_score)\ndf_eng['sentiment_neu'] = df_eng['comments'].apply(neutral_score)\ndf_eng['sentiment_pos'] = df_eng['comments'].apply(positive_score)\ndf_eng['sentiment_compound'] = df_eng['comments'].apply(compound_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_eng.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pos = df_eng.loc[df_eng.sentiment_compound >= 0.7]\n\npos_comments = df_pos['comments'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neg = df_eng.loc[df_eng.sentiment_compound < 0.0]\n\nneg_comments = df_neg['comments'].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Topic Modeling with LDA**"},{"metadata":{},"cell_type":"markdown","source":"*1. Cleaning and Preprocessing*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\nexclude = set(string.punctuation)\nlemma = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(doc):\n    stop_free = \" \".join([word for word in doc.lower().split() if word not in stop])\n    punc_free = \"\".join(token for token in stop_free if token not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\n\ndoc_clean = [clean(comment).split() for comment in pos_comments] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import corpora\ndictionary = corpora.Dictionary(doc_clean)\ncorpus = [dictionary.doc2bow(text) for text in doc_clean]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n#find 3 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find 5 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_clean = [clean(comment).split() for comment in neg_comments]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = corpora.Dictionary(doc_clean)\ncorpus = [dictionary.doc2bow(text) for text in doc_clean]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find 3 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  find 5 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}