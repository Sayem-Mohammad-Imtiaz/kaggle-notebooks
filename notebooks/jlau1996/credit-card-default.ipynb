{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of contents\n\n- [Imports](#im)\n- [Read in Data](#1) \n- [EDA](#2) \n- [Data Preprocessing](#3) \n- [Modeling](#4) \n- [Hyperparameter tuning](#5) \n- [Model Interpretation](#6) \n- [Results on test set](#7) ","metadata":{}},{"cell_type":"markdown","source":"## Imports <a name=\"im\"></a>","metadata":{}},{"cell_type":"code","source":"#imports\nimport os\n%matplotlib inline\nimport string\nimport sys\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# data\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\n\n# Classifiers\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\n\n# classifiers / models\nfrom sklearn.linear_model import LogisticRegression\n\n# other\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm.sklearn import LGBMClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, plot_confusion_matrix\n\n# altair \nimport altair as alt\nalt.renderers.enable('mimetype')\nalt.data_transformers.disable_max_rows()\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:49.822014Z","iopub.execute_input":"2021-09-07T00:23:49.822432Z","iopub.status.idle":"2021-09-07T00:23:52.543576Z","shell.execute_reply.started":"2021-09-07T00:23:49.822325Z","shell.execute_reply":"2021-09-07T00:23:52.542633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read in Data <a name=\"1\"></a>","metadata":{}},{"cell_type":"code","source":"#read in data \ndata = pd.read_csv(\"../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:52.545209Z","iopub.execute_input":"2021-09-07T00:23:52.545655Z","iopub.status.idle":"2021-09-07T00:23:52.670917Z","shell.execute_reply.started":"2021-09-07T00:23:52.545613Z","shell.execute_reply":"2021-09-07T00:23:52.669959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split data into train and test splits\ntrain_df, test_df = train_test_split(data, test_size = 0.2, random_state=123)\n\n#rename default col name\ntrain_df = train_df.rename(columns={'default.payment.next.month': 'default'})\ntest_df = test_df.rename(columns={'default.payment.next.month': 'default'})\n\n#rename column for consistency\ntrain_df = train_df.rename(columns={\"PAY_0\": \"PAY_1\"})\ntest_df = test_df.rename(columns={\"PAY_0\": \"PAY_1\"})","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:52.673348Z","iopub.execute_input":"2021-09-07T00:23:52.673778Z","iopub.status.idle":"2021-09-07T00:23:52.706508Z","shell.execute_reply.started":"2021-09-07T00:23:52.673734Z","shell.execute_reply":"2021-09-07T00:23:52.705488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA <a name=\"2\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Features\n**Note: all amounts are in NT (New Taiwan) Dollars**\n\n* `ID`: ID of each client\n* `LIMIT_BAL`: Amount of given credit (includes individual and family/supplementary credit)\n* `SEX`: Male or Female\n* `EDUCATION`: graduate school, university, high school, other, or unknown\n* `MARRIAGE`: Marital Status (married, single, or other)\n* `AGE`: Age in years\n\nAll the features below use the following scale: -2: No consumption; -1: Paid in full; 0: The use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n\n* `PAY_1`, `PAY_2`, ..., `PAY_6`: Repayment status in September, August,... , April 2005\n* `BILL_AMT1`, `BILL_AMT2`, ..., `BILL_AMT6`: Amount of bill statement in September, August,... , April 2005\n* `PAY_AMT1`, `PAY_AMT2`, ..., `PAY_AMT6`: Amount of previous payment in September, August,... , April 2005\n* `default`: Default payment (1=yes, 0=no)","metadata":{}},{"cell_type":"code","source":"# renaming columns for plots for easier usage\nplot_df = train_df.copy()\nplot_df[\"SEX\"].replace({1: \"male\", 2: \"female\"}, inplace=True)\nplot_df[\"MARRIAGE\"].replace({0: 3}, inplace=True)\nplot_df[\"MARRIAGE\"].replace({1: \"married\", 2: \"single\", 3: \"other\" }, inplace=True)\nplot_df[\"EDUCATION\"].replace({0: 5, 6: 5}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:52.708298Z","iopub.execute_input":"2021-09-07T00:23:52.708715Z","iopub.status.idle":"2021-09-07T00:23:52.727077Z","shell.execute_reply.started":"2021-09-07T00:23:52.708671Z","shell.execute_reply":"2021-09-07T00:23:52.725977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alt.Chart(plot_df, title = \"Proportion of non-default (0) to default (1) cases\").mark_bar().encode(\n    alt.X(\"default:O\"),\n    alt.Y(\"count(default)\"),\n    alt.Color(\"default:N\", legend= None),\n    tooltip='count(default)'\n).properties(\n    width=300,\n    height=300\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:52.728466Z","iopub.execute_input":"2021-09-07T00:23:52.728884Z","iopub.status.idle":"2021-09-07T00:23:56.450382Z","shell.execute_reply.started":"2021-09-07T00:23:52.728843Z","shell.execute_reply":"2021-09-07T00:23:56.449135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalized counts of target class\ntrain_df[\"default\"].value_counts(normalize=\"True\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:56.451865Z","iopub.execute_input":"2021-09-07T00:23:56.452281Z","iopub.status.idle":"2021-09-07T00:23:56.466924Z","shell.execute_reply.started":"2021-09-07T00:23:56.452235Z","shell.execute_reply":"2021-09-07T00:23:56.465663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Around 22% of cases defaulted, and 78% did not default. As there is class imbalance and predicting both classes are equally as important, F1-Macro score will be used as the scoring metric. F1-macro takes the F1-score of each class and returns the average of the 2 scores. By taking the average of the 2 scores, it is ignoring the imbalanced data and weighing both classes equally. ","metadata":{}},{"cell_type":"code","source":"#default based on gender\ngender_plot = alt.Chart(plot_df, title=\"Default cases based on gender\").mark_bar().encode(\n    alt.Y('count(default)', stack =\"normalize\", axis=alt.Axis(format='%')),\n    alt.X('SEX:O'),\n    alt.Color('default:O', scale=alt.Scale(scheme='tableau10')),\n    tooltip='count(default)'\n).properties(\n    width=200,\n    height=300\n)#.configure_axisX(labelAngle=360)\n\n#default based on marriage status plot\nmarriage_plot = alt.Chart(plot_df, title=\"Default cases based on marriage\").mark_bar().encode(\n    alt.Y('count(default)', stack =\"normalize\", axis=alt.Axis(format='%')),\n    alt.X('MARRIAGE:O'),\n    alt.Color('default:O', scale=alt.Scale(scheme='tableau10')),\n    tooltip='count(default)'\n).properties(\n    width=300,\n    height=300\n)#.configure_axisX(labelAngle=360)\n\n#concat gender and marriage plot\nalt.hconcat(gender_plot, marriage_plot).configure_axisX(labelAngle=360)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:56.468287Z","iopub.execute_input":"2021-09-07T00:23:56.46862Z","iopub.status.idle":"2021-09-07T00:23:59.991628Z","shell.execute_reply.started":"2021-09-07T00:23:56.468592Z","shell.execute_reply":"2021-09-07T00:23:59.990269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> There doesn't seem to be a visual difference in defaults when looking at sex and marital status of an individual","metadata":{}},{"cell_type":"code","source":"lom = [\"PAY_6\", \"PAY_5\", \"PAY_4\", \"PAY_3\", \"PAY_2\", \"PAY_1\",]\n\nalt.Chart(plot_df).mark_bar(size=20).encode(\n   alt.X(alt.repeat('repeat'), type='quantitative'),\n   alt.Y(\"count(default)\", stack =\"normalize\", axis=alt.Axis(format='%')),\n    alt.Color('default:O', scale=alt.Scale(scheme='tableau10')),\n    tooltip='count(default)'\n).properties(height = 250, width = 250\n).repeat(repeat = lom, columns =3, title = \"Repayment Status from April - Septmber 2005\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:23:59.995206Z","iopub.execute_input":"2021-09-07T00:23:59.995693Z","iopub.status.idle":"2021-09-07T00:24:03.51747Z","shell.execute_reply.started":"2021-09-07T00:23:59.995641Z","shell.execute_reply":"2021-09-07T00:24:03.516486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The above plot explores the count of default to non-default cases for the repayment status in months April to September. \n> * Recall: -2: No consumption; -1: Paid in full; 0: The use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n> \n> It looks as though a payment delay of 1/2 months and above is associated with a higher chance of defaulting. Another observation is that values of 1 are missing from April and May, this could be due to some error when collecting the data. ","metadata":{}},{"cell_type":"code","source":"#heatmap for all features, excluding ID. \ndf_cor = train_df.drop(columns = [\"ID\"])\ncorrmat = df_cor.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(corrmat, vmax=1., square=True, annot=False, cmap=plt.cm.Blues)\nplt.title(\"Correlation map\", fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:24:03.519104Z","iopub.execute_input":"2021-09-07T00:24:03.51942Z","iopub.status.idle":"2021-09-07T00:24:04.274442Z","shell.execute_reply.started":"2021-09-07T00:24:03.519376Z","shell.execute_reply":"2021-09-07T00:24:04.273479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The correlation matrix highlights 2 interesting points; the 2 dark blue squares in the middle of the plot. One square shows that the repayment status going from month to month is positively correlated with each other. Meaning that the repayment status is likely to be similar every month, whether in month 1 or month 5. Similarly, the monthly Bill amount is also highly correlated, and this makes sense because people usually have more or less the same expenses occurring monthly. ","metadata":{}},{"cell_type":"code","source":"# available credit = Limit balance - bill amount\n# take available creidt for the 6 bill amount months and get the average available credit\n\nbill_month = [\"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\"]\n\ntmp_df = pd.DataFrame()\nfor i in bill_month:\n    tmp_df[\"avail\" + i] = plot_df[\"LIMIT_BAL\"] - plot_df[i]\n\nplot_df[\"average_available_credit\"] = round(np.mean(tmp_df, axis=1))\n\n\n# plot\nalt.Chart(plot_df, title = \"Default cases based on Average available credit\").mark_bar().encode(\n    alt.X(\"average_available_credit\", bin=True, title= \"Average available credit\"),\n    #alt.Y(\"count(LIMIT_BAL)\"),\n    alt.Y(\"count(default)\", stack =\"normalize\", axis=alt.Axis(format='%')),\n    alt.Color('default:O', scale=alt.Scale(scheme='tableau10')),\n    tooltip='count(default)'\n).properties(\n    width=500,\n    height=300\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:24:04.275681Z","iopub.execute_input":"2021-09-07T00:24:04.275968Z","iopub.status.idle":"2021-09-07T00:24:07.982772Z","shell.execute_reply.started":"2021-09-07T00:24:04.27594Z","shell.execute_reply":"2021-09-07T00:24:07.981945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The above plot shows the default to non-default counts for `average_available_credit`. Available credit is calculated by `LIMIT_BAL` - `BILL_AMT`. Taking the average of the available credits of the 6 months will help reduce noise such as one-time large purchases. As monthly bill amounts are relatively stable/predictable as shown from the correlation matrix earlier, it can be a possible indicator for future prediction. \n\n> The first and last three bins in the plot above have very few observations, leading to a hard to see pattern. If we focus on the 2nd to 7th bin (> 500 observations each), it shows that users with less available credit after monthly bill are more likely to default. The plot also shows that individuals that use more than their limit balance have the highest chance of defaulting. ","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing <a name=\"3\"></a>","metadata":{}},{"cell_type":"code","source":"#Separate data into X_train and y_train \nX_train, y_train = train_df.drop(columns = ['default']), train_df['default']\nX_test, y_test = test_df.drop(columns = ['default']), test_df['default']\n\n\n#defining features\ndrop_features = [\"ID\"]\nnumeric_features = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4','BILL_AMT5',\n                    'BILL_AMT6','PAY_AMT1','PAY_AMT2','PAY_AMT3', 'PAY_AMT4','PAY_AMT5','PAY_AMT6',\n                    'PAY_1', 'PAY_2','PAY_3', 'PAY_4', 'PAY_5','PAY_6']\ncategorical_features = ['EDUCATION', 'MARRIAGE']\nbinary_features = ['SEX']\n\n#transfomer pipelines for features\nnumeric_transformer = make_pipeline(\n    StandardScaler(),\n)\n\ncategorical_transformer = make_pipeline(\n    OneHotEncoder(handle_unknown=\"ignore\"), \n)\n\nbinary_transformer = make_pipeline(\n    OneHotEncoder(drop=\"if_binary\"), \n)\n\n#preprocessor \npreprocessor = make_column_transformer(\n    (\"drop\", drop_features),\n    (numeric_transformer, numeric_features),\n    (categorical_transformer, categorical_features),\n    (binary_transformer, binary_features),\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:24:07.983949Z","iopub.execute_input":"2021-09-07T00:24:07.984402Z","iopub.status.idle":"2021-09-07T00:24:07.996413Z","shell.execute_reply.started":"2021-09-07T00:24:07.984353Z","shell.execute_reply":"2021-09-07T00:24:07.995253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling <a name=\"4\"></a>","metadata":{}},{"cell_type":"code","source":"#function to calculate cv score with sd\ndef mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n    \"\"\"\n    Returns mean and std of cross validation\n    \"\"\"\n    scores = cross_validate(model, X_train, y_train, **kwargs)\n\n    mean_scores = pd.DataFrame(scores).mean()\n    std_scores = pd.DataFrame(scores).std()\n    out_col = []\n\n    for i in range(len(mean_scores)):\n        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n\n    return pd.Series(data=out_col, index=mean_scores.index)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:24:07.997551Z","iopub.execute_input":"2021-09-07T00:24:07.997835Z","iopub.status.idle":"2021-09-07T00:24:08.012897Z","shell.execute_reply.started":"2021-09-07T00:24:07.997804Z","shell.execute_reply":"2021-09-07T00:24:08.011968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dummy Classifier**","metadata":{}},{"cell_type":"code","source":"#store results and define metric\nresults = {}\nscoring_metric = \"f1_macro\"\n\n# dummy baseline model\ndummy = DummyClassifier()\n\nresults['Dummy'] = mean_std_cross_val_scores(dummy, X_train, y_train, return_train_score=True, scoring=scoring_metric)\npd.DataFrame(results)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:24:08.014257Z","iopub.execute_input":"2021-09-07T00:24:08.014673Z","iopub.status.idle":"2021-09-07T00:24:08.124677Z","shell.execute_reply.started":"2021-09-07T00:24:08.014547Z","shell.execute_reply":"2021-09-07T00:24:08.123689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"#logistic regression pipeline and cross validate\npipe_lr = make_pipeline(preprocessor, LogisticRegression(class_weight=\"balanced\", max_iter= 1000, random_state = 123))\n\nresults[\"Logistic_Regression\"] = mean_std_cross_val_scores(pipe_lr, X_train, y_train, return_train_score=True, scoring=scoring_metric)\n\n#print results\npd.DataFrame(results)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:24:08.125965Z","iopub.execute_input":"2021-09-07T00:24:08.126242Z","iopub.status.idle":"2021-09-07T00:24:12.822432Z","shell.execute_reply.started":"2021-09-07T00:24:08.126217Z","shell.execute_reply":"2021-09-07T00:24:12.821482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**KNN, Random Forest, and LGBM Classifiers**","metadata":{}},{"cell_type":"code","source":"#ratio for imbalanced data\nratio = np.bincount(y_train)[0] / np.bincount(y_train)[1]\n\n#pipelines for KNN, RandomForestClassifier, and LGBMClassifier \npipe_knn = make_pipeline(preprocessor, KNeighborsClassifier())\npipe_rf = make_pipeline(preprocessor, RandomForestClassifier(class_weight=\"balanced\", random_state= 123))\npipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(scale_pos_weight=ratio, random_state=123))\n\nclassifiers = {\n    \"kNN\" : pipe_knn,\n    \"random forest\": pipe_rf,\n    \"LGBM\": pipe_lgbm\n}\n\n#for loop to loop over the classifiers for cross validation \nfor name, classifier in classifiers.items():\n    results[name] = mean_std_cross_val_scores(classifier, X_train, y_train, \n                                              return_train_score=True, \n                                              scoring=scoring_metric)\n\n#print results\npd.DataFrame(results)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:24:12.824083Z","iopub.execute_input":"2021-09-07T00:24:12.824764Z","iopub.status.idle":"2021-09-07T00:26:26.78085Z","shell.execute_reply.started":"2021-09-07T00:24:12.824722Z","shell.execute_reply":"2021-09-07T00:26:26.779896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tuning <a name=\"5\"></a>\n* For simplicity, only the model with the best results using default hyperparameters will be tuned","metadata":{}},{"cell_type":"code","source":"#Parameters for LGBMClassifier Hyperparameter tuning \nparam_grid = {\n        'lgbmclassifier__max_depth': np.arange(1,50),\n        'lgbmclassifier__learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'lgbmclassifier__n_estimators': np.arange(20,100),\n        'lgbmclassifier__num_leaves': np.arange(1,100)\n}\n\n#RandomizedSearchCV for lgbm model\nrandomizedcv_lgbm = RandomizedSearchCV(pipe_lgbm, param_grid, n_jobs=-1, \n                                       n_iter=10, \n                                       cv=5, \n                                       return_train_score = True, \n                                       scoring = scoring_metric,\n                                       random_state = 123)\nrandomizedcv_lgbm.fit(X_train, y_train);","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:26:26.782131Z","iopub.execute_input":"2021-09-07T00:26:26.78259Z","iopub.status.idle":"2021-09-07T00:26:41.202762Z","shell.execute_reply.started":"2021-09-07T00:26:26.782556Z","shell.execute_reply":"2021-09-07T00:26:41.201759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#results of LGBM hyperparameter tuning \npd.DataFrame(randomizedcv_lgbm.cv_results_)[\n    [\n        \"mean_train_score\",\n        \"mean_test_score\",\n        \"param_lgbmclassifier__n_estimators\",\n        \"param_lgbmclassifier__max_depth\",\n        \"param_lgbmclassifier__learning_rate\",\n        \"param_lgbmclassifier__num_leaves\",\n        \"mean_fit_time\",\n        \"rank_test_score\",\n    ]\n].set_index(\"rank_test_score\").sort_index().head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:26:41.204206Z","iopub.execute_input":"2021-09-07T00:26:41.204709Z","iopub.status.idle":"2021-09-07T00:26:41.229038Z","shell.execute_reply.started":"2021-09-07T00:26:41.204662Z","shell.execute_reply":"2021-09-07T00:26:41.228132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use best hyperparamters on lgbm model\nbest_lgbm_model = randomizedcv_lgbm.best_estimator_\nresults[\"lgbm (tuned)\"] = mean_std_cross_val_scores(\n    best_lgbm_model, X_train, y_train, return_train_score=True, scoring=scoring_metric\n)\npd.DataFrame(results)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:26:41.230342Z","iopub.execute_input":"2021-09-07T00:26:41.230666Z","iopub.status.idle":"2021-09-07T00:26:43.893426Z","shell.execute_reply.started":"2021-09-07T00:26:41.230636Z","shell.execute_reply":"2021-09-07T00:26:43.892369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> It looks like hyperparameter tuning improved the score. The best score is currently 0.689 F1-macro using a Gradient Boosted tree model (LightGBM). ","metadata":{}},{"cell_type":"markdown","source":"# Model Interpretation <a name=\"6\"></a>","metadata":{}},{"cell_type":"code","source":"# get list of feature names after preprocessing data \ncategorical_OHE = list(\n    best_lgbm_model.named_steps[\"columntransformer\"]\n    .named_transformers_['pipeline-2']\n    .named_steps[\"onehotencoder\"]\n    .get_feature_names(categorical_features)\n)\n\nbinary_OHE = list(\n    best_lgbm_model.named_steps[\"columntransformer\"]\n    .named_transformers_['pipeline-3']\n    .named_steps[\"onehotencoder\"]\n    .get_feature_names(binary_features)\n)\n\nfeature_names = numeric_features + binary_OHE + categorical_OHE","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:26:43.894945Z","iopub.execute_input":"2021-09-07T00:26:43.895342Z","iopub.status.idle":"2021-09-07T00:26:43.902012Z","shell.execute_reply.started":"2021-09-07T00:26:43.895301Z","shell.execute_reply":"2021-09-07T00:26:43.900993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\n\n#lgbm model with best parameters \n\npreprocessor.fit(X_train, y_train)\n\nX_train_enc = pd.DataFrame(\n    data=preprocessor.transform(X_train),\n    columns=feature_names,\n    index=X_train.index,\n)\n\nlgbm_tuned = LGBMClassifier(\n    scale_pos_weight=ratio,\n    random_state=123,\n    learning_rate = randomizedcv_lgbm.best_params_[\"lgbmclassifier__learning_rate\"],\n    max_depth = randomizedcv_lgbm.best_params_[\"lgbmclassifier__max_depth\"],\n    n_estimators = randomizedcv_lgbm.best_params_[\"lgbmclassifier__n_estimators\"],\n    num_leaves = randomizedcv_lgbm.best_params_[\"lgbmclassifier__num_leaves\"],\n)\n\nlgbm_tuned.fit(X_train_enc, y_train)\nlgbm_explainer = shap.TreeExplainer(lgbm_tuned)\nlgbm_shap_values = lgbm_explainer.shap_values(X_train_enc)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:26:43.904438Z","iopub.execute_input":"2021-09-07T00:26:43.904856Z","iopub.status.idle":"2021-09-07T00:27:32.274792Z","shell.execute_reply.started":"2021-09-07T00:26:43.904813Z","shell.execute_reply":"2021-09-07T00:27:32.273832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shap summary plot\nshap.summary_plot(lgbm_shap_values[0], X_train_enc)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:27:32.276081Z","iopub.execute_input":"2021-09-07T00:27:32.276405Z","iopub.status.idle":"2021-09-07T00:27:38.071689Z","shell.execute_reply.started":"2021-09-07T00:27:32.276357Z","shell.execute_reply":"2021-09-07T00:27:38.070518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The plot above explores the feature importances of the LightGBM model. The most important features on this plot are ranked from the most important to least important starting from the top. So here we see that Pay_1, which is the repayment status of the month September is the most important feature followed by pay amount and limit balance. This also ties back to the exploratory plots from earlier, we saw that gender and marriage did not seem to have an effect on default and as we see here; those features are not as important in this model.","metadata":{}},{"cell_type":"markdown","source":"# Results on Test Set <a name=\"7\"></a>\n","metadata":{}},{"cell_type":"code","source":"# predict on test set using best model\nbest_model = randomizedcv_lgbm.best_estimator_.fit(X_train, y_train)\npreds = best_model.predict(X_test)\npd.DataFrame(f1_score(y_test, preds, average = 'macro'), index = [\"F1_macro on test set\"], columns = [\"LGBMClassifier\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:27:38.073323Z","iopub.execute_input":"2021-09-07T00:27:38.073758Z","iopub.status.idle":"2021-09-07T00:27:38.598733Z","shell.execute_reply.started":"2021-09-07T00:27:38.073716Z","shell.execute_reply":"2021-09-07T00:27:38.597736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint(classification_report(y_test, best_model.predict(X_test), target_names=[\"No default\", \"Default\"]))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:27:38.602221Z","iopub.execute_input":"2021-09-07T00:27:38.602754Z","iopub.status.idle":"2021-09-07T00:27:38.651801Z","shell.execute_reply.started":"2021-09-07T00:27:38.602706Z","shell.execute_reply":"2021-09-07T00:27:38.650739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\nplot_confusion_matrix(best_model, \n                      X_test, \n                      y_test, \n                      display_labels=[\"No default\", \"Default on payment\"], \n                      values_format=\"d\", \n                      cmap=plt.cm.Blues,)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T00:27:38.653507Z","iopub.execute_input":"2021-09-07T00:27:38.653853Z","iopub.status.idle":"2021-09-07T00:27:38.901623Z","shell.execute_reply.started":"2021-09-07T00:27:38.65382Z","shell.execute_reply":"2021-09-07T00:27:38.900562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The F1 macro score on the test set using LGBMClassifier is 0.693. I trust the results because the test score is consistent with the cross validation score and the test set has not been touched or influenced during training or preprocessing. Further improvements include feature engineering and more model exploration. \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}