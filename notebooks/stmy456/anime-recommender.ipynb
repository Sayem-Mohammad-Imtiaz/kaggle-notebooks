{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.impute import SimpleImputer\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T11:38:57.077499Z","iopub.execute_input":"2021-06-12T11:38:57.077896Z","iopub.status.idle":"2021-06-12T11:38:57.089529Z","shell.execute_reply.started":"2021-06-12T11:38:57.077861Z","shell.execute_reply":"2021-06-12T11:38:57.088449Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trying to build an anime recommender based on \n    1) just a Tf-idf analysis of the synopsis for similar recommendations, \n    2) weighting numeric and categorical columns e.g. genre/studios/score for similar recommendations, \n    3) combine both recommenders together to get a weighted recommender, and \n    4) add in preferences in the form of the rating.\n\nI'll be doing some exploratory data analysis along the way. This is my first notebook, so plz be kind :) \n\nFirst, importing in the necessary libraries and the list of animes. We use .head() to get a view of what we have. Some columns e.g. type are dropped as the type can be inferred from the number of episodes.","metadata":{}},{"cell_type":"code","source":"anime = pd.read_csv('/kaggle/input/anime-list-for-recommendation-system-june-2021/anime_list.csv')\n\nanime_cleaned = anime.copy().drop(columns = ['duration','airing','aired','background','premiered','licensors','producers','status','type'], axis = 1)\n#as most TV episodes are about 25 min per ep, background is empty, and I don't care when it's aired/premiered. \nanime_cleaned.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:38:57.09632Z","iopub.execute_input":"2021-06-12T11:38:57.096861Z","iopub.status.idle":"2021-06-12T11:38:57.371543Z","shell.execute_reply.started":"2021-06-12T11:38:57.096822Z","shell.execute_reply":"2021-06-12T11:38:57.37072Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try to use the TFIDF vectorizer to vectorise the synopsis and find those with the closest relation.","metadata":{}},{"cell_type":"code","source":"tfidfvec = TfidfVectorizer(stop_words = 'english')\nanime_cleaned['synopsis']=anime_cleaned['synopsis'].fillna('')\ntfidf_matrix = tfidfvec.fit_transform(anime_cleaned['synopsis'])\ntfidf_matrix.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:38:57.372979Z","iopub.execute_input":"2021-06-12T11:38:57.373297Z","iopub.status.idle":"2021-06-12T11:38:58.784336Z","shell.execute_reply.started":"2021-06-12T11:38:57.373265Z","shell.execute_reply":"2021-06-12T11:38:58.783406Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the linear kernel to compute similarity:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import linear_kernel\ncosine_sim=linear_kernel(tfidf_matrix,tfidf_matrix)\ncosine_sim.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:38:58.786284Z","iopub.execute_input":"2021-06-12T11:38:58.786614Z","iopub.status.idle":"2021-06-12T11:39:04.890881Z","shell.execute_reply.started":"2021-06-12T11:38:58.786582Z","shell.execute_reply":"2021-06-12T11:39:04.889889Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to create a reverse mapping of titles to the index that they have.","metadata":{}},{"cell_type":"code","source":"indices = pd.Series(anime_cleaned.index,index = anime_cleaned['title']).drop_duplicates()\nprint(indices[:5])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:39:04.892347Z","iopub.execute_input":"2021-06-12T11:39:04.892668Z","iopub.status.idle":"2021-06-12T11:39:04.901749Z","shell.execute_reply.started":"2021-06-12T11:39:04.892637Z","shell.execute_reply":"2021-06-12T11:39:04.900693Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining a recommender function to ","metadata":{}},{"cell_type":"code","source":"def recommender(title, cosine_sim = cosine_sim):\n    #get the index based on title\n    index = indices[title];\n    #get cosine similarity for title\n    cosine_sim_for_title = list(enumerate(cosine_sim[index]))\n    #sort by cosine similarity\n    sorted_by_second = sorted(cosine_sim_for_title, key = lambda x:x[1])\n    chosen = sorted_by_second[1:6]\n    anime_indices = [i[0] for i in chosen]\n    return anime_cleaned['title'].iloc[anime_indices]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:39:04.903134Z","iopub.execute_input":"2021-06-12T11:39:04.903705Z","iopub.status.idle":"2021-06-12T11:39:05.014823Z","shell.execute_reply.started":"2021-06-12T11:39:04.903648Z","shell.execute_reply":"2021-06-12T11:39:05.013533Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what they recommend for Attack on Titan.","metadata":{}},{"cell_type":"code","source":"recommender('Shingeki no Kyojin')\n#Attack on Titan's Japanese name","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:39:05.016268Z","iopub.execute_input":"2021-06-12T11:39:05.016652Z","iopub.status.idle":"2021-06-12T11:39:05.061802Z","shell.execute_reply.started":"2021-06-12T11:39:05.016618Z","shell.execute_reply":"2021-06-12T11:39:05.060814Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Part 1 down, part 2, 3 and 4 to go. The recommender function can be improved by looking at other aspects, not just the synopsis. Things like the genre, rating, etc. \n\nNeed to do some exploratory analysis first on this dataset.","metadata":{}},{"cell_type":"code","source":"import matplotlib.style as style\nstyle.use('fivethirtyeight')\nsns.histplot(anime_cleaned.score, bins =50)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:39:05.063055Z","iopub.execute_input":"2021-06-12T11:39:05.063389Z","iopub.status.idle":"2021-06-12T11:39:05.578313Z","shell.execute_reply.started":"2021-06-12T11:39:05.063344Z","shell.execute_reply":"2021-06-12T11:39:05.577166Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, the score for many animes is -1 as it's a null value. So I'll plot the data distribution of ratings of anime ignoring null values to obtain a more representative distribution of scores across animes.\n\nWhen doing the eventual recommender, we will need to impute the values of null values. Ideally they will be randomly distributed, assuming a normal distribution of mean mu and standard deviation sigma respectively. However, dealing with different columns will need different null values - score will be the random distribution, popularity, scored_by will both be replaced with 0.","metadata":{}},{"cell_type":"code","source":"toreplace = ['score','episodes','rank','popularity','scored_by']\nfor torepl in toreplace:\n    anime_cleaned[torepl].replace(to_replace= -1,value = np.nan,inplace = True)\n    \nsns.histplot(anime_cleaned['score'], bins = 50)\nprint(anime_cleaned.score.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:39:05.580669Z","iopub.execute_input":"2021-06-12T11:39:05.580955Z","iopub.status.idle":"2021-06-12T11:39:05.862752Z","shell.execute_reply.started":"2021-06-12T11:39:05.580926Z","shell.execute_reply":"2021-06-12T11:39:05.861711Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mu = anime_cleaned['score'].mean()\nsigma=np.std(anime_cleaned.score)\n\nprint(mu)\nprint(anime_cleaned['score'].quantile(0.80))\nprint(sigma)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:39:05.864231Z","iopub.execute_input":"2021-06-12T11:39:05.864558Z","iopub.status.idle":"2021-06-12T11:39:05.875131Z","shell.execute_reply.started":"2021-06-12T11:39:05.864526Z","shell.execute_reply":"2021-06-12T11:39:05.87386Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}