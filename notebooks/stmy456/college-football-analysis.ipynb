{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-17T03:25:46.274357Z","iopub.execute_input":"2021-09-17T03:25:46.275252Z","iopub.status.idle":"2021-09-17T03:25:47.80761Z","shell.execute_reply.started":"2021-09-17T03:25:46.275128Z","shell.execute_reply":"2021-09-17T03:25:47.806313Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# College Football Analysis\n\nThe college football season is upon us once again - and with it, its peculiarities. Unlike the NFL which is geared towards equality, the CFB season possesses one of the most recurrent hierarchies in football, as solid recruiting classes deliver results and more publicity, thereby giving rise to the creation of future top-tier recruiting classes. In the long run, this creates dynasties like Clemson, Alabama and Ohio State.\n\nBut how does all this performance look like in terms of the number of yards gained or lost by each school, and which offensive and defensive metrics are the best predictor of in-game success? Let's look through the dataset to find out more.\n\nFirstly, we need to do some cleaning of the data. The following actions are done on the dataset:\n1. Read the file\n2. Split the team and conference name\n3. Creation of new fields, 'Rush Percentage' and 'Pass Percentage' to determine the proportion of plays of each type\n4. Win Percentage, to even out the success of teams as different conferences play a different number of games.","metadata":{}},{"cell_type":"code","source":"path = '../input/college-football-team-stats-2019'\ncfb = []\nfiles = glob.glob(path+'/*.csv')\nfor file in files:\n    x = pd.read_csv(file, index_col = None, header = 0)\n    season = '20'+file.split('/')[3].split('.')[0][3:5]\n    x['Season']=season\n    x['Conference']=x['Team'].str.split('(', expand = True)[1].str.strip(')')\n    x['Team']=x['Team'].str.split('(', expand = True)[0]\n    cfb.append(x)\n    \ncfb_data = pd.concat(cfb)\ncfb_data = cfb_data.reset_index()\ncfb_data['Season'] = pd.to_datetime(cfb_data['Season']).dt.year\ncfb_data.rename(columns ={'Feild.Goals':'Field.Goals'},inplace = True)\ncfb_data['Pass.Percent']=cfb_data['Pass.Attempts']/(cfb_data['Pass.Attempts']+cfb_data['Rush.Attempts'])\ncfb_data['Rush.Percent']=cfb_data['Rush.Attempts']/(cfb_data['Pass.Attempts']+cfb_data['Rush.Attempts'])\ncfb_data['Win.Percent']=cfb_data['Win']/cfb_data['Games']\ncfb_data.head()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-17T03:25:47.809503Z","iopub.execute_input":"2021-09-17T03:25:47.809767Z","iopub.status.idle":"2021-09-17T03:25:48.007246Z","shell.execute_reply.started":"2021-09-17T03:25:47.809733Z","shell.execute_reply":"2021-09-17T03:25:48.006568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's open up with showing how the general success of offenses, measured through Offensive Yards per Play, translates to the number of offensive touchdowns scored and by extension the offensive rank. \n\nThe hue of the scatterplot dictates the offensive rank of a team. Feel free to hover over each point to see the details relating to individual teams.","metadata":{}},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=cfb_data['Off.Yards.Play'],y=cfb_data['Off.TDs'],\n                          mode = 'markers',\n                          marker_color = cfb_data['Off.Rank'],\n                          text = cfb_data['Team'] + \" \" + cfb_data['Season'].astype(str)                       \n                          ))\n\nfig.update_layout(title='College Football Offenses')\nfig.update_xaxes(title ='Offensive Yards per Play')\nfig.update_yaxes(title = 'Offensive Touchdowns')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-17T03:25:48.008358Z","iopub.execute_input":"2021-09-17T03:25:48.008853Z","iopub.status.idle":"2021-09-17T03:25:48.057651Z","shell.execute_reply.started":"2021-09-17T03:25:48.008811Z","shell.execute_reply":"2021-09-17T03:25:48.056901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking into the data, there are actually several peculiarities amongst it. The outliers with the highest number of offensive yards per play are the Oklahoma teams of the 2017-19 seasons, with a pass-happy 'Air Raid' offense led by Baker Mayfield, Kyler Murray and Jalen Hurts. It's well known that passing garners more offensive yards per play than rushing (more on that later), and this explains the three OKlahoma teams at the extreme right side of the graph.\n\nHaving the most touchdowns is the LSU 2019 team led by Joe Burrow and with may players taken in the first rounds of the NFL draft such as Justin Jefferson. Notably, the 2013, 2014 and 2019 Ohio State teams have relatively high offensive touchdowns despite not having as high offensive yards per play.","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=cfb_data['Pass.Yards.Attempt'],y = cfb_data['Pass.Touchdowns'], \n                         mode = 'markers',\n                         marker_color = cfb_data['Off.Rank'],\n                         showlegend = False,\n                        text = cfb_data['Team'] + \" \" + cfb_data['Season'].astype(str) + \" \"+ \"Passing\"))\nfig.add_trace(go.Scatter(x=cfb_data['Yards.Rush'],y = cfb_data['Rushing.TD'], \n                         mode = 'markers',\n                         marker_color = cfb_data['Off.Rank'],\n                         showlegend = False,\n                        text = cfb_data['Team'] + \" \" + cfb_data['Season'].astype(str) + \" \"+ \"Rushing\"))\nfig.update_layout(title='Rushing and Passing Offensive Stats')\nfig.update_xaxes(title = 'Yards per Attempt')\nfig.update_yaxes(title = 'Touchdowns')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-17T03:25:48.059113Z","iopub.execute_input":"2021-09-17T03:25:48.059578Z","iopub.status.idle":"2021-09-17T03:25:48.104611Z","shell.execute_reply.started":"2021-09-17T03:25:48.059525Z","shell.execute_reply":"2021-09-17T03:25:48.103787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rushing and Passing Offensive Stats\n\nBreaking down the offensive statistics even further into rushing and passing yards, we are able to see this division. Ohio State teams have a relatively high number of rush yards per attempt, as they have been utilising a lot of dual-threat quarterbacks which are able to rush and pass.\n\nNotably, the service academies have the lowest number of passing touchdowns, but have the highest number of yards per attempt. This is the result of them normally running the triple option offense, a run-heavy offense, that any attempts at passing are able to take the opponent by surprise and result in a large yardage gain. There is an [excellent article](https://www.sbnation.com/college-football/2016/12/10/13863464/army-navy-triple-option-offense-flexbone) on SBnation explaining why the service academies run such an offense.\n\nThe usual suspects show up for pass-heavy offenses, with the LSU team of 2019 led by Joe Burrow, Ja'Marr Chase and Justin Jefferson, the 2018, 2019 and 2020 Alabama teams with Tua Tagovailoa, Jalen Hurts and Mac Jones, and the Oklahoma teams mentioned earlier.","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nsize= cfb_data['Off.Rank']\ninverse = [1/i for i in size]\nfig.add_trace(go.Scatter(x = cfb_data['Pass.Percent'],\n                         y=cfb_data['Rush.Percent'],\n                         mode = 'markers',\n                         marker = dict(size = inverse,color = cfb_data['Off.TDs'],sizeref = 2.*max(cfb_data['Off.Rank'])/(100**2)),\n                         text = cfb_data['Team'] + \" \" + cfb_data['Season'].astype(str)))\nfig.update_layout(title ='Pass and Rush Percentages')\nfig.update_xaxes(title = 'Pass Percentage')\nfig.update_yaxes(title = 'Rush Percentage')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-17T03:25:48.105562Z","iopub.execute_input":"2021-09-17T03:25:48.105809Z","iopub.status.idle":"2021-09-17T03:25:48.144762Z","shell.execute_reply.started":"2021-09-17T03:25:48.105781Z","shell.execute_reply":"2021-09-17T03:25:48.143846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do pass and rush percentages have some correlation with success?\n\nThe size of each plot is inversely correlated to the offensive rank (i.e. the No 1 ranked offenses will have larger plots), while the colour is correlated to the number of offensive touchdowns. Somehow, Kent St had the #1 ranked offense through 12/27/2020 for the 2020 season; as they only played 4 games due to Covid, the number of touchdowns is much lower. In fact, all the dark-coloured teams that have large bubbles on the graph had such seasons in 2020.\n\nIt should be noted that highly successful offenses largely have a pass percentage between **0.35 and 0.45** and a rush percentage between **0.55 and 0.65**. The notable outliers are LSU in 2019, led by the play of Joe Burrow, and Texas Tech in 2016, which was led by Kliff Kingsbury's Air Raid and Patrick Mahomes, who we now know to be one of the best quarterbacks in the NFL.","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.metrics import mean_squared_error\n\ncfb_analysis = cfb_data[['Pass.Attempts','Pass.Completions','Interceptions.Thrown.x','Pass.Yards.Attempt','Rush.Attempts','Yards.Rush','Fumbles.Lost']]\ny=cfb_data['Win.Percent']\nss = preprocessing.StandardScaler()\nx = ss.fit_transform(cfb_analysis)\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size = .3,random_state = 123)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:48.145917Z","iopub.execute_input":"2021-09-17T03:25:48.146165Z","iopub.status.idle":"2021-09-17T03:25:48.248721Z","shell.execute_reply.started":"2021-09-17T03:25:48.146136Z","shell.execute_reply":"2021-09-17T03:25:48.247716Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How well can we predict a team's offensive success using certain metrics?\n\nWe will conduct a linear regression amongst selected metrics such as the number of pass completions and the fumbles lost in order to solve this problem. As these metrics have a large variety of absolute values - if any team has the same number of lost fumbles as that of pass attempts, something is very wrong with the team - we have to first utilise the StandardScaler in order to standardise the values. \n\nOur target variable will be the **win percentage**, the variable that we had created earlier in order to equalise between the different teams played. Our target metric will be that of mean squared error.","metadata":{}},{"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(x_train, y_train)\npredictions = lr.predict(x_test)\nerror = mean_squared_error(predictions, y_test)\nprint(error)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:48.250156Z","iopub.execute_input":"2021-09-17T03:25:48.250411Z","iopub.status.idle":"2021-09-17T03:25:48.258784Z","shell.execute_reply.started":"2021-09-17T03:25:48.250379Z","shell.execute_reply":"2021-09-17T03:25:48.257551Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An error of 0.02297 is definitely not negligible, but it can be considered to be rather good given that the mean win percentage is 0.500. This model is decent at predicting win percentage based on offensive stats - but which features contributed the most to the model?","metadata":{}},{"cell_type":"code","source":"importances = lr.coef_\nfeat_importances = pd.Series(importances, index=cfb_analysis.columns)\nfeat_importances.plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:48.260999Z","iopub.execute_input":"2021-09-17T03:25:48.261351Z","iopub.status.idle":"2021-09-17T03:25:48.532116Z","shell.execute_reply.started":"2021-09-17T03:25:48.261306Z","shell.execute_reply":"2021-09-17T03:25:48.531346Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this graph, we are able to see the most significant factors of success and their relative effects on win percentage (positive or negative). The following conclusions can be drawn:\n\n**1. Pass numbers are more significant than the success of passes or the rushing stats**\n\nThe most significant two factors are pass completions and pass attempts; on the contrary, the yards per pass and the yards per rush have about the same significance in the model. This might be explained that teams that are able to complete passes are able to get more first downs.\n\n**2. Pass attempts are inversely correlated with win percentage.**\n\nAs expected, fumbles and interceptions have a negative outcome on winning. However, passing attempts as a negative indicator seems rather counterintuitive, but makes sense upon further thinking. Teams that attempt more passes may be stuck in longer-yardage situations that takes away the possibility of rushing, or may be trying to move the ball quickly in order to make up for a deficit. \n\n**3. The best metric may not have been included in the model.**\n\nThe best metric may have been the number of pass completions per attempts. This would dictate the efficiency of the offense to a greater extent as a 'connected' metric between the two metrics which have the greatest bearing on win percentage.","metadata":{}},{"cell_type":"markdown","source":"# Are we able to achieve better results using lasso regression?\n\nLasso regression (another excellent article can be found [here](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b)) is a type of regression that incentivises the usage of models with fewer parameters, and penalises large errors to a greater extent than it does small ones. The objective of lasso is to prevent overfitting, and it normally provides a lower level of MSE for the test set while being able to provide a less complex model. \n\nBy using a range of alpha in lasso, we will plot out the MSE in order to find out the optimal lasso value for a good performance of the model - too low, and it is similar to the linear regression; too high, and we risk simplifying the model too greatly.","metadata":{}},{"cell_type":"code","source":"lerror = []\nal_range = np.arange(0,0.5,0.02)\n\nfor al in al_range:\n    lasso = Lasso(alpha = al)\n    lasso.fit(x_train, y_train)\n    lassopred = lasso.predict(x_test)\n    lasso_error = mean_squared_error(lassopred,y_test)\n    lerror.append(lasso_error)\nplt.xlabel('Lasso Alpha')\nplt.ylabel('Mean Squared Error')\nsns.lineplot(x=al_range, y=lerror)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:48.533401Z","iopub.execute_input":"2021-09-17T03:25:48.533908Z","iopub.status.idle":"2021-09-17T03:25:48.802565Z","shell.execute_reply.started":"2021-09-17T03:25:48.533876Z","shell.execute_reply":"2021-09-17T03:25:48.801598Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Why did lasso not provide results as expected?\n\nDespite the presence of a small dip in MSE with a tiny lasso alpha, the MSE greatly increased after. Since a lasso with 0.0 alpha is essentially a linear regression, it suggests that the linear regression has been optimised to a large extent to reduce MSE. This can be attributed to a few reasons:\n\n**1. Pre-selected independent variables**\n\nIn this model, I preselected independent variables instead of using all variables relating to offense in order to minimise duplication. One example of duplication would be that of rushing attempts, rushing yards, and rushing yards per attempt. It does not make sense to keep all three of these variables together, and other variables with either perfect correlation or high correlation were excluded by myself when I was creating the initial linear regression model. Thus, I had inadvertently simplified the model to a large extent such that lasso was not able to create substantial improvements after.\n\n**2. Low number of independent variables**\n\nWhile the previous point explained why the lasso was not able to give substantial improvements, it does not explain why the MSE drastically increased. This is due to the low number of independent variables meaning that the variables that remained were potentially more significant as predictors, and dropping any one of them meant that the model was weakened as a result. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfro = RandomForestRegressor(n_estimators = 150,criterion = 'mse', random_state =123, max_samples = 200, max_features = 'log2')\nrfro.fit(x_train, y_train)\nrfropredictions = rfro.predict(x_test)\nrfroerror = mean_squared_error(rfropredictions, y_test)\nprint(rfroerror)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:48.805801Z","iopub.execute_input":"2021-09-17T03:25:48.806352Z","iopub.status.idle":"2021-09-17T03:25:49.139726Z","shell.execute_reply.started":"2021-09-17T03:25:48.80631Z","shell.execute_reply":"2021-09-17T03:25:49.138579Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the Random Forest Regressor, we get a further improvement on the data performance. This is expected as decision tree-based models, such as Random Forest, are likely to give better performance than a simple linear regression.","metadata":{}},{"cell_type":"markdown","source":"# Defense Analysis\n\nNow, it is time to do the same for defence - to collate the average performance of defences and find out which predictors are most important for a good defence. First, we do a general analysis of the defences.","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=cfb_data['Yards.Play.Allowed'],y=cfb_data['Off.TDs.Allowed'],\n                          mode = 'markers',\n                          marker_color = cfb_data['Def.Rank'],\n                          text = cfb_data['Team'] + \" \" + cfb_data['Season'].astype(str)                       \n                          ))\n\nfig.update_layout(title='College Football Defenses')\nfig.update_xaxes(title = 'Yards per Play Allowed')\nfig.update_yaxes(title = 'Offensive Touchdowns Allowed')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-17T03:25:49.14127Z","iopub.execute_input":"2021-09-17T03:25:49.141796Z","iopub.status.idle":"2021-09-17T03:25:49.167983Z","shell.execute_reply.started":"2021-09-17T03:25:49.141598Z","shell.execute_reply":"2021-09-17T03:25:49.166532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen, there are the normal powerhouses - Clemson and Alabama which allow both very few yards per play and have a low number of offensive TDs allowed. However, the nature of defence as being something that varies from year to year can be seen very clearly in the number of 'one-hit wonders' in the bottom quadrant of the graph, such as Boston College or Mississippi State. This agrees with the general consensus that defensive production is less replicable than offensive production.\n\nNotably, there are some defences with a high Yards per Play allowed but very few offensive touchdowns allowed, such as Miami in 2020. This might be due to certain outlying games - the Miami defence allowing over 500 yards in the season ender of 2020 being one of them, or that colleges like these implemented a 'bend-not-break' defence, where in critical (redzone) situations, they restrict offenses to field goals/turnovers rather than giving up touchdowns.","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=cfb_data['Yards.Attempt.Allowed'],y = cfb_data['Opp.Pass.TDs.Allowed'], \n                         mode = 'markers',\n                         marker_color = cfb_data['Def.Rank'],\n                         showlegend = False,\n                        text = cfb_data['Team'] + \" \" + cfb_data['Season'].astype(str) + \" \"+ \"Opponents Passing\"))\nfig.add_trace(go.Scatter(x=cfb_data['Yds.Rush.Allowed'],y = cfb_data['Opp.Rush.Touchdowns.Allowed'], \n                         mode = 'markers',\n                         marker_color = cfb_data['Def.Rank'],\n                         showlegend = False,\n                        text = cfb_data['Team'] + \" \" + cfb_data['Season'].astype(str) + \" \"+ \" Opponents Rushing\"))\nfig.update_layout(title='Rushing and Passing Defensive Stats')\nfig.update_xaxes(title = 'Yards per Attempt Allowed')\nfig.update_yaxes(title = 'Touchdowns Allowed')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-17T03:25:49.169728Z","iopub.execute_input":"2021-09-17T03:25:49.170077Z","iopub.status.idle":"2021-09-17T03:25:49.205076Z","shell.execute_reply.started":"2021-09-17T03:25:49.170035Z","shell.execute_reply":"2021-09-17T03:25:49.204028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows the differing performance in pass and rush defense. ","metadata":{}},{"cell_type":"markdown","source":"# How well can we predict win percentage based on defensive indicators?\n\nVarious metrics such as Tackles for Loss, sacks, interceptions and fumbles were chosen for this ML. Similarly to the offensive performance ML, we will be using Linear Regression, lasso and Random Forests.","metadata":{}},{"cell_type":"code","source":"defense = cfb_data[['Yards.Attempt.Allowed','Yds.Rush.Allowed','Average.Sacks.per.Game','Tackle.For.Loss.Per.Game','Fumbles.Recovered','Opponents.Intercepted']]\nssdef = preprocessing.StandardScaler()\nxdef = ssdef.fit_transform(defense)\nxdef_train, xdef_test, ydef_train, ydef_test = train_test_split(xdef,y,test_size = 0.3, random_state = 123)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:49.206518Z","iopub.execute_input":"2021-09-17T03:25:49.206852Z","iopub.status.idle":"2021-09-17T03:25:49.229026Z","shell.execute_reply.started":"2021-09-17T03:25:49.206811Z","shell.execute_reply":"2021-09-17T03:25:49.227987Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrdef = LinearRegression()\nlrdef.fit(xdef_train, ydef_train)\npredictionsdef = lrdef.predict(xdef_test)\nerrordef = mean_squared_error(predictionsdef, ydef_test)\nprint(errordef)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:49.230569Z","iopub.execute_input":"2021-09-17T03:25:49.230928Z","iopub.status.idle":"2021-09-17T03:25:49.246021Z","shell.execute_reply.started":"2021-09-17T03:25:49.230885Z","shell.execute_reply":"2021-09-17T03:25:49.244884Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importancesd = lrdef.coef_\nfeat_importancesd = pd.Series(importancesd, index=defense.columns)\nfeat_importancesd.plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:49.247348Z","iopub.execute_input":"2021-09-17T03:25:49.247767Z","iopub.status.idle":"2021-09-17T03:25:49.483318Z","shell.execute_reply.started":"2021-09-17T03:25:49.247733Z","shell.execute_reply":"2021-09-17T03:25:49.482268Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can conclude the following from the results:\n\n**1. The average sacks per game metric was an offensive rather than defensive indicator.**\n\nIt is quite inconceivable that the sacks metric would have a negative correlation with that of win percentage, as sacks normally push an opposing quarterback further from the first-down marker and the endzone. The logical conclusion is that this was a measure of the sacks **on** the team's quarterback, rather than a sack **of** the opposing team's quarterback.\n\nDue to the lack of a suitable sack-related indicator within the dataset, I chose to keep it inside. There is code below showing the result if I took out the sacks metric. Of course, as sacks are critical to the progress of a game, there is an increase in MSE when the sacks metric is removed.\n\n**2. Interceptions are the most critical individual play in determining win percentage**\n\nThis was expected as interceptions are always live and result in a turnover of the ball. Unlike fumbles, which are live but due to the large number of people in the vicinity usually results in the recovering team being downed by contact, and tackles for loss, which end the play, interceptions can result in a solid gain of yardage in addition to recovering the ball.\n\n**3. Consistent defensive performance is more critical than 'splash' plays**\n\nTaking out sacks, the two most critical indicators are that of the passing yardage and rushing yardage allowed (as expected). While the impact of plays that result in interceptions is greater on an individual level as compared to forcing an incompletion, it is more important to hold an opposing offense to lower levels of yardage.\n\nBelow, we will use the Lasso to find if any improvements can be made.","metadata":{}},{"cell_type":"code","source":"lerrordef = []\nal_rangedef = np.arange(0,1,0.02)\n\nfor al in al_rangedef:\n    lassodef = Lasso(alpha = al)\n    lassodef.fit(xdef_train, ydef_train)\n    lassodefpred = lassodef.predict(xdef_test)\n    lassodef_error = mean_squared_error(lassodefpred,ydef_test)\n    lerrordef.append(lassodef_error)\nplt.xlabel('Lasso Alpha')\nplt.ylabel('Mean Squared Error')\nsns.lineplot(x=al_rangedef, y=lerrordef)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:49.484659Z","iopub.execute_input":"2021-09-17T03:25:49.484968Z","iopub.status.idle":"2021-09-17T03:25:49.90738Z","shell.execute_reply.started":"2021-09-17T03:25:49.484938Z","shell.execute_reply":"2021-09-17T03:25:49.906322Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lasso does not give us improved performance, likely due to the same reasons as mentioned earlier in the offensive analysis section. Now, we will try to improve on the model with Random Forests.","metadata":{}},{"cell_type":"code","source":"\nrfr = RandomForestRegressor(n_estimators = 150,criterion = 'mse', random_state =123, max_samples = 150, max_features = 'log2')\nrfr.fit(xdef_train, ydef_train)\nrfrpredictions = rfr.predict(xdef_test)\nrfrerror = mean_squared_error(rfrpredictions, ydef_test)\nprint(rfrerror)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:49.909658Z","iopub.execute_input":"2021-09-17T03:25:49.909996Z","iopub.status.idle":"2021-09-17T03:25:50.150004Z","shell.execute_reply.started":"2021-09-17T03:25:49.909966Z","shell.execute_reply":"2021-09-17T03:25:50.149272Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Why did Random Forests not work?\n\nSurprisingly, Random Forests does not give a better performance than that of linear regression. We need to find out why, as decision trees' performance is usually higher than that of a 'vanilla' linear regression. First, we will look at the feature importances of each item within the Random Forest model.","metadata":{}},{"cell_type":"code","source":"rfrimportancesd = rfr.feature_importances_\nrfrfeat_importancesd = pd.Series(importancesd, index=defense.columns)\nrfrfeat_importancesd.plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:50.151108Z","iopub.execute_input":"2021-09-17T03:25:50.151431Z","iopub.status.idle":"2021-09-17T03:25:50.398825Z","shell.execute_reply.started":"2021-09-17T03:25:50.151392Z","shell.execute_reply":"2021-09-17T03:25:50.397649Z"},"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Random Forest model ascribes similar importances to each feature as that of the linear regression. Thus, it is unlikely that the feature weights are the source of poorer performance. Rather, it is possible that the nature of the data would have resulted in Random Forests performing worse - but we first need to understand how Random Forests work.","metadata":{}},{"cell_type":"markdown","source":"![](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_0021.png)","metadata":{}},{"cell_type":"markdown","source":"As shown by this picture from scikit-learn (source: https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_0021.png), Random Forests make a decision based on a certain threshold value (if x is larger than 2, then we will follow tree A; if not, we will follow tree B). This has certain drawbacks, as it does not react well to data with many outliers present in the test set. Random Forests are not as capable of extrapolation of data as compared to a linear regression as a result.","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows = 3, cols = 2)\n\nfig.add_trace(go.Histogram(x=defense['Opponents.Intercepted'],histnorm = 'probability'),row = 1, col =1)\nfig.add_trace(go.Histogram(x=defense['Fumbles.Recovered'],histnorm = 'probability'),row = 1, col =2)\nfig.add_trace(go.Histogram(x=defense['Tackle.For.Loss.Per.Game'],histnorm = 'probability'),row = 2, col =1)\nfig.add_trace(go.Histogram(x=defense['Average.Sacks.per.Game'],histnorm = 'probability'),row = 2, col =2)\nfig.add_trace(go.Histogram(x=defense['Yds.Rush.Allowed'],histnorm = 'probability'),row = 3, col =1)\nfig.add_trace(go.Histogram(x=defense['Yards.Attempt.Allowed'],histnorm = 'probability'),row = 3, col =2)\nfig.update_layout(title='Independent Variables in Defense Analysis')\nfig.update_xaxes(title = 'Normalized Interceptions', row = 1, col =1)\nfig.update_xaxes(title = 'Normalized Fumbles Recovered', row = 1, col =2)\nfig.update_xaxes(title = 'Normalized Tackles for Loss', row = 2, col = 1)\nfig.update_xaxes(title = 'Normalized Average Sacks', row = 2, col = 2)\nfig.update_xaxes(title = 'Normalized Yards per Rush Allowed', row = 3, col =1)\nfig.update_xaxes(title = 'Normalized Pass Yards per Attempt Allowed', row = 3, col = 2)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:50.400622Z","iopub.execute_input":"2021-09-17T03:25:50.400964Z","iopub.status.idle":"2021-09-17T03:25:50.523015Z","shell.execute_reply.started":"2021-09-17T03:25:50.400922Z","shell.execute_reply":"2021-09-17T03:25:50.522319Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen from the subplots of data, while the Yards per Rush generally follows a normal distribution, visually it is obvious that the pass yards per attempt allowed metric does not really resemble that of a normal distribution - implying the presence of substantial outliers which lie far from the central peak. Given that the pass yards per attempt metric was one of the most significant metrics in the model, it follows that Random Forests would be impaired in their ability to draw conclusions from the data as a result.\n\nBelow are the MSEs from the linear regression and random forests without the sacks metric included. As the yards per attempt metric is still present, Random Forests fail to provide better performance than the ordinary linear regression.","metadata":{}},{"cell_type":"code","source":"defense2 = cfb_data[['Yards.Attempt.Allowed','Yds.Rush.Allowed','Tackle.For.Loss.Per.Game','Fumbles.Recovered','Opponents.Intercepted']]\nssdef2 = preprocessing.StandardScaler()\nxdef2 = ssdef2.fit_transform(defense2)\nxdef_train2, xdef_test2, ydef_train2, ydef_test2 = train_test_split(xdef2,y,test_size = 0.3, random_state = 123)\nlrdef2 = LinearRegression()\nlrdef2.fit(xdef_train2, ydef_train2)\npredictionsdef2 = lrdef2.predict(xdef_test2)\nerrordef2 = mean_squared_error(predictionsdef2, ydef_test2)\nprint(errordef2)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:50.524084Z","iopub.execute_input":"2021-09-17T03:25:50.52519Z","iopub.status.idle":"2021-09-17T03:25:50.541531Z","shell.execute_reply.started":"2021-09-17T03:25:50.525148Z","shell.execute_reply":"2021-09-17T03:25:50.540259Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr2 = RandomForestRegressor(n_estimators = 150,criterion = 'mse', random_state =123, max_samples = 150, max_features = 'log2')\nrfr2.fit(xdef_train2, ydef_train2)\nrfrpredictions2 = rfr2.predict(xdef_test2)\nrfrerror2 = mean_squared_error(rfrpredictions2, ydef_test2)\nprint(rfrerror2)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:25:50.542898Z","iopub.execute_input":"2021-09-17T03:25:50.543162Z","iopub.status.idle":"2021-09-17T03:25:50.786405Z","shell.execute_reply.started":"2021-09-17T03:25:50.543132Z","shell.execute_reply":"2021-09-17T03:25:50.785305Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you, and I hope you liked the notebook!","metadata":{}}]}