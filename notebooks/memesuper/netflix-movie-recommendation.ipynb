{"cells":[{"metadata":{"_cell_guid":"4092e050-1938-4283-9b18-396c60e94ee1","_uuid":"d0978db1b40af98cf11b5b185ef264a9891d183d"},"cell_type":"markdown","source":"Each data file (there are 4 of them) contains below columns:\n\n* Movie ID (as first line of each new movie record / file)\n* Customer ID\n* Rating (1 to 5)\n* Date they gave the ratings\n\nThere is another file contains the mapping of Movie ID to the movie background like name, year of release, etc"},{"metadata":{"_cell_guid":"637b34e2-b123-4b2d-8e70-97631b0321f9","_uuid":"1f60257741c703435318df7e05e2a46c6e11af63"},"cell_type":"markdown","source":"Let's import the library we needed before we get started:"},{"metadata":{"_cell_guid":"046298b9-7ef7-4e68-aef2-a1fe316be5a0","_uuid":"3bc39967a41f9ec3989f971c49916b822b0806b7","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport re\nfrom scipy.sparse import csr_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom surprise import Reader, Dataset, SVD, evaluate\nsns.set_style(\"darkgrid\")","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"be4477f1-7a11-48f4-8147-262a6198609f","_uuid":"665b9a4bceca7bb318e39f1a5825170b18c6cc63"},"cell_type":"markdown","source":"Next let's load first data file and get a feeling of how huge the dataset is:"},{"metadata":{"_cell_guid":"0343ba37-0654-469c-98e5-812ecbaca528","scrolled":false,"_uuid":"2a5476e11ee4539c129f2da35fccdacf2c296765","trusted":true},"cell_type":"code","source":"# Skip date\ndf1 = pd.read_csv('../input/combined_data_1.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n\ndf1['Rating'] = df1['Rating'].astype(float)\n\nprint('Dataset 1 shape: {}'.format(df1.shape))\nprint('-Dataset examples-')\nprint(df1.iloc[::5000000, :])","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"0a34c296-c957-4a70-a705-8fdadb2b45d1","_uuid":"9b69835a204cd7b97dec453f5dfcb82c2f5bf9bc","trusted":false,"collapsed":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5d0ced5-5376-4ff5-86f9-e642a7adbd92","_uuid":"3509640b273342e38c2635d1bb003e0d33de9e8c"},"cell_type":"markdown","source":"Let's try to load the 3 remaining dataset as well:"},{"metadata":{"_cell_guid":"4a093a49-8a80-4afd-bc13-17b84b284142","_uuid":"a6ca9915b92abd2681ae9a355d446e73b6fbe795","collapsed":true,"trusted":false},"cell_type":"code","source":"# df2 = pd.read_csv('../input/combined_data_2.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n# df3 = pd.read_csv('../input/combined_data_3.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n# df4 = pd.read_csv('../input/combined_data_4.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n\n\n# df2['Rating'] = df2['Rating'].astype(float)\n# df3['Rating'] = df3['Rating'].astype(float)\n# df4['Rating'] = df4['Rating'].astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"11ca529c-e11c-4ec1-b9e9-d6c6c45163de","_uuid":"ebf5b154314c1268b4fffdf0449172b71e393c4f"},"cell_type":"markdown","source":"Now we combine datasets:"},{"metadata":{"_cell_guid":"ded88177-b586-48f2-bf3d-e1a892aca10e","_uuid":"4ea5a28d0108d2b272f1d30cf749080c4e94e66d","trusted":true},"cell_type":"code","source":"# load less data for speed\n\ndf = df1\n# df = df1.append(df2)\n# df = df.append(df3)\n# df = df.append(df4)\n\ndf.index = np.arange(0,len(df))\nprint('Full dataset shape: {}'.format(df.shape))\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"78a857d7-1ab1-4d93-b750-9c14b4ba2c9a","_uuid":"5bfa706c8f28f965b669dcfb285c9c32c1478bad"},"cell_type":"markdown","source":"## Data viewing"},{"metadata":{"_cell_guid":"48f3f057-706a-4667-b58e-79d70893cbb1","_uuid":"b96e6aebfe14e3be18722b759654b732b8fa4d51"},"cell_type":"markdown","source":"Let's give a first look on how the data spread:"},{"metadata":{"_cell_guid":"0d82d7df-6c77-44f2-a0bc-70ae0324329f","_uuid":"7e8780821d463af5bdcee9ec2662cf27d89745e4","trusted":true},"cell_type":"code","source":"p = df.groupby('Rating')['Rating'].agg(['count'])\n\n# get movie count\nmovie_count = df.isnull().sum()[1]\n\n# get customer count\ncust_count = df['Cust_Id'].nunique() - movie_count\n\n# get rating count\nrating_count = df['Cust_Id'].count() - movie_count\n\nax = p.plot(kind = 'barh', legend = False, figsize = (15,10))\nplt.title('Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, cust_count, rating_count), fontsize=20)\nplt.axis('off')\n\nfor i in range(1,6):\n    ax.text(p.iloc[i-1][0]/4, i-1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i-1][0]*100 / p.sum()[0]), color = 'white', weight = 'bold')\n\n","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"a7394a2b-8c79-40b8-b967-765d3ae0ad10","_uuid":"dddad55f2699f3f4c02ae64a3e470c314e248643"},"cell_type":"markdown","source":"We can see that the rating tends to be relatively positive (>3). This may be due to the fact that unhappy customers tend to just leave instead of making efforts to rate. We can keep this in mind - low rating movies mean they are generally really bad"},{"metadata":{"_cell_guid":"581427e0-87df-46b1-a0af-7eb06932b1a3","_uuid":"bf7bd867b322b3e40c4eb1204d345029b4eb31b6"},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{"_cell_guid":"3165defc-df86-49a8-ba51-6abb9fa253b1","_uuid":"f232d44b5a8282bdcfbab54861bbd7990132e2c7"},"cell_type":"markdown","source":"Movie ID is really a mess import! Looping through dataframe to add Movie ID column WILL make the Kernel run out of memory as it is too inefficient. I achieve my task by first creating a numpy array with correct length then add the whole array as column into the main dataframe! Let's see how it is done below:"},{"metadata":{"_cell_guid":"d06e0993-d5ff-4f75-87a7-7659f5427ebf","_uuid":"498476341fad8d25d24090c07ea4b48299f9424a","trusted":true,"scrolled":true},"cell_type":"code","source":"df_nan = pd.DataFrame(pd.isnull(df.Rating))\ndf_nan = df_nan[df_nan['Rating'] == True]\ndf_nan = df_nan.reset_index()\n\nmovie_np = []\nmovie_id = 1\n\nfor i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n    # numpy approach\n    temp = np.full((1,i-j-1), movie_id)\n    movie_np = np.append(movie_np, temp)\n    movie_id += 1\n\n# Account for last record and corresponding length\n# numpy approach\nlast_record = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\nmovie_np = np.append(movie_np, last_record)\n\nprint('Movie numpy: {}'.format(movie_np))\nprint('Length: {}'.format(len(movie_np)))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b30cf5cda44af33e372da114cebafa6cae01df2c"},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7da935d-a055-4ce6-9509-9c0439fda1de","_uuid":"73c7888f9cf7e1d0f705c6a14019d9371eaa9bf3","trusted":true},"cell_type":"code","source":"# remove those Movie ID rows\ndf = df[pd.notnull(df['Rating'])]\n\ndf['Movie_Id'] = movie_np.astype(int)\ndf['Cust_Id'] = df['Cust_Id'].astype(int)\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])\n","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"9fa0349e-24f9-48c0-a7f9-1309b98df81c","_uuid":"2927b234482dc30125cc9c54ba18262e9bb2675a","trusted":true},"cell_type":"code","source":"print(df.iloc[1:10, :])\n","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"fd1a2d66-78b0-4191-8ca2-0caef60e91fa","_uuid":"7abf85f047576e1c8fe7742e28bd2a55d33c366c"},"cell_type":"markdown","source":"## Data slicing"},{"metadata":{"_cell_guid":"6532819a-7b08-45c4-8b25-952568d7d465","_uuid":"b0107145609698c552ad9e74fd192cbbe93c4bb3"},"cell_type":"markdown","source":"The data set now is super huge. I have tried many different ways but can't get the Kernel running as intended without memory error. Therefore I tried to reduce the data volumn by improving the data quality below:\n\n* Remove movie with too less reviews (they are relatively not popular)\n* Remove customer who give too less reviews (they are relatively less active)\n\nHaving above benchmark will have significant improvement on efficiency, since those unpopular movies and non-active customers still occupy same volumn as those popular movies and active customers in the view of matrix (NaN still occupy space). This should help improve the statistical signifiance too.\n\nLet's see how it is implemented:"},{"metadata":{"_cell_guid":"1db45c46-ee82-4db5-be2c-919258c09d47","_uuid":"b8987bf7e2cfcdc2a69fb767c4033d05240cc5a3","trusted":true},"cell_type":"code","source":"f = ['count','mean']\n\ndf_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)\ndf_movie_summary.index = df_movie_summary.index.map(int)\nmovie_benchmark = round(df_movie_summary['count'].quantile(0.8),0)\ndrop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\n\nprint('Movie minimum times of review: {}'.format(movie_benchmark))\n\ndf_cust_summary = df.groupby('Cust_Id')['Rating'].agg(f)\ndf_cust_summary.index = df_cust_summary.index.map(int)\ncust_benchmark = round(df_cust_summary['count'].quantile(0.8),0)\ndrop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\n\nprint('Customer minimum times of review: {}'.format(cust_benchmark))","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"bebeaf19-b3a0-45d9-8a91-deaff2881d71","_uuid":"bc6022b8d87bfb7679984bcbd4b928a54ef19be8"},"cell_type":"markdown","source":"Now let's trim down our data, whats the difference in data size?"},{"metadata":{"_cell_guid":"61f85e6a-3438-456b-b169-f42c0270a752","_uuid":"f09c53f0e7b7fea039437c43e5163a5a59250b70","trusted":true},"cell_type":"code","source":"print('Original Shape: {}'.format(df.shape))\ndf = df[~df['Movie_Id'].isin(drop_movie_list)]\ndf = df[~df['Cust_Id'].isin(drop_cust_list)]\nprint('After Trim Shape: {}'.format(df.shape))\nprint('-Data Examples-')\nprint(df.iloc[::5000000, :])","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"7f10cc54-4021-4748-9f2f-933d541acee4","_uuid":"ea0da55846a3795aead5d0365d5fcf91b03636ab"},"cell_type":"markdown","source":"Let's pivot the data set and put it into a giant matrix - we need it for our recommendation system:"},{"metadata":{"_cell_guid":"9e5a21fd-ccff-4fd3-aebe-cd82e5734ba9","_uuid":"528c8ecb8bbd94130e38e68362184087dcc39f83","trusted":true},"cell_type":"code","source":"df_p = pd.pivot_table(df,values='Rating',index='Movie_Id',columns='Cust_Id', fill_value=0.0)\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5925b40fa3dd119589c8480e2e64cd6bc0c79467"},"cell_type":"code","source":"df_p.iloc[1:20,1:20]\n# print(df_p.shape) \n# (900, 95325)\ndf_dic = df_p > 0\ndf_dic.iloc[0:20,0:20]","execution_count":49,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49a590d7e8b8de8900416d49044f815aedbb5926"},"cell_type":"code","source":"matr = df_p.values\nout = np.dot(matr,matr.T)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f6d370ecdbd7e422659b1d879e6de1460b8fcdbd"},"cell_type":"code","source":"matr_dic = df_dic.values*1\nout_dic = np.dot(matr_dic, matr_dic.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c41ac2ebf986619693c6e6669be36a26b0ddd40"},"cell_type":"code","source":"# save results\nnp.savetxt(\"adj_ratings.csv\", out, delimiter=\",\")\nnp.savetxt(\"adj_binary.csv\", out_dic, delimiter=\",\")\n","execution_count":57,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e84eca01af4342aeb23626b7584f1d4e17d3181"},"cell_type":"code","source":"# save movie id\n\npd.DataFrame(df_p.index).to_csv('MovieID.csv')","execution_count":63,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}