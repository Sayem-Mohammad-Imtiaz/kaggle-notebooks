{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Aim: Exploring the temporal and topographic pattern of Covid19 confirmed cases in the US\n### 1. How the case numbers fluctuate in the US across the states?\n### 2. How are the states related to each other in terms of their case fluctuation?\n\n## Methods:\n1. Calculate covariation of the fluctuation since Mid Feb to present among all the states (Pairwise Pearson correlation matrix)\n2. Perform clustering on the covariation pattern to group the states into clusters\n\n## Results\n### There are several temporal patterns across the Covid19 case fluctuation in the US (50 states + D.C.), e.g.,\n    * NY and neighbour stats saw early high rise and has been okay since (e.g., NY, CT)\n    * The South saw igh rise during the summer (e.g., AZ, FL)\n    * Mid-atlantic has been stably high to moderate except a short period in June (e.g., MD)\n    * Of couser it has been high everywhere recently...\n    * Hawaii is by its own, with high rise around August and hasn't been affected too much by the recent surge.\n    \n## Conclusion\n* States may be grouped by geography and/or similar socioeconomic status.\n* It would be intersting to see how things are going to unfold in the future."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Preparation and having a glance of the overall data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"from matplotlib import pyplot as pl\nfrom sklearn import cluster,metrics\nfrom scipy.cluster import hierarchy as hc\nfrom scipy import signal\ndef flattenMatrix(m):\n    # helper to get the upper-triangle values \n    if np.shape(m)[0] != np.shape(m)[1]:\n        print('Input is not a square %s' % m.shape)\n        return\n    n = int(m.shape[0])\n    flatv = np.zeros(int((n*(n-1))/2))\n    count = 0 \n    for i in range(n):\n        for j in range(1+i,n):\n            flatv[count] = m[i,j]\n            count += 1\n    return flatv\n# Confirmed cases in the US\ndf_us_conf = pd.read_csv('/kaggle/input/covid19-data-from-john-hopkins-university/CONVENIENT_us_confirmed_cases.csv',header=[0,1],index_col=0)\n#print(df_us_conf.shape)\nstates = np.unique(df_us_conf.columns.get_level_values(0))#print(len(states),states)\nn_states = len(states)\ndates = df_us_conf.axes[0]\ndates = dates.map(lambda x: '-'.join(x.split('/')[:-1]))\nn_days = len(dates)\n# Sum up numbers for each state\nds = np.zeros((n_states, n_days))\nfor ist, st in enumerate(states):\n    '''If to exclude cases Out of state and Unassigned:\n    counties = df_us_conf[st].axes[1]\n    ctr = (counties.map(lambda x: x.startswith('Out of')))\n    ctr = np.asarray((ctr.values | (counties=='Unassigned')),dtype=bool)\n    counties = counties[~ctr]'''\n    print('%d %s totalN=%d' % (ist,st,df_us_conf[st].sum().sum()))\n    # sum all counties\n    ds[ist] = df_us_conf[st].sum(1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"pl.figure(figsize=(18,5))\npl.plot(ds.T)\nfor ist, st in enumerate(states):\n    pl.text(ds[ist].argmax(), ds[ist].max(),st)\npl.xticks(np.arange(0,n_days,30),dates[np.arange(0,n_days,30)])\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# May do some simple smoothing to mitigate day to day variation due to uninteresting reason (e.g., weekends)\nlmbd = 1\nds_sm = np.empty_like(ds)\nfor ist, st in enumerate(states):\n    ds_sm[ist] = signal.cspline1d(ds[ist],lmbd)\npl.figure(figsize=(18,5))\npl.plot(ds_sm.T)\nfor ist, st in enumerate(states):\n    pl.text(int(ds_sm[ist].argmax()), int(ds_sm[ist].max()), st)\npl.xticks(np.arange(0,n_days,15),dates[np.arange(0,n_days,15)])\npl.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Clustering the states based on their pairwise correlation \n** Use Pearson correlation to make it scale invariant, since the numbers are not normalized by population.\n* Selecting a time-period by specifying start and end day\n* Excluding some states/regions: here I chose to exclude the territories due to potential unreliability"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# start and end date ('month-day')\ndate_begin = '2-15'\ndate_end = '11-18'\ntime_period = np.nonzero((dates==date_begin) | (dates==date_end))[0]\n# regions/states to exclude (Territories)\nstates_exclude = ['American Samoa','Diamond Princess', 'Grand Princess','Northern Mariana Islands','Guam','Virgin Islands','Puerto Rico']\nlbl_exclude_states = np.asarray([(st not in states_exclude) for st in states])\nstates_sub = states[lbl_exclude_states]\nds_tmp = ds_sm[lbl_exclude_states, time_period[0]:time_period[1]]\nprint(ds_tmp.shape)\ncm = np.corrcoef(ds_tmp) \npl.figure(figsize=(5,5))\npl.imshow(cm,cmap='jet')\npl.xticks(range(n_states-len(states_exclude)),states_sub,rotation=90)\npl.yticks(range(n_states-len(states_exclude)),states_sub)\npl.colorbar()\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Perform simple Hierarchical clustering\ndm = 1-cm \ndm[np.isnan(dm)] = 1\nZ = hc.linkage(flattenMatrix(dm), method='ward')\npl.figure(figsize=(10,4))\ndn = hc.dendrogram(Z , labels=states_sub)\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Flatten the dendrogram into given n of clusters\nn_clusters = 6\nclustering = hc.fcluster(Z,n_clusters,criterion='maxclust')#clustering\nfor i in range(1,1+n_clusters):\n    print('#%d n=%d' % (i,sum(clustering==i)))\n    print(states_sub[clustering==i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Visualize the mean time-course of each cluster of states\n### Note that the absolute numbers (i.e., y-axis) are not of interest here (thus the values are scaled).\n### The focus is the trend along the time of each cluster."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# May assign a label arbitrarily for each cluster for vis purpose, otherwise just use the first state's name plus clustersize:\nclusterNames =  []#\nfor i in range(1,1+n_clusters):\n    clusterNames.append(states_sub[clustering==i][0]+str(sum(clustering==i)))\npl.figure(figsize=(18,5))\nfor targCluster in range(1,1+n_clusters):\n    #pl.subplot(n_clusters,1,targCluster)\n    subds = ds_sm[lbl_exclude_states][clustering==targCluster]\n    subds = (subds - np.nanmean(subds))/np.nanstd(subds) #scaling for visualization purpose\n    pl.errorbar(range(n_days),np.mean(subds,0),yerr=np.std(subds,axis=0)/np.sqrt(subds.shape[0]),\\\n                linewidth=2,label = clusterNames[targCluster-1])\npl.xticks(np.arange(0,n_days,10),dates[np.arange(0,n_days,10)])\npl.axvspan(xmin=time_period[0], xmax=time_period[1],color='gray',alpha=.16)#marked the time period used for clustering\npl.legend()\npl.xlabel('Date')\npl.ylabel('Normalized confirmed case N')\npl.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}