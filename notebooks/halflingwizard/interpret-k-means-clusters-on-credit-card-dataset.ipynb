{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![credit cards](https://unsplash.com/photos/na8l3EPqpvY/download?force=true&w=640)\n# Let's Cluster some Credit Cards!\nIn this notebook I'll try to cluster some credit cards. we go through preprocessing, choose a model and train it on our data, and then evaluate and interpret our outcomes. ","metadata":{}},{"cell_type":"markdown","source":"# Libraries\nLet's begin by importing libraries. I have added comments that shows the use case of each library.","metadata":{}},{"cell_type":"code","source":"# Essentials:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# t-SNE visualization\nfrom sklearn.manifold import TSNE\n\n# imputation\nfrom sklearn.impute import KNNImputer\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n# PCA\nfrom sklearn.decomposition import PCA\n\n# K-means for Clustering\nfrom sklearn.cluster import KMeans\n\n# elbow method\nfrom yellowbrick.cluster import KElbowVisualizer\n\n# cluster metrics\nfrom sklearn.metrics import davies_bouldin_score\nfrom sklearn.metrics import silhouette_score\n\n# Silhouette Visualizer\nfrom yellowbrick.cluster import SilhouetteVisualizer","metadata":{"execution":{"iopub.status.busy":"2021-08-26T06:19:19.338526Z","iopub.execute_input":"2021-08-26T06:19:19.339219Z","iopub.status.idle":"2021-08-26T06:19:21.032021Z","shell.execute_reply.started":"2021-08-26T06:19:19.339093Z","shell.execute_reply":"2021-08-26T06:19:21.030973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Take a look at Dataset\nLet's take a look at our dataset. we're going to:\n- load the dataset\n- understand the features\n- check for missing values ...\n- ... and outliers\n- determine whether it's possible to cluster these datapoints (using t-SNE)","metadata":{}},{"cell_type":"markdown","source":"## Load the dataset:","metadata":{}},{"cell_type":"code","source":"cc_general = pd.read_csv('../input/ccdata/CC GENERAL.csv')\ncc_general.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T06:19:37.867768Z","iopub.execute_input":"2021-08-26T06:19:37.868177Z","iopub.status.idle":"2021-08-26T06:19:37.967925Z","shell.execute_reply.started":"2021-08-26T06:19:37.868147Z","shell.execute_reply":"2021-08-26T06:19:37.966984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understand the features:","metadata":{}},{"cell_type":"code","source":"cc_general.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T06:19:40.273056Z","iopub.execute_input":"2021-08-26T06:19:40.273444Z","iopub.status.idle":"2021-08-26T06:19:40.353252Z","shell.execute_reply.started":"2021-08-26T06:19:40.273413Z","shell.execute_reply":"2021-08-26T06:19:40.352236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CUSTID : Identification of Credit Card holder (Categorical)\n\nBALANCE : Balance amount left in their account to make purchases \n\nBALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n\nPURCHASES : Amount of purchases made from account\n\nONEOFFPURCHASES : Maximum purchase amount done in one-go\n\nINSTALLMENTSPURCHASES : Amount of purchase done in installment\n\nCASHADVANCE : Cash in advance given by the user\n\nPURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n\nONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n\nPURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n\nCASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n\nCASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n\nPURCHASESTRX : Numbe of purchase transactions made\n\nCREDITLIMIT : Limit of Credit Card for user\n\nPAYMENTS : Amount of Payment done by user\n\nMINIMUM_PAYMENTS : Minimum amount of payments made by user\n\nPRCFULLPAYMENT : Percent of full payment paid by user\n\nTENURE : Tenure of credit card service for user","metadata":{}},{"cell_type":"markdown","source":"## Check for missing values:","metadata":{}},{"cell_type":"code","source":"cc_general.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T06:19:43.640823Z","iopub.execute_input":"2021-08-26T06:19:43.641204Z","iopub.status.idle":"2021-08-26T06:19:43.652725Z","shell.execute_reply.started":"2021-08-26T06:19:43.64117Z","shell.execute_reply":"2021-08-26T06:19:43.651584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for outliers:\n\nUsing IQR, we can follow the below approach to find outliers:\n\n- Calculate the first and third quartile (Q1 and Q3).\n- Further, evaluate the interquartile range, IQR = Q3-Q1.\n- Estimate the lower bound, the lower bound = Q1*1.5\n- Estimate the upper bound, upper bound = Q3*1.5\n- The data points that lie outside of the lower and the upper bound are outliers.","metadata":{}},{"cell_type":"code","source":"def outlier_percent(data):\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    minimum = Q1 - (1.5 * IQR)\n    maximum = Q3 + (1.5 * IQR)\n    num_outliers =  np.sum((data < minimum) |(data > maximum))\n    num_total = data.count()\n    return (num_outliers/num_total)*100","metadata":{"execution":{"iopub.status.busy":"2021-08-26T06:19:46.763344Z","iopub.execute_input":"2021-08-26T06:19:46.763727Z","iopub.status.idle":"2021-08-26T06:19:46.769255Z","shell.execute_reply.started":"2021-08-26T06:19:46.763696Z","shell.execute_reply":"2021-08-26T06:19:46.768468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_categorical_data = cc_general.drop(['CUST_ID'], axis=1)\nfor column in non_categorical_data.columns:\n    data = non_categorical_data[column]\n    percent = str(round(outlier_percent(data), 2))\n    print(f'Outliers in \"{column}\": {percent}%')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T06:19:49.265432Z","iopub.execute_input":"2021-08-26T06:19:49.265941Z","iopub.status.idle":"2021-08-26T06:19:49.343633Z","shell.execute_reply.started":"2021-08-26T06:19:49.265899Z","shell.execute_reply":"2021-08-26T06:19:49.341954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\nin this part, I'm going to:\n1. remove the outliers\n2. impute missing data\n3. scale the data\n4. Reduce dimentions using PCA","metadata":{}},{"cell_type":"markdown","source":"## Removing the outliers\nfirst, let's get rid of the noise. we're going to first set all outliers as `NaN`, so it will be taken care of in the next stage, where we impute the missing values. ","metadata":{}},{"cell_type":"code","source":"for column in non_categorical_data.columns:\n    data = non_categorical_data[column]\n    \n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    minimum = Q1 - (1.5 * IQR)\n    maximum = Q3 + (1.5 * IQR)\n \n    outliers = ((data < minimum) |(data > maximum))\n    non_categorical_data[column].loc[outliers] = np.nan\n    \nnon_categorical_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:21:40.247238Z","iopub.execute_input":"2021-08-23T18:21:40.247713Z","iopub.status.idle":"2021-08-23T18:21:40.311308Z","shell.execute_reply.started":"2021-08-23T18:21:40.247668Z","shell.execute_reply":"2021-08-23T18:21:40.310431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imputing the missing data\nI use `KNN imputer`: Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set.","metadata":{}},{"cell_type":"code","source":"imputer = KNNImputer()\nimp_data = pd.DataFrame(imputer.fit_transform(non_categorical_data), columns=non_categorical_data.columns)\nimp_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:21:40.314503Z","iopub.execute_input":"2021-08-23T18:21:40.314842Z","iopub.status.idle":"2021-08-23T18:21:46.127414Z","shell.execute_reply.started":"2021-08-23T18:21:40.314811Z","shell.execute_reply":"2021-08-23T18:21:46.126489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scale the Data\nI use `StandardScaler`","metadata":{}},{"cell_type":"code","source":"std_imp_data = pd.DataFrame(StandardScaler().fit_transform(imp_data), columns=imp_data.columns)\nstd_imp_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:21:46.129127Z","iopub.execute_input":"2021-08-23T18:21:46.12946Z","iopub.status.idle":"2021-08-23T18:21:46.197766Z","shell.execute_reply.started":"2021-08-23T18:21:46.129423Z","shell.execute_reply":"2021-08-23T18:21:46.196686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dimention Reduction using PCA\nK-means, DBSCAN and agglomerative clustering, all use the Euclidean distance, which starts to lose its meaning when the number of dimensions starts increasing. so, before using these methods, we have to reduce the number of dimensions. I'm going to use PCA, which is by far the most popular dimensionality reduction algorithm. \n\n<div class=\"alert alert-warning\" role=\"alert\">\n  ⚠ If you are not familiar with PCA or need to learn more about it, I highly recommend you read <a href=\"https://github.com/HalflingWizard/MachineLearning/blob/main/4-%20Dimensionality%20Reduction/PCA.md\">my Notes</a> on this dimentionality reduction method, In which I cover almost anything you need to know about this algorithm.\n</div>\n\nhere I set parameter `n_components` equals to 0.9, which means that the PCA will automatically produce enough PCs that will preserve 90% of variance in the dataset.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=0.9, random_state=42)\npca.fit(std_imp_data)\nPC_names = ['PC'+str(x) for x in range(1,len(pca.components_)+1)]\npca_data = pd.DataFrame(pca.transform(std_imp_data), columns=PC_names)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:21:46.198952Z","iopub.execute_input":"2021-08-23T18:21:46.199213Z","iopub.status.idle":"2021-08-23T18:21:46.399738Z","shell.execute_reply.started":"2021-08-23T18:21:46.199186Z","shell.execute_reply":"2021-08-23T18:21:46.398594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24, 16))\nplt.imshow(pca.components_.T,\n           cmap=\"Spectral\",\n           vmin=-1,\n           vmax=1,\n          )\nplt.yticks(range(len(std_imp_data.columns)), std_imp_data.columns)\nplt.xticks(range(len(pca_data.columns)), pca_data.columns)\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Contribution\")\nplt.title(\"Contribution of Features to Components\")\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:21:46.401514Z","iopub.execute_input":"2021-08-23T18:21:46.402293Z","iopub.status.idle":"2021-08-23T18:21:47.038384Z","shell.execute_reply.started":"2021-08-23T18:21:46.402237Z","shell.execute_reply":"2021-08-23T18:21:47.037413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model\nnow that we have done the preprocessing step, we can perform K-means clustering on our data. \n\n<div class=\"alert alert-warning\" role=\"alert\">\n  ⚠ If you are not familiar with K-Means or need to learn more about it, I highly recommend you read <a href=\"https://github.com/HalflingWizard/MachineLearning/blob/main/3-%20Clustering/K-Means.md\">my Notes</a> on this clustering method, In which I cover almost anything you need to know about this algorithm.\n</div>\n\nfirst, we have to find good parameters for our model.","metadata":{}},{"cell_type":"markdown","source":"## Find the `n_clusters` parameter using the elbow method","metadata":{}},{"cell_type":"code","source":"model = KMeans(random_state=42)\ndistortion_visualizer = KElbowVisualizer(model, k=(2,10))\n\ndistortion_visualizer.fit(pca_data)       \ndistortion_visualizer.show()       ","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:21:47.039809Z","iopub.execute_input":"2021-08-23T18:21:47.040086Z","iopub.status.idle":"2021-08-23T18:22:05.115981Z","shell.execute_reply.started":"2021-08-23T18:21:47.040059Z","shell.execute_reply":"2021-08-23T18:22:05.115052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so, as you can see, it seems the best value for `k` is 4.","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:12:33.276464Z","iopub.execute_input":"2021-08-23T14:12:33.276908Z","iopub.status.idle":"2021-08-23T14:12:33.283033Z","shell.execute_reply.started":"2021-08-23T14:12:33.276874Z","shell.execute_reply":"2021-08-23T14:12:33.282123Z"}}},{"cell_type":"code","source":"km_model = KMeans(distortion_visualizer.elbow_value_, random_state=42)\nlabels = km_model.fit_predict(pca_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:05.117149Z","iopub.execute_input":"2021-08-23T18:22:05.117452Z","iopub.status.idle":"2021-08-23T18:22:06.431464Z","shell.execute_reply.started":"2021-08-23T18:22:05.11742Z","shell.execute_reply":"2021-08-23T18:22:06.430533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I add these labels to 3 dataframes:\n- `cc_general`: original dataframe\n- `std_imp_data`: imputed, standard dataframe\n- `pca_data`: Transformed data after PCA","metadata":{}},{"cell_type":"code","source":"cc_general['LABELS'] = labels\nstd_imp_data['LABELS'] = labels\npca_data['LABELS'] = labels","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:06.432706Z","iopub.execute_input":"2021-08-23T18:22:06.433009Z","iopub.status.idle":"2021-08-23T18:22:06.43975Z","shell.execute_reply.started":"2021-08-23T18:22:06.432978Z","shell.execute_reply":"2021-08-23T18:22:06.439019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how our data is distributed among these 4 clusters:","metadata":{}},{"cell_type":"code","source":"pca_data.LABELS.value_counts().plot.pie(autopct='%1.0f%%', pctdistance=0.7, labeldistance=1.1)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:06.440764Z","iopub.execute_input":"2021-08-23T18:22:06.441169Z","iopub.status.idle":"2021-08-23T18:22:06.614838Z","shell.execute_reply.started":"2021-08-23T18:22:06.441138Z","shell.execute_reply":"2021-08-23T18:22:06.613897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the Model\nLet's see how good/bad is our model.\n\nwe start by caculating two metrics: \n- **The Davis-Bouldin Index** is the average similarity between each cluster and the closest cluster. Scores range from 0 and up. 0 indicates better clustering.\n- **The Silhouette Coefficient** is a value between -1 and 1. The higher the score, the better. 1 indicates tight clusters, and 0 means overlapping clusters. ","metadata":{}},{"cell_type":"code","source":"print(f'Davies-Bouldin index = {davies_bouldin_score(pca_data, labels)}')\nprint(f'Silhouette Score = {silhouette_score(pca_data, labels)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:06.618697Z","iopub.execute_input":"2021-08-23T18:22:06.619044Z","iopub.status.idle":"2021-08-23T18:22:08.206183Z","shell.execute_reply.started":"2021-08-23T18:22:06.61901Z","shell.execute_reply":"2021-08-23T18:22:08.205015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now, let's get the Silhouette Plot:","metadata":{}},{"cell_type":"code","source":"visualizer = SilhouetteVisualizer(km_model, colors='yellowbrick')\nvisualizer.fit(pca_data.drop(['LABELS'],axis=1))\nvisualizer.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:08.207694Z","iopub.execute_input":"2021-08-23T18:22:08.207986Z","iopub.status.idle":"2021-08-23T18:22:11.5649Z","shell.execute_reply.started":"2021-08-23T18:22:08.207954Z","shell.execute_reply":"2021-08-23T18:22:11.563894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the vertical dotted red line in this plot is the average score. It looks like our clustering method is good, since each cluster bumps out above the average, and the cluster scores look decent.","metadata":{}},{"cell_type":"markdown","source":"# Interpret the results\nalright, we have nice clusters, but what do they mean? let's figure out.","metadata":{}},{"cell_type":"markdown","source":"## PCs vs Labels!\nLet's see which PCs have higher values in each label.","metadata":{}},{"cell_type":"code","source":"def spider_plot(data, title):\n    means = data.groupby(\"LABELS\").mean().to_numpy()\n    names = data.columns[0:-1]\n    label_loc = np.linspace(start=0, stop=2 * np.pi, num=len(names))\n    categories = np.arange(0, len(means))\n    plt.figure(figsize=(10,10))\n    plt.subplot(polar=True)\n    for i in range(len(means)):\n        plt.plot(label_loc, means[i], label=f'class {categories[i]}')\n    plt.title(f'Feature comparison ({title})', size=20)\n    lines, labels = plt.thetagrids(np.degrees(label_loc), labels=names)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:11.56599Z","iopub.execute_input":"2021-08-23T18:22:11.566245Z","iopub.status.idle":"2021-08-23T18:22:11.573628Z","shell.execute_reply.started":"2021-08-23T18:22:11.566219Z","shell.execute_reply":"2021-08-23T18:22:11.572532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spider_plot(pca_data, 'PCA Data')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:11.574882Z","iopub.execute_input":"2021-08-23T18:22:11.575142Z","iopub.status.idle":"2021-08-23T18:22:12.210996Z","shell.execute_reply.started":"2021-08-23T18:22:11.575113Z","shell.execute_reply":"2021-08-23T18:22:12.209965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"hmmm... It looks like PC1, PC2 and PC3 were the most important Principal Components. Let's get the same plot, this time for original features:","metadata":{}},{"cell_type":"markdown","source":"## Standard, Imputed Data vs Labels!\nthe following plot should give us a better understanding on our clusters:","metadata":{}},{"cell_type":"code","source":"spider_plot(std_imp_data, 'Std, Impt Data')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:12.212495Z","iopub.execute_input":"2021-08-23T18:22:12.212873Z","iopub.status.idle":"2021-08-23T18:22:12.965178Z","shell.execute_reply.started":"2021-08-23T18:22:12.212829Z","shell.execute_reply":"2021-08-23T18:22:12.964155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! now we are talking. I know it's hard to read, but I just want you to notice these points:\n- Class 0 contains customers who don't make a lot of money (look at their `BALANCE`, it is the lowest of all) but this doen't keep them away from purchasing stuff! in terms of `PURCHASES`, they are the second class. how do they do this? take a closer look: they don't buy stuff in one go (they have lowest ammount of `ONEOFF_PURCHASES` and `ONEOFF PURCHASES FREQUENCY`) their key to success is _instalments!_ It's easy: if you don't make enough money to buy stuff in one go, just pay over a period of time. (They have highest values of `INSTALLMENTSPURCHASES` and `PURCHASESINSTALLMENTSFREQUENCY`  I refer to these people as **Dreamers** because although they don't make much money, lack of money doesn't prevent them from reaching for their dreams!\n- Class 1 shows customers who are not very rich, and don't take risks. their `BALANCE` (amount left in their account to make purchases) is below average, and they don't purchase much. (their `PURCHASES` is below average as well, and their `PURCHASE FREQUENCY` is very low.) I call these people **Economicals**. to them, every penny is important.\n- Class 2 contains customers who have a good income (second highest `BALANCE`) and are enjoying it! they purchase a lot (highest `PURCHASESFREQUENCY`), both in installments and in one-go. let's call them **Bourgeoisie**!\n- Class 3 is mysterious. look at them! they have highest `BALANCE`, but lowest `PURCHASE` of all! it seems they only use their fortunes when they want pay in advance (highest `CASHADVANCE`, `CASHADVANCEFREQUENCY` and `CASHADVANCETRX`). A cash advance is a service provided by most credit card and charge card issuers. The service allows cardholders to withdraw cash, either through an ATM or over the counter at a bank or other financial agency, up to a certain limit. For a credit card, this will be the credit limit (or some percentage of it). so, these guys don't use their credit cards to buy stuff, instead, they get chash from ATMs to do so. why? it is beause they want to buy something illegal? Let's just call them **The Mafia** for now.","metadata":{}},{"cell_type":"markdown","source":"## Evaluating our hypothesis\nnow, I want to plot our data using only `BALANCE` and `PURCHASES`. this is my hypothesis:\n- If `BALANCE` is low and `PURCHASES` is high ➡ Class 0 (Dreamers ✨)\n- If `BALANCE` is low and `PURCHASES` is low ➡ Class 1 (Economicals 💲)\n- If `BALANCE` is high and `PURCHASES` is high ➡ Class 2 (Bourgeoisie 🛍️)\n- If `BALANCE` is high and `PURCHASES` is low ➡ Class 3 (The Mafia 🕶️)","metadata":{}},{"cell_type":"code","source":"def colorful_scatter(data):   \n    LABEL_COLOR_MAP = {0 : 'y',\n                       1 : 'g',\n                       2 : 'm',\n                       3 : 'k'\n                       }\n    sns.jointplot(data=data, x=\"BALANCE\", y=\"PURCHASES\", hue=\"LABELS\", palette=LABEL_COLOR_MAP, alpha=0.6, height=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:12.966567Z","iopub.execute_input":"2021-08-23T18:22:12.96686Z","iopub.status.idle":"2021-08-23T18:22:12.973436Z","shell.execute_reply.started":"2021-08-23T18:22:12.96683Z","shell.execute_reply":"2021-08-23T18:22:12.972415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colorful_scatter(cc_general)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:12.974549Z","iopub.execute_input":"2021-08-23T18:22:12.974852Z","iopub.status.idle":"2021-08-23T18:22:14.103711Z","shell.execute_reply.started":"2021-08-23T18:22:12.974825Z","shell.execute_reply":"2021-08-23T18:22:14.102628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's hard to read... let's use our normalized dataframe:","metadata":{}},{"cell_type":"code","source":"colorful_scatter(std_imp_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:14.107463Z","iopub.execute_input":"2021-08-23T18:22:14.107754Z","iopub.status.idle":"2021-08-23T18:22:15.283853Z","shell.execute_reply.started":"2021-08-23T18:22:14.107724Z","shell.execute_reply":"2021-08-23T18:22:15.28296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following KDE plot also helps proving my point:","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(data=std_imp_data, x=\"BALANCE\", y=\"PURCHASES\", hue=\"LABELS\", palette={0 : 'y', 1 : 'g', 2 : 'm', 3 : 'k'}, alpha=.7, height=20)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:39:50.115354Z","iopub.execute_input":"2021-08-23T18:39:50.115886Z","iopub.status.idle":"2021-08-23T18:39:58.634504Z","shell.execute_reply.started":"2021-08-23T18:39:50.11584Z","shell.execute_reply":"2021-08-23T18:39:58.633304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like my hypothesis was quite right. \nin this plot, it is clear that:\n- people of Class 0 spend a lot while they have low balance.\n- people of Class 1 have a low balance and spend less than others\n- people of Class 2 have high balance and purchase a lot\n- people of Class 3 don't purchase much, although they have lots of money\n\nwe can further investigate this hypothesis using kde plots:","metadata":{}},{"cell_type":"code","source":"def kde_plot(data,x):\n    LABEL_COLOR_MAP = {0 : 'y',\n                   1 : 'g',\n                   2 : 'm',\n                   3 : 'k'\n                   }\n    sns.kdeplot(data=data, x=x, hue=\"LABELS\", palette=LABEL_COLOR_MAP)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:15.285337Z","iopub.execute_input":"2021-08-23T18:22:15.285919Z","iopub.status.idle":"2021-08-23T18:22:15.290828Z","shell.execute_reply.started":"2021-08-23T18:22:15.285878Z","shell.execute_reply":"2021-08-23T18:22:15.289915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kde_plot(cc_general, 'PURCHASES_FREQUENCY')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:15.291991Z","iopub.execute_input":"2021-08-23T18:22:15.292322Z","iopub.status.idle":"2021-08-23T18:22:15.567904Z","shell.execute_reply.started":"2021-08-23T18:22:15.292288Z","shell.execute_reply":"2021-08-23T18:22:15.566852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"looking at this plot, it is obvious that ***the mafia*** and ***economicals*** are purchasing less often than ***dreamers*** and ***Bourgeoisie***","metadata":{}},{"cell_type":"code","source":"kde_plot(cc_general, 'PURCHASES_INSTALLMENTS_FREQUENCY')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:15.56935Z","iopub.execute_input":"2021-08-23T18:22:15.569768Z","iopub.status.idle":"2021-08-23T18:22:15.838468Z","shell.execute_reply.started":"2021-08-23T18:22:15.569718Z","shell.execute_reply":"2021-08-23T18:22:15.837423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot shows how ***the dreamers*** are trying to buy whatever they persue, by buying first and paying later.","metadata":{}},{"cell_type":"code","source":"kde_plot(cc_general, 'CASH_ADVANCE_FREQUENCY')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:22:15.839657Z","iopub.execute_input":"2021-08-23T18:22:15.839939Z","iopub.status.idle":"2021-08-23T18:22:16.108342Z","shell.execute_reply.started":"2021-08-23T18:22:15.839908Z","shell.execute_reply":"2021-08-23T18:22:16.107216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and this plot shows that the infamous ***mafia*** are getting cash from ATMs more often than other groups. should we call the cops? 😈","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\nCongrats! we found data hidden in this dataset by using cool ML tools. 🥳\n\nLet's keep learning!\n\n<div class=\"alert alert-danger\" role=\"alert\" style=\"text-align:center;\">\n    I hope you enjoyed this tutorial. If you did, please consider subscribing to <b><a href=\"https://www.youtube.com/channel/UC34Gj0-vHuBiTNEYlP7wczg\">my YouTube Channel ▶</a></b>\n</div>\n\n<center><h2><span style=\"font-family:cursive;\"> Also, please Upvode! 😜 </span></h2></center>","metadata":{}}]}