{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<table align=\"center\" width=100%>\n    <tr>\n        <td width=\"35%\">\n            <img src=\"https://www.cnet.com/a/img/TQz8Ib-VK2VLerOOig8ky841Fgs=/940x0/2019/06/27/347b8f9c-65a4-448b-b641-73a2becfbf83/vegan-wine-club.jpg\">\n        </td>\n        <td>\n            <div align=\"center\">\n                <font color=\"#7F0542 \";size=300px>\n                    <b>Wine Quality Prediction\n                    </b>\n                </font>\n            </div>\n        </td>\n    </tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"# Problem Statement üç∑üçæ","metadata":{}},{"cell_type":"markdown","source":"**Predicting the quality of wine with respect to different physiochemical parameters such as alcohol, acidity, density, pH, etc.¬∂**","metadata":{}},{"cell_type":"markdown","source":"# Data Dictionary","metadata":{}},{"cell_type":"markdown","source":"**Input variables (based on physicochemical tests):** \n\n**1 - fixed acidity** : Amount of Tartaric acid found\n\n**2 - volatile acidity** : Amount of Acetic acid found\n\n**3 - citric acid** : Amount of Citric acid found\n\n**4 - residual sugar** : Amount of sugar left post fermentation\n\n**5 - chlorides** : Amount of salts present in wine\n\n**6 - free sulfur dioxide** : Amount of Sulfur Dioxide present in free form\n\n**7 - total sulfur dioxide** : Amount of Sulfur Dioxide present in wine\n\n**8 - density** : Density of wine\n\n**9 - pH** : Indicate the pH value of wine ranging from 0 to 14\n\n**10 - sulphates** : Amount of Potassium Sulphate in wine\n\n**11 - alcohol** : Alcohol content in wine\n\n**Output variable (based on sensory data):**\n\n**12 - quality (score between 0 and 10)** : Indicates quality of wine ranging from 1 to 10 where, the higher the value the better the wine","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n\n1. **[Import Libraries](#import_lib)**\n2. **[Set Options](#set_options)**\n3. **[Read Data](#Read_Data)**\n4. **[Exploratory Data Analysis](#data_preparation)**\n    - 4.1 - [Preparing the Dataset](#Data_Preparing)\n        - 4.1.1 - [Data Dimension](#Data_Shape)\n        - 4.1.2 - [Data Types](#Data_Types)\n        - 4.1.3 - [Missing Values](#Missing_Values)\n        - 4.1.4 - [Duplicate Data](#duplicate)\n        - 4.1.5 - [Indexing](#indexing)\n        - 4.1.6 - [Final Dataset](#final_dataset)\n    - 4.2 - [Understanding the Dataset](#Data_Understanding)\n        - 4.2.1 - [Summary Statistics](#Summary_Statistics)\n        - 4.2.2 - [Correlation](#correlation)\n        - 4.2.3 - [Analyze Categorical Variables](#analyze_cat_var)\n        - 4.2.4 - [Anaylze Target Variable](#analyze_tar_var)\n        - 4.2.5 - [Analyze Relationship Between Target and Independent Variables](#analyze_tar_ind_var)\n        - 4.2.6 - [Feature Engineering](#feature_eng)\n5. **[Data Pre-Processing](#data_pre)**\n    - 5.1 - [Outliers](#out)\n        - 5.1.1 - [Discovery of Outliers](#dis_out)\n        - 5.1.2 - [Removal of Outliers](#rem_out)\n        - 5.1.3 - [Rechecking of Correlation](#rec_cor)\n    - 5.2 - [Categorical Encoding](#cat_enc)\n    - 5.3 - [Feature Scaling](#fea_sca)\n    - 5.2 - [Train-Test Split](#split)\n6. **[Logistic Regression](#log_reg)**\n7. **[Naive Bayes Algorithm](#nai_bay)**\n8. **[K Nearest Neighbors (KNN)](#knn)**\n9. **[Decision Tree for Classification](#dec_tre)**\n10. **[Random Forest](#ran_for)**\n11. **[AdaBoost](#ada)**\n12. **[Gradient Boosting](#gra_boo)**\n13. **[Extreme Gradient Boosting (XGB)](#xgb)**\n14. **[Stack Generalisation](#stack)**\n15. **[Displaying Score Summary](#dis_sco)**\n16. **[Feature Importance](#fea_imp)**\n17. **[Conclusion](#conclu)**\n18. **[Deployment](#deploy)**\n19. **[References](#Refer)**","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Libraries <a id='import_lib'></a>","metadata":{}},{"cell_type":"code","source":"# import 'Pandas' \nimport pandas as pd \n\n# import 'Numpy' \nimport numpy as np\n\n# import subpackage of Matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# import 'Seaborn' \nimport seaborn as sns\n\n# to suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n \n# to display the float values upto 6 decimal places     \npd.options.display.float_format = '{:.6f}'.format\n\n# import train-test split \nfrom sklearn.model_selection import train_test_split\n\n# import various functions from statsmodels\nimport statsmodels\nimport statsmodels.api as sm\n\n# import StandardScaler to perform scaling\nfrom sklearn.preprocessing import StandardScaler \n\n# import various functions from sklearn \nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score \n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV \n\n# import function to perform feature selection\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport scipy\nfrom scipy.stats import shapiro\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.naive_bayes import GaussianNB","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:34.931259Z","iopub.execute_input":"2021-07-19T09:35:34.931605Z","iopub.status.idle":"2021-07-19T09:35:35.055962Z","shell.execute_reply.started":"2021-07-19T09:35:34.931577Z","shell.execute_reply":"2021-07-19T09:35:35.054976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Set Options <a id='set_options'></a>","metadata":{}},{"cell_type":"code","source":"# display all columns of the dataframe\npd.options.display.max_columns = None\n# display all rows of the dataframe\npd.options.display.max_rows = None\n# return an output value upto 6 decimals\npd.options.display.float_format = '{:.6f}'.format","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.057209Z","iopub.execute_input":"2021-07-19T09:35:35.057472Z","iopub.status.idle":"2021-07-19T09:35:35.062056Z","shell.execute_reply.started":"2021-07-19T09:35:35.057446Z","shell.execute_reply":"2021-07-19T09:35:35.061106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Read Data <a id='Read_Data'></a>","metadata":{}},{"cell_type":"code","source":"# load the csv file\n# store the data in 'df_admissions'\ndf_wine = pd.read_csv('../input/d/nischithasai/wine-quality/winequalityN.csv')\n\n# display first five observations using head()\ndf_wine.head(10).style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.064058Z","iopub.execute_input":"2021-07-19T09:35:35.06434Z","iopub.status.idle":"2021-07-19T09:35:35.173277Z","shell.execute_reply.started":"2021-07-19T09:35:35.064313Z","shell.execute_reply":"2021-07-19T09:35:35.171472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.175911Z","iopub.execute_input":"2021-07-19T09:35:35.176522Z","iopub.status.idle":"2021-07-19T09:35:35.205502Z","shell.execute_reply.started":"2021-07-19T09:35:35.176454Z","shell.execute_reply":"2021-07-19T09:35:35.204475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Exploratory Data Analysis <a id='data_preparation'></a>","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Preparing the Dataset <a id='Data_Preparing'></a>","metadata":{}},{"cell_type":"markdown","source":"### 4.1.1 Data Dimensions <a id='Data_Shape'></a>","metadata":{}},{"cell_type":"code","source":"df_wine.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.207152Z","iopub.execute_input":"2021-07-19T09:35:35.207451Z","iopub.status.idle":"2021-07-19T09:35:35.213977Z","shell.execute_reply.started":"2021-07-19T09:35:35.207423Z","shell.execute_reply":"2021-07-19T09:35:35.212806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataset I have 6497 records across 13 features","metadata":{}},{"cell_type":"markdown","source":"### 4.1.2 Data Types <a id='Data_Types'></a>","metadata":{}},{"cell_type":"code","source":"df_wine.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.215309Z","iopub.execute_input":"2021-07-19T09:35:35.215825Z","iopub.status.idle":"2021-07-19T09:35:35.234647Z","shell.execute_reply.started":"2021-07-19T09:35:35.215782Z","shell.execute_reply":"2021-07-19T09:35:35.233264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataset I have **1 object, 11 float and 1 int columns**\nBut according to our metadata , the column **quality** should be off object datatype","metadata":{}},{"cell_type":"code","source":"df_wine['quality']=df_wine['quality'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.237271Z","iopub.execute_input":"2021-07-19T09:35:35.237786Z","iopub.status.idle":"2021-07-19T09:35:35.245955Z","shell.execute_reply.started":"2021-07-19T09:35:35.237734Z","shell.execute_reply":"2021-07-19T09:35:35.2445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.249031Z","iopub.execute_input":"2021-07-19T09:35:35.249603Z","iopub.status.idle":"2021-07-19T09:35:35.262211Z","shell.execute_reply.started":"2021-07-19T09:35:35.249558Z","shell.execute_reply":"2021-07-19T09:35:35.26103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After converting the datatype of **quality** our dataset contains **2 object columns, 1 int column and 11 float columns**","metadata":{}},{"cell_type":"markdown","source":"### 4.1.3 Missing Values <a id='Missing_Values'></a>","metadata":{}},{"cell_type":"code","source":"missing_value = pd.DataFrame({\n    'Missing Value': df_wine.isnull().sum(),\n    'Percentage': (df_wine.isnull().sum() / len(df_wine))*100\n})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.2642Z","iopub.execute_input":"2021-07-19T09:35:35.264866Z","iopub.status.idle":"2021-07-19T09:35:35.297916Z","shell.execute_reply.started":"2021-07-19T09:35:35.264822Z","shell.execute_reply":"2021-07-19T09:35:35.297019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_value.sort_values(by='Percentage', ascending=False).style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.299071Z","iopub.execute_input":"2021-07-19T09:35:35.299491Z","iopub.status.idle":"2021-07-19T09:35:35.314845Z","shell.execute_reply.started":"2021-07-19T09:35:35.29945Z","shell.execute_reply":"2021-07-19T09:35:35.313934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualising missing values using Heatmap**","metadata":{}},{"cell_type":"code","source":"# set the figure size\nplt.figure(figsize=(25,15))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(df_wine.isnull(), cbar=False)\n\n\n# display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:35.316102Z","iopub.execute_input":"2021-07-19T09:35:35.316405Z","iopub.status.idle":"2021-07-19T09:35:36.756097Z","shell.execute_reply.started":"2021-07-19T09:35:35.316376Z","shell.execute_reply":"2021-07-19T09:35:36.755126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have **7 columns with a few missing values** present in the dataset","metadata":{}},{"cell_type":"markdown","source":"**Missing Values Replacement**","metadata":{}},{"cell_type":"code","source":"df=df_wine[['fixed_acidity','pH','volatile_acidity','sulphates','citric_acid','chlorides','residual_sugar']]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:36.757324Z","iopub.execute_input":"2021-07-19T09:35:36.757606Z","iopub.status.idle":"2021-07-19T09:35:36.763238Z","shell.execute_reply.started":"2021-07-19T09:35:36.757578Z","shell.execute_reply":"2021-07-19T09:35:36.762311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in enumerate(df):\n    plt.figure(figsize=(30,5))\n    #sns.set_theme(style=\"darkgrid\",palette='deep')\n    sns.boxplot(x=column[1], data=  df,color='red')\n    plt.xlabel(column[1],fontsize=18)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:36.764307Z","iopub.execute_input":"2021-07-19T09:35:36.764733Z","iopub.status.idle":"2021-07-19T09:35:37.703027Z","shell.execute_reply.started":"2021-07-19T09:35:36.764681Z","shell.execute_reply":"2021-07-19T09:35:37.702039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since,there are many outliers present in the data for each column with missing values, I are replacing the null values using median.","metadata":{}},{"cell_type":"code","source":"for column in df.columns:\n    df_wine[column]= df_wine[column].fillna(df_wine[column].median())","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:37.704119Z","iopub.execute_input":"2021-07-19T09:35:37.704383Z","iopub.status.idle":"2021-07-19T09:35:37.713322Z","shell.execute_reply.started":"2021-07-19T09:35:37.704357Z","shell.execute_reply":"2021-07-19T09:35:37.712178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:37.714485Z","iopub.execute_input":"2021-07-19T09:35:37.714776Z","iopub.status.idle":"2021-07-19T09:35:37.728085Z","shell.execute_reply.started":"2021-07-19T09:35:37.714747Z","shell.execute_reply":"2021-07-19T09:35:37.726801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualising missing values using Heatmap**","metadata":{}},{"cell_type":"code","source":"# set the figure size\nplt.figure(figsize=(10,8))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(df_wine.isnull(), cbar=False)\n\n# display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:37.729541Z","iopub.execute_input":"2021-07-19T09:35:37.729955Z","iopub.status.idle":"2021-07-19T09:35:38.421749Z","shell.execute_reply.started":"2021-07-19T09:35:37.729912Z","shell.execute_reply":"2021-07-19T09:35:38.42075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, There are **No missing values** present in the dataset","metadata":{}},{"cell_type":"markdown","source":"### 4.1.4 Duplicate Data <a id='duplicate'></a>","metadata":{}},{"cell_type":"code","source":"duplicate = df_wine.duplicated().sum()\nprint('There are {} duplicated rows in the data'.format(duplicate))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.422863Z","iopub.execute_input":"2021-07-19T09:35:38.423117Z","iopub.status.idle":"2021-07-19T09:35:38.438657Z","shell.execute_reply.started":"2021-07-19T09:35:38.423093Z","shell.execute_reply":"2021-07-19T09:35:38.437498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Getting rid of duplicate data**","metadata":{}},{"cell_type":"code","source":"df_wine.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.43976Z","iopub.execute_input":"2021-07-19T09:35:38.440181Z","iopub.status.idle":"2021-07-19T09:35:38.451534Z","shell.execute_reply.started":"2021-07-19T09:35:38.44015Z","shell.execute_reply":"2021-07-19T09:35:38.450395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking for duplicate data after removal of duplicates**","metadata":{}},{"cell_type":"code","source":"duplicate = df_wine.duplicated().sum()\nprint('There are {} duplicated rows in the data'.format(duplicate))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.455235Z","iopub.execute_input":"2021-07-19T09:35:38.455531Z","iopub.status.idle":"2021-07-19T09:35:38.466526Z","shell.execute_reply.started":"2021-07-19T09:35:38.455502Z","shell.execute_reply":"2021-07-19T09:35:38.465767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.5 Indexing <a id='indexing'></a>","metadata":{}},{"cell_type":"code","source":"df_wine.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.469075Z","iopub.execute_input":"2021-07-19T09:35:38.469514Z","iopub.status.idle":"2021-07-19T09:35:38.477273Z","shell.execute_reply.started":"2021-07-19T09:35:38.469475Z","shell.execute_reply":"2021-07-19T09:35:38.476259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **5329 records** after dropping duplicates","metadata":{}},{"cell_type":"code","source":"df_wine.tail(5).style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.478556Z","iopub.execute_input":"2021-07-19T09:35:38.478859Z","iopub.status.idle":"2021-07-19T09:35:38.512648Z","shell.execute_reply.started":"2021-07-19T09:35:38.478832Z","shell.execute_reply":"2021-07-19T09:35:38.511676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The last 5 index values range from 6491-6496 but I have only 5329 records thus the indexes need to be reset**","metadata":{}},{"cell_type":"code","source":"df_wine.reset_index(inplace=True,drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.514021Z","iopub.execute_input":"2021-07-19T09:35:38.514581Z","iopub.status.idle":"2021-07-19T09:35:38.519392Z","shell.execute_reply.started":"2021-07-19T09:35:38.514538Z","shell.execute_reply":"2021-07-19T09:35:38.518252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.tail().style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.520818Z","iopub.execute_input":"2021-07-19T09:35:38.521377Z","iopub.status.idle":"2021-07-19T09:35:38.558785Z","shell.execute_reply.started":"2021-07-19T09:35:38.521336Z","shell.execute_reply":"2021-07-19T09:35:38.557699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.6 Final Dataset <a id='final_dataset'></a>","metadata":{}},{"cell_type":"code","source":"df_wine.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.559993Z","iopub.execute_input":"2021-07-19T09:35:38.560281Z","iopub.status.idle":"2021-07-19T09:35:38.56597Z","shell.execute_reply.started":"2021-07-19T09:35:38.560252Z","shell.execute_reply":"2021-07-19T09:35:38.564802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.head().style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.56723Z","iopub.execute_input":"2021-07-19T09:35:38.567606Z","iopub.status.idle":"2021-07-19T09:35:38.605473Z","shell.execute_reply.started":"2021-07-19T09:35:38.567566Z","shell.execute_reply":"2021-07-19T09:35:38.604548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final dataset has **5329 records and 13 features with no missing and duplicate values**","metadata":{}},{"cell_type":"markdown","source":"## 4.2 Understanding the Dataset <a id='Data_Understanding'></a>","metadata":{}},{"cell_type":"markdown","source":"### 4.2.1 Summary Statistics <a id='Summary_Statistics'></a>","metadata":{}},{"cell_type":"markdown","source":"**Numeric Variables**","metadata":{}},{"cell_type":"code","source":"df_wine.describe(include=np.number).style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.608934Z","iopub.execute_input":"2021-07-19T09:35:38.609254Z","iopub.status.idle":"2021-07-19T09:35:38.663807Z","shell.execute_reply.started":"2021-07-19T09:35:38.609225Z","shell.execute_reply":"2021-07-19T09:35:38.662688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above table , I can infer:\n\n    1. The minimum pH value is 2.7 and maximum pH value found is 4.01 thus all wine are acidic in nature\n    \n    2. The alocohl content in wine range from 8 to 15 with an average of 10.5\n    \n    3. The free_sulfur_dioxide in wine is less than 41 for 75% which is still 4 times less than that of total sulfur dioxide\n    \n    4. The maximum amount of fixed_acidity is 15.9 while 75% of it is less than 7.7 which implies possible outliers","metadata":{}},{"cell_type":"markdown","source":"**Categorical Variables**","metadata":{}},{"cell_type":"code","source":"df_wine.describe(include = object).style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.665294Z","iopub.execute_input":"2021-07-19T09:35:38.665614Z","iopub.status.idle":"2021-07-19T09:35:38.689276Z","shell.execute_reply.started":"2021-07-19T09:35:38.665583Z","shell.execute_reply":"2021-07-19T09:35:38.688074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above table, I can infer:\n\n    1. There are two types of wine with white frequenting more\n    \n    2. There are seven unqiue records for the quality of wine with 6 being chosen the most","metadata":{}},{"cell_type":"markdown","source":"### 4.2.2 Correlation <a id='correlation'></a>","metadata":{}},{"cell_type":"code","source":"corr_matrix=df_wine.corr()\ncorr_matrix.style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.690656Z","iopub.execute_input":"2021-07-19T09:35:38.69132Z","iopub.status.idle":"2021-07-19T09:35:38.735882Z","shell.execute_reply.started":"2021-07-19T09:35:38.691273Z","shell.execute_reply":"2021-07-19T09:35:38.734799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(11,9))\ndropSelf = np.zeros_like(corr_matrix)\ndropSelf[np.triu_indices_from(dropSelf)] = True\n\nsns.heatmap(corr_matrix, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\n\nsns.set(font_scale=1.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:38.737224Z","iopub.execute_input":"2021-07-19T09:35:38.73754Z","iopub.status.idle":"2021-07-19T09:35:39.491037Z","shell.execute_reply.started":"2021-07-19T09:35:38.737511Z","shell.execute_reply":"2021-07-19T09:35:39.490173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inferences:**\n1. free_sufur_dioxide is highly positively correlated with total_sulfur_dioxide\n    \n2. density is moderatly positively correlated with fixed_acidity and residual_sugar whilst moderately negatively        correlated with alcohol\n    \n3. Relation degrees are very low with each other, such as citric_acid, free_sulfur_dioxide, sulpahtes and pH","metadata":{}},{"cell_type":"markdown","source":"### 4.2.3 Analyse Categorical Variables <a id='analyze_cat_var'></a>","metadata":{}},{"cell_type":"code","source":"data = df_wine.groupby('type')['quality'].count()\nfig, ax = plt.subplots(figsize=[10,6])\nlabels = ['red','white']\nax = plt.pie(x=data, autopct=\"%.1f%%\", explode=[0.05]*2, labels=labels, colors=['darkred','white'],\n             wedgeprops={\"edgecolor\":\"black\"},pctdistance=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:39.492231Z","iopub.execute_input":"2021-07-19T09:35:39.492754Z","iopub.status.idle":"2021-07-19T09:35:39.591362Z","shell.execute_reply.started":"2021-07-19T09:35:39.492689Z","shell.execute_reply":"2021-07-19T09:35:39.590724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above pie chart, I can infer:\n\n     About 75% of the data is pertaining to white wine while the remaining is of red wine","metadata":{}},{"cell_type":"markdown","source":"### 4.2.4 Analyse Target Variable <a id='analyze_tar_var'></a>","metadata":{}},{"cell_type":"code","source":"quaity_mapping = { 3 : \"Low\",4 : \"Low\",5: \"Low\",6 : \"High\",7: \"High\",8 : \"High\",9 : \"High\"}\ndf_wine[\"quality\"] =  df_wine[\"quality\"].map(quaity_mapping)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:39.592819Z","iopub.execute_input":"2021-07-19T09:35:39.593448Z","iopub.status.idle":"2021-07-19T09:35:39.603291Z","shell.execute_reply.started":"2021-07-19T09:35:39.593393Z","shell.execute_reply":"2021-07-19T09:35:39.60246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_target = df_wine['quality'].copy()\n\ndf_target.value_counts()\nsns.countplot(x = df_target)\n\n# use below code to print the values in the graph\n# 'x' and 'y' gives position of the text\n# 's' is the text \nplt.text(x = -0.05, y = df_target.value_counts()[0]+1, s = str(round((df_target.value_counts()[0])*100/len(df_target),2)) + '%')\nplt.text(x = 0.95, y = df_target.value_counts()[1]+1, s = str(round((df_target.value_counts()[1])*100/len(df_target),2)) + '%')\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.yticks([0,1000,2000,3000,4000])\nplt.title('Count Plot for Target Variable (wine_quality)', fontsize = 15)\nplt.xlabel('Target Variable', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.tight_layout()\n# to show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:39.604689Z","iopub.execute_input":"2021-07-19T09:35:39.605391Z","iopub.status.idle":"2021-07-19T09:35:39.842004Z","shell.execute_reply.started":"2021-07-19T09:35:39.60535Z","shell.execute_reply":"2021-07-19T09:35:39.840933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graph, I can infer:\n\n     Majority of the wine is high quality and the target column is balanced.","metadata":{}},{"cell_type":"markdown","source":"### 4.2.5 Analyse Relationship between Target and Independent Variables <a id='analyze_tar_ind_var'></a>","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,4))\npd.options.display.float_format = '{:,.2f}'.format\n\nbar_chart = df_wine.groupby(['type','quality'])['quality'].count().unstack('type')\nbar_chart= (bar_chart.T/bar_chart.T.sum()).T\nax = bar_chart.plot(kind='bar', stacked=True, color=['r','w'], edgecolor='black', ax=ax)\n\nlabels = []\nfor j in bar_chart.columns:\n    for i in bar_chart.index:\n          label = str('{0:.2%}'.format(bar_chart.loc[i][j]))\n          labels.append(label)\n\npatches = ax.patches\n\nfor label, rect in zip(labels, patches):\n    width = rect.get_width()\n    if width > 0:\n        x = rect.get_x()\n        y = rect.get_y()\n        height = rect.get_height()\n        ax.text(x + width/2., y + height/2., label, ha='center', va='center', color='black')\n\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=0)\nax.set_yticklabels(labels='')\nax.set_ylabel('% of records')\nplt.legend(bbox_to_anchor = (1, 1.01), edgecolor='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:39.843241Z","iopub.execute_input":"2021-07-19T09:35:39.843523Z","iopub.status.idle":"2021-07-19T09:35:40.19843Z","shell.execute_reply.started":"2021-07-19T09:35:39.843496Z","shell.execute_reply":"2021-07-19T09:35:40.197382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graph:\n\n    I can infer that majority is white wine compared to red wine in both high and low quality.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=df_wine, x=\"quality\", y =\"alcohol\").set(title='Quality v/s Alcohol')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-19T09:35:40.199622Z","iopub.execute_input":"2021-07-19T09:35:40.199936Z","iopub.status.idle":"2021-07-19T09:35:40.362816Z","shell.execute_reply.started":"2021-07-19T09:35:40.199907Z","shell.execute_reply":"2021-07-19T09:35:40.3618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above boxplot, I can infer:\n\n     Wine with high alcohol content have gotten higher ratings than that of wine with low alcohol content.","metadata":{}},{"cell_type":"code","source":"def KdeAndBox(at1,at2):\n    plt.figure(figsize=(14,9))\n    plt.subplot(2,2,1)\n    sns.kdeplot(df_wine.loc[df_wine[\"quality\"]==\"Low\"][at1],shade=True)\n    sns.kdeplot(df_wine.loc[df_wine[\"quality\"]==\"High\"][at1],shade=True)\n\n    plt.legend([\"Low\",\"High\"])\n    plt.title(at1.upper(),fontsize=15)\n    plt.subplot(2,2,2)\n    sns.kdeplot(df_wine.loc[df_wine[\"quality\"]==\"Low\"][at2],shade=True)\n    sns.kdeplot(df_wine.loc[df_wine[\"quality\"]==\"High\"][at2],shade=True)\n    plt.legend([\"Low\",\"High\"])\n    plt.title(at2.upper(),fontsize=15)\n    plt.subplot(2,2,3)\n    sns.violinplot(data=df_wine,y=at1,x=\"quality\")\n    plt.subplot(2,2,4)\n    sns.violinplot(data=df_wine,y=at2,x=\"quality\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:40.365687Z","iopub.execute_input":"2021-07-19T09:35:40.366005Z","iopub.status.idle":"2021-07-19T09:35:40.375221Z","shell.execute_reply.started":"2021-07-19T09:35:40.365976Z","shell.execute_reply":"2021-07-19T09:35:40.374305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KdeAndBox(\"fixed_acidity\",\"volatile_acidity\")","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:40.37641Z","iopub.execute_input":"2021-07-19T09:35:40.376728Z","iopub.status.idle":"2021-07-19T09:35:41.335349Z","shell.execute_reply.started":"2021-07-19T09:35:40.376685Z","shell.execute_reply":"2021-07-19T09:35:41.334284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above KDE plot, I can infer:\n    1. The distribution for both, volatile_acidity and fixed_acidity for Low and High wine quality seem to be highly        positively skewed.\n    2. For fixed_acidity, values between 6-7.5 depict highest probability density irrespective of quality and there is not   much difference between the probability density for Low and High.\n    3. For volatile_acidity, values between 0-0.5 depict highest probabilty density irrespective of quality but the         probabiltity density for High quality wine is greater than that of Low quality.","metadata":{}},{"cell_type":"code","source":"KdeAndBox(\"citric_acid\",\"alcohol\")","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:41.336891Z","iopub.execute_input":"2021-07-19T09:35:41.33748Z","iopub.status.idle":"2021-07-19T09:35:42.149546Z","shell.execute_reply.started":"2021-07-19T09:35:41.337439Z","shell.execute_reply":"2021-07-19T09:35:42.148793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above KDE plot, I can infer:\n    1. The distribution for citric_acid when the quality is Low seems to be close to normally distributed whilst when        quality is High it seems to be moderatley positively skewed\n    2. The distribution for alcohol when the quality is Low seems to be highly positively skewed whilst when the quality is High the distribution seems very close to being normally distributed\n    3. For citric_acid, values between 0-0.5 depict highest probability density irrespective of quality and there is a lot  of difference between the probability density for Low and High.\n    4. For alcohol, values between 8-10 depict highest probabilty density for Low quality wine while high quality wine seems to be normally distributed with the highest point between 10-12","metadata":{}},{"cell_type":"code","source":"KdeAndBox(\"chlorides\",\"density\")","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:42.150562Z","iopub.execute_input":"2021-07-19T09:35:42.150983Z","iopub.status.idle":"2021-07-19T09:35:42.962727Z","shell.execute_reply.started":"2021-07-19T09:35:42.150954Z","shell.execute_reply":"2021-07-19T09:35:42.961945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above KDE plot, I can infer:\n    1. The distribution for chlorides seems to be extremely highly positive skewed irrespective of the quality\n    2. The distribution for density when the quality is Low seems to be negatively skewed whilst when the quality is High   the distribution seems to be positively skewed\n    3. For chlorides, values between 0-0.1 depict highest probability density irrespective of quality and there is a lot  of difference between the probability density for Low and High.\n    4. For density, values between 0.99-1 depict highest probabilty density irrespective of the quality","metadata":{}},{"cell_type":"code","source":"KdeAndBox(\"total_sulfur_dioxide\",\"free_sulfur_dioxide\")","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:42.963725Z","iopub.execute_input":"2021-07-19T09:35:42.964128Z","iopub.status.idle":"2021-07-19T09:35:43.768515Z","shell.execute_reply.started":"2021-07-19T09:35:42.9641Z","shell.execute_reply":"2021-07-19T09:35:43.767582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above KDE plot, I can infer:\n    1. The distribution for total_sulfur_dioxide seems to be normally distributed for both quality types\n    2. The distribution for free_sulfur_dioxide seems to be highly positively skewed for both quality types\n    3. For total_sulfur_dioxide, values between 50-150 depict highest probability density when quality of wine is High       whilst the probably density function seems to be evenly spread across 50-200\n    4. For free_sulfur_dioxide, values between 0-50 depict highest probabilty density irrespective of the quality","metadata":{}},{"cell_type":"code","source":"KdeAndBox(\"pH\",\"sulphates\")","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:43.769915Z","iopub.execute_input":"2021-07-19T09:35:43.770462Z","iopub.status.idle":"2021-07-19T09:35:44.540829Z","shell.execute_reply.started":"2021-07-19T09:35:43.770421Z","shell.execute_reply":"2021-07-19T09:35:44.539829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above KDE plot, I can infer:\n    1. The distribution for pH seems to be normally distributed for both quality types\n    2. The distribution for sulphates seems to be highly positively skewed for both quality types\n    3. For pH, values between 3-3.5 depict highest probability density for both quality types\n    4. For sulphates, values around 0.5 depict highest probabilty density irrespective of the quality","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,4.5))\nplt.subplot(1,2,1)\nsns.kdeplot(df_wine.loc[df_wine[\"quality\"]==\"Low\"][\"residual_sugar\"],shade=True)\nsns.kdeplot(df_wine.loc[df_wine[\"quality\"]==\"High\"][\"residual_sugar\"],shade=True)\n\nplt.legend([\"Low\",\"High\"])\nplt.title(\"residual sugar\".upper(),fontsize=15)\nplt.subplot(1,2,2)\nsns.violinplot(data=df_wine,y=\"residual_sugar\",x=\"quality\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:44.542049Z","iopub.execute_input":"2021-07-19T09:35:44.542342Z","iopub.status.idle":"2021-07-19T09:35:45.10122Z","shell.execute_reply.started":"2021-07-19T09:35:44.542315Z","shell.execute_reply":"2021-07-19T09:35:45.100312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above KDE plot, I can infer:\n    1. The distribution for residual_sugar seems to be highly positively skewed for both quality types\n    3. For residual_sugar, values between 0-5 depict highest probability density for both quality types","metadata":{}},{"cell_type":"code","source":"# Seaborn pairplot\nsns_plot = sns.pairplot(df_wine,corner=True,hue='type',palette='dark:salmon_r',height=4.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:35:45.102464Z","iopub.execute_input":"2021-07-19T09:35:45.102753Z","iopub.status.idle":"2021-07-19T09:36:21.051139Z","shell.execute_reply.started":"2021-07-19T09:35:45.102727Z","shell.execute_reply":"2021-07-19T09:36:21.049802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.6 Feature Engineering <a id='feature_eng'></a>","metadata":{}},{"cell_type":"markdown","source":"**Sulfur dioxide ratio**\n\nSince free sulfur dioxide is the unbound part of total sulfur dioxide, I will caculate the ratio of this two features. This feature has higher correlation to quality than each of the individuals","metadata":{}},{"cell_type":"code","source":"df_wine['sulfur_dioxide_ratio'] = df_wine['free_sulfur_dioxide']/df_wine['total_sulfur_dioxide']","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.05282Z","iopub.execute_input":"2021-07-19T09:36:21.053135Z","iopub.status.idle":"2021-07-19T09:36:21.058954Z","shell.execute_reply.started":"2021-07-19T09:36:21.053103Z","shell.execute_reply":"2021-07-19T09:36:21.057784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.drop(['free_sulfur_dioxide'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.060082Z","iopub.execute_input":"2021-07-19T09:36:21.060352Z","iopub.status.idle":"2021-07-19T09:36:21.075397Z","shell.execute_reply.started":"2021-07-19T09:36:21.060327Z","shell.execute_reply":"2021-07-19T09:36:21.074022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.head().style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.076974Z","iopub.execute_input":"2021-07-19T09:36:21.077323Z","iopub.status.idle":"2021-07-19T09:36:21.11416Z","shell.execute_reply.started":"2021-07-19T09:36:21.077291Z","shell.execute_reply":"2021-07-19T09:36:21.113511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data Preprocessing <a id='data_pre'></a>","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Outliers <a id='out'></a>","metadata":{}},{"cell_type":"markdown","source":"### 5.1.1 Discovery of Outliers<a id='dis_out'></a>","metadata":{}},{"cell_type":"code","source":"df_num_features=df_wine.select_dtypes(include=np.number)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.115179Z","iopub.execute_input":"2021-07-19T09:36:21.115601Z","iopub.status.idle":"2021-07-19T09:36:21.121436Z","shell.execute_reply.started":"2021-07-19T09:36:21.115572Z","shell.execute_reply":"2021-07-19T09:36:21.120521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Identifying outliers using IQR**","metadata":{}},{"cell_type":"code","source":"Q1 = df_num_features.quantile(0.25)\nQ3 = df_num_features.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.122793Z","iopub.execute_input":"2021-07-19T09:36:21.123115Z","iopub.status.idle":"2021-07-19T09:36:21.139669Z","shell.execute_reply.started":"2021-07-19T09:36:21.123086Z","shell.execute_reply":"2021-07-19T09:36:21.138544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outlier = pd.DataFrame((df_num_features < (Q1 - 1.5 * IQR)) | (df_num_features > (Q3 + 1.5 * IQR)))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.140932Z","iopub.execute_input":"2021-07-19T09:36:21.141508Z","iopub.status.idle":"2021-07-19T09:36:21.151575Z","shell.execute_reply.started":"2021-07-19T09:36:21.141478Z","shell.execute_reply":"2021-07-19T09:36:21.150601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in outlier.columns:\n    print('Total number of Outliers in column {} are {}'.format(i, (len(outlier[outlier[i] == True][i]))))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.153537Z","iopub.execute_input":"2021-07-19T09:36:21.153954Z","iopub.status.idle":"2021-07-19T09:36:21.171173Z","shell.execute_reply.started":"2021-07-19T09:36:21.153913Z","shell.execute_reply":"2021-07-19T09:36:21.17041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing outliers using Boxplots**","metadata":{}},{"cell_type":"code","source":"for column in enumerate(df_num_features):\n    plt.figure(figsize=(30,5))\n    sns.set_theme(style=\"darkgrid\")\n    sns.boxplot(x=column[1], data=  df_num_features,color='red')\n    plt.xlabel(column[1],fontsize=18)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:21.178886Z","iopub.execute_input":"2021-07-19T09:36:21.179395Z","iopub.status.idle":"2021-07-19T09:36:23.210611Z","shell.execute_reply.started":"2021-07-19T09:36:21.179361Z","shell.execute_reply":"2021-07-19T09:36:23.209512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.2 Removal of Outliers<a id='rem_out'></a>","metadata":{}},{"cell_type":"markdown","source":"**Checking the normality of numeric features**","metadata":{}},{"cell_type":"code","source":"import scipy\nfrom scipy.stats import shapiro\nstat, p_value = shapiro(df_num_features)\n\n# print the test statistic and corresponding p-value \nprint('Test statistic:', stat)\nprint('P-Value:', p_value)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:23.214675Z","iopub.execute_input":"2021-07-19T09:36:23.214972Z","iopub.status.idle":"2021-07-19T09:36:23.22668Z","shell.execute_reply.started":"2021-07-19T09:36:23.214945Z","shell.execute_reply":"2021-07-19T09:36:23.225718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the numeric features are not normal I are removing the outliers using IQR method","metadata":{}},{"cell_type":"code","source":"df_wine = df_wine[~((df_wine < (Q1 - 1.5 * IQR)) |(df_wine > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf_wine.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:23.227856Z","iopub.execute_input":"2021-07-19T09:36:23.228176Z","iopub.status.idle":"2021-07-19T09:36:23.253817Z","shell.execute_reply.started":"2021-07-19T09:36:23.228148Z","shell.execute_reply":"2021-07-19T09:36:23.252798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.reset_index(inplace=True,drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:23.255063Z","iopub.execute_input":"2021-07-19T09:36:23.255355Z","iopub.status.idle":"2021-07-19T09:36:23.260113Z","shell.execute_reply.started":"2021-07-19T09:36:23.255328Z","shell.execute_reply":"2021-07-19T09:36:23.259125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.tail().style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:23.261328Z","iopub.execute_input":"2021-07-19T09:36:23.261587Z","iopub.status.idle":"2021-07-19T09:36:23.298723Z","shell.execute_reply.started":"2021-07-19T09:36:23.261562Z","shell.execute_reply":"2021-07-19T09:36:23.297523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.3 Re-checking Correlation<a id='rec_cor'></a>","metadata":{}},{"cell_type":"code","source":"data_num_features = df_wine.select_dtypes(include=np.number)\n# print the names of the numeric variables \nprint('The numerical columns in the dataset are: ',data_num_features.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:23.300391Z","iopub.execute_input":"2021-07-19T09:36:23.300863Z","iopub.status.idle":"2021-07-19T09:36:23.31409Z","shell.execute_reply.started":"2021-07-19T09:36:23.300822Z","shell.execute_reply":"2021-07-19T09:36:23.312955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr =  data_num_features.corr()\n\n# print the correlation matrix\ncorr.style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:23.315524Z","iopub.execute_input":"2021-07-19T09:36:23.315898Z","iopub.status.idle":"2021-07-19T09:36:23.361854Z","shell.execute_reply.started":"2021-07-19T09:36:23.315869Z","shell.execute_reply":"2021-07-19T09:36:23.360736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(11,9))\ndropSelf = np.zeros_like(corr_matrix)\ndropSelf[np.triu_indices_from(dropSelf)] = True\n\nsns.heatmap(corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\n\nsns.set(font_scale=1.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:23.363297Z","iopub.execute_input":"2021-07-19T09:36:23.363709Z","iopub.status.idle":"2021-07-19T09:36:24.160585Z","shell.execute_reply.started":"2021-07-19T09:36:23.363647Z","shell.execute_reply":"2021-07-19T09:36:24.159434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recheck of correlation after treating outliers. There has been a slight change with respect to the correlation between numeric values","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Categorical Encoding<a id='cat_enc'></a>","metadata":{}},{"cell_type":"code","source":"df_wine['type']=pd.get_dummies(df_wine['type'])\nquaity_mapping = {\"Low\":0, \"High\":1}\ndf_wine[\"quality\"] =  df_wine[\"quality\"].map(quaity_mapping)\ndf_wine.head().style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-19T09:36:24.162167Z","iopub.execute_input":"2021-07-19T09:36:24.162591Z","iopub.status.idle":"2021-07-19T09:36:24.203399Z","shell.execute_reply.started":"2021-07-19T09:36:24.162547Z","shell.execute_reply":"2021-07-19T09:36:24.20247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wine.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.204966Z","iopub.execute_input":"2021-07-19T09:36:24.205266Z","iopub.status.idle":"2021-07-19T09:36:24.21343Z","shell.execute_reply.started":"2021-07-19T09:36:24.205239Z","shell.execute_reply":"2021-07-19T09:36:24.212409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Feature Scaling<a id='fea_sca'></a>","metadata":{}},{"cell_type":"code","source":"df_num_features=df_wine.drop(['type','quality'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.214868Z","iopub.execute_input":"2021-07-19T09:36:24.215204Z","iopub.status.idle":"2021-07-19T09:36:24.225441Z","shell.execute_reply.started":"2021-07-19T09:36:24.215176Z","shell.execute_reply":"2021-07-19T09:36:24.224326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking normality for numerical columns**","metadata":{}},{"cell_type":"code","source":"for col in df_num_features.columns:\n    print(\"Column \", col, \" :\", shapiro(df_num_features[col]))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.226991Z","iopub.execute_input":"2021-07-19T09:36:24.227293Z","iopub.status.idle":"2021-07-19T09:36:24.248613Z","shell.execute_reply.started":"2021-07-19T09:36:24.227265Z","shell.execute_reply":"2021-07-19T09:36:24.247506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since none of the numerical features are normally distributed (p-value<0.05) , I will perform Min-Max normalisation to scale the data","metadata":{}},{"cell_type":"code","source":"mms = MinMaxScaler()\nmmsfit = mms.fit(df_num_features)\ndfxz = pd.DataFrame(mms.fit_transform(df_num_features), columns = df_num_features.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.250074Z","iopub.execute_input":"2021-07-19T09:36:24.250468Z","iopub.status.idle":"2021-07-19T09:36:24.263088Z","shell.execute_reply.started":"2021-07-19T09:36:24.250425Z","shell.execute_reply":"2021-07-19T09:36:24.261975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfxz.head().style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.264206Z","iopub.execute_input":"2021-07-19T09:36:24.264617Z","iopub.status.idle":"2021-07-19T09:36:24.292606Z","shell.execute_reply.started":"2021-07-19T09:36:24.264575Z","shell.execute_reply":"2021-07-19T09:36:24.291632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat=df_wine[['type','quality']]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.293854Z","iopub.execute_input":"2021-07-19T09:36:24.294236Z","iopub.status.idle":"2021-07-19T09:36:24.302841Z","shell.execute_reply.started":"2021-07-19T09:36:24.294203Z","shell.execute_reply":"2021-07-19T09:36:24.302085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfxz = pd.concat([dfxz, df_cat], axis = 1)\ndfxz.head().style.set_properties(**{'background-color':'black','color':'white','border-color':'red'})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.303917Z","iopub.execute_input":"2021-07-19T09:36:24.304303Z","iopub.status.idle":"2021-07-19T09:36:24.341772Z","shell.execute_reply.started":"2021-07-19T09:36:24.304263Z","shell.execute_reply":"2021-07-19T09:36:24.340524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfxz.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.343014Z","iopub.execute_input":"2021-07-19T09:36:24.343281Z","iopub.status.idle":"2021-07-19T09:36:24.356838Z","shell.execute_reply.started":"2021-07-19T09:36:24.343255Z","shell.execute_reply":"2021-07-19T09:36:24.355452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 Train-Test Split<a id=\"split\"></a>","metadata":{}},{"cell_type":"markdown","source":"Before applying various classification techniques to predict the quality of the wine, let us split the dataset in train and test set.","metadata":{}},{"cell_type":"code","source":"X=dfxz.drop('quality',axis=1)\ny=dfxz['quality']","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.358413Z","iopub.execute_input":"2021-07-19T09:36:24.358875Z","iopub.status.idle":"2021-07-19T09:36:24.366012Z","shell.execute_reply.started":"2021-07-19T09:36:24.35883Z","shell.execute_reply":"2021-07-19T09:36:24.364982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add a constant column to the dataframe\n# while using the 'Logit' method in the Statsmodels library, the method do not consider the intercept by default\n# I can add the intercept to the set of independent variables using 'add_constant()'\nX = sm.add_constant(X)\n\n# split data into train subset and test subset\n# set 'random_state' to generate the same dataset each time you run the code \n# 'test_size' returns the proportion of data to be included in the testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.3)\n\n# check the dimensions of the train & test subset using 'shape'\n# print dimension of train set\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\n\n# print dimension of test set\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.36759Z","iopub.execute_input":"2021-07-19T09:36:24.368048Z","iopub.status.idle":"2021-07-19T09:36:24.387593Z","shell.execute_reply.started":"2021-07-19T09:36:24.368006Z","shell.execute_reply":"2021-07-19T09:36:24.386361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a generalized function to create a dataframe containing the scores for the models.","metadata":{}},{"cell_type":"code","source":"# create an empty dataframe to store the scores for various algorithms\nscore_card1 = pd.DataFrame(columns=['Probability Cutoff', 'AUC Score', 'Precision Score', 'Recall Score',\n                                       'Accuracy Score', 'Kappa Score', 'f1-score'])\n\n# append the result table for all performance scores\n# performance measures considered for model comparision are 'AUC Score', 'Precision Score', 'Recall Score','Accuracy Score',\n# 'Kappa Score', and 'f1-score'\n# compile the required information in a user defined function \ndef update_score_card1(model, cutoff):\n    \n    # let 'y_pred_prob' be the predicted values of y\n    y_pred_prob = model.predict(X_test[features])\n\n    # convert probabilities to 0 and 1 using 'if_else'\n    y_pred = [ 0 if x < cutoff else 1 for x in y_pred_prob]\n    \n    # assign 'score_card' as global variable\n    global score_card1\n\n    # append the results to the dataframe 'score_card'\n    # 'ignore_index = True' do not consider the index labels\n    score_card1 = score_card1.append({'Probability Cutoff': cutoff,\n                                    'AUC Score' : metrics.roc_auc_score(y_test, y_pred),\n                                    'Precision Score': metrics.precision_score(y_test, y_pred),\n                                    'Recall Score': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa Score':metrics.cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.389253Z","iopub.execute_input":"2021-07-19T09:36:24.389738Z","iopub.status.idle":"2021-07-19T09:36:24.401712Z","shell.execute_reply.started":"2021-07-19T09:36:24.389671Z","shell.execute_reply":"2021-07-19T09:36:24.400614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Logistic Regression<a id=\"log_reg\"></a>","metadata":{}},{"cell_type":"code","source":"logreg = sm.Logit(y_train, X_train).fit()\n\n# print the summary of the model\nprint(logreg.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.403018Z","iopub.execute_input":"2021-07-19T09:36:24.403293Z","iopub.status.idle":"2021-07-19T09:36:24.495408Z","shell.execute_reply.started":"2021-07-19T09:36:24.403267Z","shell.execute_reply":"2021-07-19T09:36:24.494405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** The `Pseudo R-squ.` obtained from the above model summary is **0.2312**  which is also the value of `McFadden's R-squared`. This value can be obtained from the formula:\n\n<p style='text-indent:25em'> <strong> McFadden's R-squared = $ 1 - \\frac{Log-Likelihood}{LL-Null} $</strong> </p>\n\nWhere,<br>\nLog-Likelihood: It is the maximum value of the log-likelihood function<br>\nLL-Null: It is the maximum value of the log-likelihood function for the model containing only the intercept \n\nThe LLR p-value is less than 0.05, implies that the model is significant.\n\nEven though the model is significant there are few features which are insignificant (P-value < 0.05)","metadata":{}},{"cell_type":"markdown","source":"**Backward Elimination Model**","metadata":{}},{"cell_type":"markdown","source":"To obtain the best significant features which are realated to target variable I perform backward elimination process below:","metadata":{}},{"cell_type":"code","source":"sfs_backward=sfs(estimator=LogisticRegression(),k_features='best',forward=False,verbose=0,scoring='accuracy')\nsfs_model=sfs_backward.fit(X_train,y_train)\nfeatures=list(sfs_model.k_feature_names_)\nprint(\"The best features obtained from elimination process:\",features)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:24.496995Z","iopub.execute_input":"2021-07-19T09:36:24.497554Z","iopub.status.idle":"2021-07-19T09:36:37.804864Z","shell.execute_reply.started":"2021-07-19T09:36:24.497513Z","shell.execute_reply":"2021-07-19T09:36:37.803922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now,building a Logistic regression model obtained from the above elimination process","metadata":{}},{"cell_type":"code","source":"logreg_backward = sm.Logit(y_train, X_train[features]).fit()\n\n# print the summary of the model\nprint(logreg_backward.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:37.80603Z","iopub.execute_input":"2021-07-19T09:36:37.806316Z","iopub.status.idle":"2021-07-19T09:36:37.888166Z","shell.execute_reply.started":"2021-07-19T09:36:37.806288Z","shell.execute_reply":"2021-07-19T09:36:37.887263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** The `Pseudo R-squ.` obtained from the above model summary is **0.2310**  \nThe LLR p-value is less than 0.05, implies that the model is significant.","metadata":{}},{"cell_type":"markdown","source":"**Identifying the Best Cut-off Value**","metadata":{}},{"cell_type":"markdown","source":"Now, let us consider a list of values as cut-off to calculate the different performance measures.","metadata":{}},{"cell_type":"code","source":"# consider a list of values for cut-off\ncutoff = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\n# use the for loop to compute performance measures for each value of the cut-off\n# call the update_score_card() to update the score card for each cut-off\n# pass the model and cut-off value to the function\nfor value in cutoff:\n    update_score_card1(logreg_backward, value)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:37.889351Z","iopub.execute_input":"2021-07-19T09:36:37.889645Z","iopub.status.idle":"2021-07-19T09:36:38.173074Z","shell.execute_reply.started":"2021-07-19T09:36:37.889615Z","shell.execute_reply":"2021-07-19T09:36:38.171941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the score card \nprint('Score Card for Logistic regression:')\n\n# sort the dataframe based on the probability cut-off values ascending order\n# 'reset_index' resets the index of the dataframe\n# 'drop = True' drops the previous index\nscore_card1 = score_card1.sort_values('Probability Cutoff').reset_index(drop = True)\n\n# color the cell in the columns 'AUC Score', 'Accuracy Score', 'Kappa Score', 'f1-score' having maximum values\n# 'style.highlight_max' assigns color to the maximum value\n# pass specified color to the parameter, 'color'\n# pass the data to limit the color assignment to the parameter, 'subset' \nscore_card1.style.highlight_max(color = 'red', subset = ['Accuracy Score'])","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.174594Z","iopub.execute_input":"2021-07-19T09:36:38.175229Z","iopub.status.idle":"2021-07-19T09:36:38.20264Z","shell.execute_reply.started":"2021-07-19T09:36:38.175183Z","shell.execute_reply":"2021-07-19T09:36:38.201582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** The above dataframe shows that,all the highest scores of different perfomance metrics.\nThe optimal probability cut-off score which is considered for futher analysis is taken by considering the **Accuracy Score**.","metadata":{}},{"cell_type":"markdown","source":"**Predictions on the train set.**","metadata":{}},{"cell_type":"code","source":"# let 'y_pred_prob1' be the predicted values of y\ny_pred_prob1 = logreg_backward.predict(X_train[features])\n\n# print the y_pred_prob1\ny_pred_prob1.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.204223Z","iopub.execute_input":"2021-07-19T09:36:38.204865Z","iopub.status.idle":"2021-07-19T09:36:38.218283Z","shell.execute_reply.started":"2021-07-19T09:36:38.204819Z","shell.execute_reply":"2021-07-19T09:36:38.217236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I decided the cut-off to be 0.5. i.e. if 'y_pred_prob1' is less than 0.5, then consider it to be 0 else consider it to be 1.","metadata":{}},{"cell_type":"code","source":"# convert probabilities to 0 and 1 using 'if_else'\ny_pred1 = [ 0 if x < 0.5 else 1 for x in y_pred_prob1]\ny_pred1[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.220152Z","iopub.execute_input":"2021-07-19T09:36:38.220771Z","iopub.status.idle":"2021-07-19T09:36:38.230463Z","shell.execute_reply.started":"2021-07-19T09:36:38.220723Z","shell.execute_reply":"2021-07-19T09:36:38.229323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predictions on the test set.**","metadata":{}},{"cell_type":"code","source":"# let 'y_pred_prob' be the predicted values of y\ny_pred_prob = logreg_backward.predict(X_test[features])\n\n# print the y_pred_prob\ny_pred_prob.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.232136Z","iopub.execute_input":"2021-07-19T09:36:38.232788Z","iopub.status.idle":"2021-07-19T09:36:38.254117Z","shell.execute_reply.started":"2021-07-19T09:36:38.232739Z","shell.execute_reply":"2021-07-19T09:36:38.252946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert probabilities to 0 and 1 using 'if_else'\ny_pred = [ 0 if x < 0.5 else 1 for x in y_pred_prob]\ny_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.258321Z","iopub.execute_input":"2021-07-19T09:36:38.259109Z","iopub.status.idle":"2021-07-19T09:36:38.275263Z","shell.execute_reply.started":"2021-07-19T09:36:38.259057Z","shell.execute_reply":"2021-07-19T09:36:38.273631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"# create a confusion matrix\n# pass the actual and predicted target values to the confusion_matrix()\ncm = confusion_matrix(y_test, y_pred)\n\n# label the confusion matrix  \n# pass the matrix as 'data'\n# pass the required column names to the parameter, 'columns'\n# pass the required row names to the parameter, 'index'\nconf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n# plot a heatmap to visualize the confusion matrix\n# 'annot' prints the value of each grid \n# 'fmt = d' returns the integer value in each grid\n# 'cmap' assigns color to each grid\n# as I do not require different colors for each grid in the heatmap,\n# use 'ListedColormap' to assign the specified color to the grid\n# 'cbar = False' will not return the color bar to the right side of the heatmap\n# 'linewidths' assigns the width to the line that divides each grid\n# 'annot_kws = {'size':25})' assigns the font size of the annotated text \nsns.heatmap(conf_matrix, annot= True, fmt = 'd', cmap ='Reds', cbar = False, linewidths = 0.1, annot_kws = {'size':25})\n\n# set the font size of x-axis ticks using 'fontsize'\nplt.xticks(fontsize = 20)\n\n# set the font size of y-axis ticks using 'fontsize'\nplt.yticks(fontsize = 20)\n\n# display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.279803Z","iopub.execute_input":"2021-07-19T09:36:38.280262Z","iopub.status.idle":"2021-07-19T09:36:38.473335Z","shell.execute_reply.started":"2021-07-19T09:36:38.280217Z","shell.execute_reply":"2021-07-19T09:36:38.472626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Report**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_train, y_pred1))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-19T09:36:38.474342Z","iopub.execute_input":"2021-07-19T09:36:38.474753Z","iopub.status.idle":"2021-07-19T09:36:38.4914Z","shell.execute_reply.started":"2021-07-19T09:36:38.474715Z","shell.execute_reply":"2021-07-19T09:36:38.490754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 76% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test Report**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.49245Z","iopub.execute_input":"2021-07-19T09:36:38.4929Z","iopub.status.idle":"2021-07-19T09:36:38.505186Z","shell.execute_reply.started":"2021-07-19T09:36:38.492865Z","shell.execute_reply":"2021-07-19T09:36:38.503894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the model  is 74% accurate.","metadata":{}},{"cell_type":"markdown","source":"From the above classification reports,I can infer that there is a little difference when compared to test and train reports.\nHence I conclude that the model is bit overfitted.","metadata":{}},{"cell_type":"markdown","source":"**ROC Curve**","metadata":{}},{"cell_type":"code","source":"# the roc_curve() returns the values for false positive rate, true positive rate and threshold\n# pass the actual target values and predicted probabilities to the function\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# plot the ROC curve\nplt.plot(fpr, tpr)\n\n# set limits for x and y axes\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n# plot the straight line showing worst prediction for the model\nplt.plot([0, 1], [0, 1],'r--')\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('ROC curve ', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n# add the AUC score to the plot\n# 'x' and 'y' gives position of the text\n# 's' is the text \n# use round() to round-off the AUC score upto 4 digits\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score:', round(metrics.roc_auc_score(y_test, y_pred_prob),4)))\n                               \n# plot the grid\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.506485Z","iopub.execute_input":"2021-07-19T09:36:38.506809Z","iopub.status.idle":"2021-07-19T09:36:38.747917Z","shell.execute_reply.started":"2021-07-19T09:36:38.506777Z","shell.execute_reply":"2021-07-19T09:36:38.746863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** The red dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).<br>\nFrom the above plot, I can see that our classifier (logistic regression with features obtained from Backward elimination method) is away from the dotted line; with the AUC score **0.7957**","metadata":{}},{"cell_type":"markdown","source":"**Score Card**","metadata":{}},{"cell_type":"code","source":"#defining a score card\nscore_card=pd.DataFrame(columns=['Model_Name','Accuracy(Train)','Accuracy(Test)','Diff_b/w_train&test(Acc)','AUC_Score','Avg(Acc)'])","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.749601Z","iopub.execute_input":"2021-07-19T09:36:38.750034Z","iopub.status.idle":"2021-07-19T09:36:38.758199Z","shell.execute_reply.started":"2021-07-19T09:36:38.749991Z","shell.execute_reply":"2021-07-19T09:36:38.757186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting Cross Validation Score\ncv_lr = cross_val_score(estimator = LogisticRegression() , X = X_train[features], y = y_train, cv = 10,scoring='accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:38.759373Z","iopub.execute_input":"2021-07-19T09:36:38.759653Z","iopub.status.idle":"2021-07-19T09:36:39.285295Z","shell.execute_reply.started":"2021-07-19T09:36:38.759625Z","shell.execute_reply":"2021-07-19T09:36:39.284142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_card=score_card.append({'Model_Name': 'Logistic Regression',\n                             'Accuracy(Train)': metrics.accuracy_score(y_train, y_pred1),\n                             'Accuracy(Test)':metrics.accuracy_score(y_test, y_pred),\n                             'Diff_b/w_train&test(Acc)': abs(metrics.accuracy_score(y_train, y_pred1)-metrics.accuracy_score(y_test, y_pred)),\n                             'AUC_Score':metrics.roc_auc_score(y_test, y_pred_prob),\n                             'Avg(Acc)':cv_lr.mean()},ignore_index=True)\nscore_card","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.290671Z","iopub.execute_input":"2021-07-19T09:36:39.293426Z","iopub.status.idle":"2021-07-19T09:36:39.342505Z","shell.execute_reply.started":"2021-07-19T09:36:39.293347Z","shell.execute_reply":"2021-07-19T09:36:39.34138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Some Pre-defined functions**","metadata":{}},{"cell_type":"markdown","source":"#### A generalized function to calculate the performance metrics for the train set.","metadata":{}},{"cell_type":"code","source":"# create a generalized function to calculate the metrics values for train set\ndef get_train_report(model):\n    \n    # for training set:\n    # train_pred: prediction made by the model on the train dataset 'X_train'\n    # y_train: actual values of the target variable for the train dataset\n\n    # predict the output of the target variable from the train data \n    train_pred = model.predict(X_train)\n\n    # return the performace measures on train set\n    return(classification_report(y_train, train_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.347403Z","iopub.execute_input":"2021-07-19T09:36:39.350239Z","iopub.status.idle":"2021-07-19T09:36:39.358491Z","shell.execute_reply.started":"2021-07-19T09:36:39.350179Z","shell.execute_reply":"2021-07-19T09:36:39.357442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### A generalized function to calculate the performance metrics for the test set.","metadata":{}},{"cell_type":"code","source":"# create a generalized function to calculate the performance metrics values for test set\ndef get_test_report(model):\n    \n    # for test set:\n    # test_pred: prediction made by the model on the test dataset 'X_test'\n    # y_test: actual values of the target variable for the test dataset\n\n    # predict the output of the target variable from the test data \n    test_pred = model.predict(X_test)\n\n    # return the classification report for test data\n    return(classification_report(y_test, test_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.363963Z","iopub.execute_input":"2021-07-19T09:36:39.366758Z","iopub.status.idle":"2021-07-19T09:36:39.375424Z","shell.execute_reply.started":"2021-07-19T09:36:39.366678Z","shell.execute_reply":"2021-07-19T09:36:39.374302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function to plot the confusion matrix.","metadata":{}},{"cell_type":"code","source":"# define a to plot a confusion matrix for the model\ndef plot_confusion_matrix(model):\n    \n    # predict the target values using df_test\n    y_pred = model.predict(X_test)\n    \n    # create a confusion matrix\n    # pass the actual and predicted target values to the confusion_matrix()\n    cm = confusion_matrix(y_test, y_pred)\n\n    # label the confusion matrix  \n    # pass the matrix as 'data'\n    # pass the required column names to the parameter, 'columns'\n    # pass the required row names to the parameter, 'index'\n    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n    # plot a heatmap to visualize the confusion matrix\n    # 'annot' prints the value of each grid \n    # 'fmt = d' returns the integer value in each grid\n    # 'cmap' assigns color to each grid\n    # as I do not require different colors for each grid in the heatmap,\n    # use 'ListedColormap' to assign the specified color to the grid\n    # 'cbar = False' will not return the color bar to the right side of the heatmap\n    # 'linewidths' assigns the width to the line that divides each grid\n    # 'annot_kws = {'size':25})' assigns the font size of the annotated text \n    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Reds', cbar = False, \n                linewidths = 0.1, annot_kws = {'size':25})\n\n    # set the font size of x-axis ticks using 'fontsize'\n    plt.xticks(fontsize = 20)\n\n    # set the font size of y-axis ticks using 'fontsize'\n    plt.yticks(fontsize = 20)\n\n    # display the plot\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.381041Z","iopub.execute_input":"2021-07-19T09:36:39.383902Z","iopub.status.idle":"2021-07-19T09:36:39.396657Z","shell.execute_reply.started":"2021-07-19T09:36:39.383836Z","shell.execute_reply":"2021-07-19T09:36:39.395589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function to plot the ROC curve.","metadata":{}},{"cell_type":"code","source":"# define a function to plot the ROC curve and print the ROC-AUC score\ndef plot_roc(model):\n    \n    # predict the probability of target variable using X_test\n    # consider the probability of positive class by subsetting with '[:,1]'\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    \n    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n    # pass the actual target values and predicted probabilities to the function\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n    # plot the ROC curve\n    plt.plot(fpr, tpr)\n\n    # set limits for x and y axes\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n\n    # plot the straight line showing worst prediction for the model\n    plt.plot([0, 1], [0, 1],'r--')\n\n    # add plot and axes labels\n    # set text size using 'fontsize'\n    plt.title('ROC curve ', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n    # add the AUC score to the plot\n    # 'x' and 'y' gives position of the text\n    # 's' is the text \n    # use round() to round-off the AUC score upto 4 digits\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(metrics.roc_auc_score(y_test, y_pred_prob),4)))\n\n    # plot the grid\n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.401457Z","iopub.execute_input":"2021-07-19T09:36:39.402022Z","iopub.status.idle":"2021-07-19T09:36:39.414324Z","shell.execute_reply.started":"2021-07-19T09:36:39.401978Z","shell.execute_reply":"2021-07-19T09:36:39.413283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function for  predicting Cross Validation Score","metadata":{}},{"cell_type":"code","source":"# Predicting Cross Validation Score\ndef cross_valid_score(obj):\n    cv = cross_val_score(estimator = obj , X = X_train, y = y_train, cv = 10,scoring='accuracy')\n    return cv.mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.416194Z","iopub.execute_input":"2021-07-19T09:36:39.416651Z","iopub.status.idle":"2021-07-19T09:36:39.428592Z","shell.execute_reply.started":"2021-07-19T09:36:39.416609Z","shell.execute_reply":"2021-07-19T09:36:39.427574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function to update the score card","metadata":{}},{"cell_type":"code","source":"def update_score_card(model_name,model):\n    global score_card\n    train_pred = model.predict(X_train)\n    test_pred=model.predict(X_test)\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    score_card=score_card.append({'Model_Name': model_name,\n                             'Accuracy(Train)': metrics.accuracy_score(y_train, train_pred),\n                             'Accuracy(Test)':metrics.accuracy_score(y_test, test_pred),\n                             'Diff_b/w_train&test(Acc)': abs(metrics.accuracy_score(y_train, train_pred)-metrics.accuracy_score(y_test, test_pred)),\n                             'AUC_Score':metrics.roc_auc_score(y_test, y_pred_prob),\n                             'Avg(Acc)':cross_valid_score(model)},ignore_index=True)\n    return score_card","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.43024Z","iopub.execute_input":"2021-07-19T09:36:39.430718Z","iopub.status.idle":"2021-07-19T09:36:39.441029Z","shell.execute_reply.started":"2021-07-19T09:36:39.430653Z","shell.execute_reply":"2021-07-19T09:36:39.440128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Naive Bayes Algorithm<a id=\"nai_bay\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### Building a naive bayes model on a training dataset.","metadata":{}},{"cell_type":"code","source":"# instantiate the 'GaussianNB'\ngnb = GaussianNB()\n\n# fit the model using fit() on train data\ngnb_model = gnb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.442227Z","iopub.execute_input":"2021-07-19T09:36:39.442507Z","iopub.status.idle":"2021-07-19T09:36:39.459227Z","shell.execute_reply.started":"2021-07-19T09:36:39.442472Z","shell.execute_reply":"2021-07-19T09:36:39.458423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(gnb_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.460175Z","iopub.execute_input":"2021-07-19T09:36:39.460562Z","iopub.status.idle":"2021-07-19T09:36:39.630082Z","shell.execute_reply.started":"2021-07-19T09:36:39.460534Z","shell.execute_reply":"2021-07-19T09:36:39.629323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Report**","metadata":{}},{"cell_type":"code","source":"train_report = get_train_report(gnb_model)\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.631031Z","iopub.execute_input":"2021-07-19T09:36:39.631404Z","iopub.status.idle":"2021-07-19T09:36:39.650917Z","shell.execute_reply.started":"2021-07-19T09:36:39.631377Z","shell.execute_reply":"2021-07-19T09:36:39.649629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 72% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test Report**","metadata":{}},{"cell_type":"code","source":"test_report = get_test_report(gnb_model)\n\n# print the performace measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.65228Z","iopub.execute_input":"2021-07-19T09:36:39.652743Z","iopub.status.idle":"2021-07-19T09:36:39.665301Z","shell.execute_reply.started":"2021-07-19T09:36:39.652689Z","shell.execute_reply":"2021-07-19T09:36:39.664593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 69% accurate.","metadata":{}},{"cell_type":"markdown","source":"From the above classification reports,I can infer that there is a little difference when compared to test and train reports.\nHence I conclude that the model is bit overfitted.","metadata":{}},{"cell_type":"markdown","source":"**ROC curve**","metadata":{}},{"cell_type":"code","source":"# call the function to plot the ROC curve\n# pass the  gaussian naive bayes model to the function\nplot_roc(gnb_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.666261Z","iopub.execute_input":"2021-07-19T09:36:39.66664Z","iopub.status.idle":"2021-07-19T09:36:39.900833Z","shell.execute_reply.started":"2021-07-19T09:36:39.666613Z","shell.execute_reply":"2021-07-19T09:36:39.89977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:**\nFrom the above plot, I can see that our classifier (Gaussian NaiveBayes) is away from the dotted line; with the AUC score **0.7455**","metadata":{}},{"cell_type":"markdown","source":"#### Score Card","metadata":{}},{"cell_type":"code","source":"update_score_card('Navie Bayes',gnb_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.902011Z","iopub.execute_input":"2021-07-19T09:36:39.902304Z","iopub.status.idle":"2021-07-19T09:36:39.991613Z","shell.execute_reply.started":"2021-07-19T09:36:39.902277Z","shell.execute_reply":"2021-07-19T09:36:39.990713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. K Nearest Neighbors (KNN)<a id=\"knn\"></a>","metadata":{}},{"cell_type":"markdown","source":"**Finding  Optimal Value of K (using GridSearchCV)**","metadata":{}},{"cell_type":"code","source":"# create a dictionary with hyperparameters and its values\n# n_neighnors: number of neighbors to consider\n# usually, I consider the odd value of 'n_neighnors' to avoid the equal number of nearest points with more than one class\n# pass the different distance metrics to the parameter, 'metric'\ntuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),\n                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}\n \n# instantiate the 'KNeighborsClassifier' \nknn_classification = KNeighborsClassifier()\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the knn model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 5\n# scoring: pass the scoring parameter 'accuracy'\nknn_grid = GridSearchCV(estimator = knn_classification, \n                        param_grid = tuned_paramaters, \n                        cv = 5, \n                        scoring = 'accuracy')\n\n# fit the model on X_train and y_train using fit()\nknn_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:39.992885Z","iopub.execute_input":"2021-07-19T09:36:39.993181Z","iopub.status.idle":"2021-07-19T09:36:52.435162Z","shell.execute_reply.started":"2021-07-19T09:36:39.993152Z","shell.execute_reply":"2021-07-19T09:36:52.433992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building a knn model on a training dataset using the above best parameters.","metadata":{}},{"cell_type":"code","source":"# instantiate the 'KNeighborsClassifier'\n# n_neighnors: number of neighbors to consider\n# default metric is minkowski, and with p=2 it is equivalent to the euclidean metric\nknn_classification = KNeighborsClassifier(n_neighbors =17,p=2)\n\n# fit the model using fit() on train data\nknn_model = knn_classification.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:52.436344Z","iopub.execute_input":"2021-07-19T09:36:52.436647Z","iopub.status.idle":"2021-07-19T09:36:52.448862Z","shell.execute_reply.started":"2021-07-19T09:36:52.436621Z","shell.execute_reply":"2021-07-19T09:36:52.447795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Line plot to see the accuracy rate and  error rate for each value of K using euclidean distance as a metric of KNN model","metadata":{}},{"cell_type":"code","source":"# consider an empty list to store accuracy rate\naccuracy_rate = []\n\n# use for loop to build a knn model for each K\nfor i in np.arange(1,25,2):\n    \n    # setup a knn classifier with k neighbors\n    # use the 'euclidean' metric \n    knn = KNeighborsClassifier(i, metric = 'euclidean')\n   \n    # fit the model using 'cross_val_score'\n    # pass the knn model as 'estimator'\n    # use 5-fold cross validation\n    score = cross_val_score(knn, X_train, y_train, cv = 5)\n    \n    # calculate the mean score\n    score = score.mean()\n    \n    # compute accuracy rate \n    accuracy_rate.append(score)\n\n# plot the accuracy_rate for different values of K \nplt.plot(range(1,25,2), accuracy_rate,color ='blue',linestyle ='dashed', marker ='o',markerfacecolor ='red', markersize = 10)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('accuracy Rate', fontsize = 15)\nplt.xlabel('K', fontsize = 15)\nplt.ylabel('accuracy Rate', fontsize = 15)\n\n# set the x-axis labels\nplt.xticks(np.arange(1, 25, step = 2))\n\n# plot a vertical line across the maximum accuracy rate\nplt.axvline(x = 17, color = 'red')\n\n# display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:52.450248Z","iopub.execute_input":"2021-07-19T09:36:52.450526Z","iopub.status.idle":"2021-07-19T09:36:56.211426Z","shell.execute_reply.started":"2021-07-19T09:36:52.450498Z","shell.execute_reply":"2021-07-19T09:36:56.209549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# consider an empty list to store error rate\nerror_rate = []\n\n# use for loop to build a knn model for each K\nfor i in np.arange(1,25,2):\n    \n    # setup a knn classifier with k neighbors\n    # use the 'euclidean' metric \n    knn = KNeighborsClassifier(i, metric = 'euclidean')\n   \n    # fit the model using 'cross_val_score'\n    # pass the knn model as 'estimator'\n    # use 5-fold cross validation\n    score = cross_val_score(knn, X_train, y_train, cv = 5)\n    \n    # calculate the mean score\n    score = score.mean()\n    \n    # compute error rate \n    error_rate.append(1 - score)\n\n# plot the error_rate for different values of K \nplt.plot(range(1,25,2), error_rate,color ='blue',linestyle ='dashed', marker ='o',markerfacecolor ='red', markersize = 10)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Error Rate', fontsize = 15)\nplt.xlabel('K', fontsize = 15)\nplt.ylabel('Error Rate', fontsize = 15)\n\n# set the x-axis labels\nplt.xticks(np.arange(1, 25, step = 2))\n\n# plot a vertical line across the minimum error rate\nplt.axvline(x = 17, color = 'red')\n\n# display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:56.212649Z","iopub.execute_input":"2021-07-19T09:36:56.212926Z","iopub.status.idle":"2021-07-19T09:36:59.956091Z","shell.execute_reply.started":"2021-07-19T09:36:56.2129Z","shell.execute_reply":"2021-07-19T09:36:59.955056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** I can see that the optimal value of K (= 17) obtained from the GridSearchCV() results in a lowest error rate and highest accuracy rate. ","metadata":{}},{"cell_type":"markdown","source":"**Confusion matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(knn_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:36:59.957644Z","iopub.execute_input":"2021-07-19T09:36:59.957945Z","iopub.status.idle":"2021-07-19T09:37:00.237941Z","shell.execute_reply.started":"2021-07-19T09:36:59.957917Z","shell.execute_reply":"2021-07-19T09:37:00.236998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_train_report'\n# pass the knn model to the function\ntrain_report = get_train_report(knn_model)\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:37:00.239181Z","iopub.execute_input":"2021-07-19T09:37:00.23947Z","iopub.status.idle":"2021-07-19T09:37:00.521301Z","shell.execute_reply.started":"2021-07-19T09:37:00.23944Z","shell.execute_reply":"2021-07-19T09:37:00.520358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 79% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_test_report'\n# pass the knn model to the function\ntest_report = get_test_report(knn_model)\n\n# print the performace measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:37:00.522658Z","iopub.execute_input":"2021-07-19T09:37:00.522962Z","iopub.status.idle":"2021-07-19T09:37:00.651834Z","shell.execute_reply.started":"2021-07-19T09:37:00.522932Z","shell.execute_reply":"2021-07-19T09:37:00.650898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 74% accurate.","metadata":{}},{"cell_type":"markdown","source":"**Interpretation:** From the above classification reports,I can infer that there is a little difference when compared to test and train reports.\nHence I conclude that the model is bit overfitted.","metadata":{}},{"cell_type":"markdown","source":"**ROC curve**","metadata":{}},{"cell_type":"code","source":"# call the function to plot the ROC curve\n# pass the knn model to the function\nplot_roc(knn_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:37:00.65309Z","iopub.execute_input":"2021-07-19T09:37:00.653384Z","iopub.status.idle":"2021-07-19T09:37:00.960164Z","shell.execute_reply.started":"2021-07-19T09:37:00.653357Z","shell.execute_reply":"2021-07-19T09:37:00.959132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** \nFrom the above plot, I can see that our classifier (knn_model with n_neighbors = 17) is away from the dotted line; with the AUC score **0.7933**.","metadata":{}},{"cell_type":"markdown","source":"#### Score Card","metadata":{}},{"cell_type":"code","source":"update_score_card('KNeighbors Classifier',knn_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:37:00.961272Z","iopub.execute_input":"2021-07-19T09:37:00.961532Z","iopub.status.idle":"2021-07-19T09:37:01.809459Z","shell.execute_reply.started":"2021-07-19T09:37:00.961507Z","shell.execute_reply":"2021-07-19T09:37:01.808482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Decision Tree for Classification<a id=\"dec_tre\"></a>","metadata":{}},{"cell_type":"markdown","source":"**Finding Hyperparameters using GridSearchCV (Decision Tree)**","metadata":{}},{"cell_type":"code","source":"tuned_paramaters = [{'criterion': ['entropy', 'gini'], \n                     'max_depth': [2,4,6,8,10],\n                     'max_features': [\"sqrt\", \"log2\"],\n                     'min_samples_split': [2,4,6,8,10],\n                     'min_samples_leaf': [2,4,6,8,10],\n                     'max_leaf_nodes': [2,4,6,8,10]}]\n \n# instantiate the 'DecisionTreeClassifier' \n# pass the 'random_state' to obtain the same samples for each time you run the code\ndecision_tree_classification = DecisionTreeClassifier(random_state = 10)\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the decision tree classifier model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 5\ntree_grid = GridSearchCV(estimator = decision_tree_classification, \n                         param_grid = tuned_paramaters, \n                         cv = 5)\n\n# fit the model on X_train and y_train using fit()\ntree_grid_model = tree_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for decision tree classifier: ', tree_grid_model.best_params_, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:37:01.810594Z","iopub.execute_input":"2021-07-19T09:37:01.810894Z","iopub.status.idle":"2021-07-19T09:38:43.023089Z","shell.execute_reply.started":"2021-07-19T09:37:01.810868Z","shell.execute_reply":"2021-07-19T09:38:43.021976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building the model using the above obtained tuned hyperparameters.","metadata":{}},{"cell_type":"code","source":"decision_tree = DecisionTreeClassifier(criterion = tree_grid_model.best_params_.get('criterion'),\n                                  max_depth = tree_grid_model.best_params_.get('max_depth'),\n                                  max_features = tree_grid_model.best_params_.get('max_features'),\n                                  max_leaf_nodes = tree_grid_model.best_params_.get('max_leaf_nodes'),\n                                  min_samples_leaf = tree_grid_model.best_params_.get('min_samples_leaf'),\n                                  min_samples_split = tree_grid_model.best_params_.get('min_samples_split'),\n                                  random_state = 10)\n\n# use fit() to fit the model on the train set\ndecision_tree = decision_tree.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:43.024369Z","iopub.execute_input":"2021-07-19T09:38:43.024689Z","iopub.status.idle":"2021-07-19T09:38:43.039871Z","shell.execute_reply.started":"2021-07-19T09:38:43.024656Z","shell.execute_reply":"2021-07-19T09:38:43.038584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision tree with tuned hyperparameters**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(60,30))\nplot_tree(decision_tree, filled=True);","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:43.041515Z","iopub.execute_input":"2021-07-19T09:38:43.041837Z","iopub.status.idle":"2021-07-19T09:38:44.90058Z","shell.execute_reply.started":"2021-07-19T09:38:43.041808Z","shell.execute_reply":"2021-07-19T09:38:44.899635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(decision_tree)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:44.901797Z","iopub.execute_input":"2021-07-19T09:38:44.902272Z","iopub.status.idle":"2021-07-19T09:38:45.072063Z","shell.execute_reply.started":"2021-07-19T09:38:44.902235Z","shell.execute_reply":"2021-07-19T09:38:45.070921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_train_report'\n# pass the decision tree  model to the function\ntrain_report = get_train_report(decision_tree)\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:45.073267Z","iopub.execute_input":"2021-07-19T09:38:45.073537Z","iopub.status.idle":"2021-07-19T09:38:45.093538Z","shell.execute_reply.started":"2021-07-19T09:38:45.07351Z","shell.execute_reply":"2021-07-19T09:38:45.09259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 73% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_test_report'\n# pass the decision tree model to the function\ntest_report = get_test_report(decision_tree)\n\n# print the performace measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:45.095143Z","iopub.execute_input":"2021-07-19T09:38:45.095418Z","iopub.status.idle":"2021-07-19T09:38:45.112344Z","shell.execute_reply.started":"2021-07-19T09:38:45.095391Z","shell.execute_reply":"2021-07-19T09:38:45.111251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 71% accurate.","metadata":{}},{"cell_type":"markdown","source":"From the above classification reports,I can infer that there is a difference when compared to test and train reports.\nHence I conclude that the model is bit overfitted.","metadata":{}},{"cell_type":"markdown","source":"**ROC Curve**","metadata":{}},{"cell_type":"code","source":"# call the function to plot the ROC curve\n# pass the decision tree model to the function\nplot_roc(decision_tree)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:45.113743Z","iopub.execute_input":"2021-07-19T09:38:45.114133Z","iopub.status.idle":"2021-07-19T09:38:45.358377Z","shell.execute_reply.started":"2021-07-19T09:38:45.1141Z","shell.execute_reply":"2021-07-19T09:38:45.357432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** \nFrom the above plot, I can see that our classifier (Decision tree) is away from the dotted line; with the AUC score **0.7415**.","metadata":{}},{"cell_type":"markdown","source":"#### Score Card","metadata":{}},{"cell_type":"code","source":"update_score_card('Decision Tree Classifier',decision_tree)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:45.359764Z","iopub.execute_input":"2021-07-19T09:38:45.360363Z","iopub.status.idle":"2021-07-19T09:38:45.48465Z","shell.execute_reply.started":"2021-07-19T09:38:45.360316Z","shell.execute_reply":"2021-07-19T09:38:45.483712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to decrease the overfitting and increase the performance and accuracy of the Decision tree model.I further perform some Bagging and Boosting techniques.","metadata":{}},{"cell_type":"markdown","source":"# 10. Random Forest<a id=\"ran_for\"></a>","metadata":{}},{"cell_type":"markdown","source":"**Tuning the Hyperparameters using GridSearchCV**","metadata":{}},{"cell_type":"code","source":"tuned_paramaters = [{'criterion': ['entropy', 'gini'],\n                     'n_estimators': [ 30, 50, 70],\n                     'max_depth': [10,15,20],\n                     'max_features': [\"sqrt\", \"log2\"],\n                     'min_samples_split': [2,6,10],\n                     'min_samples_leaf': [2,6,10],\n                     'max_leaf_nodes': [2,6,10]}]\n \n# instantiate the 'RandomForestClassifier' \n# pass the 'random_state' to obtain the same samples for each time you run the code\nrandom_forest_classification = RandomForestClassifier(random_state = 10)\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the random forest classifier model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 5\nrf_grid = GridSearchCV(estimator = random_forest_classification, \n                       param_grid = tuned_paramaters, \n                       cv = 5)\n\n# use fit() to fit the model on the train set\nrf_grid_model = rf_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for random forest classifier: ', rf_grid_model.best_params_, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:38:45.486002Z","iopub.execute_input":"2021-07-19T09:38:45.486309Z","iopub.status.idle":"2021-07-19T09:52:26.649618Z","shell.execute_reply.started":"2021-07-19T09:38:45.48628Z","shell.execute_reply":"2021-07-19T09:52:26.648537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building the model using the tuned hyperparameters obtained above.","metadata":{}},{"cell_type":"code","source":"# instantiate the 'RandomForestClassifier'\n# 'best_params_' returns the dictionary containing best parameter values and parameter name  \n# 'get()' returns the value of specified parameter\n# pass the 'random_state' to obtain the same samples for each time you run the code\nrandom_forest = RandomForestClassifier(criterion = rf_grid_model.best_params_.get('criterion'),\n                                   n_estimators=rf_grid_model.best_params_.get('n_estimators'),\n                                  max_depth = rf_grid_model.best_params_.get('max_depth'),\n                                  max_features = rf_grid_model.best_params_.get('max_features'),\n                                  max_leaf_nodes = rf_grid_model.best_params_.get('max_leaf_nodes'),\n                                  min_samples_leaf = rf_grid_model.best_params_.get('min_samples_leaf'),\n                                  min_samples_split = rf_grid_model.best_params_.get('min_samples_split'),\n                                  random_state = 10)\n\n# use fit() to fit the model on the train set\nrandom_forest = random_forest.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:26.651158Z","iopub.execute_input":"2021-07-19T09:52:26.651799Z","iopub.status.idle":"2021-07-19T09:52:26.840727Z","shell.execute_reply.started":"2021-07-19T09:52:26.651756Z","shell.execute_reply":"2021-07-19T09:52:26.83989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(random_forest)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:26.841947Z","iopub.execute_input":"2021-07-19T09:52:26.842554Z","iopub.status.idle":"2021-07-19T09:52:27.017748Z","shell.execute_reply.started":"2021-07-19T09:52:26.842513Z","shell.execute_reply":"2021-07-19T09:52:27.016915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_train_report'\n# pass the Random Forest  model to the function\ntrain_report = get_train_report(random_forest)\n\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:27.018974Z","iopub.execute_input":"2021-07-19T09:52:27.019544Z","iopub.status.idle":"2021-07-19T09:52:27.052522Z","shell.execute_reply.started":"2021-07-19T09:52:27.019501Z","shell.execute_reply":"2021-07-19T09:52:27.051761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 77% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_test_report'\n# pass the Random Forest model to the function\ntest_report = get_test_report(random_forest)\n\n# print the performace measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:27.05352Z","iopub.execute_input":"2021-07-19T09:52:27.053825Z","iopub.status.idle":"2021-07-19T09:52:27.077357Z","shell.execute_reply.started":"2021-07-19T09:52:27.053797Z","shell.execute_reply":"2021-07-19T09:52:27.076379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 74% accurate.","metadata":{}},{"cell_type":"markdown","source":"**ROC Curve**","metadata":{}},{"cell_type":"code","source":"# call the function to plot the ROC curve\n# pass the random forest model to the function\nplot_roc(random_forest)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:27.078868Z","iopub.execute_input":"2021-07-19T09:52:27.079291Z","iopub.status.idle":"2021-07-19T09:52:27.356724Z","shell.execute_reply.started":"2021-07-19T09:52:27.079249Z","shell.execute_reply":"2021-07-19T09:52:27.355789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** \nFrom the above plot, I can see that our classifier (random forest) is away from the dotted line; with the AUC score **0.7932**.","metadata":{}},{"cell_type":"markdown","source":"#### Score Card","metadata":{}},{"cell_type":"code","source":"update_score_card('Random Forest Classifier',random_forest)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:27.35792Z","iopub.execute_input":"2021-07-19T09:52:27.358208Z","iopub.status.idle":"2021-07-19T09:52:29.245895Z","shell.execute_reply.started":"2021-07-19T09:52:27.358179Z","shell.execute_reply":"2021-07-19T09:52:29.244976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. AdaBoost<a id=\"ada\"></a>","metadata":{}},{"cell_type":"markdown","source":"**Tune the Hyperparameters (GridSearchCV)**","metadata":{}},{"cell_type":"code","source":"tuning_parameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],'n_estimators' : [10,20,30,40,50]}\nada_model = AdaBoostClassifier()\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the AdaBoost classifier model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 3\n# scoring: pass a measure to evaluate the model on test set\nada_grid = GridSearchCV(estimator = ada_model, param_grid = tuning_parameters, cv = 3, scoring = 'roc_auc')\n\n# fit the model on X_train and y_train using fit()\nada_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for ADABoost classifier: ', ada_grid.best_params_, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:29.24719Z","iopub.execute_input":"2021-07-19T09:52:29.247479Z","iopub.status.idle":"2021-07-19T09:52:40.750641Z","shell.execute_reply.started":"2021-07-19T09:52:29.247451Z","shell.execute_reply":"2021-07-19T09:52:40.749347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building an Adaboost model on a training dataset.**","metadata":{}},{"cell_type":"code","source":"# instantiate the 'AdaBoostClassifier'\n# n_estimators: number of estimators at which boosting is terminated\n# pass the 'random_state' to obtain the same results for each code implementation\nada_model = AdaBoostClassifier(n_estimators = 40, random_state = 10,learning_rate= 0.4)\n\n# fit the model using fit() on train data\nada_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:40.751658Z","iopub.execute_input":"2021-07-19T09:52:40.751988Z","iopub.status.idle":"2021-07-19T09:52:40.949111Z","shell.execute_reply.started":"2021-07-19T09:52:40.751958Z","shell.execute_reply":"2021-07-19T09:52:40.948162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(ada_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:40.950454Z","iopub.execute_input":"2021-07-19T09:52:40.950809Z","iopub.status.idle":"2021-07-19T09:52:41.131496Z","shell.execute_reply.started":"2021-07-19T09:52:40.950776Z","shell.execute_reply":"2021-07-19T09:52:41.130576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_train_report'\n# pass the adaboost model to the function\ntrain_report = get_train_report(ada_model)\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:41.132673Z","iopub.execute_input":"2021-07-19T09:52:41.132954Z","iopub.status.idle":"2021-07-19T09:52:41.172188Z","shell.execute_reply.started":"2021-07-19T09:52:41.132928Z","shell.execute_reply":"2021-07-19T09:52:41.17112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 78% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test Report**","metadata":{}},{"cell_type":"code","source":"# compute the performance measures on test data\n# call the function 'get_test_report'\n# pass the adaboost model to the function\ntest_report = get_test_report(ada_model)\n\n# print the performace measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:41.17332Z","iopub.execute_input":"2021-07-19T09:52:41.173608Z","iopub.status.idle":"2021-07-19T09:52:41.202058Z","shell.execute_reply.started":"2021-07-19T09:52:41.17358Z","shell.execute_reply":"2021-07-19T09:52:41.201049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 75% accurate.","metadata":{}},{"cell_type":"markdown","source":"**ROC Curve**","metadata":{}},{"cell_type":"code","source":"# call the function to plot the ROC curve\n# pass the adaboost model to the function\nplot_roc(ada_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:41.203402Z","iopub.execute_input":"2021-07-19T09:52:41.20371Z","iopub.status.idle":"2021-07-19T09:52:41.451249Z","shell.execute_reply.started":"2021-07-19T09:52:41.203668Z","shell.execute_reply":"2021-07-19T09:52:41.450155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** \nFrom the above plot, I can see that our classifier (ADA Boost) is away from the dotted line; with the AUC score **0.7961**.","metadata":{}},{"cell_type":"markdown","source":"#### Score Card","metadata":{}},{"cell_type":"code","source":"update_score_card('Ada Boost Classifier',ada_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:41.452331Z","iopub.execute_input":"2021-07-19T09:52:41.452608Z","iopub.status.idle":"2021-07-19T09:52:43.376752Z","shell.execute_reply.started":"2021-07-19T09:52:41.45258Z","shell.execute_reply":"2021-07-19T09:52:43.375562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Gradient Boosting<a id=\"gra_boo\"></a>","metadata":{}},{"cell_type":"markdown","source":"**Tune the Hyperparameters (GridSearchCV)**","metadata":{}},{"cell_type":"code","source":"tuning_parameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],'n_estimators':[30, 50, 70, 90],\n                     'max_depth': [2,6,10],'min_samples_split': [2,6,10],\n                     'min_samples_leaf': [2,6,10]}\ngboost_model = GradientBoostingClassifier()\ngb_grid = GridSearchCV(estimator = gboost_model, param_grid = tuning_parameters, cv = 3, scoring = 'roc_auc')\ngb_grid.fit(X_train, y_train)\nprint('Best parameters for GBoost classifier: ', gb_grid.best_params_, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:52:43.378143Z","iopub.execute_input":"2021-07-19T09:52:43.37842Z","iopub.status.idle":"2021-07-19T10:12:38.200489Z","shell.execute_reply.started":"2021-07-19T09:52:43.378394Z","shell.execute_reply":"2021-07-19T10:12:38.199462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building a Gradient boost model on a training dataset","metadata":{}},{"cell_type":"code","source":"gboost_model = GradientBoostingClassifier(n_estimators = 70, random_state = 10,learning_rate=0.2,\n                                          min_samples_leaf=10,min_samples_split=2,max_depth= 2)\ngboost_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:12:38.201735Z","iopub.execute_input":"2021-07-19T10:12:38.202022Z","iopub.status.idle":"2021-07-19T10:12:38.543154Z","shell.execute_reply.started":"2021-07-19T10:12:38.201984Z","shell.execute_reply":"2021-07-19T10:12:38.542328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(gboost_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:12:38.544352Z","iopub.execute_input":"2021-07-19T10:12:38.54462Z","iopub.status.idle":"2021-07-19T10:12:38.708958Z","shell.execute_reply.started":"2021-07-19T10:12:38.544594Z","shell.execute_reply":"2021-07-19T10:12:38.708039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train report**","metadata":{}},{"cell_type":"code","source":"train_report = get_train_report(gboost_model)\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:12:38.71008Z","iopub.execute_input":"2021-07-19T10:12:38.710343Z","iopub.status.idle":"2021-07-19T10:12:38.730214Z","shell.execute_reply.started":"2021-07-19T10:12:38.710317Z","shell.execute_reply":"2021-07-19T10:12:38.729263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 80% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test Report**","metadata":{}},{"cell_type":"code","source":"test_report = get_test_report(gboost_model)\n\n# print the performance measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:12:38.731359Z","iopub.execute_input":"2021-07-19T10:12:38.731623Z","iopub.status.idle":"2021-07-19T10:12:38.745984Z","shell.execute_reply.started":"2021-07-19T10:12:38.731598Z","shell.execute_reply":"2021-07-19T10:12:38.745133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 76% accurate.","metadata":{}},{"cell_type":"markdown","source":"**ROC Curve**","metadata":{}},{"cell_type":"code","source":"plot_roc(gboost_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:12:38.747388Z","iopub.execute_input":"2021-07-19T10:12:38.747656Z","iopub.status.idle":"2021-07-19T10:12:38.987552Z","shell.execute_reply.started":"2021-07-19T10:12:38.74763Z","shell.execute_reply":"2021-07-19T10:12:38.986538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** \n\nFrom the above plot, I can see that our classifier(Gradient Boosting model) is away from the dotted line; with the AUC score **0.8136**.","metadata":{}},{"cell_type":"markdown","source":"**Score Card**","metadata":{}},{"cell_type":"code","source":"update_score_card('Gradient Boosting Classifier',gboost_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:12:38.988794Z","iopub.execute_input":"2021-07-19T10:12:38.989059Z","iopub.status.idle":"2021-07-19T10:12:42.089641Z","shell.execute_reply.started":"2021-07-19T10:12:38.989033Z","shell.execute_reply":"2021-07-19T10:12:42.088654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13 Extreme Gradient Boosting (XGB)<a id=\"xgb\"> </a>","metadata":{}},{"cell_type":"markdown","source":"**Tuning the Hyperparameters (GridSearchCV)**","metadata":{}},{"cell_type":"code","source":"tuning_parameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n                     'max_depth': [1,3,5,7,9],\n                     'gamma': [0, 1, 2, 3, 4]}\nxgb_model = XGBClassifier()\nxgb_grid = GridSearchCV(estimator = xgb_model, param_grid = tuning_parameters, cv = 3, scoring = 'roc_auc',verbose=0)\nxgb_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for XGBoost classifier: ', xgb_grid.best_params_, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:15:56.027382Z","iopub.execute_input":"2021-07-19T10:15:56.02778Z","iopub.status.idle":"2021-07-19T10:17:24.188765Z","shell.execute_reply.started":"2021-07-19T10:15:56.02775Z","shell.execute_reply":"2021-07-19T10:17:24.187952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building an Extreme Gradient boost model on a training dataset","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBClassifier(learning_rate=0.1, gamma = 1,max_depth=3,verbosity=0)\n\n# fit the model using fit() on train data\nxgb_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:07.458133Z","iopub.execute_input":"2021-07-19T10:14:07.458404Z","iopub.status.idle":"2021-07-19T10:14:07.616588Z","shell.execute_reply.started":"2021-07-19T10:14:07.458378Z","shell.execute_reply":"2021-07-19T10:14:07.615496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(xgb_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:07.617656Z","iopub.execute_input":"2021-07-19T10:14:07.617935Z","iopub.status.idle":"2021-07-19T10:14:07.787032Z","shell.execute_reply.started":"2021-07-19T10:14:07.617909Z","shell.execute_reply":"2021-07-19T10:14:07.786124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Report**","metadata":{}},{"cell_type":"code","source":"train_report = get_train_report(xgb_model)\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:07.788198Z","iopub.execute_input":"2021-07-19T10:14:07.788471Z","iopub.status.idle":"2021-07-19T10:14:07.808776Z","shell.execute_reply.started":"2021-07-19T10:14:07.788444Z","shell.execute_reply":"2021-07-19T10:14:07.80783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 82% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test Report**","metadata":{}},{"cell_type":"code","source":"test_report = get_test_report(xgb_model)\n\n# print the performance measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:07.810019Z","iopub.execute_input":"2021-07-19T10:14:07.810333Z","iopub.status.idle":"2021-07-19T10:14:07.828277Z","shell.execute_reply.started":"2021-07-19T10:14:07.810304Z","shell.execute_reply":"2021-07-19T10:14:07.827251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 76% accurate.","metadata":{}},{"cell_type":"markdown","source":"**ROC Curve**","metadata":{}},{"cell_type":"code","source":"plot_roc(xgb_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:07.829674Z","iopub.execute_input":"2021-07-19T10:14:07.830007Z","iopub.status.idle":"2021-07-19T10:14:08.075977Z","shell.execute_reply.started":"2021-07-19T10:14:07.829975Z","shell.execute_reply":"2021-07-19T10:14:08.074969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** \n\nFrom the above plot, I can see that our classifier(Extreme Gradient Boosting model) is away from the dotted line; with the AUC score **0.8129**.","metadata":{}},{"cell_type":"markdown","source":"#### Score Card","metadata":{}},{"cell_type":"code","source":"update_score_card('Extreme Gradient Boosting Classifier',xgb_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:08.077113Z","iopub.execute_input":"2021-07-19T10:14:08.077379Z","iopub.status.idle":"2021-07-19T10:14:10.376951Z","shell.execute_reply.started":"2021-07-19T10:14:08.077353Z","shell.execute_reply":"2021-07-19T10:14:10.375848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14. Stack Generalization<a id=\"stack\"></a>","metadata":{}},{"cell_type":"code","source":"# consider the various algorithms as base learners\nbase_learners = [('rf_model', RandomForestClassifier(criterion = 'gini', max_depth = 10, max_features = 'sqrt', \n                                                     max_leaf_nodes = 10, min_samples_leaf = 10, min_samples_split = 2, \n                                                     n_estimators = 50, random_state = 10)),\n                 ('KNN_model', KNeighborsClassifier(n_neighbors = 17, metric = 'euclidean')),\n                 ('NB_model', GaussianNB()),\n                 ('Decision_tree',DecisionTreeClassifier(criterion = 'entropy', max_depth= 6, max_features= 'sqrt',\n                                                         max_leaf_nodes= 10, min_samples_leaf= 10, min_samples_split= 2,random_state = 10))]\n\n# initialize stacking classifier \n# pass the base learners to the parameter, 'estimators'\n# pass the XGB Classifier model as the 'final_estimator'/ meta model\nstack_model = StackingClassifier(estimators = base_learners, final_estimator = XGBClassifier())\n\n# fit the model on train dataset\nstack_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:10.38875Z","iopub.execute_input":"2021-07-19T10:14:10.389083Z","iopub.status.idle":"2021-07-19T10:14:11.973065Z","shell.execute_reply.started":"2021-07-19T10:14:10.389047Z","shell.execute_reply":"2021-07-19T10:14:11.972227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(stack_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:11.974497Z","iopub.execute_input":"2021-07-19T10:14:11.974778Z","iopub.status.idle":"2021-07-19T10:14:12.239774Z","shell.execute_reply.started":"2021-07-19T10:14:11.974752Z","shell.execute_reply":"2021-07-19T10:14:12.238546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Report**","metadata":{}},{"cell_type":"code","source":"train_report = get_train_report(stack_model)\n\n# print the performace measures\nprint(train_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:12.240844Z","iopub.execute_input":"2021-07-19T10:14:12.241101Z","iopub.status.idle":"2021-07-19T10:14:12.452815Z","shell.execute_reply.started":"2021-07-19T10:14:12.241077Z","shell.execute_reply":"2021-07-19T10:14:12.451791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the training model has 77% accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Test Report**","metadata":{}},{"cell_type":"code","source":"test_report = get_test_report(stack_model)\n\n# print the performance measures\nprint(test_report)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:12.454194Z","iopub.execute_input":"2021-07-19T10:14:12.454499Z","iopub.status.idle":"2021-07-19T10:14:12.564218Z","shell.execute_reply.started":"2021-07-19T10:14:12.454462Z","shell.execute_reply":"2021-07-19T10:14:12.563385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** From the above output, I can see that the testing model is 72% accurate.","metadata":{}},{"cell_type":"markdown","source":"**ROC Curve**","metadata":{}},{"cell_type":"code","source":"plot_roc(stack_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:12.567801Z","iopub.execute_input":"2021-07-19T10:14:12.568376Z","iopub.status.idle":"2021-07-19T10:14:12.895899Z","shell.execute_reply.started":"2021-07-19T10:14:12.56833Z","shell.execute_reply":"2021-07-19T10:14:12.894793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** \n\nFrom the above plot, I can see that our classifier(Stack Generalized model) is away from the dotted line; with the AUC score **0.7574**.","metadata":{}},{"cell_type":"markdown","source":"#### Score Card","metadata":{}},{"cell_type":"code","source":"update_score_card('Stack Generalization',stack_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:12.897348Z","iopub.execute_input":"2021-07-19T10:14:12.897757Z","iopub.status.idle":"2021-07-19T10:14:28.385601Z","shell.execute_reply.started":"2021-07-19T10:14:12.897689Z","shell.execute_reply":"2021-07-19T10:14:28.384739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 15. Displaying score summary<a id=\"dis_sco\"></a>","metadata":{}},{"cell_type":"code","source":"score_card = score_card.sort_values('Diff_b/w_train&test(Acc)').reset_index(drop = True)\n\nscore_card.style.highlight_min(color = 'red', subset = ['Diff_b/w_train&test(Acc)'])","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:28.386819Z","iopub.execute_input":"2021-07-19T10:14:28.387079Z","iopub.status.idle":"2021-07-19T10:14:28.404025Z","shell.execute_reply.started":"2021-07-19T10:14:28.387055Z","shell.execute_reply":"2021-07-19T10:14:28.403059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:**  I can see that Decision tree classifier has the lowest difference between train accuracy and test                                   accuracy.Hence,I conclude Decision tree classifier is the `Best_Model`","metadata":{}},{"cell_type":"markdown","source":"# 16. Feature Importance<a id=\"fea_imp\"></a>","metadata":{}},{"cell_type":"code","source":"# create a dataframe that stores the feature names and their importance\n# 'feature_importances_' returns the features based on the average gain \nimportant_features = pd.DataFrame({'Features': X_train.columns, \n                                   'Importance': decision_tree.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:14:28.405478Z","iopub.execute_input":"2021-07-19T10:14:28.405788Z","iopub.status.idle":"2021-07-19T10:14:28.667529Z","shell.execute_reply.started":"2021-07-19T10:14:28.405761Z","shell.execute_reply":"2021-07-19T10:14:28.66653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** The above bar plot shows that, of all the features `chlorides` is of most important feature. ","metadata":{}},{"cell_type":"markdown","source":"# 17. Conclusion<a id=\"conclu\"></a>","metadata":{}},{"cell_type":"markdown","source":"**Of all the models built, I see that Decision tree classifier model has been the most effective with no overfitting.**\n\n**Some of the features which contribute more for prediction of quality are chlorides,density,citric acid,volatile_acidity and sulfur_dioxide_ratio.**\n\n**Results can be used by wine manufactures to improve the quality of wine in future and can also be used by consumers for wine selection.**\n\n**I can hereby conclude that I have successfully built a model that can predict quality of wine.**","metadata":{}},{"cell_type":"markdown","source":"# 18.Deployment<a id=\"deploy\"></a>","metadata":{}},{"cell_type":"markdown","source":"https://winesupreme.herokuapp.com/","metadata":{}},{"cell_type":"markdown","source":"# 19. References<a id=\"Refer\"></a>","metadata":{}},{"cell_type":"markdown","source":"https://www.ijsr.net/archive/v9i7/SR20718002904.pdf\n\nhttps://ijcat.com/archieve/volume8/issue9/ijcatr08091010.pdf\n\nhttps://broncoscholar.library.cpp.edu/bitstream/handle/10211.3/216015/NelsonGregory_Thesis2020.pdf?sequence=3\n\nhttp://cs229.stanford.edu/proj2015/245_report.pdf\n\nhttps://scihub.se/https://www.sciencedirect.com/science/article/pii/S1877050917328053","metadata":{}},{"cell_type":"markdown","source":"<table align=\"center\" width=100%>\n    <tr>\n        <td width=\"50%\">\n            <img src=\"https://media4.giphy.com/media/QMkPpxPDYY0fu/200.gif\">\n        </td>\n        <td>\n            <div align=\"center\">\n                <font color=\"#1A040C \" size=24px>\n                    <b>Thank You.\n                    </b>\n                </font>\n            </div>\n        </td>\n    </tr>\n</table>","metadata":{}}]}