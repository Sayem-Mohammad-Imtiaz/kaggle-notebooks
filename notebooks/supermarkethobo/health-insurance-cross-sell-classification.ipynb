{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Health Insurance Cross Sell (by Andrew Dettor)\n### Will a Health Insurance customer be open to buying Vehicle Insurance as well?\n\n### Dataset: [Health Insurance Cross Sell Prediction üè† üè•](https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction)\n### Goals:\n* Explore distributions of numerical and categorical features and their relationships with the target feature, Response (whether the customer responded to an offer about buying vehicle insurance)\n* Preprocess data in order to model it (look at missing values/outliers/skewed distributions/standardization)\n* Test out different Classification models by tuning their hyperparameters and comparing their performance\n* Explore which features were the most impactful\n* Explore potential interactions between features\n\n\n### FYI - Confusing Names for Features:\n* Vehicle_Damage - 1 = Customer got his/her vehicle damaged in the past. 0 = Customer didn't get his/her vehicle damaged in the past.\n* Vintage - Number of Days Customer has been associated with the company.\n* Policy_Sales_Channel - Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n* Annual_Premium - The amount customer needs to pay as premium in the year. (Not sure if this is talking only about their current health insurance premium or their potential vehicle insurance premium)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Read in the data*"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"fname = \"../input/health-insurance-cross-sell-prediction/train.csv\"\ndf = pd.read_csv(fname)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Exploratory Data Analysis*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No missing values! That's a plus."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A lot of these numerical values seem like they're actually categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping id because it seems useless\nid_col = df[\"id\"]\ndf = df.drop(\"id\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a list of all categorical values using data description\n\ncat_cols = [\"Gender\", \"Driving_License\", \"Region_Code\", \"Previously_Insured\", \"Vehicle_Age\", \"Vehicle_Damage\", \"Policy_Sales_Channel\"]\n\n# Make a list of all numerical values using data description\n\nnum_cols = [\"Age\", \"Annual_Premium\", \"Vintage\"]\n\n# Set the target variable\n\ntarget = \"Response\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a correlation heatmap between the numerical values\n# Source: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Compute the correlation matrix\ncorr = df[num_cols].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(5, 5))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No correlation amongst the numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create histograms for each numerical feature to see their distributions\n\nfor cname in num_cols:\n    sns.distplot(a=df[cname], kde=False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Age has a center around 25 and a smaller center around 45. It is asymmetric, with a large right skew. The data are quite spread out from the histogram's centers. Maybe Age could be normalized for the model.\n#### Annual Premium has a center around 30,000, and is generally bell-shaped and symmetric, however, there is a huge outlier around 500,000. The data are cery concentrated around its center. \n#### Vintage has a uniform distribution with no visible center and has a large spread.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the average value for each response value\n\npd.pivot_table(df, index=target, values=num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The average ages, insurance premiums, and number of days the customer has been with the company are very similar in both response groups. The biggest difference is with Age. It seems like older people are more likely to buy health insurance (not surprising)."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create barplots for each categorical feature to see their distributions\n\nfor cname in cat_cols:\n    valueCounts = df[cname].value_counts()\n    sns.barplot(valueCounts.index, valueCounts).set_title(cname)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are slightly more males than females in the dataset.\n#### Just about everyone has a driver's license, which makes sense.\n#### One region has an extraordinarily large number of people in it.\n#### Most people don't already have vehicle insurance.\n#### Most people's vehicles are within 0-2 years old.\n#### There are about the same number of people who have had vehicle damage in the past as there are with no previous vehicle damage.\n#### There about 3-5 sales channels that are used most frequently.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# See how people responded based on each value of each categorical feature\n# Source: https://www.kaggle.com/kenjee/titanic-project-example\n\ndf_with_id = pd.concat([id_col, df], axis=1)\n\nfor cname in cat_cols:\n    print(pd.pivot_table(df_with_id, index=target, columns=cname, values =\"id\", aggfunc=\"count\"))\n    print(\"\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Almost nobody who was interested in vehicle insurance already had vehicle insurance. (It's a waste of time to ask people who already have insurance to get a different insurance for the same thing.)\n\n#### Very very few people wanted vehicle insurance if they have had no damage previously. People who have had damage before were extremely more likely to want vehicle insurance.\n\n#### The number of people from each sales channel and in each region varies wildly.\n\n#### Proportionally, very few people with a vehicle age <1 year wanted to get vehicle insurance.\n\n#### Looking at the Age pivot table, it seems like a higher proportion of males wanted vehicle insurance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the proportions of people coming from the top sales channels\n\n(df[\"Policy_Sales_Channel\"].value_counts()/df[\"Policy_Sales_Channel\"].value_counts().sum()).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the proportions of people in the top regions\n\n(df[\"Region_Code\"].value_counts()/df[\"Region_Code\"].value_counts().sum()).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Feature Engineering*"},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Categorical Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot Encode the columns with low cardinality\n\nfor cname in [\"Gender\", \"Vehicle_Age\", \"Vehicle_Damage\"]:\n    df_one_hot = pd.get_dummies(df[cname], prefix=cname)\n    df = pd.concat([df, df_one_hot], axis=1)\n    df = df.drop(cname, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the spaces in column names for easier access\n# Source: https://stackoverflow.com/questions/13757090/pandas-column-access-w-column-names-containing-spaces\n\ndf.columns = [c.replace(' ', '_') for c in df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How to deal with columns with high cardinality (Region_Code, Policy_Sales_Channel):\n#### Option 1: Group categorical values such that there are few unique values, then One-Hot Encode them\n#### Option 2: Use a different type of Encoder that doesn't add so many new columns\n\n#### I'm going to go with option 2 because it'll add fewer features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use a Target Encoder \n# Replaces categorical values with the average value of the target for that value of the feature\n\nfrom category_encoders import TargetEncoder\n\n# NOTE: Only fit categorical encoders to a training set to avoid target leakage\n# Will fit the encoder within a K-Fold loop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# People are very unlikely to say yes if they're insured or if they've had no previous vehicle damage\n# Combine these two into one feature. It should be a good predictor of saying no.\ndf[\"Insured_With_No_Damage\"] = df[\"Previously_Insured\"]*df[\"Vehicle_Damage_No\"]\n\n# People are very likely to say yes if they're uninsured or if they've had previous damage\n# Should be a good predictor of saying yes\ndf[\"Not_Insured_With_Damage\"] = df[\"Previously_Insured\"].apply(lambda x: 1 if x == 0 else 0) * df[\"Vehicle_Damage_Yes\"]\n\n# If people have a new car with damage and no insurance, they probably should get insurance\ndf[\"New_Damage_No_Insurance\"] = df[\"Vehicle_Age_<_1_Year\"]*df[\"Not_Insured_With_Damage\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create features indicating if they're in the top 3 most popular sales channels / region codes\ntop3regions = df[\"Region_Code\"].value_counts().index.tolist()[0:3]\ntop3channels = df[\"Policy_Sales_Channel\"].value_counts().index.tolist()[0:3]\n\ndf[\"Top_3_Region\"] = df[\"Region_Code\"].apply(lambda x: 1 if x in top3regions else 0)\ndf[\"Top_3_Sales_Channel\"] = df[\"Policy_Sales_Channel\"].apply(lambda x: 1 if x in top3channels else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If a customer pays a high amount of money, but has been with the company for a long time, they probably have the money to pay more\n\ndf[\"Amount_Spent_Per_Day\"] = df[\"Annual_Premium\"]/df[\"Vintage\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One more feature I could add is if the customer is in the top 3 most likely to say yes regions/sales channels, rather than the top 3 most populous regions/channels.\n#### However, I would have to do this on a training set to prevent target leakage\n#### This is probably redundant because of the Target Encoder I'll be using for these columns"},{"metadata":{},"cell_type":"markdown","source":"# *Data Cleaning*"},{"metadata":{},"cell_type":"markdown","source":"## Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are no missing values, so I don't have to impute anything. However, if there were, I would impute each feature as follows:\n\nNumerical Columns: mean\\\nCategorical Columns: most frequent"},{"metadata":{},"cell_type":"markdown","source":"## Outliers"},{"metadata":{},"cell_type":"markdown","source":"#### I saw with Annual_Premium the histogram extended out to like 500,000. I should drop the outliers before normalizing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the skew\n\ndf[\"Annual_Premium\"].skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detecting outliers based on what I remember from my statistics class\n\nq75 = df[\"Annual_Premium\"].quantile(q=.75)\nq25 = df[\"Annual_Premium\"].quantile(q=.25)\nIQR = q75-q25\n\nlowerBound = q25 - 1.5*IQR\nupperBound = q75 + 1.5*IQR\n\nprint(lowerBound)\nprint(upperBound)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\n\noutliers = df.loc[(df[\"Annual_Premium\"] < lowerBound) | (df[\"Annual_Premium\"] > upperBound)]\ndf = df.drop(outliers.index)\nprint(\"Dropped\", outliers.shape[0], \"outliers.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalization "},{"metadata":{"trusted":true},"cell_type":"code","source":"# How has skew changed from removing outliers?\n\ndf[\"Annual_Premium\"].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Still moderately skewed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age skew\n\ndf[\"Age\"].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Also moderately skewed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vintage skew\n\ndf[\"Vintage\"].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Unsurprisingly not skewed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do a BoxCox transformation on Age and Annual Premium\n# Source: https://www.kaggle.com/datafan07/top-1-approach-eda-new-models-and-stacking\n\nfrom scipy.stats import boxcox\n\ndf[\"Age\"], age_lambda = boxcox(df[\"Age\"])\ndf[\"Annual_Premium\"], annualprem_lambda = boxcox(df[\"Annual_Premium\"])\n\n# Keep track of each lambda to use when preprocessing the test set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the numerical data for the models that require it\n# Don't forget to also scale the Target Encoded features after the encoder has been fit on a training set\n# Source: https://www.kaggle.com/kenjee/titanic-project-example\n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ndf[[\"Age\", \"Annual_Premium\", \"Vintage\", \"Amount_Spent_Per_Day\"]] = scale.fit_transform(df[[\"Age\", \"Annual_Premium\", \"Vintage\", \"Amount_Spent_Per_Day\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Model Selection*"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing and cross validation functions\nNOTE: I could do things a lot simpler if I didn't have to worry about target leakage with the TargetEncoder. Right now, each validation split needs to be tailored to the values in the corresponding training split. I can't just pass in a preprocessed big X into a cross validation function which randomly splits the X, because it would contain target-leaked data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preprocessed_cv_data(train_X, val_X, train_y, val_y):\n    \n    ###############################################################################################\n    # One-Hot Encode the columns with low cardinality\n    for cname in [\"Gender\", \"Vehicle_Age\", \"Vehicle_Damage\"]:\n        train_X_one_hot = pd.get_dummies(train_X[cname], prefix=cname)\n        train_X = pd.concat([train_X, train_X_one_hot], axis=1)\n        train_X = train_X.drop(cname, axis=1)\n        \n        val_X_one_hot = pd.get_dummies(val_X[cname], prefix=cname)\n        val_X = pd.concat([val_X, val_X_one_hot], axis=1)\n        val_X = val_X.drop(cname, axis=1)\n        \n    ###############################################################################################\n    # Target Encode the columns with high cardinality\n    \n    enc = TargetEncoder(cols=[\"Region_Code\", \"Policy_Sales_Channel\"])\n    train_X = enc.fit_transform(train_X, train_y)\n    val_X = enc.transform(val_X)\n        \n    ###############################################################################################\n    # Remove the spaces in column names for easier access\n    train_X.columns = [c.replace(' ', '_') for c in train_X.columns]\n    val_X.columns = [c.replace(' ', '_') for c in val_X.columns]\n    \n    # Remove the < sign bc it doesnt work for XGB Classifier\n    train_X.columns = [c.replace('<', 'less') for c in train_X.columns]\n    val_X.columns = [c.replace('<', 'less') for c in val_X.columns]\n    \n    # Remove the > sign bc it doesnt work for XGB Classifier\n    train_X.columns = [c.replace('>', 'greater') for c in train_X.columns]\n    val_X.columns = [c.replace('>', 'greater') for c in val_X.columns]\n    \n    ###############################################################################################\n    # Feature Engineering\n    train_X[\"Insured_With_No_Damage\"] = train_X[\"Previously_Insured\"]*train_X[\"Vehicle_Damage_No\"]\n    train_X[\"Not_Insured_With_Damage\"] = train_X[\"Previously_Insured\"].apply(lambda x: 1 if x == 0 else 0) * train_X[\"Vehicle_Damage_Yes\"]\n    train_X[\"New_Damage_No_Insurance\"] = train_X[\"Vehicle_Age_less_1_Year\"]*train_X[\"Not_Insured_With_Damage\"]\n    train_X[\"Amount_Spent_Per_Day\"] = train_X[\"Annual_Premium\"]/train_X[\"Vintage\"]\n    \n    val_X[\"Insured_With_No_Damage\"] = val_X[\"Previously_Insured\"]*val_X[\"Vehicle_Damage_No\"]\n    val_X[\"Not_Insured_With_Damage\"] = val_X[\"Previously_Insured\"].apply(lambda x: 1 if x == 0 else 0) * val_X[\"Vehicle_Damage_Yes\"]\n    val_X[\"New_Damage_No_Insurance\"] = val_X[\"Vehicle_Age_less_1_Year\"]*val_X[\"Not_Insured_With_Damage\"]\n    val_X[\"Amount_Spent_Per_Day\"] = val_X[\"Annual_Premium\"]/val_X[\"Vintage\"]\n    \n    ###############################################################################################\n    # More Feature Engineering\n    top3regions = train_X[\"Region_Code\"].value_counts().index.tolist()[0:3]\n    top3channels = train_X[\"Policy_Sales_Channel\"].value_counts().index.tolist()[0:3]\n\n    train_X[\"Top_3_Region\"] = train_X[\"Region_Code\"].apply(lambda x: 1 if x in top3regions else 0)\n    val_X[\"Top_3_Region\"] = val_X[\"Region_Code\"].apply(lambda x: 1 if x in top3regions else 0)\n    \n    train_X[\"Top_3_Sales_Channel\"] = train_X[\"Policy_Sales_Channel\"].apply(lambda x: 1 if x in top3channels else 0)\n    val_X[\"Top_3_Sales_Channel\"] = val_X[\"Policy_Sales_Channel\"].apply(lambda x: 1 if x in top3channels else 0)\n    \n    ###############################################################################################\n    # Remove outliers from training set Annual Premium\n    q75 = train_X[\"Annual_Premium\"].quantile(q=.75)\n    q25 = train_X[\"Annual_Premium\"].quantile(q=.25)\n    IQR = q75-q25\n\n    lowerBound = q25 - 1.5*IQR\n    upperBound = q75 + 1.5*IQR\n    \n    outliers_train = train_X.loc[(train_X[\"Annual_Premium\"] < lowerBound) | (train_X[\"Annual_Premium\"] > upperBound)]\n    train_X = train_X.drop(outliers_train.index)\n    train_y = train_y.drop(outliers_train.index)\n    \n    ###############################################################################################\n    # Normalize Age and Annual Premium\n    # Use same lambdas on validation set\n    \n    for col in [\"Age\", \"Annual_Premium\"]:\n        train_X[col], lmbda = boxcox(train_X[col])\n        val_X[col] = boxcox(val_X[col], lmbda=lmbda)\n        \n    return train_X, val_X, train_y, val_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\n# Returns the best hyperparameters and average score from 5-fold cross validation\ndef get_cv_score_and_best_model(X, y, model, paramfield, boolScale):\n    \n    totalScore = 0\n    bestScore = 0\n    bestParams = {}\n    \n    kf = KFold(n_splits=3)\n    for train_index, val_index in kf.split(X):\n\n        # Create train/test splits and preprocess using previously defined function\n        train_X, val_X, train_y, val_y = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n        train_X, val_X, train_y, val_y = get_preprocessed_cv_data(train_X, val_X, train_y, val_y)\n        \n        # Scale all numerical features if required (for SVM and KNN)\n        if boolScale:\n            toScale = [\"Age\", \"Annual_Premium\", \"Vintage\", \"Amount_Spent_Per_Day\", \"Region_Code\", \"Policy_Sales_Channel\"]\n            scale = StandardScaler()\n            train_X[toScale] = scale.fit_transform(train_X[toScale])\n            val_X[toScale] = scale.transform(val_X[toScale])\n        \n        train_length = train_X.shape[0]\n        val_length = val_X.shape[0]\n        \n        new_X = pd.concat([train_X, val_X], axis=0)\n        new_y = pd.concat([train_y, val_y], axis=0)\n        \n        \n        # Want to find the best hyperparameters for this specific train/test split\n        # Create a list where train data indices are -1 and validation data indices are 0\n        # Source: https://stackoverflow.com/questions/31948879/using-explicit-predefined-validation-set-for-grid-search-with-sklearn\n        split_index = [-1 if i<=train_length else 0 for i in range(train_length+val_length)]\n        \n        ps = PredefinedSplit(test_fold = split_index)\n        \n        # Scoring metric is from the dataset description\n        # Tries out all permutations of parameters in paramField on this split\n        clf = GridSearchCV(model, paramfield, scoring=\"roc_auc\", cv=ps)\n        \n        # Fit the classifier using the preprocessed X and y\n        # It will know what the validation set is bc the PredefinedSplit\n        clf.fit(new_X, new_y)\n        \n        # Get the best score from this split coming from the best params\n        score = clf.best_score_\n        \n        # Add to the totalScore to later return the average score\n        totalScore += score\n        \n        # If this is the best split so far, keep track of the found hyperparameters\n        if score >= bestScore:\n            bestParams = clf.best_params_\n    \n    # Find the average score across the cv folds\n    avgScore = totalScore/kf.get_n_splits()\n    \n    return avgScore, bestParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training and Hyperparameter Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a fresh X and y\ndf = pd.read_csv(fname)\nX = df.drop([\"Response\", \"id\"], axis=1)\ny = df[\"Response\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\n\n# Not many hyperparameters with this one\nparamfield = {'var_smoothing': [10**-8, 10**-9, 10**-10]}\n\n# avgScore, bestParams = get_cv_score_and_best_model(X, y, model, paramfield, False)\n\n# print(avgScore)\n# print(bestParams)\n\n# Previous Output:\n# 0.8244416193893894\n# {'var_smoothing': 1e-09}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nparamfield = {'penalty': ('l1', 'l2', 'elasticnet', 'none'),\n              'C': [.011, .033, .11, .33, 1, 3, 9],\n              'max_iter': [50, 100, 150, 200, 300],\n              }\n\n# avgScore, bestParams = get_cv_score_and_best_model(X, y, model, paramfield, False)\n\n# print(avgScore)\n# print(bestParams)\n\n# Previous Output:\n# 0.8423410561182303\n# {'C': 0.11, 'max_iter': 50, 'penalty': 'l2'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\nparamfield = {'criterion': ('gini', 'entropy'),\n              'splitter': ('best', 'random'),\n              'max_depth':[50, 100, 200, None],\n              'max_leaf_nodes':[500, 1000, 2000, None],\n             }\n\n# avgScore, bestParams = get_cv_score_and_best_model(X, y, model, paramfield, False)\n\n# print(avgScore)\n# print(bestParams)\n\n# Previous Output:\n# 0.8499812468738526\n# {'criterion': 'gini', 'max_depth': None, 'max_leaf_nodes': 500, 'splitter': 'random'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: K Nearest Neighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier()\nparamfield = {'n_neighbors': [3,4,5,6,7],\n              'weights': ('uniform', 'distance'),\n             }\n\n# avgScore, bestParams = get_cv_score_and_best_model(X, y, model, paramfield, True)\n\n# print(avgScore)\n# print(bestParams)\n\n# Previous Output:\n# 0.7800292419908607\n# {'n_neighbors': 7, 'weights': 'uniform'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: XGBoost Classifier\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nparamfield = {'learning_rate':[.033, .1, .3, .9],\n              'n_estimators':[25, 50, 100, 150, 200],\n             }\n\n# avgScore, bestParams = get_cv_score_and_best_model(X, y, model, paramfield, False)\n\n# print(avgScore)\n# print(bestParams)\n\n# Previous Output:\n# 0.8569668696113993\n# {'learning_rate': 0.1, 'n_estimators': 100}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nparamfield = {'max_depth':[50, 100, None],\n              'max_leaf_nodes':[500, 1000, None],\n              'n_estimators': [50, 100, 200]\n             }\n\n# avgScore, bestParams = get_cv_score_and_best_model(X, y, model, paramfield, False)\n\n# print(avgScore)\n# print(bestParams)\n\n# Previous Output:\n# 0.8557109570795897\n# {'max_depth': 50, 'max_leaf_nodes': 500, 'n_estimators': 200}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Support Vector Classifier\nfrom sklearn.svm import SVC\n\n# Takes longer than 9 hours to fit. The notebook times out after 9 hours.\n\n# From the documentation:\n# The implementation is based on libsvm. \n# The fit time complexity is more than quadratic with the number of samples\n# which makes it hard to scale to dataset with more than a couple of 10000 samples.\n\n# I have over 380,000 samples, so of course it takes forever.\n\n# model = SVC()\n# paramfield = {'C': [.33, 1, 3]}\n\n# Use LinearSVC instead\n\nfrom sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\nparamfield = {'C': [.11, .33, 1, 3, 9],\n             'loss': ('epsilon_insensitive', 'squared_epsilon_insensitive')\n             }\n\n# avgScore, bestParams = get_cv_score_and_best_model(X, y, model, paramfield, True)\n\n# print(avgScore)\n# print(bestParams)\n\n# Previous Output:\n# 0.8416891938364014\n# {'C': 0.11, 'loss': 'squared_epsilon_insensitive'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results\n\nWhich model to choose? (based on ROC-AUC score)\n* **Naive Bayes**: 0.82\n* **Logistic Regression**: 0.84\n* **Decision Tree**: 0.85\n* **K Nearest Neighbors**: 0.78\n* **XGBoost**: *0.86*\n* **Random Forest**: *0.86*\n* **Linear Support Vector Machine**: 0.84\n\nI should go with Decision Tree because it's simpler and has a comparable score to XGBoost and RandomForest Classifier, but I'm going to go with RandomForest because it has a very slightly higher score and it's also a very explainable model (just a bunch of decision trees)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a model on the training data\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=.8, test_size=.2)\ntrain_X, val_X, train_y, val_y = get_preprocessed_cv_data(train_X, val_X, train_y, val_y)\n\n# From GridSearchCV: {'max_depth': 50, 'max_leaf_nodes': 500, 'n_estimators': 200}\nmodel = RandomForestClassifier(max_depth=50, max_leaf_nodes=500, n_estimators=200)\nmodel.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the auc-roc score\nfrom sklearn.metrics import roc_auc_score\n\n# Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\nprint(roc_auc_score(val_y, model.predict_proba(val_X)[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Feature Selection*\nWhy?\n- Prevents overfitting on training and validation data\n- Train and do inference faster with fewer features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use L1 Regularization to see the most important features\n# The less important coefficients will be regularized to 0\n# Make C lower to remove more features\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\nlogistic = LogisticRegression(C=.5, penalty=\"l1\", solver=\"liblinear\"\n                             ).fit(train_X, train_y)\n\n# Select the nonzero coefficients\nselector = SelectFromModel(logistic, prefit=True)\n\ntrain_X_new = selector.transform(train_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list of which features were selected\n\nselected_df = pd.DataFrame(selector.inverse_transform(train_X_new),\n                          index=np.arange(train_X_new.shape[0]),\n                          columns=train_X.columns.tolist())\n\nselected_features = selected_df.columns[selected_df.sum() != 0]\n\nselected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See which features should be removed according to this method\n\nfor col in train_X.columns.tolist():\n    if col not in selected_features:\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different aproach to see feature importance\n# Permutation importance\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model).fit(val_X, val_y)\neli5.show_weights(perm, top=100, feature_names = val_X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What strikes me as odd is all the very low weights. Did I do something wrong?\\\nIf I trust this, then it seems like most of the features I made are helpful.\\\nDriving_License has no effect because almost everybody in the dataset has a driving license.\\\nI will try to find out why Age is so important in the next section."},{"metadata":{},"cell_type":"markdown","source":"# *Machine Learning Explainability*\n#### Making sense of the model's predictions"},{"metadata":{},"cell_type":"markdown","source":"## Partial Dependence Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# See how exactly Age and Vintage affect predictions\n\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\nfeatures_to_plot = ['Age', 'Vintage']\ninter1  =  pdp.pdp_interact(model=model, dataset=val_X, model_features=val_X.columns.tolist(), features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Being in the middle of the bell curve of Ages seems to greatly increase the chances of buying Vehicle Insurance. Vintage (days with company) only has an effect at the extremes. Having been with the company for a long time or having been with the company for only a few days increases the odds of getting vehicle insurance. With regards to age, it makes a wider range of ages likely to buy vehicle insurance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# See how exactly Age and Annual Premiumn affect predictions\n\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\nfeatures_to_plot = ['Age', 'Annual_Premium']\ninter1  =  pdp.pdp_interact(model=model, dataset=val_X, model_features=val_X.columns.tolist(), features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a very similar story to the previous graph. As people pay more for their annual premium, they are slightly more likely to buy vehicle insurance. The pattern with Age is the same as the previous graph."},{"metadata":{},"cell_type":"markdown","source":"## SHAP Value Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way to see relationships between variables\n# SHAP values and SHAP summary plots\n\nimport shap\n\nexplainer = shap.TreeExplainer(model)\n\nshap_values = explainer.shap_values(val_X)\n\nshap.summary_plot(shap_values[1], val_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clear indicators that make a customer more likely to buy vehicle insurance:\n* Not currently being insured\n* Has had vehicle damage previously\n* Pays a high annual premium\n* Comes from a sales channel or region code that historically has been more likely to buy vehicle insurance\n* Comes from a sales channel or region code with a high volume of people\n\nNot-so-clear indicators:\n* Having a very new car (< 1 year) sometimes increases odds and sometimes decreases odds\n* High age sometimes increases odds and sometimes decreases odds\n* Being male (slighly increases odds of buying vehicle insurance)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Partial Dependence Plot, but enhanced with SHAP values\n# Check out Age and Previously_Insured\n\nshap.dependence_plot('Age', shap_values[1], val_X, interaction_index=\"Previously_Insured\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As found previously, the further someone is from the mean age, the less likely they are to buy vehicle insurance. The graph starts down, goes up, then back down. Not having vehicle insurance previously intensifies this effect greatly, basically increasing the 'amplitude'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Partial Dependence Plot, but enhanced with SHAP values\n# Check out Annual Premium and if the customer has had damage to their vehicle in the past\n\nshap.dependence_plot('Annual_Premium', shap_values[1], val_X, interaction_index=\"Vehicle_Damage_Yes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no clear pattern for Annual Premium here. The same values can lead to a similar result for predicting buying vehicle insurance. However, not having vehicle damage previously nullifies the effect of Annual Premium. Notice how the blue dots are all centered around 0.00 on the y-axis, while the pink ones are all over the place."},{"metadata":{},"cell_type":"markdown","source":"# *Test Data*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in the training data again\n# Get a fresh X and y\ndf = pd.read_csv(fname)\n\ntrain_id_col = df[\"id\"]\n\ntrain_X = df.drop([\"Response\", \"id\"], axis=1)\ntrain_y = df[\"Response\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in the test data\n\nfname_test = \"../input/health-insurance-cross-sell-prediction/test.csv\"\ntest_X = pd.read_csv(fname_test)\n\ntest_id_col = test_X[\"id\"]\n\ntest_X = test_X.drop([\"id\"], axis=1)\ntest_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a model on all of the training data\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, _ = get_preprocessed_cv_data(train_X, test_X, train_y, _)\n\n# From GridSearchCV: {'max_depth': 50, 'max_leaf_nodes': 500, 'n_estimators': 200}\nmodel = RandomForestClassifier(max_depth=50, max_leaf_nodes=500, n_estimators=200)\n\nmodel.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict on all the test data and create a submission CSV\n# Check the dataset page for specifics on formatting\n\ntest_preds = model.predict(test_X)\n\noutput = pd.DataFrame({'id': test_id_col,\n                      'Response': test_preds})\n\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This hackathon competition is no longer accepting submissions. :("}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}