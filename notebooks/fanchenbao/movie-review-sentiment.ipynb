{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Default values\nSIZE_VOCAB = 20000  # size of the vocabulary\nEMBED_DIM = 128  # Dimension of each word after embedding\nMAX_SEQ_LEN = 200  # Max length of words in each paragraph that are used in the model.\nEPOCH = 1\nBATCH_SIZE=32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shared Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seq_pad(raw_texts, tokenizer, max_seq_len=MAX_SEQ_LEN):\n    \"\"\"Turn raw texts into sequence and then pad it.\n    \n    A tokenizer must be initialized and fitted to a given text before being passed to\n    this function. This function first turns raw texts to sequence according to a\n    provided tokenizer. Then we pad each sequence to max_seq_len with zero at the end.\n    The reason we do post-padding is that we can use CuDNN implementation of the RNN\n    layer, according to https://keras.io/guides/understanding_masking_and_padding/#padding-sequence-data\n    \n    :param raw_texts: A list of raw texts, e.g. ['blablab bla', 'foo fooobar', ...]\n    :param tokenizer: A Tokenizer instance that has run fit_on_texts() already on a sample text.\n    :param max_seq_len: Max length of a sequence (i.e. number of words in a text string). Default\n        to MAX_SEQ_LEN.\n    \"\"\"\n    seq = tokenizer.texts_to_sequences(raw_texts)\n    pad_seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_seq_len, padding='post')\n    return pad_seq\n\n\ndef predict(reviews, model):\n    \"\"\"Predict the sentiment of a list of reviews.\n    \n    The model predicts the likelihood that a review is positive. We decide that when the likelihood\n    is above 0.5, it is a positive review.\n    \n    :param reviews: A list of strings, each string is a movie review.\n    :param model: The trained model.\n    \"\"\"\n    pred = model.predict(seq_pad(reviews, tokenizer))\n    pred_senti = ['positive' if p >= 0.5 else 'negative' for p in pred]\n\n    for tr, s in zip(reviews, pred_senti):\n        print(tr, '-->', s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess The Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change the label \nlabels = np.array([1 if s == 'positive' else 0 for s in df.sentiment])\nreviews = df.review.to_numpy()\n\nX_train, X_val, y_train, y_val = train_test_split(\n    reviews,\n    labels,\n    test_size=0.5,\n    stratify=labels,\n    random_state=42,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize the reviews.\n# NOTE: this cells takes a while to run. Do not run often.\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=SIZE_VOCAB)\ntokenizer.fit_on_texts(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential()\n# NOTE: if mask_zero set to true, we will encounter this error:\n# CancelledError:  [_Derived_]RecvAsync is cancelled.\n# This is apparently an on-going issue with Keras, refer to these two\n# issues: https://github.com/tensorflow/tensorflow/issues/33721\n# and https://github.com/tensorflow/tensorflow/issues/45594\n# Note also that downgrading tensorflow to 1.14 is suggesteed as a fix\n# to the problem. While I have been able to downgrade tensorflow to 1.14\n# in Kaggle, I was not able to enable GPU with 1.14 even after installing\n# tensorflow-gpu. Hence this supposedly fix does not work for me.\nmodel.add(\n    layers.Embedding(\n        input_dim=SIZE_VOCAB,\n        output_dim=EMBED_DIM,\n#         mask_zero=True,\n        input_length=MAX_SEQ_LEN,\n    ),\n)\n# Use bidirectional LSTM. According to:\n# https://keras.io/examples/nlp/bidirectional_lstm_imdb/\nmodel.add(\n    layers.Bidirectional(layers.LSTM(64, return_sequences=True))\n)\nmodel.add(layers.Bidirectional(layers.LSTM(32)))\nmodel.add(layers.Dense(1))  # classification is binary\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compile and Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n# Must turn on GPU to speed things up. Without GPU, each epoch takes about 25 min to train.\n# With GPU, each epoch takes about 1 min to train. HUGE difference.\nmodel.fit(\n    x=seq_pad(X_train, tokenizer),\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCH,\n#     callbacks=[\n#         keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, min_delta=0.01)\n#     ],\n    validation_data=(seq_pad(X_val, tokenizer), y_val),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_reviews = [\n    'this movie sucks',\n    'this movie is great',\n    'The movie is a flop',\n    'I recommend it to my friends',\n    'I will ask my friends to watch it if I want them to suffer',\n    'This movie is so bad that words cannot express how terrible it is.',\n    'I love it',\n    'Like it',\n    'Adore it',\n    'Amazing',\n    'Terrible',\n    'I hate it',\n    'Disgusting',\n    'This is obviously a good movie that everyone should see. It captures the essense of a happy life and presents it with great artistic touch. The director uses great and beautiful shots to convey a sense of emptiness when a person is in a state of total tranquility.',\n    'I will not recommend this movie because the shots are simply too shaky. It is okay to use shaky cam if it serves a purpose, such as in the Bourne movie, the first one to be specific. However, when shaky cam is used for the sake of shaky cam because it seems like the audience like it, then it is totally pointless.',\n    'I love it. I actually love the movie. I think I love it. I have no reason not to love it. Loving the movie is what I am feeling right now',\n    'I hate it. I actually hate the movie. I think I hate it. I have no reason not to hate it. Hating the movie is what I am feeling right now',\n    'I love it. I love it. I love it. I love it. I love it. I love it. I love it. I love it. I love it. I love it. I love it. I love it. I love it.',\n    'I hate it. I hate it. I hate it. I hate it. I hate it. I hate it. I hate it. I hate it. I hate it. I hate it. I hate it. I hate it. I hate it.',\n]\npredict(test_reviews, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Average Length of Reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.DataFrame({'review': X_train, 'sentiment': y_train})\n\nprint(\n    'Average positive review length',\n    np.mean([len(r.split(' ')) for r in train_df[train_df.sentiment==1].review]),\n)\nprint(\n    'Average negative review length',\n    np.mean([len(r.split(' ')) for r in train_df[train_df.sentiment==0].review]),\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Poke The Model More"},{"metadata":{"trusted":true},"cell_type":"code","source":"poking_reviews = [\n    'This movie is so good that words cannot express how amazing it is.',\n    'This movie is so good that words cannot express how amazing it is. To begin with, it has good characters development.',\n    'This movie is so good that words cannot express how amazing it is. To begin with, it has good characters development. Each character has a full arc and their decisions are in-line with their personalities.',\n    'This movie is so good that words cannot express how amazing it is. To begin with, it has good characters development. Each character has a full arc and their decisions are in-line with their personalities. Second, the acting is on-point.',\n    'This movie is so good that words cannot express how amazing it is. To begin with, it has good characters development. Each character has a full arc and their decisions are in-line with their personalities. Second, the acting is on-point. Everyone, from the main characters to the supporting cast, does a great job such that you do not even realize that they are acting',\n    'This movie is so good that words cannot express how amazing it is. To begin with, it has good characters development. Each character has a full arc and their decisions are in-line with their personalities. Second, the acting is on-point. Third, the screenplay is good',\n    'This movie is so good that words cannot express how amazing it is. To begin with, it has good characters development. Each character has a full arc and their decisions are in-line with their personalities. Second, the acting is on-point. Third, the screenplay is very good',\n    'This movie is so good that words cannot express how amazing it is. To begin with, it has good characters development. Each character has a full arc and their decisions are in-line with their personalities. Second, the acting is good',\n]\npredict(poking_reviews, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}