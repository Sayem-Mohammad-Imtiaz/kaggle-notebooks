{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"05b2d269-c7ac-24a3-5e86-c098dfc64782"},"source":"# Glass type classification with machine learning"},{"cell_type":"markdown","metadata":{"_cell_guid":"4eb01345-ed8a-3bfb-5de4-8e0cbc1c9f43"},"source":"This is my first Kaggle notebook. Here's my plan of attack for the glass classification problem.\n\n# Contents\n\n## 1) Prepare Problem\n\n * Load libraries\n\n * Load and explore the shape of the dataset\n\n## 2) Summarize Data\n\n* Descriptive statistics\n\n* Data visualization\n\n## 3) Prepare Data\n\n* Data Cleaning\n\n* Split-out validation dataset\n\n*  Data transformation  \n\n## 4) Evaluate Algorithms\n\n* Assessing feature importance via XGBoost and PCA\n\n* Compare Algorithms\n\n## 5) Improve Accuracy\n\n* Algorithm Tuning\n\n## 6) Finalize Model\n\n* Create standalone model on entire training dataset\n\n* Predictions on test dataset"},{"cell_type":"markdown","metadata":{"_cell_guid":"ae75a7ff-7be3-301b-fc17-c0feaf2a2690"},"source":"## 1. Prepare Problem"},{"cell_type":"markdown","metadata":{"_cell_guid":"219b832e-5873-b374-3b13-e9024940cc2e"},"source":"### Loading the libraries "},{"cell_type":"markdown","metadata":{"_cell_guid":"6c7ec43b-0101-fb94-0e45-0a1a15f48782"},"source":"Let us first begin by loading the libraries that we'll use in the notebook"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5108bded-bfae-ea87-1fc2-f87051462b79"},"outputs":[],"source":"import numpy as np  # linear algebra\nimport pandas as pd  # read dataframes\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # statistical visualizations and aesthetics\nfrom sklearn.preprocessing import StandardScaler # preprocessing \nfrom sklearn.decomposition import PCA # dimensionality reduction\nfrom scipy.stats import boxcox # data transform\nfrom sklearn.model_selection import (train_test_split, KFold , cross_val_score, GridSearchCV ) # model selection modules\nfrom sklearn.pipeline import Pipeline # streaming pipelines\n# load models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import (XGBClassifier, plot_importance)\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n%matplotlib inline "},{"cell_type":"markdown","metadata":{"_cell_guid":"ace42ae9-4dcb-4246-df8b-61d272b774e2"},"source":"### Loading and exploring the shape of the dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78b7ead5-7457-2862-d531-e8a714f659d5"},"outputs":[],"source":"df = pd.read_csv('../input/glass.csv')\n\nprint(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c26f2bcd-7712-dc4a-4aff-84b927494bee"},"source":"The dataset consists of 214 observations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2efc020c-5616-328f-a9e4-1a146caa4aa3"},"outputs":[],"source":"df.head(15)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55802dc9-e552-4b6b-b697-d6aaaba3e4b9"},"outputs":[],"source":"df.dtypes"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ac1e4a7-60a8-3641-57f9-98ffe8938907"},"source":"## 2. Summarize data"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e5400e7-da03-71ce-1ecb-e7c914251522"},"source":"### Descriptive statistics"},{"cell_type":"markdown","metadata":{"_cell_guid":"646eac5f-e512-0a07-de93-c9fcccb41082"},"source":"Let's first summarize the distribution of the numerical variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b27e57c-0779-c34d-777f-b2b067edcfcf"},"outputs":[],"source":"df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa036d73-7f87-697f-7c96-1dc1f58befb7"},"source":"The features are not on the same scale. For example Si has a mean of 72.65 while Fe has a mean value of 0.057. Features should be on the same scale for an algorithm such as logistic regression (gradient descent) to converge fast. Let's go ahead and check the distribution of the glass types."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"515037ee-2c4d-4d40-ca6d-aca001cfc8f9"},"outputs":[],"source":"df['Type'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f90a223e-3d2d-197d-6dc9-7b347489d87a"},"source":"The dataset is pretty unbalanced. The instances of types 1 and 2 constitute more than 67 % of the glass types."},{"cell_type":"markdown","metadata":{"_cell_guid":"9d171ec0-f8e1-ec93-f298-4c4c1233d3fc"},"source":"###  Data Visualization"},{"cell_type":"markdown","metadata":{"_cell_guid":"69e7d5f0-eabe-0233-5a9b-88edc025e3d4"},"source":"* **Univariate plots**"},{"cell_type":"markdown","metadata":{"_cell_guid":"e5dcff8b-73f9-7800-a8a2-7582674ef980"},"source":"Let's go ahead an look at the distribution of the different features of this dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7713d993-98e5-73ae-93c2-205625882995"},"outputs":[],"source":"features = df.columns[:-1].tolist()\nfor feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], label='Skew = %.3f' %(skew))\n    plt.legend(loc='best')\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fd5a38f8-d1d7-fc87-2c54-67638dd5bc2a"},"source":"None of the features is normally distributed. The features Fe, Ba, Ca and K exhibit the highest skew coefficients. Let's do a boxplot of the several distributions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f41504ec-e040-7e5c-4fb4-4ffbcf28c3b7"},"outputs":[],"source":"sns.boxplot(df[features])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3aa1de24-14d5-4d06-f6f6-cfca73071592"},"source":"Unsurprisingly, Silicon has a mean that is much superior to the other constituents as we already saw in the previous section. Well, that is normal since glass is mainly based on silica."},{"cell_type":"markdown","metadata":{"_cell_guid":"9a802551-24cc-1d1d-9451-47f57a81947e"},"source":"* **Multivariate plots**"},{"cell_type":"markdown","metadata":{"_cell_guid":"193f3d80-ab7c-bd10-2e01-1694a4d5b3a3"},"source":"Let's now do a pairplot to visually examine the correlation between the features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de8796c8-76b9-581f-1d1e-9e8a8ce8646a"},"outputs":[],"source":"plt.figure(figsize=(8,8))\nsns.pairplot(df[features],palette='coolwarm')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"efd5c18c-c764-5914-5cb8-b42ca725c80e"},"source":"Let's go ahead and examine a heatmap of the correlations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6356d9c-0287-b7d4-a59b-2c510633a9ed"},"outputs":[],"source":"corr = df[features].corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(corr, cbar = True,  square = True, annot=True,\n           xticklabels= df.columns.tolist(), yticklabels= df.columns.tolist(),\n           cmap= 'coolwarm')\nplt.show()\nprint(corr)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e380fa58-3e3e-5bda-0833-9309d0da6520"},"source":"There seems to be a strong positive correlation between RI and Ba; also a strong positive correlation between Ba and Na is noticeable. This could give us a hint about performing Principal component analysis to decorrelate some of the input features."},{"cell_type":"markdown","metadata":{"_cell_guid":"d6c716ee-31a3-00f1-fd33-1d4562024318"},"source":"## 3. Prepare data"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c0820df-fe56-d78b-f21d-658608d6810a"},"source":"### - Data cleaning "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0b463a3-9997-212a-277c-1383a3aaa6e1"},"outputs":[],"source":"df.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"72563242-d055-8451-f33d-89fe57d64df0"},"source":"This dataset is clean; there aren't any missing values in it."},{"cell_type":"markdown","metadata":{"_cell_guid":"9ba8bc01-836e-4abb-7d30-1ad38ebaa561"},"source":"### - Split-out validation dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1633837b-e058-8a40-22db-22a79d870245"},"outputs":[],"source":"# Define X as features and y as lablels\nX = df[features]\ny = df['Type']\n# set a seed and a test size for splitting the dataset \nseed = 7\ntest_size = 0.2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size , random_state = seed)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f13a10d9-e30a-4940-cf2d-276c2f40274d"},"source":"### - Data transformation  "},{"cell_type":"markdown","metadata":{"_cell_guid":"7d9cd04d-a413-0a90-86a7-4219be1e5b7f"},"source":"Let's examine if a Box-Cox transform can contribute to the normalization of some features. It should be emphasized that all transformations should only be done on the training set to avoid data snooping. Otherwise the test error estimation will be biased."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0eb56c0-ed10-415f-4da4-8c9017bacdd1"},"outputs":[],"source":"features_boxcox = []\n\nfor feature in features:\n    bc_transformed, _ = boxcox(X_train[feature]+1)  # shift by 1 to avoid computing log of negative values\n    features_boxcox.append(bc_transformed)\n\nfeatures_boxcox = np.column_stack(features_boxcox)\ndf_bc = pd.DataFrame(data=features_boxcox, columns=features)\ndf_bc['Type'] = df['Type']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61be6a71-8da4-0d4b-a776-a7d9bd058124"},"outputs":[],"source":"df_bc.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d6583ae-a6e3-bba8-1b07-d61d55b5d5a3"},"outputs":[],"source":"for feature in features:\n    fig, ax = plt.subplots(1,2,figsize=(7,3.5))    \n    ax[0].hist(df[feature], color='blue', bins=30, alpha=0.3, label='Skew = %s' %(str(round(X_train[feature].skew(),3))) )\n    ax[0].set_title(str(feature))   \n    ax[0].legend(loc=0)\n    ax[1].hist(df_bc[feature], color='red', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df_bc[feature].skew(),3))) )\n    ax[1].set_title(str(feature)+' after a Box-Cox transformation')\n    ax[1].legend(loc=0)\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58dfa4e3-f842-66ce-a98e-8c218060a935"},"outputs":[],"source":"# check if skew is closer to zero after a box-cox transform\nfor feature in features:\n    delta = np.abs( df_bc[feature].skew() / df[feature].skew() )\n    if delta < 1.0 :\n        print('Feature %s is less skewed after a Box-Cox transform' %(feature))\n    else:\n        print('Feature %s is more skewed after a Box-Cox transform'  %(feature))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6b91d19e-75c6-1853-4561-0edebfa9ead4"},"source":"The Box-Cox transform seems to do a good job in reducing the skews of the different distributions of features. Next, we will use the transformed features to feed them into out machine learning models. Only the distribution of Si will not be transformed since such transformation leads to very high values without a big improvement in skewness."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4aa4e74-6aa4-fa51-a09f-53ba17a696a3"},"outputs":[],"source":"df_bc[\"Si\"] = df[\"Si\"]\n\nfor feature in features:\n    if feature not in [\"Si\"]:\n        X_train[feature], lmbda = boxcox(X_train[feature]+1)  # shift by 1 to avoid computing log of negative values\n        X_test[feature] = X_test[feature].apply(lambda x: ((x+1.0)**lmbda - 1.0)/lmbda if lmbda !=0 else np.log(x+1) )\n\n\nX_train, X_test = X_train.values, X_test.values\ny_train, y_test = y_train.values, y_test.values"},{"cell_type":"markdown","metadata":{"_cell_guid":"b20cd449-6df3-9014-d426-164cfc0eda8d"},"source":"### - Standarizing the dataset"},{"cell_type":"markdown","metadata":{"_cell_guid":"0ec2805a-9527-010f-e844-2bc3596d7357"},"source":"Now we have to standarize the different features to bring them to the same scale."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"833d6d11-e167-b9ca-5d9a-c16bbf8557e3"},"outputs":[],"source":"# Standarize the dataset \nfor i in range(X.shape[1]):\n    sc = StandardScaler()\n    X_train[:,i] = sc.fit_transform(X_train[:,i].reshape(-1,1)).reshape(1,-1)\n    X_test[:,i] = sc.transform(X_test[:,i].reshape(-1,1)).reshape(1,-1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ecd4339-f5b3-b5e3-2b65-78c3d35b0dee"},"source":"## 4. Evaluate Algorithms"},{"cell_type":"markdown","metadata":{"_cell_guid":"0765edc5-5eaa-a03f-5dd8-5f9471a0c3c7"},"source":"### - Assessing feature importance via XGBoost and PCA"},{"cell_type":"markdown","metadata":{"_cell_guid":"99b123a3-2068-3639-7547-8326fcbd5d5c"},"source":"* **XGBoost**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24bad968-7341-4fea-0d3e-b8f851fb7f21"},"outputs":[],"source":"model_importances = XGBClassifier(n_estimators=200)\nmodel_importances.fit(X_train, y_train)\nplot_importance(model_importances)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b3f0438c-534e-9271-d8eb-fa0e21a6d883"},"source":"* **PCA**"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd92f517-7e97-ac27-737d-3149299db9dc"},"source":"Let's go ahead and perform a PCA on the features to decorrelate the ones that are linearly dependent and then let's plot the cumulative explained variance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edc50398-249b-cd5e-b743-71384cae5e40"},"outputs":[],"source":"pca = PCA(random_state = seed)\npca.fit(X_train)\nvar_exp = pca.explained_variance_ratio_\ncum_var_exp = np.cumsum(var_exp)\nplt.bar(range(1,len(cum_var_exp)+1), var_exp, align= 'center', label= 'individual variance explained', \\\n       alpha = 0.7)\nplt.step(range(1,len(cum_var_exp)+1), cum_var_exp, where = 'mid' , label= 'cumulative variance explained', \\\n        color= 'red')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.xticks(np.arange(1,len(var_exp)+1,1))\nplt.legend(loc='best')\nplt.show()\n\n# Cumulative variance explained\nfor i, sum in enumerate(cum_var_exp):\n    print(\"PC\" + str(i+1), \"Cumulative variance: %.3f% %\" %(cum_var_exp[i]*100))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f1c2d6a-96f0-0670-b0d6-9aaf14ad59c7"},"source":"It appears that about 96 % of the variance can be explained with the first 6 principal components. PCA seems a better choice to reduce the dimensionality of the dataset than selecting the most important features via XGBoost."},{"cell_type":"markdown","metadata":{"_cell_guid":"5daf9b7e-abac-6a02-8bfb-972dcf70ca29"},"source":"### - Compare Algorithms"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f8f3969-57b0-fa78-1ea2-2e3677d64eba"},"source":"Now it's time to compare 4 different algorithms (XGBoost Classifier, Support Vector Classifier, RandomForest Classifier and KNeighbors Classifier) after reducing the dimensionality of the data to 6. We'll use 10-folds cross-validation to assess the performance of each model with the metric being the classification accuracy."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b22729c-2fc9-ef2a-7d1f-d069edec987d"},"outputs":[],"source":"pca = PCA(n_components = 6, random_state= seed)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nmodels = []\nmodels.append(('XGBoost', XGBClassifier(seed = seed) ))\nmodels.append(('SVC', SVC(random_state=seed)))\nmodels.append(('RF', RandomForestClassifier(random_state=seed, n_jobs=-1 )))\ntree = DecisionTreeClassifier(max_depth=4, random_state=seed)\nmodels.append(('KNN', KNeighborsClassifier(n_jobs=-1)))\n\nresults, names  = [], []\nnum_folds = 10\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train_pca, y_train, cv=kfold, scoring = scoring, n_jobs= -1) \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \nfig = plt.figure(figsize=(8,6))    \nfig.suptitle(\"Algorithms comparison\")\nax = fig.add_subplot(1,1,1)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a44e7822-1a6b-1aeb-919c-e26e7ed24cda"},"source":"\n**Observation:** It appears that the XGBoost Classifier (XGBClassifer), the Support Vector Classifier (SVC) and the KNeigbors Classifier yield the highest scores. However, these algorithms also yield a wide distribution (10% to 13%). It is worthy to continue our study by tuning these two algorithms."},{"cell_type":"markdown","metadata":{"_cell_guid":"5af8c34c-cefb-734e-2b6a-e13ab043a93c"},"source":"## 5. Algorithm tuning"},{"cell_type":"markdown","metadata":{"_cell_guid":"c30058df-e5f6-c3aa-c700-441550aba44b"},"source":"Let's start by tuning the hyperparameters of the XGBoost Classifier.\n\nto be continued ..."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}