{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install yfinance\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport datetime\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport torch\nimport re\nimport yfinance as yf\nimport holidays\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\ndef SentimentAnalyzer(doc):\n    pt_batch = tokenizer(doc,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")\n    outputs = model(**pt_batch)\n    pt_predictions = F.softmax(outputs.logits, dim=-1)\n    return pt_predictions.detach().cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = pd.read_csv('/kaggle/input/massive-stock-news-analysis-db-for-nlpbacktests/analyst_ratings_processed.csv', index_col=0)\ndata1.dropna(inplace = True)\ndata1.rename(columns={'stock':'ticker'}, inplace=True)\ndata1['date'] = data1['date'].apply(lambda x : x.split()[0])\ndata2 = pd.read_csv('/kaggle/input/us-equities-news-data/us_equities_news_dataset.csv', index_col=0)\ndata2.dropna(inplace = True)\ndata2.reset_index(drop=True, inplace=True)\ndata2.rename(columns={'release_date':'date'}, inplace=True)\ndata2.drop(inplace=True, columns=['category', 'content', 'provider', 'url', 'article_id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data1, data2])\ndata.drop_duplicates(subset='title', keep='first', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tickerSymbol = \"MSFT\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmpData = {}\ntotal = data['date'].nunique()\nfor i in tqdm(data[data['ticker']==tickerSymbol]['date'].unique()):\n    tmpData[i] = data.loc[(data['ticker']==tickerSymbol) & (data['date'] == i)]['title'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ONE_DAY = datetime.timedelta(days=1)\nHOLIDAYS_US = holidays.US()\ndef next_business_day(dateString):\n    datetimeObj = datetime.datetime.strptime(dateString, '%Y-%m-%d')\n    next_day = datetimeObj + ONE_DAY\n    while next_day.weekday() in holidays.WEEKEND or next_day in HOLIDAYS_US:\n        next_day += ONE_DAY\n    return next_day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def findPercentageBySentences(sentenceList):\n    posAvg, negAvg, neuAvg = 0, 0, 0\n    sentimentArr = SentimentAnalyzer(sentenceList)\n    sentimentArr = np.mean(sentimentArr, axis=0)\n    posAvg=sentimentArr[0]\n    negAvg=sentimentArr[1]\n    neuAvg=sentimentArr[2]\n    return {'numArticles': len(sentenceList), 'pos': posAvg, 'neg': negAvg, 'neu' : neuAvg}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dateSentimentGroups = {}\nfor i in tqdm(tmpData):\n    scores = findPercentageBySentences(tmpData[i])\n    dateSentimentGroups[i] = scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nticker = yf.Ticker(tickerSymbol)\nhist = ticker.history(period=\"max\")\nfor i in tqdm(dateSentimentGroups):\n  start = i\n  nextDay = next_business_day(start).strftime(\"%Y-%m-%d\")\n  try:\n    prevDay = hist.loc[start]\n    nextDay = hist.loc[nextDay]\n#     data.append([i, dateSentimentGroups[i]['numArticles'], dateSentimentGroups[i]['neu'], dateSentimentGroups[i]['pos'], dateSentimentGroups[i]['neg'], percentageChange])\n    data.append([dateSentimentGroups[i]['numArticles'], dateSentimentGroups[i]['neu'], dateSentimentGroups[i]['pos'], dateSentimentGroups[i]['neg'], prevDay['Open'], prevDay['Close']])\n  except:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.DataFrame(columns =['date', 'numArticles', 'neutral', 'positive','negative','percentageChange'], data=data)\ndf = pd.DataFrame(columns =['numArticles', 'neutral', 'positive','negative','Open', 'Close'], data=data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df[['neutral', 'positive', 'negative', 'Open']]\ny = df['Close']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normalizer = preprocessing.Normalization()\nnormalizer.adapt(np.array(X))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_and_compile_model(norm):\n    model = Sequential()\n    model.add(norm)\n    model.add(Dense(256, activation=lambda x : tf.nn.leaky_relu(x, alpha=0.01),  input_dim=3))\n    model.add(Dense(256, activation=lambda x : tf.nn.leaky_relu(x, alpha=0.01)))\n    model.add(Dense(1, activation=\"linear\"))\n    model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-3, decay=1e-3 / 200))\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_model = build_and_compile_model(normalizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\nhistory = dnn_model.fit(X_train, Y_train, validation_split=0.2, epochs=1000, batch_size=100, verbose=1, callbacks=[es])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss(history):\n  plt.plot(history.history['loss'], label='loss')\n  plt.plot(history.history['val_loss'], label='val_loss')\n  plt.ylim([0, 10])\n  plt.xlabel('Epoch')\n  plt.ylabel('Error [Closing Price]')\n  plt.legend()\n  plt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = dnn_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_model.evaluate(X_test, Y_test, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\nX = [i for i in range(len(preds))]\n  \n# Assign variables to the y axis part of the curve\ny = preds\nz = Y_test\n  \n# Plotting both the curves simultaneously\nplt.scatter(X, y, color='r', label='Predicted Close')\nplt.scatter(X, z, color='g', label='Actual Close')\n  \n# Naming the x-axis, y-axis and the whole graph\nplt.ylabel(\"Closing Price\")\nplt.title(\"Predicted vs Actual Close\")\n  \n# Adding legend, which helps us recognize the curve according to it's color\nplt.legend()\n  \n# To load the display window\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_model.save(\"finbert_msft.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}