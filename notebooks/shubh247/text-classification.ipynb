{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"messages = pd.read_csv('../input/spam.csv', delimiter = ',', encoding='latin-1')\nmessages.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the column that are not required.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\nmessages.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(messages.v1)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis(EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.groupby('v1').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we have to start thinking about the number of features or we call as feature\nengineering, better than knowledge abaout the data, better we will have the\nability to feature the data. "},{"metadata":{},"cell_type":"markdown","source":"add one more colummn related to the length of the messages. "},{"metadata":{"trusted":true},"cell_type":"code","source":"messages['length'] = messages['v2'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"messages['length'].plot.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages['length'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like highest length size of the message is 910 characters. "},{"metadata":{},"cell_type":"markdown","source":"lets try to explore is the length of the message is the distinguish feature between spam or ham messages. "},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.hist(column='length', by='v1', bins=60, figsize=(12,4));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"spam messages has more number of characters as compared to Ham Messages. "},{"metadata":{},"cell_type":"markdown","source":"# Remove Punctuation & Stopwords "},{"metadata":{},"cell_type":"markdown","source":"lets do some Text Pre-Processing. if we need to do some classification algorithm\nthen we need some numerical vector values. in order to convert word into vector we will use Bag of words. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords.words(\"english\")[100:110]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mess = 'Sample Message! Notice: it has punctuation.'\nnopunc = [c for c in mess if c not in string.punctuation]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nopunc = \"\".join(nopunc)\nnopunc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nopunc.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_mess","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets apply the same to all messages in the data frame\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_process(mess):\n    '''\n    1.remove punctuaton\n    2.remove stop words\n    3. return list of clean text words\n    '''\n    nopunc = [char for char in mess if char not in string.punctuation]\n    nopunc=\"\".join(nopunc)\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages = messages.rename(columns={\"v1\": \"label\", \"v2\": \"message\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages['spam'] = messages['label'].map({'spam': 1, 'ham': 0}).astype(int)\nmessages.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# apply Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"messages['message'].head(5).apply(text_process)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Currently all the messages are as list of tokens and now we need to covert each of those messages into a vector the Scikit Learn's models can work with.\nwe'll do that in three steps using bag-of-words model:\n\n**1. Count how many times does a word occur in each message (known as term frequency).**\n\n**2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency).**\n\n**3. Normalize the vector to unit length, to abstract from the original text length (L2 norm).**\n\n* Let's begin with the first step: *\n\n** Each vector will have as many dimensions as there are unique words in the SMS corpus. We will use Scikit's Learn CountVectorizer. This model will convert a collection of text documents to a matrix of token counts.**\n\n** we can imagine this as a 2-Dimensional matrix. where the 1-dimension is the entire vocabulary(1 row per word) and the other dimension are the actual documents, in this case a column per text message.**\n\nFor example:\n\n**|            |Message 1|Message 2|...|Message N|**\n**|Word 1 Count|0        |1        |...|0        |**\n**|Word 2 Count|0        |1        |...|0        |**\n**|...         |         |...      |...|...      |**\n**|Word N Count|0        |1        |...|1        |**\n\nSince there are so many messages, we can expect a lot of zero counts for the presence of that word in that document.\nBeacuse of this, scikit learn will output a  Sparse Matrix (https://en.wikipedia.org/wiki/Sparse_matrix)"},{"metadata":{},"cell_type":"markdown","source":"# CountVectorizer(bag-of-word)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(bow_transformer.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grab the 4th message\n\nmess4 = messages['message'][3]\nprint(mess4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow4 = bow_transformer.transform([mess4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bow4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bow4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_transformer.get_feature_names()[3996]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_transformer.get_feature_names()[9445]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply to whole dataframe\nmessages_bow = bow_transformer.transform(messages['message'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Sparse Matrix', messages_bow.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages_bow.nnz  # non zero occurance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorizing Data: TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"** Weight and Normalization is done using TF-IDF **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_transformer = TfidfTransformer().fit(messages_bow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf4 = tfidf_transformer.transform(bow4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf4) # weight values for each of the word ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages_tfidf = tfidf_transformer.transform(messages_bow)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**message are finaly converted into vector**"},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier().fit(messages_tfidf, messages['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(tfidf4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages['label'][3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pred = model.predict(messages_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**proper way to do is train-test-split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'], test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pipeline helps to save the complete workflow**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),\n    ('tfidf', TfidfTransformer()),\n    ('classifier', RandomForestClassifier())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(msg_train, label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pipeline.predict(msg_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(label_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}