{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TelCo Churn Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"TelCo is a company that offers telecommunications services (e.g. telephone, internet, television) to consumers in exchange for a subscription fee. They have provided us with a dataset concerning their customers, and they would like to know which customers are likely to **churn** (i.e. cancel their subscription) and which attributes affect a customer's likelihood to churn. If TelCo can predict which customers are going to cancel their subscription, measures can be taken to persuade the customers to stay. They could send promotional emails, offer a discount, or provide extra services. After all, retaining a customer is cheaper than attaining a new one. \n\nTo attain our goal, we will first explore the dataset using Exploratory Data Analysis, after which we can build a model that can predict which customers will churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['TotalCharges'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we will create a barchart for all categorical features that show the distribution per class (non-churn or churn)."},{"metadata":{"trusted":true},"cell_type":"code","source":"axis_y = \"percentage of customers\"\n\n#Grouped by partner\ngp_partner = df.groupby('Partner')[\"Churn\"].value_counts()/len(df)\ngp_partner = gp_partner.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped on dependents\ngp_dep = df.groupby('Dependents')[\"Churn\"].value_counts()/len(df)\ngp_dep = gp_dep.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped multiple lines per customer by churn rate\ngp_mpl = df.groupby('MultipleLines')[\"Churn\"].value_counts()/len(df)\ngp_mpl = gp_mpl.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped internet services per customer by churn rate\ngp_is = df.groupby('InternetService')[\"Churn\"].value_counts()/len(df)\ngp_is = gp_is.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped online backup per customer by churn rate\ngp_ob = df.groupby('OnlineBackup')[\"Churn\"].value_counts()/len(df)\ngp_ob = gp_ob.to_frame().rename({\"Churn\":axis_y}, axis=1).reset_index()\n\n#Grouped device protection per customer by churn rate\ngp_dp = df.groupby('DeviceProtection')[\"Churn\"].value_counts()/len(df)\ngp_dp = gp_dp.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped tech support per customer by churn rate\ngp_ts = df.groupby('TechSupport')[\"Churn\"].value_counts()/len(df)\ngp_ts = gp_ts.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped streaming TV per customer by churn rate\ngp_st = df.groupby('StreamingTV')[\"Churn\"].value_counts()/len(df)\ngp_st = gp_st.to_frame().rename({\"Churn\":axis_y}, axis=1).reset_index()        \n                                    \n#Grouped streaming movies per customer by churn rate\ngp_sm = df.groupby('StreamingMovies')[\"Churn\"].value_counts()/len(df)\ngp_sm = gp_sm.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped contract per customer by churn rate\ngp_con = df.groupby('Contract')[\"Churn\"].value_counts()/len(df)\ngp_con = gp_con.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\n#Grouped paperless billing per customer by churn rate\ngp_pb = df.groupby('PaperlessBilling')[\"Churn\"].value_counts()/len(df)\ngp_pb = gp_pb.to_frame().rename({\"Churn\":axis_y}, axis=1).reset_index()   \n\n#Grouped payment method per customer by churn rate\ngp_pm = df.groupby('PaymentMethod')[\"Churn\"].value_counts()/len(df)\ngp_pm = gp_pm.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(4,3, figsize=(25,25))\naxis[0,0].set_title(\"Has partner\")\naxis[0,1].set_title(\"Has dependents\")\naxis[0,2].set_title(\"Has multiple lines\")\naxis[1,0].set_title(\"Type of internet services\")\naxis[1,1].set_title(\"Has online backup\")\naxis[1,2].set_title(\"Has device protection\")\naxis[2,0].set_title(\"Has tech support\")\naxis[2,1].set_title(\"Has streaming TV\")\naxis[2,2].set_title(\"Has streaming movies\")\naxis[3,0].set_title(\"Type of contract\")\naxis[3,1].set_title(\"Has paperless billing\")\naxis[3,2].set_title(\"Type of payment method\")\n\nax = sns.barplot(x='Partner', y=axis_y, hue='Churn', data=gp_partner, ax=axis[0,0])\nax = sns.barplot(x='Dependents', y=axis_y, hue='Churn', data=gp_dep, ax=axis[0,1])\nax = sns.barplot(x='MultipleLines', y=axis_y, hue='Churn', data=gp_mpl, ax=axis[0,2])\nax = sns.barplot(x='InternetService', y=axis_y, hue='Churn', data=gp_is, ax=axis[1,0])\nax = sns.barplot(x='OnlineBackup', y=axis_y, hue='Churn', data=gp_ob, ax=axis[1,1])\nax = sns.barplot(x='DeviceProtection', y=axis_y, hue='Churn', data=gp_dp, ax=axis[1,2])\nax = sns.barplot(x='TechSupport', y=axis_y, hue='Churn', data=gp_ts, ax=axis[2,0])\nax = sns.barplot(x='StreamingTV', y=axis_y, hue='Churn', data=gp_st, ax=axis[2,1])\nax = sns.barplot(x='StreamingMovies', y=axis_y, hue='Churn', data=gp_sm, ax=axis[2,2])\nax = sns.barplot(x='Contract', y=axis_y, hue='Churn', data=gp_con, ax=axis[3,0])\nax = sns.barplot(x='PaperlessBilling', y=axis_y, hue='Churn', data=gp_pb, ax=axis[3,1])\nax = sns.barplot(x='PaymentMethod', y=axis_y, hue='Churn', data=gp_pm, ax=axis[3,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we have donut charts showing the distibution of gender (Male/Female) and the distribution of our classes (Non-Churn/Churn). Sadly, there are more non-churners than churners. This means that our dataset is unbalanced, which needs to be handled before we can build our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1)\n\nplt.pie(x=df['gender'].value_counts().values, labels=df['gender'].value_counts().index, autopct='%1.2f', data=df)\nmy_circle=plt.Circle((0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('Distribution of Gender')\n\nplt.figure(2)\nplt.pie(x=df['Churn'].value_counts().values, labels=df['Churn'].value_counts().index, autopct='%1.2f', data=df)\nmy_circle=plt.Circle((0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('Distribution of Churn rate')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also show the distribution of the tenure feature (the duration of a customers subscription in months) per class. This graph shows that there are more customers that churn early in comparison to intermediate months. There are also many customers that have 72 months of tenure. This might mean that there are actually customers that have a longer tenure than 72 months, but that 72 months is chosen to be a cutoff point. What is certain, however, is that the churn rate diminishes for customers with longer tenure. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(x='tenure', hue='Churn', data=df, palette='pastel')\n\nplt.title(\"Number of months the customer has stayed with the company\")\nplt.xlabel('Number of months')\nplt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will show a density plot for the continuous variables ('Monthly Charges', 'Total Charges', and 'Tenure') per class. These density plots show that customers with higher montly charges are more likely to churn, while churn is rather independent of total charges. This can be explained by the fact that customers with large total charges are by definition a customer for longer, while having large monthly charges is independent of tenure. The 'Tenure' distribution again shows that customers that churn are likely to churn early. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(1,3, figsize=(15,5))\naxis[0].set_title(\"Distribution of Monthly Charges based on Churn\")\naxis[1].set_title(\"Distribution of Total Charges based on Churn\")\naxis[2].set_title(\"Distribution of Tenure based on Churn\")\n\nsns.kdeplot(df.MonthlyCharges[df.Churn=='Yes'], label='yes', shade=True, ax=axis[0])\nsns.kdeplot(df.MonthlyCharges[df.Churn=='No'], label='No', shade=True, ax=axis[0])\n\nsns.kdeplot(df.TotalCharges[df.Churn=='Yes'], label='yes', shade=True, ax=axis[1])\nsns.kdeplot(df.TotalCharges[df.Churn=='No'], label='No', shade=True, ax=axis[1])\n\nsns.kdeplot(df.tenure[df.Churn=='Yes'], label='yes', shade=True, ax=axis[2])\nsns.kdeplot(df.tenure[df.Churn=='No'], label='No', shade=True, ax=axis[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Manipulation & Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"Before we can train our models, we need to manipulate/transform our data, and to extract relevant features. For one thing, it is necessary that the features inserted in our model are numerical. So we need to convert our categorical features (that contain text) to numerical features."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['TotalCharges'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the categorical gender column to a numerical category (Female = 0, Male = 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_mapping = {'Female': 0, 'Male': 1}\ndf['gender'] = df['gender'].map(gender_mapping).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the variables that onlyl have 'No'/'Yes' values to a binary variable (No = 0, Yes = 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"no_yes_mapping = {'No': 0, 'Yes': 1}\ndf['Partner'] = df['Partner'].map(no_yes_mapping).astype(int)\ndf['Dependents'] = df['Dependents'].map(no_yes_mapping).astype(int)\ndf['PhoneService'] = df['PhoneService'].map(no_yes_mapping).astype(int)\ndf['PaperlessBilling'] = df['PaperlessBilling'].map(no_yes_mapping).astype(int)\ndf['Churn'] = df['Churn'].map(no_yes_mapping).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are also some variables with a category like 'No internet service' or 'No phone service'. This category is equivalent to not having the service, so transform these to a binary variable as well (No = 0, Yes = 1). "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df['MultipleLines'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_yes_mapping = {'No': 0, 'No internet service': 0, 'No phone service': 0, 'Yes': 1}\ndf['MultipleLines'] = df['MultipleLines'].map(no_yes_mapping).astype(int)\ndf['OnlineSecurity'] = df['OnlineSecurity'].map(no_yes_mapping).astype(int)\ndf['OnlineBackup'] = df['OnlineBackup'].map(no_yes_mapping).astype(int)\ndf['DeviceProtection'] = df['DeviceProtection'].map(no_yes_mapping).astype(int)\ndf['TechSupport'] = df['TechSupport'].map(no_yes_mapping).astype(int)\ndf['StreamingTV'] = df['StreamingTV'].map(no_yes_mapping).astype(int)\ndf['StreamingMovies'] = df['StreamingMovies'].map(no_yes_mapping).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'TotalCharges' column has some invalid rows. Convert the 'TotalCharges' column from 'Object' dtype to a 'float' dtype. Enter 0 in the rows with NaN/incorrect values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TotalCharges'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The remaining categorical variables have to be transformed into dummy variables. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['InternetService'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df['InternetService'], prefix='InternetService')], axis=1)\ndf.drop('InternetService', axis=1, inplace=True)\ndf.rename(columns={'InternetService_Fiber optic': 'InternetService_Fiber'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df['Contract'], prefix='Contract')], axis=1)\ndf.drop('Contract', axis=1, inplace=True)\ndf.rename(columns={'Contract_Month-to-month': 'Contract_Monthly',\n                   'Contract_One year': 'Contract_1Year',\n                   'Contract_Two year': 'Contract_2Year'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df['PaymentMethod'], prefix='Payment')], axis=1)\ndf.drop('PaymentMethod', axis=1, inplace=True)\ndf.rename(columns={'Payment_Bank transfer (automatic)': 'Payment_Bank',\n                   'Payment_Credit card (automatic)': 'Payment_Creditcard',\n                   'Payment_Electronic check': 'Payment_ElectronicCheck',\n                   'Payment_Mailed check': 'Payment_MailedCheck'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the customerID column, since it won't be needed for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('customerID', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrices"},{"metadata":{},"cell_type":"markdown","source":"Change the order of the columns of our dataframe, so the churn variable is presented first."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['Churn', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n       'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n       'PaperlessBilling', 'MonthlyCharges', 'TotalCharges',\n       'InternetService_DSL', 'InternetService_Fiber', 'InternetService_No',\n       'Contract_Monthly', 'Contract_1Year', 'Contract_2Year', 'Payment_Bank',\n       'Payment_Creditcard', 'Payment_ElectronicCheck', 'Payment_MailedCheck']]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Get a correlation DataFrame.\ncorr = df.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(14, 14))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmin=-0.5, vmax=0.5, center=0,\n            square=True, linewidths=.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get a correlation DataFrame, sorted descending, and get only the 'Churn' column, since this is our dependent variable. As you can see, there are some features that have a positive correlation with 'Churn', like having a monthly contract, having higher charges, and having more services. Other features are negatively correlated with 'Churn', like a customer's tenure and having longer-term contracts. These will probably be important features for training our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\ncorr = corr[['Churn']]\ncorr.sort_values(by='Churn', ascending=False, inplace=True)\ncorr = corr.iloc[1:]\n\nf, ax = plt.subplots(figsize=(11, 9))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr, cmap=cmap, vmin=-0.5, vmax=0.5, center=0,\n            square=True, linewidths=.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Split the dependent variable (y) from the independent variables (X)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n       'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n       'PaperlessBilling', 'MonthlyCharges', 'TotalCharges',\n       'InternetService_DSL', 'InternetService_Fiber', 'InternetService_No',\n       'Contract_Monthly', 'Contract_1Year', 'Contract_2Year', 'Payment_Bank',\n       'Payment_Creditcard', 'Payment_ElectronicCheck', 'Payment_MailedCheck']]\ny = df['Churn']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of our variables are on different scales (the categorical variables between 0 and 1, while 'MonthlyCharges' and 'TotalCharges' are between 0 and 9000). A MinMaxScaler will be applied to our independent variables to convert the values to be within 0 and 1, so that all the variables will receive the same weight when training our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\ncolumns = X.columns\nX = scaler.fit_transform(X)\n\nX = pd.DataFrame(X, columns=columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since our dataset is unbalanced (more non-churners than churners (75% and 25%, respectively)), we need to apply an oversampling technique called SMOTE (Synthetic Minority Over-sampling Technique) that will oversample rows from the minority class, making it a balanced dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nos = SMOTE(random_state = 0)\nX, y = os.fit_sample(X, y)\nX = pd.DataFrame(data = X, columns=columns)\ny = pd.Series(data = y);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"We will try to fit a first logistic regression to the dataset. We use cross-validation to test how well our models perform on unseen data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\nfrom sklearn.metrics import accuracy_score\nreg = LogisticRegression(C=2, solver='liblinear')\nreg.fit(X, y)\nprint(\"Logistic Regression:\\n\")\ny_test_predict = cross_val_predict(reg, X, y, cv=10)\nprint(\"The accuracy score for the test set is:     \", cross_val_score(reg, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The coefficients of our logistic regression model are as follows: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{:30s}{:10s}\\n'.format('Feature', 'Coefficient'))\nprint('{:30s}{:5.3f}'.format('Intercept', reg.intercept_[0]))\nfor i, feature in enumerate(reg.coef_[0]):\n    print('{:30s}{:5.3f}'.format(X.columns[i], feature))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y, y_test_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all our models, we will analyze their **recall** (% of the actual churners that the model correctly predicts), **precision** (% of the predicted churners that are correct) and **accuracy** (% of the predictions that are correct). These metrics will be used to compare the models. We will also compare the training accuracy score with the test accuracy score to determine how well the model performs on unseen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nmodel_scores = pd.DataFrame(columns=['Model', 'Recall', 'Precision', 'Accuracy'])\nmodel_scores.loc[0] = ['Logistic Regression',\n                       recall_score(y, y_test_predict),\n                       precision_score(y, y_test_predict),\n                       accuracy_score(y, y_test_predict)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bias_variance = pd.DataFrame(columns=['Model', 'Training Score', 'Test Score'])\nscores = cross_validate(reg, X, y, cv=10, return_train_score=True)\nbias_variance.loc[0] = 'Logistic Regression', scores['test_score'].mean(), scores['train_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, for all models we will create a **Receiver Operating Characteristic (ROC)**, which shows the relationship between the **True Positive Rate** and the **False Positive Rate** at various threshold settings. It tells you how well the model is capable to distinguish between classes, in our case non-churners and churners. A better model has a higher **Area under the curve (AUC)**."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve, scorer, auc\nprobabilities = reg.predict_proba(X)\nfpr_reg, tpr_reg, thresholds = roc_curve(y, probabilities[:,1])\nreg_auc = auc(fpr_reg, tpr_reg)\nprint(\"Area under curve: \", reg_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.title('Receiver Operating Characteristic (Logistic Regression)')\nplt.plot(fpr_reg, tpr_reg, 'b', label = 'AUC = %0.2f' % reg_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now plot the coefficients in a sorted bar-chart to get a better overview. It is clear that **tenure** negatively affects churn and that **TotalCharges** has a positive effect on churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"coefficients  = pd.DataFrame(reg.coef_.ravel())\ncolumn_df     = pd.DataFrame(['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n                              'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n                              'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n                              'PaperlessBilling', 'MonthlyCharges', 'TotalCharges',\n                              'InternetService_DSL', 'InternetService_Fiber', 'InternetService_No',\n                              'Contract_Monthly', 'Contract_1Year', 'Contract_2Year', 'Payment_Bank',\n                              'Payment_Creditcard', 'Payment_ElectronicCheck', 'Payment_MailedCheck'])\ncoef_sumry    = (pd.merge(coefficients, column_df, left_index=True,\n                              right_index=True, how=\"left\"))\ncoef_sumry.columns = [\"coefficients\",\"features\"]\ncoef_sumry    = coef_sumry.sort_values(by=\"coefficients\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.barplot(data=coef_sumry, x='coefficients', y='features', palette='pastel')\nplt.xlabel(xlabel='Features', fontsize=20)\nplt.ylabel(ylabel='Coefficients', fontsize=20)\nplt.title(label='Feature Coefficients', fontsize=20)\nplt.xticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last but not least, we will provide a confusion matrix to determine the amount of True Negatives (TN), False Posivites (FP), False Negatives (FN) and True Posivites (TP). The red blocks are the instances that the model has correctly predicted (either the non-churners or the churners), and are fortunately much bigger than the incorrect predictions (the blue blocks)."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncfn_matrix = confusion_matrix(y, y_test_predict)\n\n# Plot confusion matrix in a beautiful manner\nax = plt.subplot()\nsns.heatmap(cfn_matrix, annot=True, fmt='g', ax = ax, cmap=cmap, linewidths=1); \n\n# labels, title and ticks\nax.set_xlabel('Predicted', fontsize=20)\nax.xaxis.set_label_position('top') \nax.xaxis.set_ticklabels(['No Churn', 'Churn'], fontsize = 15)\nax.xaxis.tick_top()\n\nax.set_ylabel('True', fontsize=20)\nax.yaxis.set_ticklabels(['No Churn', 'Churn'], fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{},"cell_type":"markdown","source":"We will now do the same, but with a **Random Forest**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier(n_estimators=350, max_features='auto', max_depth=5, criterion='gini',\n                             min_samples_leaf=3, min_samples_split=12)\nRFC.fit(X, y)\ny_test_predict = cross_val_predict(RFC, X, y, cv=10)\nprint(\"Random Forest Classifier:\\n\")\nprint(\"The accuracy score for the test set is:     \", cross_val_score(RFC, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y, y_test_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores.loc[1] = ['Random Forest',\n                       recall_score(y, y_test_predict),\n                       precision_score(y, y_test_predict),\n                       accuracy_score(y, y_test_predict)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_validate(RFC, X, y, cv=10, return_train_score=True)\nbias_variance.loc[1] = 'Random Forest', scores['test_score'].mean(), scores['train_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = RFC.predict_proba(X)\nfpr_RFC, tpr_RFC, thresholds = roc_curve(y, probabilities[:,1])\nRFC_auc = auc(fpr_RFC, tpr_RFC)\nprint(\"Area under curve is: \", RFC_auc)\n# Plot the ROC curve of the Random Forest Classifier\nplt.figure(figsize=(10, 8))\nplt.title('Receiver Operating Characteristic (Random Forest Classifier)')\nplt.plot(fpr_RFC, tpr_RFC, 'b', label = 'AUC = %0.2f' % RFC_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfn_matrix = confusion_matrix(y, y_test_predict)\n\n# Plot confusion matrix in a beautiful manner\nax = plt.subplot()\nsns.heatmap(cfn_matrix, annot=True, fmt='g', ax = ax, cmap=cmap, linewidths=1); \n\n# labels, title and ticks\nax.set_xlabel('Predicted', fontsize=20)\nax.xaxis.set_label_position('top') \nax.xaxis.set_ticklabels(['No Churn', 'Churn'], fontsize = 15)\nax.xaxis.tick_top()\n\nax.set_ylabel('True', fontsize=20)\nax.yaxis.set_ticklabels(['No Churn', 'Churn'], fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of coefficients, a Random Forest has **Feature Importances** that demonstrate how important a feature was for building the decision trees. These weights say nothing about whether the features affect churn negatively or positively though."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"importances = pd.DataFrame(RFC.feature_importances_)\nimportances_df = (pd.merge(importances, column_df, left_index=True,\n                              right_index=True, how=\"left\"))\nimportances_df.columns = [\"feature importance\", \"features\"]\nimportances_df = importances_df.sort_values(by=\"feature importance\", ascending=False)\n\n# Plot the feature importances\nplt.figure(figsize=(20, 10))\nsns.barplot(data=importances_df, x='features', y='feature importance', palette='Blues')\nplt.xlabel(xlabel='Features', fontsize=20)\nplt.ylabel(ylabel='Feature importance', fontsize=20)\nplt.title(label='Feature Importance (Random Forest Classifier)', fontsize=20)\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors"},{"metadata":{},"cell_type":"markdown","source":"Last but not least, we will make a K-Nearest Neighbors classifier:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X, y)\ny_test_predict = cross_val_predict(knn, X, y, cv=10)\nprint(\"K-Nearest Neighbors Classification (k=10): \")\nprint(\"Accuracy of the test data:\", cross_val_score(knn, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_validate(knn, X, y, cv=10, return_train_score=True)\nbias_variance.loc[2] = 'K-Nearest Neighbors', scores['test_score'].mean(), scores['train_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(y, y_test_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores.loc[2] = ['K-Nearest Neighbors',\n                       recall_score(y, y_test_predict),\n                       precision_score(y, y_test_predict),\n                       accuracy_score(y, y_test_predict)]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"probabilities = knn.predict_proba(X)\nfpr_knn, tpr_knn, thresholds = roc_curve(y, probabilities[:,1])\nknn_auc = auc(fpr_knn, tpr_knn)\nprint(\"Area under the curve is : \", knn_auc)\n\n# Plot the ROC curve of the K-Nearest Neighbors Classifier\nplt.figure(figsize=(10, 8))\nplt.title('Receiver Operating Characteristic (K-Nearest Neighbor)')\nplt.plot(fpr_knn, tpr_knn, 'b', label = 'AUC = %0.2f' % knn_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cfn_matrix = confusion_matrix(y, y_test_predict)\n\n# Plot confusion matrix in a beautiful manner\nax = plt.subplot()\nsns.heatmap(cfn_matrix, annot=True, fmt='g', ax = ax, cmap=cmap, linewidths=1); \n\n# labels, title and ticks\nax.set_xlabel('Predicted', fontsize=20)\nax.xaxis.set_label_position('top') \nax.xaxis.set_ticklabels(['No Churn', 'Churn'], fontsize = 15)\nax.xaxis.tick_top()\n\nax.set_ylabel('True', fontsize=20)\nax.yaxis.set_ticklabels(['No Churn', 'Churn'], fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"K-Nearest Neighbors is the best-scoring model on the TelCo dataset. Although it has a smaller **Precision** than the other models, its **Recall** and **Accuracy** is significantly better, as well as having the highest **AUC (Area under the curve)**. In our opinion, TelCo should aim for a higher recall instead of precision, since it is better to incorrectly label a non-churner as a churner, than to incorrectly label a churner as a non-churner. Sending an email or a discount coupon is cheap, while losing a customer means losing its revenue. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_scores","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot the ROC curve of all the models together\nplt.style.use('seaborn')\nplt.figure(figsize=(13, 9))\nplt.title('Receiver Operating Characteristic (All Models))')\nplt.plot(fpr_reg, tpr_reg, 'g', label = 'Logistic Regression, AUC = %0.2f' % reg_auc)\nplt.plot(fpr_RFC, tpr_RFC, 'r', label = 'Random Forest, AUC = %0.2f' % RFC_auc)\nplt.plot(fpr_knn, tpr_knn, 'y', label = 'K-NN, AUC = %0.2f' % knn_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.savefig('Test2.png', dpi=300, bbox_inches='tight', pad_inches=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check whether the models are overfit, here is an overview of the training and test accuracy of the models. The models are scoring better on the testing data, meaning the models are not overfit and are thus suited for predicting unseen data. "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"bias_variance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra: Survival Analysis"},{"metadata":{},"cell_type":"markdown","source":"As an extra analysis, we will also employ survival analysis on our dataset. Survival analysis is used to calculate the expected duration of time until a specific event happens, in our case whether a customer is going to churn. So instead of predicting which customers are going to churn, with survival analysis we can predict when customers are going to churn.\n\nHowever, we needed to import a package called 'lifelines' that is unavailable on Kaggle. If you want to see our Survival Analysis on this dataset, head over to the notebook on our github page: https://github.com/nieklambrechts/TelCo-Churn-Analysis"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}