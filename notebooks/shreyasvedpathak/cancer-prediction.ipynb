{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Breast Cancer Prediction\n#### In this notebook, we will try classify and predict whether it is a Malignant(M) or a Benign(B) based on the features we provide to the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing the libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's load the data and explore it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"breast_data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\nbreast_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deleting unneccesary columns.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"breast_data = breast_data.drop(['id', 'Unnamed: 32'], axis  = 1)\nbreast_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"breast_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Changing M and B in diagnosis column to 1 and 0 respectively.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"breast_data['diagnosis'] = np.where((breast_data['diagnosis'] == 'M'), 1, 0)\nbreast_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"breast_data['diagnosis'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now our dataset has only numerical values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Now we will split the data for visualization purpose.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cancer_M = breast_data[breast_data['diagnosis']==1]\ncancer_B = breast_data[breast_data['diagnosis']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features_mean=list(breast_data.columns[1:11])\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,13))\naxes = axes.ravel()\nfor idx,ax in enumerate(axes):\n    ax.figure\n    ax.hist([cancer_M[features_mean[idx]],cancer_B[features_mean[idx]]], bins = 50, alpha=0.8, stacked=True, label=['M','B'], color=['red','blue'])\n    ax.legend(loc='upper right')\n    ax.set_title(features_mean[idx])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Take a look at these plots before reading the next part.\nAs you may have noticed, Red colour corresponds to Malignant and Blue to Benign. There are features that can differentiate and classify whether it is M or B easily whereas as features which might not help us classify properly.\n\n#### Features that can help us:\n    1] radius_mean\n    2] perimeter_mean\n    3] area_mean\n    4] compactness_mean\n    5] concavity_mean\n    6] concave points_mean\n#### Features that might not be as useful:\n    1] texture_mean\n    2] smoothness_mean\n    3] symmetry_mean\n    4] fractal_dimension_mean","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting dataset into two viz. training set and testing set.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train, test = train_test_split(breast_data, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function that takes type of model, training set, testing set, features & target as arguments and prints Accuracy.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def classification_model(model, train, test, features, target):\n    model.fit(train[features], np.ravel(train[target]))\n    pred = model.predict(test[features])\n  \n    accuracy = accuracy_score(pred ,test[target])\n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models based on Logistic Regression\n#### Uses only one feature:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['radius_mean']\ntarget = ['diagnosis']\n\nmodel_logistic = LogisticRegression()\nclassification_model(model_logistic, train, test, features, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Uses all the mean valued features:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\ntarget = ['diagnosis']\n\nmodel_logistic_2 = LogisticRegression()\nclassification_model(model_logistic_2 , train, test, features, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models based on Random Forest Classifier \n#### Uses features that we shortlisted by observing the plots above:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean','concave points_mean', 'concavity_mean']\ntarget = ['diagnosis']\n\nmodel_random = RandomForestClassifier(random_state=0)\nclassification_model(model_random, train, test, features, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model based on Random Forest Classifier (uses all the mean valued features):","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n                 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nmodel_random_2 = RandomForestClassifier(random_state=0)\nclassification_model(model_random_2, train, test, features, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Classifier has a attribute called 'feature_importances_'. It tells us which feature are important in making the decision i.e. in this case M or B.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"imp_features = pd.Series(model_random_2.feature_importances_, index=features).sort_values(ascending=False)\nimp_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we will only use top 7 features according to feature_importances_ attribute.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"features = imp_features.index[:7]\ntarget = ['diagnosis']\n\nmodel_random_3 = RandomForestClassifier(random_state=0)\nclassification_model(model_random_3, train, test, features, target)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}