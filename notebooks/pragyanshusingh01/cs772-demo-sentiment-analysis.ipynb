{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport transformers\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')\n\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df.text))\ndf=df[[\"text\", \"airline_sentiment\"]]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['negative', 'neutral', 'positive']\ndef to_sentiment(rating):\n    return class_names.index(rating)\n\ndf['sentiment'] = df.airline_sentiment.apply(to_sentiment)\ndf=df[[\"text\", \"sentiment\"]]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nclean_text = []\n\nfor text in df['text']:\n    text = re.sub(\"@[A-Za-z0-9]+\",\"\",text) \n    clean_text.append(text)\n    \ndf['text'] = clean_text\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shuffling the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac=1).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT-Tokenizer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, TFBertModel\npre_tr_mdl='bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n\ninput_ids = []\nattention_masks = []\n\n\nfor sent in zip(df['text']):\n    encoded_dict = tokenizer.encode_plus(\n                        sent,\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 250,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n       \n    input_ids.append(encoded_dict['input_ids'])\n    \n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(df['sentiment'])\n# print(len(labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spliting the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\n# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\ntrain_size = int(0.85 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating data-loaders:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 16\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting BERT of our choice:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n# BertForSequenceClassification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n    num_labels = 3,\n    output_attentions = False,\n    output_hidden_states = False, \n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting the gpu for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.to(device)\nimport numpy as np\ndef flat_accuracy(preds, labels):\n    p=[]\n    for i in preds:\n        i=i.cpu().detach().numpy()\n        p.append(i.argmax())\n    labels_flat = labels.flatten().cpu().numpy()\n    return np.sum(p == labels_flat) / len(labels_flat)\ndef flat_accuracy_v2(preds, labels):\n    p=[]\n    for i in preds:\n        i=i.cpu().detach().numpy()\n        p.append(i.argmax())\n    labels_flat = labels.flatten().cpu().numpy()\n    \n    return np.sum(p == labels_flat) / len(labels_flat),labels_flat,p","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training our BERT-Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, glue_convert_examples_to_features\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nacc=[]\noptim = AdamW(model.parameters(), lr=5e-5)\nmodel.eval()\ntest_res=[]\nfor batch in validation_dataloader:\n    input_ids = batch[0].to(device)\n    attention_mask = batch[1].to(device)\n    labels = batch[2].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n    test_res.append(flat_accuracy(outputs[1],labels))\n    model.train()\n\nprint(\"UNTUNED ACCURACY==>\",sum(test_res)/len(test_res))\n\nEpochs=2\n\nfor epoch in range(Epochs):\n    print(\"Epoch:\",epoch+1,\" of \",Epochs)\n    c=0\n    l=len(train_dataloader)\n    model.train()\n\n    train_res=[]\n    for batch in train_dataloader:\n        c+=1\n#         print(\"Epoch:\",epoch+1,\"Running \",c,\" of \",l)\n        print(\"Progress {:2.1%}\".format(c/ l), end=\"\\r\")\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        train_res.append(flat_accuracy(outputs[1],labels))\n        loss.backward()\n        optim.step()\n    print(\"TRAIN ACCURACY==>\",sum(train_res)/len(train_res))\n    model.eval()\n    test_res=[]\n    for batch in validation_dataloader:\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        test_res.append(flat_accuracy(outputs[1],labels))\n    print(\"TEST ACCURACY==>\",sum(test_res)/len(test_res))\n    model.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating Test Results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_s,pred_s,dsen=[],[],[]\nmodel.eval()\nfor batch in validation_dataloader:\n    for ii in batch[0]:\n        s=tokenizer.convert_ids_to_tokens(ii)\n        s=tokenizer.convert_tokens_to_string(s)\n        dsen.append(s)\n    input_ids = batch[0].to(device)\n    attention_mask = batch[1].to(device)\n    labels = batch[2].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n    test_res.append(flat_accuracy(outputs[1],labels))\n    label_s.append(flat_accuracy_v2(outputs[1],labels)[1])\n    pred_s.append(flat_accuracy_v2(outputs[1],labels)[2])\n    \nprint(\"TEST ACCURACY==>\",sum(test_res)/len(test_res))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}