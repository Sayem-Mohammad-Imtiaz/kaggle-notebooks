{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **About Project**\n**This is the final project of An End TO End Deep Learning training in Electro Pi for AI, This project is a recommender system for Netflix for movies using Machine Learning and Deep Learning tools**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Lets import some libraries that we will use\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow.keras as tf\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n# To shift lists\nfrom collections import deque","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load single data-file\ndf_raw = pd.read_csv('../input/netflix-prize-data/combined_data_2.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n\n\n# Find empty rows to slice dataframe for each movie\ntmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\nmovie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n\n# Shift the movie_indices by one to get start and endpoints of all movies\nshifted_movie_indices = deque(movie_indices)\nshifted_movie_indices.rotate(-1)\n\n\n# Gather all dataframes\nuser_data = []\n\n# Iterate over all movies\nfor [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n    \n    # Check if it is the last movie in the file\n    if df_id_1<df_id_2:\n        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n    else:\n        tmp_df = df_raw.loc[df_id_1+1:].copy()\n        \n    # Create movie_id column\n    tmp_df['Movie_Id'] = movie_id\n    \n    # Append dataframe to list\n    user_data.append(tmp_df)\n\n# Combine all dataframes\nmovie_info = pd.concat(user_data)\ndel user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\nprint('Shape User-Ratings:\\t{}'.format(movie_info.shape))\nmovie_info.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load titles of movies\nmovie_title = pd.read_csv('../input/netflix-prize-data/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\nmovie_title.set_index('Movie_Id', inplace = True)\nmovie_title.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#put all together\ndata_merge = pd.merge(movie_info, movie_title, on='Movie_Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_merge.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's get some information from data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_merge.groupby('Name')['Rating'].mean().sort_values(ascending=False).head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_merge.groupby('Name')['Rating'].count().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data frame for ratings \nratings = pd.DataFrame(data_merge.groupby('Name')['Rating'].mean())\nratings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings['num of ratings'] = pd.DataFrame(data_merge.groupby('Name')['Rating'].count().sort_values(ascending=False))\nratings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nratings['num of ratings'].hist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nratings['Rating'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.jointplot(x='Rating',y='num of ratings',data=ratings,alpha=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Let's prepair data for our model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop columns that we won't use\ndata_model = data_merge.drop(columns=['Date','Year','Name'])\ndata_model.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_model.shape)\nprint(data_model.User.nunique())\nprint(data_model.Movie_Id.nunique())\ndata_model.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model['User']=data_model['User'].astype(int)\ndata_model.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_titles = [\"Movie_Id\",'User',\"Rating\"]\ndata_model=data_model.reindex(columns=columns_titles)\ndata_model.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we slice our data to save time for testing our model\ndata_model=data_model.sample(20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split our data\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest = train_test_split(data_model, test_size=0.3, random_state=1)\nprint(f\"Shape of train data: {Xtrain.shape}\")\nprint(f\"Shape of test data: {Xtest.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the number of unique entities in movies and users columns\nnmovies_id = data_model.Movie_Id.nunique()\nnuser_id = data_model.User.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Movie input network\ninput_movies = tf.layers.Input(shape=[1])\nembed_movies = tf.layers.Embedding(2700000 + 1,15)(input_movies)\nmovies_out = tf.layers.Flatten()(embed_movies)\n\n#user input network\ninput_users = tf.layers.Input(shape=[1])\nembed_users = tf.layers.Embedding(2700000 + 1,15)(input_users)\nusers_out = tf.layers.Flatten()(embed_users)\n\nconc_layer = tf.layers.Concatenate()([movies_out, users_out])\nx = tf.layers.Dense(4, activation='relu')(conc_layer)\nx_out = x = tf.layers.Dense(1, activation='relu')(x)\nmodel = tf.Model([input_movies, input_users], x_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = tf.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=opt, loss='mean_squared_error')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit([Xtrain.Movie_Id, Xtrain.User], Xtrain.Rating, \n                 batch_size=64, \n                 epochs=10, \n                 verbose=1,\n                 validation_data=([Xtest.Movie_Id, Xtest.User], Xtest.Rating))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = hist.history['loss']\nval_loss = hist.history['val_loss']\nplt.plot(train_loss, color='r', label='Train Loss')\nplt.plot(val_loss, color='b', label='Validation Loss')\nplt.title(\"Train and Validation Loss Curve\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save the model\nmodel.save('model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract embeddings\nmovie_em = model.get_layer('embedding')\nmovie_em_weights = movie_em.get_weights()[0]\nmovie_em_weights.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy = data_merge.copy()\ndata_copy = data_copy.set_index(\"Movie_Id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_id =list(data_merge.Movie_Id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To visualize the data on embedding projector of Tensorflow >> \n[Embedding Projector](https://projector.tensorflow.org/)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# dict_map = {}\n# for i in m_id:\n#     dict_map[i] = data_copy.iloc[i]['Name']\n    \n# out_v = open('vecs.tsv', 'w')\n# out_m = open('meta.tsv', 'w')\n# for i in m_id:\n#     book = dict_map[i]\n#     embeddings = movie_em_weights[i]\n#     out_m.write(book + \"\\n\")\n#     out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n    \n# out_v.close()\n# out_m.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making recommendations for user 50\nmovie_arr = np.array(m_id) #get all Movie IDs\nuser = np.array([50 for i in range(len(m_id))])\npred = model.predict([movie_arr, user])\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pred.reshape(-1) #reshape to single dimension\npred_ids = (-pred).argsort()[0:7]\npred_ids\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bellow, We get 20 predictions for the user number 100\nFrom the table, we notice that .. first User_Id is the user number 100 and the other Users_Ids indicates that the user have the same behavior as the first User_Id**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_merge.iloc[pred_ids]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}