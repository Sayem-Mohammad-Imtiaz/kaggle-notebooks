{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Job Recommendations \n\nThis notebook creates a model, to recommend job positions given a position requirements description . This is done only for IT jobs. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score, auc, roc_curve, roc_auc_score\n\n    \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/jobposts/data job posts.csv')\nprint(data.columns)\n# selecting only IT Jobs\ndf = data[data['IT']]\n# selecting \ncols = ['RequiredQual', 'Eligibility', 'Title', 'JobDescription', 'JobRequirment']\ndf=df[cols]\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modifying Job Titles\nSelecting only top 21 job titles, to manage class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = df['Title'].value_counts()[:21]\nkeys = classes.keys().to_list()\n\ndf = df[df['Title'].isin(keys)]\ndf['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Change job titles to base title. For exmaple, chaning Senior Java Developer to Java Developer.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"def chane_titles(x):\n    x = x.strip()\n    if x == 'Senior Java Developer':\n        return 'Java Developer'\n    elif x == 'Senior Software Engineer':\n        return 'Software Engineer'\n    elif x == 'Senior QA Engineer':\n        return 'Software QA Engineer'\n    elif x == 'Senior Software Developer':\n        return 'Senior Web Developer'\n    elif x =='Senior PHP Developer':\n        return 'PHP Developer'\n    elif x == 'Senior .NET Developer':\n        return '.NET Developer'\n    elif x == 'Senior Web Developer':\n        return 'Web Developer'\n    elif x == 'Database Administrator':\n        return 'Database Admin/Dev'\n    elif x == 'Database Developer':\n        return 'Database Admin/Dev'\n\n    else:\n        return x\n        \n    \ndf['Title'] = df['Title'].apply(chane_titles)\ndf['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building custom tokenizer to process text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nclass LemmaTokenizer(object):\n    def __init__(self):\n        # lemmatize text - convert to base form \n        self.wnl = WordNetLemmatizer()\n        # creating stopwords list, to ignore lemmatizing stopwords \n        self.stopwords = stopwords.words('english')\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.stopwords]\n\n# removing new line characters, and certain hypen patterns                  \ndf['RequiredQual']=df['RequiredQual'].apply(lambda x: x.replace('\\n', ' ').replace('\\r', '').replace('- ', ''). replace(' - ', ' to '))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Featurizing Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# train features and labels \ny = df['Title']\nX = df['RequiredQual']\n# tdif feature rep \nvectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english')\nvectorizer.fit(X)\n# transoforming text to tdif features\ntfidf_matrix = vectorizer.transform(X)\n# sparse matrix to dense matrix for training\nX_tdif = tfidf_matrix.toarray()\n# encoding text labels in categories \nenc = LabelEncoder() \nenc.fit(y.values)\ny_enc=enc.transform(y.values)\n\nX_train_words, X_test_words, y_train, y_test = train_test_split(X, y_enc, test_size=0.15, random_state=10)\n\nX_train = vectorizer.transform(X_train_words)\nX_train = X_train.toarray()\n\nX_test = vectorizer.transform(X_test_words)\nX_test = X_test.toarray()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Naive Bayes\nLooks pretty overfit"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\ngnb = GaussianNB()\ntrain_preds = gnb.fit(X_train, y_train).predict(X_train)\ntest_preds = gnb.predict(X_test)\n\nprint('Train acc: {0}'.format(accuracy_score(y_train, train_preds)))\nprint('Test acc: {0}'.format(accuracy_score(y_test, test_preds)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Logistic Regression\nBy modifiying the maximum number of iterations, and regularization, C, the above experienced overfitting was reduced significantly \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogistic = LogisticRegression(max_iter=15,verbose=1, C=0.75)\n\ntrain_preds = logistic.fit(X_train, y_train).predict(X_train)\ntest_preds = logistic.predict(X_test)\n\nprint('Train acc: {0}'.format(accuracy_score(y_train, train_preds)))\nprint('Test acc: {0}'.format(accuracy_score(y_test, test_preds)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Job Recommendations \nRecommends 2 job position alternatives given a job requirement. By obtaining probability of class predictions, and picking the top N predictions, other than true label, N closest recommendations can be got"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_data = {'Current Position Requirments': [], 'Current Position': [], 'Alternative 1': [], 'Alternative 2': []}\ny_preds_proba = logistic.predict_proba(X_test)\n\ncounter = 0 \nfor idx, (pred_row, true_job_position) in enumerate(zip(y_preds_proba, y_test)):\n    class_preds = np.argsort(pred_row)\n    # delete true class\n    for i in [-1, -2]:\n        if class_preds[i] == true_job_position:\n            class_preds=np.delete(class_preds,i)\n    # getting other 2 highest job predictions         \n    top_classes = class_preds[-2:]\n    # obtaining class name string from int label \n    class_names = enc.inverse_transform(top_classes)\n    true_job_position_name = enc.inverse_transform([true_job_position])\n    # saving to dict\n    preds_data['Current Position Requirments'].append(X_test_words.iloc[idx])\n    preds_data['Current Position'].append(true_job_position_name[0])\n    preds_data['Alternative 1'].append(class_names[1])\n    preds_data['Alternative 2'].append(class_names[0])\n\n    \n    counter +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df = pd.DataFrame.from_dict(preds_data)\npreds_df.to_csv('Recommendations.csv', index=False)\npreds_df\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}