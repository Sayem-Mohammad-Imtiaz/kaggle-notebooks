{"cells":[{"metadata":{"id":"FTsF8ly90-EB","outputId":"a04096ed-9b35-4f6d-c56a-17dee656b4ff","trusted":true},"cell_type":"code","source":"## download the nltk\nimport nltk\nnltk.download('all-corpora')","execution_count":null,"outputs":[]},{"metadata":{"id":"ajbW7oAC1RGZ","trusted":true},"cell_type":"code","source":"from nltk.tokenize      import sent_tokenize,word_tokenize\nfrom nltk.corpus        import stopwords\nfrom nltk.tokenize      import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"id":"e0zqjtHN1U4A","outputId":"26dcdea6-c5d2-4e9c-a3e7-b6f75e85d68f","trusted":true},"cell_type":"code","source":"### this is fetching the stop words\nexample_sentences  = \"this is an example of the stop words\"\nstop_words         = set(stopwords.words(\"english\"))\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"id":"RSWSLBLR1YOr","outputId":"965095f1-dec6-40a9-ec2e-7583a19748d8","trusted":true},"cell_type":"code","source":"## this is the word tokenization\nwords = word_tokenize(example_sentences);\nprint(words)\nfiltered_sentence = []\nfor w in words:\n  if w not in stop_words:\n    filtered_sentence.append(w)\nprint(filtered_sentence)","execution_count":null,"outputs":[]},{"metadata":{"id":"Gts9o9AX3CUi","outputId":"fa638512-04f6-4ce6-8e9b-8fb6d9376c78","trusted":true},"cell_type":"code","source":"## this is the stemming [we fix the world like listen and listening and make it one so it will use one only]\nfrom nltk.stem import PorterStemmer\nps      = PorterStemmer()\nexample_word = ['listen','listenning','listened']\nfor w in example_word:\n  print(ps.stem(w))","execution_count":null,"outputs":[]},{"metadata":{"id":"E91TYq7S3b_5","outputId":"b5073d23-ce35-4173-a2db-9721f2d62c01","trusted":true},"cell_type":"code","source":"## stemming  a new word\nnew_text  = \"it is very important to be pythonly when solve the problem in python. be pythonic. all pythoner do this job solved the problem in  a pythonic way\"\nwords      =  word_tokenize(new_text)\nstemmed_word = []\nfor w in words:\n  stemmed_word.append(ps.stem(w))\nprint(stemmed_word)","execution_count":null,"outputs":[]},{"metadata":{"id":"OgLmpxls3kkn","outputId":"9f7a5e25-0a67-4214-b22b-c27d18254643","trusted":true},"cell_type":"code","source":"from nltk.corpus    import state_union\nfrom nltk.tokenize  import PunktSentenceTokenizer              ## unsupervised tokenizer\ntrain_text  = state_union.raw('2005-GWBush.txt')\nsample_text = state_union.raw(\"2006-GWBush.txt\")\ncustomn_sent_tokenizer = PunktSentenceTokenizer(train_text)   ## trian\nprint(customn_sent_tokenizer)\ntokenize = customn_sent_tokenizer.tokenize(sample_text)       ## inference\nprint(tokenize)                                               ## this is tokenize win the sentence","execution_count":null,"outputs":[]},{"metadata":{"id":"AACmNBiH3n_f","outputId":"d75c2a3d-1b85-4fa4-c4b3-492f7f2b6748","trusted":true},"cell_type":"code","source":"## this is parts of speech tagging \n## find the parts of speech from the word coupous\ntokenized_word = []\ntry:\n  for i in tokenize:\n    words  = nltk.word_tokenize(i)\n    ## indexing the word with parts of speetch\n    tagged = nltk.pos_tag(words)\n    tokenized_word.append(tagged)\nexcept:\n  pass\nprint(\"Word and the parts of sppech\")\nprint(tokenized_word)","execution_count":null,"outputs":[]},{"metadata":{"id":"8IzSDfCi7Sz7","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"qBH8Bs0_9Yy1","trusted":true},"cell_type":"code","source":"## movie Reviwe for POSITIVE OR NEGATIVE WITH NLTK\nimport nltk\nimport random\nfrom nltk.corpus import movie_reviews","execution_count":null,"outputs":[]},{"metadata":{"id":"zw4ojTjiAsFY","trusted":true},"cell_type":"code","source":"documents = []\nfor category in movie_reviews.categories():\n  for movie in movie_reviews.fileids(category):\n    documents.append((list(movie_reviews.words(movie)),category))","execution_count":null,"outputs":[]},{"metadata":{"id":"mLtPwHz-CtxL","outputId":"b041b047-ff73-4964-b9d9-079113481275","trusted":true},"cell_type":"code","source":"print(documents[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"IUnYn4EHDy8P","trusted":true},"cell_type":"code","source":"random.shuffle(documents)","execution_count":null,"outputs":[]},{"metadata":{"id":"-qtC9o2AEAEl","outputId":"7b07abd7-c6e3-4c25-be50-78dca64aad69","trusted":true},"cell_type":"code","source":"## find the most common words\nall_words = []\nfor w in movie_reviews.words():\n  all_words.append(w.lower())\n## find the frequency of any word\n## which word is the most used words\nall_words = nltk.FreqDist(all_words)\nprint(all_words['disgusting'])\nprint(all_words['excellent'])","execution_count":null,"outputs":[]},{"metadata":{"id":"SoB_DEZ_EE9E","outputId":"bacf3e3e-92d3-4ee8-cc49-29950ebfbfbc","trusted":true},"cell_type":"code","source":"## take the feture\n## we take the most used 4000 words as a feture\nword_feature = list(all_words)[:3000]\nprint(word_feature)","execution_count":null,"outputs":[]},{"metadata":{"id":"agTAQhw0Er9t","trusted":true},"cell_type":"code","source":"## check if any word of he 3000 words in  the document\ndef find_features(document):\n  words    = set(document)\n  features = {}\n  for w in word_feature:\n    features[w] = (w in words)\n\n  return features ","execution_count":null,"outputs":[]},{"metadata":{"id":"hgd1ZE8IIHKk","outputId":"c2054229-41c8-4cfb-c244-f5d8fac21eac","trusted":true},"cell_type":"code","source":"print(find_features(movie_reviews.words('neg/cv000_29416.txt')))","execution_count":null,"outputs":[]},{"metadata":{"id":"sn8nQasyIbp0","trusted":true},"cell_type":"code","source":"feature_set = []\nfor x,y in documents:\n  feature_set.append((find_features(x),y))","execution_count":null,"outputs":[]},{"metadata":{"id":"LH5Pr_eLIvcD","outputId":"e6c34009-869f-4972-aed3-7e742d6fdc5e","trusted":true},"cell_type":"code","source":"feature_set[:1]","execution_count":null,"outputs":[]},{"metadata":{"id":"ZtjfInAsJLCT","trusted":true},"cell_type":"code","source":"train_set = feature_set[:1900]\ntest_set = feature_set[1900:]","execution_count":null,"outputs":[]},{"metadata":{"id":"z7JvnZZUJkLx","trusted":true},"cell_type":"code","source":"classifier = nltk.NaiveBayesClassifier.train(train_set)","execution_count":null,"outputs":[]},{"metadata":{"id":"JTZ6I-tlJ07Q","outputId":"26a53979-d229-40bf-a9b2-5aae6f626cb5","trusted":true},"cell_type":"code","source":"accuracy = nltk.classify.accuracy(classifier,test_set)\nprint(\"accuracy : {}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"id":"MnYkbwPkKKeK","outputId":"367e5be6-2b0a-4055-cf0f-e02b6abd3acc","trusted":true},"cell_type":"code","source":"classifier.show_most_informative_features(40)","execution_count":null,"outputs":[]},{"metadata":{"id":"4WBaZhzIKYUA","trusted":true},"cell_type":"code","source":"### now we are make a recomendation system based on the sentiment \n### with neural network","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy             as np\nimport pandas            as pd\nimport seaborn           as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## import the amazon review dataset\ndf = pd.read_csv(\"../input/amazon-dataset-for-recomendation/1429_1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## make a plotting of how many review and how many rating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review =  pd.DataFrame(df.groupby('reviews.rating').size().sort_values(ascending=True).rename(\"Users\").reset_index())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### so we have 4 rating the most\n### now make a prediction system\ndf = df[['reviews.rating' , 'reviews.text']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review = list(df['reviews.text'])     ## feature\nrating = list(df['reviews.rating'])   ## target ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(review.__len__())\nprint(rating.__len__())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = review.__len__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_size = 30000\ntraining_sentences = review[0:training_size]\ntesting_sentences = review[training_size:]\ntraining_labels = rating[0:training_size]\ntesting_labels = rating[training_size:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we need to tokenize the word\n## we use tensorlfow tokenizer this time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nvocab_size = 30000\noov_token  =\"<OOV>\"\ntokenizer  = Tokenizer(num_words=vocab_size,oov_token=oov_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## now make the sequence based on the bag of words\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## apply padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nprint(training_sequences[0])\nprint(training_sequences[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_padded = pad_sequences(training_sequences,maxlen=32,padding=\"post\",truncating=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_padded[0].__len__())\nprint(training_padded[1].__len__())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_padded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_sequences = tokenizer.texts_to_sequences(testing_sentences) \ntesting_padded = pad_sequences(testing_sequences,maxlen=32,padding=\"post\",truncating=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(testing_padded[0].__len__())\nprint(testing_padded[1].__len__())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(testing_padded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## this is the Embedding Layer Parameter\nvocab_size = 20000\nembedding_dim = 8\nmax_length = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length))\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(6,activation='softmax'))\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_labels = np.array(training_labels)\ntesting_labels = np.array(testing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(testing_padded,testing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict(testing_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual=[]\nfor item in predicted:\n    actual.append(np.argmax(item))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## imdb dataset review\nimport numpy             as np\nimport pandas            as pd\nimport seaborn           as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {\"positive\":1,\"negative\":0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment'] = df['sentiment'].map(d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review =  pd.DataFrame(df.groupby('sentiment').size().sort_values(ascending=True).rename(\"Users\").reset_index())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.__len__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review = list(df['review'])     ## feature\nrating = list(df['sentiment'])   ## target ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_size = 30000\ntraining_sentences = review[0:training_size]\ntesting_sentences = review[training_size:]\ntraining_labels = rating[0:training_size]\ntesting_labels = rating[training_size:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we need to tokenize the word\n## we use tensorlfow tokenizer this time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nvocab_size = 40000\noov_token  =\"<OOV>\"\ntokenizer  = Tokenizer(num_words=vocab_size,oov_token=oov_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(training_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sequences = tokenizer.texts_to_sequences(training_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## apply padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_padded = pad_sequences(training_sequences,maxlen=250,padding=\"post\",truncating=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_padded[0].__len__())\nprint(training_padded[1].__len__())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_sequences = tokenizer.texts_to_sequences(testing_sentences) \ntesting_padded = pad_sequences(testing_sequences,maxlen=250,padding=\"post\",truncating=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## this is the Embedding Layer Parameter\n## this vocab size must be greater than the tokenizer vocab size\nvocab_size = 50000\nembedding_dim = 100\nmax_length = 33","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_labels = np.array(training_labels)\ntesting_labels = np.array(testing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length))\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(250,activation='relu'))\nmodel.add(tf.keras.layers.Dropout(.2))\nmodel.add(tf.keras.layers.Dense(2,activation='softmax'))\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}