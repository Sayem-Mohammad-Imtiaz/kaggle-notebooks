{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# ========================\n# Dependencies\n# ========================\n\nimport numpy as np #Working with Arrays\nimport pandas as pd #Working with DataFrames\n\n# Data Visualization ##########\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n###############################\n\n# NLP Preprocessing ########################################################\n!pip install langdetect\nimport re\nimport string\nfrom langdetect import detect_langs\nfrom sklearn.preprocessing import OneHotEncoder\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n############################################################################\n\n# Machine Learning ################################\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation\n###################################################","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ========================\n# Get Data\n# ========================\n\npath=\"../input/60k-stack-overflow-questions-with-quality-rate/data.csv\"\n\ndataDF=pd.read_csv(path)\n\nprint(dataDF.info())\ndataDF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===================================\n# Combining text from Title and Body\n# ===================================\ndataDF['Text'] = dataDF['Title'] + ' ' + dataDF['Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Drop Text that are not predominantly in English\n# ====================================================\ndrop_index=[]\nfor i in tqdm(range(len(dataDF))):\n    detected_lan = detect_langs(dataDF['Text'].iloc[i])\n    for lan in detected_lan:\n        if lan.lang == \"en\" and lan.prob < 0.4:\n            drop_index.append(i)\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataDF=dataDF.drop(drop_index,axis=0)\nprint(\"Number of features dropped:\", len(drop_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ==========================\n# Exploratory Data Analysis\n# ==========================\n\n# Check if there are any Nan values in DataFrame\nprint(\"Nan value check\")\nprint(dataDF.isnull().any(),'\\n')\n\nprint(\"Check the total and unique text and labels\")\nprint(dataDF[['Text','Y']].describe()[:2])\n\n# Distribution of ratings\ny = dataDF['Y'].values\nplt.title(\"Distribution of Labels\")\nsns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===========================\n# Cleaning/Updating DataFrame\n# ===========================\n\n# Store Tags in a list\n# def clean_tags(text):\n#     clean=re.split('[<>]',text)\n#     clean=[w for w in clean if w != '']\n#     return clean\n# dataDF['Tags']=dataDF['Tags'].progress_apply(lambda x : clean_tags(x))\n\ndef clean_text(text):\n    # Convert text to lowercase\n    text=text.lower()\n    # Remove punctuations\n    text=''.join(c for c in text if c not in string.punctuation)\n    # Convert html to text\n    soup=BeautifulSoup(dataDF['Body'].values[0],'lxml')\n    s=soup.get_text('\\n')\n    s=s.replace('\\n','')\n    # Expand common contractions\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = ' '.join(text.split())\n    return text\ntqdm.pandas()\ndataDF['Text']=dataDF['Text'].progress_apply(lambda x : clean_text(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================\n# OneHotEncode Labels \n# ====================\nencoder=OneHotEncoder()\nencoded_arr=encoder.fit_transform(dataDF[['Y']]).toarray()\n\n\nX=dataDF['Text']\ny=encoded_arr\ndataDF=dataDF.drop(['Id','Tags','CreationDate'],axis=1)\ndataDF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===========================\n# Clear up unnecessary memory\n# ===========================\ndel dataDF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # =========================================\n# # Visualizing The Most Commonly Used Words\n# # =========================================\n# unigram_vec=CountVectorizer(stop_words='english')\n# unigram_bow=unigram_vec.fit_transform(tqdm(X))\n\n# bigram_vec=CountVectorizer(ngram_range=(2,2),stop_words='english')\n# bigram_bow=bigram_vec.fit_transform(tqdm(X))\n\n# trigram_vec=CountVectorizer(ngram_range=(3,3),stop_words='english')\n# trigram_bow=trigram_vec.fit_transform(tqdm(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ==============================\n# Preparing train and test data\n# ==============================\nfrom sklearn.model_selection import train_test_split\n\nxtrain,xtest,ytrain,ytest= train_test_split(X,y,test_size=0.2)\n\ntrain_text=xtrain.values\ntest_text=xtest.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =====================================\n# Converting text data for model input\n# =====================================\n\nvectorizer = TfidfVectorizer(stop_words='english',max_features=4000)\nvectorizer.fit(tqdm(X))\ntrain_vec=vectorizer.transform(tqdm(train_text)).toarray()\ntest_vec=vectorizer.transform(tqdm(test_text)).toarray()\n\nVOCAB_SIZE=len(vectorizer.vocabulary_)\nFEATURES=len(train_vec[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===========================\n# Model\n# ===========================\n\nmodel=Sequential()\n    \nmodel.add(Dense(units=32, \n                input_shape=(FEATURES,), \n                kernel_regularizer=keras.regularizers.l2(0.001), \n                activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=64, \n                kernel_regularizer=keras.regularizers.l2(0.001), \n                activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=128,\n                kernel_regularizer=keras.regularizers.l2(0.001), \n                activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(3,activation='softmax'))\n    \nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n    \nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callback=keras.callbacks.EarlyStopping(monitor='loss',patience=2)\n\nhistory=model.fit(train_vec,ytrain,epochs=20,validation_split=0.3,shuffle=True,callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  \"Accuracy\"\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss,acc=model.evaluate(test_vec,ytest,verbose=False)\nprint(\"Loss:\",loss)\nprint(\"Accuracy:\",acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ==============================\n# Tuning Model with Keras Tuner\n# ==============================\nprint(\"Starting Keras Tuner \\n\")\n\nimport kerastuner as kt\nimport IPython\n\n\ndef build_model(hp):\n\n    model=Sequential()\n\n    model.add(Dense(units=hp.Int('units_0', min_value=32, max_value=128, step=32), \n                    input_shape=(FEATURES,), \n                    kernel_regularizer=keras.regularizers.l2(0.001), \n                    activation='relu'))\n    model.add(Dropout(0.2))\n\n    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32), \n                    kernel_regularizer=keras.regularizers.l2(0.001), \n                    activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=32),\n                    kernel_regularizer=keras.regularizers.l2(0.001), \n                    activation='relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(3,activation='softmax'))\n    \n    hp_learning_rate=hp.Choice('learning_rate',values=[1e-2,1e-3,1e-4])\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=keras.optimizers.RMSprop(learning_rate=hp_learning_rate),\n                  metrics=['accuracy'])\n\n    model.summary()\n    \n    return model\n\ntuner=kt.Hyperband(build_model,\n                  objective='val_accuracy',\n                  max_epochs=20,\n                  factor=3,\n                  directory='my_dir',\n                  project_name='project')\n\nclass CallBack(keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait=True)\n\ntuner.search(train_vec,\n             ytrain, \n             epochs=20,\n            validation_split=0.3,\n            verbose=0)\n\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete.\\n \nThe optimal number of units in the input densely-connected\nlayer is {best_hps.get('units_0')} \\n\nThe optimal number of units in the first densely-connected\nlayer is {best_hps.get('units_1')} \\n\nThe optimal number of units in the second densely-connected\nlayer is {best_hps.get('units_2')} \\n\nThe optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_t = tuner.hypermodel.build(best_hps)\nhistory_t=model_t.fit(train_vec,ytrain,epochs=20,validation_split=0.3,shuffle=True,callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  \"Accuracy\"\nplt.plot(history_t.history['accuracy'])\nplt.plot(history_t.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history_t.history['loss'])\nplt.plot(history_t.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss,acc=model_t.evaluate(test_vec,ytest)\nprint(loss,acc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}