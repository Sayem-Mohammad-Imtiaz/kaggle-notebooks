{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Recuperação de perguntas e respostas utilizando codificador universal de frases multilíngue","metadata":{"id":"0up_CL3u5rkX"}},{"cell_type":"markdown","source":"##### https://www.tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa","metadata":{"id":"gzQvdksl682Z"}},{"cell_type":"markdown","source":"Being able to automatically answer questions accurately remains a difficult problem in natural language processing. This dataset has everything you need to try your own hand at this task. Can you correctly generate the answer to questions given the Wikipedia article text the question was originally generated from?\n\nContent:\nThere are three question files, one for each year of students: S08, S09, and S10, as well as 690,000 words worth of cleaned text from Wikipedia that was used to generate the questions.\n\nThe \"questionanswerpairs.txt\" files contain both the questions and answers. The columns in this file are as follows:\n\nArticleTitle is the name of the Wikipedia article from which questions and answers initially came.\nQuestion is the question.\nAnswer is the answer.\nDifficultyFromQuestioner is the prescribed difficulty rating for the question as given to the question-writer.\nDifficultyFromAnswerer is a difficulty rating assigned by the individual who evaluated and answered the question, which may differ from the difficulty in field 4.\nArticleFile is the name of the file with the relevant article\n\nQuestions that were judged to be poor were discarded from this data set.\nThere are frequently multiple lines with the same question, which appear if those questions were answered by multiple individuals. https://www.kaggle.com/rtatman/questionanswer-dataset","metadata":{"id":"haHGBIVt6gS5"}},{"cell_type":"markdown","source":"### Solução","metadata":{"id":"B76aWJCFWgy4"}},{"cell_type":"markdown","source":"##### Cada resposta no dataset QA (Sxx_question_answer_pairs.txt) e seu contexto (o texto em torno da frase) em seu artigo correspondente (Sxx_setx_ax.txt.clean), são codificados em embeddings de alta dimensão com o response_encoder. Esses embeddings são armazenados em um índice construído usando a biblioteca simpleneighbors para recuperação de perguntas e respostas.","metadata":{"id":"6HTvivU22083"}},{"cell_type":"code","source":"!python -m pip uninstall -y spacy","metadata":{"id":"FyV5VRd78HPI","outputId":"4a094d91-42ae-47d7-cde7-e6289961f567"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m pip install -q tensorflow_text\n!python -m pip install -q simpleneighbors[annoy]\n!python -m pip install -U pip setuptools wheel\n!python -m pip install -U spacy[cuda102]\n!python -m spacy download en_core_web_lg","metadata":{"id":"W_9jjAKk8uum","outputId":"e5a2a4a3-6e0e-4a90-9020-dd2ac59514be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nprint(spacy.__version__)\nnlp = spacy.load('en_core_web_lg')","metadata":{"id":"C2VNBU748SzM","outputId":"fa3f04d9-03c2-4d95-dcbb-9d776aa4dbe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport glob\nimport re\nimport string\nimport os\nimport tqdm.notebook as tq\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow_text import SentencepieceTokenizer\nfrom simpleneighbors import SimpleNeighbors\nimport random\nimport pprint\nfrom IPython.display import HTML, display","metadata":{"id":"oxKtBdysta4_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extrair o **contexto** dos artigos Sxx_setx_ax.txt.clean","metadata":{"id":"hK5La8lw4ITt"}},{"cell_type":"code","source":"files = glob.glob('../input/questionanswer-dataset/text_data/*.clean')\n\nlist_text = []\nlist_file = []\n\nfor file in files:\n  with open(file, 'r', encoding = 'utf-8', errors='ignore') as f:\n    text = f.read()\n  list_text.append(re.sub(r'\\n+', ' ', text).strip())\n  list_file.append((os.path.basename(file)).split('.')[0])\ndf_context = pd.DataFrame({'Context': list_text, 'ArticleFile': list_file})","metadata":{"id":"MRMdggmHErwT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Criar dataframes de QA Sxx_question_answer_pairs.txt","metadata":{"id":"GMPARX6a4p9a"}},{"cell_type":"code","source":"df_08 = pd.read_table('../input/questionanswer-dataset/S08_question_answer_pairs.txt/S08_question_answer_pairs.txt')\ndf_09 = pd.read_table('../input/questionanswer-dataset/S08_question_answer_pairs.txt/S09_question_answer_pairs.txt')\ndf_10 = pd.read_table('../input/questionanswer-dataset/S08_question_answer_pairs.txt/S10_question_answer_pairs.txt', engine = 'python', error_bad_lines = False)\n\n#df_qa = pd.concat([df_08, df_09, df_10], axis=0, ignore_index=True)","metadata":{"id":"5c0hlpqFagq3","outputId":"ad1c24a4-3e69-4fd4-bbba-35549dbf0efd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_08.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'], axis = 1, inplace=True)\ndf_09.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'], axis = 1, inplace=True)\ndf_10.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'], axis = 1, inplace=True)","metadata":{"id":"HJ03H85y12_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_08.dropna(inplace=True)\ndf_09.dropna(inplace=True)\ndf_10.dropna(inplace=True)\nprint('-' * 15, df_08.isna().sum(), sep='\\n')\nprint('-' * 15, df_09.isna().sum(), sep='\\n')\nprint('-' * 15, df_10.isna().sum(), sep='\\n')","metadata":{"id":"Jff3WZ7jJBgD","outputId":"55eae1e1-279f-4c01-f354-284a658fa4cb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Limpar coluna \"Answer\"\ndef strip_last_punctuation(s):\n  if s and s[-1] in string.punctuation:\n    return s[:-1].strip()\n  else:\n    return s.strip()\n\n\ndf_08['answer_clean'] = df_08['Answer'].str.lower().map(strip_last_punctuation)\ndf_09['answer_clean'] = df_09['Answer'].str.lower().map(strip_last_punctuation)\ndf_10['answer_clean'] = df_10['Answer'].str.lower().map(strip_last_punctuation)\n\n# Remove os dados faltantes da base de treino\ndf_08.dropna(inplace=True)\ndf_09.dropna(inplace=True)\ndf_10.dropna(inplace=True)\nprint('-' * 15, df_08.isna().sum(), sep='\\n')\nprint('-' * 15, df_09.isna().sum(), sep='\\n')\nprint('-' * 15, df_10.isna().sum(), sep='\\n')","metadata":{"id":"lk55X4GmINJa","outputId":"8c0a57f0-7e3c-4bb8-feda-69bfe4e78904"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_USE08 = df_08.merge(df_context, on='ArticleFile')\ndf_USE09 = df_09.merge(df_context, on='ArticleFile')\ndf_USE10 = df_10.merge(df_context, on='ArticleFile')","metadata":{"id":"OnME4pBOBkx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('-' * 15, df_USE08.shape, sep='\\n')\nprint('-' * 15, df_USE09.shape, sep='\\n')\nprint('-' * 15, df_USE10.shape, sep='\\n')","metadata":{"id":"ILFD5BjT4eCh","outputId":"d4a9f23b-b68c-40d9-8c84-029aceac8160"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_USE08.drop_duplicates(subset=['answer_clean', 'Question'], keep='last', inplace = True)\ndf_USE09.drop_duplicates(subset=['answer_clean', 'Question'], keep='last', inplace = True)\ndf_USE10.drop_duplicates(subset=['answer_clean', 'Question'], keep='last', inplace = True)\n\nprint('-' * 15, df_USE08.shape, sep='\\n')\nprint('-' * 15, df_USE09.shape, sep='\\n')\nprint('-' * 15, df_USE10.shape, sep='\\n')","metadata":{"id":"0tj4x1KccKh8","outputId":"2a5360a4-5e71-4322-8343-9201e44462fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Carregar modelo **Universal Encoder Q&A Model** do tensorflow hub","metadata":{"id":"CmD0QsyFxvrl"}},{"cell_type":"code","source":"module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\"\nmodel = hub.load(module_url)","metadata":{"id":"WLBifqk_PFIu","outputId":"099ca86b-d5e9-4d15-dd70-2b44a2267ec0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Funções para testar o modelo - > decodificar a questão usando question_encoder e o embedding da questão é usado para consultar o índice simpleneighbors.","metadata":{"id":"5CBZGEI51Ygs"}},{"cell_type":"code","source":"def output_with_highlight(text, highlight):\n  output = \"<li> \"\n  i = text.find(highlight)\n  while True:\n    if i == -1:\n      output += text\n      break\n    output += text[0:i]\n    output += '<b>'+text[i:i+len(highlight)]+'</b>'\n    text = text[i+len(highlight):]\n    i = text.find(highlight)\n  return output + \"</li>\\n\"\n\ndef display_nearest_neighbors(query_text, answer_text=None):\n    query_embedding = model.signatures['question_encoder'](tf.constant([query_text]))['outputs'][0]\n    search_results = index.nearest(query_embedding, n=num_results)\n\n    if answer_text:\n      result_md = '''\n      <p>Random Question:</p>\n      <p>&nbsp;&nbsp;<b>%s</b></p>\n      <p>Answer:</p>\n      <p>&nbsp;&nbsp;<b>%s</b></p>\n      ''' % (query_text , answer_text)\n    else:\n      result_md = '''\n      <p>Question:</p>\n      <p>&nbsp;&nbsp;<b>%s</b></p>\n      ''' % query_text\n\n    result_md += '''\n      <p>Retrieved sentences :\n      <ol>\n    '''\n\n    if answer_text:\n      for s in search_results:\n        result_md += output_with_highlight(s, answer_text)\n    else:\n      for s in search_results:\n        result_md += '<li>' + s + '</li>\\n'\n\n    result_md += \"</ol>\"\n    display(HTML(result_md))","metadata":{"id":"2_8gjta9v_Vv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### S08_question_answer_pairs.txt","metadata":{"id":"PUnkwItruBPo"}},{"cell_type":"markdown","source":"#### Tranformar o dataframe em listas answer_context_S08 e question_S08","metadata":{"id":"06afi62juzk1"}},{"cell_type":"code","source":"answer_context_S08 = []\nquestion_S08 = []\n\nfor index, r in df_USE08.iterrows():\n  answer_context_S08.append([r['answer_clean'],r['Context']])\n  question_S08.append(r['Question'])","metadata":{"id":"tTHu61ENuIuE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_context_S08[100]","metadata":{"id":"Td-o_GnAU_14","outputId":"3f0af4b1-ea6e-4820-c668-34d66857ae3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Computar embeddings e criar o simpleneighbors index","metadata":{"id":"4Z2AI70ZvK20"}},{"cell_type":"code","source":"batch_size = 100\n\nencodings = model.signatures['response_encoder'](  input=tf.constant([answer_context_S08[0][0]]), context=tf.constant([answer_context_S08[0][1]]))\nindex = SimpleNeighbors(len(encodings['outputs'][0]), metric='angular')\n\nprint('Computing embeddings for %s sentences' % len(answer_context_S08))\nslices = zip(*(iter(answer_context_S08),) * batch_size)\nnum_batches = int(len(answer_context_S08) / batch_size)\nfor s in tq.tqdm_notebook(slices, total=num_batches):\n  response_batch = list([r for r, c in s])\n  context_batch = list([c for r, c in s])\n  encodings = model.signatures['response_encoder'](\n    input=tf.constant(response_batch),\n    context=tf.constant(context_batch)\n  )\n  for batch_index, batch in enumerate(response_batch):\n    index.add_one(batch, encodings['outputs'][batch_index])\n\nindex.build()\nprint('simpleneighbors index for %s sentences built.' % len(answer_context_S08))","metadata":{"id":"E1maRRL_vGdH","outputId":"6dcfd16a-f05e-4b62-e674-efa7fd2fbd66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Testar o modelo","metadata":{"id":"MUedyBGRv8bV"}},{"cell_type":"code","source":"#@title ###### Acessar os \"vizinhos próximos\" **nearest neighbors** para perguntas escolhidas randomicamente\nnum_results = 5 #@param {type:\"slider\", min:5, max:40, step:1}\n\nquery = random.choice(question_S08)\ndisplay_nearest_neighbors(query)","metadata":{"id":"M_WhMhsCwJuI","outputId":"f20f6852-2d04-49c7-8207-f38571feb390"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### S09_question_answer_pairs.txt","metadata":{"id":"3CoYYSHkzo6S"}},{"cell_type":"markdown","source":"#### Tranformar o dataframe em listas answer_context_S09 e question_S09","metadata":{"id":"RpIxaQIuzUbG"}},{"cell_type":"code","source":"answer_context_S09 = []\nquestion_S09 = []\n\nfor index, r in df_USE09.iterrows():\n  answer_context_S09.append((r['answer_clean'].lower(),r['Context'].lower()))\n  question_S09.append(r['Question'].lower())","metadata":{"id":"cSYGoMunz0BT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Computar embeddings e criar o simpleneighbors index","metadata":{"id":"E-Aj7pjB2qU9"}},{"cell_type":"code","source":"batch_size = 100\n\nencodings = model.signatures['response_encoder'](  input=tf.constant([answer_context_S09[0][0]]), context=tf.constant([answer_context_S09[0][1]]))\nindex = SimpleNeighbors(len(encodings['outputs'][0]), metric='angular')\n\nprint('Computing embeddings for %s sentences' % len(answer_context_S09))\nslices = zip(*(iter(answer_context_S09),) * batch_size)\nnum_batches = int(len(answer_context_S09) / batch_size)\nfor s in tq.tqdm_notebook(slices, total=num_batches):\n  response_batch = list([r for r, c in s])\n  context_batch = list([c for r, c in s])\n  encodings = model.signatures['response_encoder'](\n    input=tf.constant(response_batch),\n    context=tf.constant(context_batch)\n  )\n  for batch_index, batch in enumerate(response_batch):\n    index.add_one(batch, encodings['outputs'][batch_index])\n\nindex.build()\nprint('simpleneighbors index for %s sentences built.' % len(answer_context_S09))","metadata":{"id":"9YBxRFm609Uv","outputId":"36a93d8b-d9f7-4d59-ce3a-0f374c0cf7df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title ###### Acessar os \"vizinhos próximos\" **nearest neighbors** para perguntas escolhidas randomicamente\nnum_results = 5 #@param {type:\"slider\", min:5, max:40, step:1}\n\nquery = random.choice(question_S09)\ndisplay_nearest_neighbors(query)","metadata":{"id":"uU62KISM2OZS","outputId":"94173dbc-0fe3-4853-8ce0-d541f5f7c1c9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### S10_question_answer_pairs.txt","metadata":{"id":"6ipenFyhx-pj"}},{"cell_type":"markdown","source":"#### Tranformar o dataframe em listas answer_context_S10 e question_S10","metadata":{"id":"t4hMXP7at9k4"}},{"cell_type":"code","source":"answer_context_S10 = []\nquestion_S10 = []\n\nfor index, r in df_USE10.iterrows():\n  answer_context_S10.append((r['Answer'].lower(),r['Context'].lower()))\n  question_S10.append(r['Question'])","metadata":{"id":"_Rbzr7tKNrJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Computar embeddings e criar o **simpleneighbors** index","metadata":{"id":"1wT7dttqyEmO"}},{"cell_type":"code","source":"batch_size = 100\n\nencodings = model.signatures['response_encoder'](  input=tf.constant([answer_context_S10[0][0]]), context=tf.constant([answer_context_S10[0][1]]))\nindex = SimpleNeighbors(len(encodings['outputs'][0]), metric='angular')\n\nprint('Computing embeddings for %s sentences' % len(answer_context_S10))\nslices = zip(*(iter(answer_context_S10),) * batch_size)\nnum_batches = int(len(answer_context_S10) / batch_size)\nfor s in tq.tqdm_notebook(slices, total=num_batches):\n  response_batch = list([r for r, c in s])\n  context_batch = list([c for r, c in s])\n  encodings = model.signatures['response_encoder'](\n    input=tf.constant(response_batch),\n    context=tf.constant(context_batch)\n  )\n  for batch_index, batch in enumerate(response_batch):\n    index.add_one(batch, encodings['outputs'][batch_index])\n\nindex.build()\nprint('simpleneighbors index for %s sentences built.' % len(answer_context_S10))\n","metadata":{"id":"B21PE-ovxWDA","outputId":"2811340c-f8ea-407e-89e0-b4c1afc3020e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Testar o modelo","metadata":{"id":"XPDPm7fe1TcI"}},{"cell_type":"code","source":"#@title ###### Acessar os \"vizinhos próximos\" **nearest neighbors** para perguntas escolhidas randomicamente\nnum_results = 5 #@param {type:\"slider\", min:5, max:40, step:1}\n\nquery = random.choice(question_S10)\ndisplay_nearest_neighbors(query)","metadata":{"id":"-QNcQoUZxgzB","outputId":"66ad02b8-4f18-43c6-9ee9-e67cfbd4d24b"},"execution_count":null,"outputs":[]}]}