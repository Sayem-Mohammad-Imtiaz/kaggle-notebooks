{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\nimport seaborn as sns\nimport scipy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing the dataset from kaggle datasets\ndataset=pd.read_csv(\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View first five rows\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the given dataset, [sex,cp,fbs,restecg,exng,slp,caa,thall,output] this categorical variables are already encoded.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In below cell, i am taking only categorical variables for checking the value counts of each categorical variable.\n\nFrom that, we clearly knows the distribution of each label in the variable. ","metadata":{}},{"cell_type":"code","source":"data=dataset.drop(['age','trtbps','chol','thalachh','oldpeak'],axis=1)\nfor i in data:\n    print(i)\n    \n    print(dataset[i].value_counts())\n    print(\"------------\"*5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from above data we observe that,[ sex,fbs,exng] features are having some imbalance between the labels. So there is a chance to get a bias problem.","metadata":{}},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking only Contineous variables\ndata1=dataset.drop(['sex','cp','fbs','restecg','exng','slp','caa','thall','output'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.describe()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# info of the dataset\ndataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation","metadata":{}},{"cell_type":"markdown","source":"Checking correlation between the variables.","metadata":{}},{"cell_type":"code","source":"\ncorrPearson = dataset.corr(method=\"pearson\")\nfigure = plt.figure(figsize=(10,8))\nsns.heatmap(corrPearson,annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title(\"PEARSON\")\nplt.xlabel(\"COLUMNS\")\nplt.ylabel(\"COLUMNS\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* From above visualisation we observe that there was no correlation between the independent variables.","metadata":{}},{"cell_type":"markdown","source":"# Missing values","metadata":{}},{"cell_type":"code","source":"# Checking missing values\ndataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" * From above we observe that there are no missing values in the data.","metadata":{}},{"cell_type":"markdown","source":"# outliers","metadata":{}},{"cell_type":"code","source":"figure = plt.figure(figsize=(13,8))\n\nplt.boxplot(data1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* From above boxplot Visualisation we clearly see that having outliers in the data.\n* for removing outliers in below code i replaced the outliers with median.\n","metadata":{}},{"cell_type":"code","source":"for i in data1:\n    if (i=='trtbps' or i=='chol' or i=='thalachh' or i=='oldpeak'):\n        print(i)\n        q1=data1[i].quantile(0.25)\n        q3=data1[i].quantile(0.75)\n        iqr=q3-q1\n        low=q1-(0.5*iqr)\n        high=q3+(0.5*iqr)\n        data1[i][data1[i]<low]=data1[i].median()\n        data1[i][data1[i]>high]=data1[i].median()\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.boxplot(data1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* from above boxplot we can checkout clearly, the outliers are replaced with median.","metadata":{}},{"cell_type":"markdown","source":"# vizualisations","metadata":{}},{"cell_type":"code","source":"# we can check all the histogram visualisation\nfigure=plt.figure(figsize=(10,8))\ndataset.hist(figsize=(18,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output distribution\nsns.countplot('output',data=dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* By obseving the above vizualisation, we decide that the data should be in normal distribution and balanced. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='sex',hue='output',data=dataset)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* By above Visualisation, we obseve that mostly males are getting Heart Attack than Females.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(data = dataset, x = 'age', y = 'chol', hue = 'output', palette='dark', height = 10, s = 100, alpha = 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* the above visualisation represents between age and chol with output.\n* Mostly the age between 40-60 and chol between 200-270 persons are getting heart attack.","metadata":{}},{"cell_type":"code","source":"figure=plt.figure(figsize=(16,8))\nsns.countplot(x='age',hue='output',data=dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Mostly the persons are having the age between (41-54) getting the heart attack. ","metadata":{}},{"cell_type":"code","source":"figure=plt.figure(figsize=(22,8))\nsns.countplot(x='thalachh',hue='output',data=dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.groupby(['sex','output'])['chol'].mean()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* in above code we are grouping the sex and output with chol.\n* we clearly see that, sex 0(female) and output 0(not get heart attack) peoples are having the average chol is 241.29\n* sex 0(female) and output 1( get heart attack) peoples are having the average chol is 239.54\n* sex 1(male) and output 0(not get heart attack) peoples are having the average chol is 239.73\n*sex 1(male) and output 1(get heart attack) peoples are having the average chol is 236.13\n\n\n* similarlly in below also with different variables ","metadata":{}},{"cell_type":"code","source":"dataset.groupby(['sex','output'])['thalachh'].mean()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.groupby(['sex','output'])['age'].mean()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Skewness","metadata":{}},{"cell_type":"markdown","source":"Calculating skewness of the variables","metadata":{}},{"cell_type":"code","source":"for i in dataset:\n    print(i,' : ',scipy.stats.skew(dataset[i], axis=0, bias=True))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* we see that in dataset are having both positive and negative skewness.","metadata":{}},{"cell_type":"code","source":"# Dividing the dependent and independent variables.\n# Droping columns \ny=dataset['output']\nx=dataset.drop(['output','sex','fbs','exng'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All independent variables\nx.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformations","metadata":{}},{"cell_type":"markdown","source":"* In below i preformed some retuning techniques ","metadata":{}},{"cell_type":"code","source":"x['age']=x['age'].apply(lambda x: np.log(x))\nx['trtbps']=x['trtbps'].apply(lambda x: 1/x)\nx['chol']=x['chol'].apply(lambda x: 1/(x))\nx['thalachh']=x['thalachh'].apply(lambda x: (x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# scaling","metadata":{}},{"cell_type":"markdown","source":"* Here i applied MinMaxScaler, to bring the all variables between 0 to 1. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscale=MinMaxScaler()\nx=scale.fit_transform(x)\nx=pd.DataFrame(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# splitting","metadata":{}},{"cell_type":"markdown","source":"* Here splitting the data into training and testing. in this i taken 80% as training data and 20% as testing data Randomly selected. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict,GridSearchCV,StratifiedKFold\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=15,stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# modeling","metadata":{}},{"cell_type":"code","source":"# importing the all classification models from libraries\nfrom sklearn.tree import DecisionTreeClassifier   #importing model\nfrom sklearn.neighbors import KNeighborsClassifier #import method\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,recall_score,confusion_matrix,precision_score,classification_report, plot_confusion_matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In below, I build all classification models and finalised the best model from all models.","metadata":{}},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"classifier=XGBClassifier()\nclassifier.fit(x_train,y_train)\ny_pre=classifier.predict(x_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"\nclassifier = DecisionTreeClassifier(criterion = 'entropy') #creating algorithm\nclassifier.fit(x_train, y_train) #applying on model\ny_pred = classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"classifier=KNeighborsClassifier(n_neighbors=50)     #create algorithm \nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test) #predicting model\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic regression","metadata":{}},{"cell_type":"code","source":"classifier=LogisticRegression(random_state=10)\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Navie Bayes","metadata":{}},{"cell_type":"code","source":"classifier=GaussianNB()\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"classifier=RandomForestClassifier(n_estimators=1500,criterion='entropy')\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"classifier=SVC(kernel='poly')\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,scoring='accuracy').mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final model","metadata":{}},{"cell_type":"markdown","source":"Support vector machine(SVM) is the final model because of having the high recall score compare to the other models. \nRecall Score= 94%","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}