{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Part 1: Sentiment Analysis on Corona-Tweets"},{"metadata":{},"cell_type":"markdown","source":"### Imporing basic libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#To display plots inline; within the notebook window\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/coronavirus-covid19-tweets/2020-03-28 Coronavirus Tweets.CSV')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Countries with most tweets\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ndata = dataset[\"country_code\"].value_counts() \n# get x and y data \npoints = data.index[:20]\nfrequency = data.values[:20]\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Countries with most tweets\")\nplt.xlabel(\"Number of Tweets\")\nplt.ylabel(\"Country\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tweets from verfied/ normal account\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ndata = dataset[\"verified\"].value_counts() \n# get x and y data \npoints = data.index\nfrequency = data.values\n#plot graph\nax.barh(points,frequency)\nax.set_yticks([0,1,2])\nax.set_yticklabels([\"Normal\",\"Verified\"])\nplt.xlabel(\"Number of Tweets\")\nplt.ylabel(\"Type of Account\")\nplt.title(\"Tweets from verfied/ normal account\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Tweets existing/ new account\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ntemp_data = dataset.copy()\ntemp_data[\"account_created_at\"] = [i[:4] for i in temp_data[\"account_created_at\"]]\ndata = temp_data[\"account_created_at\"].value_counts() \n# get x and y data \npoints = data.index\nfrequency = data.values\n#plot graph\nax.barh(points,frequency)\nax.set_yticklabels(data.index)\nplt.xlabel(\"Number of Tweets\")\nplt.ylabel(\"Languages\")\nplt.title(\"Tweets existing/ new account\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tweets-languages\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ndata = dataset[\"lang\"].value_counts() \n# get x and y data \npoints = data.index[:20]\nfrequency = data.values[:20]\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Tweets-Languges\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing necessary libraries for language processing"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('vader_lexicon')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extract necessary columns from dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting English Tweets of all countries\ntweets= dataset[[\"text\",\"country_code\"]][dataset['lang'] == 'en'].reset_index()\ntweets.drop([\"index\"],axis=1)\n\n#maintain a copy of original tweets before processing it\ntweets_original = tweets.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Retaining only alphabets (removing all punctuations and numbers)\ntweets[\"text\"] = [re.sub('[^a-zA-Z]', ' ',i) for i in tweets[\"text\"]]\n\n#Converting into lower case \ntweets[\"text\"] = [i.lower() for i in tweets[\"text\"]]\n\n#Removing Emoticons\ndef deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\ntweets[\"text\"]  = [deEmojify(i) for i in tweets[\"text\"] ]\n\n\n#Removing URLs\ndef removeURLs(str):\n    ans = \"\"\n    clean_tweet1 = re.match('(.*?)http.*?\\s?(.*?)', str)\n    clean_tweet2 = re.match('(.*?)https.*?\\s?(.*?)', str)\n    if clean_tweet1:\n        ans=ans+clean_tweet1.group(1)\n        ans=ans+clean_tweet1.group(2)\n    elif clean_tweet2: \n        ans=ans+clean_tweet2.group(1)\n        ans=ans+clean_tweet2.group(2)\n    else:\n        ans = str\n    return ans\n\n\ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: removeURLs(tweet))\n\n\n#Removing Stop Words\ncachedStopWords = set(stopwords.words(\"english\"))\ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: ' '.join([word for word in tweet.split() if word not in cachedStopWords]))\n\n#Define words that we do not want to Stem or Lemmatize\nspecialWords = [\"coronavirus\", \"covid\",\"quarantine\",\"coronavirusoutbreak\",\"virus\",\"corona\",\"lockdown\"]\n\n#Stemming\nps = PorterStemmer()\ndef stemWords(word):\n    if word in specialWords:\n            return word\n    else:\n        return ps.stem(word)\n        \ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: ' '.join([stemWords(word) for word in tweet.split()]))\n\n\n#Lemmatization: \nwnl = WordNetLemmatizer()\ndef lemmatizeWords(word):\n    if word in specialWords:\n            return word\n    else:\n        return wnl.lemmatize(word)\ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: ' '.join([lemmatizeWords(word) for word in tweet.split()]))\n\n\n#Preparing corpus\ncorpus=[]\ncorpus = [word for tweet in tweets[\"text\"] for word in tweet.split()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Frequency Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Bag of Words\nfrom operator import itemgetter  \nvectorizer = CountVectorizer(max_features = 30)\ncv = vectorizer.fit_transform(tweets[\"text\"]).toarray()\nterms = vectorizer.get_feature_names()\nfreqs = cv.sum(axis=0)\nresult = dict(zip(terms, freqs))\nprint(result)\nfeatures = []\nvals = []\nfor key, value in sorted(result.items(), key = itemgetter(1), reverse = True):\n    features.append(key)\n    vals.append(value)\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = features\nfrequency = vals\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Tweeted Words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Using Term Frequency (TF-IDF)\n\ntf=TfidfTransformer(smooth_idf=True,use_idf=True)\ntf.fit(cv)\n\n# print idf values\ndata = { \"Word\" :vectorizer.get_feature_names(), \"idf_weights\":tf.idf_}\ndf_idf = pd.DataFrame(data)\n \n# sort ascending\ndf_idf = df_idf.sort_values(by=['idf_weights'])\n\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = df_idf[\"Word\"]\nfrequency = df_idf[\"idf_weights\"]\n#plot graph\nax.barh(points,frequency)\nplt.title(\"IDF weights of Most Tweeted Words -> Least weights = Most Importance\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(\n    background_color='black',\n    max_words=50,\n    max_font_size=40, \n    scale=5,\n    random_state=1,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(corpus))\n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor=\"None\") \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.title(\"WordCloud of Corona-Tweets\")\nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"s = SentimentIntensityAnalyzer()\nscores = tweets[\"text\"].apply(lambda tweet: s.polarity_scores(tweet))\nscores_df = pd.DataFrame(list(scores))\nscores_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating sentiments for all tweets\nscores_df['result'] = scores_df['compound'].apply(lambda res: 'neutral' if res == 0 else ('positive' if res > 0 else 'negative'))\nscores_df['tweet'] = tweets_original[\"text\"]\nscores_df[\"country_code\"] = tweets_original[\"country_code\"]\nscores_df= scores_df.sort_values(by=['compound'])\n\n\n#Calculating sentiments wrt top 5 tweeting countries\nus = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'US']\nus_positive = us[[\"tweet\",\"result\",\"country_code\"]][us.result == 'positive']\nscore_us_positive = us_positive.shape[0]\nus_negative = us[[\"tweet\",\"result\",\"country_code\"]][us.result == 'negative']\nscore_us_negative = us_negative.shape[0]\nus_neutral = us[[\"tweet\",\"result\",\"country_code\"]][us.result == 'neutral']\nscore_us_neutral = us_neutral.shape[0]\n\nindia = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'IN']\nindia_positive = india[[\"tweet\",\"result\",\"country_code\"]][india.result == 'positive']\nscore_india_positive = india_positive.shape[0]\nindia_negative = india[[\"tweet\",\"result\",\"country_code\"]][india.result == 'negative']\nscore_india_negative = india_negative.shape[0]\nindia_neutral = india[[\"tweet\",\"result\",\"country_code\"]][india.result == 'neutral']\nscore_india_neutral = india_neutral.shape[0]\n\nuk = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'GB']\nuk_positive = uk[[\"tweet\",\"result\",\"country_code\"]][uk.result == 'positive']\nscore_uk_positive = uk_positive.shape[0]\nuk_negative = uk[[\"tweet\",\"result\",\"country_code\"]][uk.result == 'negative']\nscore_uk_negative = uk_negative.shape[0]\nuk_neutral = uk[[\"tweet\",\"result\",\"country_code\"]][uk.result == 'neutral']\nscore_uk_neutral = uk_neutral.shape[0]\n\nspain = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'ES']\nspain_positive = spain[[\"tweet\",\"result\",\"country_code\"]][spain.result == 'positive']\nscore_spain_positive = spain_positive.shape[0]\nspain_negative = spain[[\"tweet\",\"result\",\"country_code\"]][spain.result == 'negative']\nscore_spain_negative = spain_negative.shape[0]\nspain_neutral = spain[[\"tweet\",\"result\",\"country_code\"]][spain.result == 'neutral']\nscore_spain_neutral = spain_neutral.shape[0]\n\ncanada = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'CA']\ncanada_positive = canada[[\"tweet\",\"result\",\"country_code\"]][canada.result == 'positive']\nscore_canada_positive = canada_positive.shape[0]\ncanada_negative = canada[[\"tweet\",\"result\",\"country_code\"]][canada.result == 'negative']\nscore_canada_negative = canada_negative.shape[0]\ncanada_neutral = canada[[\"tweet\",\"result\",\"country_code\"]][canada.result == 'neutral']\nscore_canada_neutral = canada_neutral.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Analysis of Tweets around the world"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\ndata = scores_df[\"result\"].value_counts()\n# get x and y data \npoints = data.index\nfrequency = data.values\n#plot graph\nax.bar(points,frequency)\nplt.title(\"Sentiment Analysis of Tweets around the world\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Analysis in top 5 tweeting countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [[score_us_positive,score_india_positive, score_uk_positive, score_spain_positive, score_canada_positive],\n[score_us_negative,score_india_negative, score_uk_negative, score_spain_negative, score_canada_negative],\n[score_us_neutral,score_india_neutral, score_uk_neutral, score_spain_neutral, score_canada_neutral]]\nX = np.arange(5)\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.set_xticks([0,1,2,3,4,5])\nax.set_xticklabels([\"USA\",\"India\",\"UK\",\"Spain\",\"Canada\"])\nax.bar(X + 0.00, data[0], color = 'g', width = 0.25, label=\"positive\")\nax.bar(X + 0.25, data[1], color = 'r', width = 0.25, label=\"negative\")\nax.bar(X + 0.50, data[2], color = 'b', width = 0.25, label=\"neutral\")\nplt.title(\"Sentiment Analysis in top 5 tweeting countries\")\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_word_list=[]\nneu_word_list=[]\nneg_word_list=[]\npos_word_weight=[]\nneu_word_weight=[]\nneg_word_weight=[]\n\ndef get_word_sentiment(text):\n    \n    tokenized_text = nltk.word_tokenize(text)\n    #print(tokenized_text)    \n\n    for word in tokenized_text:\n            if (s.polarity_scores(word)['compound']) >= 0.6:\n                pos_word_list.append(word)\n                pos_word_weight.append(s.polarity_scores(word)['compound'])\n            elif (s.polarity_scores(word)['compound']) <= -0.6:\n                neg_word_list.append(word)\n                neg_word_weight.append(s.polarity_scores(word)['compound'])\n            else:\n                neu_word_list.append(word)\n                neu_word_weight.append(s.polarity_scores(word)['compound'])\n\nfor tweet in tweets[\"text\"]:\n    get_word_sentiment(tweet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Positive Words in Tweets:',len(pos_word_list))\nprint('Total Negative Words in Tweets:',len(neg_word_list))\nprint('Total Neutral Words in Tweets:',len(neu_word_list)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_word_list = list(set(pos_word_list))\nneg_word_list = list(set(neg_word_list))\nneu_word_list = list(set(neu_word_list))\n\nresult_pos = dict(zip(pos_word_list, pos_word_weight))\nresult_neg = dict(zip(neg_word_list, neg_word_weight))\nresult_neu = dict(zip(neu_word_list, neu_word_weight))\n\nfeatures_pos = []\nvals_pos = []\n\nfeatures_neg = []\nvals_neg = []\n\nfeatures_neu = []\nvals_neu = []\n\nfor key, value in sorted(result_pos.items(), key = itemgetter(1), reverse = True):\n    features_pos.append(key)\n    vals_pos.append(value)\nfor key, value in sorted(result_neg.items(), key = itemgetter(1), reverse = True):\n    features_neg.append(key)\n    vals_neg.append(value)\nfor key, value in sorted(result_neu.items(), key = itemgetter(1), reverse = True):\n    features_neu.append(key)\n    vals_neu.append(value)\n\ntop_positive_words = features_pos[:10]\ntop_positive_words_freq = vals_pos[:10]\n\ntop_negative_words = features_neg[:10]\ntop_negative_words_freq = vals_neg[:10]\n\ntop_neutral_words = features_neu[:10]\ntop_neutral_words_freq = vals_neu[:10]\n\n\nprint(top_positive_words)\nprint(top_negative_words)\nprint(top_neutral_words)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = top_positive_words\nfrequency = top_positive_words_freq\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Positive Words vs their Compound Score\")\nplt.xlabel(\"Compound Score\")\nplt.ylabel(\"Positive Words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = top_negative_words\nfrequency = top_negative_words_freq\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Negative Words vs their Compound Score\")\nplt.xlabel(\"Compound Score\")\nplt.ylabel(\"Negative Words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = top_neutral_words\nfrequency = top_neutral_words_freq\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Neutral Words vs their Compound Score\")\nplt.xlabel(\"Compound Score\")\nplt.ylabel(\"Neutral Words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2: WHO Corona-Tweets Summarization"},{"metadata":{},"cell_type":"markdown","source":"### Importing the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"who_dataset = pd.read_csv('../input/who-tweet-data/WHO__tweet_data.csv')\nwho_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Retaining only alphabetnumerics\nwho_dataset[\"Tweet\"] = [re.sub('[^a-zA-Z0-9]', ' ',i) for i in who_dataset[\"Tweet\"]]\n\n#Converting into lower case \nwho_dataset[\"Tweet\"] = [i.lower() for i in who_dataset[\"Tweet\"]]\n\n#Removing Emoticons\ndef deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\nwho_dataset[\"Tweet\"]  = [deEmojify(i) for i in who_dataset[\"Tweet\"]]\n\n#Removing URLs\ndef removeURLs(str):\n    ans = \"\"\n    clean_tweet1 = re.match('(.*?)http.*?\\s?(.*?)', str)\n    clean_tweet2 = re.match('(.*?)https.*?\\s?(.*?)', str)\n    if clean_tweet1:\n        ans=ans+clean_tweet1.group(1)\n        ans=ans+clean_tweet1.group(2)\n    elif clean_tweet2: \n        ans=ans+clean_tweet2.group(1)\n        ans=ans+clean_tweet2.group(2)\n    else:\n        ans = str\n    return ans\nwho_dataset[\"Tweet\"] = who_dataset[\"Tweet\"].apply(lambda tweet: removeURLs(tweet))\n\n\nsentences = [i for i in who_dataset[\"Tweet\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the word frequency table"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a word frequency table from every sentence.\n\ndef create_frequency_table(text_string) -> dict:\n    \n    #Remove all the stop words\n    stopWords = set(stopwords.words(\"english\"))\n    #Tokenise the sentence into words\n    words = word_tokenize(text_string)\n    #Stem the words to get the root word\n    ps = PorterStemmer()\n    \n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Term Frequency Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Score a sentence by its words, adding the frequency of every non-stop word in a sentence.\n#First 10 chars of each sentence is used as key instead of using whole sentence; to prevent memory overload.\n\ndef score_sentences(sentences, freqTable) -> dict:\n    \n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        for word in freqTable:\n            if word in sentence:\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[word]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[word]\n\n        #To prevent large sentences from dominating, we divide the score of a sentences by total number of words in it\n        if word_count_in_sentence > 0:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence\n\n    return sentenceValue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find the Threshold Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Consider the average score of the sentences as a threshold\n\ndef find_average_score(sentenceValue) -> int:\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original text\n    average = int(sumValues / len(sentenceValue))\n\n    return average","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate Summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select a sentence for summarization, only if the sentence score is more than the threshold score\n\ndef generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = []\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > (threshold):\n            summary.append(sentence)\n            sentence_count += 1\n\n    return summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Driver Function for WHO corona-tweet summarizarion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 Create the word frequency table\nfreq_table = create_frequency_table('. '.join(sentences))\n\n# 2 Important Algorithm: score the sentences\nsentence_scores = score_sentences(sentences, freq_table)\n\n# 3 Find the threshold\nthreshold = find_average_score(sentence_scores)\n\n# 4 Important Algorithm: Generate the summary\nsummary = generate_summary(sentences, sentence_scores, 1.5 * threshold)\n\n#5 ignore repetitive sentences in summary\ncountSummary = 0\nsummaryText = []\nfor i in summary:\n    if(summary.count(i)==1):\n        #Trimming extra white spaces\n        i = re.sub(' +', ' ', i)\n        \n        countSummary+=1\n        summaryText.append(i)\n        \n        print(str(countSummary)+\")  \"+i+\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"Number of Tweets by WHO: \"+str(len(sentences)))\nprint(\"Number of Sentences in Summary: \"+str(countSummary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":4}