{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I.importation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#algèbre linéaire\nimport numpy as np\n\n#transforamtion des données et importation depuis le csv\nimport pandas as pd\n\n#repésentation des résultats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Machine learning\nfrom sklearn import preprocessing\nimport tensorflow as tf\nimport statsmodels as st\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.seasonal import STL\nfrom sklearn.model_selection import train_test_split\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II.fonction utile"},{"metadata":{"trusted":true},"cell_type":"code","source":"#14578001(Deauville)\nstation_meteo = 14578001\n#22005003(Belle-Isle-en-Terre(Côtes-d'armor))\ntest_station_meteo = 22005003","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normaliser(dataset, parametre, parametre_unique=False):\n    if parametre_unique:\n        dataNorm = dataset\n        dataNorm[parametre]=((dataset[parametre]-dataset[parametre].min())/(dataset[parametre].max()-dataset[parametre].min()))\n        return dataNorm\n    else:\n        dataNorm=((dataset-dataset.min())/(dataset.max()-dataset.min()))\n#         dataNorm[target]=dataset[target]\n        return dataNorm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dénormaliser(dataset,dataset_origine,parametre, parametre_unique=False):\n    if parametre_unique:\n        dataNorm = dataset\n        dataNorm[parametre]=((dataset[parametre])*(dataset_origine[parametre].max()-dataset_origine[parametre].min())+(dataset_origine[parametre].min()))\n        return dataNorm\n    else:\n        dataNorm=((dataset)*(dataset_origine.max()-dataset_origine.min())+(dataset_origine.min()))\n#       dataNorm[target]=dataset[target]\n        return dataNorm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def segment(dataset, variable, intervale = 5000, futur = 0):\n    data = []\n    labels = []\n    for i in range(len(dataset)):\n        debut_index = i\n        fin_index = i + intervale\n        futur_index = i + intervale + futur\n        if futur_index >= len(dataset):\n            break\n        data.append(dataset[variable][i:fin_index])\n        labels.append(dataset[variable][fin_index:futur_index])\n    return np.array(data), np.array(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def temp_int(longeur):\n    return list(range(-longeur, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_multi_etape(historique, vrai_futur, prediction):\n    plt.figure(figsize=(12, 6))\n    num_passe = temp_int(len(historique))\n    num_futur = len(vrai_futur)\n    plt.plot(num_passe, np.array(historique[:, 0]), label='Passé donné')\n    plt.plot(np.arange(num_futur), np.array(vrai_futur), label='Véritable futur')\n    if prediction.any():\n        plt.plot(np.arange(num_futur), np.array(prediction), 'ro', label='Futur prédis')\n    plt.legend(loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III.Transformation des données"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importation de notre dataset sous panda\ndf2016 = pd.read_csv(r'/kaggle/input/meteonet/NW_Ground_Stations/NW_Ground_Stations/NW_Ground_Stations_2016.csv')\ndf2017 = pd.read_csv(r'/kaggle/input/meteonet/NW_Ground_Stations/NW_Ground_Stations/NW_Ground_Stations_2017.csv')\ndf2018 = pd.read_csv(r'/kaggle/input/meteonet/NW_Ground_Stations/NW_Ground_Stations/NW_Ground_Stations_2018.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def index(station):\n    data = df2016[(df2016['number_sta'] == station)]\n    data = data.append(df2017[(df2017['number_sta'] == station)], ignore_index=True)\n    data = data.append(df2018[(df2018['number_sta'] == station)], ignore_index=True)\n    data['date'] = pd.to_datetime(data['date'], format='%Y%m%d %H:%M')\n    data.set_index('date', inplace=True)\n    data['td'] = data['td'].interpolate('linear')\n    data['precip'] = data['precip'].interpolate('linear')\n    data['hu'] = data['hu'].interpolate('linear')\n    data['ff'] = data['ff'].interpolate('linear')\n    data = data.drop(['number_sta', 'lat', 'lon', 'height_sta'], axis = 1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#création de nos données\nmeteo_non_norm=index(station_meteo)\nmeteo_test_non_norm=index(test_station_meteo)\n\n#normalisation pour accélerer le calcul\nmeteo = normaliser(meteo_non_norm, 'td', parametre_unique=False)\nmeteo_test = normaliser(meteo_test_non_norm, 'td', parametre_unique=False)\n\n#on prend la moyenne sur des périodes de 720 messures(5 min) = 12 heures\nmeteo_cp = meteo.resample('720T').mean()\nmeteo_test_cp = meteo_test.resample('720T').mean()\n\n# on considère la température constante si on a pas de données :\nmeteo_cp = meteo_cp.fillna(method='bfill')\nmeteo_test_cp = meteo_test_cp.fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IV.Création du modèle"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection du nombre de point donné et ceux à prédir\npoint_hist = 100\npoint_predis = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prépare des sacs de données suivant la sélection et imprime la Dimension des données (permet de voir si le volume de données est trop faible ou trop grand)\ndef segmentation(dataset,parametre):\n    X, Y = segment(dataset, parametre, intervale = point_hist, futur = point_predis)\n    X = X.reshape(X.shape[0], point_hist, 1)\n    Y = Y.reshape(Y.shape[0], point_predis, 1)\n    print(\"Dimension du Passé: \", X.shape)\n    print(\"Dimension du Futur: \", Y.shape)\n    return X,Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_entrainement,Y_entrainement=segmentation(meteo_cp,'td')\nX_test,Y_test=segmentation(meteo_test_cp,'td')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Nombre d'époque d'entrainement(résultat satisfaisant vers 200)\nEPOQUE= 200\n\n#modèle du réseaux de neurones(4 rangées (100,100,50,50) dont la première LSTM)\nmodele_lstm = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(point_hist, input_shape=X_entrainement.shape[-2:]),\n    tf.keras.layers.Dense(100),\n    tf.keras.layers.Dense(50),\n    tf.keras.layers.Dense(point_predis)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Configuration du modèle(on minimise avec la méthode des moindres carrés)\nmodele_lstm.compile(optimizer='adam', metrics=['mae'], loss='mse')\n\n#Lance l'entrainement du modèle\nmodele_lstm.fit(X_entrainement, Y_entrainement, epochs=EPOQUE)\n\n#Prédis le paramètre sur la station test\nYPred = modele_lstm.predict(X_test, verbose=0)\nY_test = Y_test.reshape(Y_test.shape[0], point_predis,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# V.Affichage des résultats"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Listes contenant les valeurs des prédictions\nListe_finale = []\nValeurs_liste = []\n\n#Sur les 50 valeurs prédites on en prend une(ici 40 donc 40*)pour chaque segment sur les 3 ans \nfor i in YPred:\n    Liste_finale.append(i[40])\n\nnp_array = np.array(Liste_finale)\nprint(np_array.shape)\n\nfor i in Y_test:\n    Valeurs_liste.append(i[40])\n    \nval_np_array = np.array(Valeurs_liste)\nval_pd_array = pd.DataFrame(val_np_array)\n#denormaliser(val_pd_array,meteo_non_norm,'td',parametre_unique=False)\n\nprint(val_np_array.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Première figure: La prédiction comparé au réel sur les 3 ans avec \nplt.figure(figsize=(30,5))\nsns.set(rc={\"lines.linewidth\": 3})\nsns.lineplot(x=np.arange(val_np_array.shape[0]), y=val_np_array, color=\"green\")\nsns.set(rc={\"lines.linewidth\": 3})\nsns.lineplot(x=np.arange(np_array.shape[0]), y=np_array, color=\"coral\")\nplt.margins(x=0, y=0.5)\nplt.legend([\"Original\", \"Prédiction\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deuxième figure: Une prediction particulière avec les 100 données et les 50 prédictions.\nplot_multi_etape(X_test[1336], Y_test[1336], YPred[1336])\n#Aucune échelle est bonne il faut faire l'inverse de la noramalisation si on veut une bonne échelle.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}