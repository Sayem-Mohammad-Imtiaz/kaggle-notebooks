{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Context**\n\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read data\ndata = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',\n                   encoding='ISO-8859-1', \n                   usecols=['v1', 'v2'])\ndata.rename(columns={'v1':'labels', 'v2':'content'}, inplace=True)\n\nprint(\"data shape: \", data.shape)\ndisplay(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#distribution of the labels to check for imbalance\nsns.countplot(data.labels);\ndata['labels'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an imbalanced dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Example for a ham content\nham_indices = data[data.labels == 'ham'].index\nrandom_ham = np.random.choice(ham_indices)\n\ndata.iloc[random_ham, :].content","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Example for a spam content\nspam_indices = data[data.labels == 'spam'].index\nrandom_ham = np.random.choice(spam_indices)\n\ndata.iloc[random_ham, :].content","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wordcount plots**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(8,12))\nwordcloud = WordCloud(max_words=100).\\\n            generate(' '.join(data.loc[data['labels'] == 'ham', 'content'].to_list()))\nplt.imshow(wordcloud)\nplt.title('ham words');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(8,12))\nwordcloud = WordCloud(max_words=100).\\\n            generate(' '.join(data.loc[data['labels'] == 'spam', 'content'].to_list()))\nplt.imshow(wordcloud)\nplt.title('spam words');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**top 30 words for spam**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nspam_text = ' '.join(data.iloc[spam_indices]['content'])\nspam_list = spam_text.lower().split()\n\ncnt = Counter()\n\nfor word in spam_list:\n    if word not in stopwords.words('english'):\n        cnt[word] += 1\n        \ncnt.most_common(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top 30 words for ham**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_text = ' '.join(data.iloc[ham_indices]['content'])\nham_list = ham_text.lower().split()\n\ncnt = Counter()\n\nfor word in ham_list:\n    if word not in stopwords.words('english'):\n        cnt[word] += 1\n        \ncnt.most_common(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Removing too small sentences or too large ones","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count = data['content'].apply(lambda s: len(s.split()))\nword_count.plot.hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count[(word_count < word_count.quantile(0.98)) & (word_count > word_count.quantile(0.02))].plot.hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[(word_count < word_count.quantile(0.98)) & (word_count > word_count.quantile(0.02))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removed a bit over 200 sentences\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**TFIDF**\n\nA method that will help us convert text data to numerical data. This method assigns weights for each document term, taking into consideration the frequency of a term in a document and the frequency of a term across all documents.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = TfidfVectorizer(max_features=5000, stop_words='english')\nsparse_mat = cv.fit_transform(data['content'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = sparse_mat.toarray()\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print some of the features\nprint(cv.get_feature_names()[1000:1005])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**splitting data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data\nfrom sklearn.model_selection import train_test_split\ny = data['labels']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boosting\n\nA procedure that combines the outputs of many \"weak classifiers(or tree stumps) to produce a powerful committee. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nada_model = AdaBoostClassifier( n_estimators=100)\nada_model.fit(X_train, y_train)\n\nada_preds = ada_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n#printing metrics\nprint(\"Accuracy score: \", accuracy_score(y_test, ada_preds))\nprint(\"Confusion matrix: \\n\",confusion_matrix(y_test, ada_preds))\nprint(\"Classification report: \\n\",classification_report(y_test, ada_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**top 10 important word/features in our Adaboost model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importance_index = np.argsort(ada_model.feature_importances_)[-10:]\n\nprint(\"10 top important words: \\n\", pd.Series(cv.get_feature_names())[feat_importance_index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting top 3 tree stumps(weak classifiers)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top_3 = np.argsort(ada_model.estimator_weights_)[-3:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tree(ada_model.estimators_[top_3[0]], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, feature_names=cv.get_feature_names());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tree(ada_model.estimators_[top_3[1]], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, feature_names=cv.get_feature_names());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tree(ada_model.estimators_[top_3[2]], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, feature_names=cv.get_feature_names());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bagging","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\nmodel_rf = RandomForestClassifier(n_estimators=100)\nmodel_rf = model_rf.fit(X_train, y_train)\n#Predicting\ny_rf_pred = model_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print metrics\nprint('accuracy_score: ', accuracy_score(y_test, y_rf_pred))\nprint('confusion matrix: \\n', confusion_matrix(y_test, y_rf_pred))\nprint('report: \\n', classification_report(y_test, y_rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**top 10 important word/features in our Random Forest model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importance_index = np.argsort(model_rf.feature_importances_)[-10:]\n\nprint(\"10 top important words: \\n\", pd.Series(cv.get_feature_names())[feat_importance_index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the first decision tree in the RF model.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(35,15))\nplot_tree(model_rf.estimators_[0], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, max_depth=5, \n               feature_names=cv.get_feature_names());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipelines\n\nThe advanced way","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['content'], data['labels'], stratify=data['labels'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adaboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(steps=[\n                ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n                ('model', AdaBoostClassifier(n_estimators=100))])\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(steps=[\n                ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n                ('model', RandomForestClassifier(n_estimators=100))])\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will only continue with Random Forest.** \n\n1) I will to use TruncatedSVD to reduce the dimensions. TSVD is good for sparse data whereas PCA is good for dense data.\n\n2) I will try oversampling methods since our data is imbalanced. These methods may improve the f1-score and the recall. I will use SMOTE and RandomOverSampler","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Part 1 - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\npipeline = Pipeline(steps=[\n                ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n                ('dim_reduction', TruncatedSVD(n_components=100)),\n                ('model', RandomForestClassifier(n_estimators=100))])\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Part 2- \n\nover-sampling is simply a process of repeating some samples of the minority class and balance the number of samples between classes in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(\n                    TfidfVectorizer(max_features=5000, stop_words='english'),\n                    RandomOverSampler(),\n                    RandomForestClassifier(n_estimators=100))\n\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))\nprint(confusion_matrix(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we improved the recall for our minority class(spam) and the f1-score.\n\n**accruacy score is not the right metric when we have imbalance in the data**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}