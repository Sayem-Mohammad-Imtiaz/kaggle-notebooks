{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 1"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#read data\ndf_true = pd.read_csv(r'/kaggle/input/fake-and-real-news-dataset/True.csv')\ndf_fake = pd.read_csv(r'/kaggle/input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding column\ndf_true['RealNews?'] = True\ndf_fake['RealNews?'] = False\ndf = df_true.append(df_fake)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Aggregation + Transformation\ndf['document'] = df[['title', 'text']].agg(' '.join, axis=1)\ndf['document'] = df['document'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Task Done\n\n* Cleaning of text \n* Get the frequency of each word in both classes\n* Cal P(real) = real / (real + fake) P(fake) = 1 - P(real)\n* Cal total Vocabulary count,each class each word wordCount\n* P(w|c==real) = wc(w|c==real) + 1 / (wc(w|c==real) + (real_vocab_count))\n* P(w|c==fake) = wc(w|c==fake) + 1 / wc(w|c==fake) + (fake_vocab_count)\n* Store the prob of all words in vocab with p(w|real) and p(w|fake)\n\n#Then when a test document comes, do initial preprocessing and do compute \n* P(doc|real) = log(P(w1|real)) + log(P(w1|real)) + log(P(w1|real)) + ....log(P(wn|real)) + log(P(real))\n* P(doc|fake) = log(P(w1|fake)) + log(P(w2|fake)) + log(P(w1|fake)) + ....log(P(wn|fake)) + log(P(fake))\n\n* Compare P(doc|real) and P(doc|fake) and assign respective labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\n\n\nclass WordCount:\n    \n    \n    def __init__(self,df): \n        \n        \n        self.df = df\n        self.__dict_real = {}\n        self.__dict_fake = {}\n        self.__total_vocab = set()\n        self.__word_prob = {}\n        self.p_real = 0\n        self.p_fake = 0\n    \n    \n    \n    def predict(self,document):\n        \n        \n        document = document.lower()\n        words = re.split(r'\\W+',document)\n        \n        log_of_real = np.log(self.p_real)\n        log_of_fake = np.log(self.p_fake)\n        \n        for word in words:\n            if(word in self.__word_prob):\n                real_prob,fake_prob = self.__word_prob[word]\n                log_of_real += np.log(real_prob)\n                log_of_fake += np.log(fake_prob)\n        \n        \n        if(log_of_real > log_of_fake):\n            return 'Real News'\n            \n        return 'Fake News'\n            \n    \n    def computeProbablities(self):\n        \n        total_nc = len(self.df)\n        \n        \n        #Prior class prob\n        self.p_real = len(self.df[self.df['RealNews?']]) / total_nc\n        self.p_fake = 1 - self.p_real\n        \n    \n        sum_wc_real = sum(self.__dict_real.values())\n        sum_wc_fake = sum(self.__dict_fake.values())\n        \n        alpha = 1\n        \n        #vocab_count = len(self.__total_vocab)\n        \n        for word in self.__total_vocab:\n            \n            wc_r = self.__dict_real[word] if(word in self.__dict_real)  else 0\n            wc_f = self.__dict_fake[word] if (word in self.__dict_fake) else 1\n        \n            p_wc_real = (wc_r + alpha) / (sum_wc_real + len(self.__dict_real))\n            p_wc_fake = (wc_f + alpha) / (sum_wc_fake + len(self.__dict_fake))\n        \n            self.__word_prob[word] = (p_wc_real,p_wc_fake)\n    \n                 \n    def wordCount(self):\n        \n        \n        \n        def processEachLine(line,type_):\n            \n            words = re.split(r'\\W+',line)\n            \n            if(type_ == 'Real'):\n                \n                \n                for word in words:\n                    \n                    self.__total_vocab.add(word)\n                    \n                    #Storing word count in real dict\n                    if(word in self.__dict_real):\n                        self.__dict_real[word] +=1\n                        \n                    else:\n                        self.__dict_real[word] = 1\n    \n            \n            else:\n                for word in words:\n                    \n                    #Adding word to set\n                    self.__total_vocab.add(word)\n                    \n                    #Storing word count in fake dict\n                    if(word in self.__dict_fake):\n                        self.__dict_fake[word] +=1\n                        \n                    else:\n                        self.__dict_fake[word] = 1\n            \n            \n            \n        df_real = self.df[self.df['RealNews?']]\n        df_fake = self.df[~self.df['RealNews?']]\n        \n        \n        for i in range(len(df_real)):\n            processEachLine(df_real['document'].iloc[i],'Real')\n            \n        for i in range(len(df_fake)):\n            processEachLine(df_fake['document'].iloc[i],'Fake')\n            \n        \n        #print(len(self.__dict_real))\n        #print(len(self.__dict_fake))\n        #print(len(self.__total_vocab))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCount(df_train)\nwc.wordCount()\nwc.computeProbablities()\nprint('Completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction\n\ncount_true = 0\nfalse_pos = 0\nfalse_neg = 0\ntrue_pos = 0\ntrue_neg  =0 \n\nfor i in range(len(df_test)):\n    \n    line = df_test['document'].iloc[i]\n    prediction = wc.predict(line) \n    \n    ground_truth = 'Real News' if(df_test['RealNews?'].iloc[i]) else 'Fake News'\n    \n    if(ground_truth == 'Real News' and prediction == 'Fake News'):\n        false_pos +=1\n        \n    elif(ground_truth == 'Fake News' and prediction == 'Real News'):\n        false_neg +=1\n        \n    if(ground_truth == prediction):\n        \n        if(prediction == 'Real News'):\n            true_pos +=1\n            \n        else:\n            true_neg +=1\n            \n\naccuracy = (true_pos + true_neg) / len(df_test) * 100\nprecision = true_pos / (true_pos + false_pos) * 100\nrecall = true_pos / (true_pos + false_neg) * 100\nf1 = (2 / (1/recall + 1/precision))\nprint('Accuracy ',accuracy)\nprint('Precision ',precision)\nprint('Recall ',recall)\nprint('F1 ',f1)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom collections import Counter\nfrom numba import jit, cuda,jitclass\nfrom timeit import default_timer as timer \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Each document is preprocessed with removal of single character, multiple spaces, converted to lowercase and got rid of whitespaces too.<br>\n* Then we generate the Vector for train for each document, where we store this as a dictionary and have vocab list so that vectors maintained uniformity when it goes for training<br>\n* Then, we compute TF, where we have stored number of words in a document inside 'sum' variable for each doc, so it comes easier for 1 + log(wc / sum) for each word in each doc.<br>\n* Then we compute IDF where we check each word occurred in another docs with help of matrix of dictionary where key is document_id and values are again a dictionary with word:count, in this way we can easily recognize # of times word appeared in a document. Then we multiplt TF and IDF to get TF-IDF for each word. This is one time process <br>\n* Then we train the model on training data, where we do the train_test_split to check how training has been performed on small subset of train dataset splitted from train. Then we save the model<br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nclass TfIdf:\n    \n    def __init__(self,df):\n        self.df = df\n        self.vocab = set()\n        self.word_to_int = {}\n        self.int_to_word = {}\n    \n    def preprocess(self,text):\n\n        # Remove single character, digits, mutiple spaces, lowercase, whitespaces\n        processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n        processed_feature = re.sub('\\d+',' ',processed_feature)\n        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n        processed_feature = processed_feature.lower()\n        processed_feature = processed_feature.rstrip().lstrip()\n        \n        return processed_feature\n\n        \n    def generateVectors(self,df):\n        \n        doc_dict = {}\n        \n        for i in range(len(df)):\n            document = df['processed_doc'].iloc[i]\n            \n            sum_ = 0\n            temp = {}\n            for w in re.split(r'\\W+',document):\n                \n                if(w in self.word_to_int):\n                    if(w in temp):\n                        temp[w] +=1\n                    else:\n                        temp[w] = 1\n                        \n                    sum_ +=1\n                \n            for vc in self.vocab:\n                if not(vc in temp):\n                    temp[vc] = 0\n                    \n            temp['sum'] = sum_\n            doc_dict[i] = temp\n            \n        return doc_dict\n    \n    def computeTF(self,matrix,df): \n        \n        \n        tf_dict = {}\n        \n        for i in range(len(df)):\n            \n            total_words  = matrix[i]['sum']\n            temp = {}\n            for k,count in matrix[i].items():\n                if(k == 'sum'):\n                    continue\n                    \n                else:\n                    temp[k] = 1 + np.log(count / total_words) if count != 0 else 0\n            \n            tf_dict[i] = temp\n            \n        \n        return tf_dict\n            \n    def computeIDF(self,matrix):\n    \n        self.no_of_docs = {i:0 for i in self.vocab}\n        tf_idf_matrix = {}\n\n        for doc_id,d in matrix.items():\n            for w,c in d.items():\n                if(c != 0):\n                    self.no_of_docs[w] +=1\n\n\n        n_documents = len(self.df)\n\n        #compute inverse frequency log values\n        for w,dc in self.no_of_docs.items(): self.no_of_docs[w] = np.log(n_documents / dc)\n\n        #compute tf-idf for each word and each document\n        for doc_id,d in matrix.items():\n            temp = {}\n            for w,tf in d.items():\n                temp[w] = tf * self.no_of_docs[w]\n\n            tf_idf_matrix[doc_id] = temp\n\n        return tf_idf_matrix\n    \n    \n    \n    def trainLogisticRegression(self,tf_idf):\n  \n      \n        #Indexing the word for uniformity in training\n        list_ = []\n\n        for id_,d in tf_idf.items():\n            \n            temp = []\n            for vc in self.vocab:\n                temp.append(d[vc])\n                \n            list_.append(temp)\n       \n        X = pd.DataFrame(list_)\n\n        #preparing label\n        self.df['label'] = self.df['RealNews?'].apply(lambda x: 1 if x == True else 0)\n        Y = self.df['label']\n\n        #Splitting into train and test\n        X_train,X_test,y_train,y_test = train_test_split(X,Y,train_size=0.8,test_size=0.2, shuffle=True)\n\n        #Train the model\n        clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n\n        #Printing metrics\n        print('Metric In Train Follows Precision, Recall, Fscore and Support')\n        print(precision_recall_fscore_support(y_test, y_pred,average='weighted'))\n        \n        #Saving a model after training\n        pickle.dump(clf, open('lr_model', 'wb'))\n\n    \n    \n    def tfidfCompute(self):\n            \n        self.df['processed_doc'] = self.df['document'].apply(lambda x: self.preprocess(x))\n        \n        text = ''\n        for i in range(len(self.df)): text += self.df['processed_doc'].iloc[i] + ' '\n            \n        words = re.split(r'\\W+',text)\n        wc_dict = Counter(words)\n        \n        #Word count >= 3\n        \n        #Storing the corpus of words\n        \n        for k,v in wc_dict.items():\n            if(v >= 4 and len(k) > 3):\n                self.vocab.add(k)\n        \n        #No need for this part\n        for idx,word in enumerate(self.vocab):\n            self.word_to_int[word] = idx\n            self.int_to_word[idx] = word\n            \n        \n        print('Unique words in Train doc ',len(self.vocab))\n        \n        #generate vectors i.e word:count for each document\n        doc2vec = self.generateVectors(self.df)\n        \n        \n        #generate term freq for each word in each doc\n        start = timer()\n        df_tf = self.computeTF(doc2vec,self.df)\n        print('TF computed in secs ',timer()-start)\n        \n        #generate inverse term freq and store in list of dict\n        start = timer()\n        tf_idf = self.computeIDF(df_tf)\n        print('IDF computed in secs ',timer()-start)\n        \n        #train the model and test it's accuracy\n        self.trainLogisticRegression(tf_idf)\n        \n        \ntrain = TfIdf(df_train.iloc[0:1000,:])\ntrain.tfidfCompute()\n\n\nprint('Completed') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the model on df_test"},{"metadata":{},"cell_type":"markdown","source":"* We perform preprocessing on df_test documents <br>\n* Compute TF for each word wordcount in each document<br>\n* Then we get rid of words which are not present in train vocab<br>\n* Then we load the saved model, do the prediction and compute performance metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictTest(matrix,df):\n    \n    y_test = df['RealNews?'].apply(lambda x: 1 if x == True else 0)\n    \n    X_test = []\n\n    for id_,d in matrix.items():\n        temp = []\n        for vc in train.vocab:\n            temp.append(d[vc])\n            \n        X_test.append(temp)\n    \n    model = pickle.load(open('./lr_model', 'rb'))\n    y_pred = model.predict(X_test)\n    \n    print('------Test Accuracy Metric for Tf-IDF-----------')\n    print(precision_recall_fscore_support(y_test, y_pred,average='weighted'))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeTestTFIDF(matrix):\n    \n    tf_idf_matrix = {}\n    \n    #compute tf-idf for each word and each document\n    \n    for doc_id,d in matrix.items():\n        temp = {}\n        for w,tf in d.items():\n            temp[w] = tf *  train.no_of_docs[w]\n\n        tf_idf_matrix[doc_id] = temp\n\n    return tf_idf_matrix\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def checkIntoTrainVocab(dict_):\n    \n    vocab = train.vocab\n    \n    for did,v in dict_.items():\n        temp = {}\n        \n        for w in vocab:\n\n            if(w in v):\n                temp[w] = v[w]\n                \n            else:\n                temp[w] = 0\n                \n        v = temp\n        \n    return dict_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Procedure Followed for Test Data**<br>\nInclude Generating Vector<br>\nRemoval of vocab which are not present in trained_vocab<br>\nGetting the term freq of test data<br>\nGetting the tf-idf of test data<br>\nDo the prediction and check with y label in df_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['processed_doc'] = df_test['document'].apply(lambda x: train.preprocess(x))\ntest_set = df_test.iloc[0:1000]\n\ntest_vec = train.generateVectors(test_set)\ntest_vec = checkIntoTrainVocab(test_vec)\ntest_vec_tf = train.computeTF(test_vec,test_set)\ntest_tfidf = computeTestTFIDF(test_vec_tf)\npredictTest(test_tfidf,test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3"},{"metadata":{},"cell_type":"markdown","source":"* We use sklearn package to perfom idf computation for ngram = (2,2) and without ngram\n* In order to avoid code duplication for training and testing purpose I have made small functions and call functions with additional parameters passed i.e n_gram = True or n_gram = False\n* Then the corpus is fitted on training data for 1000 document as it would lead to memory consumption exceed limit for huge matrix\n* We save the model for predicition on df_test (for both cases)\n* We load the respective model and do the prediction for df_test on 1000 documents"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the train data corpus\ncorpus = df_train['document'].iloc[0:1000]\ntest_set = df_test.iloc[0:1000]\ntrain_set = df_train.iloc[0:1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## N_gram tweak"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef nGram(n_gram=False):\n\n    if(n_gram):\n        tfIdfVectorizer=TfidfVectorizer(use_idf=True, ngram_range = (2,2),stop_words = {'english'})\n        print('--------Training IDFs with n_gram--------')\n        \n    else:\n        tfIdfVectorizer = TfidfVectorizer(use_idf=True)\n        print('--------Training IDFs without n_gram--------')\n        \n    tfIdf = tfIdfVectorizer.fit_transform(corpus)\n    \n    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n    df = df.sort_values('TF-IDF', ascending=False)\n\n    \n    print(df.head(10))\n    \n    return tfIdfVectorizer,tfIdf\n\ntfIdf_n,tf_n = nGram(n_gram=True)\ntfIdf_wn,tf_wn = nGram(n_gram = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Train the LR\ndef trainModel(tfIdf,n_gram):\n    \n    #Get the tfidf form of each doc\n\n    X = tfIdf.todense()\n    Y = train_set['RealNews?'].iloc[0:1000].apply(lambda x: 1 if x==True else 0)\n\n\n    X_train,X_test,y_train,y_test = train_test_split(X,Y,train_size=0.8,test_size=0.2, shuffle=True)\n\n    lr = LogisticRegression(random_state=0).fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    \n    if(n_gram):\n        \n        pickle.dump(lr, open('lr_model_ngram_sklearn', 'wb'))\n        print('Training Accuracy Metric with n_gram ------ Precision, Recall, Fscore and Support')\n    else:\n        pickle.dump(lr, open('lr_model_wngram_sklearn', 'wb'))\n        print('Training Accuracy Metric without n_gram------ Precision, Recall, Fscore and Support')\n        \n    print(precision_recall_fscore_support(y_test, y_pred,average='weighted'))\n    \ntrainModel(tf_n,True)\ntrainModel(tf_wn,False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* With ngrams we make groups of 2 words in consecutive manner and the corpus if now (w1,w2) combined \n* This can make in determining suspicious words if they are combined, like (lucky,winner) rather lucky and winner\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef testingModel(tf,n_gram):\n\n    #Fit the test data \n\n    corpus_t = test_set['document'].iloc[0:1000]\n    tfIdf_t = tf.transform(corpus_t)\n    \n    #Test the Prediction for Test Data\n\n    X_test = tfIdf_t.todense()\n    y_test = test_set['RealNews?'].apply(lambda x: 1 if x==True else 0)\n\n    if(n_gram):\n        model = pickle.load(open('./lr_model_ngram_sklearn', 'rb'))\n        print('----Testing Accuracy for TF-IDF with ngram-------------')\n    else:\n        model = pickle.load(open('./lr_model_wngram_sklearn', 'rb'))\n        print('----Testing Accuracy for TF-IDF without ngram-------------')\n        \n    y_pred = model.predict(X_test)\n    \n    print(precision_recall_fscore_support(y_test, y_pred,average='weighted'))\n    \n    \ntestingModel(tfIdf_n,True)\ntestingModel(tfIdf_wn,False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multinomial Naive Bayes Implementation\n"},{"metadata":{},"cell_type":"markdown","source":"* Here we use, CountVectorizer to convert document into vector so that it could be fed as an input to Naive Bayes classifier\n* For fitting training data, we use transform_fit and when we want to fit test data we use transform method so that we don't calcuate test words into traindataset\n* Then we do the prediction on df_test dataset to compare the results with the hands-on code for Naive Bayes\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sub = df_train.iloc[0:10000]\ntest_sub = df_test.iloc[0:2000]\n\ny = train_sub['RealNews?']\nX = train_sub['document']\n\ny_test = test_sub['RealNews?']\nX_test = test_sub['document']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvec = vectorizer.fit_transform(X)\nX = vec.toarray()\n\n#fitting the test data\nvec_test= vectorizer.transform(X_test)\nX_test = vec_test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\n\ndef sklearn_NaiveBayes(X,y):\n    \n    X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,test_size=0.2, shuffle=True)\n    nb.fit(X_train,y_train)\n    y_pred = nb.predict(X_test)\n    \n    print('------------Training Accuracy ------------- ')\n    print(precision_recall_fscore_support(y_pred,y_test,average='weighted'))\n    \n\nsklearn_NaiveBayes(X,y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-------------Testing Accuracy---------------')\ny_pred = nb.predict(X_test)\nprint(precision_recall_fscore_support(y_pred,y_test,average='weighted'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}