{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# EDA and price prediction of used vehicles"},{"metadata":{},"cell_type":"markdown","source":"### Craigslist is the world's largest collection of used vehicles for sale. The dataset contains all information of used vehicles such as model, year, condition, cylinders, drive, size, paint color, price etc. "},{"metadata":{},"cell_type":"markdown","source":"**In order to do the predict the price, first we need to understand, clean, and scale the data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pandas import DataFrame\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import rc\nimport plotly.graph_objs as go\nfrom sklearn import preprocessing\nimport matplotlib\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.tree import DecisionTreeRegressor\nimport math\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz\n# from sklearn.externals.six import StringIO\nfrom IPython.display import Image\n# import pydotplus\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read in data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles = pd.read_csv(r\"../input/craigslist-carstrucks-data/vehicles.csv\") #reading the file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Understand the data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I want to remove some irrelavent columns that are not important for our prediction. These columns may important/necessary for some other type of prediction/analysis.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles.drop(['id', 'url','region_url', 'vin', 'image_url', 'lat', 'long', 'description'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vehicles.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see there are many columns with missing values. We need to handle those missing values and clean the data for accuarate prediction.**\n\n**Clean the data.**\n**Since here missing values are very large in numbers, we are only going to keep columns which have less than 55% of missing values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_val = vehicles.isna().sum()\ndef na_filter(na, threshold = .55): #only select variables that passees the threshold\n    col_pass = []\n    for i in na.keys():\n        if na[i]/vehicles.shape[0]<threshold:\n            col_pass.append(i)\n    return col_pass\nvehicles_cleaned = vehicles[na_filter(null_val)]\nvehicles_cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Following is just to show different catagories of categorical variables.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.manufacturer.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.model.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.cylinders.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.fuel.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.title_status.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.transmission.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.drive.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.type.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.paint_color.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**price, year, and odometer are numerical varaibles.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_cleaned.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To handle rest of the missing values, we will drop all rows with missing values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df = vehicles_cleaned.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vehicles_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We are left with 118898 rows and 14 coulmns. Also, our dataset doesn't contain any missing values now.**\n\n**Handling outliers.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3,6))\nsns.boxplot(y='price', data=vehicles_df,showfliers=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df.price.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df.price.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Price of the vehicle can never be zero. So, we will remove rows with price as 0.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df = vehicles_df[vehicles_df['price']>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will remove outliers of price using IQR.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = vehicles_df['price']\nremoved_outliers = y.between(y.quantile(.05), y.quantile(.95))\nremoved_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(removed_outliers.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index_names = vehicles_df[~removed_outliers].index # INVERT removed_outliers!!\nprint(index_names) # The resulting 11027 prices to drop.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df.drop(index_names, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3,6))\nsns.boxplot(y='odometer', data=vehicles_df,showfliers=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's change the order of the column and place target column first for simplicity.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df = vehicles_df[['price','region','year','manufacturer','model','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','state']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check distribution of all predictors with respect to target(price) for general understanding.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = vehicles_df['price']\nx = vehicles_df['odometer']\nplt.scatter(x, y)\nplt.xlabel('odometer')\nplt.ylabel('price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(y=\"manufacturer\", x=\"price\",kind=\"boxen\", data=vehicles_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"drive\", y=\"price\",kind=\"bar\", palette=\"ch:.25\", data=vehicles_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=vehicles_df.fuel, y=vehicles_df.price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(y=\"type\", x=\"price\",kind=\"violin\", data=vehicles_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"price\", y=\"paint_color\", kind=\"boxen\",\n            data=vehicles_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"title_status\", y=\"price\",kind=\"violin\", palette=\"ch:.25\", data=vehicles_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = vehicles_df['price']\nx = vehicles_df['year']\nplt.scatter(x, y)\nplt.xlabel('year')\nplt.ylabel('price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In order to prepare data for predictive modeling we will use Label Encoder since we have many categories for categorical variables. Label encoding is simply converting each value in a column to a number.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df[['region','manufacturer','model','cylinders','fuel','title_status','transmission','drive'\n             ,'type','paint_color','state']] = vehicles_df[['region','manufacturer','model','cylinders','fuel','title_status',\n                                                            'transmission','drive','type','paint_color','state']].apply(le.fit_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see that odometer is a feature with larger magnitude. We need to reduce the scale of it to prevent from dominating the prediction model.**\n\n**In order to have fair glass to see all variables from the same lands, I have applied MinMaxScaler so prediction model will perform better.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nvehicles_df[\"odometer\"] = np.sqrt(preprocessing.minmax_scale(vehicles_df[\"odometer\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vehicles_df.to_csv(r'C:/Users/Aneri/Desktop/Python datsets/vehicles_df.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predictive Modeling.**"},{"metadata":{},"cell_type":"markdown","source":"### Multiple Linear Regression."},{"metadata":{},"cell_type":"markdown","source":"**I am going to use three method for variable selection in MLR and select the one with highest accuracy.**\n\n**1) Filter Method: As the name suggest, I will filter and take subset of relevant features. I have done filtering using correlation matrix with Pearson Correlation.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = vehicles_df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see from the heatmap that drive, odometer, cylinders, fuel, and year have relatively high postive and negative relationship with price. So we will drop the features apart from this.**\n\n**One of the assumptions of linear regression is that the independent variables need to be uncorrelated with each other. We also need to check if these variables are related with eachother.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vehicles_df[[\"drive\",\"odometer\"]].corr())\nprint(vehicles_df[[\"odometer\",\"cylinders\"]].corr())\nprint(vehicles_df[[\"cylinders\",\"fuel\"]].corr())\nprint(vehicles_df[[\"fuel\",\"year\"]].corr())\nprint(vehicles_df[[\"year\",\"drive\"]].corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It doesn't seem that these variables have high relation with each other.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_p = vehicles_df[[\"drive\",\"odometer\",\"cylinders\",\"fuel\",\"year\"]]\ntarget_p = vehicles_df[[\"price\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting our dataset randomly with the test data containing 25% of the data,\nx_train, x_test, y_train, y_test = train_test_split(features_p,target_p, \n                                                    test_size=0.25, \n                                                    random_state=0)\n\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(x_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(x_test) ,'rows')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run the regression model with Pearson Correlation method\nreg_model_p = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the training data to the model,\nreg_model_p.fit(x_train, y_train)\n#outputs the coefficients\nprint('Intercept :', reg_model_p.intercept_[0], '\\n')\nprint(pd.DataFrame({'features':x_train.columns,'coeficients':reg_model_p.coef_[0]}))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction\nlr_pred_p = reg_model_p.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, lr_pred_p))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, lr_pred_p))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lr_pred_p)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = r2_score(y_test, lr_pred_p)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2)Backward Elimination(Wrapper Method): This is an iterative and computationally expensive process but it is more accurate. As the name suggest, we feed all the possible features to the model at first. We check the performance of the model and then iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range.**\n\n**The performance metric used here to evaluate feature performance is pvalue. If the pvalue is above 0.05 then we remove the feature, else we keep it.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = vehicles_df.loc[:,vehicles_df.columns != 'price']\ntarget = vehicles_df.loc[:,vehicles_df.columns == 'price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**p values of all features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(features)\nX_1\n#Fitting sm.OLS model\nmodel = sm.OLS(target,X_1).fit()\nmodel.pvalues","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Backward Elimination\ncols = list(features.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = features[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(target,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our final set of variables are shown above. It seems it has all the variables in dataset. Let's apply them into the model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_be = vehicles_df.loc[:,vehicles_df.columns != 'price']\ntarget_be = vehicles_df.loc[:,vehicles_df.columns == 'price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting our dataset randomly with the test data containing 25% of the data,\nX_train, X_test, y_train, y_test = train_test_split(features_be,target_be, \n                                                    test_size=0.25, \n                                                    random_state=0)\n\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(X_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(X_test) ,'rows')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the regression model with backward elimination\nreg_model_be = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the training data to the model,\nreg_model_be.fit(X_train, y_train)\n#outputs the coefficients\nprint('Intercept :', reg_model_be.intercept_[0], '\\n')\nprint(pd.DataFrame({'features':X_train.columns,'coeficients':reg_model_be.coef_[0]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred_be = reg_model_be.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, lr_pred_be))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, lr_pred_be))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lr_pred_be)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nscore = r2_score(y_test, lr_pred_be)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3) Recursive Feature Elimination(Wrapper Method): This method is recursively removing attributes and building a model on those attributes that remain. It uses accuracy metric to rank the feature according to their importance.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = vehicles_df.loc[:,vehicles_df.columns != 'price']\ntarget = vehicles_df.loc[:,vehicles_df.columns == 'price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no of features\nnof_list=np.arange(1,13)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.25, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train.values.ravel())\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train.values.ravel())\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**12 features give optimum score. Now let's figure out these 12 features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(features.columns)\nmodel = LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 12)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(features,target.values.ravel())  \n#Fitting the data to model\nmodel.fit(X_rfe,target)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_rfe = vehicles_df[['region', 'year', 'manufacturer', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'paint_color','state']]\ntarget_rfe = vehicles_df.loc[:,vehicles_df.columns == 'price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting our dataset randomly with the test data containing 25% of the data,\nX_train, X_test, y_train, y_test = train_test_split(features_rfe,target_rfe, \n                                                    test_size=0.25, \n                                                    random_state=0)\n\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(X_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(X_test) ,'rows')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the regression model for recursive feature elimination\nreg_model_rfe = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the training data to the model,\nreg_model_rfe.fit(X_train, y_train)\n#outputs the coefficients\nprint('Intercept :', reg_model_rfe.intercept_[0], '\\n')\nprint(pd.DataFrame({'features':X_train.columns,'coeficients':reg_model_rfe.coef_[0]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred_rfe = reg_model_rfe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, lr_pred_rfe))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, lr_pred_rfe))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lr_pred_rfe)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = r2_score(y_test, lr_pred_rfe)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Both backward elimination and recursive feature elimination are giving highest R value.**"},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree(CART)\n\n**As we know that variable selection and reduction is automatic in CART, let's apply the algorithm.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_final = vehicles_df[['region','year','manufacturer','model','cylinders','fuel','odometer','title_status','transmission','drive'\n             ,'type','paint_color','state']]\ny_final = vehicles_df[['price']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_final,y_final, test_size = 0.25, random_state=0)\n#view number of training and testing data\nprint('Our training prediction variable contains :',len(y_train) ,'rows')\nprint('Our training independent variable contains :',len(x_train) ,'rows')\nprint('Our testing prediction variable contains :',len(y_test) ,'rows')\nprint('Our testing independent variable contains :',len(x_test) ,'rows')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree = DecisionTreeRegressor()\nmodel = dtree.fit(x_train, y_train)  #train parameters: features and target\npred = dtree.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_representation = tree.export_text(dtree)\nprint(text_representation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we know that full tree is always overfitted and this tree is also pretty long. Let’s change a couple of parameters to see if there is any effect on the accuracy and also to make the tree shorter.**\n\n**Criterion: defines what function will be used to measure the quality of a split. The options are \"mse\",\"mae\", and \"friedman mse\".**\n\n**Max_depth: defines the maximum depth of the tree. If it’s “none”, the tree will be as long as possible, when all the leaves are pure (risk of overfitting the model).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = []\nacc_mse = []\nacc_mae= []\nacc_friedman_mse = []\nfor i in range(1,30):\n    dtree = DecisionTreeRegressor(criterion='mse', max_depth=i)\n    dtree.fit(x_train, y_train)\n    pred = dtree.predict(x_test)\n    acc_mse.append(np.sqrt(metrics.mean_squared_error(y_test, pred)))\n    dtree = DecisionTreeRegressor(criterion='mae', max_depth=i)\n    dtree.fit(x_train, y_train)\n    pred = dtree.predict(x_test)\n    acc_mae.append(np.sqrt(metrics.mean_squared_error(y_test, pred)))\n    ####\n    dtree = DecisionTreeRegressor(criterion='friedman_mse', max_depth=i)\n    dtree.fit(x_train, y_train)\n    pred = dtree.predict(x_test)\n    acc_friedman_mse.append(np.sqrt(metrics.mean_squared_error(y_test, pred)))\n    ####\n    max_depth.append(i)\n    d = pd.DataFrame({'acc_mse':pd.Series(acc_mse), \n    'acc_mae':pd.Series(acc_mae),\n    'acc_friedman_mse':pd.Series(acc_friedman_mse),\n    'max_depth':pd.Series(max_depth)})\n                                                \n# visualizing changes in parameters\nplt.plot('max_depth','acc_mse', data=d, label='mse')\nplt.plot('max_depth','acc_mae', data=d, label='mae')\nplt.plot('max_depth','acc_friedman_mse', data=d, label='friedman_mse')\nplt.xlabel('max_depth')\nplt.ylabel('RMSE')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We want the value of RMSE as short as possible. The depth 14 is giving lowest Root Mean Squared Error with criterian mse or friedman_mse. Thus, I am going to apply mae for deptth 14.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_m = DecisionTreeRegressor(criterion='mae',max_depth = 14)\nmodel = dtree_m.fit(x_train, y_train)  #train parameters: features and target\npred = dtree_m.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(random_state=1).fit(x_train, y_train.values.ravel())\nrf_pred = rf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, rf_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, rf_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, rf_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**These are the few algorithms/models we applied on given dataset.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}