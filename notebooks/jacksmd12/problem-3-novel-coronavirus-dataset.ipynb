{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Affiliation: Whiting School of Engineering, Johns Hopkins University (685.621) Programming Assignment 2 - Problem 3"},{"metadata":{},"cell_type":"markdown","source":"\n\nAuthors: Jack Shu, Sriharshareddy Katpally, Sarah Henry\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# readh in datasets:\ndata_dir = '/kaggle/input/novel-corona-virus-2019-dataset'\nconfirmed = pd.read_csv(os.path.join(data_dir, 'time_series_covid_19_confirmed.csv'))\ndeaths = pd.read_csv(os.path.join(data_dir, 'time_series_covid_19_deaths.csv'))\nrecovered = pd.read_csv(os.path.join(data_dir, 'time_series_covid_19_recovered.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('confirmed data shape: ' + str(confirmed.shape) )\nprint('deaths data shape: ' + str(deaths.shape))\nprint('recovered data shape: ' + str(recovered.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(confirmed.columns == deaths.columns)\n# print(confirmed.columns == recovered.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_melt = pd.melt(confirmed, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'],\n                   var_name='Date', value_name='Confirmed')\ndeaths_melt = pd.melt(deaths, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'],\n                     var_name='Date', value_name='Deaths')\nrecovered_melt = pd.melt(recovered, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'],\n                        var_name='Date', value_name='Recovered')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confirmed_melt.shape)\nprint(deaths_melt.shape)\nprint(recovered_melt.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =confirmed_melt.merge(deaths_melt, how='outer', on=['Province/State', 'Country/Region', 'Lat', 'Long', 'Date'])\ndf = df.merge(recovered_melt, how='left', on=['Province/State', 'Country/Region', 'Lat', 'Long', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the following country does not have recovery info: ' + np.unique(df[df['Recovered'].isna()]['Country/Region']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since the number of country without recovery is relatively small, just drop them\ndf = df[df['Recovered'].isna() != True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# problem 3 - 1"},{"metadata":{},"cell_type":"markdown","source":"visualizing the cases by lat, long"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_melt = pd.melt(df, id_vars = ['Province/State', 'Country/Region', 'Lat', 'Long', 'Date'])\nfig = px.scatter_geo(df_melt, lat='Lat', lon='Long', size='value', color='variable',\n                    animation_frame='Date', projection='natural earth')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# problem 3 - 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to weekly average \ndf_weekly = df\ndf_weekly.index = pd.to_datetime(df_weekly['Date'])\ndf_weekly = df_weekly.drop(['Province/State', 'Date', 'Lat', 'Long'], axis=1)\ndf_weekly = df_weekly.groupby(['Country/Region']).resample('W').mean()\ndf_weekly = df_weekly.reset_index()\n\n# adding 1 to the confirmed cases to get around divide by 0 error before the spread\ndf_weekly['Death_Rate'] = df_weekly['Deaths'] / (df_weekly['Confirmed'] + 1)\n\ndf_weekly['Date'] = df_weekly['Date'].dt.strftime('%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"random_sample_country = pd.Series(np.unique(df_weekly['Country/Region'])).sample(10, random_state=5)\n\n# always have US\nrandom_sample_country = random_sample_country.append(pd.Series(['US']))\n\ndf_weekly = df_weekly[df_weekly['Country/Region'].isin(random_sample_country)]\nfig = px.scatter(df_weekly, x='Confirmed', y='Death_Rate', \n                 color='Country/Region',\n                 range_x=[min(df_weekly['Confirmed']), max(df_weekly['Confirmed'])],\n                 range_y=[min(df_weekly['Death_Rate']), max(df_weekly['Death_Rate'])],\n                 animation_group='Country/Region', animation_frame='Date')\n\nfig.update_traces(marker_size=20, opacity = 0.7)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# problem 3-3"},{"metadata":{},"cell_type":"markdown","source":"Since a different visualization were applied for part 2 (e.g. bubble chart) than in problem 2, a qualitative analysis will be applied.\n\nComparing problem 3-1 (visualization of lat, long by date) vs. problem 2 - 1 (visualization of lat, long, by date) showed a similar pattern, the coronavirus started in Asia and then spreads to Europe and then eventually to the Americas as time progresses.\n\nIn problem 3-2, the bubble chart indicated the US number of cases initially were low (before it spread over from Aisa), followed by a rapid increase in the death rate (peaked during the week of 3/8/20). This may be due to the initial cases being more severe and hence easily detectable. As the number of cases increased, the death rate dropped off and stablized. But the number of confirmed cases increased rapidly after week of 3/15/20.\n\nThe rapid increase in the confirmed cases were consistent with the observations and predictions from problem 2-2, 2-4, 2-5 where the predictions showed a rapid increase in the number of cases, albeit not quite exponentially as one might expect for a pandemic.\n\nFrom the analysis it is clear that the two factors will be critical in understanding the growth rate: time and geographical location (time & space) which will be explored in the algorithm in section problem 3-4, and 3-5\n"},{"metadata":{},"cell_type":"markdown","source":"# problem 3-4"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weekly = df\ndf_weekly.index = pd.to_datetime(df_weekly['Date'])\ndf_weekly = df_weekly.groupby(['Country/Region']).resample('W').mean()\ndf_weekly = df_weekly.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_knn = df_weekly\n\n# create new feature for 'days'\ndf_knn['days'] = (df_knn.loc[:, 'Date'].apply(pd.Timestamp) - pd.Timestamp(df_knn.loc[:,'Date'].min())).dt.days\n\nfig = px.scatter(df_knn, x = 'days', y = 'Country/Region', size='Confirmed', color='Country/Region')\nfig.show()\n\ndf_knn = df_knn.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rescaling using MinMax method\nX = df_knn.loc[:, ['Lat', 'Long', 'days']]\ncols = X.columns\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), columns = cols)\n\n# # rescaling using normalize\n# X = df_svr.loc[:, ['Lat', 'Long', 'days']]\n# cols = X.columns\n# from sklearn.preprocessing import normalize\n# X = normalize(X)\n\ny = df_knn.loc[:, 'Confirmed']\n\n# break into training and test data set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=15)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN model\nneigh = KNeighborsRegressor(n_neighbors=2, weights='distance', algorithm='brute', p=2)\nneigh.fit(X_train, y_train)\npred_train = neigh.predict(X_train)\npred_test = neigh.predict(X_test)\n\nRMSLE_train = np.sqrt(mean_squared_log_error(y_train, pred_train))\nRMSLE_test = np.sqrt(mean_squared_log_error(y_test, pred_test))\n\nprint('Train data RMSLE: ' + str(RMSLE_train))\nprint('Test data RMSLE: ' + str(RMSLE_test))\nprint('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# method for finding the growth rate for n-days. Defaults to start at day 0, with US coordinates\n\ndef confirmed_growth_rate(day_n, start_day=0, Lat = 37.0902, Long = -95.7129):\n    day_n_X = scaler.transform(pd.DataFrame([[Lat, Long, day_n]]))\n    day_n_X = pd.DataFrame(day_n_X, columns=cols)\n    pred_n_cases = neigh.predict(day_n_X)\n    \n    start_X = scaler.transform(pd.DataFrame([[Lat, Long, start_day]]))\n    start_X = pd.DataFrame(start_X, columns=cols)\n    pred_start_cases = neigh.predict(start_X)\n    \n    # check for divide by 0 problem\n    if pred_start_cases == 0:\n        pred_start_cases = 1\n    \n    growth_rate = (pred_n_cases - pred_start_cases) / pred_start_cases\n    return growth_rate\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_range = np.arange(0, 30)\ngrowth_rate = []\ninterval = 15 # days\n\n\nfor day in day_range:\n    start_day = day\n    day_n = day + interval\n    growth_rate.append(confirmed_growth_rate(day_n, start_day)[0])\n\n# confirmed_growth_rate(day_n=15, start_day=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(x=day_range, y=growth_rate)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# problem 3-5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# explore relationship between confirmed, deaths and recovered\ncases = df_knn[['Confirmed', 'Deaths', 'Recovered']]\ncases_melt = pd.melt(cases, id_vars='Confirmed')\n\nfig = px.scatter(cases_melt, x='Confirmed', y='value', facet_col='variable')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nBased on the exploratory analyis, there may be a linear (postive) relationship between the number of confirmed cases vs. the deaths and recovered, hence it may be a confounding variable. Hence, the following analyis will leave out both the 'Death' and 'Confirmed' variable\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset data to predict recovery rate\nX = df_knn[['Lat', 'Long', 'days']]\ncols = X.columns\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), columns = cols)\n\ny = df_knn[['Recovered']]\n\n# break into training and test data set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN model\nneigh = KNeighborsRegressor(n_neighbors=2, weights='distance', algorithm='brute', p=2)\nneigh.fit(X_train, y_train)\npred_train = neigh.predict(X_train)\npred_test = neigh.predict(X_test)\n\nRMSLE_train = np.sqrt(mean_squared_log_error(y_train, pred_train))\nRMSLE_test = np.sqrt(mean_squared_log_error(y_test, pred_test))\n\nprint('Train data RMSLE: ' + str(RMSLE_train))\nprint('Test data RMSLE: ' + str(RMSLE_test))\nprint('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# method for finding the recovery rate for n-days. Defaults to start at day 0, with US coordinates\n\ndef recovery_rate(day_n, start_day=0, Lat = 37.0902, Long = -95.7129):\n    day_n_X = scaler.transform(pd.DataFrame([[Lat, Long, day_n]]))\n    day_n_X = pd.DataFrame(day_n_X, columns=cols)\n    pred_n_cases = neigh.predict(day_n_X)\n    \n    start_X = scaler.transform(pd.DataFrame([[Lat, Long, start_day]]))\n    start_X = pd.DataFrame(start_X, columns=cols)\n    pred_start_cases = neigh.predict(start_X)\n    \n    # check for divide by 0 problem\n    if pred_start_cases == 0:\n        pred_start_cases = 1\n    \n    recovery_rate = (pred_n_cases - pred_start_cases) / pred_start_cases\n    return recovery_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_range = np.arange(0, 30)\nrecover_rate = []\ninterval = 15 # days\n\n\nfor day in day_range:\n    start_day = day\n    day_n = day + interval\n    recover_rate.append(recovery_rate(day_n, start_day)[0])\n\n\n# recovery_rate(day_n=15, start_day=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(x = day_range, y= recover_rate)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# run time analysis for KNN (problem 3-4 & 3-5)"},{"metadata":{},"cell_type":"markdown","source":"For traning the KNN model it just requires storing the X_train and y_train values, which only requires constant time. For predicting the KNN model the distance of every X_test would need be be compared with every X_train to calculate the distance.\n\nAssuming there is 'd' dimensions, with 'n' X_train data points and 'm' observations in X_test, then the run time would then be T(n) = d * n * m. Which follows the form of T(n) = a * n + b, thereform the time complexity would be of O(n)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}