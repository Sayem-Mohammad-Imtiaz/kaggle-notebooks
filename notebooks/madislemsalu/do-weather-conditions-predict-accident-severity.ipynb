{"cells":[{"metadata":{"_uuid":"e93a1fc06f76d91eaf14fed8b2f23834514fa982","_cell_guid":"51571dc9-9a5c-4524-82e2-9d5c538387f2"},"cell_type":"markdown","source":"**This machine learning pipeine will be exploring whether weather, light, and road conditions – that increase the amount of information available about the situation – can increase the accuracy of predicting the severity of an accident.  **"},{"source":"# Libraries \nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n#from sklearn import preprocessing\nimport seaborn as sns\n#from sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 70) # Since we're dealing with moderately sized dataframe,\npd.set_option('display.max_rows', 13)# max 13 columns and rows will be shown","outputs":[],"metadata":{"_uuid":"d8d686182288a82e05068668a647e374f2284c80","collapsed":true,"_cell_guid":"a97ee808-ca8a-4b2a-84b7-c419baa68daa"},"cell_type":"code","execution_count":null},{"source":"df=pd.read_csv(\"../input/Kaagle_Upload.csv\", sep=\",\", decimal=\",\", engine='python') # Read the data from a csv\ndf=df.dropna() # The dataset is huge, therefore, dropping any rows with missing values is fine\ndf.head()\ndf.isnull().sum().sum()\n\n# First I select variables based on prefrence, then for df2 I add weather related conditions of:\n#'road_surface_conditions','light_conditions','weather_conditions'\n#Feel free to mix these variables up\ndf2 = df[['special_conditions_at_site','pedestrian_movement','road_surface_conditions','light_conditions','weather_conditions','age_of_vehicle','sex_of_driver','age_of_driver','junction_location', 'junction_detail','junction_control','did_police_officer_attend_scene_of_accident','accident_severity','day_of_week']]\ndf1 = df[['special_conditions_at_site','pedestrian_movement','age_of_vehicle','sex_of_driver','age_of_driver','junction_location','junction_detail','junction_control','did_police_officer_attend_scene_of_accident','day_of_week','accident_severity']]\n\n\ndf1.replace(-1, np.nan, inplace=True) # -1 should be imputed to NaN to be recognized as missing in the next row\ndf1=df1.dropna() # I drop all the rows with missing data once again\ndf1.shape\n\ndf2.replace(-1, np.nan, inplace=True) # Same as previously \ndf2=df2.dropna()\ndf2.shape","outputs":[],"metadata":{"_uuid":"e27c76db67b4f3c0f839a1f0a6ce787c97980b3f","collapsed":true,"_cell_guid":"6bb983e1-b7a1-48a0-ba81-184065f5a11c"},"cell_type":"code","execution_count":null},{"source":"# Here I took a subset of features from the previous cell. This is so I could narrow it more down / \n# This can be considered redundant, but was mostly part of the workflow when looking at different variables\ndf1 = df1[['special_conditions_at_site','pedestrian_movement','age_of_vehicle','sex_of_driver','age_of_driver','junction_location','junction_detail',\n           'junction_control','day_of_week',\n           'accident_severity']]\n\ndf2 = df2[['special_conditions_at_site','pedestrian_movement','road_surface_conditions','light_conditions','weather_conditions','age_of_vehicle','sex_of_driver','age_of_driver',\n          'junction_location', 'junction_detail','junction_control',\n          'accident_severity','day_of_week']]\n\ndf1.shape\ndf2.shape","outputs":[],"metadata":{"_uuid":"135e2c245f10d32eb49095fe0ee44d88217d7285","collapsed":true,"_cell_guid":"108ce87d-fc67-414b-aa06-020885f7ce25"},"cell_type":"code","execution_count":null},{"source":"\nimport matplotlib.pyplot as plt\ncorrmat = df2.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n\n#ax = sns.pairplot(df, size)\nplt.show()","outputs":[],"metadata":{"_uuid":"047ffb10409078a6a1f065186e5a6804318b25f8","collapsed":true,"_cell_guid":"4fcd5017-0ec9-46ec-a043-4e6d14c5f01d"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"b6d320bcb05dd3c2f3ac253515f6ff4324f86a3a","_cell_guid":"ed1a5488-1859-48dc-a5d2-4882f565bf7f"},"cell_type":"markdown","source":"**The next step was to plot a Pearson correlation matrix to identify the amount of linear relationship be-tween variables in order to gain insight into dataand to determine whether linear based algorithms aresuitable. The matrix is color-coded - a value of one is represented by beige and shows a completely positive linear correlation. Dark purple represents a zero that suggests no linear correlation. As seen by the graph, there no linear relationships present, besides between the added features of weather condition, road surface,and light condition.This makes sense, as weather-, road-, and light conditions are dependent on each other. When it is raining, one can presume that the road condition atthe same time is also wet. Absence of other linear relationships can be explained by the fact that almost everything is a categorical variable. Even the weather related conditions barely achieve 0.4 on the Pearson correlation as they are nominal features as well.\nHence there is no justification and indication touse predictive models based on linearity** "},{"source":"\n#cols2 = ['junction_detail','light_conditions','weather_conditions','casualty_type','day_of_week','junction_control','road_surface_conditions','casualty_severity']\n\nk = 6 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'accident_severity')['accident_severity'].index\ncm = np.corrcoef(df2[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","outputs":[],"metadata":{"_uuid":"a076b93f817c93a61379bdcbfb683dc435664c0f","collapsed":true,"_cell_guid":"5cf751e8-8287-4227-90c7-abd9e6bcb31b"},"cell_type":"code","execution_count":null},{"source":"df2.head()","outputs":[],"metadata":{"_uuid":"bf8d4bd3de40ebe865234488f2ae782f083cebd6","collapsed":true,"_cell_guid":"f421d1f1-e3bc-44a1-b84b-c6af3e8d3c0d"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"2f94a6314dd0deff94bb252d00253a63ebc5124f","_cell_guid":"6d9c0b4f-8cdd-4ee6-8a31-912add6998a1"},"cell_type":"markdown","source":"**In the cell below, one can hot encode the categorical variables. However, I noticed that the performance alteration was miniscule.**"},{"source":"# cols_with = ['junction_location','junction_detail','light_conditions','weather_conditions','day_of_week','junction_control','road_surface_conditions']\n# cols_without = ['junction_location','junction_detail','day_of_week','junction_control']\n# import seaborn as sns\n# def one_hot(df, cols):\n#     for each in cols:\n#         dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n#         df = pd.concat([df, dummies],axis=1)\n#     df = df.drop(cols, axis=1)\n#     return df  %%!\n# df2 = one_hot(df2,cols_with)\n# df1 = one_hot(df1,cols_without)\n","outputs":[],"metadata":{"_uuid":"b9b2b8b1e17ba0af2484f5ae7cd91b2fc64c3863","collapsed":true,"_cell_guid":"3f681585-d25c-46d5-a673-dd6011b521f0"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"088b0beb725e850f98d1e1eaf76704b602da4fc4","_cell_guid":"4fe34bd2-520d-4368-ac37-a435cf3546a3"},"cell_type":"markdown","source":"**The next step was to normalize the only features that were not categorical: age of the driver and age of the car. Normalization involves taking the logarithm of the given features. This is done to because high values for certain variables computationally skew results more in favour of that variable, than their actual contribution. In this case, age of the driver for example has values ranging from 18-88. When the majority of other categorical variables are binary or limited within 1-8 categories. **"},{"source":"from scipy.stats import norm\nfrom scipy import stats\n#histogram and normal probability plot\nsns.distplot(df1['age_of_driver'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df1['age_of_driver'], plot=plt)\nplt.show()\n","outputs":[],"metadata":{"_uuid":"019ffd7c2ca8de940123d91300e9d8bf35765474","collapsed":true,"_cell_guid":"a50323ad-0f09-4b60-9be9-a254e4678b3a"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"ac5116d53232599257591c64b578e706c60846c8","_cell_guid":"b93403b3-bdcc-4391-a233-dedfaf817d17"},"cell_type":"markdown","source":"**In this case, age of the driver and age of the vehicle were the only variables with a high numerical variance, and therefore logarithms were taken of both variables. Furthermore, taking the logarithm of both the age of the driver and age of the vehicle improved the fit by altering the scale, and making the variables more \"normally\" distributed.**"},{"source":"df2['age_of_driver'] = np.log1p(df2['age_of_driver']) \ndf2['age_of_vehicle'] = np.log1p(df2['age_of_vehicle'])# standardise the feature\n\ndf1['age_of_driver'] = np.log1p(df1['age_of_driver']) \ndf1['age_of_vehicle'] = np.log1p(df1['age_of_vehicle'])#\n","outputs":[],"metadata":{"_uuid":"14e232538f96a698116f4c1058842bea3eb850db","collapsed":true,"_cell_guid":"81a9d3f5-6e07-4f1d-900e-f86e0c5687d5"},"cell_type":"code","execution_count":null},{"source":"sns.distplot(df1['age_of_driver'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df1['age_of_driver'], plot=plt)\nplt.show()","outputs":[],"metadata":{"_uuid":"463d0ebb91244b8208808b69ce03a675b38a98bf","collapsed":true,"_cell_guid":"587c5841-1e7d-4c2f-8bb6-f38fb3c74cf5"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"b173232f4a8d6a76208303ddc2e050c323e8271f","_cell_guid":"679c2c75-74b6-4952-a1f6-3008f077ff05"},"cell_type":"markdown","source":"**After taking the log, one can notice that the values range from approximately 2.5 to 4.5. This increases the performance of machine learning algorithms, as the numerical values do not have disproportionate amounts of computing value compared to all the other categorical variables.\n**"},{"source":"df2","outputs":[],"metadata":{"_uuid":"081b6158f6aafb21a73cdf296cbff5bfbefc1737","collapsed":true,"_cell_guid":"69c62ed7-aceb-4a0e-8b94-3dff62c95b39"},"cell_type":"code","execution_count":null},{"source":"df1= df1[:15000] #keep 1500 to decrease running times\ndf2= df2[:15000] #keep 15000\n\nY = df2.accident_severity.values\nY1 = df1.accident_severity.values\nY","outputs":[],"metadata":{"_uuid":"98b5384f81d89387be8f1fa9afed734c7b2b3e1e","collapsed":true,"_cell_guid":"1a3582ad-03b0-4564-864b-1a8774ee407b"},"cell_type":"code","execution_count":null},{"source":"cols = df2.shape[1]\nX = df2.loc[:, df2.columns != 'accident_severity']\nX1 = df1.loc[:, df1.columns != 'accident_severity']\nX.columns;","outputs":[],"metadata":{"_uuid":"c165c4155c70b18736ac1bfb8aedf981b664e4cc","collapsed":true,"_cell_guid":"b82d9e53-6ce9-44b3-a518-074be959b7a2"},"cell_type":"code","execution_count":null},{"source":"X.shape\nX1.shape","outputs":[],"metadata":{"_uuid":"bcb0ecc911473d6f267c4557c0d57eb0c28d6fee","collapsed":true,"_cell_guid":"f770ef96-2867-4691-8f72-b3c46b22aa68"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"cd7bbb64516cd671566812fec2f91c856246ef39","_cell_guid":"c190c9a9-ca7e-4e26-9310-70027fe0250c"},"cell_type":"markdown","source":"**Train machine learning algorithms without weather related data included**"},{"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train1, X_test1,Y_train1,Y_test1 = train_test_split(X1, Y1, test_size=0.33, random_state=99)\n#Without weather\nsvc = SVC()\nsvc.fit(X_train1, Y_train1)\nY_pred = svc.predict(X_test1)\nacc_svc1 = round(svc.score(X_test1, Y_test1) * 100, 2)\nacc_svc1\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train1, Y_train1)\nY_pred = knn.predict(X_test1)\nacc_knn1 = round(knn.score(X_test1, Y_test1) * 100, 2)\nacc_knn1\n\n\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train1, Y_train1)\nY_pred = logreg.predict(X_test1)\nacc_log1 = round(logreg.score(X_train1, Y_train1) * 100, 2)\nacc_log1\n\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train1, Y_train1)\nY_pred = gaussian.predict(X_test1)\nacc_gaussian1 = round(gaussian.score(X_test1, Y_test1) * 100, 2)\nacc_gaussian1\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train1, Y_train1)\nY_pred = perceptron.predict(X_test1)\nacc_perceptron1 = round(perceptron.score(X_test1, Y_test1) * 100, 2)\nacc_perceptron1\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train1, Y_train1)\nY_pred = linear_svc.predict(X_test1)\nacc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\nacc_linear_svc1\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train1, Y_train1)\nY_pred = sgd.predict(X_test1)\nacc_sgd1 = round(sgd.score(X_test1, Y_test1) * 100, 2)\nacc_sgd1\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train1, Y_train1)\nY_pred = decision_tree.predict(X_test1)\nacc_decision_tree1 = round(decision_tree.score(X_test1, Y_test1) * 100, 2)\nacc_decision_tree1\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train1, Y_train1)\nY_pred = random_forest.predict(X_test1)\nrandom_forest.score(X_train1, Y_train1)\nacc_random_forest1 = round(random_forest.score(X_test1, Y_test1) * 100, 2)\nacc_random_forest1\n","outputs":[],"metadata":{"_uuid":"54e8b2405ac6606d449c406e5ca88b7d3fbc53fe","collapsed":true,"_cell_guid":"bbe501fd-7d24-495c-85e5-8b7db2e11676"},"cell_type":"code","execution_count":null},{"source":"# Same with weather related data\n# Support Vector Machines\nX_train, X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.33, random_state=99)\n#with weather condition\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_test, Y_test) * 100, 2)\nacc_svc\n#KNN\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_test, Y_test) * 100, 2)\nacc_knn\n\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 2)\nacc_gaussian\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_test, Y_test) * 100, 2)\nacc_perceptron\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_test, Y_test) * 100, 2)\nacc_linear_svc\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_test, Y_test) * 100, 2)\nacc_sgd\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2)\nacc_decision_tree\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2)\nacc_random_forest","outputs":[],"metadata":{"_uuid":"1033ca19bf14b98ee32fcb90ca6cd8e09c49435e","collapsed":true,"_cell_guid":"77d79af9-d737-47ff-90fd-1f497af1e287"},"cell_type":"code","execution_count":null},{"source":"print(\"Machine Learning algorithm scores without weather related conditions\")\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc1, acc_knn1, acc_log1, \n              acc_random_forest1, acc_gaussian1, acc_perceptron1, \n              acc_sgd1, acc_linear_svc1, acc_decision_tree1]})\nmodels.sort_values(by='Score', ascending=False)\nprint(\"Machine Learning algorithm scores with weather related conditions\")\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)\n\n","outputs":[],"metadata":{"_uuid":"59d0a7a34f5e0537602a8ab1e8f5df769de82b3b","collapsed":true,"_cell_guid":"fd966c00-59aa-4379-9cf4-2c24ac99ff70"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"7308c1b29e96a953b04a00621c2b1293f74616b5","_cell_guid":"aa4e3615-d241-4bbc-ae19-f57df3a0dcf6"},"cell_type":"markdown","source":"**The results indicated that adding weather-related features to a machine learning algorithm in predicting severity of an accident did not substantially change the accuracy of models. \n**\n**The results indicate a high accuracy.\nAs this is multilabel classification, the accuracy measure in this case computes the amount of labels predicted that exactly match the corresponding set of labels.**\n\n**However, we have to take into account the accuracy paradox as sometimes it may be desirable to select a model with a lower accuracy because it has a greater predictive power on the problem. In our dataset  there is a large class imbalance as most accidents are classified as mild(class 3) as shown in the graph below.**"},{"metadata":{"_uuid":"579c4755a0fb42b6de5168ee53d48707f72ba9c0","_cell_guid":"70dec494-e22d-4413-8226-2ba4e90fe73e"},"cell_type":"markdown","source":""},{"source":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = df1.loc[:,df1.columns != 'accident_severity'], df1.loc[:,'accident_severity']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))\ny_test.value_counts()","outputs":[],"metadata":{"_uuid":"78f546c459fd30976cca68a4f37649e24018144a","collapsed":true,"_cell_guid":"d1148777-681f-4f20-ba23-cf3ea76443da"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"32afe452b310045548f1ce772a84875daeb7dd29","_cell_guid":"c4091235-3af0-4f17-a409-bb7b9b66e4c0"},"cell_type":"markdown","source":"**A model can predict the value of the majority class for all predictions and yield a high accuracy although almost all of the predictions would concern the majority class; hence, yielding a very high accuracy. Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. Precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned. Moreover, another metric is F1 score which is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score and returns an compromise between precision and recall. **\n\n**A clean and unambiguous way to present the prediction results of a classifier is to use a use a confusion matrix. On below is for without weather conditions and one below is with weather conditions included\n**"},{"source":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = df2.loc[:,df2.columns != 'accident_severity'], df2.loc[:,'accident_severity']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))\ny_test.value_counts()","outputs":[],"metadata":{"_uuid":"3642a69b56e5ae44f48481a68ec341ea0205be44","collapsed":true,"_cell_guid":"bfd2ac1b-6ae5-42fa-874f-609bd2ca21ab"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"3afc3217de22976f399aaea503d5725b819411ea","_cell_guid":"7ccd0051-abbf-4ce8-9d64-8b55217d3df1"},"cell_type":"markdown","source":"**The precision, recall, and F1 score are also at high levels of 0.89, 0.92, and 0.9 respectively - meaning that the classification is successful and the accuracy of the model is more or less 90\\% when investigated on multiple metrics. **\n\n**When taking into consideration the weather condition, the lighting condition, and road surface conditions the accuracy of machine learning models are as follows: **"},{"source":"sns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","outputs":[],"metadata":{"_uuid":"aba005788197862a3f780d880227469ffa9f120d","collapsed":true,"_cell_guid":"7bb3d089-bfc8-48ae-8b2f-151b36e2afe3"},"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"6a91e6058852fefa114e455e1ce7cf3467cbe7f5","_cell_guid":"d4fd73f3-0343-4cdd-b7fc-db2a32fb0b49"},"cell_type":"markdown","source":"**The results indicated that adding weather-related features to a machine learning algorithm in predicting severity of an accident did not change the accuracy of the model. When adding three features of light condition, weather condition, and the condition of the road surface, the measures of recall, precision, and f1-score remained unchanged. **\n\n**When looking at the overall performance of all of the algorithms, there was an increase in accuracy between the data with weather conditions when compared to data without weather related conditions. Namely, random forest algorithm increased performance by 0.59%. The previous top performer when no weather related conditions were introduced, Logistic Regression, sustained the same level of accuracy. Hence, it was concluded to further scrutinize the recall, precision, and f1-score of random forest algorithm to see whether there was an actual change in prediction power. **\n\n"}],"metadata":{"language_info":{"version":"3.6.3","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"nbformat":4}