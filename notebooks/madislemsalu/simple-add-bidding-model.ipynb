{"cells":[{"metadata":{},"cell_type":"markdown","source":"### The aim of this kernel is to build a model that predicts the best facebook ad campaign for a given customer. \n#### There will be feature engineering, missing value imputation, and visualisations"},{"metadata":{},"cell_type":"markdown","source":"This data science assignment involved building an add-bidding model to offer the right\ntype of ad out of three distinct campaigns to the right people.\nFirstly, the given data had two\ncolumns of campaign id and fb\ncampaign id with about half of the\ndata missing. Upon further\ninvestigation, the missing data\nseemed to be missing not at\nrandom(MNAR). This was derived\ndue to the missing value columns\nfollowing a identical distribution with\nrest of the data.\n\nI made the assumption that the missing campaign_ids would most likely be 1178, as only\nthree distinct campaign ids were given in this data and 1178 would follow a natural pattern\nin the data structure. I applied KNN-imputation with the choice of K-value of 3 that\ncompared the three closest data points and it imputed everything to 1178. However, this\napproach may not be correct and the missing columns may also include the other two\ncampaign ids.\n\nRegarding data preprocessing, I used binary categorization for gender and hot-encoded\nage. This was done mainly due to technical correctness in order to remove perceived\nmathematical ordering of the data by the machine learning model. I also took the log of the\ncontinuous variables for scaling and combating kurtosis.\n\nI employed simple Logistic Regression due to the small data size and small cost-benefit for\nexploration of more advanced models. I implemented Logistic Regression with recursive\nfeature elimination to see which variables were redundant for predictive power. It turned\nout that all of the features had an increase on the predictive power."},{"metadata":{"trusted":true,"_uuid":"5693056da7ffaf21a3d1f450da7204faf2b31612"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt # For plotting\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\npd.options.display.max_rows = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d46f63e84b76b75117121350b40d347eaa629cb"},"cell_type":"code","source":"df = pd.read_csv(\"../input/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f8adf823ea968fbdefe9dc42b0687435bdea6e3"},"cell_type":"markdown","source":"## Restructuring"},{"metadata":{"_uuid":"d84db8849b7c2a52a85c072ba91d6734eea06103"},"cell_type":"markdown","source":"### The data has some structural problems starting from row number 761. Let's restructure the dataset and apply  basic feature engineering and then visualise wheter the we can use the data from row 761 onwards. "},{"metadata":{"trusted":true,"_uuid":"a06f6165ad167591ec7b4e28e06fa430a7c5e486"},"cell_type":"code","source":"\n#split the data to two dataframes - df2 with missing values\ndf1 = df[0:761]\ndf2 = df[761:]\n# restructure the df by shifting the columns to match between df1 and df2\nc = list(df2)\nfor x in range(12):\n    c[x+1] = c[x+3]\n    \ndf2.columns = c\n# further restructuring\ndf2 = df2.iloc[:, :-2]\ndf2.rename(columns={'campaign_id': 'reporting_start','fb_campaign_id': 'reporting_end'}, inplace=True)\n\ndf2.insert(3, 'campaign_id',np.NaN)\ndf2.insert(4,'fb_campaign_id',np.NaN)\n\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"affd1fee8aa18cdb64a24d01677290116603344c"},"cell_type":"code","source":"df = df1.append(df2, ignore_index=True) # final dataframe \ndf.head() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b25f6cabfb31dc8d1056c88d6b96e6eafb5d50a"},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"d22be0cfa6cc0a891f5276cffcd78d15939da788"},"cell_type":"code","source":"import datetime\n#see how long the campaign durations have been\ndf['reporting_start'] = pd.to_datetime(df['reporting_start'] )\ndf['reporting_end'] = pd.to_datetime(df['reporting_end'] )\n\ndf['campaign_duration']= df['reporting_start']-df['reporting_end']\n\ndf['campaign_duration'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4de3baae742f86a9a4d990ef6c9f88f546282c14"},"cell_type":"markdown","source":"##### Since all of the campaigns lasted within one day, I decided to remove the variables as they do not offer any information gain"},{"metadata":{"trusted":true,"_uuid":"e53b5211c2a92103b987da268f74ce8a89232de8"},"cell_type":"code","source":"df.drop(['campaign_duration','reporting_start','reporting_end'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b72fe680d5f93b5221cfb61c6639f51f679605df"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ndf['gender']=lb.fit_transform(df['gender']) # label encode gender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78f17f47996e656f0dc029369fe9e47626b4fbb0"},"cell_type":"code","source":"df['total_conversion'] = df['total_conversion'].astype(int) # change these variables to the proper format of an integer\ndf['approved_conversion'] = df['approved_conversion'].astype(int)\ndf['impressions'] = df['impressions'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2efb8c76ef81937c1a32fc8ab4ce23306764ae62"},"cell_type":"code","source":"df = pd.concat([df,pd.get_dummies(df['age'],prefix='age')],axis=1) # get dummies for age \ndf.drop('age',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb9fb11b6c8847cbba0cde19ee3ec3a5be002ca1"},"cell_type":"code","source":"# The dataframe should be solid now. \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ec7a4853b918b280e16f6e8cdbd72bf50e40345"},"cell_type":"markdown","source":"### Imputation"},{"metadata":{"_uuid":"aab4280142ff935dca2d445dbbe7741d95eb940b"},"cell_type":"markdown","source":"### The two column values of campaign_id and fb_campaign_id are missing. As campaign_id has only three distinct values, it will be a fundamental part of the add bidding model. Consequently, I will focus my attention on it. "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"18c2b1630cd6e02bccac0619b9dc6091a353c3a5"},"cell_type":"code","source":"sns.countplot(df[\"campaign_id\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac8d1e8f7ad05a3f5b1edd662be19b5bf96729d6"},"cell_type":"code","source":"# imputation of missing values\nfrom fancyimpute import KNN, NuclearNormMinimization, SoftImpute, IterativeImputer, BiScaler\nX = pd.DataFrame(KNN(k=3).fit_transform(df))\nX.columns = df.columns\nX.index = df.index\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"234d171c27080dd6e3a5cdea40e951842a597d9b"},"cell_type":"code","source":"X['campaign_id'] = X['campaign_id'].astype(int)\nX['fb_campaign_id'] = X['fb_campaign_id'].astype(int)\nX['campaign_id'] = X['campaign_id'].replace(1177,1178) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7a8ece56a298d0e40d69ac4897b9579f451c825"},"cell_type":"code","source":"sns.countplot(X[\"campaign_id\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63a1700934969b188acf18d1108e7f71ff45033f"},"cell_type":"code","source":"sns.countplot(df[\"campaign_id\"]) # compared to the original\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd4327c6b5f3ba913dc5c84ca840c74473d7b5a"},"cell_type":"markdown","source":"### Visualisation\n\n#### Now that the dataframe is restructured - let's identify some patterns in the data. Specifc focus point is the difference between the two dataframes"},{"metadata":{"trusted":true,"_uuid":"1aa11232c4543657da35a98a3533f609e4e6eab1"},"cell_type":"code","source":"def distComparison(df1, df2): # A function to see the distribution of each feature\n    a = len(df1.columns)\n    if a%2 != 0:\n        a += 1\n    \n    n = np.floor(np.sqrt(a)).astype(np.int64)\n    \n    while a%n != 0:\n        n -= 1\n    \n    m = (a/n).astype(np.int64)\n    coords = list(itertools.product(list(range(m)), list(range(n))))\n    \n    numerics = df1.select_dtypes(include=[np.number]).columns\n    cats = df1.select_dtypes(include=['category']).columns\n    \n    fig = plt.figure(figsize=(15, 15))\n    axes = gs.GridSpec(m, n)\n    axes.update(wspace=0.25, hspace=0.25)\n    \n    for i in range(len(numerics)):\n        x, y = coords[i]\n        ax = plt.subplot(axes[x, y])\n        col = numerics[i]\n        sns.kdeplot(df1[col].dropna(), ax=ax, label='df').set(xlabel=col)\n        sns.kdeplot(df2[col].dropna(), ax=ax, label='df_missing')\n        \n    for i in range(0, len(cats)):\n        x, y = coords[len(numerics)+i]\n        ax = plt.subplot(axes[x, y])\n        col = cats[i]\n\n        df1_temp = df1[col].value_counts()\n        df2_temp = df2[col].value_counts()\n        df1_temp = pd.DataFrame({col: df1_temp.index, 'value': df1_temp/len(df1), 'Set': np.repeat('df1', len(df1_temp))})\n        df2_temp = pd.DataFrame({col: df2_temp.index, 'value': df2_temp/len(df2), 'Set': np.repeat('df2', len(df2_temp))})\n\n        sns.barplot(x=col, y='value', hue='Set', data=pd.concat([df1_temp, df2_temp]), ax=ax).set(ylabel='Percentage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c1b378440d46fafbc81def8161001a9395f2c0"},"cell_type":"code","source":"import itertools\nimport matplotlib.gridspec as gs\n\ndf_missing= X[761:]\ndf_not_missing= X[0:761]\n\ndistComparison(df_not_missing, df_missing)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4dfa4c31efb218554d30928dc7a1026459759f6"},"cell_type":"markdown","source":"### We can discover some interesting patterns from the data. \n### The age distribution is fairly same in both datasets, df_missing having proportionally more younger people. The gender is interestingly split. Df_missing consists proportionally more of females and df proportionally more of males. \n\n### Lastly, it seems to be that df_missing has a higher mean and a far higher standard deviation for features of clicks, spent, and conversions. "},{"metadata":{"_uuid":"75c425dd4fc02cf207fffdded4747d3e4513cb58"},"cell_type":"markdown","source":"## Model Fitting"},{"metadata":{"trusted":true,"_uuid":"86c4841440c83d5ae8a5f4f9124dc57467d186e6"},"cell_type":"code","source":"df = X.copy()\n\ndf.spent=df.spent.astype(int)\ndf.interest1=df.interest1.astype(int)\ndf.interest2=df.interest2.astype(int)\ndf.interest3=df.interest3.astype(int)\ndf.campaign_id=df.campaign_id.astype('category')\n\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c716cf21ec2998fc2a4fc6ccb7836bab995b8b30"},"cell_type":"code","source":"df.dropna(inplace=True)\n\ndf['approved_conversion'] = df['approved_conversion'].replace([range(2,22)], 1)\n\ndf.approved_conversion=df.approved_conversion.astype('category')\ndf['approved_conversion'].value_counts()\n\n## to have class balance and for the purpose of the add-bidding model. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89325e1df93d657f3b9c5e24acf0f8f1b88daf6e"},"cell_type":"code","source":"from sklearn.metrics import  classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"871f0d281de54dd2c19bfa6a847a39d5f2039201"},"cell_type":"code","source":"df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8fb5b4fafb7c835cf0d245ad6ed52f6f10bbf1c"},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f7d9d6ed2629249d6619642bea06eab02ea4c19"},"cell_type":"code","source":"# Taking the log of the continious variables to mitigate kurtosis and skewdness as much as possible\n\ncol= [['interest1','interest2','interest3']]\nfor cols in col:\n    df[cols] = np.log(df[cols])\n    df[cols] = np.log(df[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d763818beb409b21db8812aeb7e5da4685c07c86"},"cell_type":"code","source":"X = df[[ 'campaign_id','interest1','interest2','interest3','gender','age_30-34','age_35-39','age_40-44','age_45-49']]\ny = df['approved_conversion']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e17447b672c668c5697af7be770c7f95b7177694"},"cell_type":"code","source":"## With oversampling follwed by undersampling to improve the score. Uncomment to see the improvement\n\nfrom imblearn.combine import SMOTETomek\n\n# smt = SMOTETomek(ratio='auto')\n# X, y = smt.fit_sample(X, y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c83417d6bd001cd9feeb7554b69abb449b31de18"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n\nlogmodel = LogisticRegression()\nlogmodel= RFE(logmodel, 9)\nlogmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c399d02e0edf233d350b7b92cb15bd436b7bd0d2"},"cell_type":"code","source":"predictions = logmodel.predict(X_test)\nprint(classification_report(y_test,predictions))\nprint(logmodel.ranking_) ## For RFE \nprint(logmodel.support_) ## For RFE\n\n# All features are required, if you set RFE to less than 9 the performance (f1-score) will decrease","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"758e8eba328fe35f1ff7d786be72c08fd591b628"},"cell_type":"markdown","source":"## Add-bidding Model"},{"metadata":{"trusted":false,"_uuid":"dc005d29db93bb44a5245ca4218ac1068526ca0f"},"cell_type":"code","source":"X_test[:10]  # Here is the testing data that the model hasn't seen before. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64db387556a7f54794b32e40c006b25ed920bcf7"},"cell_type":"code","source":"# model's prediction for the first 10 rows of test data\nlogmodel.predict(X_test[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"658b9be72096410abd9ad03761d6aea88b886a7a"},"cell_type":"code","source":"#Prediction for the first row\nX1 = X_test[:1]\nX1\nlogmodel.predict(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc59099ac2ada37e93ec2ff6d5de62353d36971e"},"cell_type":"code","source":"# prediction when the campaign_id is changed to 916\nX1['campaign_id'] = X1['campaign_id'].replace(936, 916)\nlogmodel.predict(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b94c4bd4265490b94b42bffc2c0cfc319104040e"},"cell_type":"code","source":"#prediction to when the campaign id is change to 1178\nX1['campaign_id'] = X1['campaign_id'].replace(916, 1178)\nlogmodel.predict(X1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ee5298958355ce57c27e7d9d219a1c777054539"},"cell_type":"markdown","source":"### Given any input vector, one can change the campaign_id variable to see which campaign would end up in an conversion according to the model. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}