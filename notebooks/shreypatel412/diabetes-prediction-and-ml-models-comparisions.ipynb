{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from mlxtend.plotting import plot_decision_regions\nimport numpy as np\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport missingno as msno\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport seaborn as sns\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import train_test_split\nsns.set()\nimport warnings\nfrom sklearn import model_selection\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n#plt.style.use('ggplot')\n#ggplot is R based visualisation package that provides better graphics with higher level of abstraction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the dataset\n## basic statistic details about the data (note only numerical columns would be displayed here unless parameter include=\"all\")\n\n\ndiabetes_data = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n\n#Print the first 5 rows of the dataframe.\ndiabetes_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diabetes_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#It is better to replace zeros with nan since after that counting\n# them would be easier and zeros need to be replaced with suitable values#cleaning the data set\ndiabetes_data_copy = diabetes_data.copy(deep = True)\ndiabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\n## showing the count of Nans\n\n\nprint(diabetes_data_copy.isnull().sum())\n#As we can see that there are null values in multiple columns\n#To fill these Nan values the data distribution needs to be understood \n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diabetes_data.hist(figsize = (10,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Aiming to impute nan values for the columns in accordance with their distribution \n## basicaly this is recursive cell to replace the nan's and zeros for other columns to finalize the cleaned dataset\n\ndiabetes_data_copy['Glucose'].fillna(diabetes_data_copy['Glucose'].mean(), inplace = True)\ndiabetes_data_copy['BloodPressure'].fillna(diabetes_data_copy['BloodPressure'].mean(), inplace = True)\ndiabetes_data_copy['SkinThickness'].fillna(diabetes_data_copy['SkinThickness'].median(), inplace = True)\ndiabetes_data_copy['Insulin'].fillna(diabetes_data_copy['Insulin'].median(), inplace = True)\ndiabetes_data_copy['BMI'].fillna(diabetes_data_copy['BMI'].median(), inplace = True)\ndiabetes_data_copy.isna().sum()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here after cleansing the dataset we have plots \npaftercleaning = diabetes_data_copy.hist(figsize = (10,10))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame(diabetes_data)\ndfupdated=pd.DataFrame(diabetes_data_copy)\nprint(diabetes_data.shape,diabetes_data_copy.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## data type analysis\n#plt.figure(figsize=(5,5))\n#sns.set(font_scale=2)\nsns.countplot(y=diabetes_data.dtypes.map(str) ,data=diabetes_data)\nplt.xlabel(\"count of each data type\")\nplt.ylabel(\"data types\")\n\n\nplt.show()\ndiabetes_data.info(verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## null count analysis\n\np=msno.bar(diabetes_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the balance of the data by plotting the count of outcomes by their value\n## here we get over view for the data of wether what majority is classified for \ndiabetes_data.Outcome.value_counts().plot(kind=\"pie\")\nprint(\"The below graph shows that the data is biased towards datapoints having outcome value as 0 where it means that diabetes was not present actually. \\nThe number of non-diabetics is almost twice the number of diabetic patients\\n\",       diabetes_data.Outcome.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scatter matrix of uncleaned data\n\np=scatter_matrix(diabetes_data,figsize=(25, 25)) \n\n#This pairs plot builds onto two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows us to see the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof)between two variables.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pair plot for clean data \n\np=sns.pairplot(diabetes_data_copy, hue = 'Outcome')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pearson's Correlation Coefficient: helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. \n\n#The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\n# A heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.\n\n#Heatmap for unclean data\nplt.figure(figsize=(18,16))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_data.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Heatmap for clean data \n\nplt.figure(figsize=(18,16))  \np=sns.heatmap(diabetes_data_copy.corr(), annot=True,cmap ='RdYlGn')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaling the data data Z is rescaled such that Œº = 0 and ùõî = 1, and is done through this formula: ùôØ= ( ùô≠(ùôû)-Œº ) / ùõî \n# Standardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance).\n#It is useful to standardize attributes for a model that relies on the distribution of attributes such as Gaussian processes.\n\nfrom sklearn.preprocessing import StandardScaler \nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(diabetes_data_copy.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\n# Data rescaling is an important part of data preparation before applying machine learning algorithms.\n\nX.head() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X = diabetes_data.drop(\"Outcome\",axis = 1)\ny = diabetes_data_copy.Outcome  # assigning the label column\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Why Scaling the data for KNN?\n# it is always advisable to bring all the features to the same scale for applying distance based algorithms like KNN.\n# We can imagine how the feature with greater range with overshadow or dimenish the smaller feature completely and this will impact the performance of all distance     based model as it will give higher weightage to variables which have higher magnitude","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Train Split and Cross Validation methods\n\n# Train Test Split : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the      model performance much better.\n\n# Cross Validation: When model is split into training and testing it can be possible that specific type of data point may go entirely into either training or testing   portion. This would lead the model to perform poorly. Hence over-fitting and underfitting problems can be well avoided with cross validation techniques\n\n# About Stratify : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to    parameter stratify.\n\n# For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your       random split has 25% of 0's and 75% of 1's.\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n#print(\"X_train : \\n\\n{} \\nX_test : \\n\\n{} \\nY_train : \\n\\n{} \\nY_test : \\n\\n{}\".format(X_train,X_test,y_train,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nmodels.append(('Logistic Regression              ', LogisticRegression()))\nmodels.append(('Linear Discriminant Analysis     ', LinearDiscriminantAnalysis()))\nmodels.append(('Random Forest Classifier         ', RandomForestClassifier()))\nmodels.append(('KNeighbors Classifier            ', KNeighborsClassifier()))\nmodels.append(('Decision Tree Classifier         ', DecisionTreeClassifier()))\nmodels.append(('Gaussian Naive Bayes             ', GaussianNB()))\nmodels.append(('Support vector machine Classifier', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    pipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\n    pipeline.fit(X_train, y_train)\n    cv_results = model_selection.cross_val_score(pipeline, X, y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: - (mean.)=%f - (stdev.)=(%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg,\"\\n\")\n    \n# boxplot algorithm comparison\nfig = plt.figure()\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\n\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix\n\npipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\npipeline.fit(X_train, y_train)\nprediction = pipeline.predict(X_test)\n\nprint(f\"Accuracy Score : {round(accuracy_score(y_test, prediction) * 100, 2)}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}