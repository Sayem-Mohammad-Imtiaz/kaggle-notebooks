{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"##importing required libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##reading the Wine Quality data\ndf=pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"data dimensions: \", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()##checking missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets check the independent variables with Quality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'quality', y = 'alcohol', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'quality', y = 'fixed acidity', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'quality', y = 'volatile acidity', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'quality', y = 'citric acid', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'quality', y = 'sulphates', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'quality', y = 'pH', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### form the above plots we can see that \n* in higher quality wines, sulphates and citric acid are in increased amount\n* in 7 and 8 quality wines, alcohol content is slightly more\n* fixed acidity and pH has no impact on quality of wines\n* volatile acidity is less in higher quality wines","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Correlation Matrix\ncorr = df.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since we have 8 types of quality, we are encoding it to Good quality and bad quality. Creating a new variable which has 1 for quality>=7 i.e. good quality and 0 for bad i.e. <7","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Classification version of target variable\ndf['goodquality'] = [1 if x >= 7 else 0 for x in df['quality']]\n##now the target variable is goodquality\n# Separate feature variables and target variable\nX = df.drop(['quality','goodquality'], axis = 1)\ny = df['goodquality']\nprint(\"Independent Variables: \",X.shape)\nprint(\"Dependent Variables: \",y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See proportion of good vs bad wines\nprint(df['goodquality'].value_counts())\nsns.countplot(df['goodquality'],label=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='density',y='fixed acidity',data=df,fit_reg=False,hue='goodquality') \nsns.lmplot(x='pH',y='fixed acidity',data=df,fit_reg=False,hue='goodquality') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Standardize the data\nfrom sklearn.preprocessing import StandardScaler\nX_features = X\nX = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into Train and test - 80-20 ratio\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\nprint(\"Training Data: \",X_train.shape, y_train.shape)\nprint(\"Test Data: \",X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1- Logistic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n## checking the metrics of model on test data\nacc_log = round(logreg.score(X_test, y_test)*100, 2)\nprint(\"Accuracy on test data: \",acc_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, make_scorer\npred_log = logreg.predict(X_test)\nauc_log = round(roc_auc_score(y_test, pred_log)*100,2)\nprint(\"AUC: \",  auc_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2- SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\n## checking the metrics of model on test data\nacc_svc = round(svc.score(X_test, y_test)*100, 2)\nprint(\"Accuracy on test set: \",acc_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_svc = svc.predict(X_test)\nauc_svc = round(roc_auc_score(y_test, pred_svc)*100,2)\nprint(\"AUC: \",  auc_svc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3- KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(X_train, y_train)\n## checking the metrics of model on test data\nacc_knn = round(knn.score(X_test, y_test)*100, 2)\nprint(\"Accuracy on test data: \",acc_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Predicting on test data and checking the metrics\npred_knn = knn.predict(X_test)\nauc_knn = round(roc_auc_score(y_test, pred_knn)*100,2)\nprint(\"AUC: \",  auc_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 4- Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nacc_gaussian = round(gaussian.score(X_test, y_test) * 100, 2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_nb = gaussian.predict(X_test)\nauc_nb = round(roc_auc_score(y_test, pred_nb)*100,2)\nprint(\"AUC: \",  auc_nb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 5- Perceptron","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nacc_p = round(perceptron.score(X_test, y_test) * 100, 2)\nacc_p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_p = perceptron.predict(X_test)\nauc_p = round(roc_auc_score(y_test, pred_p)*100,2)\nprint(\"AUC: \",  auc_p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 6- Cart ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n#Building the model\nmodel_ct = DecisionTreeClassifier(criterion='gini',random_state=1)\nmodel_ct.fit(X_train,y_train) ## training the model\n## checking the metrics of model on test data\nacc_ct=round(model_ct.score(X_test, y_test)*100,2)\nprint(\"Accuracy on test data: \",acc_ct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\npred_ct = model_ct.predict(X_test)\nauc_ct = round(roc_auc_score(y_test, pred_ct)*100,2)\nprint(\"AUC: \",  auc_ct)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 7- Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=101, random_state=0)\nrf.fit(X_train, y_train)\n## checking the metrics of model on train/test data\nacc_rf=round(rf.score(X_test, y_test)*100,2)\nprint(\"Accuracy on test data: \",acc_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rf = rf.predict(X_test)\nauc_rf = round(roc_auc_score(y_test, pred_rf)*100,2)\nprint(\"AUC: \",  auc_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 8 - Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(random_state=1)\ngb.fit(X_train, y_train)\n## checking the metrics of model on test data\nacc_gb=round(gb.score(X_test, y_test)*100,2)\nprint(\"Accuracy on test set: \",acc_gb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_gb = gb.predict(X_test)\nauc_gb = round(roc_auc_score(y_test, pred_gb)*100,2)\nprint(\"AUC: \",  auc_gb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 9 - XGBoosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgb = xgb.XGBClassifier(random_state=1)\nxgb.fit(X_train, y_train)\n## checking the metrics of model on test data\nacc_xgb=round(xgb.score(X_test, y_test)*100,2)\nprint(\"Accuracy on test data: \",acc_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_xgb = xgb.predict(X_test)\nauc_xgb = round(roc_auc_score(y_test, pred_xgb)*100,2)\nprint(\"AUC: \",  auc_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 10- Stochastic Gradient Descent","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nsgd.fit(X_train, y_train)\n## checking the metrics of model on test data\nacc_sgd=round(sgd.score(X_test, y_test)*100,2)\nprint(\"Accuracy on test set: \",acc_sgd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_sgd = sgd.predict(X_test)\nauc_sgd = round(roc_auc_score(y_test, pred_sgd)*100,2)\nprint(\"AUC: \",  auc_sgd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Gradient Boosting', \n              'Decision Tree','XGBoosting'],\n    'Accuracy': [acc_svc, acc_knn, acc_log, \n              acc_rf, acc_sgd, acc_p, \n              acc_sgd, acc_gb, acc_ct, acc_xgb],\n    'AUC' : [auc_svc, auc_knn, auc_log, \n              auc_rf, auc_sgd, auc_p, \n              auc_sgd, auc_gb, auc_ct,auc_xgb]})\nmodels.sort_values(by='Accuracy', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### From the above table we can see that Random Forest, KNN and SVM have given best accuracy but AUC is low for SVM. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_importances = pd.Series(rf.feature_importances_, index=X_features.columns)\nxgb_importances = pd.Series(xgb.feature_importances_, index=X_features.columns)\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nrf_importances.nlargest(25).sort_values(ascending=True).plot(kind='barh', title='Importance by Rf')\nplt.subplot(1,2,2)\nxgb_importances.nlargest(25).sort_values(ascending=True).plot(kind='barh',title='Importance by XGB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From both the models: Alcohol, Sulphates and Volatile acidity are top 3 Variables which have significant impact on Wine Quality.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Further steps- Tuning models for better performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Finding best param for SVM model\nparam = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\ngrid_svc = GridSearchCV(svc, param_grid=param, scoring='accuracy', cv=10)\ngrid_svc.fit(X_train, y_train)\nprint(\"Accuracy after grid search: \",round(grid_svc.score(X_test, y_test)*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best=grid_svc.best_params_\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's run our SVC again with the best parameters.\nsvc2 = SVC(C = 1.2, gamma =  1.2, kernel= 'rbf')\nsvc2.fit(X_train, y_train)\npred_svc2 = svc2.predict(X_test)\nacc_svc2 =round(svc2.score(X_test, y_test)*100,2)\nprint(\"Accuracy on test set: \",acc_svc2)\nauc_svc2 = round(roc_auc_score(y_test, pred_svc2)*100,2)\nprint(\"AUC: \",  auc_svc2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that accuracy of SVM is increased to 93.12 now!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets try to do some evaluation for random forest model using cross validation.\nrfc_eval = cross_val_score(estimator = rf, X = X_train, y = y_train, cv = 10)\nrfc_eval.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': [101,201,251], 'max_features': [6,7,8], 'max_depth':[7,8,9]}\nrf1 = GridSearchCV(RandomForestClassifier(), param_grid, cv=10, \n                   scoring=make_scorer(accuracy_score))\nrf1.fit(X_train, y_train)\nprint(\"Accuracy :\",round(rf1.score(X_test, y_test)*100,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We are not able to increase the accuracy of Rf using cross validation and Grid Search any further.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('---Comparison Of Best 2 Models---')\nprint('RF Model Accuracy:',acc_rf,',Auc:',auc_rf)\nprint('SVM Model Accuracy:',acc_svc2,',Auc:',auc_svc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"=== Confusion Matrix of RF Model ===\")\nprint(confusion_matrix(y_test, pred_rf))\nprint(\"=== Classification Report of RF Model ===\")\nprint(classification_report(y_test, pred_rf))\nprint('\\n')\nprint(\"=== Confusion Matrix of SVM Model ===\")\nprint(confusion_matrix(y_test, pred_svc2))\nprint(\"=== Classification Report of SVM Model ===\")\nprint(classification_report(y_test, pred_svc2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparing the 2 confusion Matrix, we saw that SVM has better Recall. We can choose the metrics as per our problem requirement. Thankyou!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}