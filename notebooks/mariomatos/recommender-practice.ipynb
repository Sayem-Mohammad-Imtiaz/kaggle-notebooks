{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Scratchpad notebook to follow along: https://www.kaggle.com/ibtesama/getting-started-with-a-movie-recommendation-system","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf1 = pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_credits.csv')\ndf2 = pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_movies.csv')\n\ndf1 = df1.rename({'movie_id': 'id'}, axis=1)\ndf1 = df1.drop(['title'], axis=1)\n\ndf2 = df2.merge(df1, on='id')\n\ndf2.head()\n\ndf2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C = df2['vote_average'].mean()\nm = df2['vote_count'].quantile(0.9)\nC, m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter out movies that don't have 90 % of vote count\nq_movies = df2.copy().loc[df2['vote_count'] >= m]\nq_movies.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    return (v/(v+m) * R) + (m/(m+v) * C)\n\nq_movies['score'] = q_movies.apply(weighted_rating, axis=1)\nq_movies = q_movies.sort_values('score', ascending=False)\n\nq_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pop = df2.sort_values('popularity', ascending=False)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\n\nplt.barh(pop['title'].head(6), pop['popularity'].head(6), align='center') \nplt.gca().invert_yaxis()\nplt.xlabel('Popularity')\nplt.title('Popular Movies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['overview'].head(5)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ntfidf = TfidfVectorizer(stop_words='english')\n\ndf2['overview'] = df2['overview'].fillna('')\n\ntfidf_matrix = tfidf.fit_transform(df2['overview'])\n\ntfidf_matrix.shape\n\ncosine_sim = linear_kernel(tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying cosine similarity\n\ndocuments = [\n    'alpine snow winter boots.',\n    'snow winter jacket.',\n    'active swimming briefs',\n    'active running shorts',\n    'alpine winter gloves'\n]\n\ncntvt = CountVectorizer(stop_words='english')\n\ntfidf_matrix = cntvt.fit_transform(documents)\ncntvt.get_feature_names()\ntfidf_matrix.todense()\n\ncos_sim = cosine_similarity(tfidf_matrix)\ncos_sim\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = pd.Series(df2.index, index=df2['title']).drop_duplicates()\n\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]\n    movie_indices = [i[0] for i in sim_scores]\n    return df2['title'].iloc[movie_indices]\n\nidx = indices[\"The Dark Knight Rises\"]\ndf2['title'].iloc[[i[0] for i in (sorted(list(enumerate(cosine_sim[idx])), key=lambda x: x[1], reverse=True)[1:11])]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Avengers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# literal_eval is a python function to evaluate correctness of string data. It\n# will also create python objects for you \nfrom ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres']\n\ndf2['cast'][0]\ndf2['crew'][0]\ndf2['keywords'][0]\ndf2['genres'][0]\n\nfor feature in features:\n    df2[feature] = df2[feature].apply(literal_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# return top 3\ndef get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        if len(names) > 3:\n            names = names[:3]\n        return names\n    \n    return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['director'] = df2['crew'].apply(get_director)\n\nfeatures = ['cast', 'keywords', 'genres']\nfor feature in features:\n    df2[feature] = df2[feature].apply(get_list)\n    \ndf2[['title', 'cast', 'director', 'keywords', 'genres']].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning and prepa\n\ndef clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    df2[feature] = df2[feature].apply(clean_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create \"soup\" for the vectorization used to compute the cosine similarity matrix\n\ndef create_soup(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\ndf2['soup'] = df2.apply(create_soup, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(df2['soup'])\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)\ndf2 = df2.reset_index()\nindices = pd.Series(df2.index, index=df2['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Godfather', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# User-User, Item-Item Collaborative filtering\n\nfrom surprise import Reader, Dataset, KNNBasic\nfrom surprise.model_selection import cross_validate, KFold\n\nreader = Reader()\nratings = pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\nratings.head()\n\ndata = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n\nsvd = KNNBasic()\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = data.build_full_trainset()\nsvd.fit(trainset)\n\nratings[ratings['userId'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd.predict(1, 3671)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}