{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size = \"6\">**Default of Credit Card Clients - Predictive Models**</font></center></h1>\n<a id='0'><font size = \"5\">**Content**</font></a>\n- <a href='#1'>Introduction</a>\n- <a href='#2'>Load Packages and Data</a>\n- <a href='#3'>Check and Examination of the Data</a>\n    - <a href='#31'>Overview the data</a>\n    - <a href='#32'>Check Data Unbalance</a>\n    - <a href='#33'>Data Conversion</a>\n- <a href='#4'>Data Exploration and Data Visualization</a>\n- <a href='#5'>Predictive models</a>\n    - <a href='#51'>Random Forrest Classifier</a> \n    - <a href='#52'>Decision Tree Classifier</a>\n    - <a href='#53'>KNN Classifier</a> \n    - <a href='#54'>Adaboost Classifier</a> \n    - <a href='#55'>Roc-Auc Curve</a>\n    - <a href='#56'>Cross Validation for Algorithms</a>\n    - <a href='#57'>Comparison of Algorithms</a>\n    - <a href='#58'>Change the Features Used for Algorithm</a>\n- <a href='#6'>Conclusions</a>\n- <a href='#7'>References</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"1\">Introduction</a>  \n\n## Preface\nFirst of all, I would like to thank my friend ***Hülya Nur Aytaç***, who helped me with this project. My goal in this project is to examine the characteristics of people who have not paid or have not paid off their loan debt based on this data. During this review, I will use data visualization, machine learning and similar tools. Since this problem is a classification problem, I will use different algorithms -they are available for classification problem- for create model. I will try finding that optimal parameters for these algorithms. Finally, I will compare that achievement scores of algorithms and I will have an idea for this problem.\n\n## Information\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from ***April 2005*** to ***September 2005***. \n\n## Inspiration and Idea\nSome ideas for exploration:\n\n* How does the probability of default payment vary by categories of different demographic variables?\n* Which variables are the strongest predictors of default payment?\n\n## Content of Data\n* **ID**: ID of each client\n* **LIMIT_BAL**: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n* **SEX**: Gender (1=male, 2=female)\n* **EDUCATION**: (0=?, 1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* **MARRIAGE**: Marital status (0=?,1=married, 2=single, 3=others)\n* **AGE**: Age in years\n* **PAY_0**: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n* **PAY_2**: Repayment status in August, 2005 (scale same as above)\n* **PAY_3**: Repayment status in July, 2005 (scale same as above)\n* **PAY_4**: Repayment status in June, 2005 (scale same as above)\n* **PAY_5**: Repayment status in May, 2005 (scale same as above)\n* **PAY_6**: Repayment status in April, 2005 (scale same as above)\n* **BILL_AMT1**: Amount of bill statement in September, 2005 (NT dollar)\n* **BILL_AMT2**: Amount of bill statement in August, 2005 (NT dollar)\n* **BILL_AMT3**: Amount of bill statement in July, 2005 (NT dollar)\n* **BILL_AMT4**: Amount of bill statement in June, 2005 (NT dollar)\n* **BILL_AMT5**: Amount of bill statement in May, 2005 (NT dollar)\n* **BILL_AMT6**: Amount of bill statement in April, 2005 (NT dollar)\n* **PAY_AMT1**: Amount of previous payment in September, 2005 (NT dollar)\n* **PAY_AMT2**: Amount of previous payment in August, 2005 (NT dollar)\n* **PAY_AMT3**: Amount of previous payment in July, 2005 (NT dollar)\n* **PAY_AMT4**: Amount of previous payment in June, 2005 (NT dollar)\n* **PAY_AMT5**: Amount of previous payment in May, 2005 (NT dollar)\n* **PAY_AMT6**: Amount of previous payment in April, 2005 (NT dollar)\n* **default.payment.next.month**: Default payment (1=yes, 0=no)"},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"2\">Load Packages and Data</a>\n## Load Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score,roc_auc_score,accuracy_score,roc_curve\nimport itertools\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Properties of Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there are no empty values here. But as an alternative, we can look at it like this."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"3\">Check and Examination of Data</a>\n## <a id=\"31\">Overview the Data</a>\nIn this section, I will both rename and assign values. As state below, I find a few missing or suspicious data.  Fortunately, I can assignment because that data is so scarce that it can be ignored.\n* In the Education attribute, 0-4-5-6 numbers value assigned as a 4."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.EDUCATION.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In the Marriage attribute, the value 0 means unknown and the value 3 means other. Therefore, the value 0 assigned as 3."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.MARRIAGE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, attributes need to be renamed. For example,\n* default.payment.next.month ---> def_pay\n* PAY_0 ---> PAY_1"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename(columns={'default.payment.next.month': 'def_pay', \n                            'PAY_0': 'PAY_1'})\nfill = (data.EDUCATION == 5) | (data.EDUCATION == 6) | (data.EDUCATION == 0)\ndata.loc[fill, 'EDUCATION'] = 4\ndata.loc[data.MARRIAGE == 0, 'MARRIAGE'] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.EDUCATION.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.MARRIAGE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"32\">Check Data Unbalanced</a>\nIn this section, statistical analysis and interpretation of attributes will be performed.\n\n 1. Firstly, I want to learn about the demographic structure and measure its imbalance."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.EDUCATION.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.SEX.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.MARRIAGE.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.AGE.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to this information, most person who in this dataset, graduate of university, single or women. In next sections, I will examine different situations by each other of these attributes. I will be searching effect of these attributes against default payment. Instead of using the age feature in this way, I could use it more intermittently.\n"},{"metadata":{},"cell_type":"markdown","source":" 2. Now, I research on payment and bill. Actually, If we think as the period, in April 2005 between September 2005, we have sequentially periodically previous payment, bill amount and payment. In contrast, I don't know from previous period to April 2005."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_3.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_4.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_5.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_6.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.BILL_AMT1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.BILL_AMT2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.BILL_AMT3.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.BILL_AMT4.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.BILL_AMT5.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.BILL_AMT6.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_AMT1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_AMT2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_AMT3.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_AMT4.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_AMT5.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PAY_AMT6.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.LIMIT_BAL.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This statistical information, show us these attributes don't unbalanced. "},{"metadata":{},"cell_type":"markdown","source":"## <a id='33'>Data Conversion</a>\nIn this section, I will look for relationships to reach our target question by creating new data attributes. \nI create a function called <i>formgroup</i> to group columns easily. In addition, I will make searches with combinations of other features that affect the feature I am targeting and in addition, I will apply intermittent partitioning to make better use of the age column."},{"metadata":{"trusted":true},"cell_type":"code","source":"def formgroup(Col1, Col2):\n    res = data.groupby([Col1, Col2]).size().unstack()\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['SE_MA'] = data.SEX * data.MARRIAGE\nformgroup('SE_MA', 'def_pay')\ndata['SE_MA_2'] = 0\ndata.loc[((data.SEX == 1) & (data.MARRIAGE == 1)) , 'SE_MA_2'] = 1 #married man\ndata.loc[((data.SEX == 1) & (data.MARRIAGE == 2)) , 'SE_MA_2'] = 2 #single man\ndata.loc[((data.SEX == 1) & (data.MARRIAGE == 3)) , 'SE_MA_2'] = 3 #divorced man\ndata.loc[((data.SEX == 2) & (data.MARRIAGE == 1)) , 'SE_MA_2'] = 4 #married woman\ndata.loc[((data.SEX == 2) & (data.MARRIAGE == 2)) , 'SE_MA_2'] = 5 #single woman\ndata.loc[((data.SEX == 2) & (data.MARRIAGE == 3)) , 'SE_MA_2'] = 6 #divorced woman\nformgroup('SE_MA_2', 'def_pay')\ndel data['SE_MA']\ndata = data.rename(columns={'SE_MA_2': 'SE_MA'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to classify the ages by dividing them into 5 separate parts. The distribution of the histogram is as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['AgeBin'] = 0 #creates a column of 0\ndata.loc[((data['AGE'] > 20) & (data['AGE'] < 30)) , 'AgeBin'] = 1\ndata.loc[((data['AGE'] >= 30) & (data['AGE'] < 40)) , 'AgeBin'] = 2\ndata.loc[((data['AGE'] >= 40) & (data['AGE'] < 50)) , 'AgeBin'] = 3\ndata.loc[((data['AGE'] >= 50) & (data['AGE'] < 60)) , 'AgeBin'] = 4\ndata.loc[((data['AGE'] >= 60) & (data['AGE'] < 70)) , 'AgeBin'] = 5\ndata.loc[((data['AGE'] >= 70) & (data['AGE'] < 81)) , 'AgeBin'] = 6\nplt.figure()\nplt.title('20den baslayarak 10 ar 10 ar yaş aralığı ve sayıları')\ndata.AgeBin.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Payment distributions according to age ranges are as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"agedefpay=formgroup('AgeBin', 'def_pay')\nprint(agedefpay)\nagesex=formgroup('AgeBin', 'SEX')\nprint(agesex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['SE_AG'] = 0\ndata.loc[((data.SEX == 1) & (data.AgeBin == 1)) , 'SE_AG'] = 1 #erkek 20'li\ndata.loc[((data.SEX == 1) & (data.AgeBin == 2)) , 'SE_AG'] = 2 #erkek 30'lu\ndata.loc[((data.SEX == 1) & (data.AgeBin == 3)) , 'SE_AG'] = 3 #erkek 40'lı\ndata.loc[((data.SEX == 1) & (data.AgeBin == 4)) , 'SE_AG'] = 4 #erkek 50'li\ndata.loc[((data.SEX == 1) & (data.AgeBin == 5)) , 'SE_AG'] = 5 #erkek 60+\ndata.loc[((data.SEX == 2) & (data.AgeBin == 1)) , 'SE_AG'] = 6 #kadın 20'li\ndata.loc[((data.SEX == 2) & (data.AgeBin == 2)) , 'SE_AG'] = 7 #kadın 30'lu\ndata.loc[((data.SEX == 2) & (data.AgeBin == 3)) , 'SE_AG'] = 8 #kadın 40'lı\ndata.loc[((data.SEX == 2) & (data.AgeBin == 4)) , 'SE_AG'] = 9 #kadın 50'li\ndata.loc[((data.SEX == 2) & (data.AgeBin == 5)) , 'SE_AG'] = 10 #kadın 60+\nformgroup('SE_AG', 'def_pay')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aktif Kullanım"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['active_6'] = 1\ndata['active_5'] = 1\ndata['active_4'] = 1\ndata['active_3'] = 1\ndata['active_2'] = 1\ndata['active_1'] = 1\ndata.loc[((data.PAY_6 == 0) & (data.BILL_AMT6 == 0) & (data.PAY_AMT6 == 0)) , 'active_6'] = 0\ndata.loc[((data.PAY_5 == 0) & (data.BILL_AMT5 == 0) & (data.PAY_AMT5 == 0)) , 'active_5'] = 0\ndata.loc[((data.PAY_4 == 0) & (data.BILL_AMT4 == 0) & (data.PAY_AMT4 == 0)) , 'active_4'] = 0\ndata.loc[((data.PAY_3 == 0) & (data.BILL_AMT3 == 0) & (data.PAY_AMT3 == 0)) , 'active_3'] = 0\ndata.loc[((data.PAY_2 == 0) & (data.BILL_AMT2 == 0) & (data.PAY_AMT2 == 0)) , 'active_2'] = 0\ndata.loc[((data.PAY_1 == 0) & (data.BILL_AMT1 == 0) & (data.PAY_AMT1 == 0)) , 'active_1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series([data[data.active_6 == 1].def_pay.count(),\n          data[data.active_5 == 1].def_pay.count(),\n          data[data.active_4 == 1].def_pay.count(),\n          data[data.active_3 == 1].def_pay.count(),\n          data[data.active_2 == 1].def_pay.count(),\n          data[data.active_1 == 1].def_pay.count()], [6,5,4,3,2,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['average_5'] = ((data['BILL_AMT5'] - (data['BILL_AMT6'] - data['PAY_AMT5']))) / data['LIMIT_BAL']\ndata['average_4'] = (((data['BILL_AMT5'] - (data['BILL_AMT6'] - data['PAY_AMT5'])) +\n                 (data['BILL_AMT4'] - (data['BILL_AMT5'] - data['PAY_AMT4']))) / 2) / data['LIMIT_BAL']\ndata['average_3'] = (((data['BILL_AMT5'] - (data['BILL_AMT6'] - data['PAY_AMT5'])) +\n                 (data['BILL_AMT4'] - (data['BILL_AMT5'] - data['PAY_AMT4'])) +\n                 (data['BILL_AMT3'] - (data['BILL_AMT4'] - data['PAY_AMT3']))) / 3) / data['LIMIT_BAL']\ndata['average_2'] = (((data['BILL_AMT5'] - (data['BILL_AMT6'] - data['PAY_AMT5'])) +\n                 (data['BILL_AMT4'] - (data['BILL_AMT5'] - data['PAY_AMT4'])) +\n                 (data['BILL_AMT3'] - (data['BILL_AMT4'] - data['PAY_AMT3'])) +\n                 (data['BILL_AMT2'] - (data['BILL_AMT3'] - data['PAY_AMT2']))) / 4) / data['LIMIT_BAL']\ndata['average_1'] = (((data['BILL_AMT5'] - (data['BILL_AMT6'] - data['PAY_AMT5'])) +\n                 (data['BILL_AMT4'] - (data['BILL_AMT5'] - data['PAY_AMT4'])) +\n                 (data['BILL_AMT3'] - (data['BILL_AMT4'] - data['PAY_AMT3'])) +\n                 (data['BILL_AMT2'] - (data['BILL_AMT3'] - data['PAY_AMT2'])) +\n                 (data['BILL_AMT1'] - (data['BILL_AMT2'] - data['PAY_AMT1']))) / 5) / data['LIMIT_BAL']\naverage=data[['LIMIT_BAL', 'average_5', 'BILL_AMT5', 'average_4', 'BILL_AMT4','average_3', 'BILL_AMT3',\n    'average_2', 'BILL_AMT2', 'average_1', 'BILL_AMT1', 'def_pay']].sample(20)\nprint(average)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Periodical active use limit approximately according to the monthly period."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['InvoiceLimit_6'] = (data.LIMIT_BAL - data.BILL_AMT6) / data.LIMIT_BAL\ndata['InvoiceLimit_5'] = (data.LIMIT_BAL - data.BILL_AMT5) / data.LIMIT_BAL\ndata['InvoiceLimit_4'] = (data.LIMIT_BAL - data.BILL_AMT4) / data.LIMIT_BAL\ndata['InvoiceLimit_3'] = (data.LIMIT_BAL - data.BILL_AMT3) / data.LIMIT_BAL\ndata['InvoiceLimit_2'] = (data.LIMIT_BAL - data.BILL_AMT2) / data.LIMIT_BAL\ndata['InvoiceLimit_1'] = (data.LIMIT_BAL - data.BILL_AMT1) / data.LIMIT_BAL\nInvoiceLimit=data[['InvoiceLimit_6', 'InvoiceLimit_5', 'InvoiceLimit_4', 'InvoiceLimit_3', 'InvoiceLimit_2',\n   'InvoiceLimit_1', 'def_pay']].sample(20)\nprint(InvoiceLimit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, I would like to separate the numerical data that shows the time of payment of payments made in the data set. This is because we can look for a new payment plan and the impact of these attributes on our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['PAY_1_-1'] = (data.PAY_1 == -1)\ndata['PAY_1_-2'] = (data.PAY_1 == -2)\ndata['PAY_1_0'] = (data.PAY_1 == 0)\ndata['PAY_1_1'] = (data.PAY_1 == 1)\ndata['PAY_1_2'] = (data.PAY_1 == 2)\ndata['PAY_1_3'] = (data.PAY_1 == 3)\ndata['PAY_1_4'] = (data.PAY_1 == 4)\ndata['PAY_1_5'] = (data.PAY_1 == 5)\ndata['PAY_1_6'] = (data.PAY_1 == 6)\ndata['PAY_1_7'] = (data.PAY_1 == 7)\ndata['PAY_1_8'] = (data.PAY_1 == 8)\n\ndata['PAY_2_-1'] = (data.PAY_1 == -1)\ndata['PAY_2_-2'] = (data.PAY_1 == -2)\ndata['PAY_2_0'] = (data.PAY_1 == 0)\ndata['PAY_2_1'] = (data.PAY_1 == 1)\ndata['PAY_2_2'] = (data.PAY_1 == 2)\ndata['PAY_2_3'] = (data.PAY_1 == 3)\ndata['PAY_2_4'] = (data.PAY_1 == 4)\ndata['PAY_2_5'] = (data.PAY_1 == 5)\ndata['PAY_2_6'] = (data.PAY_1 == 6)\ndata['PAY_2_7'] = (data.PAY_1 == 7)\ndata['PAY_2_8'] = (data.PAY_1 == 8)\n\ndata['PAY_3_-1'] = (data.PAY_1 == -1)\ndata['PAY_3_-2'] = (data.PAY_1 == -2)\ndata['PAY_3_0'] = (data.PAY_1 == 0)\ndata['PAY_3_1'] = (data.PAY_1 == 1)\ndata['PAY_3_2'] = (data.PAY_1 == 2)\ndata['PAY_3_3'] = (data.PAY_1 == 3)\ndata['PAY_3_4'] = (data.PAY_1 == 4)\ndata['PAY_3_5'] = (data.PAY_1 == 5)\ndata['PAY_3_6'] = (data.PAY_1 == 6)\ndata['PAY_3_7'] = (data.PAY_1 == 7)\ndata['PAY_3_8'] = (data.PAY_1 == 8)\n\ndata['PAY_4_-1'] = (data.PAY_1 == -1)\ndata['PAY_4_-2'] = (data.PAY_1 == -2)\ndata['PAY_4_0'] = (data.PAY_1 == 0)\ndata['PAY_4_1'] = (data.PAY_1 == 1)\ndata['PAY_4_2'] = (data.PAY_1 == 2)\ndata['PAY_4_3'] = (data.PAY_1 == 3)\ndata['PAY_4_4'] = (data.PAY_1 == 4)\ndata['PAY_4_5'] = (data.PAY_1 == 5)\ndata['PAY_4_6'] = (data.PAY_1 == 6)\ndata['PAY_4_7'] = (data.PAY_1 == 7)\ndata['PAY_4_8'] = (data.PAY_1 == 8)\n\ndata['PAY_5_-1'] = (data.PAY_1 == -1)\ndata['PAY_5_-2'] = (data.PAY_1 == -2)\ndata['PAY_5_0'] = (data.PAY_1 == 0)\ndata['PAY_5_1'] = (data.PAY_1 == 1)\ndata['PAY_5_2'] = (data.PAY_1 == 2)\ndata['PAY_5_3'] = (data.PAY_1 == 3)\ndata['PAY_5_4'] = (data.PAY_1 == 4)\ndata['PAY_5_5'] = (data.PAY_1 == 5)\ndata['PAY_5_6'] = (data.PAY_1 == 6)\ndata['PAY_5_7'] = (data.PAY_1 == 7)\ndata['PAY_5_8'] = (data.PAY_1 == 8)\n\ndata['PAY_6_-1'] = (data.PAY_1 == -1)\ndata['PAY_6_-2'] = (data.PAY_1 == -2)\ndata['PAY_6_0'] = (data.PAY_1 == 0)\ndata['PAY_6_1'] = (data.PAY_1 == 1)\ndata['PAY_6_2'] = (data.PAY_1 == 2)\ndata['PAY_6_3'] = (data.PAY_1 == 3)\ndata['PAY_6_4'] = (data.PAY_1 == 4)\ndata['PAY_6_5'] = (data.PAY_1 == 5)\ndata['PAY_6_6'] = (data.PAY_1 == 6)\ndata['PAY_6_7'] = (data.PAY_1 == 7)\ndata['PAY_6_8'] = (data.PAY_1 == 8)\ndata['PAY_6_8'] = (data.PAY_1 == 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='4'>Data Exploration and Data Visualization</a>"},{"metadata":{},"cell_type":"markdown","source":"We will continue here by looking for attributes that are effective for our target and trying to see a specific attribute. By doing this I will explain the data set and make it explainable with visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.SEX,data.def_pay,normalize=False).plot(kind=\"bar\",rot=0,figsize=(20,6))\nplt.title('Default Payment by Sex')\nplt.xlabel('Sex (1 = Male, 2 = Female)' )\nplt.legend([\"No Payment\", \"Paying\"])\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see our target characteristics are unstable here. I need to make sure that any model I install doesn't memorize the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.MARRIAGE,data.def_pay,normalize=False).plot(kind=\"bar\",rot=0,figsize=(20,6))\nplt.title('Default Payment by Marriage')\nplt.xlabel('Marriage(1=married, 2=single ,3=others)' )\nplt.legend([\"No Payment\", \"Paying\"])\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.EDUCATION,data.def_pay,normalize=False).plot(kind=\"bar\",rot=0,figsize=(20,6))\nplt.title('Default Payment by Education')\nplt.xlabel('Education(1=Graduate School, 2=University ,3=High School ,4=Others)' )\nplt.legend([\"No Payment\", \"Paying\"])\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.AgeBin,data.def_pay,normalize=False).plot(kind=\"bar\",rot=0,figsize=(20,6))\nplt.title('Default Payment by AgeBin')\nplt.xlabel('Age Bin\\n (for 1) Age = [20,30) \\n (for 2) Age = [30,40) \\n' +\n           ' (for 3) Age = [40,50) \\n (for 4) Age = [50,60) \\n (for 5) Age = [60,70) \\n(for 6) Age = [70,81)')\nplt.legend([\"No Payment\", \"Paying\"])\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.AgeBin,data.def_pay,normalize=False).plot(kind=\"bar\",rot=0,figsize=(20,6))\nplt.title('Default Payment by AgeBin')\nplt.xlabel('Age Bin\\n (for 1) Age = [20,30) \\n (for 2) Age = [30,40) \\n' +\n           ' (for 3) Age = [40,50) \\n (for 4) Age = [50,60) \\n (for 5) Age = [60,70) \\n(for 6) Age = [70,81)')\nplt.legend([\"No Payment\", \"Paying\"])\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.SE_MA,data.def_pay,normalize=False).plot(kind=\"bar\",rot=0,figsize=(20,6))\nplt.title('Default Payment by SEX and MARRIAGE')\nplt.xlabel('SEX & MARRIAGE\\n 1 = Married Man, 2 = Single Man, 3 = Divorced Man, 4 = Married Woman, 5 = Single Woman, 6 = Divorced Woman')\nplt.legend([\"No Payment\", \"Paying\"])\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.SE_AG,data.def_pay,normalize=False).plot(kind=\"bar\",rot=0,figsize=(20,6))\nplt.title('Default Payment by SEX and AGE')\nplt.xlabel('SEX & AGE\\n 1 = 20s Man, 2 = 30s Man, ..., 5 = 60s Man, 6 = 20s Woman, 7 = 30s Woman, ..., 10 = 60s Woman')\nplt.legend([\"No Payment\", \"Paying\"])\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"5\">Predictive Models</a>\nFirstly, I must be choice the features of dataset for create predictive models. In previous stages, I was create many columns for to gain better results. For example, AgeBin, SE_AG etc. These are could be given idea to me. That's reason why I use a lot of columns because everything normal and nobody do not default payment. \nWhile I create model, I will select random state equal to 42. Then, I divide dataset as training and testing with percent of 20. After the model, I will use cross validation for prevent over-learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['LIMIT_BAL', 'EDUCATION','BILL_AMT1', 'BILL_AMT2',\n            'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n            'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', \n            'SE_MA', 'AgeBin', 'SE_AG', 'average_5', 'average_4',\n            'average_3', 'average_2', 'average_1', 'InvoiceLimit_5', 'InvoiceLimit_6',\n            'InvoiceLimit_4', 'InvoiceLimit_3', 'InvoiceLimit_2','InvoiceLimit_1',\n            'active_6','active_5','active_4','active_3','active_2','active_1','PAY_1_-1',\n            'PAY_1_-2', 'PAY_1_0', 'PAY_1_1', 'PAY_1_2', 'PAY_1_3', 'PAY_1_4', \n            'PAY_1_5', 'PAY_1_6', 'PAY_1_7', 'PAY_1_8', 'PAY_2_-1', 'PAY_2_-2', \n            'PAY_2_0', 'PAY_2_1', 'PAY_2_2', 'PAY_2_3', 'PAY_2_4', 'PAY_2_5', \n            'PAY_2_6', 'PAY_2_7', 'PAY_2_8', 'PAY_3_-1', 'PAY_3_-2', 'PAY_3_0', \n            'PAY_3_1', 'PAY_3_2', 'PAY_3_3', 'PAY_3_4', 'PAY_3_5', 'PAY_3_6', \n            'PAY_3_7', 'PAY_3_8', 'PAY_4_-1', 'PAY_4_-2', 'PAY_4_0', 'PAY_4_1', \n            'PAY_4_2', 'PAY_4_3', 'PAY_4_4', 'PAY_4_5', 'PAY_4_6', 'PAY_4_7', \n            'PAY_4_8', 'PAY_5_-1', 'PAY_5_-2', 'PAY_5_0', 'PAY_5_2', 'PAY_5_3', \n            'PAY_5_4', 'PAY_5_5', 'PAY_5_6', 'PAY_5_7', 'PAY_5_8', 'PAY_6_-1', \n            'PAY_6_-2', 'PAY_6_0', 'PAY_6_2', 'PAY_6_3', 'PAY_6_4', 'PAY_6_5', \n            'PAY_6_6', 'PAY_6_7', 'PAY_6_8']\ntarget = 'def_pay'\ny = data['def_pay'].copy()\nX = data[features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\ndata_train = X_train.join(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"51\">Random Forest Classifier</a>\nWhile I create model of random forest classifier, I choose optimal parameters. I used grid search cv algorithms for optimal parameters but I don't show in here."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfclassifier = RandomForestClassifier(random_state=42,n_estimators=200,criterion='entropy',\n                                       max_features='sqrt',max_depth=7,verbose=False)\nrfclassifier.fit(X_train[features], y_train)\nrfprediction = rfclassifier.predict(X_test[features])\nprint('Accuracy of Random Forest Classifier: ',accuracy_score(rfprediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmrf = pd.crosstab(y_test.values, rfprediction, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cmrf, fmt=\"d\",\n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Green\", cmap=\"Greens\")\nplt.title('Confusion Matrix for Random Forest', fontsize=14)\nplt.show()\n\nrocaucscorerf=roc_auc_score(y_test.values, rfprediction)\nprint('Roc Score: ',rocaucscorerf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"52\">Decision Tree Classifier</a>\nIf you want to choose optimal parameters for any algorithm, either you will using Grid Search Cv algorithm or you will trying different parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtclassifier = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n                       max_features=None, max_leaf_nodes=20,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=5,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=42, splitter='best')\ndtclassifier.fit(X_train, y_train)\ndtprediction = dtclassifier.predict(X_test)\nprint('Accuracy of Decision Tree:', accuracy_score(dtprediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmdt = pd.crosstab(y_test.values, dtprediction, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax2) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cmdt, fmt = \"d\",\n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax2,\n            linewidths=.2,linecolor=\"Red\", cmap=\"Reds\")\nplt.title('Confusion Matrix for Decision Tree', fontsize=14)\nplt.show()\n\nrocaucscoredt=roc_auc_score(y_test.values, dtprediction)\nprint('Roc Score: ',rocaucscoredt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"53\">K-Nearest Neighbors</a>\nI use minkowski distance as metric for optimal knn algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"knnclassifier=KNeighborsClassifier(n_neighbors=8,algorithm='auto',\n                                    leaf_size=30,metric='minkowski')\nknnclassifier.fit(X_train, y_train)\ntrainaccuracy=knnclassifier.score(X_train, y_train)\ntestaccuracy=knnclassifier.score(X_test, y_test)\npredictionknn=knnclassifier.predict(X_test)\nprint('train accuracy: {}\\ntest accuracy: {}\\n'.format(trainaccuracy,testaccuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmknn = pd.crosstab(y_test.values, predictionknn, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax3) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cmknn, fmt=\"d\",\n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax3,\n            linewidths=.2,linecolor=\"Blue\", cmap=\"Blues\")\nplt.title('Confusion Matrix for KNN', fontsize=14)\nplt.show()\n\n\nrocaucscoreknn=roc_auc_score(y_test.values, predictionknn )\nprint('Roc Score: ',rocaucscoreknn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"54\">AdaBoost</a>\nLearning rate is very important topic because if learning rate is chosen small value, model might not learning to success. On the contrary, if learning rate is chosen big value, model could assume it was the most successful at learning. In summary, if learning rate is chosen optimally, I can obtain most successful model."},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboostclassifier = AdaBoostClassifier(base_estimator=None, \n                                         n_estimators=50, \n                                         learning_rate=1.5, \n                                         algorithm='SAMME', \n                                         random_state=42)\n\nadaboostclassifier.fit(X_train[features], y_train.values)\nadaboostprediction = adaboostclassifier.predict(X_test[features])\nprint('Accuracy of Ada Boost:', accuracy_score(adaboostprediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmadaboost = pd.crosstab(y_test.values, adaboostprediction, \n                     rownames=['Actual'], colnames=['Predicted'])\nfig, (ax4) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cmadaboost, fmt=\"d\",\n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax4,\n            linewidths=.2,linecolor=\"Purple\", cmap=\"Purples\")\nplt.title('Confusion Matrix for Adaboost', fontsize=14)\nplt.show()\n\nrocaucscoreadaboost=roc_auc_score(y_test.values, adaboostprediction)\nprint('Roc Score: ',rocaucscoreadaboost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"55\">ROC-AUC CURVE</a>\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba_DT = dtclassifier.predict_proba(X_test)[::,1]\nfpr1, tpr1, _ = roc_curve(y_test, y_pred_proba_DT)\nauc1 = roc_auc_score(y_test, y_pred_proba_DT)\n\ny_pred_proba_RF = rfclassifier.predict_proba(X_test)[::,1]\nfpr2, tpr2, _ = roc_curve(y_test,  y_pred_proba_RF)\nauc2 = roc_auc_score(y_test, y_pred_proba_RF)\n\ny_pred_proba_KNN = knnclassifier.predict_proba(X_test)[::,1]\nfpr3, tpr3, _ = roc_curve(y_test,  y_pred_proba_KNN)\nauc3 = roc_auc_score(y_test, y_pred_proba_KNN)\n\ny_pred_proba_ADABOOST = adaboostclassifier.predict_proba(X_test)[::,1]\nfpr4, tpr4, _ = roc_curve(y_test,  y_pred_proba_ADABOOST)\nauc4 = roc_auc_score(y_test, y_pred_proba_ADABOOST)\n\nplt.figure(figsize=(10,7))\nplt.title('ROC', size=15)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr1,tpr1,label=\"Decision Tree, auc=\"+str(round(auc1,2)))\nplt.plot(fpr2,tpr2,label=\"Random Forest, auc=\"+str(round(auc2,2)))\nplt.plot(fpr3,tpr3,label=\"KNearest Neighbor, auc=\"+str(round(auc3,2)))\nplt.plot(fpr4,tpr4,label=\"AdaBoost, auc=\"+str(round(auc4,2)))\nplt.legend(loc='best', title='Models', facecolor='white')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.box(False)\nplt.grid()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"56\">K-Fold Cross Validation for Algorithms</a>\nI want to compare and select models for my problem because this method has a lower bias than other methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_list = [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n                                   max_features=None, max_leaf_nodes=20,\n                                   min_impurity_decrease=0.0, min_impurity_split=None,\n                                   min_samples_leaf=1, min_samples_split=5,\n                                   min_weight_fraction_leaf=0.0, presort=False,\n                                   random_state=42, splitter='best'), \n            RandomForestClassifier(random_state=42,n_estimators=200,criterion='entropy',\n                                    max_features='sqrt',max_depth=7,verbose=False),\n            KNeighborsClassifier(n_neighbors=8,algorithm='auto',\n                                    leaf_size=30,metric='minkowski'), \n            AdaBoostClassifier(base_estimator=None, \n                                    n_estimators=50, \n                                    learning_rate=1.5, \n                                    algorithm='SAMME', \n                                    random_state=42)\n           ]\n# use Kfold to evaluate the normal training set\nkf = KFold(n_splits=5,random_state=42,shuffle=True)\n\nmdl = []\nfold = []\nfscr = []\nrocscr = []\naccscr = []\n\n\nfor i,(train_index, test_index) in enumerate(kf.split(data_train)):\n    training = data.iloc[train_index,:]\n    valid = data.iloc[test_index,:]\n    print(i)\n    for clf in clf_list:\n        model = clf.__class__.__name__\n        feats = training[features] #defined above\n        label = training['def_pay']\n        valid_feats = valid[features]\n        valid_label = valid['def_pay']\n        clf.fit(feats,label) \n        pred = clf.predict(valid_feats)\n        fscore = f1_score(y_true = valid_label, y_pred = pred)\n        rocscore = roc_auc_score(valid_label, pred)\n        accscore = accuracy_score(y_true = valid_label, y_pred = pred)\n        fold.append(i+1)\n        fscr.append(fscore)\n        rocscr.append(rocscore)\n        accscr.append(accscore)\n        mdl.append(model)\n        print(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"57\">Comparison of Algorithms</a>\nI obtain most successful model with Random Forest Classifier or Adaboost Classifier for this problem. According to Roc-Auc Curve, I want to develop Random Forest Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"performance = pd.DataFrame({'Model': mdl, 'Score':fscr,\n                            'Roc_Auc_Score':rocscr,'Accuracy_Score':accscr,'Fold':fold})\n\ndtcc = performance[performance['Model'] == 'DecisionTreeClassifier']\nrfcc = performance[performance['Model'] == 'RandomForestClassifier']\nabcc = performance[performance['Model'] == 'AdaBoostClassifier']\nknnn = performance[performance['Model'] == 'KNeighborsClassifier']\n\nplt.figure(figsize=(15,10))\nplt.plot(dtcc.Fold,dtcc.Score,'red',label=\"DT F1\",marker='o')\nplt.plot(dtcc.Fold,dtcc.Roc_Auc_Score,'firebrick',label=\"DT Roc\",marker='x')\nplt.plot(dtcc.Fold,dtcc.Accuracy_Score,'rosybrown',label=\"DT Acc\",marker='.')\n\nplt.plot(rfcc.Fold,rfcc.Score,'olive',label=\"RF F1\",marker='o')\nplt.plot(rfcc.Fold,rfcc.Roc_Auc_Score,'yellowgreen',label=\"RF Roc\",marker='x')\nplt.plot(rfcc.Fold,rfcc.Accuracy_Score,'lightgreen',label=\"RF Acc\",marker='.')\n\nplt.plot(knnn.Fold,knnn.Score,'purple',label=\"KNN F1\",marker='o')\nplt.plot(knnn.Fold,knnn.Roc_Auc_Score,'violet',label=\"KNN Roc\",marker='x')\nplt.plot(knnn.Fold,knnn.Accuracy_Score,'fuchsia',label=\"KNN Acc\",marker='.')\n\nplt.plot(abcc.Fold,abcc.Score,'lightskyblue',label=\"AdaB F1\",marker='o')\nplt.plot(abcc.Fold,abcc.Roc_Auc_Score,'blue',label=\"AdaB Roc\",marker='x')\nplt.plot(abcc.Fold,abcc.Accuracy_Score,'navy',label=\"AdaB Acc\",marker='.')\n\nplt.title(\"Classifiers\")\nplt.grid()\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.plot(dtcc.Fold,dtcc.Score,'r',label=\"DecisionTreeClassifier\",marker='o')\nplt.plot(rfcc.Fold,rfcc.Score,'b',label=\"RandomForestClassifier\",marker='o')\nplt.plot(knnn.Fold,knnn.Score,'c',label=\"KNeighborsClassifier\",marker='o')\nplt.plot(abcc.Fold,abcc.Score,'g',label=\"AdaBoostClassifier\",marker='o')\nplt.title(\"Classifiers F1 Score\")\nplt.grid()\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.plot(dtcc.Fold,dtcc.Roc_Auc_Score,'r',label=\"DecisionTreeClassifier\",marker='o')\nplt.plot(rfcc.Fold,rfcc.Roc_Auc_Score,'b',label=\"RandomForestClassifier\",marker='o')\nplt.plot(knnn.Fold,knnn.Roc_Auc_Score,'c',label=\"KNeighborsClassifier\",marker='o')\nplt.plot(abcc.Fold,abcc.Roc_Auc_Score,'g',label=\"AdaBoostClassifier\",marker='o')\nplt.title(\"Classifiers Roc Score\")\nplt.grid()\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.plot(dtcc.Fold,dtcc.Accuracy_Score,'r',label=\"DecisionTreeClassifier\",marker='o')\nplt.plot(rfcc.Fold,rfcc.Accuracy_Score,'b',label=\"RandomForestClassifier\",marker='o')\nplt.plot(knnn.Fold,knnn.Accuracy_Score,'c',label=\"KNeighborsClassifier\",marker='o')\nplt.plot(abcc.Fold,abcc.Accuracy_Score,'g',label=\"AdaBoostClassifier\",marker='o')\nplt.title(\"Classifiers Accuracy Score\")\nplt.grid()\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"58\">Change the Features Used for Algorithm</a>\nI want to show features important. That's reason why, I select more little features. While I select new features, I want to choose the more efficient features for default payment."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2',\n            'BILL_AMT3', 'BILL_AMT5', 'PAY_AMT1',\n            'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', \n            'average_4','average_3', 'average_2', 'average_1', 'InvoiceLimit_5',\n            'InvoiceLimit_4', 'InvoiceLimit_3', 'InvoiceLimit_2','InvoiceLimit_1', \n            'PAY_1_1', 'PAY_1_2',\n            'PAY_2_0', 'PAY_2_2', 'PAY_2_3', \n            'PAY_3_0', \n            'PAY_3_1', 'PAY_3_2', 'PAY_3_3', 'PAY_4_0', 'PAY_4_1', \n            'PAY_4_2', 'PAY_4_3', 'PAY_5_0', 'PAY_5_2', 'PAY_5_3',\n            'PAY_6_2']\n\nrfclassifier_new = RandomForestClassifier(random_state=42,n_estimators=200,criterion='entropy',\n                                       max_features='sqrt',max_depth=7,verbose=False)\nrfclassifier_new.fit(X_train[new_features], y_train)\nrfprediction_new = rfclassifier_new.predict(X_test[new_features])\nprint('Accuracy of New Random Forest: ',accuracy_score(rfprediction_new,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmrf_new = pd.crosstab(y_test.values, rfprediction_new, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax5) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cmrf_new, fmt=\"d\",\n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax5,\n            linewidths=.2,linecolor=\"Green\", cmap=\"Greens\")\nplt.title('Confusion Matrix in New Random Forest', fontsize=14)\nplt.show()\nrocaucscorerf=roc_auc_score(y_test.values, rfprediction)\nprint('Roc Score: ',rocaucscorerf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.DataFrame({'Features': new_features, 'Importance of Features': rfclassifier_new.feature_importances_})\ntmp = tmp.sort_values(by='Importance of Features',ascending=False)\nplt.figure(figsize = (25,15))\nplt.title('Importance of Features',fontsize=14)\ns = sns.barplot(x='Features',y='Importance of Features',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"6\">Conclusion</a>\n<p>This success may be successful for the entire data set, but I have not found a clear classification as the common characteristic of credit card users who make the default payment. I just find that payment delays play an important role for those performing with 2 to default payment. This shows that between the dates given to me there may be an economic crisis or any social problem may have occurred.As a solution, I think that configuring a payment plan would be more accurate for credit card users to make their default payments. I also believe I could have found a more accurate result if there had been a classification of loans taken in this data set.</p>\n<p> ***\"14 March – Mainland China passed the Anti-Secession Law, a bill to prevent Taiwan from being an independent nation.\"***</p>\n<p>In view of this situation, I believe there may have been an economic crisis in Taiwan because blocking independence could also be a major obstacle to the country's economic independence. I believe that if there was a drastic difference in the country's import and export figures after the creation of this law, the default payments could be realized through configuration. As a result, instead of looking for answers to the personality characteristic problem of the people who make the default payments with this data set, we can ask the questions of how to take precautions or how to configure payment during a period of economic crisis. A new payment plan, which will be delayed by 2 months as shown in this data, will be an important decision for the realization of the default payment. </p>"},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"7\">Referances</a>\n* https://en.wikipedia.org/wiki/2005_in_Taiwan\n* https://pandas.pydata.org/\n* https://numpy.org/\n* https://matplotlib.org/\n* https://en.wikipedia.org/wiki/Random_forest\n* https://en.wikipedia.org/wiki/Decision_tree\n* https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n* https://en.wikipedia.org/wiki/AdaBoost\n* https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}