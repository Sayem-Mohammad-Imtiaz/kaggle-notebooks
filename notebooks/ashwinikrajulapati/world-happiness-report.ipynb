{"cells":[{"cell_type":"code","source":"## 1.0 Call libraries\n\nimport time                   # To time processes\nimport warnings               # To suppress warnings\nimport plotly.graph_objs as go\nimport os                     # For os related operations\nimport sys                    # For data size\nimport numpy as np            # Data manipulation\nimport pandas as pd           # Dataframe manipulatio \nimport matplotlib.pyplot as plt                   # For graphics\n\n\nfrom sklearn import cluster, mixture              # For clustering\nfrom sklearn.preprocessing import StandardScaler  # For scaling dataset                  \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n%matplotlib inline\n","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1cea78fd-5003-44f9-a4cf-879ca7f37e24","_uuid":"20f49289230ca8b895d8bf4106072a633cb808c5"}},{"cell_type":"code","source":"# 2. Read data\nX= pd.read_csv(\"../input/2017.csv\", header=0)\nCountry_df=pd.read_csv(\"../input/2017.csv\", header=0)\n#X= pd.read_csv(\"iris.csv\", header = 0)\nCountry_df=Country_df['Country' ] \n","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c8535e0a-504b-472e-a115-8e0af068a5a3","collapsed":true,"_uuid":"4ba4e6bf751feb8dc7aaca4c8269a97fa0d0002a"}},{"cell_type":"code","source":"# 3. Explore and scale\nX.columns.values\nX.shape                 # 155 X 12\nX = X.iloc[:, 2: ]      # Ignore Country and Happiness_Rank columns\nX.head(2)\nX.dtypes\nX.info\n","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"af891af3-9749-495a-bfb8-81a6e3efd52e","_uuid":"d4e61ac2c950a5f9ce976f09e6ef420c47ba43e6"}},{"cell_type":"code","source":"\n# 3.1 Normalize dataset for easier parameter selection\n# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n#    Standardize features by removing the mean\n#      and scaling to unit variance\n# 3.1.2 Instantiate scaler object\nss = StandardScaler()\n# 3.1.3 Use ot now to 'fit' &  'transform'\nss.fit_transform(X)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"22a9dddd-ee2e-4ff4-87b0-6f346b51803f","_uuid":"c50a5a53e23b4fabcadd8ce33c52509e4172fd5e"}},{"cell_type":"code","source":"\n#### 5. Begin Clustering   \n                                  \n# 5.1 How many clusters\n#     NOT all algorithms require this parameter\nn_clusters = 2    ","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ad5d325c-f958-43d6-ba38-5a44bc81b6a0","collapsed":true,"_uuid":"4065d32f9770efd52f855fbe8b8248b49c1c9f82"}},{"cell_type":"code","source":"                                  \n## 5 KMeans\n# Ref: http://scikit-learn.org/stable/modules/clustering.html#k-means                                  \n# KMeans algorithm clusters data by trying to separate samples in n groups\n#  of equal variance, minimizing a criterion known as the within-cluster\n#   sum-of-squares.                         \n\n# 5.1 Instantiate object\nkm = cluster.KMeans(n_clusters =n_clusters )\n\n# 5.2.1 Fit the object to perform clustering\nkm_result = km.fit_predict(X)\n\n","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a1462d91-f9bc-4a02-82ee-1427012a6db4","collapsed":true,"_uuid":"be53ba478b289fb2b95e1453018a4642b673fffc"}},{"cell_type":"code","source":"\n## 6. Mean Shift\n# http://scikit-learn.org/stable/modules/clustering.html#mean-shift\n# This clustering aims to discover blobs in a smooth density of samples.\n#   It is a centroid based algorithm, which works by updating candidates\n#    for centroids to be the mean of the points within a given region.\n#     These candidates are then filtered in a post-processing stage to\n#      eliminate near-duplicates to form the final set of centroids.\n# Parameter: bandwidth dictates size of the region to search through. \n\n# 6.1\nbandwidth = 0.1  \n\n# 6.2 No of clusters are NOT predecided\nms = cluster.MeanShift(bandwidth=bandwidth)\n\n# 6.3\nms_result = ms.fit_predict(X)\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"\n## 7. Mini Batch K-Means\n#  Ref: \n#     http://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means\n#  Similar to kmeans but clustering is done in batches to reduce computation time\n\n# 7.1 \ntwo_means = cluster.MiniBatchKMeans(n_clusters=n_clusters)\n\n# 7.2\ntwo_means_result = two_means.fit_predict(X)\n\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"## 8. Spectral clustering\n# http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering    \n# SpectralClustering does a low-dimension embedding of the affinity matrix\n#  between samples, followed by a KMeans in the low dimensional space. It\n#   is especially efficient if the affinity matrix is sparse.\n#   SpectralClustering requires the number of clusters to be specified.\n#     It works well for a small number of clusters but is not advised when \n#      using many clusters.\n\n# 8.1\nspectral = cluster.SpectralClustering(n_clusters=n_clusters)\n\n# 8.2\nsp_result= spectral.fit_predict(X)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"## 9. DBSCAN\n# http://scikit-learn.org/stable/modules/clustering.html#dbscan\n#   The DBSCAN algorithm views clusters as areas of high density separated\n#    by areas of low density. Due to this rather generic view, clusters found\n#     by DBSCAN can be any shape, as opposed to k-means which assumes that\n#      clusters are convex shaped.    \n#    Parameter eps decides the incremental search area within which density\n#     should be same\n\neps = 0.3\n\n# 9.1 No of clusters are NOT predecided\ndbscan = cluster.DBSCAN(eps=eps)\n\n# 9.2\ndb_result= dbscan.fit_predict(X)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"# 10. Affinity Propagation\n# Ref: http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation    \n# Creates clusters by sending messages between pairs of samples until convergence.\n#  A dataset is then described using a small number of exemplars, which are\n#   identified as those most representative of other samples. The messages sent\n#    between pairs represent the suitability for one sample to be the exemplar\n#     of the other, which is updated in response to the values from other pairs. \n#       Two important parameters are the preference, which controls how many\n#       exemplars are used, and the damping factor which damps the responsibility\n#        and availability messages to avoid numerical oscillations when updating\n#         these messages.\n\ndamping = 0.9\npreference = -200\n\n# 10.1  No of clusters are NOT predecided\naffinity_propagation = cluster.AffinityPropagation(\n        damping=damping, preference=preference)\n\n# 10.2\naffinity_propagation.fit(X)\n\n# 10.3\nap_result = affinity_propagation .predict(X)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"## 11. Birch\n# http://scikit-learn.org/stable/modules/clustering.html#birch    \n# The Birch builds a tree called the Characteristic Feature Tree (CFT) for the\n#   given data and clustering is performed as per the nodes of the tree\n\n# 11.1\nbirch = cluster.Birch(n_clusters=n_clusters)\n\n# 11.2\nbirch_result = birch.fit_predict(X)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"# 14. Gaussian Mixture modeling\n#  http://203.122.28.230/moodle/course/view.php?id=6&sectionid=11#section-3\n#  It treats each dense region as if produced by a gaussian process and then\n#  goes about to find the parameters of the process\n\n# 14.1\ngmm = mixture.GaussianMixture( n_components=n_clusters, covariance_type='full')\n\n# 14.2\ngmm.fit(X)\n\n# 14.3\ngmm_result = gmm.predict(X)\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"#Plot Graphs\n# 5.3 Draw scatter plot of two features, coloyued by clusters\nplt.subplot(4, 2, 1)\nplt.title(\"KMeans\")\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=km_result)\n\n# 6.4\nplt.subplot(4, 2, 2)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=ms_result)\nplt.title(\"MeanShift\")\n# 7.3\nplt.subplot(4, 2, 3)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c= two_means_result)\nplt.title(\"MiniBatch\")\n# 8.3\nplt.subplot(4, 2, 4)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=sp_result)\nplt.title(\"Spectral\")\n# 9.3\nplt.subplot(4, 2, 5)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5], c=db_result)\nplt.title(\"DBScan\")\n# 10.4\nplt.subplot(4, 2, 6)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=ap_result)\nplt.title(\"Affinity\")\n# 11.3\nplt.subplot(4, 2, 7)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=birch_result)\nplt.title(\"Birch\")\nplt.subplot(4, 2, 8)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=gmm_result)\nplt.title(\"Gaussian\")\nplt.subplots_adjust(bottom=-0.5, top=2.0)\n","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"  Result_DataFrame = pd.DataFrame(columns=[\"Country\",\"KMeans\",\"MeanShift\",\"Minibatch\",\"Spectral\",\"DBSCAN\",\"Affinity\",\"Birch\",\"Gaussian\"])\n  Result_DataFrame.loc[:,\"Country\"]=Country_df\n  Result_DataFrame.loc[:,\"KMeans\"]=km_result\n  Result_DataFrame.loc[:,\"MeanShift\"]=ms_result\n  Result_DataFrame.loc[:,\"Minibatch\"]=two_means_result\n  Result_DataFrame.loc[:,\"Spectral\"]=sp_result\n  Result_DataFrame.loc[:,\"DBSCAN\"]=db_result\n  Result_DataFrame.loc[:,\"Affinity\"]=ap_result\n  Result_DataFrame.loc[:,\"Birch\"]=birch_result\n  Result_DataFrame.loc[:,\"Gaussian\"]=gmm_result\n  \n \n  Result_DataFrame","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"\ndata = dict(type = 'choropleth', \n           locations = Country_df,\n           locationmode = 'country names',\n           z = km_result, \n           text = Country_df,\n           colorbar = {'title':'Cluster Group'})\nlayout = dict(title = 'K-Means Clustering', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap1 = go.Figure(data = [data], layout=layout)\niplot(choromap1)","outputs":[],"execution_count":null,"metadata":{}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}