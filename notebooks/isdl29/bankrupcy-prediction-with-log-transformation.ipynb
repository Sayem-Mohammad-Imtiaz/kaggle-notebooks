{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 0. Notebook Setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import matthews_corrcoef\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndatafile=\"/kaggle/input/company-bankruptcy-prediction/data.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data inspection","metadata":{}},{"cell_type":"code","source":"rawdata=pd.read_csv(datafile)\nrawdata","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rawdata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data cleaning","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Undersampling","metadata":{}},{"cell_type":"markdown","source":"Despite being a huge dataset, it's very imbalanced.","metadata":{}},{"cell_type":"code","source":"counts=rawdata.groupby(\"Bankrupt?\")[\"Bankrupt?\"].count()\ncounts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will make the positive bankrupt cases more difficult to identify. To deal with it, undersampling is applied to the dataset: All the positives are preserved and most of the negatives are dropped so we can set the fraction of positives of the new balanced dataset. In this case, we set the positive fraction to 40%. ","metadata":{}},{"cell_type":"code","source":"def undersample(data, class_column, minor_class_frac):\n    counter=data.groupby(class_column)[class_column].count()\n    minor_class=counter[counter==counter.min()].index[0]\n    major_class=counter[counter==counter.max()].index[0]\n    \n    minor_class_data=data[data[class_column]==minor_class]\n    major_class_data=data[data[class_column]==major_class]\n    \n    ratio=counter[minor_class]/counter[major_class]\n    \n    major_class_undersampling=(1./minor_class_frac - 1.)*ratio\n    \n    major_class_data=major_class_data.sample(frac=major_class_undersampling)\n                          \n    newdata=pd.concat([minor_class_data,major_class_data])\n    \n    return newdata\n\nbalancedata=undersample(rawdata,\"Bankrupt?\", minor_class_frac=0.4)\ndiscardata=pd.concat([rawdata,balancedata]).drop_duplicates(keep=False)\n\nbalancedata.groupby(\"Bankrupt?\")[\"Bankrupt?\"].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balancedata","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Logarithmic transformation to variables with extreme magnitude order differences","metadata":{}},{"cell_type":"markdown","source":"Some variables have values with very different magnitude order. Most of them have some values in the range of 10^(-4) and others  around 10^9. Both ranges have details in their distribution we don't want to miss. Now the boxplots of such variables will be shown.","metadata":{}},{"cell_type":"code","source":"for column in balancedata.columns:\n    if column!=\"Bankrupt?\":\n        m=balancedata[column].max()\n        if m>10:\n            fig,ax_=plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n            ax_[0]=sns.boxplot(data=balancedata[balancedata[column]<=10], \n                               y=column, \n                               x=\"Bankrupt?\",ax=ax_[0])\n            ax_[0].set_title(\"Lower order values\")\n            ax_[1]=sns.boxplot(data=balancedata[balancedata[column]>10], \n                               y=column, \n                               x=\"Bankrupt?\",ax=ax_[1])\n            ax_[1].set_title(\"Upper order values\")\n            plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to \"squeeze\" the values while keeping the details of the set of values of both ranges. In order to do so. We make the following transformation:\n\n\\begin{equation}\nx'=\\ln{\\left(x^{\\frac{1}{k}}+1 \\right)}\n\\end{equation}\n\nwhere\n\n\\begin{equation}\nk=\\frac{\\ln{(x_{\\text{max}})}}{\\ln{(e-1)}}\n\\end{equation}\n\nso when x=0, x'=0 and when x=x_max, x'=1. Also the greater the value, the greater the squeezing effect.\n\nNow let's show an example. We have picked a variable with this issue. First we apply the logarithmic transformation and then the standard scaling to represent what the machine learning algorithms will get an the input.","metadata":{}},{"cell_type":"code","source":"x=balancedata[\" Operating Expense Rate\"]\nf=np.log(x**(np.log(np.e-1)/np.log(x.max()))+1)\nd=pd.DataFrame()\nd[\"Original\"]=x\nd[\"LogTrans\"]=f\nstdscaler=StandardScaler()\nstdscaler.fit(f.values.reshape(-1, 1))\nd[\"LogTrans + StdScaling\"]=stdscaler.transform(f.values.reshape(-1, 1))\nd.tail(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax_=plt.subplots(nrows=2,ncols=2,figsize=(10,10))\n\nax_[0,0]=sns.boxplot(data=balancedata[balancedata[\" Operating Expense Rate\"]<=10], \n                     y=\" Operating Expense Rate\", \n                     x=\"Bankrupt?\",\n                     ax=ax_[0,0])\nax_[0,0].set_title(\"Lower order values\")\nax_[0,1]=sns.boxplot(data=balancedata[balancedata[\" Operating Expense Rate\"]>10],\n                     y=\" Operating Expense Rate\", \n                     x=\"Bankrupt?\",\n                     ax=ax_[0,1])\nax_[0,1].set_title(\"Upper order values\")\n\nm=balancedata[\" Operating Expense Rate\"].max()\n\nax_[1,0]=sns.boxplot(data=balancedata[balancedata[\" Operating Expense Rate\"]<=10], \n                     y=balancedata[\" Operating Expense Rate\"].apply(lambda x: np.log(x**(np.log(np.e-1)/np.log(m))+1)), \n                     x=\"Bankrupt?\",\n                     ax=ax_[1,0])\nax_[1,0].set_title(\"Lower order values (LogTransformed)\")\nax_[1,1]=sns.boxplot(data=balancedata[balancedata[\" Operating Expense Rate\"]>10],\n                     y=balancedata[\" Operating Expense Rate\"].apply(lambda x: np.log(x**(np.log(np.e-1)/np.log(m))+1)), \n                     x=\"Bankrupt?\",\n                     ax=ax_[1,1])\nax_[1,1].set_title(\"Upper order values (LogTransformed)\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It represents quite well what we wanted to achieve. The price to pay is distorting the general value distribution, so the algorithms that suppose linear features-output dependencies will be affected, but the one that depend mainly on the relative differences between positive and negative distribution will perform even better, since they will be more sensitive to the lower order values distribution details.\n\nNow the value of k for each affected variable will be shown, along with the maximum value, the mean and the median of each one.","metadata":{}},{"cell_type":"code","source":"dinfo={}\nfor col in balancedata.columns:\n    info={}\n    info[\"median\"]=balancedata[col].median()\n    info[\"mean\"]=balancedata[col].mean()\n    info[\"max\"]=balancedata[col].max()\n    info[\"k\"]=np.log(balancedata[col].max())/np.log(np.e-1)\n    dinfo[col]=info\ndatainfo=pd.DataFrame.from_dict(dinfo, orient=\"index\")\ndatainfo[datainfo[\"max\"]>1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data analysis","metadata":{}},{"cell_type":"markdown","source":"Now the distribution of the regular variables and the tranformed ones will be shown.","metadata":{}},{"cell_type":"code","source":"for column in balancedata.columns:\n    if column!=\"Bankrupt?\":\n        if balancedata[column].max()>10:\n            m=balancedata[column].max()\n            sns.displot(data=balancedata, \n                        x=balancedata[column].apply(lambda x: np.log(x**(np.log(np.e-1)/np.log(m))+1)), \n                        hue=\"Bankrupt?\", element=\"step\", stat=\"probability\",common_norm=False)\n            plt.title(\"LogScaled\")\n        else:\n            sns.displot(data=balancedata, \n                        x=balancedata[column], \n                        hue=\"Bankrupt?\", element=\"step\", stat=\"probability\",common_norm=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to the nature of the variables, removing the outliers will reduce dramatically the data size. Let's remove the outliers using the quantile criteria:\n\n\\begin{equation}\nQ_1- q · \\ Q_{13}<{(X,y)} \\leq Q_3+q · \\ Q_{13}\n\\end{equation}\n\nwhere\n\n\\begin{equation}\nQ_{13}=Q_3-Q_1\n\\end{equation}\n\nfor q=3 (removing just the extreme outliers).","metadata":{}},{"cell_type":"code","source":"def outlier_remover(columnseries,q):\n    Q1=columnseries.describe()[\"25%\"]\n    Q3=columnseries.describe()[\"75%\"]\n    Q13=Q3-Q1\n    lowerbound=Q1-q*Q13\n    upperbound=Q3+q*Q13\n    newcolumnseries=columnseries[columnseries.between(lowerbound,upperbound)]\n    return newcolumnseries\n\ndef outlier_clean(dataframe, exception_col=[],quo=1.5):\n    if quo==\"inf\": \n        newdataframe=dataframe\n    else:\n        newdataframe=pd.DataFrame()\n        for columnname in dataframe.columns:\n            if columnname not in exception_col:\n                newdataframe[columnname]=outlier_remover(dataframe[columnname],q=quo)\n            else:\n                newdataframe[columnname]=dataframe[columnname]\n        newdataframe=newdataframe.dropna()\n    return newdataframe\n    \ncleandata=outlier_clean(balancedata, exception_col=[\"Bankrupt?\"] ,quo=3)\ncleandata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data size has reduced from 550 samples to just 90. We have to use the full balanced dataset to machine learn it.","metadata":{}},{"cell_type":"markdown","source":"## 4. Data learning","metadata":{}},{"cell_type":"markdown","source":"### Methodology\n\nUsing first the balanced data with the logarithmic transformation:\n1) Split the data into k+1 sets. Then use the first k sets to apply a k-fold cross validation evaluation with F1-Score as the evaluation score with several algorithms with different hyperparameters. In this case, we will use k=5.\n\n2) Get also the Accuracy, the Matthews Correlation Coefficient and the F2-Score of the best algorithms.\n\n3) Use the remaining set to test the best algorithms found with the KFCV and also get the scores listed in 2).\n\n4) Finally use the discarded negatives to do another test to the algorithms and get the accuracy, which is the only useful score since all the real outputs are negative so there will be just true negatives and false positives.\n\nThen we repeat the same steps for the data set without the logarithmic transformation and finally we compare the results.","metadata":{}},{"cell_type":"code","source":"learndata=balancedata\nfinalresults=[]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data with logarithmic transformation","metadata":{}},{"cell_type":"code","source":"data_col=list(learndata.columns)\ny_col=data_col.pop(data_col.index(\"Bankrupt?\"))\nX_col=data_col\n\nX_data=pd.DataFrame()\nX_discardata=pd.DataFrame()\nfor col in X_col:\n    maxv=learndata[col].max()    \n    if maxv>10:\n        X_data[col]=learndata[col].apply(lambda x: np.log(x**(np.log(np.e-1)/np.log(maxv))+1))\n        X_discardata[col]=discardata[col].apply(lambda x: np.log(x**(np.log(np.e-1)/np.log(maxv))+1))\n    else:\n        X_data[col]=learndata[col]\n        X_discardata[col]=discardata[col]\ny_data=learndata[y_col]\ny_discardata=discardata[y_col]\n\n# K value of K-Fold CV assignation.\n\nk=5\nr=29\n\n# Split into CV Training data and Testing data. We choose the test size to be equal to the validation size of our K-Fold CV.\n# For doing so, we split the data into K+1 pieces and we use the first K pieces to K-Fold CV and the left one for final testing.\n\nX_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=1/(k+1), random_state=r)\nX_discarded, y_discarded = X_discardata.values, y_discardata.values\n\n# Standard scaling of the input data X.\n\nXscaler=StandardScaler()\nXscaler.fit(X_train)\nX_train=Xscaler.transform(X_train)\nX_test=Xscaler.transform(X_test)\nX_discarded=Xscaler.transform(X_discarded)\n\n# Creation of the variables we will use to store the evaluation results for each algorythm.\n\nmodelScore_train={}\nmodelScore_test={}\nmodelScore_discarded={}\nmodelScore_KFCV={}\nmodel_KFCV={}\n\nmodelConfussionMatrix_train={}\nmodelConfussionMatrix_test={}\nmodelConfussionMatrix_discarded={}\n\nbankruptevents_train=pd.DataFrame()\nbankruptevents_test=pd.DataFrame()\nbankruptevents_discarded=pd.DataFrame()\nbankruptevents_train[\"Bankrupt?\"]=y_train\nbankruptevents_test[\"Bankrupt?\"]=y_test\nbankruptevents_discarded[\"Bankrupt?\"]=y_discarded\n\n# Scorers setup and F1 assignation as the evaluation score.\nscorersCV={\"Acc\": make_scorer(accuracy_score),\n           \"MCC\": make_scorer(matthews_corrcoef), \n           \"F1\": make_scorer(fbeta_score, beta=1), \n           \"F2\": make_scorer(fbeta_score, beta=2)}\nscorerCVkey=\"F1\"\n\n# Definition of the algorythm evaluation function.\n\ndef tuned_model(model, param_grid, modelname, results):\n    tuner=GridSearchCV(model,param_grid=param_grid,scoring=scorersCV,cv=k,refit=scorerCVkey)\n    tuner.fit(X_train,y_train)\n    best_model=tuner.best_estimator_\n    best_params=tuner.best_params_\n    model_KFCV[modelname]=best_model\n    print(f\"Best model: {best_model}\")\n    \n    modelScore_KFCV[modelname]={}\n    resultcols=['rank_test_'+scorerCVkey]\n    resultsdata=pd.DataFrame(tuner.cv_results_)\n    for paramkey in param_grid:\n        paramcolname=\"param_\"+paramkey\n        resultcols.append(paramkey)\n        resultsdata=resultsdata.rename(columns={paramcolname:paramkey})\n        \n    for i in range(0,k):\n        old_split_score_name=\"split%s_test_%s\" % (i,scorerCVkey)\n        new_split_score_name=\"s%s_%s\" % (i,scorerCVkey)\n        resultsdata=resultsdata.rename(columns={old_split_score_name: new_split_score_name})\n        resultcols.append(new_split_score_name)\n    \n    for scorerkey in scorersCV:\n        old_mean_score_name=\"mean_test_\"+scorerkey\n        new_mean_score_name=\"mean_\"+scorerkey\n        resultsdata=resultsdata.rename(columns={old_mean_score_name: new_mean_score_name})\n        resultcols.append(new_mean_score_name)\n        \n        best_row=resultsdata[resultsdata[\"params\"]==best_params]\n        modelScore_KFCV[modelname][scorerkey]=round(best_row[new_mean_score_name].values[0],3)\n        \n    if results==True: print(resultsdata[resultcols].sort_values(by=resultcols[0]).round(3))\n        \n    return best_model\n\ndef plot_confusion_matrix(confusion_matrix,model_name):\n    ax=plt.axes()\n    confusionmatrix=sns.heatmap(data=confusion_matrix,annot=True,ax=ax)\n    ax.set_title(model_name)\n    plt.show()\n\ndef execute_model(model,modelname):\n    model.fit(X_train,y_train)\n    y_train_pred=model.predict(X_train)\n    y_test_pred=model.predict(X_test)\n    y_discarded_pred=model.predict(X_discarded)\n    bankruptevents_train[modelname]=y_train_pred\n    bankruptevents_test[modelname]=y_test_pred\n    bankruptevents_discarded[modelname]=y_discarded_pred    \n    modelScore_train[modelname]={}\n    modelScore_test[modelname]={}\n    modelScore_discarded[modelname]={}    \n    for scorerkey in scorersCV:\n        scorermaker=scorersCV[scorerkey]\n        scorer=scorermaker._score_func\n        scorerparams=scorermaker._kwargs\n        modelScore_train[modelname][scorerkey]=round(scorer(y_train,y_train_pred,**scorermaker._kwargs) , 3)\n        modelScore_test[modelname][scorerkey]=round(scorer(y_test,y_test_pred,**scorermaker._kwargs) , 3)\n        modelScore_discarded[modelname][scorerkey]=round(scorer(y_discarded,y_discarded_pred,**scorermaker._kwargs) , 3)        \n    modelConfussionMatrix_train[modelname]=confusion_matrix(y_train,y_train_pred,labels=[0, 1],normalize=\"true\")\n    modelConfussionMatrix_test[modelname]=confusion_matrix(y_test,y_test_pred,labels=[0, 1],normalize=\"true\")\n    modelConfussionMatrix_discarded[modelname]=confusion_matrix(y_discarded,y_discarded_pred,labels=[0, 1],normalize=\"true\")\n    \ndef eval_model(model_func,model_pgrid, model_name, results_info=False):\n    model=tuned_model(model=model_func, param_grid=model_pgrid, modelname=model_name, results=results_info)\n    execute_model(model,model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{}},{"cell_type":"code","source":"LogitRegr_params={\"C\":[1,0.5,0.1,0.01],\n                  \"solver\":[\"lbfgs\", \"liblinear\"]}\neval_model(LogisticRegression(),LogitRegr_params,\"Logistic Regr.\",True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stochastic Gradient Descend Classifier","metadata":{}},{"cell_type":"code","source":"SGDClf_params={\"alpha\":[0.1,0.05,0.01,0.005],\n               \"loss\":[\"hinge\",\"log\",\"modified_huber\"]}\neval_model(SGDClassifier(),SGDClf_params,\"Stochastic Gradient Descend Clf.\",True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"DTClf_params={\"ccp_alpha\":[0,0.05,0.001],\n              \"criterion\":[\"gini\", \"entropy\"],\n              \"max_features\":[None,\"auto\", \"sqrt\", \"log2\"]}\neval_model(DecisionTreeClassifier(), DTClf_params, \"Decision Tree Clf.\", True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Multi Layer Perceptron Classifier","metadata":{}},{"cell_type":"code","source":"MLPClf_params={\"alpha\":[0.05,0.01,0.001,0.0001],\n               \"activation\":[\"tanh\",\"relu\"],\n               \"hidden_layer_sizes\":[20,50],\n               \"learning_rate\":[\"constant\",\"adaptive\"]}\neval_model(MLPClassifier(), MLPClf_params, \"Multi-Layer Perceptron Clf.\", True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"RFClf_params={\"ccp_alpha\":[0.01,0.03,0.05,0.005],\n              \"criterion\":[\"gini\", \"entropy\"],\n              \"n_estimators\":[1,2,5,10]}\neval_model(RandomForestClassifier(), RFClf_params, \"Random Forest Clf.\", True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SCORES","metadata":{}},{"cell_type":"code","source":"print(\"Score (%d-fold CV Train data)\" % k)\nfor key in modelScore_KFCV:\n    print(\"%s: %s\" % (key, modelScore_KFCV[key]))\nprint(\"\\n\")    \nprint(\"Score (Test data)\")\nfor key in modelScore_test:\n    print(\"{}: {}\".format(key,modelScore_test[key]))\nprint(\"\\n\")    \nprint(\"Accuracy (Discarded data)\")\nfor key in modelScore_discarded:\n    print(\"{}: {}\".format(key,modelScore_discarded[key][\"Acc\"]))\n    \nfor key in modelScore_KFCV:\n    row={}\n    row[\"Algorithm\"]=key\n    row[\"LogTrans\"]=1\n    row[\"F1-Score\"]=modelScore_KFCV[key][\"F1\"]\n    row[\"Accuracy\"]=modelScore_KFCV[key][\"Acc\"]\n    row[\"MCC\"]=modelScore_KFCV[key][\"MCC\"]\n    row[\"F2-Score\"]=modelScore_KFCV[key][\"F2\"]    \n    row[\"Discarded Accuracy\"]=modelScore_discarded[key][\"Acc\"]\n    exists=[]\n    for row_ in finalresults:\n        existsbool=False\n        if (row_[\"Algorithm\"]==row[\"Algorithm\"]) and (row_[\"LogTrans\"]==row[\"LogTrans\"]): existsbool=True\n        exists.append(existsbool)\n    if True in exists:\n        finalresults[exists.index(True)]=row\n    else:\n        finalresults.append(row)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TEST SET PREDICTIONS OVERVIEW (SOME PREDICTIONS + NORMALIZED CONFUSSION MATRICES)","metadata":{}},{"cell_type":"code","source":"bankruptevents_test.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for modelname in bankruptevents_test.columns:\n    if modelname!=\"Bankrupt?\":\n       c_matrix=modelConfussionMatrix_test[modelname]\n       c_matrix_plot=plot_confusion_matrix(c_matrix,modelname)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data without logarithmic transformation","metadata":{}},{"cell_type":"code","source":"data_col=list(learndata.columns)\ny_col=data_col.pop(data_col.index(\"Bankrupt?\"))\nX_col=data_col\n\nX_data=pd.DataFrame()\nX_discardata=pd.DataFrame()\nfor col in X_col:\n    X_data[col]=learndata[col]\n    X_discardata[col]=discardata[col]\ny_data=learndata[y_col]\ny_discardata=discardata[y_col]\n\n# K value of K-Fold CV assignation.\n\nk=5\nr=29\n\n# Split into CV Training data and Testing data. We choose the test size to be equal to the validation size of our K-Fold CV.\n# For doing so, we split the data into K+1 pieces and we use the first K pieces to K-Fold CV and the left one for final testing.\n\nX_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=1/(k+1), random_state=r)\nX_discarded, y_discarded = X_discardata.values, y_discardata.values\n\n# Standard scaling of the input data X.\n\nXscaler=StandardScaler()\nXscaler.fit(X_train)\nX_train=Xscaler.transform(X_train)\nX_test=Xscaler.transform(X_test)\nX_discarded=Xscaler.transform(X_discarded)\n\n# Creation of the variables we will use to store the evaluation results for each algorythm.\n\nmodelScore_train={}\nmodelScore_test={}\nmodelScore_discarded={}\nmodelScore_KFCV={}\nmodel_KFCV={}\n\nmodelConfussionMatrix_train={}\nmodelConfussionMatrix_test={}\nmodelConfussionMatrix_discarded={}\n\nbankruptevents_train=pd.DataFrame()\nbankruptevents_test=pd.DataFrame()\nbankruptevents_discarded=pd.DataFrame()\nbankruptevents_train[\"Bankrupt?\"]=y_train\nbankruptevents_test[\"Bankrupt?\"]=y_test\nbankruptevents_discarded[\"Bankrupt?\"]=y_discarded\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{}},{"cell_type":"code","source":"eval_model(LogisticRegression(),LogitRegr_params,\"Logistic Regr.\",True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stochastic Gradient Descend Classifier","metadata":{}},{"cell_type":"code","source":"eval_model(SGDClassifier(),SGDClf_params,\"Stochastic Gradient Descend Clf.\",True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"eval_model(DecisionTreeClassifier(), DTClf_params, \"Decision Tree Clf.\", True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Multi Layer Perceptron Classifier","metadata":{}},{"cell_type":"code","source":"eval_model(MLPClassifier(), MLPClf_params, \"Multi-Layer Perceptron Clf.\", True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"eval_model(RandomForestClassifier(), RFClf_params, \"Random Forest Clf.\", True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SCORES","metadata":{}},{"cell_type":"code","source":"print(\"Score (%d-fold CV Train data)\" % k)\nfor key in modelScore_KFCV:\n    print(\"%s: %s\" % (key, modelScore_KFCV[key]))\nprint(\"\\n\")    \nprint(\"Score (Test data)\")\nfor key in modelScore_test:\n    print(\"{}: {}\".format(key,modelScore_test[key]))\nprint(\"\\n\")    \nprint(\"Accuracy (Discarded data)\")\nfor key in modelScore_discarded:\n    print(\"{}: {}\".format(key,modelScore_discarded[key][\"Acc\"]))   \n    \nfor key in modelScore_KFCV:\n    row={}\n    row[\"Algorithm\"]=key\n    row[\"LogTrans\"]=0\n    row[\"F1-Score\"]=modelScore_KFCV[key][\"F1\"]\n    row[\"Accuracy\"]=modelScore_KFCV[key][\"Acc\"]\n    row[\"MCC\"]=modelScore_KFCV[key][\"MCC\"]\n    row[\"F2-Score\"]=modelScore_KFCV[key][\"F2\"]    \n    row[\"Discarded Accuracy\"]=modelScore_discarded[key][\"Acc\"]\n    exists=[]\n    for row_ in finalresults:\n        existsbool=False\n        if (row_[\"Algorithm\"]==row[\"Algorithm\"]) and (row_[\"LogTrans\"]==row[\"LogTrans\"]): existsbool=True\n        exists.append(existsbool)\n    if True in exists:\n        finalresults[exists.index(True)]=row\n    else:\n        finalresults.append(row)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TEST SET PREDICTIONS OVERVIEW (SOME PREDICTIONS + NORMALIZED CONFUSSION MATRICES)","metadata":{}},{"cell_type":"code","source":"bankruptevents_test.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for modelname in bankruptevents_test.columns:\n    if modelname!=\"Bankrupt?\":\n       c_matrix=modelConfussionMatrix_test[modelname]\n       c_matrix_plot=plot_confusion_matrix(c_matrix,modelname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FINAL RESULTS","metadata":{}},{"cell_type":"markdown","source":"Now the scores of the algorithms applied to both logtransformed and non transformed datasets will be shown, ordered from thw best to the worst F1-Score.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(finalresults).sort_values(by=\"F1-Score\",ascending=False)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The logarithmic transformation has improved the performance of the algorithms slightly.\n\nIn general the best algorithms with the dataset with the logarithmic transformation show a F1-Score well above 0.8, which it's not ideal, but it's not bad at all.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}