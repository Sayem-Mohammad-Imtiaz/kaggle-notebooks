{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. Objective.\n\n**Identify the files that are related to the vaccination process.**\n\nThe process will search within the metadata file and then specifically search for the filtered files.\n\nIn future updates we will use nlp tools to find which documents have positive results.\n\nThank what was explained in:\n\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n\nhttps://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv"},{"metadata":{},"cell_type":"markdown","source":"2. Load Metadata"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we check the first lines of the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we check the final lines of the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To have a good initial filter, we will reform the title field, so that we can later filter on the subject of vaccines."},{"metadata":{"trusted":true},"cell_type":"code","source":"title = df.copy()\ntitle = title.dropna(subset=['title'])\ntitle['title'] = title['title'].str.replace('[^a-zA-Z]', ' ', regex=True)\ntitle['title'] = title['title'].str.lower()\ntitle.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we make the filter by the title field"},{"metadata":{"trusted":true},"cell_type":"code","source":"title['keyword_vaccine'] = title['title'].str.find('vaccine') \ntitle.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the result prompt -1, then the title doesn't contained the keyword."},{"metadata":{"trusted":true},"cell_type":"code","source":"included_vaccine = title.loc[title['keyword_vaccine'] != -1]\nincluded_vaccine","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the filter for the articles that contain the vaccine title, we will filter the files and load them for further analysis, for this we create an array with the names of the sha that is part of the name of the json file."},{"metadata":{"trusted":true},"cell_type":"code","source":"shaid = []\nfor index, row in included_vaccine.iterrows():\n    id = str(row['sha']) + \".json\"\n    shaid.append(id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we go through the json files and we compare them with the metadata array"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport os\ndatafiles = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if not (filename==''):\n            if(filename in shaid):\n                ifile = os.path.join(dirname, filename)\n                if ifile.split(\".\")[-1] == \"json\":\n                    datafiles.append(ifile)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we check how many files crossed"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(datafiles)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we go through the selected files and create an arrangement with the body of the article."},{"metadata":{"trusted":true},"cell_type":"code","source":"ArrBodyText = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    bodytext = ''\n    for item in doc['body_text']:\n        bodytext = bodytext + item['text']\n        \n    ArrBodyText.append({id:bodytext})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we check that the correct information will be loaded"},{"metadata":{"trusted":true},"cell_type":"code","source":"ArrBodyText[20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a filter, we will start to perform other text techniques\nto be continue..."},{"metadata":{},"cell_type":"markdown","source":"4. Text Analysis\n\nWe will use the NLP library called NLTK, first we will make a split on an item of the specific arrangement to be able to separate the words."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_split = str(ArrBodyText[20]).split()\nlen(text_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify common words\nfreq = pd.Series(' '.join(text_split).split()).value_counts()[:20]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify uncommon words\nfreq1 =  pd.Series(' '.join(text_split).split()).value_counts()[-20:]\nfreq1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text pre-processing can be divided into two broad categories â€” noise removal & normalization. Data components that are redundant to the core text analytics can be considered as noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nstem = PorterStemmer()\nword = \"inversely\"\nprint(\"stemming:\",stem.stem(word))\nprint(\"lemmatization:\", lem.lemmatize(word, \"v\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for text preprocessing\nimport re\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\n#nltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a list of stop words and adding custom stopwords and Creating a list of custom stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\"))\n\nnew_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\",\"et\",'al']\nstop_words = stop_words.union(new_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\nfor i in range(0, 4548):\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', text_split[i])\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#View corpus item\ncorpus[222]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Exploration\nWe will now visualize the text corpus that we created after pre-processing to get insights on the most frequently used words."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word cloud\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n#matplotlib inline\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=90, \n                          random_state=62\n                         ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}