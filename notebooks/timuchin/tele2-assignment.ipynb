{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"43313261e8149c603b6d91ad46993ee5ca055a90"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddbe30d4e352fda5471d73984aa42384fa903359"},"cell_type":"markdown","source":"# Data manipulation"},{"metadata":{"trusted":true,"_uuid":"9d590f5e00d61598c172e9ae694975383dd04d67","collapsed":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/mushrooms.csv\") # Load data\ndf # Take a look","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6cd8fa0ad0e2d9b9bcc4a98c04a378e4f3f7211","collapsed":true},"cell_type":"code","source":"# Check unique values \nfor col in df.columns:\n    print(df[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"277626d14a3cced3709990175894fcef737212fa"},"cell_type":"code","source":"df = df.drop('veil-type', axis = 1) # Drop veil-type as it is a constant ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06c972f85ba890e804e3689038ca1e8ba61a8eae","collapsed":true},"cell_type":"code","source":"df_encoded = pd.get_dummies(df) # One-hot encoding\ndf_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"091c4b02124e1053595074051e0ba477ad504bcd","collapsed":true},"cell_type":"code","source":"# Check encoded dataset\nfor col in df_encoded.columns:\n    print(df_encoded[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f46b425d364a08cdb8d788d7004abe5ca930dd7","collapsed":true},"cell_type":"code","source":"df_encoded.describe() # Everything seems fine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cc16ae81291038e20a967c5db22a9b3123668f99"},"cell_type":"code","source":"# Split the data into independent and explanatory variables \ny = df_encoded.iloc[:, 0]\nX = df_encoded.iloc[:, 2:118]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"738ec415a9dadc7d1b37577310d58a66f65e9959","collapsed":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec88c296feccec1c8de5940b73ef3174f20ad6fe","collapsed":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ecfb63db93acd5b7a1058f0ac088073b548091e"},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"_uuid":"4f14e4db8ed56e078ac8105983938e46c3b53188"},"cell_type":"markdown","source":"In logistic regression, the response variable describes the probability that the outcome is the positive case. If the response variable is equal to or exceeds a discrimination threshold, the positive class is predicted; otherwise, the negative class is predicted. The response variable is modeled as a function of a linear combination of the explanatory variables using the logistic function."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"21abf1319248887c5b6c7fab915f7d155c4c3817"},"cell_type":"code","source":"# Split datasets into training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"646bb153944e8b351c62c0c4c69fa9ce926cc93e"},"cell_type":"code","source":"from sklearn.linear_model.logistic import LogisticRegression\nlogitclassifier = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"31f0f9eae5224cde0b6411223aa9886a4e4e7399"},"cell_type":"code","source":"logitclassifier.fit(X_train, y_train)\npredictions = logitclassifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0e682f99a25466625f3ccf39adae268b5566117","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, predictions)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68c15b7948555aba231b95182e6619638eac2686","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.matshow(confusion_matrix)\nplt.colorbar()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"583e1e0d2f138d8f953030e55888c55d70c5e177","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n# Standard classification performance metrics\naccuracy = cross_val_score(logitclassifier, X_train, y_train) # Accuracy measures a fraction of the classifier's predictions that are correct\nprecisions = cross_val_score(logitclassifier, X_train, y_train, scoring = 'precision') # Precision is the fraction of positive predictions that are correct\nrecalls = cross_val_score(logitclassifier, X_train, y_train, scoring = 'recall') # Recall is the fraction of the truly positive instances that the classifier recognizes\nf1s = cross_val_score(logitclassifier, X_train, y_train, scoring = 'f1') # Harmonic mean of the precision and recall\nprint('Accuracy', np.mean(accuracy), accuracy)\nprint('Precision', np.mean(precisions), precisions)\nprint('Recalls', np.mean(recalls), recalls)\nprint('F1', np.mean(f1s), f1s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03451967ecc0002aad6a992e1174de44ca07b875","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\npredictions = logitclassifier.predict_proba(X_test)\nfalse_positive_rate, recall, thresholds = roc_curve(y_test, predictions[:, 1])\n# ROC curve is insensitive to data sets with unbalanced class proportions; unlike precision and recall, \n# the ROC curve illustrates the classifier's performance for all values of the discrimination threshold\nroc_auc = auc(false_positive_rate, recall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b68f5781ae7c004122f56056c57c74658447d63","collapsed":true},"cell_type":"code","source":"plt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc8b69cf24c5dadcb97bde91329a578fad3cf356","collapsed":true},"cell_type":"code","source":"# Tuning the model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nparameters = {\n    'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    'penalty' : ['l1', 'l2']\n}\nlogit = GridSearchCV(logitclassifier, parameters, n_jobs = -1, iid = 'True')\nlogit.fit(X_train, y_train)\nprint ('Best score: %0.3f' % logit.best_score_)\nfor param_name in sorted(parameters.keys()):\n    print ('\\t%s: %r' % (param_name, logit.best_params_[param_name]))\npredictions = logit.predict(X_test)\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75764f67d8e4375a25b97c5add9ee48c37ea8f98"},"cell_type":"markdown","source":"# Decision Trees"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b559e1cca9983ee4428820c5f3643cf2b4b7d61e"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ntreeclassifier = DecisionTreeClassifier(criterion = 'entropy')\nparameters = {\n    'max_depth' : [150, 155, 160],\n    'min_samples_leaf' : [1, 2, 3]\n}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"9eab6b190a50a3a499cdede3846dd71d096d970d","collapsed":true},"cell_type":"code","source":"# Tuning the tree\ntree = GridSearchCV(treeclassifier, parameters, n_jobs = -1, scoring = 'f1')\ntree.fit(X_train, y_train)\nprint ('Best score: %0.3f' % tree.best_score_)\nprint ('Best parameter set:')\nbest_parameters = tree.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\npredictions = tree.predict(X_test)\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"227bc0f6b45dd9354ad079a5824b68c184d60ccd"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"09defa5a27d2506e8dc4677bdd11805cebb44fc7"},"cell_type":"code","source":"# A random forest is a collection of decision trees that have been trained on randomly selected subsets\n# of the training instances and explanatory variables.\nforestclassifier = RandomForestClassifier(criterion = 'entropy')\nparameters = {\n    'n_estimators' : [5, 10, 20, 50],\n    'max_depth' : [50, 150, 250],\n    'min_samples_leaf' : [1, 2, 3]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d44088fb09ce142c5cb6dc68a95850f50cb900e2","collapsed":true},"cell_type":"code","source":"# Tuning random forest\nforest = GridSearchCV(forestclassifier, parameters, n_jobs = -1, scoring = 'f1')\nforest.fit(X_train, y_train)\nprint ('Best score: %0.3f' % forest.best_score_)\nprint ('Best parameter set:')\nbest_parameters = forest.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\npredictions = forest.predict(X_test)\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"207a56eff0257bc7e5b2c05f301a741e18338cc7"},"cell_type":"markdown","source":"# SVM"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"81cb695cf4a1155c14aa36f82d43ee5580d27bf9"},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e4f22b3e134b7aeaa277666cfb0c9ec1942ed9cc"},"cell_type":"code","source":"# A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane.\nsvmclassifier = SVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7054de13e69821f40d93d00686767f8215f8e8c5"},"cell_type":"code","source":"parameters = {\n    'C' : [0.1, 0.5, 1, 2, 5, 10, 50, 100],\n    'kernel' : ['rbf', 'linear', 'poly', 'sigmoid']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e845ad6738da85b9ff2d19f0bbde33b27e6c2ba","collapsed":true},"cell_type":"code","source":"svm = GridSearchCV(svmclassifier, parameters, n_jobs = -1, scoring = 'f1')\nsvm.fit(X_train, y_train)\nprint ('Best score: %0.3f' % svm.best_score_)\nprint ('Best parameter set:')\nbest_parameters = svm.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\npredictions = svm.predict(X_test)\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a8e63fc4961733cd908b310ecad30d1e52fab1b"},"cell_type":"markdown","source":"# Boosting"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"67a708032a1c429c0f398ff6960266ad743326fd"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nadaboost = AdaBoostClassifier(random_state = 1)\ngradientboost = GradientBoostingClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4b5b42c0ab81688553157d50586e8cdf174b4299"},"cell_type":"code","source":"parameters = {\n    'learning_rate': [0.1, 0.5, 1, 2, 5, 10],\n    'n_estimators': [5, 10, 20, 50, 100, 150],\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8f2e933a7e6a796eab57ad55b2487f614f121bd","collapsed":true},"cell_type":"code","source":"# Tuning AdaBoost\nAdaboost = GridSearchCV(adaboost, parameters, n_jobs = -1, iid = 'True')\nAdaboost.fit(X_train, y_train)\nprint ('Best score: %0.3f' % Adaboost.best_score_)\nprint ('Best parameter set:')\nbest_parameters = Adaboost.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\npredictions = Adaboost.predict(X_test)\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6c9ea05fa9976a33dd21ad83a74aa1bfe2e777c"},"cell_type":"code","source":"parameters = {\n    'learning_rate' : [0.1, 0.5, 1, 2, 5, 10],\n    'n_estimators' : [5, 10, 20, 50, 100, 150],\n    'max_depth' : [1, 2, 5, 10],\n    'min_samples_leaf' : [1, 2, 3]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0582b23c6dbfadec1d48bc24b404a91be50a8a7","collapsed":true},"cell_type":"code","source":"# Tuning GradientBoost\nGradientBoosting = GridSearchCV(gradientboost, parameters, n_jobs = -1, iid = 'True')\nGradientBoosting.fit(X_train, y_train)\nprint ('Best score: %0.3f' % GradientBoosting.best_score_)\nprint ('Best parameter set:')\nbest_parameters = GradientBoosting.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\npredictions = GradientBoosting.predict(X_test)\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58aca6088ef4a1a18f778fe2c92700edc3e069b4"},"cell_type":"markdown","source":"# Voting Classifier\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2270dc6951a48007f60b083f868005fedebac158"},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"749e4d4b4c3ef7faff126a86e0bb8aba0c0ecec9"},"cell_type":"code","source":"# Use previously defined best parameters for each model\neclf = VotingClassifier(estimators = [('lr', LogisticRegression(C = 10, penalty = 'l1')), \n                                      ('rf', RandomForestClassifier(max_depth = 150, min_samples_leaf = 1)), \n                                      ('SVM', SVC(C = 0.5, kernel = 'linear')), \n                                      ('AB', AdaBoostClassifier(learning_rate = 0.5, n_estimators = 50)), \n                                      ('GB', GradientBoostingClassifier(learning_rate = 0.1, max_depth = 5, min_samples_leaf = 1, n_estimators = 50))], \n                        voting = 'hard', \n                        n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5355fbba2ab287ea49a642b549dc7678c66e3f52","collapsed":true},"cell_type":"code","source":"eclf.fit(X_train, y_train)\npredictions = eclf.predict(X_test)\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}