{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is Pandas?\n\nPandas is the most powerful and flexible open source data analysis package available in python.Pandas is well suited for different kind of data such as Tabular,ordered and unordered data,arbitrary matrix and any other form of observational/statistical data."},{"metadata":{},"cell_type":"markdown","source":"# How to install Pandas?\n\nPandas is part of the Anaconda distribution and and can be installed with Anaconda and minitab.\n\nsyntax:** conda install pandas**\n\npandas also can be installed by using pip command.\n\nsyntax: **pip install pandas**"},{"metadata":{},"cell_type":"markdown","source":"# Datastructure of Pandas?\n\n2 primary data structure of Pandas are:\n\n1. Series(1-Dimensional) : 1D labeled homogeneously -typed array\n2. DataFrame(2-Dimensional) : size mutable tabular structure with potentialy heterogeneously -typed data\n\n3. Another Datastructure is ***Panel***. Panel is 3-Dimensional.\n\n**Series:** is a container for scalar\n\n**DataFrame: **is a container for series."},{"metadata":{},"cell_type":"markdown","source":"# Why Pandas package is widely used ?\n\n* Easy handling of missing data (represented as NAN) in floating point as well as non-floating point data\n\n* Size mutability: columns can be inserted and deleted from Data Frame and higher dimensional objects\n\n* Automatioc and explicit data Alignment\n\n* Powerful,flexible group by functionality to perform split-apply-combine operations on data sets ,for both aggregating and transforming data\n\n* Intelligent label-based slicing ,fancy indexing and subsetting of large data sets\n \n* intuitive merging and joining data sets\n\n* Flexible reshaping and pivoting of Data sets\n\n* Hierarchical labeling of axes\n\n* Robust IO tools for loading data from flat files (CSV and delimited) ,excel files ,databases and saving/loading data from from the ultra  fast HDF5 format\n\n* Time series specific functionality : date range generation and frequency conversion,date shifting and lagging\n "},{"metadata":{},"cell_type":"markdown","source":"# Series Creation\n\n* A series can be created by calling the pd.Series() function on a Numphy 1D array or python object (like tuple,list,dictionary) \n\n* Index can if constructed and if the index is not explicitly not mentioned , a numeric (from 0 to length-1) index is automatically geenrated.\n\n**Syntax: pd.Series(data= ,index= ,dtype= ,name= )**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic Series with an array ,no index\nimport pandas as pd\nimport numpy as np\nseries1=pd.Series(np.random.randn(5))\nseries1  #Here type is automatically included from data and a numeric index is created","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s2=pd.Series(np.arange(1,10,2),index=list('abcde'),dtype=float,name='ID')\ns2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Series from a numeric list\n\ns3=pd.Series([-55,4,6,7])\ns3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating from a tuple\n\nS4=pd.Series((4,5,6))\nS4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating Series from a dictionary \n\nS5=pd.Series({'a':10,'b':12,'c':14})\nS5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A series can be converted into list or dictionary using below methods.\n\n* tolist()\n\n* to_dict()"},{"metadata":{},"cell_type":"markdown","source":"# Series Attributes\n\n**Series attributes provide the metadata about the contents of the structure.**\n\n**Attributes are : values,index,shape**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the underlying Numpy array\ns2.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the index\ns2.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the size on the disk\n\ns2.nbytes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the number of elements\n\ns2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Subsetting a Series\n\nThere are many ways to slice a series. It is possible to subset a series using\n\n* label-based indexing by passing index labels associated with the values \n\n* fancy indexing using methods like loc,iloc,ix,at,iat\n\n* boolean indexing for subsetting with logical arrays"},{"metadata":{},"cell_type":"markdown","source":"# 1.label-based indexing by passing index labels associated with the values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label and integer based indexing \n\nlist1={'bread':1,'banana':1,'Milk':1,'eggs':2}\nMenu=pd.Series(list1)\nMenu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#A single label\n\nMenu['Milk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a slice of labels\n\nMenu['bread':'Milk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another label slice\n\nMenu['banana':]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#positional slicing\n\nMenu[0:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# More positional slicing\n\nMenu[0::2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Menu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reversing the Series\n\nMenu[::-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Subsetting with fancy indexing  Methods\n\n* loc(): for label based subsetting\n* iloc: for integer based subsetting\n\n.ix[] : Its a hybrid method which supports both integer based and label based value for selction and subsetting of the data\n\nat[] : Access a single value for a row/column label pair . it is similar to loc() ,but as returns only single value hence faster than loc()\n\niat[] :  Acces a single value for row/column pair by integer position. It is similar to iloc(). As it returns only single value its faster than iloc\n\nix,at,iat are deprecated methods. So it is advisable to use loc and iloc "},{"metadata":{"trusted":true},"cell_type":"code","source":"s2.ix[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#USing loc\nMenu.loc['Milk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Menu.loc['bread':'Milk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using iloc\nMenu.iloc[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Menu.iloc[1:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.Boolean Indexing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a boolean series using logical comparision\nMenu > 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Subset the series using the boolean to retain values where True\nMenu[Menu>1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Peeking at the Data\n\nhead() and tail() methods are used to view small sample of a Series or DataFrame.\n\nThe default number of element to display is five."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npath=\"../input/us-counties-covid-19-dataset/us-counties.csv\"\nData=pd.read_csv(path)\nData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.tail(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Type Conversion\n\nastype() method is used for type conversion"},{"metadata":{"trusted":true},"cell_type":"code","source":"S=pd.Series(['23','24','57'])\nS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treating Outlier\n\n**clip(upper=threshold) /clip(lower=threshold)**  is used to clip outliers at a particular threshold value. All values lower than the supplied lower threshold will be replaced by threshold value. Similary for all values greater higher than upper threshold will be replaced by upper threshhold value.\n\nNotes: clip_upper() and clip_lower() are now deprecated methods\n\nIn data wrangling we generally clip values at the 1st-99th percentile or the 5th-95th percentile\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"s1=pd.Series([-100,-23,-2,5,6,7,8,56,78,98,1000])\ns1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s1.clip(upper=98)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s1.clip(lower=-23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s1.clip_upper(98)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s1.clip_lower(-23)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Replacing Values\n\nreplace() method is used to replace source value with target value."},{"metadata":{"trusted":true},"cell_type":"code","source":"School=pd.Series(['teacher','student','building'])\nSchool","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"School.replace({'teacher':'Teacher1','building':'Building1'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"School","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking if a Value belongs to a list in a Series\n\nisin()"},{"metadata":{"trusted":true},"cell_type":"code","source":"School.isin(['teacher','foo'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding uniques and their frequency\n\nMethods are:\n\n* unique: Find the array of distinct values in categorical series\n\n* nunique: To count number of distinct values\n\n* value_counts: To create a frequency table"},{"metadata":{"trusted":true},"cell_type":"code","source":"S=pd.Series(list('abcde'*3))\nS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing with Duplicates\n\nduplicated() : produces a boolean that marks every instance of a value after its first occurance as True\n\ndrop_duplicates(): returns the Series with the duplicates removed. To drop permanently duplicated value ,pass the **inplace=True **argument"},{"metadata":{"trusted":true},"cell_type":"code","source":"S.duplicated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding the largest/smallest values\n\n* idxmax : find the index of Largest value\n* idxmin : find the index of smallest value\n* nlargest : find n-largest \n* nsmallest : find n-smallest\n\nNote: index label is returned with these values ,and this can be especially helpful in many cases\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import Series\ns10=Series(np.random.randint(0,50,6),index=list('xyzabc'))\ns10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ns10.idxmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s10.idxmin()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nlargest() ->provide number of largest elements you want to check\ns10.nlargest(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s10.nsmallest(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s10.nsmallest(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sorting the Data \n\nsort_values(): Sorting Series by values\n\nsort_index() :sorting series by index\n\nNote: in order to make the sorting permanent ,we need to pass an inplace=True argument"},{"metadata":{"trusted":true},"cell_type":"code","source":"s10.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s10.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s10.sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s10.sort_index(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mathematical Summeries\n\n* mean : give the average value of data\n* median :provide the middle value of data\n* std    : Give the spreading of data\n* quantile : finds the requested percentile\n* describe : produces the summary statistics of Data\n\n\nThese functions come handy when we are exploring Data for patterns\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"S=pd.Series(np.random.randn(1000))\nS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.quantile([0.25,0.30,0.9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing With Missing Data\n\nisnull,notnull are complementary methods that work on a Series with missing data to produce boolean series to identify \nmissing or non-missing values respectively.\n\n\nNote: Numpy np.nan and the base python None type are identified as missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"S1=pd.Series([1.12,4,5,np.nan,5.67,None])\nS1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S1.isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S1.notnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Value Imputation\n\n***fillna,ffill,bfill***: helps us to deal with missing daat by choosing either impute them with a particular value \n\nffill: forward fill : propagates the non-null value forward untill  another non-null value encountered\n\nbfill: backward fill : prpagates the first observed non-null value backward untill another non-null value is met\n\n***dropna:*** to drop the missing data altogether \n\n\n***Note*: **It is common practice in data science to replace missing values in a numeric variable by its mean or median if Data is skewed and in categorical variable with its mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"S2=pd.Series([1.12,1.13,5.6,np.nan,9.0,None])\nS2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S2.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S2.fillna(S2.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S2.ffill()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S2.bfill()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S2.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply a function to each element\n\nmap() :\n* It is the most important of all series methods. \n* It takes a general-purpose or user-defined function and applies it to each value in series.\n* Combines with base python's lambda functions ,it can be incredibly powerful tool in transforming given series."},{"metadata":{"trusted":true},"cell_type":"code","source":"S=pd.Series(['Smita Smith','John Mchel','Donald Trump','Melania Trump'])\nS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the length of each name\nS.map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the initials\n\nS.map(lambda x: '.'.join([i[0] for i in x.split(' ')]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the Data\n\nPlot() : plot method is the gateway to draw potential visualizations like bar charts,histograms,scatterplots,boxplots and more."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a categorical series\n\nS4=pd.Series(list('a'*3)+list('b'*5)+list('c'*9)+list('d'*2))\nS4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S4.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S5=pd.Series(np.random.randn(1000))\nS5.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S5.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DataFrame"},{"metadata":{},"cell_type":"markdown","source":"Creating a Data Frame:\n\none of the most common ways of creating a DataFrame is from a dictionary of arrays or lists or from Numpy 2D arrays. \n\nHere's the syntax,\n\nDataFrame(data= ,index= ,columns= )\n"},{"metadata":{},"cell_type":"markdown","source":"# Creating DataFrame from a 2D Array"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(np.arange(24).reshape(8,3))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=np.arange(20,32).reshape(3,4)\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df2=pd.DataFrame(data=np.arange(20,32).reshape(3,4),columns=list('ABCD'),index=list('XYZ'))\nDf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using a Dict of equal length Lists\n\n* The key of the dictionary will be used as column names.\n\n* The values will form the data in the table.\n\n* We can optionally provide list of strings to be used as the index(or row labels)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict1={'ints':np.arange(5),'floats':np.arange(0.1,0.6,0.1),'strings':list('abcde')}\ndict1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=pd.DataFrame(dict1,index=list('xyzab'))\ndf3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DataFrame Attributes\n\nsome of the attributes are \n\n* index\n* columns\n* dtypes\n* shape\n* info"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get row labels\n\npath=\"../input/us-counties-covid-19-dataset/us-counties.csv\"\nData=pd.read_csv(path)\nData.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get columns names\n\nData.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get data types of each column\n\nData.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Subsetting DataFrame\n\nPandas provides variety of ways to extract from a given dataset\n\n* A value\n* a row or column(Returns a series)\n* Multiple rows/columns(Returns subset of the DataFrame)"},{"metadata":{},"cell_type":"markdown","source":"# Selecting a single Row/Column\n\nAn individual column can be retrieved as a Series using \n\n* the sqaure bracket accessor ----------        (syntax: df['column_name'])\n* The dot accessor           ----------         (syntax:  df.column_name)\n* one of the accessor -loc,iloc ----------      (syntax:  df.iloc[:,column_number]"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The square bracket accessor\ndf3['ints']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The dot accessor\ndf3.floats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By using loc\ndf3.loc[:,'strings']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df3.iloc[0:2,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using .loc and a row label\ndf3.loc['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using .iloc and row position\n\ndf3.iloc[3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting 2 or more rows/columns\n\nThis can be accomplished by\n\n* Passing list of column labels to the double square bracket\n \n* passing list or slice of row/column labels/positions to loc,iloc\n\n* Passing boolean series to loc,ilocfor selecting particular rows and columns\n\nSyntax\n\n**loc is label based**\n\ndf.loc[list/slice of_row_labels,list_of_column_labels]\n\n**iloc is integer based**\n\ndf.iloc[row_position,column_positions]\n\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.randint(10,50,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a new data frame\n\ndf4=pd.DataFrame(np.random.randint(0,100,25).reshape(5,5),index=list('abcde'),columns=list('PQRST'))\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Multiple columns\ndf4[['P','Q']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.loc[:,['R','S']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subset multiple columns using .iloc\ndf4.iloc[:,2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select multiple rows using []\ndf4['b':'d']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#select multiple rows using .loc\ndf4.loc['b':'d',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subset multiple rows using .iloc\n\ndf4.iloc[2:4,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding/Removing/Renaming columns or Rows\n\nSome methods for acieving this are:\n\n* assign\n* drop\n* rename\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Method for creating new column\n\ndf4['U']=df4['P']+df4['Q']\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assign for creating columns on the fly\ndf4.assign(U=lambda x:x['P']+x['Q'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"drop: returns a copy of the DataFrame after deleting the rows and columns\n\naxis = parameter controls which axis (row or column) we want to drop the series from, axis=0 for rows and axis=1 for columns\n\ninplace = parameter decides whether the change must be made permanent\n\ndropna() which helps us get rid of rows with missing data\n\nrename() takes a Dataframe as input and a dictionary that maps old names to new names for columns \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.drop('P',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop multiple of columns by providing a list of labels,axis=1 is for dropping columns\n\ndf4.drop(['P','Q'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Same effect for dropping rows,\ndf4.drop(['a','c'],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By default it takes axis=0 to delete rows\ndf4.drop(['b','c'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.rename(columns={'P':'P_new','R':'R_new'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Maths/Stats Operations\n\nDifferent methods are\n\n* sum() : Sum of values\n\n* count() : number of non null observations\n\n* mean() : Mean/average of values\n\n* Mad(): Mean absolute deviation\n\n* median(): middle value\n\n* min(): Minimum\n\n* max():Maximum\n\n* mode() : Mode or highest frequency of observations\n\n* abs(): absolute value\n\n* prod(): product of values\n\n* std(): Bessel-corrected sample standard deviation\n\n* var(): unbiased variance\n\n* sem(): standard erroe of the mean\n \n* skew(): sample skewness\n\n* kurt():sample kurtosis\n\n* quantile(): sample quantile\n \n* cumsum():cumulative sum\n \n* cumprod():cumulative product\n\n* cummax():cumulative max\n\n* cummin(): cumulative min\n\n**describe(): **\n\n* provides summary statistics of all numeric variables in  the DataFrame\n\n* For categorical data ,it will give simple summary of the number of unique values and most frequently occuring values\n\n* include parameter gives the option whether to choose data for \"numerical\"/\"categorical\"/ for both data\n\n    * describe(include=['number'])\n    \n    * describe(include=['object'])\n    \n    * describe(include='all')\n    \n \nskipna = parameter that signals whether to exclude missing data (True by default)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can specify the perentiles which we need but median will be default\n\nData.describe(percentiles=[0.01,0.05,0.95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorical_columns=[col for col in Data.columns if Data[col].dtype=='object']\nCategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cat_data=Data[['date', 'county', 'state']]\nCat_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Describe() for categorical data\nCat_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get summeries for numerical variable which is default\nData.describe(include=['number'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FGet summeries for categorical variables\nData.describe(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get summeries for all variables\nData.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing Values\n\nSame methods as we used for series we can use for dataframes.\n\n* isnull()\n* notnull()\n* dropna() with the axis= and inplace= parameter\n\n\nMissing value imputations is done using the fillna(),ffill(),bfill()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create some missing data\n\ndf4.iloc[::2]=np.nan\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Detect some missing data\ndf4.isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace missing value with 0 manually\n\ndf4[df4.isnull()]=0\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4[::2]=np.NAN\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace missing with 0 (using fillna)\ndf4.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop rows with missing data\ndf4.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sorting Data\n\n\nSorting data is a basic task that allows us to figure out if a given variable has outliers by looking at the values at its extreme\n\nascending= parameter is used to control the nature of sorting.By default it takes True. So to get Data in descending order we need to pass ascending=False\n\nsort_index(): we use to reorder rows or columns\n\nsort_values(): to sort on column values, by= parameter specify the columns on wich we want to sort the data     "},{"metadata":{"trusted":true},"cell_type":"code","source":"df5=pd.DataFrame(np.random.randn(9).reshape(3,3),index=list('cba'),columns=list('prq'))\ndf5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort_index() will sort the index (rows) of the DataFrame\ndf5.sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To sort column names\ndf5.sort_index(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sort the data by the value\ndf5.sort_values(by=['p'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.sort_values(by=['p','r'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Duplicates\n\nThe methods duplicated,drop_duplicates help us in identifying rows that are duplicates of other rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a duplicate row in the data\n\ndf5.loc['x',:]=df5.loc['c']\ndf5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.duplicated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Binning Data with cut and qcut\n\nBinning is the process of converting ** *numerical variables to categoricals***. \n\nWe use this methods widely whenw need to convrt numericalvariables(such as Age,Salary) into ranges such as (Age groups,salary groups)\n\nFunctions are:\n\n* pd.cut()\n* pd.qcut()\n\ncut(): each bin will have the same width ,but they may not have the same number of records\nqcut(): each bin will have the same number of records but they may not have the same width\n\nThey take Arguments as following:\n\n* var: the continuous variable to discretize\n* bins: specified as a number or a list of bin edges\n* right=True. a boolean to include edge or not\n* labels= for naming the bins\n* precision=\n\n\n**Note:**\nIf your data has missing values or a lot of duplicate values -typically the case with skewed discrete distributions) it will cause qcut to fail as it would not be able to detect unique bin edges."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create an array\n\nA=(np.random.randn(1000)*100).astype(int)\nA[1:6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bins produced by pd.cut\npd.cut(A,10)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bins produced by qcut\n\npd.qcut(A,10)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of values in cut bins follow the underlyingdistribution\n\npd.cut(A,10).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of values in qcut bins\n\npd.qcut(A,10).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To create bin boundaries manually we can pass list of binboundaries to the bins= argument of cut\n\npd.cut(A,bins=range(-350,350,50)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Dummies from Categorical Data\n\nA dummy variable is one that takes the value 0 or 1 to indicate the absense or presence of particular variable.\n\nFor example suppose 'GENDER' is one of the qualitative explanatory variables relevant to data problem ,then female and male will be the level of this variable. We can create two dummy variables from this variable (dummy_male,dummy_female)\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Creating Dummies Manually"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame({'key':list('aabbbcc'),'val':np.random.randn(7).round(2)})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummy for level-a\n\n  (df['key']=='a').astype(int)  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummy for level b\n\n(df['key']=='b').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummy for level-c\n\n(df['key']=='c').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummy for all labels\ndf.assign(dummy_a=lambda x:(x['key']=='a').astype(int),dummy_b=lambda x: (x['key']=='b').astype(int),dummy_c=lambda x:(x['key']=='c').astype(int))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating dummies with pd.get_dummies()"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(df['key'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Joining dummies to original data frame\n\ndf.join(pd.get_dummies(df['key']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dummy Variable Trap\n\n\n\nIf we build dummy variables for every level of categorical variable ,it leads to a situation where one or more of these dummies would be highly ***correlated*** leading to problems of ***multicollinearity***. It is thus advised to create (K-1) dummies for a categorical valriable with K-levels.\n\n\nSo steps to create dummy variableare:\n\n* Identify the categorical variables you want to create dummies\n\n* create dummies for n-1 catefories for each\n\n* Join the dummies into original table\n\n* Drop the categorical variable in step1"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create dummies,drop the categorical variable and one of the dummies to avoid trap\n\ndf.join(pd.get_dummies(df['key'],prefix='dummy')).drop('key',axis=1).drop('dummy_c',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reshaping Data\n\nLong Data: Usually for storage efficiency ,data in relational databases is stored in \"long\" or \"stacked\" format. This  means there are fewer columns and more rows,with label duplication in keys.\n\n\nWide Data: For certain type of Data analysis expecially for time series data we might prefer to have the data in the wide format(More columns ,unique labels in keys)\n\nNote: long and wide are just different represenation of same data.\n\n# Stack() and unstack()\n\nFor pandas dataframe with hierachical indices,stack() and unstack() provide a convenient way to reshape the data from wide-to-long or long-to-wide formats.\n\n    stack(): pivots the columns into rows(wide-to-long)\n    unstack(): pivots rows into colums(long-to-wide)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a dataset in long format\nX=pd.DataFrame({'Group':list('PQR'*4),'Item':list('ABCD')*3,'Status':(np.random.randn(12)).round(2)}).set_index(['Group','Item'])\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.unstack()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a data set in wide format\n\nY=pd.DataFrame(np.random.randn(12).reshape(3,4).round(2),index=list('ABC'),columns=list('PQRS'))\nY\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.stack()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pivot() \n\ncolumns= parameter is for columns\nindex= parameter is to take column which can be used as row \nValues= a column to fill in the data\n\nSimply put pivot() is just a convenient wrapper function that replaces the need to create a hierarchical index using set_index and reshaping with stack\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.reset_index(inplace=True)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.pivot(index='Group',columns='Item',values='Status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# pivot_table() :\n\nis similar to pivot ,but \n*         can work with duplicate indices \n*         lets you specify an aggregation function"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame({'C1':list(('x'*4+'y'*4)*2),'C2':list('aabbaabbbaaabbaa'),'N1':np.random.randn(16)})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.pivot_table(index='C1',columns='C2',values='N1',aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# apply() and applymap()\n\nThere are often situations in data analysis where we want to apply an in-built or user defined function to values of data avoiding any iterations and for-loop.\n\naaply() and applyma() methods provide powerful yet simple interface for doing this.\n\n       applymap() allows you to apply a function to each element of a DataFrame\n       apply() allows youto apply function to rows/columns (controlled via the axis=parameter ) of a DaraFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ex applymap()\n\nDf=pd.DataFrame(np.random.randn(20).reshape(4,5),index=list('abcd'),columns=list('pqrst'))\nDf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some Basic Funtions in Pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the squareroot of each number\nDf.applymap(lambda x:np.sqrt(np.abs(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using apply to standardize variable in dataset\n#standardization is the process of producing each numeric variable's mean to zero and standard deviation to 1 by substracting \n#each value from its column mean and dividing it by the column standard deviation\n\nDf=pd.DataFrame(np.random.randn(1000).reshape(200,5),columns=['col_'+str(x) for x in range(5)])\nDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply standardization function to each column\n\nDf_standardize=Df.apply(lambda x: (x-x.mean())/x.std())\nDf_standardize.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if standardization is successful\nDf_standardize.mean().round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df_standardize.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split-Apply-Combine using groupby\n\nTo do the summarization of data during analysis we need group by function.\n\nThis includes evaluating summary statistics over variables ,or groups within variables ,within group transformations computing pivot-tables and by-group analysis.\n\nIt is same as SQL group by function.\n\nThese process involves 3 teps:\n*      Applying a  function to each group individually. There are 3 classes of functions we might consider:\n         \n             Aggregate-Estimate summary statistics (like counts,means) for each group. This willchange the size of data.          \n             Transform- within group standardization,imputation using group values.The size of the data wil not change.\n          \n             Filter-Ignore rows that belong to a particular group\n         \n             A combination of these three\n         \n*      Combining the results into a series or DataFrame\n\n\nAttributes are:\n\n* size(): which returns the count of elements in each group\n \n* can be subsetted using columnnames or arraysof columnnames to select variable for aggregation\n\n* Have optimized methods for general aggregation methods like\n\n       * count,sum\n       * mean,median,std,var\n       * first,last\n       * min,max\n* specialized methods like .describe() also apply to these objects\n\nMost important DataFrame Groupbyobject methods are :\n*     agg()\n*     transform()\n*     apply()\n         "},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame({'K1':list('abcd')*25,'K2':list('xy'*25+'yx'*25),'V1':np.random.rand(100),'V2':np.random.rand(100)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('K1').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('K2').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping by 2 keys\n\ndf.groupby(['K1','K2']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column wise aggregations(Using optimized statistical methods)\n\ndf.groupby('K1')['V1'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The .agg() method\n\nIt takes argument as follows:\n\n* list of function names to be applied to all selected columns\n* tuples of (colname,function) to be applied to all selected columns\n* dict of(colname,function) to be applied to each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.Apply more than 1 functions to selected columns by passing name of functions as a list\n\ndf.groupby('K1')['V1'].agg(['min','max','mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('K2')[['V1','V2']].agg(['min','max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Provide names for aggregated columns\n\ndf.groupby('K1')['V1','V2'].agg([('smallest','min'),('largest','max')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply max and min to V1;and mean and sum to V2;all grouped by K1\n\ndf.groupby('K1')['V1','V2'].agg({'V1':('max','min'),'V2':('mean','sum')})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The .apply() method\n\nThis method helps us to apply a function along an axis of the DataFrame\n    1.A general or user defined function\n    2.Any other parameters that the function would take\n    \nsyntax: \nDataFrame.apply(self, func, axis=0, raw=False, result_type=None, args=(), **kwds)\nNote: Any function passed through apply should be designed to work on  a subset of the DataFrame(smaller tables with the same columns ,and all rows corresponding to a particularlevel of groupby variable)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('K2').apply(max)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('K1')['V1'].apply(lambda x:x+100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine 2 DataFrames with pd.merge\n\npandas.merge is similar to the SQL join opertions,it links rows of tables using one or more keys\n\n\nSyntax:\n     \n     pd.merge(df1,df2,how='left',on='key',\n              left_on=None,right_on=None,left_index=False,right_index=False,sort=True,copy=True,suffixes=('x','y'))\n\n\nThe syntax includes specifications of the following rguments:\n\n* Which column to merge on:\n    * the on='key' if the same key is present in the two Dfs\n    * or left_on='lkey',right_on='rkey' if the keys have different nmaes in the DFs\n    * To emerge on multiple keys ,pass list of column names \n* The nature of join:\n    * The how= option with left,right,outer \n    * By default the join is inner join\n* Tuple of String values to append to overlapping column names to identify them in the merged dataset\n    * The suffixes= option\n    * defaults to ('_X','_Y')\n* If you wish to merge on the index ,pass left_index=True or right_index=True or both\n* Sort the result DataFrame by the join keys in lexicographical order or not\n    * sort= option\n    * Defaults to True,setting to False will improve performance substantially in many cases\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df0=pd.DataFrame({'key':list('abcde'),'data0':np.random.randint(0,100,5)})\ndf1=pd.DataFrame({'key':['b','b','a','c','a','a','b'],'data1':np.random.randint(0,100,7)})\ndf2=pd.DataFrame({'key':['a','b','d','f','g'],'data2':np.random.randint(0,100,5)})\ndf3=pd.DataFrame({'lkey':['b','b','a','c','a','a','b'],'data3':np.random.randint(0,100,7)})\ndf4=pd.DataFrame({'rkey':['a','b','d'],'data4':np.random.randint(0,100,3)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Example\ndf0=pd.DataFrame({'key':list('abcde'),'data0':np.random.randint(0,100,5)})\ndf0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame({'key':['b','b','a','c','a','a','b'],'data1':np.random.randint(0,100,7)})\ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=pd.DataFrame({'key':['a','b','d','f','g'],'data2':np.random.randint(0,100,5)})\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=pd.DataFrame({'lkey':['b','b','a','c','a','a','b'],'data3':np.random.randint(0,100,7)})\ndf3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4=pd.DataFrame({'rkey':['a','b','d'],'data4':np.random.randint(0,100,3)})\ndf4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Default merge (inner join) when both DatatFrame have  common keys"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(df0,df1)\n#Note we could have written : pd.merge(df0,df1,on='key',how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If there is no matching column names  then default merge will through an error\n\n#pd.merge(df0,df4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. OuterJoin\n\nThis leads to a union of keys ,missing values are imputed in the resulting dataset whenever a match is not found in either table"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(df0,df2,how='outer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Leftjoin"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(df1,df2,how='left')\n#As value of c is not present in Data it will be filled by NAN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merging with keys having different names in the two dataframes\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(df1,df4,left_on='key',right_on='rkey')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge with DataFrame containing overlaping column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add a column with the same name to df1 and  df2\ndf1['newcol']=np.random.randn(7)\ndf2['newcol']=np.random.randn(5)\npd.merge(df1,df2,on='key')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We could specify other suffixes explicitly with the suffixes= option"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(df1,df2,on='key',suffixes=['_df1','_df2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge when the key is the index in one (or both) of the DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.set_index('lkey',inplace=True)\npd.merge(df2,df3,how='left',left_on='key',right_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine multiple DataFrame with DataFrame.join()\n\n.join() is a convenient DataFrame method for combining many DataFrame objects with the same or similar indexes but (non overlapping) columns into a single result DataFrame.\n    * Bydefault ,the join method performs left join on the join keys\n    * For simple index-on-index merges we can pass  a list of DataFrames to join\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.join(df3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df1.join(df3) is equivalent to below command\npd.merge(df1,df3,how='left',right_index=True,left_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create dataframe with partially overlapping index\n\ndf=pd.DataFrame(np.random.randint(0,50,32).reshape(8,4),columns=list('WXYZ'),index=list('abcdefgh'))\ndf1=df.iloc[2:,0:2]\ndf2=df.iloc[:5,2:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Perform the join\ndf1.join(df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The real strength of the method is apparent when we have multiple dataframes.Then instead of having to write a number of merge statements,we can get away with writing a single join"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df.iloc[0:3,0:2]\ndf3.columns=['P','Q']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.join([df2,df3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So we observed that default action is  a left join . So all values of the index in df1 are retained in the output,and missing values are imputed where ever no matching values are found.\n\n* This action can be altered by passing the how= parameter"},{"metadata":{},"cell_type":"markdown","source":"# Combine multiple DataFrames/Series with pd.concat()\n\nThe concat() function in pandas is used to concatenate pandas objects along a particular axis with optional set logic along the other axis.\n\nThis operational is synonymous with binding,stacking,union all functions in other languages .\n\nConcatenatation can happen along either axis,the action being governed by the axis= paremeter.\n\n    With axis=0 the objects will be appened vertically\\ i.e the resulting object will have more rows.\n    With axis=1 the objects will be concatenated horizontally ,leading to an  object with more columns\n    \nDepending on whether the objects have overlapping index (or column ) labels,the concat also include merging of the data where an overlap is found."},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating series with non-overlapping indices\ns1=pd.Series(np.random.randn(3),index=list('abc'))\ns2=pd.Series(np.random.randn(4),index=list('defg'))\ns3=pd.Series(np.random.randn(2),index=list('hi'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.For Series objects with no index overlap"},{"metadata":{"trusted":true},"cell_type":"code","source":"#With axis=0 default\npd.concat([s1,s2,s3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With axis=1\npd.concat([s1,s2,s3],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. For Series objects withan overlapping indexes ,we can specify the join= parameter to intersect the data.****"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a new series\ns4=pd.Series(np.random.randn(5),index=list('abcde'))\ns4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Concta with overlapping index\npd.concat([s1,s4],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if you will specify a join type ,this will be equivalent to a merge\npd.concat([s1,s4],axis=1,join='inner')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. For DataFrame objects with no overlapping index:\n\n    * axis=0 will produce a concatenation\n    * axis=1 will produce as merge,imputing NAN values where necessary"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create DataFrames with non-overlapping indexes\n\ndf1=pd.DataFrame(np.random.randn(9).reshape(3,3),index=list('abc'),columns=list('XYZ'))\ndf2=pd.DataFrame(np.random.randn(4).reshape(2,2),index=list('pq'),columns=list('XZ'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# default axis=0 \npd.concat([df1,df2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Axis=1\npd.concat([df1,df2],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.For DataFrame object with an overlapping index"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame(np.random.randn(9).reshape(3,3),index=list('abc'),columns=list('XYZ'))\ndf2=pd.DataFrame(np.random.randn(4).reshape(2,2),index=list('ac'),columns=list('XZ'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Default axis=0\npd.concat([df1,df2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# axis=1\n\npd.concat([df1,df2],axis=1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}