{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#imports\nimport os\nimport string\nimport numpy as np # linear algebra\nimport pandas as pd\nimport random\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport gensim\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional,Conv2D, Conv1D, GlobalMaxPooling1D, Dropout, ReLU, \\\n                                    Reshape, MaxPooling2D, Concatenate, TimeDistributed,Flatten\nfrom keras import regularizers\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, accuracy_score\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import CuDNNLSTM, CuDNNGRU","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read reviews from csv file\ndf = pd.read_csv(\"../input/imdb-review-dataset/imdb_master.csv\", encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Process all reviews\ndef process_reviews(reviews):\n    p_reviews=[]\n    for review in reviews:\n        #remove html tags\n        soup = BeautifulSoup(review)\n        review = soup.get_text()\n        #tokenize review: separate each review them into words\n        #TODO try sentence tokenizer\n        tokens = word_tokenize(review)\n        #convert words to lower case\n        tokens = [t.lower() for t in tokens]\n        #remove punctuations\n        table = str.maketrans('', '', string.punctuation)\n        stripped = [t.translate(table) for t in tokens]\n        #remove non alphabetic tokens\n        words = [word for word in stripped if word.isalpha()]\n        #filter stopwords\n        stop_words = set(stopwords.words('english'))\n        words = [w for w in words  if not w in stop_words]\n        words = [wnl.lemmatize(w) for w in words]\n        p_reviews.append(words)\n        \n    return p_reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def blstm_model(num_words, e_size, e_matrix, max_length, activation):\n    model = Sequential()\n    embedding_layer = Embedding(num_words,\n                                e_size,\n                                embeddings_initializer = Constant(e_matrix), \n                                input_length = max_length,\n                                trainable=True)\n    model.add(embedding_layer)\n    model.add(Bidirectional(CuDNNLSTM(100, dropout = 0.2, recurrent_dropout= 0.2)))\n    model.add(Dense(2, activation=activation))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN model implemented by Kim Yoon\n# Paper: Convolutional Neural Networks for Sentence Classification\n# url: https://www.aclweb.org/anthology/D14-1181/\n\n# Hyperparameters\ndef yoon_cnn(num_words, embedding_size, embedding_matrix, input_length):\n    filter_sizes = [3, 4, 5]  # defined convs regions\n    num_filters = 100  # num_filters per conv region\n    drop = 0.5\n\n    embedding_layer = Embedding(num_words,\n                                embedding_size,\n                                embeddings_initializer=Constant(embedding_matrix),\n                                input_length=input_length,\n                                trainable=True)\n\n    inputs = Input(shape=(input_length,), dtype='int32')\n    embedding = embedding_layer(inputs)\n    reshape = Reshape((input_length, embedding_size, 1))(embedding)\n\n    conv_0 = Conv2D(num_filters, (filter_sizes[0], embedding_size), activation='relu',\n                    kernel_regularizer=regularizers.l2(0.01))(reshape)\n    conv_1 = Conv2D(num_filters, (filter_sizes[1], embedding_size), activation='relu',\n                    kernel_regularizer=regularizers.l2(0.01))(reshape)\n    conv_2 = Conv2D(num_filters, (filter_sizes[2], embedding_size), activation='relu',\n                    kernel_regularizer=regularizers.l2(0.01))(reshape)\n\n    maxpool_0 = MaxPooling2D((input_length - filter_sizes[0] + 1, 1), strides=(1, 1))(conv_0)\n    maxpool_1 = MaxPooling2D((input_length - filter_sizes[1] + 1, 1), strides=(1, 1))(conv_1)\n    maxpool_2 = MaxPooling2D((input_length - filter_sizes[2] + 1, 1), strides=(1, 1))(conv_2)\n\n    merged_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n    flatten = Flatten()(merged_tensor)\n    dropout1 = Dropout(drop)(flatten)\n    output = Dense(units=2, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout1)\n\n    # this creates a model that includes\n    model = Model(inputs, output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_metrics(test_labels, predictions):\n    print('Confusion Matrix')\n    print(confusion_matrix(test_labels.argmax(axis=1), predictions))\n    print('')\n    print('Classification Report')\n    print(classification_report(test_labels.argmax(axis=1), predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnn_model(num_words, embedding_size, embedding_matrix, input_length):\n    model = Sequential()\n\n    model.add(Embedding(num_words,\n                        embedding_size,\n                        embeddings_initializer=Constant(embedding_matrix),\n                        input_length=input_length,\n                        trainable=True))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(250,3,padding='valid', activation='relu', strides=1))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(250))\n    model.add(Dropout(0.2))\n    model.add(ReLU())\n    model.add(Dense(2, activation='softmax'))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_callbacks(best_model_path, monitor, mode, patience):\n    es = EarlyStopping(monitor=monitor, mode=mode, verbose=1, patience=patience)\n    mc = ModelCheckpoint(best_model_path, monitor=monitor, mode=mode, verbose=1, save_best_only=True)\n    callbacks = [es, mc]\n    return callbacks\n\ndef plot_graph(title, train_hist, val_hist, x_label, y_label, legend, loc):\n    plt.plot(train_hist)\n    plt.plot(val_hist)\n    plt.title(title)\n    plt.ylabel(y_label)\n    plt.xlabel(x_label)\n    plt.legend(legend, loc=loc)\n#     plt.savefig(path)\n    plt.show()\n    plt.clf()\n\ndef decode_one_hot_labels(test_labels):\n    return np.argmax(test_labels, axis=1)\n\ndef get_metrics(test_labels, predictions):\n    print('Confusion Matrix')\n    print(confusion_matrix(test_labels.argmax(axis=1), predictions))\n    print('')\n    print('Classification Report')\n    print(classification_report(test_labels.argmax(axis=1), predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get all reviews and labels\nreviews = df.loc[((df['type'] == 'train') | (df['type'] == 'test')) & ((df['label'] == 'neg') | (df['label'] == 'pos'))]\n\nprint(reviews.shape)\nprint(reviews['label'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change values of labels\n#neg = 0 pos = 1\nreviews.loc[reviews['label']=='neg', 'label'] = 0\nreviews.loc[reviews['label']=='pos', 'label'] = 1\nprint(reviews['type'].unique())\nprint(reviews['label'].unique())\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split review and label dataframe\nreview_data = reviews['review']\nreview_labels = reviews['label']\n\nreview_data = review_data.tolist()\nreview_labels = review_labels.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Largest number of words in the reviews\nmax_length = max(len(r.split()) for r in review_data)\nprint(max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(review_data[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialize lemmatizer\nwnl = WordNetLemmatizer()\n#Example\ns_review = wnl.lemmatize('movies')\nprint(s_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean the data\nclean_reviews = process_reviews(review_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(clean_reviews))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train the Word2Vec model with a embedding size=100\nembedding_size = 100\n\nw2v_model = gensim.models.Word2Vec(sentences = clean_reviews, size = embedding_size, window=5, workers=10, min_count=1, iter=10)\nwords = list(w2v_model.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(w2v_model.wv.most_similar('love'))\nprint('')\nprint(w2v_model.wv.most_similar_cosmul(positive = ['woman','king'], negative = ['man']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.doesnt_match('chinese british coffee spanish'.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save embeddings to file\nfile = \"w2v_embeddings_imdb.txt\"\nw2v_model.wv.save_word2vec_format(file, binary=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read embeddings file\nembeddings_file = open(os.path.join('','w2v_embeddings_imdb.txt'), encoding='utf-8')\n\ndictionary = {}\n\n#Create a dictionary of each word and its learned vectors\nfor line in embeddings_file:\n    values = line.split()\n    word = values[0]\n    vectors = np.asarray(values[1:])\n    dictionary[word] = vectors\n    \nembeddings_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary['car']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate a Tokenizer: it assigns each word in the corpus a integer values\ntokenizer = Tokenizer()\n#This method creates the vocabulary index based on word frequency\ntokenizer.fit_on_texts(clean_reviews)\n#It takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\nsequences = tokenizer.texts_to_sequences(clean_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sequences[1])\nprint('')\nprint(sequences[0:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pad sequnces to the same length\npadded_reviews = pad_sequences(sequences, maxlen = max_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert labels to numpy array\nreview_labels = np.asarray(review_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Unique tokens: '+str(len(word_index)))\nprint('Shape of reviews:', padded_reviews.shape)\nprint('Shape of review labels:',review_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_index)+1\n\n#(180305, 100)\nembedding_matrix = np.zeros((num_words, embedding_size))\n\n#word_index(word, number of coressponding word)\nfor word, num in word_index.items():\n    #if int value of word is > the token dictionary size, ignore\n    if num > num_words:\n        continue\n    #Get features vectors for each word in token dictionary from our trained embedding layer i.e w2v\n    #Store in the embedding matrix(number coressponding to word, feature vectors)\n    embedding_vector = dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[num] = embedding_vector\n    else:\n        embedding_matrix[num] = np.random.randn(embedding_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # instantiating the model in the strategy scope creates the model on the TPU\n\n    #     model = tf.keras.Sequential( … ) # define your model normally\n    #     model.compile( … )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shuffle out data\nindices = np.arange(padded_reviews.shape[0])\nnp.random.shuffle(indices)\n\npadded_reviews = padded_reviews[indices]\nreview_labels = review_labels[indices]\n# print(len(padded_reviews))\n\n#Split our data in training and test data 80:20\nX_train, X_test, y_train, y_test = train_test_split(padded_reviews, review_labels, test_size=0.2)\n\n#OneHot encode labels\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize mode\nbest_model_path = \"semtiment_model.h5\"\ncallbacks = create_callbacks(best_model_path, 'val_loss', 'min', 2)\n        \nmodel = yoon_cnn(num_words, embedding_size, embedding_matrix, max_length)\nmodel.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train[0]\n# Training a BiLSTM takes forever\nepochs = 200\nhistory = model.fit(X_train, \n                    y_train, \n                    batch_size=BATCH_SIZE, \n                    epochs=epochs, \n                    validation_split=0.2,\n                    callbacks=callbacks,\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test model\nscore,acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = BATCH_SIZE)\nprint(\"Test score: %.4f\" % (score))\nprint(\"Model Accuracy: %.4f\" % (acc))\n\npreds = model.predict(X_test, verbose=2)\npreds = np.argmax(preds, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show metrics\nget_metrics(y_test,preds)\n\naccuracy = accuracy_score(y_test.argmax(axis=1),preds)\nprint('Accuracy: ', str(accuracy))\n\nprecision = precision_score(y_test.argmax(axis=1),preds,average='binary')\nprint('Precision: ', str(precision))\n\nrecall = recall_score(y_test.argmax(axis=1),preds,average='binary')\nprint('Recall: ', str(recall))\n\nf1 = f1_score(y_test.argmax(axis=1), preds, average='binary')\nprint('F1 score: ', str(f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graph('Accuracy', history.history['accuracy'], history.history['val_accuracy'], \n           'epochs', 'Acc', ['train', 'val'], 'upper left')\n\nplot_graph('Loss', history.history['loss'], history.history['val_loss'], \n           'epochs', 'Loss', ['train', 'val'], 'upper left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean a reviews\n\ndef clean_reviews(user_reviews):\n\n  usr_reviews = []\n\n  for user_review in user_reviews:\n    #remove html tags\n    soup = BeautifulSoup(user_review)\n    user_review = soup.get_text()\n    #tokenize review: separate each review them into words\n    #TODO try sentence tokenizer\n    tokens = word_tokenize(user_review)\n    #convert words to lower case\n    tokens = [t.lower() for t in tokens]\n    #remove punctuations\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [t.translate(table) for t in tokens]\n    #remove non alphabetic tokens\n    words = [word for word in stripped if word.isalpha()]\n    #filter stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words  if not w in stop_words]\n    words = [wnl.lemmatize(w) for w in words]\n\n    usr_reviews.append(words)\n\n  return usr_reviews\n\n#Get max length\ndef get_max_length(data):\n  list_len = [len(i) for i in data]\n  max_length = max(list_len)\n  print(max_length)\n\ndef tokenize_reviews(data, tokenizer):\n  # user_tokenizer = Tokenizer()\n  # user_tokenizer.fit_on_texts(data)\n  user_sequences = tokenizer.texts_to_sequences(data)\n\n  return user_sequences\n\ndef pad_reviews(seq, max_l):\n    padded = pad_sequences(seq, maxlen = max_l, padding='post')\n    return padded\n\ndef display_results(reviews,preds):\n  pos = 0\n  n_preds = preds.argmax(axis=1)\n  for i in range(0, len(reviews)):\n    print(reviews[i])\n    if n_preds[i] == 0:\n        print('*negative*')\n        print(' Confidence: '+ str(round((preds[i].max() * 100),2))+'%')\n    else:\n      print('*positive*')\n      print(' Confidence: '+ str(round((preds[i].max() * 100),2))+'%')\n      pos += 1\n\n    print('------------------------------------')\n\n  pos_percent = (pos/len(reviews))*100\n  print('#Total reviews: ', len(reviews))\n  print('#Positive reviews: ', pos)\n  print(\"#Total positives:\", round(pos_percent,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predictions\nuser_reviews = [\"My family and I normally do not watch local movies for the simple reason that they are poorly made, they lack the depth, and just not worth our time.<br /><br />The trailer of \\\"Nasaan ka man\\\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon. The movie exceeded our expectations. The cinematography was very good, the story beautiful and the acting awesome. Jericho Rosales was really very good, so's Claudine Barretto. The fact that I despised Diether Ocampo proves he was effective at his role. I have never been this touched, moved and affected by a local movie before. Imagine a cynic like me dabbing my eyes at the end of the movie? Congratulations to Star Cinema!! Way to go, Jericho and Claudine!!\",\n                \"Believe it or not, this was at one time the worst movie I had ever seen. Since that time, I have seen many more movies that are worse (how is it possible??) Therefore, to be fair, I had to give this movie a 2 out of 10. But it was a tough call.\",\n                \"This was the worst movie in my entire life.\",\n                \"I will recommend this movie to the entire universe.\",\n                \"Even aside from its value as pure entertainment, this movie can serve as a primer to young adults about the tensions in the Middle East.\",\n                \"Derivative, uneven, clumsy and absurdly sexist.\",\n                \"I've seen it twice and it's even better the second time.\",\n                \"This was a very good movie. I really enjoyed it.\",\n                \"Horrible waste of time. I do not recommend this movie to anyone\",\n                \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",\n                \"At the bottom end of the apocalypse movie scale is this piece of pish called 'The Final Executioner'.. at least where I come from. A bloke is trained by an ex-cop to seek vengeance on those that killed his woman and friends in cold blood.. and that's about it. Lots of fake explosions and repetitive shootings ensue. Has one of the weirdest array of costumes I've seen in a film for a while, and a massive fortress which is apparently only run by 7 people. GREAT job on the dubbing too guys(!) Best moment: when our hero loses a swordfight and is about to be skewered through the neck, he just gets out his gun and BANG! Why not do that earlier? It's a mystery. As is why anyone would want to sit through this in the first place. I'm still puzzling over that one myself now.. 2/10\",\n                \"A boy and a girl is chased by a local warrior because the boy killed (by accident) the warriors father (or whoever he was). And they travel through the nature of Africa's most ruff areas.<br /><br />The acting in this movie isn't that good (except for that elephant kid). But it's a very good adventure and it's not very censored, there is some blood, flesh and nudity (which lighten up the movie a bit).<br /><br />I give this movie a 7 because of it's picture of the African nature and it's action.\",\n                \"Hell yeah!!! For someone who always sleeps while watching a movie, I can I liked it. Though I didnt understand language and relied on the subtitles. I learnt a new culture.\",\n                \"I would watch this movie again.\"] \n\n\n#Clean reviews\nprocessed_user_reviews = clean_reviews(user_reviews)\n# print(processed_user_reviews)\n\n#Tokenize data\nuser_sequences = tokenize_reviews(processed_user_reviews, tokenizer)\n# print(user_sequences)\n\n#Pad the reviews\npadded_user_reviews = pad_reviews(user_sequences, max_length)\n# print(padded_user_reviews)\n\n#Predict sentiment\npreds = model.predict(padded_user_reviews)\ndisplay_results(user_reviews, preds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}