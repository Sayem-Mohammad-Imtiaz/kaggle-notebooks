{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Importing the Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport re\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Data Wrangling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set = pd.read_csv('/kaggle/input/pfizer-vaccine-tweets/vaccination_tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.DataFrame({\"user_created\": data_set['user_created'], \"user_followers\": data_set['user_followers'],\n                       \"user_friends\": data_set['user_friends'], \"user_favourites\": data_set['user_favourites'], \n                       \"user_verified\": data_set['user_verified'], \"date\": data_set['date'],\n                       \"text\": data_set['text'], \"source\": data_set['source'],\n                       \"retweets\": data_set['retweets'], \"favorites\": data_set['favorites'], \n                       \"text_length\": data_set['text'].apply(len)})\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Exploratory Data Analysis**"},{"metadata":{},"cell_type":"markdown","source":"### Source vs. Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\nsns.countplot(y = dataset['source'], data = data_set)\nplt.title(\"Source vs Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tweets per day"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\ndataset['date'] = pd.to_datetime(dataset['date']).dt.date\n\nplt.figure(figsize = (12, 8))\ndataset.sort_values('date', inplace = True)\nsns.countplot(y = dataset['date'], data = dataset)\nplt.title(\"Tweets per day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Heatmap (Correlations)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ndataset_cor = dataset[['user_followers', 'user_friends', 'user_favourites', 'retweets', 'favorites', 'text_length']].corr()\naxes = sns.heatmap(dataset_cor, linecolor = 'white', linewidths = 1, cmap = 'coolwarm', annot = True)\naxes.set_title('Heatmap')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top mentions (@)"},{"metadata":{"trusted":true},"cell_type":"code","source":"mention = []\nfor i in range(0, len(dataset)):\n    review1 = dataset['text'][i]\n    review1 = re.findall('@[a-zA-Z0-9_]+', review1)\n    for j in review1:\n        mention.append(j)\n        \nmention","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(mention)\ndf = df[0].value_counts()\n\nfrom nltk.probability import FreqDist\nfreqdist = FreqDist()\n\nfor words in df:\n    freqdist[words] =+1 \n    \nfreqdist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[:20,]\nplt.figure(figsize = (12, 8))\nsns.barplot(df.values, df.index, alpha = 0.8)\nplt.title(\"Top @(mention)\")\nplt.ylabel(\"Account Name\")\nplt.xlabel(\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top Hashtags used (#)"},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtags = []\nfor i in range(0, len(dataset)):\n    review1 = dataset['text'][i]\n    review1 = re.findall('#[a-zA-Z0-9_]+', review1)\n    for j in review1:\n        hashtags.append(j)\n        \nhashtags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(hashtags)\ndf1 = df1[0].value_counts()\n\nfrom nltk.probability import FreqDist\nfreqdist1 = FreqDist()\n\nfor words in df1:\n    freqdist1[words] +=1\n\nfreqdist1    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1[:20, ]\nplt.figure(figsize = (12, 8))\nsns.barplot(df1.values, df1.index, alpha = 0.8)\nplt.title(\"Top Hashtag Used\")\nplt.ylabel(\"Hashtags (#)\")\nplt.xlabel(\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Data Cleaning**\n\n### Applying Sentiments on the tweets\n\nTextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\ndataset['sentiment'] = ' '\ndataset['polarity'] = None\n\nfor i,j in enumerate(dataset.text):\n    blob = TextBlob(j)\n    dataset['polarity'][i] = blob.sentiment.polarity\n    if blob.sentiment.polarity >= 0 :\n        dataset['sentiment'][i] = 'positive'\n    else:\n        dataset['sentiment'][i] = 'negative'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\nsns.countplot(x = dataset.sentiment, data = dataset)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tweets per day based on the Sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\nsns.countplot(y = dataset['date'], hue = dataset['sentiment'], data = dataset)\nplt.title(\"Tweets per Day based on the Sentiment\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"senti = pd.get_dummies(dataset['sentiment'], drop_first = True)\ndataset = pd.concat([dataset, senti], axis = 1)\ndataset = dataset.drop('sentiment', axis = 1)\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame({\"Text\": dataset['text'], \"Sentiment\": dataset['positive']})\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Corpus of Words (Clean text)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nwords = []\nfor i in range(0, len(data)):\n    review2 = data['Text'][i]\n    review2 = re.sub('\"\"', ' ', review2)\n    review2 = re.sub('https://[a-zA-Z0-9./]+', ' ',review2)\n    review2 = re.sub('@[a-zA-Z0-9._]+', ' ', review2)\n    review2 = re.sub('#[a-zA-Z0-9._]+', ' ', review2)\n    # review2 = re.sub('\\n', ' ', review2)\n    review2 = re.sub('[^a-zA-Z]', ' ', review2)\n    review2 = review2.lower()\n    review2 = review2.split()\n    review2 = [word for word in review2 if not word in stopwords.words('english')]\n    for j in review2:\n        words.append(j)\n    review2 = [ps.stem(word) for word in review2]\n    review2 = ' '.join(review2)\n    corpus.append(review2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top words used"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame(words)\ndf2 = df2[0].value_counts()\n\nfrom nltk.probability import FreqDist\nfreqdist2 = FreqDist()\n\nfor words in df2:\n    freqdist2[words] +=1\n\nfreqdist2    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2[:20, ]\nplt.figure(figsize = (12, 8))\nsns.barplot(df2.values, df2.index, alpha = 0.8)\nplt.title(\"Top Words Used\", fontdict = {'fontsize' : 15})\nplt.ylabel(\"Words\")\nplt.xlabel(\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_final = pd.DataFrame(corpus, columns = ['Content'])  # To convert a List into a DataFrame\ndataset_final = pd.concat([data['Sentiment'], dataset_final], axis = 1)\ndataset_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Applying Various Classification Models**"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Data into Training and Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset_final.Content, dataset_final.Sentiment, test_size = 0.25)\n\nX_1 = X_train.reset_index(drop = True)\nX_2 = X_test.reset_index(drop = True)\n\nY_1 = y_train.reset_index(drop = True)\nY_2 = y_test.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Tfidf Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(X_1)\nX_test = vectorizer.transform(X_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 300)\nlr.fit(X_train, y_train)\n\ny_pred_log = lr.predict(X_test)\n\nprint(\"Accuracy of Logistic Regression is: {}%\".format(accuracy_score(y_test, y_pred_log) * 100))\nprint(\"Confusion Matrix of Logistic Regression is: \\n{}\".format(confusion_matrix(y_test, y_pred_log)))\nprint(\"{}\".format(classification_report(y_test, y_pred_log)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Classifier (SVC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf')\nsvc.fit(X_train, y_train)\n\ny_pred_svc = svc.predict(X_test)\n\nprint(\"Accuracy of Support Vector Classifier is: {}%\".format(accuracy_score(y_test, y_pred_svc) * 100))\nprint(\"Confusion Matrix of Support Vector Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_svc)))\nprint(\"{}\".format(classification_report(y_test, y_pred_svc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n\ny_pred_mnb = mnb.predict(X_test)\n\nprint(\"Accuracy of MultinomialNB is: {}%\".format(accuracy_score(y_test, y_pred_mnb) * 100))\nprint(\"Confusion Matrix of MultinomialNB is: \\n{}\".format(confusion_matrix(y_test, y_pred_mnb)))\nprint(\"{}\".format(classification_report(y_test, y_pred_mnb)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'entropy')\ndt.fit(X_train, y_train)\n\ny_pred_dt = dt.predict(X_test)\n\nprint(\"Accuracy of Decision tree Classifier is: {}%\".format(accuracy_score(y_test, y_pred_dt) * 100))\nprint(\"Confusion Matrix of Decision tree Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_dt)))\nprint(\"{}\".format(classification_report(y_test, y_pred_dt)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 300, criterion = 'entropy')\nrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\n\nprint(\"Accuracy of Random Forest Classifier is: {}%\".format(accuracy_score(y_test, y_pred_rf) * 100))\nprint(\"Confusion Matrix of Random Forest Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_rf)))\nprint(\"{}\".format(classification_report(y_test, y_pred_rf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGD Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\n\ny_pred_sgd = sgd.predict(X_test)\n\nprint(\"Accuracy of Sochastic Gradient Descent Classifier is: {}%\".format(accuracy_score(y_test, y_pred_sgd) * 100))\nprint(\"Confusion Matrix of Sochastic Gradient Descent Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_sgd)))\nprint(\"{}\".format(classification_report(y_test, y_pred_sgd)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top Favorited Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"fav = dataset[['favorites','text']].sort_values('favorites',ascending = False)[:5].reset_index()\nprint(\"**Top 5 most Favourited tweets:**\\n\")\nfor j in range(0, 5):\n    print(j,'.', fav['text'][j],'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most Retweeted Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"fav = dataset[['retweets','text']].sort_values('retweets',ascending = False)[:5].reset_index()\nprint(\"**Top 5 most Favourited tweets:**\\n\")\nfor j in range(0, 5):\n    print(j,'.', fav['text'][j],'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}