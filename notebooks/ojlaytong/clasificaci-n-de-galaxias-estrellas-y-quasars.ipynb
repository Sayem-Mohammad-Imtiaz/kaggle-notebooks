{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementación de modelos ANN para clasificación de objetos astrofísicos identificados por el SDSS\n\n## ( Galaxias, Estrellas y Quasars)","metadata":{}},{"cell_type":"markdown","source":"Para dar solución a una máquina que logre clsificar las imágenes del Sloan Digital Sky Survey DR14, se realizaun modelo perceptron multicapa, se describe paso por paso cada una de sus aspectos más importantes.","metadata":{}},{"cell_type":"code","source":"#import Data\nimport pandas as pd\ndata_f1=pd.read_csv('/kaggle/input/sloan-digital-sky-survey/Skyserver_SQL2_27_2018 6_51_39 PM.csv')\ndata_f1=data_f1.drop(['rerun'], axis=1)\ndata_f1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim=display(data_f1.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El target es la variable class, la cual es de tipo categorica y tiene como niveles:","metadata":{}},{"cell_type":"code","source":"label = data_f1['class']   #Accediendo a una columna\nlabel.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con el fin de organizar nuestros datos establecemos que:","metadata":{}},{"cell_type":"code","source":"data_f1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estadísticas de las columnas númericas\ndata_f1.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = (data_f1.dtypes == 'float') | (data_f1.dtypes == 'int64')\nnum","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = []\nfor c in num.index:\n    if num[c] == True:\n        num_cols.append(c)                #.append  agrega un ítem al final de la lista, en este caso agrega c a num_cols\n        \ndisplay(num_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_f1_num = data_f1[num_cols]               #genera busqueda de \ndata_f1_num.head()\n\n# Estadísticas de las columnas númericas\ndata_f1_num.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \ndata_f1_num['dec'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_f1_num['u'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_f1_num['g'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.catplot(x=\"class\", kind=\"count\", palette=\"ch:.25\", data=data_f1);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"class\", y=\"u\", kind=\"box\", data=data_f1);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Función lapply\n\nLa función lapply permite de alguna manera idnetificar los datos nulos del DataFrame","metadata":{}},{"cell_type":"code","source":"data_f1_num.notnull().apply(pd.Series.value_counts) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Se puede observar que ninguna label presenta datos faltantes, ahora bien miremos si hay valores 0.\n* En lo que corresponde a valores nulos es decir 0, se tiene quer en la variable redshift existen 19 ceros, quizas este puede ser un problema en el momento de generar el modelo.","metadata":{}},{"cell_type":"code","source":"(data_f1 != 0).apply(pd.Series.value_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z=pd.concat([data_f1_num,label],axis=1)            #concatenando vas features con la variable respuesta.\nz.head()\nsns.pairplot(z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_f1_num.iloc[0:5:, 0:5]\ndisplay(type(data_f1_num))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.pairplot(estrellas_num)\nimport seaborn as sb\nsb.pairplot(data_f1_num,diag_kind='kde')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_dat = data_f1_num.corr()\ncorrelation_dat.style.background_gradient(cmap='coolwarm', axis=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dividiendo los datos de entrenamiento y de validación","metadata":{}},{"cell_type":"code","source":"X = data_f1.iloc[:, [1,2,3,4,5,6,7,13]]\nY = data_f1.iloc[:, [12]]\nY = Y.replace({\"GALAXY\":0, \"STAR\":1,\"QSO\":2})\nprint(X.head())\n\n\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ny = to_categorical(Y)\nX_std = StandardScaler().fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_std,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train_test_split es una función en la selección del modelo de Sklearn para dividir matrices de datos en dos subconjuntos : datos para entrenamiento y datos para prueba. No obstante, train_test_split hará particiones aleatorias para los dos subconjuntos. Esta función toma como parámetros el conjunto de datos X_std y las etiquetas y.\n\ntrain_size es el otro parámetro y establece el tamaño del conjunto de datos de entrenamiento, en este caso se usa 0.2 el cual hace referencia a 80% para test y 20% para prueba.","metadata":{}},{"cell_type":"code","source":"X_trainT , X_test ,Y_trainT, Y_test = train_test_split(X_std , y , test_size=0.2)\nX_train , X_test10 ,Y_train, Y_test10 = train_test_split(X_trainT , Y_trainT , test_size=0.125)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# Modelo perceptron multicapa \n---\n\n\nConsiderando que las redes neuronales son grupo de neuronas distribuidas en capas y conectadas por sus pesos (salidas con un valor asignado a la información que recibieron) se procede a describir el modelo ANN por medio de:\n\n#### Primera capa\n\nCapa densa de 5 neuronas y entrada de tamaño 8, con función de activación ReLu (Unidad rectificada lineal). Además un dropout de 0.45.\n\n#### Segunda capa\n\nUna capa densa con 256 neuronas y función de activación ReLu (Unidad rectificada lineal) y dropout de 0.45.\n\n#### Tercera capa\n\nUna capa densa con 3 neuronas y función de activación Softmax.\nCaba aclarar que los lotes son de tamaño 128.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.utils import to_categorical, plot_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = 8\nnum_labels = 3\nbatch_size = 128\ndropout = 0.45\n\nfrom tensorflow.keras.regularizers import l2\n\nmodel = Sequential()\n\nmodel.add(Dense(5, input_dim=input_size))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\n\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\n\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# Compilación\n---","metadata":{}},{"cell_type":"markdown","source":"Un algoritmo de deep learning demora tanto en entrenar (entre otras cosas) porque eventualmente encuentra muchos mínimos locales en la superficie de error.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# Entremiento del modelo\n---","metadata":{}},{"cell_type":"markdown","source":"Aquí se caracterizará el checkpoint para poder guardar el modelo y después poder usarlo para hacer predicciones.","metadata":{}},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')>0.99):\n            print(\"\\nReached 0.99 accuracy so cancelling training!\")\n            self.model.stop_training = True\n\n# crea una instancia de clase\naccu_callback = myCallback()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Directory where the checkpoints will be saved\nimport os \ncheckpoint_dir = './training_checkpoints_mnist_mlp'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"mnist_mlp_ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True, \n    monitor='val_accuracy', mode='max',\n    save_best_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se procede a realizar el entrenamiento del modelo ANN que fue caracterizado en lineas anteriores con epoch de tamaño 20 y un tamaño de validación de 0.2.","metadata":{}},{"cell_type":"code","source":"epochs = 50\nvalidation_size =0.2\n\nhistory = model.fit(X_train, Y_train, \n                    epochs=epochs, \n                    batch_size=batch_size, \n                    validation_split=validation_size,\n                    callbacks=[accu_callback,checkpoint_callback],\n                    verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\nEvaluación del modelo ANN\n---","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nn_row = 1\nn_col = 2\nfig, ax = plt.subplots(n_row, n_col, sharex = False, sharey = False, figsize=(12,6))\n\n\nax[0].plot(epochs, acc, 'r', label='Training accuracy', color = 'green')\nax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')\nax[0].legend(fontsize=12,loc=0)\nax[0].set_title('Training and Validation Accuracy',fontsize=16)\nax[0].set_ylabel('measure',fontsize=14)\nax[0].set_xlabel('epoch', fontsize = 14)\nax[0].set_xlim([1, len(acc)])\n\nax[1].plot(epochs, loss, 'r', label='Training Loss', color = 'green')\nax[1].plot(epochs, val_loss, 'b', label='Validation Loss')\nax[1].legend(fontsize=12)\nax[1].set_title('Training and Validation Loss',fontsize=16)\nax[1].set_ylabel('measure',fontsize=14)\nax[1].set_xlabel('epoch', fontsize = 14)\nax[1].set_xlim([1, len(acc)])\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}