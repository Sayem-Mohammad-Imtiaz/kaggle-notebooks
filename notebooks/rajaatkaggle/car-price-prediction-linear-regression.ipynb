{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Problem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. \n\n \n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car\n\nHow well those variables describe the price of a car\n\n### Data Set\n\nhttps://www.kaggle.com/saivivekreddy00/car-price\n\n### Business Goal \n\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. \n\n\n### Model Evaluation\n\nCalculate the R-squared score on the test set.\n\n\n### Steps Followed\n\n##### 1. Read and Understand Data\n##### 2. Visualize the Data\n##### 3. Data Preparation\n##### 4. Building the model\n##### 5. Residual Analysis of the train data\n##### 6. Making Predictions\n##### 7. Conclusion\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us start by importing pandas and numpy !"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Read and Understand Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"car = pd.read_csv('../input/car-price-prediction/CarPrice_Assignment.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basic Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"car.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 205 data items in the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"car.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are no missing values in the data set\n\nFrom the dictionary file ( please see \"Data Dictionary - carprices.xlsx\" ), symboling seems to be an Ordinal categorical type."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(car['symboling'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us drop the car_ID variable which looks useless to analysis.\ncar_original = car.copy()\ncar = car.drop(\"car_ID\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Symboling to object type from int64 as it is actually a categorical variable.\n#car['symboling'] = car['symboling'].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the mean, median, max etc , looks like the distribution of numerical attributes don't have severe outliers."},{"metadata":{},"cell_type":"markdown","source":"##### Convert text categorical variables to lower string"},{"metadata":{"trusted":true},"cell_type":"code","source":"car_categorical = car.select_dtypes(include=['object'])\ncar_categorical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.update(car.select_dtypes(include='object')\\\n    .apply(lambda x: x.astype(str).str.lower()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(car['CarName'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#car['CarName'].loc[car[\"CarName\"].str.startswith('N')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a variable named CarName which is comprised of two parts - \nthe first word is the name of 'car company' and the second is the 'car model'.\nFor example, chevrolet impala has 'chevrolet' as the car company name and \n'impala' as the car model name. \nLet's consider only company name as the independent variable for model building. "},{"metadata":{"trusted":true},"cell_type":"code","source":"car['CarName'] = car['CarName'].apply(lambda x: x.split()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#car.CarName.value_counts()\nsorted(dict(car.CarName.value_counts()).items())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following car companies' names are kind of duplicated - mazda as maxda, porsche as porcshce, toyota as toyouta, volkswagen as vokswagen. We need to correct these."},{"metadata":{"trusted":true},"cell_type":"code","source":"car['CarName'] = car['CarName'].apply(lambda x: x.replace('maxda', 'mazda'))\ncar['CarName'] = car['CarName'].apply(lambda x: x.replace('porcshce', 'porsche'))\ncar['CarName'] = car['CarName'].apply(lambda x: x.replace('toyouta', 'toyota'))\n#car['CarName'] = car['CarName'].apply(lambda x: x.replace('vokswagen', 'volkswagen'))\n\ncar.loc[(car['CarName'] == \"vw\") | (car['CarName'] == \"vokswagen\"), 'CarName'] = 'volkswagen'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(dict(car.CarName.value_counts()).items())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Visualize the Data"},{"metadata":{},"cell_type":"markdown","source":"#### Numerical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%matplotlib inline \nimport matplotlib.pyplot as plt\n\ncar.hist(bins=50, figsize=(20,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations :\n    1. enginesize, compressionratio, horsepower, peakrpm, citympg have some extreme outliers.\n    2. The attributes are different in scale.\n    3. The target variable price is skewed to the right and has extreme outliers around 40000.\n    \n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"car.price.quantile([.25, .5, .75, .90, .92, .95, .97, .99]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Try removing outliers from the dataset with respect to price"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(car[car.price <= car.price.quantile(0.95)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car = car[car.price <= car.price.quantile(.95)]\ncar.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_numeric = car.select_dtypes(include=['float64', 'int64'])\ncar_numeric.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Let us see the pair plots between price (dependent variable) and other numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nattributes1 = ['price', 'symboling', 'wheelbase','carlength','carwidth','carheight','curbweight', 'enginesize']\nsns.pairplot(car[attributes1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Except for carheight and symboling, all the other attributes seem to have good linear correlation with price.\nLet us examine the remaining numerical attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes2 = ['price','boreratio','stroke','compressionratio','horsepower','peakrpm','citympg','highwaympg']\nsns.pairplot(car[attributes2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### price seems to have \nGood positive correlation with horsepower. <br>\nSome positive correlation with boreratio. <br>\nGood negative correlation with citympg and highwaympg. <br>\nNot good correlation with stroke, peakrpm and compressionratio.\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"##### Attributes that have good correlation with each other.\nhighwaympg and citympg have good positive correlation <br>\nBoth highwaympg and citympg have negative correlation with horsepower  "},{"metadata":{},"cell_type":"markdown","source":"##### From the above plots, we see that the problem is well suited for Linear Regression modelling"},{"metadata":{},"cell_type":"markdown","source":"Let's also look at the correlation matrix on how other numerical features are correlated with the target variable price."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = car.corr()\ncorr_matrix['price'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From above, we can see \nthe following features have highest positive correlation with price -<br> \ncurbweight, carwidth, enginesize, horsepower and carlength. <br><br>\n\nthe following features have highest negative correlation with price -<br> \ncitympg (City mileage) and highwaympg (Highway mileage) <br>\n\nSo we get a feeling like prices are higher for cars with lower mileage and vice versa. Makes sense because usually city cars have higher mileage and lower prices..right ?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure size\nplt.figure(figsize=(16,8))\n\n# Heatmap\nsns.heatmap(corr_matrix, cmap=\"YlGnBu\", annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Above heatmap shows a lot of correlation between a number of features. \nSo we can easily guess that not all the features will be needed for a successful model and there is good scope for feature elimination. "},{"metadata":{},"cell_type":"markdown","source":"#### Now let us look about Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"car_categorical = car.select_dtypes(include=['object'])\ncar_categorical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n#plt.subplot(1,3,1)\n#sns.boxplot(x = 'symboling', y = 'price', data = car)\nplt.subplot(1,3,1)\nsns.boxplot(x = 'CarName', y = 'price', data = car)\nplt.subplot(1,3,2)\nsns.boxplot(x = 'fueltype', y = 'price', data = car)\nplt.subplot(1,3,3)\nsns.boxplot(x = 'aspiration', y = 'price', data = car)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Diesel cars look generally costlier than gas."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us take a closer look at Car Companies\n\nplt.figure(figsize=(20, 20))\nsns.boxplot(x = 'CarName', y = 'price', data = car)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"bmw, buick, porsche and volvo cars look costlier than others"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nplt.subplot(1,3,1)\nsns.boxplot(x = 'doornumber', y = 'price', data = car)\nplt.subplot(1,3,2)\nsns.boxplot(x = 'carbody', y = 'price', data = car)\nplt.subplot(1,3,3)\nsns.boxplot(x = 'drivewheel', y = 'price', data = car)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cars with four doors look costlier, likewise convertible and wagon cars"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nplt.subplot(1,4,1)\nsns.boxplot(x = 'enginelocation', y = 'price', data = car)\nplt.subplot(1,4,2)\nsns.boxplot(x = 'enginetype', y = 'price', data = car)\nplt.subplot(1,4,3)\nsns.boxplot(x = 'cylindernumber', y = 'price', data = car)\nplt.subplot(1,4,4)\nsns.boxplot(x = 'fuelsystem', y = 'price', data = car)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be only front enginelocation type in the data. Let's confirm it and drop the feature if it is the case, because it is not going to add value to modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"car.enginelocation.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car = car.drop('enginelocation', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"##### Apply one-hot encoding to the categorical variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"car_categorical = car.select_dtypes(include=['object'])\ncar_categorical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_categorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#category_list = car_categorical.columns\n#print(category_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummy1 = pd.get_dummies(car_categorical[category_list], drop_first=True)\ndummy1 = pd.get_dummies(car_categorical, drop_first=True)\ndummy1                                       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the results to the original housing dataframe\n#housing = pd.concat([housing, status], axis = 1)\ncar = pd.concat([car, dummy1], axis = 1)\ncar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop categorical variables as we have created the dummies for it\n#car.drop(category_list, axis = 1, inplace = True)\ncar.drop(car_categorical.columns, axis = 1, inplace = True)\ncar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the Data into Training and Testing Sets and rescaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\n\ntrain_set, test_set = train_test_split(car, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(car.shape)\ntrain_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rescaling the numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_vars = car_numeric.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set[numerical_vars] = scaler.fit_transform(train_set[numerical_vars])\ntrain_set.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dividing into X and Y sets for the model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_set.pop('price')\nX_train = train_set\n#X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Building the model"},{"metadata":{},"cell_type":"markdown","source":"##### Let us first use RFE to select the best 13 features ( coarse tuning ) as we saw a significant correlation between a number of features. Then we will go with manual removal of features one by one ( fine tuning )."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE \nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\n\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RFE(lr, 13)\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Features selected by RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Features NOT selected by RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Building model using statsmodel, for the detailed statistics. We will use only the RFE picked features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_train dataframe with RFE selected variables\nX_train_rfe = X_train[cols]\nX_train_rfe.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable as statsmodels will not add it by default\nimport statsmodels.api as sm\nX_train_lm = sm.add_constant(X_train_rfe)\nX_train_lm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Fit Statsmodels model\nlm = sm.OLS(y_train, X_train_lm).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Print the model summary\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\nR-squared:                       0.919\nAdj. R-squared:                  0.911\nProb (F-statistic):           1.39e-59\n\nBoth R2 and Adj.R2 look very good. Note the Prob (F-statistic). It looks good with a very low value ( < 0.05 ) and shows all the predictor variables together as a whole are significant.\n\n##### cylindernumber_three seems to have high p-value of 0.191 which says this feature is insignificant.\nLet's go and check VIF of the variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm.drop('const',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### VIF of all the features are in acceptable range i.e less than 5."},{"metadata":{},"cell_type":"markdown","source":"##### Let's drop cylindernumber_three which has a high p-value "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop('cylindernumber_three',axis=1)\n#X = X.drop('enginetype_dohcv', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Again build a statsmodel fit on the reduced set of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lm = sm.add_constant(X)\nX_train_lm.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(y_train, X_train_lm).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\nR-squared:                       0.918\nAdj. R-squared:                  0.910\nProb (F-statistic):           2.97e-60\n\nA very slight drop in R2 and Adj.R2 from the previous model, but still both look very good. \nAlso there is a significant drop in Prob (F-statistic) which is very good.\n\np-values of all the features are < 0.05 which is all good.\n\nLet us check VIF too"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm.drop('const',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\n\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### VIF of all the features are in acceptable range i.e less than 5."},{"metadata":{},"cell_type":"markdown","source":"#### p-values and VIF are all looking fine within acceptable ranges. So we can try this model on test set."},{"metadata":{},"cell_type":"markdown","source":"### 5. Residual Analysis of the train data"},{"metadata":{},"cell_type":"markdown","source":"##### We have to predict and check if the error terms are normally distributed - This is one of the assumptions of Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_price = lm.predict(X_train_lm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Yes ! The error terms are in a nice normal distribution, though not perfect."},{"metadata":{},"cell_type":"markdown","source":"### 6. Making Predictions"},{"metadata":{},"cell_type":"markdown","source":"##### Let us predict test set using the model. \n\nFirst we have to apply the standard scaler (that we fit to the train set ) to the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set[numerical_vars] = scaler.transform(test_set[numerical_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dividing into X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = test_set.pop('price')\nX_test = test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Use only the significant features that we narrowed down finally when we built the model above"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test_new dataframe by dropping unnecessary features from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_test_pred = lm.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_test_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test, y_test_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_test_pred', fontsize=16)                          # Y-label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nr2_score(y_true=y_test, y_pred=y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### There seems to be significant gap between R2 of train set (0.918) and test set (0.82).  This might be a overfit on train set.\n##### Let us see if there is high correlation between the selected features and try to drop few more features.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure size\nplt.figure(figsize=(8,5))\n\n# Heatmap\nsns.heatmap(car[X_train_new.columns].corr(), cmap=\"YlGnBu\", annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Try dropping further from the predictor variables with higher correlation. In this case, we will drop  drivewheel_rwd which seems to have high collinearity with curbweight."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop drivewheel_rwd which had  VIF > 2 and also high correlation with curbweight\nX = X.drop('drivewheel_rwd',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build model again with reduced feature set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lm2 = sm.add_constant(X)\nX_train_lm2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm2 = sm.OLS(y_train, X_train_lm2).fit()\nprint(lm2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n\nR-squared:                       0.895\nAdj. R-squared:                  0.886\n\n##### R2 and Adj-R2 has reduced from previous model. But still look very good. \n\nCarName_saab and carbody_hardtop have high p-values.\n\nLet us check the VIF values"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm2.drop('const',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### VIF values are looking fine. Let us drop CarName_saab which has high p-value"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop('CarName_saab', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build model again with reduced feature set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lm2 = sm.add_constant(X)\nX_train_lm2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm2 = sm.OLS(y_train, X_train_lm2).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n\ncarbody_hardtop has high p-value of 0.217.\n\nLet us check VIF again"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm2.drop('const',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### VIF values are looking fine. Let us drop carbody_hardtop which has high p-value"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop('carbody_hardtop', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build model again with reduced feature set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lm2 = sm.add_constant(X)\nX_train_lm2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm2 = sm.OLS(y_train, X_train_lm2).fit()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lm2.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n\nR-squared: 0.893\nAdj R-squared: 0.885\n    \n\n##### R2 and Adj-R2 look very good and p-values are all within control.\nLet us check the VIF values.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm2.drop('const',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### VIF values are also fine < 5"},{"metadata":{},"cell_type":"markdown","source":"##### Let us check the correlation heatmap again for the reduced feature set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure size\nplt.figure(figsize=(8,5))\n\n# Heatmap\nsns.heatmap(car[X_train_new.columns].corr(), cmap=\"YlGnBu\", annot=True)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The correlations are also looking fine with no high correlations. So let us predict test set using this model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test_new dataframe by dropping unnecessary features from X_test\nX_test_new = X_test[X_train_new.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_test_pred = lm2.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_test_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test, y_test_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_test_pred', fontsize=16)                          # Y-label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(y_true=y_test, y_pred=y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"##### Test R2 looks good at 0.87 which is much closer to R2 on train set that is 0.89."},{"metadata":{},"cell_type":"markdown","source":"##### So, we built a good model which performs very well both on train and test sets, with the following features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}