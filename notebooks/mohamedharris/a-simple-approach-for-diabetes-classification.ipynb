{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A Beginner's approach to Diabetes Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hello Viewers!\n\nThanks for taking a moment to stop by and looking at my Kernel. This notebook is aimed at drafting a simple approach to solve the diabetes classification problem.\n\nWithout talking much, let's get into the solution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## But, Where's the data?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here it is...\n\nWe have a dataset on health attributes of several patients and we need to classify whether the patient has diabetes or not.\n\nPeople do talk about being independant and dependant. What are they?\n\nSimple!\n\n* **Independant Features** : These are the predictor variables using which you predict the outcome.\n* **Dependant Feature** : This is the target variable which we are going to predict.\n\nOkay, what independant features do we have for now?\n* Pregnancies\n* Glucose\n* Blood Pressure\n* Skin Thickess\n* Insulin\n* BMI\n* Diabetes Pedigree Function\n* Age\n\nBased on these attributes, we will predict the target variable.\n* Outcome","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries and Dataset","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-deep')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have imported the required libraries, let us grab the dataset. I'm storing a copy of it, as we may require it in the future.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\noriginal = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check Point - Quick Inspection of the Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's run some simple scripts to check the shape, info and summary statistics of the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data has', df.shape[0], 'rows and', df.shape[1], 'columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do you see something abnormal? Just look at the minimum value of each feature.\n\n* Pregnancies : 0\n    * This is acceptable as this dataset has the mix of genders.\n* Glucose\n    * Can Glucose level be zero in any case? Something to ponder over.\n* Blood Pressure\n    * Even patients with low blood pressure would never have BP as 0.\n* Skin Thickness\n    * Can skin thickness be zero? I do not think so, but we'll note this for further investigation.\n* Insulin\n    * Insulin could be very low in fasting conditions. But zero? Okay, hold it for now!\n* BMI\n    * Zero BMI? Impossible!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The summary statistics is the best measure of getting a quick insight about the accuracy of the data. The above abnormalities could be either erratic or rare occurences. We'll check the dataset further during our EDA process.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msna\nmsna.matrix(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is evident from the above figure that the dataset does not have any missing values.\n\n*Oh! You noticed that from the df.info() method itself? Cool! You've got an eagle's eye*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Univariate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Target Variable","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check the composition of the target varible. \n\nHere 0 corresponds to being normal and 1 to diabetic. We'll use the normalize parameter to tell the value_counts() method that we need the percentage rather than the actual numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outcome'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"65% of the people in the dataset are normal while the rest are diabetic.\n\nIs this an imbalanced dataset? I would hesitate to say so, we still have 35% of the class of interest. So let's go ahead.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### PDF and ECDF - Predictor Variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Confused by the terms?\n\nThese are the statistical methods to check the distribution of the data.\n\n* **Probability Distribution Function**:\nProbability of getting a value, if it is randomly chosen from the data.\n\n* **Empirical Cumulative Distribution Function**:\nProbability of getting less than a value, if it is randomly chosen from the data.\n\nWe'll check the plots to further understand this definitions.\n\nShall we practise writing simple functions as part of our analysis?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18, 7)\n\ndef univariate_plot(x):\n    plt.subplot(121)\n    sns.distplot(x, color = 'seagreen')\n    plt.title('Probability Distribution Function', fontsize = 15)\n    plt.ylabel('Probability')\n    \n    n = len(x)\n    a = np.sort(x)\n    b = np.arange(1, 1 + n) / n\n    plt.subplot(122)\n    plt.plot(a, b, color = 'seagreen', marker = '.', linestyle = 'none')\n    mean_x = np.mean(x)\n    plt.axvline(mean_x, label = 'Mean', color = 'k')\n    skew = '               Skew : ' + str(round(x.skew(), 2))\n    plt.annotate(skew, xy = (mean_x, 0.5), fontsize = 16)\n    plt.legend()\n    plt.title('Empirical Cumulative Distribution Function', fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a simple function which gets a feature as an input argument and plot the PDF and ECDF of the feature.\n\nIn addition to this, we'll also use the function to calculate the skewness of the feature.\n\n* Skewness: Distortion present in the data. If the feature is not normally distributed (Gaussian), it may skew either towards the left or right. Most models would perform better with normally distributed data, so let's check how the features are distributed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Age feature is right skewed as you see the tail is long in the right end. How to cross verify this? The skewness factor is 1.13, which is positive. The positive skewness means that the feature is right skewed.\n\nFrom the ECDF graph, it is observed the probability of getting an observation with age less than the mean age is ~60%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['BMI'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['Pregnancies'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['Glucose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['BloodPressure'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['SkinThickness'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['Insulin'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_plot(df['DiabetesPedigreeFunction'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have checked the distribution of features, let's check the dataset to get answers for our questions we have framed by looking at the summary statistics.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The maximum number of pregnancies is 17. Let's check the dataset with observations having more than 10 pregnancies. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Pregnancies'] > 10, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The age of people who had more than 10 pregnancies seem to be either middle staged or old. Let's keep this feature as such.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['BMI'] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we slice the dataset with BMI values equal to zero, we also see zero values in neighboring features as well. These observations must be erratic.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Glucose'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['BloodPressure'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['SkinThickness'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Insulin'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index = df.loc[(df['BMI'] == 0) & (df['BloodPressure'] == 0) & (df['SkinThickness'] == 0) & (df['Insulin'] == 0), :].index\ndf.drop(drop_index, axis = 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've dropped the observations which have zero values in all the four features. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns.tolist():\n    print(i, '-', len(df.loc[df[i] == 0, :]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outcome'] = df['Outcome'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've converted the target variable to string type as this would help us in plotting doing the bivariate analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (17, 6)\n\ndef plot_box(x):\n    plt.subplot(121)\n    sns.boxplot(y = x, x = 'Outcome', data = df)\n    plt.title(x, fontsize = 16)\n    \n    plt.subplot(122)\n    sns.violinplot(y = x, x = 'Outcome', data = df)\n    plt.title(x, fontsize = 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('Pregnancies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('Insulin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('BMI')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('BloodPressure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('SkinThickness')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('DiabetesPedigreeFunction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box('Glucose')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have just completed the bivariate analysis, the common thing we have noticed that there were many outliers present in the features. We'll treat them in the subsequent sections.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Transformation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's first substitute the zero values in the features with np.NaN so that we can go ahead and simply impute the entries as the next step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['BMI'] = np.where(df['BMI'] == 0, np.nan, df['BMI'])\ndf['Glucose'] = np.where(df['Glucose'] == 0, np.nan, df['Glucose'])\ndf['BloodPressure'] = np.where(df['BloodPressure'] == 0, np.nan, df['BloodPressure'])\ndf['SkinThickness'] = np.where(df['SkinThickness'] == 0, np.nan, df['SkinThickness'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have converted them to null entries, let's impute them with fillna() method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['BMI'].fillna(27, inplace = True)\ndf['Glucose'].fillna(df['Glucose'].mean(), inplace = True)\ndf['BloodPressure'].fillna(df['BloodPressure'].mean(), inplace = True)\ndf['SkinThickness'].fillna(df['SkinThickness'].mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the summary statistics of the dataset again to verify whether the changes are applied.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll drop the entries which have abnormal entries. We'll have a threshold for these features and drop the observations which exceed them. Although dropping entries should not be the first option, I'm doing it as there are not many observations which we would lose by doing so.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['SkinThickness'] > 60]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df.loc[df['SkinThickness'] > 60].index, axis = 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['BMI'] > 55]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df.loc[df['BMI'] > 55].index, axis = 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have did some changes and they definitely would have influenced the skewness of the feature. We'll use the skew() method to check the skewness. Let's also write a function to plot the distribution of the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.select_dtypes(['int64', 'float64']).columns.tolist():\n    print(i, ':', df[i].skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def skew_visual():    \n    plt.rcParams['figure.figsize'] = (20, 8)\n\n    plt.subplot(241)\n    sns.distplot(df['Pregnancies'], color = 'k')\n    plt.title('PDF - Pregnancies')\n\n    plt.subplot(242)\n    sns.distplot(df['Glucose'], color = 'k')\n    plt.title('PDF - Glucose')\n\n    plt.subplot(243)\n    sns.distplot(df['BloodPressure'], color = 'k')\n    plt.title('PDF - BloodPressure')\n\n    plt.subplot(244)\n    sns.distplot(df['Insulin'], color = 'k')\n    plt.title('PDF - Insulin')\n\n    plt.subplot(245)\n    sns.distplot(df['SkinThickness'], color = 'k')\n    plt.title('PDF - SkinThickness')\n\n    plt.subplot(246)\n    sns.distplot(df['Age'], color = 'k')\n    plt.title('PDF - Age')\n\n    plt.subplot(247)\n    sns.distplot(df['DiabetesPedigreeFunction'], color = 'k')\n    plt.title('PDF - DiabetesPedigreeFunction')\n\n    plt.subplot(248)\n    sns.distplot(df['BMI'], color = 'k')\n    plt.title('PDF - BMI')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew_visual()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, it is obvious that there are four features which are skewed. Let's check different transformations to see whether they can be fixed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']:\n    print(i, ':', np.sqrt(df[i]).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']:\n    print(i, ':', np.log1p(df[i]).skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't we think it is better to have a dataframe with the feature names and the skewness factor for each of them. We can compare the skewness by appyling different transformations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Feature' : ['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction'],\n             'Actual' : [df[i].skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]],\n             'Squared' : [np.sqrt(df[i]).skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]],\n             'Cubed' : [(df[i] ** (1/3)).skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]],\n             'Logged' : [np.log1p(df[i]).skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_v1 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's save a copy of the dataset as we are going to transform the features. Let's create new features with the transformations applied and we can drop the redundant features later. We'll plot the distribution of the features with the new features to check h","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pregnancies_trans'] = np.sqrt(df['Pregnancies'])\ndf['Insulin_trans'] = np.log1p(df['Insulin'])\ndf['Age_trans'] = np.log1p(df['Age'])\ndf['DiabetesPedigreeFunction_trans'] = df['DiabetesPedigreeFunction'] ** (1/3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def skew_visual_trans():    \n    plt.rcParams['figure.figsize'] = (20, 8)\n\n    plt.subplot(241)\n    sns.distplot(df['Pregnancies_trans'], color = 'k')\n    plt.title('PDF - Pregnancies')\n\n    plt.subplot(242)\n    sns.distplot(df['Glucose'], color = 'k')\n    plt.title('PDF - Glucose')\n\n    plt.subplot(243)\n    sns.distplot(df['BloodPressure'], color = 'k')\n    plt.title('PDF - BloodPressure')\n\n    plt.subplot(244)\n    sns.distplot(df['Insulin_trans'], color = 'k')\n    plt.title('PDF - Insulin')\n\n    plt.subplot(245)\n    sns.distplot(df['SkinThickness'], color = 'k')\n    plt.title('PDF - SkinThickness')\n\n    plt.subplot(246)\n    sns.distplot(df['Age_trans'], color = 'k')\n    plt.title('PDF - Age')\n\n    plt.subplot(247)\n    sns.distplot(df['DiabetesPedigreeFunction_trans'], color = 'k')\n    plt.title('PDF - DiabetesPedigreeFunction')\n\n    plt.subplot(248)\n    sns.distplot(df['BMI'], color = 'k')\n    plt.title('PDF - BMI')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew_visual_trans()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transformations did not give the expected result, as we see some distortions. We'll try binning the values in those features to get rid of it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pregnancies_bin'] = np.where(df['Pregnancies'] == 0, 0,\n                                np.where((df['Pregnancies'] > 0) & (df['Pregnancies'] <= 5), 1,\n                                        np.where((df['Pregnancies'] > 5) & (df['Pregnancies'] <= 10), 2, 3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Insulin_bin'] = np.where(df['Insulin'] == 0, 0,\n                                np.where((df['Insulin'] > 0) & (df['Insulin'] <= 50), 1,\n                                        np.where((df['Insulin'] > 50) & (df['Insulin'] <= 200), 2, 3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = np.arange(0, 100, 10)\nnames = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\ndf['Age_bin'] = pd.cut(df['Age'], bins = bins, labels = names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_v2 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction_trans', 'Age_bin', 'Insulin_bin', 'Pregnancies_bin', 'Outcome']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outcome'] = df['Outcome'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the summary statistics, we have observed that the range of values in every feature differ. Let's apply the feature scaling techniques to avoid this.\n\nThere are different scaling techniques.\n* Normalization - This would scale the values between a minimum and maximum value. In most cases, it would be between 0 and 1.\n* Standardization - This technique is aimed at making the standard deviation of the feature to zero.\n\nI'm not including the mathematical expressions for the above techniques. We can do this manually but let's go with the predefined methods available in Scikit - Learn's preprocessing module.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled_mms = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df_scaled_mms.columns.tolist()\n\ndf_scaled_mms = pd.DataFrame(mms.fit_transform(df), columns = cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled_sc = pd.DataFrame(sc.fit_transform(df), columns = cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled_mms.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some points to note here.\n\n* The minimum value of all the features is 0.\n* The maximum is at 1.\n* The mean is in the same range across the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled_sc.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standardized dataset has the unit standard deviation (1). So we have applied two techniques and stored in different dataframes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check the correlation matrix for each of the dataframes to check the correlation between the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10, 8)\n\nsns.heatmap(df.corr() * 100, annot = True, cmap = 'coolwarm')\nplt.title('Correlation - Before Scaling', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df_scaled_mms.corr() * 100, annot = True, cmap = 'plasma')\nplt.title('Correlation - Normalized Data', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df_scaled_sc.corr() * 100, annot = True, cmap = 'Set1')\nplt.title('Correlation - Standardized Data', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmaps, we did not observe any difference in correlation among the dataframes. So let's go ahead with the normalized dataframe and include only the features which most correlate with the target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled_mms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We'll import the required libraries for model building and the evaluation of the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's segregate the independant and dependant features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_scaled_mms.drop(columns = ['Insulin_bin', 'Outcome'])\nY = df_scaled_mms['Outcome']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we'll split the dataset into train and test datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.25, stratify = Y, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train_X - ', train_x.shape)\nprint('Test_X - ', test_x.shape)\nprint('Train_Y - ', train_y.shape)\nprint('Test_Y - ', test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll write a function which takes a model name as an argument to fit, predict and evaluate the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_build(x):\n    model = x\n    model.fit(train_x, train_y)\n    pred = model.predict(test_x)\n    print('Accuracy :', accuracy_score(test_y, pred))\n    print('F1 :', f1_score(test_y, pred))\n    kfold = KFold(n_splits = 5, random_state = 56)\n    cv_res = cross_val_score(x, train_x, train_y, cv = kfold, scoring = 'accuracy')\n    print('CV Accuracy Score - 5 Splits :', cv_res.mean())\n    print('Precision :', precision_score(test_y, pred))\n    print('Recall :', recall_score(test_y, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_build(LogisticRegression())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_build(DecisionTreeClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_build(RandomForestClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_build(KNeighborsClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_build(SVC())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_build(GaussianNB())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(train_x, train_y)\npred = rf.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we've tried with various models, let's take the model with the good recall score and check the confusion matrix for the predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(confusion_matrix(test_y, pred), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What do we observe?\n\n* 105 normal people are identified as normal (TP)\n* 39 diabetic people are classified as diabetic (TN)\n* 19 normal people are identified as diabetic (FN)\n* 27 diabetic people are classified as normal (FP)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now the question arises, why did we chose the recall score as the evaluation metric?\n\nAs per the problem statement, the model should classify the diabetic people rightly. The impact would be more if diabetic people are classified as normal than the vice versa.\n\nAs FP has more influence, let's check the recall score of the model. Recall is the measure of the total 1's correctly classified out of the actual 1's.\n\nLooking at the problem statement, our model should have high recall score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_x.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see a voting classifier would help us get high recall score than 59, which the Random Forest baseline model has scored.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [('DT', DecisionTreeClassifier()), ('RF', RandomForestClassifier())]\nvc = VotingClassifier(estimators)\nvc.fit(train_x, train_y)\npred = vc.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy :', accuracy_score(test_y, pred))\nprint('Precision :', precision_score(test_y, pred))\nprint('Recall :', recall_score(test_y, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This did not turn fruitful as the combination of the models is putting the recall score down. We can try any combination using the Voting Classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(confusion_matrix(test_y, pred), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's try to find the optimal parameters to be given to the Random Forest Classifier by trying out with different combinations of values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"depth = []\nestimators = []\nrecall = []\n\nfor i in np.arange(2, 10):\n    for j in np.arange(100, 300, 10):\n        rf = RandomForestClassifier(max_depth = i, n_estimators = j, n_jobs = -1)\n        rf.fit(train_x, train_y)\n        pred = rf.predict(test_x)\n        r = recall_score(test_y, pred)\n        depth.append(i)\n        estimators.append(j)\n        recall.append(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame({'Depth' : depth,\n                   'Estimators' : estimators,\n                   'Recall Score' : recall})\nres.sort_values(by = 'Recall Score', ascending = False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(max_depth = 9, n_estimators = 120, n_jobs = -1)\nrf.fit(train_x, train_y)\npred = rf.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(confusion_matrix(test_y, pred), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above dataframe, max_depth of 9 and 120 estimators are giving a recall score of 59. I do not see any improvement. Let's try some other techniques.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us adjust the thresholds by using the predict_proba method. \n\nWe'll use three thresholds - 0.4, 0.5 and 0.6. The model would classify as 1, if the user defined threshold is exceeded. We'll compare the recall scores for different thresholds and conclude on the best one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = rf.predict_proba(test_x)[:, 1]\n\nprobabilities = pd.DataFrame({'Probability' : prob,\n             'P(0.4)' : '',\n             'P(0.5)' : '',\n             'P(0.6)' : ''})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities['P(0.4)'] = np.where(probabilities['Probability'] > 0.4, 1, 0)\nprobabilities['P(0.5)'] = np.where(probabilities['Probability'] > 0.5, 1, 0)\nprobabilities['P(0.6)'] = np.where(probabilities['Probability'] > 0.6, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I guess selecting 0.4 as the threshold would work out. Let's plot the confusion matrix for the same and check the recall score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(confusion_matrix(test_y, probabilities['P(0.4)']), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(test_y, probabilities['P(0.4)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the confusion matrix, the model is now classifying the actual 1's better than before. The recall score has improved from the earlier approaches.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can further improve the model by doing feature engineering(which requires the subject matter expertise) and bring in various ensemble techniques which are not covered in this kernel. \n\nHope you enjoyed reading this notebook. Kindly upvote and leave a comment if you like my work. Please let me know your suggestions in the comments section. Thanks!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}