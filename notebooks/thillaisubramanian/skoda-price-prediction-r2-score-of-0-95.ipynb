{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Introduction","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Various Data of used car around 100,000 listings, including various manufacturers such as Audi, BMW, Benz, Ford, Skoda and so on are available. With the help of this data set and the Machine Learning knowledge, we have to generate a Model which is like a tool to Predict the Market Price of the Car.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Scope of the Project","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From this data, We are focussing on one Particular Brand Skoda. Later from this knowledge it can applied for any other available Brands. Since it is about predicting value of a Car, this is a Regression problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our approach as follows:\n1. Cleaning the Data set.\n2. Sorting out the Missing Values.\n3. Replacing zero values with reasonable Mean values.\n4. Visualizing the Data.\n5. Finding the influencing factors for Price.\n6. Encoding the Object type datas.\n7. Model Selection for Training.\n8. Tuning the Hyperparameter of the Selected Model.\n9. Finding the Best Model.\n10. Predicting and Validating the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np                 # linear algebra\nimport pandas as pd                # data processing \nimport matplotlib.pyplot as plt    # data visualization\nimport seaborn as sns              # data visualization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1) Data Exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing and Cleaning the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# out of many brand data file we are focussing only on Skoda\ndf = pd.read_csv('../input/used-car-dataset-ford-and-mercedes/skoda.csv')\ndf.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above file it includes the features such as Model of the car, Year - purchased year, Price value, Transmission, Mileage - total number of miles reached so far, Fueltype, Tax - road tax, mpg - fuel consumption and Enginesize.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to view various info of available data\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above information it does not include any missing values. Out of nine features three are Object type and remaining are Numeric data type. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the shape of our data \ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to look for missing values\ndf.notnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For my convinence the Year feature has been changed in to Age. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to calculate vehicle age \ndf['age'] = 2020 - df['year']\ndf = df.drop(columns = 'year')    # removed that column\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to count number of zero value in each column\ndf.isin([0]).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above information we have few zero values in the features Tax and Enginesize. To get more accuracy on the model we have decided to include the reasonable values for these features.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# totally we have these much zero values for 'engineSize' and 'tax'\n# the age zero values represents the car purchased year on 2020\nprint(sum(df['engineSize'] == 0))\nprint(sum(df['tax'] == 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have certain number of Zero Values in Enginesize and Tax. We are initially replacing that value with a Nan type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the zero values \ndf[[\"engineSize\",\"tax\"]] = df[[\"engineSize\",\"tax\"]].replace(0,np.NaN)   # replacing Zero by Nan values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_to_fill = df.groupby(\"model\").median()       # Groupby.Median: Compute median of groups, excluding missing values. \n\nfor model, row in median_to_fill.iterrows():        # Iterrows: Iterate over DataFrame rows\n    rows_to_fill = (df[\"model\"] == model)\n    df[rows_to_fill] = df[rows_to_fill].fillna(row) # Fillna: Fills the NaN values with a given substitute number","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As said earlier, the zero values has now been calculated through median function based on Model feature. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to count number of zero value in each column\ndf.isin([0]).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above data it seems like they have meaningful values. We will conclude that in upcoming sections through Correlation matrix and Data visualization methods. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(df.corr(), annot=True) # annot=True shows the values inside the box","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Correlation matrix shows the clear correlation inbetween variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.\n\nThe Mileage and Age clearly indicates the Strong influence against Price. It has a valid reason and logic to accept this influence. The second set of pair Tax and mpg has bit lower influence over Price. Finally the Enginesize increases and the Price increases too, pretty much it's reasonable to agree that the size grows along with the cost of the Vehicle apparently.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr().abs()   # abs: generates the absolute value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above absolute value distribution simply it shows the correlation between each and every variables available in the data clearly.\n\nThe number of attributes in our Data are nine which has six Numerical and three Categorial i.e. Quantitative data types. Since we are doing a regression analysis it is important to visualize the importance of Quantitative datas as well. In the upcoming Encoding section we will convert these category datas in our desired numerical format.\n\nBecause of these two different data types such as numerical and categorial in our Model, we will visualize the data in two sets as follows:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_type = df[['price','mileage','tax','engineSize','mpg','age']] # Includes only Int and Float type Datas\n\nfig = plt.figure(figsize=(15,10))                             # Figsize: Dimension of the plot \n                                   \nfor index,col in enumerate(numeric_type):                     # Enumerate iterates over the numeric_type\n    sns.set_style('whitegrid')                                # Background theme of the plot\n    plt.subplot(3,3,index+1)                                  # Controls the rows and columns, Index for placing\n    sns.set(font_scale = 1.0)                                 # Scaling the Font size\n    sns.distplot(df[col],kde = False, color='blue')           # Dist plot: univariate distribution of observations\nfig.tight_layout(pad=1.0)                                     # To adjust subplots Label ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation from the Data: \n1. The distribution of Price are right skew and shows the mean of 14275 GBP, most of the vehicles are in this value.\n2. The distribution of Mileage also right skew almost many of the vehicles are within the range of 20000 miles.\n3. The Tax distribution shows the mean value of around 120 GBP.\n4. The EngineSize distribution shows mean value of 1.4 for the vehicle.\n5. The distribution of mpg shows the value of 60 for most of the vehicle. \n6. The Age distribution shows that 16 years old vehicle and most of them are in 2.5 years.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"category_type = df[['model','transmission','fuelType']]         # Includes only Object type Datas\n\nfig = plt.figure(figsize=(20,5))\n\nfor index,col in enumerate(category_type):\n    sns.set_style('whitegrid')\n    plt.subplot(1,3,index+1)\n    if(index == 0):\n        plt.xticks(rotation=90)                                   # To make X-axis Label in vertical represantation\n    sns.set(font_scale = 1.0)\n    sns.countplot(df[col], order = df[col].value_counts().index)  # categorical bin using bars representation\nfig.tight_layout(pad=1.0)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation from the Data:\n1. From the Model distribution half of the Car Models sold are Fabia and Octavia.\n2. More than 50 percent of the Car sold are coming under the Manual Transmission category. \n3. It is evident that People bought Vehicle with a Fuel type of Petrol the most. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Plotting over Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# various numeric_type data influence over price\nnumeric_type = df[['mileage','tax','engineSize','mpg','age']]               # Updated the set without Price\n\nfig = plt.figure(figsize=(20,20))\n\nfor index,col in enumerate(numeric_type):                                   \n        sns.set_style('whitegrid')                                          \n        plt.subplot(4,3,index+1)                   \n        sns.set(font_scale = 1.0)\n        sns.scatterplot(data = df, x = col, y = 'price',color='blue', alpha = 0.5) \nfig.tight_layout(pad=1.0)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation  from the Data:\n1. In the Mileage plot couple of Vehicles has reached above 250000 miles which has low Prices. \n2. The Tax distribution over price shows high Tax value for couple of Vehicles in low Price range might be a Outliers. \n3. The Engine size of 2.5 shows low price might be chance of Outliers. \n4. The mpg distribution shows bunch of Vehicles has around 170 to 200 mpg which should be a Outliers. \n5. The Age plot shows reasonable distribution over Price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# various category_type data influence over price\n\nfig = plt.figure(figsize=(20,5))\n\nfor index,col in enumerate(category_type):\n    sns.set_style('whitegrid')\n    plt.subplot(1,3,index+1)\n    if(index == 0):\n        plt.xticks(rotation=90)\n    sns.set(font_scale = 1.0)\n    sns.barplot(x=df[col], y='price', data = df, ci = None) # ci: to avoid error bars\nfig.tight_layout(pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation from the Data:\n1. Various Model in this Skoda brand shows various Price values.\n2. In this Brand Automatic transmission type has higher Price values. \n3. The Hybrid type Vehicles are evidently high in Price than any other fuel type available options.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Representation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In our Model Data we have seperated our features as 2 types. In Numerical data type we have no any missing values. In Categorial data type, we need to perform one hot coding before letting them to join our model. \n\nThere are two ways to encode the data : Label Encoding and One Hot Encoder.\n\nHere we have chosen to go with One Hot Encoder. What one hot encoding does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value. In our model we will be getting around twenty columns. \n\nDecided to choose One Hot Encoder: We might run into situations where, after label encoding, we might confuse our model into thinking that a column has data with some kind of order or hierarchy, when we clearly don’t have it. To avoid this, we ‘OneHotEncode’ that column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Encoding ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting all the categorial data into some useful numerical data for better evaluation using One-hot Encoding\nfrom sklearn.preprocessing import OneHotEncoder                                     # To perform encoding of data\ndf_onehot = pd.get_dummies(df,columns=['model', 'transmission','fuelType'])         # Encoding shown columns \nprint(df_onehot.shape)\ndf_onehot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the Train and Test data\nfrom sklearn.model_selection import train_test_split         # Splitting up the data as Train and Test set respectively\nX = df_onehot.drop(columns=['price'])                        # X includes all data except target variable\ny = df_onehot['price'].copy()                                # y has only target variable-Price\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0, test_size = 0.30) # Test size 30%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Model Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To select our correct Model it is always better to consider couple of algorithms and evaluate their performance through cross-validation method. We are interested in the following algorithms:\n\n1. Linear Regression\n2. Gradient Boosting\n3. Decision Tree\n4. Random Forest\n\nIn our case We have not Standardized our values because the algorithms that we chosen here does not requires the value to be standardized scalar.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score                         # Evaluate a score by cross-validation\nfrom sklearn.metrics import r2_score                                        # Coefficient of determination \n\n# Finding the best fit algorithm for our model\nmodel_list = [(LinearRegression(), 'LinearRegression'),                     # List included all desired algorithms\n              (GradientBoostingRegressor(),'GradientBoostingRegressor'),\n              (DecisionTreeRegressor(),'DecisionTreeRegressor'),\n              (RandomForestRegressor(),'RandomForestRegressor'),\n              ]\n\nmodel_score = []\n\nfor i in model_list:\n    model = i[0]                                                           # Scoring: Coefficient of determination r2\n    score = cross_val_score(model,X_train,y_train,cv=4, scoring='r2')      # model: estimator, cv: splitting strategy\n    print(f'{i[1]} score = {score.mean().round(2)*100}')                   # Score.mean: Shows mean of all scores                                     \n    model_score.append([i[1],score.mean()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from the Model selection block we found that out of all algorithms Gradient Boosting Regressor and Random Forest Regressor perfoms well. So we have decided to take Gradient Boosting regressor and tune its hyperparameter to attain maximum accuracy for our analysis.\n\nGradient boosting is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Tuning model's Hyperparameter","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now it is important to tune the Hyperparameter of the model to attain better accuracy. There are no any optimum value to tune any model it can be acheived only by continuous tuning. In this model we have decided to tune the following three Hyperparameters:\n\n1. n_estimators - number of boosting stages to perform, large number usually results in better performance.\n2. max_depth - maximum depth limits the number of nodes in the tree, best tuning parameter for best performance  \n3. learning_rate - Shrinks the contributiuon of each tree, choosing three different value for good performance\n\nTotally Grid search will run 3x3x3 = 27 models to find the best combination of the Hyperparameters. Actually based on our selection,in each model Gridsearch Cv will run cross validation with 4 folds. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\n# selecting the hyperparameter\nparam_grid = [\n    {'n_estimators' : [200,300,500],                 # number of boosting stages to perform\n          'max_depth' : [2,4,6],                     # maximum depth limits the number of nodes in the tree \n          'learning_rate' : [0.1,0.3,0.5]}           # learning rate: Shrinks the contributiuon of each tree                          \n]\n\n# Through grid search finding the best Model\ngrid_search = GridSearchCV(GradientBoostingRegressor(), # estimator object\n                           param_grid,                  # includes hyperparameter values\n                           cv=4,                        # cross-validation splitting strategy     \n                           scoring = 'r2')              # coefficient of determination","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train,y_train)                   # fitting the values in to grid search \ny_pred = grid_search.predict(X_test)               # predicting the Price value \nmy_model = grid_search.best_estimator_             # Best estimator has the parameters of better perfomance\nmy_model                                           # Best model ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The my_model has the best performing parameters among all other possible combinations. This model will be used to train and evaluate results for our analysis.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model.fit(X_train,y_train)                     # Training the best model with datas\nprediction = my_model.predict(X_test)             # Predicting the Price values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To generate a comparison table between predicted and actual Price of Car\nresult = X_test.copy()\nresult[\"predicted\"] = my_model.predict(X_test)\nresult[\"actual\"]= y_test.copy()\nresult =result[['predicted', 'actual']]\nresult['predicted'] = result['predicted'].round(2)\nresult.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above comparison table it is pretty clear that our model have perfomed in a better way. Let see the predicted and actual price values in plot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data visulaization of actual price and predicted price of Car\nXX = np.linspace(0, 40000, 1881)                                 # return numbers in selected range \nplt.scatter(XX, y_pred, color=\"green\", alpha = 0.2)              # green dots represents y_pred against XX         \nplt.scatter(XX, y_test, color=\"blue\", alpha = 0.5)               # blue dots represents y_test against XX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction Error Plot:\n\nA prediction error plot shows the actual targets from the dataset against the predicted values generated by our model. This allows us to see how much variance is in the model. Data scientists can diagnose regression models using this plot by comparing against the 45 degree line, where the prediction exactly matches the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Result visualization\nfrom yellowbrick.regressor import PredictionError   # To plot prediction error\n# Instantiate the linear model and visualizer\nvisualizer = PredictionError(my_model)\nvisualizer.fit(X_train, y_train)                    # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)                    # Evaluate the model on the test data\nvisualizer.show()                                   # Finalize and render the figure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Y-axis as predicted price and X-axis as actual price, we have grey dashed line which has one hundred percent accuracy it means actual = predicted. Our model has a Rsquared value of 0.95. Our best model really perfomed well.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So we conclude that through Hyperparameter tuning we have increased our model performance and finally achieved the Rsquared value of 0.95","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Future Works","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From this knowledge we can also predict the Price value of any other available Car data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nHere come to the end of this notebook, this is the first regression problem solving for me, I'll try with more challenging dataset and algorithm in next problem. I would greatly appreciate it if you kindly give me some feedback for this notebook. If you like it, please hit upvote! Thanks for visiting ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}