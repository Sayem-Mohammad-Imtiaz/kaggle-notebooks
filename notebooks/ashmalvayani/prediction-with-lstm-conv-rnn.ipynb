{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-04T09:18:50.748334Z","iopub.execute_input":"2021-09-04T09:18:50.748779Z","iopub.status.idle":"2021-09-04T09:18:50.763899Z","shell.execute_reply.started":"2021-09-04T09:18:50.748739Z","shell.execute_reply":"2021-09-04T09:18:50.762773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**first check that tensorflow version is abover 2.0.0**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:50.765935Z","iopub.execute_input":"2021-09-04T09:18:50.766624Z","iopub.status.idle":"2021-09-04T09:18:50.775178Z","shell.execute_reply.started":"2021-09-04T09:18:50.766587Z","shell.execute_reply":"2021-09-04T09:18:50.77431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download the data and have a look at it","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nreader = pd.read_csv(\"/kaggle/input/sunspots/Sunspots.csv\")\nreader.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:50.802394Z","iopub.execute_input":"2021-09-04T09:18:50.80278Z","iopub.status.idle":"2021-09-04T09:18:50.83166Z","shell.execute_reply.started":"2021-09-04T09:18:50.802744Z","shell.execute_reply":"2021-09-04T09:18:50.830822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reader.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:50.83569Z","iopub.execute_input":"2021-09-04T09:18:50.83772Z","iopub.status.idle":"2021-09-04T09:18:50.865816Z","shell.execute_reply.started":"2021-09-04T09:18:50.837682Z","shell.execute_reply":"2021-09-04T09:18:50.864984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [\"Time\",\"Date\", \"MMTSN\"]\nreader.columns = cols\n\nreader.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:50.870189Z","iopub.execute_input":"2021-09-04T09:18:50.872434Z","iopub.status.idle":"2021-09-04T09:18:50.883258Z","shell.execute_reply.started":"2021-09-04T09:18:50.87236Z","shell.execute_reply":"2021-09-04T09:18:50.882311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_step = []\nsunspots = []\n\nfor item in reader[\"MMTSN\"]:\n    sunspots.append(item)\n\nfor item in reader[\"Time\"]:\n    time_step.append(item)\n    \nseries = np.array(sunspots)\ntime = np.array(time_step)\n\nprint(series[:5],time[:5], sep=\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:50.887894Z","iopub.execute_input":"2021-09-04T09:18:50.889802Z","iopub.status.idle":"2021-09-04T09:18:50.906143Z","shell.execute_reply.started":"2021-09-04T09:18:50.889764Z","shell.execute_reply":"2021-09-04T09:18:50.905348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's Plot them","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:51.055066Z","iopub.execute_input":"2021-09-04T09:18:51.057559Z","iopub.status.idle":"2021-09-04T09:18:51.31427Z","shell.execute_reply.started":"2021-09-04T09:18:51.057511Z","shell.execute_reply":"2021-09-04T09:18:51.31287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now split the data","metadata":{}},{"cell_type":"code","source":"split_time = 3000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 30\nbatch_size = 32\nshuffle_buffer_size = 1000","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:51.315704Z","iopub.execute_input":"2021-09-04T09:18:51.316143Z","iopub.status.idle":"2021-09-04T09:18:51.321516Z","shell.execute_reply.started":"2021-09-04T09:18:51.316106Z","shell.execute_reply":"2021-09-04T09:18:51.320415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_train","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:51.323479Z","iopub.execute_input":"2021-09-04T09:18:51.323833Z","iopub.status.idle":"2021-09-04T09:18:51.331541Z","shell.execute_reply.started":"2021-09-04T09:18:51.323799Z","shell.execute_reply":"2021-09-04T09:18:51.330751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"make a function for windowed data set","metadata":{}},{"cell_type":"code","source":"def windowed_dataset(series, windoe_size, batch_size, shuffle_size):\n    series = tf.expand_dims(series,axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size+1, shift = 1, drop_remainder=True)\n    ds = ds.flat_map(lambda w:w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_size)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:51.333179Z","iopub.execute_input":"2021-09-04T09:18:51.333539Z","iopub.status.idle":"2021-09-04T09:18:51.342253Z","shell.execute_reply.started":"2021-09-04T09:18:51.333505Z","shell.execute_reply":"2021-09-04T09:18:51.341376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:51.343587Z","iopub.execute_input":"2021-09-04T09:18:51.343959Z","iopub.status.idle":"2021-09-04T09:18:51.354918Z","shell.execute_reply.started":"2021-09-04T09:18:51.343896Z","shell.execute_reply":"2021-09-04T09:18:51.353933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a model and use sequential layers to the model. The lambda layers will help performing arbitary operations on the model.\nwe've used two LSTM layers with both return_sequences = True so that they can be fed into the Dense layers next, feel free to play with neurons in Dense layers but for now I've set 30,10,1","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nwindow_size = 64\nbatch_size = 256\n\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\", activation=\"relu\",input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.Dense(30, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:51.356292Z","iopub.execute_input":"2021-09-04T09:18:51.356672Z","iopub.status.idle":"2021-09-04T09:18:51.861832Z","shell.execute_reply.started":"2021-09-04T09:18:51.356597Z","shell.execute_reply":"2021-09-04T09:18:51.861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## for now, I'll just train with 100 epochs to find the perfect learning rate instead of just setting it as anything default","metadata":{}},{"cell_type":"code","source":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:18:51.862994Z","iopub.execute_input":"2021-09-04T09:18:51.863376Z","iopub.status.idle":"2021-09-04T09:19:53.072607Z","shell.execute_reply.started":"2021-09-04T09:18:51.863339Z","shell.execute_reply":"2021-09-04T09:19:53.071748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's plot learning rate","metadata":{}},{"cell_type":"code","source":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 60])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:19:53.076204Z","iopub.execute_input":"2021-09-04T09:19:53.076478Z","iopub.status.idle":"2021-09-04T09:19:53.502474Z","shell.execute_reply.started":"2021-09-04T09:19:53.076452Z","shell.execute_reply":"2021-09-04T09:19:53.501628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's retrain with the least learning rate that can be seen from the graph. check the lowest trough**","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nwindow_size=60\nbatch_size=100\n\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\",input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set,epochs=500)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:20:58.080696Z","iopub.execute_input":"2021-09-04T09:20:58.081141Z","iopub.status.idle":"2021-09-04T09:26:48.743557Z","shell.execute_reply.started":"2021-09-04T09:20:58.0811Z","shell.execute_reply":"2021-09-04T09:26:48.742746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now forecast the model","metadata":{}},{"cell_type":"code","source":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:31:49.913903Z","iopub.execute_input":"2021-09-04T09:31:49.914254Z","iopub.status.idle":"2021-09-04T09:31:51.205078Z","shell.execute_reply.started":"2021-09-04T09:31:49.914212Z","shell.execute_reply":"2021-09-04T09:31:51.204132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:31:51.208425Z","iopub.execute_input":"2021-09-04T09:31:51.208682Z","iopub.status.idle":"2021-09-04T09:31:51.367636Z","shell.execute_reply.started":"2021-09-04T09:31:51.208658Z","shell.execute_reply":"2021-09-04T09:31:51.366913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's check Mean Absolute Error","metadata":{}},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:31:54.193823Z","iopub.execute_input":"2021-09-04T09:31:54.194165Z","iopub.status.idle":"2021-09-04T09:31:54.20262Z","shell.execute_reply.started":"2021-09-04T09:31:54.194133Z","shell.execute_reply":"2021-09-04T09:31:54.201417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean absolute error is not very close to 0 but we can see it's better than with just LSTM or just Convolutions which would be over 20","metadata":{}},{"cell_type":"code","source":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\nloss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()\n\nzoomed_loss = loss[200:]\nzoomed_epochs = range(200,500)\n\n\n# Plot training and validation loss per epoch\nplt.plot(zoomed_epochs, zoomed_loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:31:57.09894Z","iopub.execute_input":"2021-09-04T09:31:57.099284Z","iopub.status.idle":"2021-09-04T09:31:57.413555Z","shell.execute_reply.started":"2021-09-04T09:31:57.099231Z","shell.execute_reply":"2021-09-04T09:31:57.412621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rnn_forecast)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:32:01.433869Z","iopub.execute_input":"2021-09-04T09:32:01.434186Z","iopub.status.idle":"2021-09-04T09:32:01.442602Z","shell.execute_reply.started":"2021-09-04T09:32:01.434156Z","shell.execute_reply":"2021-09-04T09:32:01.441629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}