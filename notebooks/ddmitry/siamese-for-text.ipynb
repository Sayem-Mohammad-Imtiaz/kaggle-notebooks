{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as ps\nfrom pathlib import Path\nfrom collections import OrderedDict\n\nimport matplotlib.pyplot as plt\nfrom fastprogress import master_bar, progress_bar\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(f'[!] Using {DEVICE}')\n\nNUM_WORKERS = os.cpu_count() - 1\nprint(f'[!] Using cpus - {NUM_WORKERS} for data loaders')\n\nDATA_FOLDER = Path('..') / 'input'\ntrain_df = ps.read_csv(DATA_FOLDER / 'SPAM text message 20170820 - Data.csv')\nprint(train_df.shape)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e73a2d16d58096d84069dbb2b581365f345dcff"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10_000)\ntokenizer.fit_on_texts(train_df['Message'].values)\nword_indices = tokenizer.texts_to_sequences(train_df['Message'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c577424fcc66a7aa06fc1163a3cb0fb498b9796"},"cell_type":"code","source":"train_df['target'] = train_df['Category'].map({'ham': 1, 'spam': 0})\ntrain_df['tokenized message length'] = [len(item) for item in word_indices]\n\nword_indices = pad_sequences(word_indices, maxlen=20)\nprint(word_indices.shape)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cee45a5d06f7e8746205a503e5f3d7da17b841c9"},"cell_type":"code","source":"print('Mean sentence length is', int(train_df['tokenized message length'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2860c2ecf930bd2faa96367ab88153deed9165db"},"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, word_indices, labels):\n        self.word_indices = word_indices\n        self.labels = labels\n        self.__unique_labels = set(labels)\n        self.label2index = {lbl: np.where(labels == lbl)[0] for lbl in self.__unique_labels}\n        \n    def __len__(self):\n        return len(self.word_indices)\n    \n    def __getitem__(self, idx):\n        seq1, label = self.word_indices[idx], self.labels[idx]\n        target = np.random.randint(0, 2)  # 0 or 1\n        if target == 1:\n            pair_idx = np.random.choice(self.label2index[label])\n        else:\n            other_class = np.random.choice(list(self.__unique_labels - {label}))\n            pair_idx = np.random.choice(self.label2index[other_class])\n        seq2 = self.word_indices[pair_idx]\n        return torch.from_numpy(seq1).long(), torch.from_numpy(seq2).long(), torch.LongTensor([target])\n    \n\nclass DummyDataset(Dataset):\n    def __init__(self, word_indices, labels):\n        self.data, self.labels = word_indices, labels\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.data[idx]).long(), torch.LongTensor([self.labels[idx]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07f598df094e4eb4885d4a73611e944060d9cdb1"},"cell_type":"code","source":"class EvaEmb(nn.Module):\n    def __init__(self, \n                 embedding_size: int, \n                 embedding_dim: int, \n                 num_classes: int, \n                 hidden_rnn_size: int = 60, \n                 dropout_rate: float = 0.3, \n                 lstm_layers: int = 1,\n                 gru_layers: int = 1):\n        super(EvaEmb, self).__init__()\n        self.embedding = nn.Embedding(embedding_size, embedding_dim)\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_rnn_size,\n            bias=True,\n            num_layers=lstm_layers,\n            batch_first=True,\n            dropout=dropout_rate,\n            bidirectional=True\n        )\n        self.gru = nn.GRU(\n            input_size=hidden_rnn_size * 2,\n            hidden_size=hidden_rnn_size,\n            bias=True,\n            num_layers=gru_layers,\n            batch_first=True,\n            dropout=dropout_rate,\n            bidirectional=True\n        )\n        self.max_pool = nn.MaxPool1d(3, stride=2)\n        self.avg_pool = nn.AvgPool1d(3, stride=2)\n        input_size = (hidden_rnn_size * 2 - 3) // 2 + 1\n        input_size *= 2  # 2 pool layers\n        self.liear_part = nn.Sequential(\n            OrderedDict([\n                ('block1', nn.Sequential(\n                    nn.Linear(input_size, int(input_size * 0.75)), \n                    nn.ReLU(True),\n                    nn.BatchNorm1d(int(input_size * 0.75)),\n                    nn.Dropout(dropout_rate)\n                )),\n                ('head', nn.Linear(int(input_size * 0.75), num_classes))\n            ])\n        )\n        \n    def forward(self, input_sequence):\n        x = self.embedding(input_sequence)  # batch * seq -> batch * seq * emb_size\n        \n        x_lstm, _hidden = self.lstm(x)\n        x_gru, _hidden = self.gru(x_lstm)\n        \n        x_max, x_avg = self.max_pool(x_gru)[:, -1], self.avg_pool(x_gru)[:, -1]\n        \n        classes = self.liear_part(torch.cat((x_max, x_avg), 1))\n        return classes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7a184ea41d1ff27a4ad0144532a7fa6f573be9a"},"cell_type":"code","source":"class SiameseNet(nn.Module):\n    def __init__(self, embedding_net):\n        super(SiameseNet, self).__init__()\n        self.embedding_net = embedding_net\n\n    def forward(self, x1, x2):\n        output1 = self.embedding_net(x1)\n        output2 = self.embedding_net(x2)\n        return output1, output2\n\n    def get_embedding(self, x):\n        return self.embedding_net(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4dc18956a557d19ca6e787f6f2ba4c16a58fbe"},"cell_type":"code","source":"class ContrastiveLoss(nn.Module):\n    \"\"\"\n    Contrastive loss\n    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n    \"\"\"\n\n    def __init__(self, margin):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n        self.eps = 1e-9\n\n    def forward(self, output1, output2, target, size_average=True):\n        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n        losses = 0.5 * (target.float() * distances +\n                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n        return losses.mean() if size_average else losses.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd1fb4fa39d2c5af8f6e9602369f0408146ccb2f"},"cell_type":"code","source":"dataset = TextDataset(word_indices, train_df['target'].values)\ndata_loader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3154f2a915b53a5181d49d4eedc3509a60e0e6ad"},"cell_type":"code","source":"emb_model = EvaEmb(10_000, 64, 2, hidden_rnn_size=32, lstm_layers=2, gru_layers=2)\nmodel = SiameseNet(emb_model).to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7b7436d0f8f1c5e08ac7f5df42f90837290bb9c"},"cell_type":"code","source":"loss = ContrastiveLoss(margin=1)\noptimizer = optim.Adam(model.parameters(), lr=1e-3 / 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"594f473a28e73ba2cac1d04c23502e1660891c67"},"cell_type":"code","source":"def train(model, loss_function, optimizer, num_epochs, train_loader):\n    train_losses = []\n    epochs = [i for i in range(1, num_epochs + 1)]\n    epochs_iterator = master_bar(epochs)\n    for epoch_number in epochs_iterator:\n        epoch_losses = []\n        model.train()\n        for data1, data2, lbl in progress_bar(train_loader, parent=epochs_iterator):\n            data1, data2, lbl = data1.to(DEVICE), data2.to(DEVICE), lbl.to(DEVICE)\n            out1, out2 = model(data1, data2)\n            loss = loss_function(out1, out2, lbl)\n            epoch_losses.append(loss.item())\n            # optimisation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epochs_iterator.child.comment = f'loss: {loss.item():.10f}'\n        epoch_loss = np.mean(epoch_losses)\n        train_losses.append(epoch_loss)\n        epochs_iterator.first_bar.comment = f'train loss - {epoch_loss}'\n    plt.figure(figsize=(12, 15))\n    plt.plot(epochs, train_losses)\n    plt.title('Train losses')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30f453bc75c4d01a7ac0a9f4119f0764fbc21380"},"cell_type":"code","source":"model = train(model, loss, optimizer, 300, data_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2e001c3ad9362f329f521cd0789c9c9240e6363"},"cell_type":"code","source":"def get_embs(model, data_loader):\n    model.eval()\n    embs, lbls = [], []\n    with torch.no_grad():\n        for data, lbl in progress_bar(data_loader):\n            data = data.to(DEVICE)\n            out = model.get_embedding(data).cpu().detach().numpy()\n            embs.append(out)\n            lbls.append(lbl.numpy())\n    return np.concatenate(embs), np.concatenate(lbls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b6d8edef9478f72773fec607125563f3ad7c9df"},"cell_type":"code","source":"data_for_plot = DummyDataset(word_indices, train_df['target'].values)\ndata_for_plot_loader = DataLoader(data_for_plot, batch_size=256, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"078341e1343e99376d42e138a11583eed7171d67"},"cell_type":"code","source":"embs, lbls = get_embs(model, data_for_plot_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3e54ff0386f89df2383b0e579f301844e22de63"},"cell_type":"code","source":"labels = train_df['target'].values\nplt.figure(figsize=(15, 15))\nfor _cls, _color in zip([0, 1], ['b', 'r']):\n    indices = np.where(labels == _cls)\n    tmp = embs[indices]\n    plt.plot(tmp[:, 0], tmp[:, 1], '.', color=_color)\nplt.legend(['spam', 'ham'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ca448bcf51dfe4945bbf7e227b0e1091d3961c0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}