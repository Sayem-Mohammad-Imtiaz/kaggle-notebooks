{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook creates a very simple U-Net to segment clouds\nImportant remarks:\n- The test-set is not being used\n- The train set will be splitted into train and validation sets.\n- The dataset creation is discussed in: https://medium.com/@cordmaur/how-to-create-a-custom-dataset-loader-in-pytorch-from-scratch-for-multi-band-satellite-images-c5924e908edf\n- More information full explanation can can be found on the medium article: https://medium.com/@cordmaur/creating-a-very-simple-u-net-model-with-pytorch-for-semantic-segmentation-of-satellite-images-223aa216e705\n- The training phase with 50 epochs takes around 3:30hs. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-04T13:39:57.219237Z","iopub.execute_input":"2021-06-04T13:39:57.219529Z","iopub.status.idle":"2021-06-04T13:39:57.224977Z","shell.execute_reply.started":"2021-06-04T13:39:57.219483Z","shell.execute_reply":"2021-06-04T13:39:57.223475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom PIL import Image\nimport torch\nimport matplotlib.pyplot as plt\nimport time\n\n\n# !pip install tensorflow==2.0","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:39:57.22669Z","iopub.execute_input":"2021-06-04T13:39:57.227292Z","iopub.status.idle":"2021-06-04T13:39:57.234541Z","shell.execute_reply.started":"2021-06-04T13:39:57.227021Z","shell.execute_reply":"2021-06-04T13:39:57.233774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install keras-unet\n# import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:39:57.236097Z","iopub.execute_input":"2021-06-04T13:39:57.236666Z","iopub.status.idle":"2021-06-04T13:39:57.243668Z","shell.execute_reply.started":"2021-06-04T13:39:57.236402Z","shell.execute_reply":"2021-06-04T13:39:57.243026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.layers import Input","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:39:57.245167Z","iopub.execute_input":"2021-06-04T13:39:57.245836Z","iopub.status.idle":"2021-06-04T13:39:57.253082Z","shell.execute_reply.started":"2021-06-04T13:39:57.245496Z","shell.execute_reply":"2021-06-04T13:39:57.252264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the dataset","metadata":{}},{"cell_type":"code","source":"class CloudDataset(Dataset):\n    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, pytorch=True):\n        super().__init__()\n        \n        # Loop through the files in red folder and combine, into a dictionary, the other bands\n        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir) for f in r_dir.iterdir() if not f.is_dir()]\n        self.pytorch = pytorch\n        \n    def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir, gt_dir):\n        \n        files = {'red': r_file, \n                 'green':g_dir/r_file.name.replace('red', 'green'),\n                 'blue': b_dir/r_file.name.replace('red', 'blue'), \n                 'nir': nir_dir/r_file.name.replace('red', 'nir'),\n                 'gt': gt_dir/r_file.name.replace('red', 'gt')}\n\n        return files\n                                       \n    def __len__(self):\n        \n        return len(self.files)\n     \n    def open_as_array(self, idx, invert=False, include_nir=False):\n\n        raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n                            np.array(Image.open(self.files[idx]['green'])),\n                            np.array(Image.open(self.files[idx]['blue'])),\n                           ], axis=2)\n    \n        if include_nir:\n            nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n            raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n    \n        if invert:\n            raw_rgb = raw_rgb.transpose((2,0,1))\n    \n        # normalize\n        return (raw_rgb / np.iinfo(raw_rgb.dtype).max)\n    \n\n    def open_mask(self, idx, add_dims=False):\n        \n        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n        raw_mask = np.where(raw_mask==255, 1, 0)\n        \n        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n    \n    def __getitem__(self, idx):\n        \n        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch, include_nir=True), dtype=torch.float32)\n        y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.torch.int64)\n        \n        return x, y\n    \n    def open_as_pil(self, idx):\n        \n        arr = 256*self.open_as_array(idx)\n        \n        return Image.fromarray(arr.astype(np.uint8), 'RGB')\n    \n    def __repr__(self):\n        s = 'Dataset class with {} files'.format(self.__len__())\n\n        return s","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:39:57.254555Z","iopub.execute_input":"2021-06-04T13:39:57.254988Z","iopub.status.idle":"2021-06-04T13:39:57.276655Z","shell.execute_reply.started":"2021-06-04T13:39:57.254814Z","shell.execute_reply":"2021-06-04T13:39:57.275601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = Path('../input/38cloud-cloud-segmentation-in-satellite-images/38-Cloud_training')\ndata = CloudDataset(base_path/'train_red', \n                    base_path/'train_green', \n                    base_path/'train_blue', \n                    base_path/'train_nir',\n                    base_path/'train_gt')\nlen(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:39:57.278337Z","iopub.execute_input":"2021-06-04T13:39:57.278961Z","iopub.status.idle":"2021-06-04T13:40:00.766849Z","shell.execute_reply.started":"2021-06-04T13:39:57.278631Z","shell.execute_reply":"2021-06-04T13:40:00.76621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = data[1000]\nx.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:00.769853Z","iopub.execute_input":"2021-06-04T13:40:00.770095Z","iopub.status.idle":"2021-06-04T13:40:00.789949Z","shell.execute_reply.started":"2021-06-04T13:40:00.770051Z","shell.execute_reply":"2021-06-04T13:40:00.789356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(10,9))\nax[0].imshow(data.open_as_array(150))\nax[1].imshow(data.open_mask(150))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:00.792847Z","iopub.execute_input":"2021-06-04T13:40:00.793078Z","iopub.status.idle":"2021-06-04T13:40:01.287932Z","shell.execute_reply.started":"2021-06-04T13:40:00.793035Z","shell.execute_reply":"2021-06-04T13:40:01.287109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, valid_ds = torch.utils.data.random_split(data, (6000, 2400))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:01.291345Z","iopub.execute_input":"2021-06-04T13:40:01.294723Z","iopub.status.idle":"2021-06-04T13:40:01.302323Z","shell.execute_reply.started":"2021-06-04T13:40:01.294663Z","shell.execute_reply":"2021-06-04T13:40:01.301563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:01.307176Z","iopub.execute_input":"2021-06-04T13:40:01.309054Z","iopub.status.idle":"2021-06-04T13:40:01.318625Z","shell.execute_reply.started":"2021-06-04T13:40:01.308996Z","shell.execute_reply":"2021-06-04T13:40:01.31775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size=12)\nvalid_dl = DataLoader(valid_ds, batch_size=12)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:01.320134Z","iopub.execute_input":"2021-06-04T13:40:01.320702Z","iopub.status.idle":"2021-06-04T13:40:01.330607Z","shell.execute_reply.started":"2021-06-04T13:40:01.320644Z","shell.execute_reply":"2021-06-04T13:40:01.329837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xb, yb = next(iter(train_dl))\nxb.shape, yb.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:01.332034Z","iopub.execute_input":"2021-06-04T13:40:01.332597Z","iopub.status.idle":"2021-06-04T13:40:02.593977Z","shell.execute_reply.started":"2021-06-04T13:40:01.332538Z","shell.execute_reply":"2021-06-04T13:40:02.59314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\ndef doubleConv(in_channels, out_channels):\n    conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = True),\n        nn.ReLU(inplace = True),\n        nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = True),\n        nn.ReLU(inplace = True)\n        )\n    return conv\n\nclass UNET(nn.Module):\n    def __init__(self):\n        super(UNET, self).__init__()\n\n        self.max_pool= nn.MaxPool2d(kernel_size = 2, stride = 2)\n        self.down_conv1 = doubleConv(4, 64)\n        self.down_conv2 = doubleConv(64, 128)\n        self.down_conv3 = doubleConv(128, 256)\n        self.down_conv4 = doubleConv(256, 512)\n        self.down_conv5 = doubleConv(512, 1024)\n\n        self.up_pool1 = nn.ConvTranspose2d(in_channels = 1024, out_channels = 512, kernel_size = 2, stride = 2)\n        self.up_pool2 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 2, stride = 2)\n        self.up_pool3 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 2, stride = 2)\n        self.up_pool4 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 2, stride = 2)\n        \n        self.upconv1 = doubleConv(1024, 512)\n        self.upconv2 = doubleConv(512, 256)\n        self.upconv3 = doubleConv(256, 128)\n        self.upconv4 = doubleConv(128, 64)\n        self.out = nn.Conv2d(in_channels = 64, out_channels = 2, kernel_size = 3, padding = True)\n        \n    def __call__(self, image):\n        x1 = self.down_conv1(image)\n        x2 = self.max_pool(x1)\n        x3 = self.down_conv2(x2)\n        x4 = self.max_pool(x3)\n        x5 = self.down_conv3(x4)\n        x6 = self.max_pool(x5)\n        x7 = self.down_conv4(x6)\n        x8 = self.max_pool(x7)\n        x9 = self.down_conv5(x8)\n        \n        x10 = self.up_pool1(x9)\n        x11 = self.upconv1(torch.cat([x10, x7], 1))\n        x12 = self.up_pool2(x11)\n        x13 = self.upconv2(torch.cat([x12, x5], 1))\n        x14 = self.up_pool3(x13)\n        x15 = self.upconv3(torch.cat([x14, x3], 1))\n        x16 = self.up_pool4(x15)\n        x17 = self.upconv4(torch.cat([x16, x1], 1))\n        x = self.out(x17)\n        \n#         x_arr = [x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17]\n#         for i in x_arr:\n#             print(i.shape)\n#         print(x.shape)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:02.595341Z","iopub.execute_input":"2021-06-04T13:40:02.595644Z","iopub.status.idle":"2021-06-04T13:40:02.616122Z","shell.execute_reply.started":"2021-06-04T13:40:02.595598Z","shell.execute_reply":"2021-06-04T13:40:02.615124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet = UNET()\n\n# from keras_unet.models import vanilla_unet\n\n# unet = vanilla_unet(input_shape=(384, 384, 4))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:02.617612Z","iopub.execute_input":"2021-06-04T13:40:02.618255Z","iopub.status.idle":"2021-06-04T13:40:02.831071Z","shell.execute_reply.started":"2021-06-04T13:40:02.617923Z","shell.execute_reply":"2021-06-04T13:40:02.830162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow import keras\n# keras.utils.plot_model(unet, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:02.832374Z","iopub.execute_input":"2021-06-04T13:40:02.832703Z","iopub.status.idle":"2021-06-04T13:40:02.836498Z","shell.execute_reply.started":"2021-06-04T13:40:02.832644Z","shell.execute_reply":"2021-06-04T13:40:02.835341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing one pass\nxb, yb = next(iter(train_dl))\n\nxb.shape, yb.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:02.83808Z","iopub.execute_input":"2021-06-04T13:40:02.838609Z","iopub.status.idle":"2021-06-04T13:40:02.998759Z","shell.execute_reply.started":"2021-06-04T13:40:02.838308Z","shell.execute_reply":"2021-06-04T13:40:02.997935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np_tensor = xb.numpy()\n# xb = tf.convert_to_tensor(np_tensor)\n\npred = unet(xb)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:02.999947Z","iopub.execute_input":"2021-06-04T13:40:03.000209Z","iopub.status.idle":"2021-06-04T13:40:34.547214Z","shell.execute_reply.started":"2021-06-04T13:40:03.000162Z","shell.execute_reply":"2021-06-04T13:40:34.546307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom IPython.display import clear_output\n\ndef train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=300):\n    start = time.time()\n    model.cuda()\n\n    train_loss, valid_loss = [], []\n\n    best_acc = 0.0\n\n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch, epochs - 1))\n        print('-' * 10)\n\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train(True)  # Set trainind mode = true\n                dataloader = train_dl\n            else:\n                model.train(False)  # Set model to evaluate mode\n                dataloader = valid_dl\n\n            running_loss = 0.0\n            running_acc = 0.0\n\n            step = 0\n\n            # iterate over data\n            for x, y in dataloader:\n                x = x.cuda()\n                y = y.cuda()\n                step += 1\n\n                # forward pass\n                if phase == 'train':\n                    # zero the gradients\n                    optimizer.zero_grad()\n                    outputs = model(x)\n                    loss = loss_fn(outputs, y)\n\n                    # the backward pass frees the graph memory, so there is no \n                    # need for torch.no_grad in this training pass\n                    loss.backward()\n                    optimizer.step()\n                    # scheduler.step()\n\n                else:\n                    with torch.no_grad():\n                        outputs = model(x)\n                        loss = loss_fn(outputs, y.long())\n\n                # stats - whatever is the phase\n                acc = acc_fn(outputs, y)\n\n                running_acc  += acc*dataloader.batch_size\n                running_loss += loss*dataloader.batch_size \n\n                if step % 100 == 0:\n                    # clear_output(wait=True)\n                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n                    # print(torch.cuda.memory_summary())\n                \n                \n            epoch_loss = running_loss / len(dataloader.dataset)\n            epoch_acc = running_acc / len(dataloader.dataset)\n\n            clear_output(wait=True)\n            print('Epoch {}/{}'.format(epoch, epochs - 1))\n            print('-' * 10)\n            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n            print('-' * 10)\n\n            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n\n    time_elapsed = time.time() - start\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n    \n    return train_loss, valid_loss    \n\ndef acc_metric(predb, yb):\n    return (predb.argmax(dim=1) == yb.cuda()).float().mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:34.548668Z","iopub.execute_input":"2021-06-04T13:40:34.548952Z","iopub.status.idle":"2021-06-04T13:40:34.567047Z","shell.execute_reply.started":"2021-06-04T13:40:34.548908Z","shell.execute_reply":"2021-06-04T13:40:34.565795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\n# opt = torch.optim.Adam(unet.parameters(), lr=0.0001)\nopt = torch.optim.SGD(unet.parameters(), lr=0.0001, momentum=0.9)\n\n# Utpo 10^-8\n# unet.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\ntrain_loss, valid_loss = train(unet, train_dl, valid_dl, loss_fn, opt, acc_metric, epochs=300)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:40:34.568659Z","iopub.execute_input":"2021-06-04T13:40:34.569241Z","iopub.status.idle":"2021-06-04T13:49:03.098212Z","shell.execute_reply.started":"2021-06-04T13:40:34.568912Z","shell.execute_reply":"2021-06-04T13:49:03.095327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify a path to save to\nPATH = \"./model.pt\"\n# Save\ntorch.save(unet.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:03.102039Z","iopub.execute_input":"2021-06-04T13:49:03.102375Z","iopub.status.idle":"2021-06-04T13:49:03.345228Z","shell.execute_reply.started":"2021-06-04T13:49:03.102326Z","shell.execute_reply":"2021-06-04T13:49:03.344478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks = [\n#     EarlyStopping(patience=10, verbose=1),\n#     ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n#     ModelCheckpoint('model-tgs-salt.h5', verbose=1, save_best_only=True, save_weights_only=True)\n# ]\n\n# results = model.fit(X_train, y_train, batch_size=32, epochs=50, callbacks=callbacks,\\\n#                     validation_data=(X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:03.830218Z","iopub.execute_input":"2021-06-04T13:49:03.832893Z","iopub.status.idle":"2021-06-04T13:49:03.841464Z","shell.execute_reply.started":"2021-06-04T13:49:03.832832Z","shell.execute_reply":"2021-06-04T13:49:03.840427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.plot(train_loss, label='Train loss')\nplt.plot(valid_loss, label='Valid loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:03.847765Z","iopub.execute_input":"2021-06-04T13:49:03.849553Z","iopub.status.idle":"2021-06-04T13:49:04.138567Z","shell.execute_reply.started":"2021-06-04T13:49:03.84949Z","shell.execute_reply":"2021-06-04T13:49:04.137491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_to_img(xb, idx):\n    img = np.array(xb[idx,0:3])\n    return img.transpose((1,2,0))\n\ndef predb_to_mask(predb, idx):\n    p = torch.functional.F.softmax(predb[idx], 0)\n    return p.argmax(0).cpu()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:04.143861Z","iopub.execute_input":"2021-06-04T13:49:04.146443Z","iopub.status.idle":"2021-06-04T13:49:04.156213Z","shell.execute_reply.started":"2021-06-04T13:49:04.144127Z","shell.execute_reply":"2021-06-04T13:49:04.155348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xb, yb = next(iter(train_dl))\n\nwith torch.no_grad():\n    predb = unet(xb.cuda())\n    \npredb.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:04.16151Z","iopub.execute_input":"2021-06-04T13:49:04.164409Z","iopub.status.idle":"2021-06-04T13:49:04.744919Z","shell.execute_reply.started":"2021-06-04T13:49:04.16435Z","shell.execute_reply":"2021-06-04T13:49:04.744166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs = 12\nfig, ax = plt.subplots(bs,3, figsize=(15,bs*5))\nfor i in range(bs):\n    ax[i,0].imshow(batch_to_img(xb,i))\n    ax[i,1].imshow(yb[i])\n    ax[i,2].imshow(predb_to_mask(predb, i))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:04.746214Z","iopub.execute_input":"2021-06-04T13:49:04.746542Z","iopub.status.idle":"2021-06-04T13:49:11.289007Z","shell.execute_reply.started":"2021-06-04T13:49:04.746488Z","shell.execute_reply":"2021-06-04T13:49:11.288213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CloudTestDataset(Dataset):\n    def __init__(self, r_dir, g_dir, b_dir, nir_dir, pytorch=True):\n        super().__init__()\n        \n        # Loop through the files in red folder and combine, into a dictionary, the other bands\n        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir) for f in r_dir.iterdir() if not f.is_dir()]\n        self.pytorch = pytorch\n        \n    def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir):\n        \n        files = {'red': r_file, \n                 'green':g_dir/r_file.name.replace('red', 'green'),\n                 'blue': b_dir/r_file.name.replace('red', 'blue'), \n                 'nir': nir_dir/r_file.name.replace('red', 'nir'),}\n\n        return files\n                                       \n    def __len__(self):\n        \n        return len(self.files)\n     \n    def open_as_array(self, idx, invert=False, include_nir=False):\n\n        raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n                            np.array(Image.open(self.files[idx]['green'])),\n                            np.array(Image.open(self.files[idx]['blue'])),\n                           ], axis=2)\n    \n        if include_nir:\n            nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n            raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n    \n        if invert:\n            raw_rgb = raw_rgb.transpose((2,0,1))\n    \n        # normalize\n        return (raw_rgb / np.iinfo(raw_rgb.dtype).max)\n    \n\n    def open_mask(self, idx, add_dims=False):\n        \n        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n        raw_mask = np.where(raw_mask==255, 1, 0)\n        \n        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n    \n    def __getitem__(self, idx):\n        \n        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch, include_nir=True), dtype=torch.float32)\n#         y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.torch.int64)\n        \n        return x\n    \n    def open_as_pil(self, idx):\n        \n        arr = 256*self.open_as_array(idx)\n        \n        return Image.fromarray(arr.astype(np.uint8), 'RGB')\n    \n    def __repr__(self):\n        s = 'Dataset class with {} files'.format(self.__len__())\n\n        return s","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:11.290191Z","iopub.execute_input":"2021-06-04T13:49:11.290583Z","iopub.status.idle":"2021-06-04T13:49:11.3141Z","shell.execute_reply.started":"2021-06-04T13:49:11.29054Z","shell.execute_reply":"2021-06-04T13:49:11.313059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = Path('../input/38cloud-cloud-segmentation-in-satellite-images/38-Cloud_test')\ndata = CloudTestDataset(base_path/'test_red', \n                    base_path/'test_green', \n                    base_path/'test_blue', \n                    base_path/'test_nir')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:11.315517Z","iopub.execute_input":"2021-06-04T13:49:11.316005Z","iopub.status.idle":"2021-06-04T13:49:46.232584Z","shell.execute_reply.started":"2021-06-04T13:49:11.315956Z","shell.execute_reply":"2021-06-04T13:49:46.231818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = data","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:46.233907Z","iopub.execute_input":"2021-06-04T13:49:46.23418Z","iopub.status.idle":"2021-06-04T13:49:46.239516Z","shell.execute_reply.started":"2021-06-04T13:49:46.234138Z","shell.execute_reply":"2021-06-04T13:49:46.237544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dl = DataLoader(test_ds, batch_size=12)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:46.241714Z","iopub.execute_input":"2021-06-04T13:49:46.242528Z","iopub.status.idle":"2021-06-04T13:49:46.24998Z","shell.execute_reply.started":"2021-06-04T13:49:46.242057Z","shell.execute_reply":"2021-06-04T13:49:46.249365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for xb in test_dl:\n#     xb = xb.cuda()\n#     with torch.no_grad():\n#         predb = unet(xb.cuda())\n#     bs = 12\n#     for i in range(bs):\n# #         plt.figure(figsize=(384,384))\n#         ax.imsave(predb_to_mask(predb, \"gt_\"+str(yb)))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:49:46.250929Z","iopub.execute_input":"2021-06-04T13:49:46.251161Z"},"trusted":true},"execution_count":null,"outputs":[]}]}