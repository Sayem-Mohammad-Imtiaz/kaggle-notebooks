{"cells":[{"metadata":{},"cell_type":"markdown","source":"implementation is available in my kaggle notebook : https://www.kaggle.com/aaabbbiwnwn/face-generator-02\n\n"},{"metadata":{},"cell_type":"markdown","source":"author : mohammad Ibrahimkhah , 2020\n"},{"metadata":{},"cell_type":"markdown","source":"based on jeff Heaton's implementation of radford paper : https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n"},{"metadata":{},"cell_type":"markdown","source":"radford paper : https://arxiv.org/abs/1511.06434"},{"metadata":{"trusted":true},"cell_type":"code","source":"### libraries\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Reshape, Dropout, Dense \nfrom tensorflow.keras.layers import Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os \nimport time\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### time format\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CONSTANTS\n\n## resolution factor \nGENERATE_RES = 2 \n## resolution : 1=32, 2=64, 3=96, 4=128, etc...\nGENERATE_SQUARE = 32 * GENERATE_RES \nprint(f\"Will generate {GENERATE_SQUARE}px square images.\")\nIMAGE_CHANNELS = 3\n\n## preview image \nPREVIEW_ROWS = 4\nPREVIEW_COLS = 7\nPREVIEW_MARGIN = 16\n\n## random vector size to generate images from\nSEED_SIZE = 128\n\n## making requred directories\n# !rm -r /kaggle/working/images\n# !rm /kaggle/working/*\n# !rm /kaggle/working/images/*.png\nos.mkdir('/kaggle/working/images')\n\n## paths Configuration\nDATA_PATH = '/kaggle/input'\nDATA_OUTPUT_PATH = '/kaggle/working'\nIMAGE_OUTPUT_PATH = '/kaggle/working/images'\n\n# GAN Configuration\nEPOCHS = 51\nBATCH_SIZE = 32\nBUFFER_SIZE = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### reading dataset and saving it as numpy arrary\n\ntraining_binary_path = os.path.join(DATA_OUTPUT_PATH, f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n\nprint(f\"Looking for file: {training_binary_path}\")\n\nif not os.path.isfile(training_binary_path):\n    start = time.time()\n    print(\"Loading training images...\")\n\n    training_data = []\n    faces_path = os.path.join(DATA_PATH,'celeba-dataset/img_align_celeba/img_align_celeba')\n    \n    counter = 0\n    for filename in tqdm(os.listdir(faces_path)):\n        if counter == 50000 :\n            break\n        path = os.path.join(faces_path,filename)\n        image = Image.open(path).resize((GENERATE_SQUARE, GENERATE_SQUARE),Image.ANTIALIAS)\n        training_data.append(np.asarray(image))\n        counter = counter + 1\n\n    training_data = np.reshape(training_data,(-1,GENERATE_SQUARE, GENERATE_SQUARE,IMAGE_CHANNELS))\n    # converting from int to float32\n    training_data = training_data.astype(np.float32)\n    # converting pixels from rage 0 to 255 -> range -1 to 1\n    training_data = training_data / 127.5 - 1.\n\n    print(\"Saving training image binary...\")\n    np.save(training_binary_path,training_data)\n    elapsed = time.time()-start\n    print (f'Image preprocess time: {hms_string(elapsed)}')\nelse:\n    print(\"Loading previous training array...\")\n    training_data = np.load(training_binary_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### batch and shuffle the data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### generator and discriminator model\n\ndef build_generator(seed_size, channels):\n    model = Sequential()\n\n    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n    model.add(Reshape((4,4,256)))\n\n    model.add(Dropout(0.25))\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    \n    model.add(Dropout(0.25))\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=9,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n   \n    model.add(Dropout(0.25))\n    model.add(UpSampling2D())\n    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    model.add(Dropout(0.25))\n    model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n    model.add(Conv2D(128,kernel_size=9,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n    model.add(Activation(\"tanh\"))\n\n    return model\n\n\ndef build_discriminator(image_shape):\n    model = Sequential()\n\n    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, padding=\"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size=9, strides=2, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, kernel_size=3, strides=3, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### saving perview images \n## output path : /kaggle/working/images\n\ndef save_images(cnt,noise):\n  image_array = np.full(( \n      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n      255, dtype=np.uint8)\n  \n  generated_images = generator.predict(noise)\n  generated_images = 0.5 * generated_images + 0.5\n\n  image_count = 0\n  for row in range(PREVIEW_ROWS):\n      for col in range(PREVIEW_COLS):\n        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] = generated_images[image_count] * 255\n        image_count += 1\n  \n  filename = os.path.join(IMAGE_OUTPUT_PATH,f\"train-{cnt}.png\")\n  im = Image.fromarray(image_array)\n  im.save(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### testing generator\ngenerator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\nnoise = tf.random.normal([1, SEED_SIZE])\ngenerated_image = generator(noise, training=False)\nplt.imshow(generated_image[0, :, :, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### testing discriminator\nimage_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\ndiscriminator = build_discriminator(image_shape)\ndecision = discriminator(generated_image)\nprint (decision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### loss functions\ncross_entropy = tf.keras.losses.BinaryCrossentropy()\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### one step in training\n@tf.function\ndef train_step(images):\n  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = generator(seed, training=True)\n\n    real_output = discriminator(images, training=True)\n    fake_output = discriminator(generated_images, training=True)\n\n    gen_loss = generator_loss(fake_output)\n    disc_loss = discriminator_loss(real_output, fake_output)\n    \n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n  return gen_loss,disc_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### function for ploting generator loss vs discriminator loss\ndef plot_gan_losses(g_loss, d_loss):\n    plt.plot(g_loss)\n    plt.plot(d_loss)\n    plt.title('GAN Loss evolution')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['Generator', 'Discriminator'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### training process\ndef train(dataset, epochs):\n  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, SEED_SIZE))\n  start = time.time()\n\n  for epoch in range(epochs):\n    epoch_start = time.time()\n\n    gen_loss_list = []\n    disc_loss_list = []\n\n    for image_batch in dataset:\n      t = train_step(image_batch)\n      gen_loss_list.append(t[0])\n      disc_loss_list.append(t[1])\n\n    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n\n    epoch_elapsed = time.time()-epoch_start\n    print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, time={hms_string(epoch_elapsed)}')\n    # save perview image every 10 epoch\n    if ((epoch)%10 == 0):\n        save_images(epoch,fixed_seed)\n\n  elapsed = time.time()-start\n  print (f'Training time: {hms_string(elapsed)}')\n  plot_gan_losses(gen_loss_list, disc_loss_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(train_dataset, EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### saving model\ngenerator.save(os.path.join(DATA_OUTPUT_PATH,\"face_generator.h5\"))\ndiscriminator.save(os.path.join(DATA_OUTPUT_PATH,\"face_discriminator.h5\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}