{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face Detection Using CNN Algorithms","metadata":{}},{"cell_type":"markdown","source":">**This project is for building CNN architecture for facial features and face detection by plotting points on the facial features and box around the face**","metadata":{}},{"cell_type":"code","source":"# import packages\nimport PIL\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport keras\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.preprocessing import image\n\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Wrangling","metadata":{}},{"cell_type":"markdown","source":"### 1.1. Data Loading","metadata":{}},{"cell_type":"markdown","source":">  \n**Data Source:** https://www.kaggle.com/jessicali9530/celeba-dataset  \nBut we are going to **use only the data of the first 35000 images** as a start then we are going to update the model step by step beacuse the data is very very big **(1 GB)**","metadata":{}},{"cell_type":"code","source":"# This cell for the values that will be used across the entire notebook\n# pathes\nkey_points_data_path = \"../input/celeba-dataset/list_landmarks_align_celeba.csv\"\nimages_data_path = \"../input/celeba-dataset/img_align_celeba/img_align_celeba\"\n\n# data size\nimages_data_size = 35000\n\n# originsl image dimensions\nx_org = 178    # original x value\ny_org = 218    # original y value\n\n# new image dimensions\nx_ = 45                             # new value of x\nimage_size_ratio = x_org / y_org    # dimensions ratio\ny_ = int(image_size_ratio * x_)     # new value of y\n\n# image sizes\noriginal_image_size = (x_org, y_org)\nnew_image_size = (x_,y_)\n\n# the image size that will be used in the training process\nimage_size_training = new_image_size\n\n# check the new size value\nnew_image_size","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.1. Key points data","metadata":{}},{"cell_type":"code","source":"# load the dataset (key points)\ndf_org = pd.read_csv(key_points_data_path)\ndf_org = df_org[:images_data_size]\n\n# check\ndf_org.head(3)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_org.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.2. Images Data","metadata":{}},{"cell_type":"code","source":"# convert images to arrays to use it in training\nimages_data = list()\nfor idx in range(df_org.shape[0]):\n    # to get the path based on index\n    path = \"{}/{}\".format(str(images_data_path),str(df_org.iloc[idx].image_id))\n    \n    # to read the image\n    image = PIL.Image.open(path).resize(image_size_training)\n    image_array = np.asarray(image) / 255\n    \n    # append the image array to images_data\n    images_data.append(image_array)\n    \n# convert images_data to be array not list\nimages_data = np.array(images_data)\n\n# check\nimages_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nplt.imshow(images_data[18]);","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final data sets which we will work with\nprint(\"Images Data Arrays Shape:\", images_data.shape)\nprint(\"Key Points Data Shape:\", df_org.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Data Cleaning (Preprocessing)","metadata":{}},{"cell_type":"code","source":"# check if there is any null values in the data\ndf_org.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the numerical values properties in the data\ndf_org.describe()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Data is clean and ready for analysis**","metadata":{}},{"cell_type":"markdown","source":"### 1.3. Images reading and plotting","metadata":{}},{"cell_type":"code","source":"# function to read images based on index\ndef image_array(index, size=image_size_training, path=images_data_path):\n    \"\"\"\n    This functions is for converting images to arrays to deal with it in the model.\n    \n    Input:  index of the image that we want to convert to array\n            size of the image that we want for the array of the image\n            path of the images data to get the image\n            \n    Output: the image array as numpy array\n    \"\"\"\n    # to get the path based on index\n    path = \"{}/{}\".format(str(path),str(df_org.iloc[index].image_id))\n    \n    # to read the image\n    image = PIL.Image.open(path).resize(size)\n    image_array = np.asarray(image)\n    \n    return image_array","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get a list of all key points of the face\ndef image_key_points_list(index, df = df_org):\n    \"\"\"\n    This function for getting the key points on the face as list to deal with it in plotting sections\n    \"\"\"\n    # box dictionary\n    points_list = [df.iloc[index].lefteye_x,\n                   df.iloc[index].lefteye_y,\n                   df.iloc[index].righteye_x,\n                   df.iloc[index].righteye_y,\n                   df.iloc[index].nose_x,\n                   df.iloc[index].nose_y,\n                   df.iloc[index].leftmouth_x,\n                   df.iloc[index].leftmouth_y,\n                   df.iloc[index].rightmouth_x,\n                   df.iloc[index].rightmouth_y]\n    \n    return points_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to plot the image with green box around the faces\ndef plotting_image_with_box(index, df = df_org, size=original_image_size):\n    \"\"\"\n    This function for plotting the image with points on facial features and box around the face\n    \"\"\"\n    test_image = image_array(index, size)\n    points_list = image_key_points_list(index, df)\n    \n    # face points\n    le_x, le_y, re_x, re_y = points_list[0], points_list[1], points_list[2], points_list[3]\n    n_x, n_y = points_list[4], points_list[5]\n    lm_x, lm_y, rm_x, rm_y = points_list[6], points_list[7], points_list[8], points_list[9]\n    \n    # Create figure and axes\n    fig, ax = plt.subplots()\n    # plot the image\n    ax.imshow(test_image)\n    # plot the points on the face\n    ax.plot([le_x,re_x,n_x,lm_x,rm_x], [le_y,re_y,n_y,lm_y,rm_y], 'bo-')\n    \n    # plot the box around the face\n    width = abs(le_x-rm_x-60)\n    height = abs(le_y-rm_y-75)\n    rect = patches.Rectangle((le_x-30, le_y-40), width, height, linewidth=4, edgecolor='g', facecolor='none')\n    ax.add_patch(rect);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nindex = 18\nplotting_image_with_box(index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Preparing the data for ML model","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Rescaling key points to be consistent with the new image size for training","metadata":{}},{"cell_type":"code","source":"# copy a version from the data to prepare it for analysis\ndf = df_org.copy()\n\n# check\ndf.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for updating key points for a new size\ndef rescale_key_points(oldsize=original_image_size, newsize=image_size_training):\n    \"\"\"\n    This function is for rescaling the key points from the original scale to a nwe scale from our chossen and we reduce\n    the image size to make the analysis faster and using lower memory\n    \"\"\"\n    # old and nwe sizes (x,y) values\n    x_axis_old = oldsize[0]\n    y_axis_old = oldsize[1]\n\n    x_axis_new = newsize[0]\n    y_axis_new = newsize[1]\n\n    x_ratio = x_axis_new / x_axis_old\n    y_ratio = y_axis_new / y_axis_old\n    \n    # converting the keypoints values to be trained with the new size of the images\n    keypoints_x = ['lefteye_x', 'righteye_x', 'nose_x', 'leftmouth_x', 'rightmouth_x']\n    keypoints_y = ['lefteye_y', 'righteye_y', 'nose_y', 'leftmouth_y', 'rightmouth_y']\n    \n    df[keypoints_x] = (df[keypoints_x] * x_ratio).astype('int')\n    df[keypoints_y] = (df[keypoints_y] * y_ratio).astype('int')\n    \n    return 0\n\n# call the function\nrescale_key_points()\n\n# check\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Split the data into training and test datasets","metadata":{}},{"cell_type":"code","source":"# training data\ntrain_labels = df[:images_data_size - 1000]\ntrain_images = images_data[:images_data_size - 1000]\n\n\n# test data (1000 sample)\ntest_labels = df[images_data_size - 1000 + 1:]\ntest_images = images_data[images_data_size - 1000 + 1:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Split the data into train and validation datasets","metadata":{}},{"cell_type":"code","source":"# droping image_id column as we will not use it in the model\ny = train_labels.drop(['image_id'], axis = 1)      # labels\nX = train_images                                   # images\n\n# check\ny.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to split the data into two sets (one for training and one for testing)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.17,random_state = 42)\n\n# check the ratio\nX_val.shape[0]/X_train.shape[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Building and training the model","metadata":{}},{"cell_type":"code","source":"# diminsions of the image in the traing process\nx_ = image_size_training[0]\ny_ = image_size_training[1]\n\n# build the model\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=8, kernel_size=(3, 3), padding='same', activation=\"relu\", input_shape=(y_,x_,3)))\nmodel.add(Conv2D(filters=8, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10, activation='relu'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the summary of the model layers\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting the model with our data\ntraining_process = model.fit(X_train, y_train, epochs=200, validation_data=(X_val, y_val), batch_size=300, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Test the model","metadata":{}},{"cell_type":"code","source":"def predictions_test_model(index):\n    img = tf.keras.preprocessing.image.load_img(\"{}/0{}.jpg\".format(images_data_path, index),target_size=(y_,x_,3))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = img/255\n    points_list = model.predict(img.reshape(1,y_,x_,3)).astype('int')[0]\n    \n    # converting key points values to the original size\n    x_ratio = 1.05 * (original_image_size[0] / image_size_training[0])\n    y_ratio = 1.085 * (original_image_size[1] / image_size_training[1])\n    \"\"\"\n    In the previous ratios we multiply them by contant to reduce the noise that happened when we rescaled the points in\n    the previous training, there is no meaning for these numbers (i just pick them with trails)\n    \"\"\"\n    \n    points_list[0] = int(points_list[0] * x_ratio)\n    points_list[2] = int(points_list[2] * x_ratio)\n    points_list[4] = int(points_list[4] * x_ratio)\n    points_list[6] = int(points_list[6] * x_ratio)\n    points_list[8] = int(points_list[8] * x_ratio)\n    \n    points_list[1] = int(points_list[1] * y_ratio)\n    points_list[3] = int(points_list[3] * y_ratio)\n    points_list[5] = int(points_list[5] * y_ratio)\n    points_list[7] = int(points_list[7] * y_ratio)\n    points_list[9] = int(points_list[9] * y_ratio)\n    \n    return points_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to plot the image with green box around the faces\ndef test_image_with_box_plot(index, pred_or_actual = 'pred', pointsColor='bo-' ,boxcolor='g'):\n    img = tf.keras.preprocessing.image.load_img(\"{}/0{}.jpg\".format(images_data_path, index),target_size=(y_org,x_org,3))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    test_image = img/255\n    \n    # predictions of key points on the face\n    if pred_or_actual == 'pred':\n        points_list = predictions_test_model(index)        # this for predections of the model\n    elif pred_or_actual == 'actual':\n        points_list = image_key_points_list(index)   # this for the actual labels of the test data\n    \n    # face points\n    le_x, le_y, re_x, re_y = points_list[0], points_list[1], points_list[2], points_list[3]\n    n_x, n_y = points_list[4], points_list[5]\n    lm_x, lm_y, rm_x, rm_y = points_list[6], points_list[7], points_list[8], points_list[9]\n\n    # Create figure and axes\n    fig, ax = plt.subplots()\n    # plot the image\n    ax.imshow(test_image)\n    # plot the points on the face\n    ax.plot([le_x,re_x,n_x,lm_x,rm_x], [le_y,re_y,n_y,lm_y,rm_y], pointsColor)\n    \n    # plot the box around the face\n    width = abs(le_x-rm_x-60)\n    height = abs(le_y-rm_y-75)\n    rect = patches.Rectangle((le_x-30, le_y-40), width, height, linewidth=4, edgecolor=boxcolor, facecolor='none')\n    ax.add_patch(rect);\n    return points_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the model performace\nindex = 34100\nprint('RED box for predections\\n') \nprint('GREEN box for actual labels\\n')\ntest_image_with_box_plot(index, pred_or_actual = 'pred', pointsColor='mo-' ,boxcolor='r')\ntest_image_with_box_plot(index, pred_or_actual = 'actual')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# losses of both training and validation sets\nloss = training_process.history['loss']\nval_loss = training_process.history['val_loss']\n\n# plot both losses\nplt.plot(loss)\nplt.plot(val_loss)\nplt.legend(['loss', 'val_loss']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy of the model\naccuracy = model.evaluate(X_val,y_val)[1] * 100\nprint(\"Accuracy of the model = \", round(accuracy,2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Save the model","metadata":{}},{"cell_type":"code","source":"# saving the model as h5 file\nmodel.save('model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}