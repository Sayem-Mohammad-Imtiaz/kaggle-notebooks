{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP with LSTM and shallow neural networks","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 1. Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this kernel I will work with NLP trying to classify movies with a possitive or negative feedback based on a preview review.\nI will create two different model, the first one with a shallow neural network and the second one with a LSTM neural network. Previously, we will do some standardization of the text and created some different models applied to the text. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 2. Import libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n#from string import punctuation\n\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n\n# NLP libraires\n\nimport gensim # pip install gensim\nfrom gensim.models.word2vec import Word2Vec # word2vec model gensim class\nTaggedDocument = gensim.models.doc2vec.TaggedDocument\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 3. Load data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, let´s load the different datasets, training and test. Both contain the reviews and the target, in this case 0  or 1 depending if the output is negative or possitive respectively. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv\", encoding='UTF-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Test.csv\", encoding='UTF-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df_train,df_test])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.label.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 4. Tokenization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, I will standardize the text with some different functions. Basically the text will be lowered and tokenized. I will also add the new text treated to new column. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df_train,df_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(text):\n    try:\n\n        text  = text.lower()\n        tokens = TweetTokenizer().tokenize(text)\n        \n        return tokens\n    except:\n        return 'NC'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process(data):\n    # progress_map is a variant of the map function plus a progress bar.\n    # Handy to monitor DataFrame creations:\n    data['tokens'] = data['text'].progress_map(tokenizer)\n    data = data[data.tokens != 'NC']\n    data.reset_index(inplace=True)\n    data.drop('index', inplace=True, axis=1)\n    return data\n\ndata = process(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 5. Training and test set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Once the text has been standarzided, I will create again both sets, training and test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_train = data[\"tokens\"][:len(df_train)].values\ntokenized_test = data[\"tokens\"][len(df_train):].values\ny_train = data[\"label\"][:len(df_train)].values\ny_test = data[\"label\"][len(df_train):].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenized_train[0])\nprint(y_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 6. Word2Vec model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next, I will create a word vector model. The vector dimension will be 40 and the minimun count, 5. \n\nThe words will be vectorized and trained with the training dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_dim = 40\n\n\n# Model word2vec:\ntext_w2v = Word2Vec(size=vec_dim, min_count=5) \n\n# Building the vocabulary:\ntext_w2v.build_vocab(tokenized_train)\n\n# Training the model:\ntext_w2v.train(tokenized_train, total_examples=text_w2v.corpus_count, epochs=text_w2v.epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_w2v.save(\"text_w2v.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let´s represent some of the words with the trained model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_w2v.wv['good']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_w2v.wv.most_similar('good')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_w2v.wv.most_similar('mountain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_w2v.wv.most_similar('actor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_w2v.wv.most_similar('amazing')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 7. t-SNE algorithm and visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can also use the algorithm t-SNE to visualize and reduce the vector to two main dimensions. \n\nIn the link below you can find more information related to the algorithm: \n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing bokeh library for interactive data visualization\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.plotting import figure, show, output_notebook\n\n# defining the chart\noutput_notebook()\nfig = bp.figure(plot_width=700, plot_height=600, title=\"Map of word vectors\",\n                tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n                x_axis_type=None, y_axis_type=None, min_border=1)\n\n# getting a list of word vectors. limit to 2500. each is of 40 dimensions\nword_vectors = [text_w2v.wv[w] for w in list(text_w2v.wv.vocab.keys())[:2500]]\n\n# dimensionality reduction. converting the vectors to 2d vectors\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, early_exaggeration=10, random_state=0, init='pca')\ntsne_w2v = tsne_model.fit_transform(word_vectors)\n\n# putting everything in a dataframe\ntsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\ntsne_df['words'] = list(text_w2v.wv.vocab.keys())[:2500]\n\n# plotting. the corresponding word appears when you hover on the data point.\nfig.scatter(x='x', y='y', source=tsne_df)\nhover = fig.select(dict(type=HoverTool))\nhover.tooltips={\"word\": \"@words\"}\nshow(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 8. Models","execution_count":null},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# <font color=\"#ff0000\"> Model 1: Shallow neural network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will reduce each review in a single vextor but first let´s create a TF-IDF model. It will calculate each word weight for each review.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text_w2v.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('building tf-idf matrix ...')\nvectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=5)\nvectorizer.fit(tokenized_train)\nIDFs = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\nprint('size of vocabulary obtained with TfidfVectorizer:', len(IDFs))\nprint('size of vocabulary obtained with word2vec:', len(text_w2v.wv.vocab))\nprint(\"Some idfs:\")\naux = list(IDFs.items())\nfor i in list(range(3))+list(range(1000,1005)):\n    print(\"  \", aux[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IDFs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also save the TF-IDF model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open(\"IDFs.pkl\", \"wb\") as f:\n    pickle.dump(IDFs, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next function we apply Wor2Vec taking into consideration the word weights.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def Text2Vec(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += text_w2v.wv[word].reshape((1, size)) * IDFs[word]\n            count += 1.\n        except KeyError: # handling the case where the token is not\n                         # in the corpus. useful for testing.\n            continue\n    if count != 0:\n        vec /= count\n    return vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We apply the function to each review, getting as output a vector of length 20.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_vecs_train = np.zeros((len(tokenized_train), vec_dim ))\nfor i,x in tqdm(enumerate(tokenized_train)):\n    text_vecs_train[i] = Text2Vec(x, vec_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_vecs_test = np.zeros((len(tokenized_test), vec_dim))\nfor i,x in tqdm(enumerate(tokenized_test)):\n    text_vecs_test[i] = Text2Vec(x, vec_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(text_vecs_train.shape)\nprint(text_vecs_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also represent the reviews with its label using PCA. It will create two different clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, normalize\n\nscaler = StandardScaler()\nscaler.fit(text_vecs_train)\ntext_vecs_train_sc = scaler.transform(text_vecs_train)\ntext_vecs_test_sc  = scaler.transform(text_vecs_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\nX_pca_train = pca.fit_transform(text_vecs_train_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool, LabelSet, ColumnDataSource, Range1d\n\n#from bokeh.plotting import figure, show, output_notebook\n\npc_x = 0\npc_y = 1\n\nn_visualizar_por_clase = 5000\n\npcs_names = [\"main component \"+str(i+1) for i in range(vec_dim)]\n\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nmarkers = ['o', 's']\n\n# defining the chart\noutput_notebook()\np = bp.figure(plot_width=700, plot_height=600, title=\"Text vecs, PCA space\",\n              tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n              x_axis_label=pcs_names[pc_x],\n              y_axis_label=pcs_names[pc_y],              \n              #x_axis_type=None, y_axis_type=None,\n              min_border=1)\np.title.text_font_size = '16pt'\np.xaxis.axis_label_text_font_style='normal'\np.xaxis.axis_label_text_font_size='16pt'\np.yaxis.axis_label_text_font_style='normal'\np.yaxis.axis_label_text_font_size='16pt'\n\np.xgrid.visible = False\np.ygrid.visible = False\n\nfor label,color,marker in zip(np.unique(y_train),colors,markers):\n    inds = np.where(y_train==label)[0][:n_visualizar_por_clase]\n    dictf = {'x':X_pca_train[inds,pc_x],\n             'y':X_pca_train[inds,pc_y],\n             'Class':len(inds)*[label],\n             'Text':[\" \".join(a) for a in tokenized_train[inds]],\n             'row':inds}\n    p.scatter(x='x', y='y', source=ColumnDataSource(dictf), color=color,\n              legend='Class {}'.format(label), alpha=0.1)\n    hover = p.select(dict(type=HoverTool))\n    \n    hover.tooltips={\"Class\":\"@Class\",\n                    \"Text\":\"@Text\",\n                    \"row\":\"@row\"}\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, I create and train the neural network.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_graph(tr_acc, val_acc):\n    ax=plt.figure(figsize=(10,4)).gca()\n    plt.plot(1+np.arange(len(tr_acc)), 100*np.array(tr_acc))\n    plt.plot(1+np.arange(len(val_acc)), 100*np.array(val_acc))\n    plt.title('Model hit rate (%)', fontsize=18)\n    plt.ylabel('Hit rate (%)', fontsize=18)\n    plt.xlabel('Epoch', fontsize=18)\n    plt.legend(['Training', 'Validation'], loc='upper left')\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel1 = Sequential()\nmodel1.add(Dense(32, activation='relu', input_dim=vec_dim))\nmodel1.add(Dense(1, activation='sigmoid'))\nmodel1.compile(optimizer='rmsprop',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nfrom keras.callbacks import ModelCheckpoint\n\nBATCH_SIZE = 32\nnepochs = 15\nTRAIN1 = True\nfilepath1 = \"best_model1.h5\"\n\n\nif TRAIN1:\n    acum_tr_acc = []\n    acum_val_acc = []\n    checkpoint = ModelCheckpoint(filepath1, monitor='val_accuracy', verbose=1,\n                                 save_best_only=True,\n                                 mode='max') \n    callbacks_list = [checkpoint]\n    \n    for i in range(nepochs):\n        history = model1.fit(text_vecs_train_sc, y_train,\n                             batch_size=BATCH_SIZE,\n                             epochs=1,\n                             verbose=2,\n                             callbacks=callbacks_list,\n                             validation_split=0.3,\n                             shuffle=False,\n                            )\n    \n        acum_tr_acc = acum_tr_acc + history.history['accuracy']\n        acum_val_acc = acum_val_acc + history.history['val_accuracy']\n        if len(acum_tr_acc) > 1:\n            training_graph(acum_tr_acc, acum_val_acc)\n\nmodel1 = load_model(filepath1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_prob1 = model1.predict(text_vecs_train_sc)\ny_test_pred_prob1 = model1.predict(text_vecs_test_sc)\n\ny_train_pred1 = y_train_pred_prob1.round()\ny_test_pred1  = y_test_pred_prob1.round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy (training): %.2f%%\" % (100*accuracy_score(y_train, y_train_pred1)))\nprint(\"Accuracy (test): %.2f%%\"     % (100*accuracy_score(y_test,  y_test_pred1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#ff0000\"> Model 2: LSTM neural network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For this model, I will create word sequences for each review. Basically, the review will be codified as a word sequences based on the word vector model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"index2word = list(text_w2v.wv.vocab)\nembedding_matrix = text_w2v.wv[index2word]\n\npadding_vector = np.zeros((1, vec_dim))\nindex2word.insert(0,'()')\nindex2word = np.array(index2word)\n\nword2index = dict(zip(index2word, range(len(index2word))))\n\nembedding_matrix = np.vstack((padding_vector, embedding_matrix))\n\nprint(np.shape(index2word))\nprint(np.shape(embedding_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index2word[253]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2index[\"square\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenized_train[30][:5])\n\nword2index[tokenized_train[30][4]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def code_word(y):\n    try:\n        return word2index[y]\n    except KeyError:\n        return\n\nX_train_coded = []\nfor text in tokenized_train:\n    X_train_coded.append([y for y in [code_word(x) for x in text] if y != None])\n\nX_test_coded = []\nfor text in tokenized_test:\n    X_test_coded.append([y for y in [code_word(x) for x in text] if y != None])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenized_train[1])\nX_train_coded[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index2word[X_train_coded[1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = [len(x) for x in X_train_coded]\ni = np.argmax(aux)\ntext_mas_palabras = X_train_coded[i]\nprint(\"Maximum number of words in text:\", len(text_mas_palabras))\nprint(\"Text:\", tokenized_train[i])\nprint(\"Text (word codes):\", text_mas_palabras)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating an histogram of the number of words in each review, we can see the distribution and have an idea of the whole dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nf = sns.countplot(aux)\nplt.axis([0,400,0,300])\nplt.title(\"Histogram: number of words\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we define the number of word per review. All the reviews will have the same length.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words_text = 400\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nX_train_pad = pad_sequences(X_train_coded, maxlen=max_words_text)\nX_test_pad  = pad_sequences(X_test_coded,  maxlen=max_words_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pad[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let´s create the LSTM model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.embeddings import Embedding\nfrom keras.layers import LSTM\n\nembedding_layer = Embedding(embedding_matrix.shape[0],\n                            embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            input_length=max_words_text,\n                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_dropout=False\n\n# create the model\nmodel2 = Sequential()\n#model2.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length,\n#                    activity_regularizer='l2'))\nmodel2.add(embedding_layer)\nif set_dropout:\n    model2.add(Dropout(0.2))\n#model2.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2))\nmodel2.add(LSTM(20, return_sequences=True))\nmodel2.add(LSTM(20))\nif set_dropout:\n    model2.add(Dropout(0.2))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy']) #    'adam'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_pad.shape)\nprint(X_test_pad.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\n\nBATCH_SIZE = 256\nnepochs = 5\n#TRAIN2 = False\nTRAIN2 = True\nfilepath2 = \"best_model2.h5\"\n\n\nif TRAIN2:\n    acum_tr_acc = []\n    acum_val_acc = []\n    checkpoint = ModelCheckpoint(filepath2, monitor='val_accuracy', verbose=1,\n                                 save_best_only=True,\n                                 mode='max') \n    callbacks_list = [checkpoint]\n    \n    for i in range(nepochs):\n        history = model2.fit(X_train_pad, y_train,\n                             batch_size=BATCH_SIZE,\n                             epochs=1,\n                             verbose=1,\n                             callbacks=callbacks_list,\n                             validation_split=0.3,\n                             shuffle=False,\n                            )\n    \n        acum_tr_acc = acum_tr_acc + history.history['accuracy']\n        acum_val_acc = acum_val_acc + history.history['val_accuracy']\n        if len(acum_tr_acc) > 1:\n            training_graph(acum_tr_acc, acum_val_acc)\n\nmodel2 = load_model(filepath2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_prob2 = model2.predict(X_train_pad)\ny_test_pred_prob2  = model2.predict(X_test_pad)\n\ny_train_pred2 = y_train_pred_prob2.round()\ny_test_pred2  = y_test_pred_prob2.round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_train2 = accuracy_score(y_train, y_train_pred2)\nscore_test2  = accuracy_score(y_test,  y_test_pred2)\n\nprint(\"Accuracy (training): %.2f%%\" % (100*score_train2))\nprint(\"Accuracy (test)    : %.2f%%\" % (100*score_test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#fcc200\"> 9. Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Both model have a similar accuracy. I just runned the modesl without playing so much with the tunning parameters or the word vector model but surely we could get better results experimenting with those parameters. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}