{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n!ls '/kaggle/input'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nfrom collections import Counter\nimport itertools\nimport torch\nfrom sklearn import model_selection\nimdb_df = pd.read_csv('/kaggle/input/imdb-review-dataset/imdb_master.csv', encoding='latin-1')\ndev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, label_id):\n        self.input_ids = input_ids\n        self.label_id = label_id\nclass Vocab:\n    def __init__(self, itos, unk_index):\n        self._itos = itos\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()\nclass SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size\n        \n    def tokenize(self, text):\n        return nltk.tokenize.word_tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform(self, texts):\n        result = []\n        for text in texts:\n            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in texts]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tokenized_texts:\n            tokens = ['<S>'] + tokens + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])\ndef features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}\ntext2id = SimpleTextTransformer(10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset,DataLoader\nfrom torch.utils import data\nbatch_size = 64\n\n\ntrain_tensor, train_labels = features_to_tensor(train_features)\nval_tensor,     val_labels = features_to_tensor(val_features)\ntest_tensor,   test_labels = features_to_tensor(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TensorDataset(train_tensor, train_labels)\nval_dataset   = TensorDataset(val_tensor, val_labels)\ntest_dataset  = TensorDataset(test_tensor, test_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size)\nval_loader   = DataLoader(val_dataset, batch_size = batch_size)\ntest_loader  = DataLoader(test_dataset, batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass network(nn.Module):\n    def __init__(self):\n        super(network, self).__init__()\n        self.preproc = nn.Sequential(\n            nn.Embedding(10000,100)\n        )\n        self.hidden = nn.Sequential(\n            nn.Conv1d(in_channels=100, out_channels=60, kernel_size=3), \n            nn.ReLU(),     \n            nn.Conv1d(in_channels=60, out_channels=100, kernel_size=3), \n            nn.ReLU(), \n            nn.MaxPool1d(10))\n        \n        self.output = nn.Sequential(\n            nn.Linear(1900,1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        batch = x.size(0)\n        x = self.preproc(x)\n        x = x.transpose(2,1)\n        \n        y = self.hidden(x).view(batch, -1)\n        return  self.output(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ndef fit(net,crit,train_loader,val_loader,optimizer, epochs):\n    best=0\n    #net.cuda()\n    for i in range(epochs):\n        tr_loss = 0\n        val_loss = 0\n        val_accuracy =0\n        for xx,yy in train_loader:\n            #xx, yy = xx.cuda(), yy.cuda()\n            optimizer.zero_grad()\n            y = net.forward(xx)\n            loss = crit(y,yy.float().view(len(yy),-1))\n            tr_loss += loss\n            loss.backward()\n            optimizer.step()\n        tr_loss /= len(train_loader)\n        with torch.no_grad():\n            for xx,yy in val_loader:\n                all_preds = []\n                #xx, yy = xx.cuda(), yy.cuda()\n                y = net.forward(xx)\n                loss = crit(y,yy.float().view(len(yy),-1))\n                val_loss += loss\n                for index in y:\n                    if index>0.5:\n                        all_preds.append(1)\n                    else:\n                        all_preds.append(0)\n                yy = yy.cpu().numpy()\n                val_accuracy += accuracy_score(all_preds,yy)\n            val_accuracy /= len(val_loader)\n            if val_accuracy>best:\n                best = val_accuracy\n                torch.save(net.state_dict(), \"../model.py\")\n        print((\"epoch:%d, train loss:%f, validation accuracy:%f\" % (i,tr_loss.item(),val_accuracy.item())))\n    net.cpu()\n    print(\"Train ended. Best accuracy is %f\" % float(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = network()\nfrom torch.optim import Adam\ncriterion = nn.BCELoss()\noptimizer = Adam(model.parameters(), lr=0.005)\nfit(model,criterion,train_loader,val_loader,optimizer,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nall_preds = []\ncorrect_preds = []\nwith torch.no_grad():\n    model.eval()\n    for xx, yy in test_loader:\n        #model.cuda()\n        #xx = xx.cuda()\n        output = model.forward(xx)\n        for i in output:\n            if i>0.5:\n                all_preds.append(1)\n            else:\n                all_preds.append(0)\n        correct_preds.extend(yy.tolist())\n\nprint(metrics.classification_report(correct_preds, all_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ndf = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\")\ndf = df.drop(['Unnamed: 0','file'],axis=1)\ndf.columns = ['type',\"review\",\"sentiment\"]\ndf.head()\nprint(df.head())\ndf = df[df.sentiment != 'unsup']\ndf['sentiment'] = df['sentiment'].map({'pos': 1, 'neg': 0})\n\ndf_train = df[df.type == 'train']\ndf_test = df[df.type == 'test']\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef ngram_vectorize(train_texts, train_labels, val_texts):\n    kwargs = {\n        'ngram_range' : (1, 2),\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : 'word',\n        'min_df' : 2,\n    }\n    \n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    x_train = tfidf_vectorizer.fit_transform(train_texts)\n    x_val = tfidf_vectorizer.transform(val_texts)\n    \n    selector = SelectKBest(f_classif, k=min(6000, x_train.shape[1]))\n    selector.fit(x_train, train_labels)\n    x_train = selector.transform(x_train).astype('float32')\n    x_val = selector.transform(x_val).astype('float32')\n    return x_train, x_val\n\ndf_bag_train, df_bag_test = ngram_vectorize(df_test['review'], df_test['sentiment'], df_train['review'])\ndf_bag_train, df_bag_test = ngram_vectorize(df_test['review'], df_test['sentiment'], df_train['review'])\nfrom sklearn import metrics\n\nnb = MultinomialNB()\nnb.fit(df_bag_train, df_train['sentiment'])\nnb_pred = nb.predict(df_bag_test)\nprint(metrics.classification_report(df_test['sentiment'], nb_pred))\ncm = metrics.confusion_matrix(nb_pred, df_test['sentiment'])\nprint('Accuracy ',metrics.accuracy_score(df_test['sentiment'], nb_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}