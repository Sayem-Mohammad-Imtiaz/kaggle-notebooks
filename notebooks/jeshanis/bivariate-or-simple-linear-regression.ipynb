{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Simple Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"If you start with machine learning, linear regression models are the first predictive models you may learn. Regression models estimate the nature of the relationship between independent and dependent variables. Although they are conceptually simple, they have some key features that make them flexible, powerful and easy to explain. \n\nWhile newer and conceptually more complicated models can outperform linear regression, linear models are still widely used, especially where data collection can be expensive and highly interpretable models are of considerable value. Extensions to linear regression such as ridge and lasso can help to avoid over fitting in feature-rich models and even perform feature selection. Logistic regression adapts the linear frame to classification problems. \n\nThis is a very simple demo for Vanilla Linear Regression Model (LRM). Let’s look at how a plane-vanilla linear regression works."},{"metadata":{},"cell_type":"markdown","source":"- ***weatherww2 Dataset for regression analysis***\n\nYou can find detailed data [here][1].\n\n\n[1]: https://www.kaggle.com/smid80/weatherww2/data\n\n\n- ***Goal***\n  - Find a relationship between minimum and maximum temperature.\n  - Predict maximum temperature by given the minimum temperature."},{"metadata":{},"cell_type":"markdown","source":"> ## Reading Data and Extracting the Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some usefull packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dataset\ndf = pd.read_csv('../input/weatherww2/Summary of Weather.csv')\n\n# Selecting min and max temperature columns\ndf = df[['MinTemp', 'MaxTemp']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To experiment simple linear regression, two features `MinTemp` and `MaxTemp` are extracted from this dataset. "},{"metadata":{},"cell_type":"markdown","source":"Let's now visualise our target and pradictor variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plot\nplt.figure(figsize=(10, 5))\nplt.scatter(df['MinTemp'], df['MaxTemp'],s=10)\nplt.xlabel('Min Temperature °C',fontsize=15)\nplt.ylabel('Max Temperature °C',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above graph showing the scatter data points of dependent variable Maximum Temperature and independent variable Minimum Temperature. With one predictor variable it looks like a line but there are few data points which are deviating from normal trend. \n\nIn linear regression, outliers can greatly affect the regression (the slope, r-value, and r-squared).  It may be best to remove them from linear regression."},{"metadata":{},"cell_type":"markdown","source":"- Outlier treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop anomalies data points\ndf.drop(df[(df['MinTemp'] < -15) & (df['MaxTemp'] > 15)].index, inplace = True)\ndf.drop(df[(df['MinTemp'] > 8) & (df['MaxTemp'] < -15)].index, inplace = True)\n\n# Scatter plot after removing anomalies datapoint\nplt.figure(figsize=(10, 5))\nplt.scatter(df['MinTemp'], df['MaxTemp'],s=10)\nplt.xlabel('Min Temperature °C',fontsize=15)\nplt.ylabel('Max Temperature °C',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is, like every real world data set, a little noisy, but there’s clearly a trend: as we increase x, y increases as well. Perhaps this relationship can be well estimated with a line. Let’s get a sense for how the model works."},{"metadata":{},"cell_type":"markdown","source":"## Performing Simple Linear Regression\n\nA linear model attempts to find the simplest relationship between a feature variable and the output as possible. Often this is described as ‘fitting a line’.\n\nEquation of linear regression<br>\n$y = c + m_1x_1 + m_2x_2 + ... + m_nx_n$\n\n-  $y$ is the response\n-  $c$ is the intercept\n-  $m_1$ is the coefficient for the first feature\n-  $m_n$ is the coefficient for the nth feature<br>\n\nFor each unit we increase x, y increases by m units (or decreases if m is negative). The term c is an intercept term which shifts our line up or down without changing the slope.\n\nIn our case:\n\n$MaxTemp = c + m_1 \\times MinTemp$\n\nThe $m$ values are called the model **coefficients** or **model parameters**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plot with few possible regression lines\nplt.figure(figsize=(10, 5))\nplt.scatter(df['MinTemp'], df['MaxTemp'],s=10)\nplt.xlabel('Min Temperature °C',fontsize=15)\nplt.ylabel('Max Temperature °C',fontsize=15)\n\nx1 = [-38,35]\ny1 = [-32,53]\nplt.plot(x1, y1, color='orange')\n\nx2 = [-38,35]\ny2 = [-27,44]\nplt.plot(x2, y2, color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which line seems to capture the trend best? That is not necessarily clear. The orange line seems to be closest to the points on the left, but then, as we approach the center of the distribution, it is not clear. On the right side, the orange line may have crossed the mark and is too high. But how do we choose which line?"},{"metadata":{},"cell_type":"markdown","source":"### Generic Steps in model building using `statsmodels`\n\nWe first assign the feature variable, `MinTemp`, in this case, to the variable `X` and the response variable, `MaxTemp`, to the variable `y`."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df['MinTemp']\ny = df['MaxTemp']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train-Test Split\n\nWe now need to split our variable into training and testing sets. We perform this by importing `train_test_split` from the `sklearn.model_selection` library. It is usually a good practice to keep 70% of the data in our train dataset and the rest 30% in our test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now take a look at the train dataset\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building a Linear Model\n\nFirst need to import the `statsmodel.api` library using which we will perform the linear regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By default, the `statsmodels` library fits a line on the dataset which passes through the origin. But in order to have an intercept, we need to manually use the `add_constant` attribute of `statsmodels`. And once we've added the constant to our `X_train` dataset, we can go ahead and fit a regression line using the `OLS` (Ordinary Least Squares) attribute of `statsmodels` as shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant to get an intercept\nX_train_sm = sm.add_constant(X_train)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y_train, X_train_sm).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the parameters, i.e. the intercept and the slope of the regression line fitted\nlr.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing a summary operation lists out all the different parameters of the regression line fitted\nprint(lr.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Looking at some key statistics from the summary"},{"metadata":{},"cell_type":"markdown","source":"The values we are concerned with are - \n1. The coefficients and significance (p-values)\n2. R-squared and Adjusted R-squared \n3. F statistic and its significance"},{"metadata":{},"cell_type":"markdown","source":"##### 1. The coefficient for MinTemp has a very low p value\nThe coefficient is statistically significant. So the association is not purely by chance. "},{"metadata":{},"cell_type":"markdown","source":"##### 2. R-squared and Adjusted R-squared are 0.778\nMeaning that 77.8% of the variance in `MaxTemp` is explained by `MinTemp`\n\nThis is a decent R-squared value.\n\nSince there is only one independent variable, adjusted R-squared is same as absolute R-squared."},{"metadata":{},"cell_type":"markdown","source":"###### 3. F statistic has a very low p value (practically low)\nMeaning that the model fit is statistically significant, and the explained variance isn't purely by chance."},{"metadata":{},"cell_type":"markdown","source":"The fit is significant. Let's visualize how well the model fit the data.\n\nFrom the parameters that we get, our linear regression equation becomes:\n\n$ MaxTemp = 10.67 + 0.92 \\times MinTemp $"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Best fit line\nplt.figure(figsize=(10, 5))\nplt.scatter(X_train, y_train, s=10)\nplt.plot(X_train, 10.6760 + 0.9201*X_train, 'r')\nplt.xlabel('Min Temperature °C',fontsize=15)\nplt.ylabel('Max Temperature °C',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Residual analysis \nTo validate assumptions of the model, and hence the reliability for inference"},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of the error terms\nWe need to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = lr.predict(X_train_sm)\nres = (y_train - y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 4))\nsns.distplot(res, bins = 15)\nfig.suptitle('Error Terms', fontsize = 15)                  # Plot heading \nplt.xlabel('y_train - y_train_pred', fontsize = 15)         # X-label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The residuals are following the normally distributed with a mean 0. All good!"},{"metadata":{},"cell_type":"markdown","source":"#### Looking for patterns in the residuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.scatter(X_train,res)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are confident that the model fit isn't by chance, and has decent predictive power. The normality of residual terms allows some inference on the coefficients.\n\nAlthough, the variance of residuals increasing with X indicates that there is significant variation that this model is unable to explain."},{"metadata":{},"cell_type":"markdown","source":"As we can see, the regression line is a pretty good fit to the data"},{"metadata":{},"cell_type":"markdown","source":"## Predictions on the Test Set\n\nNow that we have fitted a regression line on our train dataset, it's time to make some predictions on the test data. For this, we first need to add a constant to the `X_test` data like we did for `X_train` and then we can simply go on and predict the y values corresponding to `X_test` using the `predict` attribute of the fitted regression line."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant to X_test\nX_test_sm = sm.add_constant(X_test)\n\n# Predict the y values corresponding to X_test_sm\ny_pred = lr.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Looking at the MSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns the root mean squared error\nmean_squared_error(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Looking at the RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns the mean squared error\nnp.sqrt(mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Checking the R-squared on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"r_squared = r2_score(y_test, y_pred)\nr_squared","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualizing the fit on the test set"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.scatter(X_test, y_test, s=10)\nplt.plot(X_test, 10.6760 + 0.9201*X_test, 'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression using `linear_model` in `sklearn`\n\nApart from `statsmodels`, there is another package namely `sklearn` that can be used to perform linear regression. We will use the `linear_model` library from `sklearn` to build the model. Since, we hae already performed a train-test split, we don't need to do it again.\n\nThere's one small step that we need to add, though. When there's only a single feature, we need to add an additional column in order for the linear regression fit to be performed successfully."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_lm, X_test_lm, y_train_lm, y_test_lm = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lm = X_train_lm.values.reshape(-1,1)\nX_test_lm = X_test_lm.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_lm.shape)\nprint(y_train_lm.shape)\nprint(X_test_lm.shape)\nprint(y_test_lm.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Representing LinearRegression as lr(Creating LinearRegression Object)\nlm = LinearRegression()\n\n# Fit the model using lr.fit()\nlm.fit(X_train_lm, y_train_lm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.intercept_)\nprint(lm.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The equationwe get is the same as what we got before!\n\n$ MaxTemp = 10.67 + 0.92 \\times MinTemp $"},{"metadata":{},"cell_type":"markdown","source":"Sklearn linear model is useful as it is compatible with a lot of sklearn utilites (cross validation, grid search etc.)"},{"metadata":{},"cell_type":"markdown","source":"---\n                                   End\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}