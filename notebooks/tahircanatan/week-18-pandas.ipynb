{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T17:22:23.503375Z","iopub.execute_input":"2021-06-02T17:22:23.503783Z","iopub.status.idle":"2021-06-02T17:22:23.512137Z","shell.execute_reply.started":"2021-06-02T17:22:23.503746Z","shell.execute_reply":"2021-06-02T17:22:23.511196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:23.513808Z","iopub.execute_input":"2021-06-02T17:22:23.514153Z","iopub.status.idle":"2021-06-02T17:22:24.005163Z","shell.execute_reply.started":"2021-06-02T17:22:23.514125Z","shell.execute_reply":"2021-06-02T17:22:24.003909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Read the dataset as a dataframe. Create a copy of your dataframe. Solve the rest of the questions using this dataframe copy.","metadata":{}},{"cell_type":"code","source":"tdf = df.copy()\ntdf\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.00802Z","iopub.execute_input":"2021-06-02T17:22:24.008509Z","iopub.status.idle":"2021-06-02T17:22:24.127221Z","shell.execute_reply.started":"2021-06-02T17:22:24.008456Z","shell.execute_reply":"2021-06-02T17:22:24.126093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(tdf))\nprint(\"********************\")\nprint(tdf.shape)  #satir ve sutun sayilari hakkinda bilgi verir\nprint(\"********************\")\nprint(tdf.size) # icinde kac eleman oldugunu soyler\nprint(\"********************\")\nprint(tdf.dtypes)\nprint(\"********************\")\nprint(tdf.index)  # index hakkinda bize bilgiler verir\nprint(\"********************\")\nprint(tdf.columns)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.128642Z","iopub.execute_input":"2021-06-02T17:22:24.128954Z","iopub.status.idle":"2021-06-02T17:22:24.140329Z","shell.execute_reply.started":"2021-06-02T17:22:24.128925Z","shell.execute_reply":"2021-06-02T17:22:24.139517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Find out the describe and info attributes of the dataframe. Analyze these information and create a short write-up according to your findings.","metadata":{}},{"cell_type":"code","source":"tdf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.141284Z","iopub.execute_input":"2021-06-02T17:22:24.141531Z","iopub.status.idle":"2021-06-02T17:22:24.319938Z","shell.execute_reply.started":"2021-06-02T17:22:24.141506Z","shell.execute_reply":"2021-06-02T17:22:24.318822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.321319Z","iopub.execute_input":"2021-06-02T17:22:24.321673Z","iopub.status.idle":"2021-06-02T17:22:24.430211Z","shell.execute_reply.started":"2021-06-02T17:22:24.321639Z","shell.execute_reply":"2021-06-02T17:22:24.429074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Find out the shape and size info of the dataset.","metadata":{}},{"cell_type":"code","source":"tdf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.431588Z","iopub.execute_input":"2021-06-02T17:22:24.43193Z","iopub.status.idle":"2021-06-02T17:22:24.43751Z","shell.execute_reply.started":"2021-06-02T17:22:24.431889Z","shell.execute_reply":"2021-06-02T17:22:24.436456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf.size","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.440094Z","iopub.execute_input":"2021-06-02T17:22:24.440502Z","iopub.status.idle":"2021-06-02T17:22:24.451691Z","shell.execute_reply.started":"2021-06-02T17:22:24.440469Z","shell.execute_reply":"2021-06-02T17:22:24.450467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Find out the types values of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"dtypes=tdf.dtypes\ndtypes = pd.DataFrame(dtypes, columns = ['dtypes'] )\ndtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.454934Z","iopub.execute_input":"2021-06-02T17:22:24.455235Z","iopub.status.idle":"2021-06-02T17:22:24.469359Z","shell.execute_reply.started":"2021-06-02T17:22:24.455206Z","shell.execute_reply":"2021-06-02T17:22:24.468527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5) Find out the non-null counts of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"nonnull = pd.DataFrame(tdf.count(), columns = ['nonnull'] )\nnonnull","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.470366Z","iopub.execute_input":"2021-06-02T17:22:24.470655Z","iopub.status.idle":"2021-06-02T17:22:24.584303Z","shell.execute_reply.started":"2021-06-02T17:22:24.470604Z","shell.execute_reply":"2021-06-02T17:22:24.583375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n6. Find out the null counts of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"isnull = pd.DataFrame(tdf.isnull().sum(), columns = ['isnull'] )\nisnull","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.585548Z","iopub.execute_input":"2021-06-02T17:22:24.585865Z","iopub.status.idle":"2021-06-02T17:22:24.690092Z","shell.execute_reply.started":"2021-06-02T17:22:24.585834Z","shell.execute_reply":"2021-06-02T17:22:24.689334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7) Find out the unique counts of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"nununique = pd.DataFrame(tdf.nunique(), columns = ['nununique'] )\nnununique","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.691249Z","iopub.execute_input":"2021-06-02T17:22:24.691759Z","iopub.status.idle":"2021-06-02T17:22:24.907956Z","shell.execute_reply.started":"2021-06-02T17:22:24.691726Z","shell.execute_reply":"2021-06-02T17:22:24.906954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8) Merge the dataframes you created in questions 4-5-6-7. Expected output:","metadata":{}},{"cell_type":"code","source":"frames = [ dtypes, nonnull, isnull, nununique]\nframes1= pd.concat((frames), axis = 1, ignore_index = False)\nframes1.reset_index(inplace=True)\nframes1=frames1.rename(columns={'index':\"Columns\"})\nframes1","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.909333Z","iopub.execute_input":"2021-06-02T17:22:24.909676Z","iopub.status.idle":"2021-06-02T17:22:24.929071Z","shell.execute_reply.started":"2021-06-02T17:22:24.909619Z","shell.execute_reply":"2021-06-02T17:22:24.927804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9) Lowercase all column names.","metadata":{}},{"cell_type":"code","source":"tdf.columns = map(str.lower, tdf.columns)\ntdf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:24.930613Z","iopub.execute_input":"2021-06-02T17:22:24.930971Z","iopub.status.idle":"2021-06-02T17:22:25.044969Z","shell.execute_reply.started":"2021-06-02T17:22:24.930937Z","shell.execute_reply":"2021-06-02T17:22:25.043727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"10) Change all the No values to NoRain and all the Yes values to Rain in raintoday and raintomorrow columns.","metadata":{}},{"cell_type":"code","source":"tdf.raintoday.replace('No', 'noRain', inplace=True)\ntdf.raintomorrow.replace('No', 'noRain', inplace=True)\ntdf.raintoday.replace('Yes', 'Rain', inplace=True)\ntdf.raintomorrow.replace('Yes', 'Rain', inplace=True)\ntdf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:25.046432Z","iopub.execute_input":"2021-06-02T17:22:25.046844Z","iopub.status.idle":"2021-06-02T17:22:25.179249Z","shell.execute_reply.started":"2021-06-02T17:22:25.046802Z","shell.execute_reply":"2021-06-02T17:22:25.178162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"11) Change the data type of \"date\" (object) column to datetime64 and reformat the date as DD/MM/YYYY.","metadata":{}},{"cell_type":"code","source":"tdf['date'] = pd.to_datetime(tdf['date'], format='%Y-%m-%d').dt.strftime('%d/%m/%Y')\ntdf\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:25.18079Z","iopub.execute_input":"2021-06-02T17:22:25.181144Z","iopub.status.idle":"2021-06-02T17:22:26.44638Z","shell.execute_reply.started":"2021-06-02T17:22:25.181107Z","shell.execute_reply":"2021-06-02T17:22:26.445294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"13) Create a new column called \"difference\", calculate the difference between maxtemp and mintemp columns for each row, and store the value in this new column.","metadata":{}},{"cell_type":"code","source":"tdf['difference'] = tdf['maxtemp'] - tdf['mintemp']\ntdf.difference","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:26.447931Z","iopub.execute_input":"2021-06-02T17:22:26.448346Z","iopub.status.idle":"2021-06-02T17:22:26.458444Z","shell.execute_reply.started":"2021-06-02T17:22:26.448302Z","shell.execute_reply":"2021-06-02T17:22:26.457364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"14) Remove the evaporation and sunshine columns from the dataset permanently.","metadata":{}},{"cell_type":"code","source":"tdf.drop(['evaporation', 'sunshine'], axis=1, inplace=True)\ntdf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:26.459682Z","iopub.execute_input":"2021-06-02T17:22:26.460063Z","iopub.status.idle":"2021-06-02T17:22:26.601015Z","shell.execute_reply.started":"2021-06-02T17:22:26.460025Z","shell.execute_reply":"2021-06-02T17:22:26.599984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"14) Find out the most rainy day for each city.","metadata":{}},{"cell_type":"code","source":"cities = tdf.drop_duplicates(subset = [\"location\"])\ncities.location\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:24.192882Z","iopub.execute_input":"2021-06-02T17:31:24.193259Z","iopub.status.idle":"2021-06-02T17:31:24.216477Z","shell.execute_reply.started":"2021-06-02T17:31:24.193227Z","shell.execute_reply":"2021-06-02T17:31:24.215566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf.loc[tdf.groupby('location').rainfall.idxmax()]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:36:34.814766Z","iopub.execute_input":"2021-06-02T17:36:34.815135Z","iopub.status.idle":"2021-06-02T17:36:34.945941Z","shell.execute_reply.started":"2021-06-02T17:36:34.815106Z","shell.execute_reply":"2021-06-02T17:36:34.944974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"15) Filter out all the data for the city 'Albury' and then sort according to maxtemp column.","metadata":{}},{"cell_type":"code","source":"tdf[tdf.location == 'Albury'].sort_values(by='maxtemp', ascending=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:26.807241Z","iopub.execute_input":"2021-06-02T17:22:26.807731Z","iopub.status.idle":"2021-06-02T17:22:26.882987Z","shell.execute_reply.started":"2021-06-02T17:22:26.807697Z","shell.execute_reply":"2021-06-02T17:22:26.881911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"16) Find out the NaN counts for each column.","metadata":{}},{"cell_type":"code","source":" tdf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:26.884709Z","iopub.execute_input":"2021-06-02T17:22:26.885397Z","iopub.status.idle":"2021-06-02T17:22:26.988073Z","shell.execute_reply.started":"2021-06-02T17:22:26.885349Z","shell.execute_reply":"2021-06-02T17:22:26.987079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"17) Remove the rows with NaN values in \"windgustdir\" column from the dataframe permanently.","metadata":{}},{"cell_type":"code","source":"tdf.dropna(subset=['windgustdir'], inplace=True)\ntdf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:26.992469Z","iopub.execute_input":"2021-06-02T17:22:26.992799Z","iopub.status.idle":"2021-06-02T17:22:27.1366Z","shell.execute_reply.started":"2021-06-02T17:22:26.992768Z","shell.execute_reply":"2021-06-02T17:22:27.135432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"18) Create a new dataframe, use \"Location\" column as the index of the dataframe, display the min, max, and median values of \"evaporation\" and \"sunshine\" columns in this dataframe.","metadata":{}},{"cell_type":"code","source":"tdf1 = df.copy()\ntdf1=tdf1.sort_values(by='Location').groupby('Location').agg({'Evaporation': max, 'Sunshine':max})\ntdf1\n#sort_values(by='Location')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:41:02.728797Z","iopub.execute_input":"2021-06-02T17:41:02.729216Z","iopub.status.idle":"2021-06-02T17:41:02.933417Z","shell.execute_reply.started":"2021-06-02T17:41:02.729182Z","shell.execute_reply":"2021-06-02T17:41:02.932317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf2 = df.copy()\ntdf2=tdf2.sort_values(by='Location').groupby('Location').agg({'Evaporation': min, 'Sunshine':min})\ntdf2","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:41:22.429231Z","iopub.execute_input":"2021-06-02T17:41:22.429654Z","iopub.status.idle":"2021-06-02T17:41:22.636738Z","shell.execute_reply.started":"2021-06-02T17:41:22.4296Z","shell.execute_reply":"2021-06-02T17:41:22.635675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf3 = df.copy()\ntdf3=tdf3.groupby(tdf3.Location)[['Evaporation', 'Sunshine']].median()\ntdf3","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:41:38.04256Z","iopub.execute_input":"2021-06-02T17:41:38.04333Z","iopub.status.idle":"2021-06-02T17:41:38.101465Z","shell.execute_reply.started":"2021-06-02T17:41:38.043272Z","shell.execute_reply":"2021-06-02T17:41:38.100218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf4 = pd.merge(pd.merge(tdf1,tdf2,on='Location', suffixes=[None,'_min']),tdf3,on='Location',suffixes=['_max','_median'])\ntdf4 = tdf4[['Evaporation_max', 'Evaporation_min', 'Sunshine_max', 'Sunshine_min', 'Evaporation_median', 'Sunshine_median']]\ntdf4","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:41:49.589303Z","iopub.execute_input":"2021-06-02T17:41:49.589699Z","iopub.status.idle":"2021-06-02T17:41:49.645564Z","shell.execute_reply.started":"2021-06-02T17:41:49.589663Z","shell.execute_reply":"2021-06-02T17:41:49.644437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"19) Find out the hottest day of \"Perth\". Example output: Timestamp('2015-01-05 00:00:00')","metadata":{}},{"cell_type":"code","source":"tdf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:27.681754Z","iopub.execute_input":"2021-06-02T17:22:27.68212Z","iopub.status.idle":"2021-06-02T17:22:27.794752Z","shell.execute_reply.started":"2021-06-02T17:22:27.682075Z","shell.execute_reply":"2021-06-02T17:22:27.793653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Perth=tdf[tdf.location == 'Perth'].sort_values(by='maxtemp', ascending=False)\nPerth.date.iloc[0]\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:22:27.796342Z","iopub.execute_input":"2021-06-02T17:22:27.796847Z","iopub.status.idle":"2021-06-02T17:22:27.825129Z","shell.execute_reply.started":"2021-06-02T17:22:27.796792Z","shell.execute_reply":"2021-06-02T17:22:27.824026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"20) Group your dataframe by location and find out the averages of all numeric values.","metadata":{}},{"cell_type":"code","source":"tdf.groupby(\"location\").mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:25:58.930509Z","iopub.execute_input":"2021-06-02T17:25:58.930905Z","iopub.status.idle":"2021-06-02T17:25:59.014019Z","shell.execute_reply.started":"2021-06-02T17:25:58.930873Z","shell.execute_reply":"2021-06-02T17:25:59.012711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}