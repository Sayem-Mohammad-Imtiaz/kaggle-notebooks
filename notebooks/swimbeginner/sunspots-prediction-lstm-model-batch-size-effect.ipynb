{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sunspots prediction in time series with Keras LSTM model: batch_size effect \n       \n<img src=\"https://www.weather.gov/images/fsd/astro/Sun_sunspot.jpg\" alt=\"Sunspots\" title=\"Sunspots\" />\n\n                                               picture from [weather.gov]\n                                               (https://www.weather.gov/images/fsd/astro/Sun_sunspot.jpg)\n\n* In the Laurence Moroney's \"Sequences, Time Series and Predictions\" class (https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction), week 4 project is to use LSTM to predict sunspots activities. He suggest us to play with the batch size parameters to lower the ripple in the accuracy vs epoch, and find the best predictions (lowest MAE).  \n### Summary<br>\nInitially, I expected larger batch_size will lead to better training loss/mae, less ripple in the loss/mae vs epochs curve and better validation results. In the end, I got better training loss/mae, not noticable decrease in ripple. The validation mae results become worse, which may due to limited validation data. \n\nThe tensorflow dataset method for timeseries input is not very intuitive, and alternative simple method with numpy array is compared. The training takes about the same time, but forecast is much slower than dataset method.\n\n1).[EDA and data loading](#eda) \n\n2).[Model setup](#setup)\n\n3).[Loss, Accuracy results, and batch_size analysis (32 vs 64)](#results)\n\n4) [Data setup with numpy method -- forecast is much slower than dataset method](#numpy)\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\n\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time (month)\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='eda'> </a>\n## EDA and data loading ","metadata":{"trusted":true}},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/sunspots/Sunspots.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series=data.iloc[:,2].to_numpy()\ntime=data.iloc[:,0].to_numpy()\nplt.figure(figsize=(12,6))\nplot_series(time, series)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='setup'></a>\n## Model setup\n\nUse Stacked LSTM for more complexity and better performance.\n\nOne trick to save time in the stacked LSTM<br>\nmake sure turn the second LSTM return_sequences=False:<br>\n***model=...<br>\n  tf.keras.layers.LSTM(60, return_sequences=True),<br>\n  tf.keras.layers.LSTM(60, return_sequences=False),*\n...<br>**\n\n   To match this setup, making sure the window setup with only one output (window[-1]): <br>\n***def windowed_dataset(<br>\n   ds = ds.map(lambda window: (window[:-1], window[-1]))***\n   \n   the best learning rate is ~8e-6. (Please check some other kagglers' notebook, which use the similar methods. for example:Kutay's notebook<br>\n   https://www.kaggle.com/kutaykutlu/time-series-tensorflow-rnn-lstm-introduction)","metadata":{}},{"cell_type":"code","source":"split_time=2800\ntime_train=time[:split_time]\nx_train=series[:split_time]\ntime_valid=time[split_time:]\nx_valid=series[split_time:]\n\nwindow_size=30\nbatch_size=32\nshuffle_buffer_size=1000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series=tf.expand_dims(series,axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[-1]))\n    return ds.batch(batch_size).prefetch(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer=shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=False),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\nmodel.summary()\ninitial_weights = model.get_weights()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-9 * 10**(epoch / 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-9, momentum=0.9)\nmodel.compile(loss=\"mse\", optimizer=optimizer)\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule], verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrs = 1e-9 * (10 ** (np.arange(100) / 20))\nplt.semilogx(lrs, history.history[\"loss\"])\nplt.axis([1e-9, 1e-3, 0, 10000])    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time as tm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.set_weights(initial_weights)\nstart_time=tm.time()\noptimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory_batch32 = model.fit(train_set,epochs=200, verbose=0)\nend_time=tm.time()\nduration=start_time-end_time\nprint(f'the traning time is: {duration}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='results'></a>\n# Loss (huber), Accuracy, MAE vs Batch_size (32 vs 64)\n\n1. For batch_size=32, the loss and accuracy training results look good, the loss/mae curve vs epochs has a lot of ripple. \nI increase the batch_size, and expected to get better testing results and less ripple in the training loss/accuracy curve.But...\n2. For batch_size=64, the loss and accuracy training results look better, but the MAE for the testing results are even worse. The ripple looks similar, not better, comparing to batch_size=32 (estimation by eye in different scales). \n  ","metadata":{}},{"cell_type":"code","source":"def model_forecast(model, series, window_size):\n #   series=tf.expand_dims(series,axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time=tm.time()\nrnn_forecast = model_forecast(model, series[...,np.newaxis], window_size)\nrnn_forecast_val = rnn_forecast[split_time - window_size:-1, 0]\nend_time=tm.time()\nduration=end_time-start_time\nprint(f'the forecast time is: {duration}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_forecast.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast_val).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ndef plot_accuracy(history):\n    mae=history.history['mae']\n    loss=history.history['loss']\n    epochs=range(len(loss))\n    \n    fig = plt.figure(figsize=(12,6))\n    fig.add_subplot(1,2,1)\n    sns.lineplot(epochs, mae)\n    sns.lineplot(epochs, loss)\n    plt.title('MAE and Loss')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend([\"MAE\", \"Loss\"])\n    \n    epochs_zoom = epochs[100:]\n    mae_zoom = mae[100:]\n    loss_zoom = loss[100:]\n\n    fig.add_subplot(1,2,2)\n    sns.lineplot(epochs_zoom, mae_zoom)\n    sns.lineplot(epochs_zoom, loss_zoom)\n    plt.title('MAE and Loss')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend([\"MAE\", \"Loss\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracy(history_batch32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For batch_size=64, repeat the model train and prediction","metadata":{}},{"cell_type":"code","source":"batch_size=64\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer=shuffle_buffer_size)\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time=tm.time()\nhistory_batch64 = model.fit(train_set,epochs=200, verbose=0)\nend_time=tm.time()\nduration=start_time-end_time\nprint(f'the traning time with batch 64 is: {duration}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time=tm.time()\nrnn_forecast_b64 = model_forecast(model, series[...,np.newaxis], window_size)\nrnn_forecast_short_b64 = rnn_forecast_b64[split_time - window_size:-1, 0]\nend_time=tm.time()\nduration=end_time-start_time\nprint(f'the forecast time with batch 64is: {duration}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast_short_b64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast_short_b64).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracy(history_batch64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='numpy'>  </a>\n## Data setup with numpy method,\n### The training takes about the same time, but the forecast takes much longer ","metadata":{}},{"cell_type":"code","source":"def window_dataset_np(series, window_size, batch_size):\n    x_train, y_train=[],[]\n    for i in range(0,series.shape[0]-window_size):\n        x_train.append(series[i:i+window_size])\n        y_train.append(series[i+window_size])\n    x_train=np.array(x_train)\n    x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n    #np.random.shuffle(x_train) #shuffle the x_train, but not insync with y_train\n    y_train=np.array(y_train)\n    return x_train, y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32\nx_train_np, y_train_np=window_dataset_np(x_train, window_size, batch_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nx_train_np, y_train_np = shuffle(x_train_np, y_train_np, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.set_weights(initial_weights)\nstart_time=tm.time()\nhistory_np = model.fit(x_train_np, y_train_np,epochs=200, verbose=0, batch_size=batch_size)#, shuffle=True)\nend_time=tm.time()\nduration=end_time-start_time\nprint(f'the traning time with numpy dataset is: {duration}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time=tm.time()\n\nforecast = []\nresults = []\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time:time + window_size, np.newaxis][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nend_time=tm.time()\nduration=end_time-start_time\nprint(f'the forecast time with numpy setup is: {duration}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The numpy method for training process took about the same time, but forecast process took much longer time to calculate than the dataset method.","metadata":{}},{"cell_type":"code","source":"results_2=np.array(forecast)\nprint(results_2.shape, results.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###The forecast results are 3-dimensions, which need to be cut to 1 dimension","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\n#plot_series(time_valid, results_2)\nplot_series(time_valid, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracy(history_np)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}