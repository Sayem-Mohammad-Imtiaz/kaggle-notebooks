{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import nbinom\nfrom collections import defaultdict\n\nimport random\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA , TruncatedSVD\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport string\nimport re\nfrom wordcloud import WordCloud\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\n\n\nstop = set(stopwords.words('english'))\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"dataset = pd.read_csv('../input/mbti-type/mbti_1.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nsns.countplot(y = 'type' , data = dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nThe distribution of length of posts is skewed right, centered around 7800 with most lengths between 4000 to 10000, a range of roughly 10000, some outliers are present in extreme left\n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Length of the text\nplt.figure(figsize = (12,8))\nplt.subplot(2,1,1)\ndataset['length'] = dataset.posts.apply(lambda x:len(x))\nplt.suptitle('Length of the Posts',fontsize = 30 , color = 'blue')\nsns.distplot(dataset.length , kde=True , bins = 30 ).set(title = 'Distribution of Length of Posts')\nplt.figure(figsize = (12,8))\nplt.subplot(2,1,2)\nsns.boxplot(dataset.length).set(title = 'Boxplot of Length of Posts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Length of words in each posts\n\nwords = dataset.posts.str.split().map(lambda x:len(x))\nplt.figure(figsize = (12,5))\nsns.distplot(words , kde = False ).set(title = 'Length of words in each post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Length of words in each category","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def CI(words):\n    unbiased_point_estimate = np.mean(words)\n    std = np.std(words)\n    z_star = 1.96\n    estimated_se = std/len(words)**0.5\n    \n    lcb = np.around(unbiased_point_estimate - z_star*estimated_se,decimals = 2)\n    ucb = np.around(unbiased_point_estimate + z_star*estimated_se,decimals = 2)\n    return (lcb,ucb)\n    \nCI(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"mean_length_word = {}\nconfidence_interval = {}\ndef category_length(data , category):\n    dx = dataset[dataset.type == category]\n    words = dx['posts'].str.split().map(lambda x:len(x))\n    mean_length_word[category] = np.around(np.mean(words),decimals = 2)\n    confidence_interval[category] = CI(words)\n    sns.distplot(words).set(title = 'Length of word in ' + category)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"categories = dataset.type.unique()\nfor i in categories:\n    category_length(dataset,i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With 95% cinfidence, the length of word in each category is estimated to be in between 1095 to 1323. THere is not a significant difference but there are some categories which shows clear difference, like INFJ and (ENTP,INTP,INTJ,ENTJ,INFP,ISFP,ISTP,ISTJ,ESTP,ESFP)","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"confidence_interval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Average Length of posts in each category is almost same","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"dx = dataset.groupby(['type'])['length'].apply(lambda x: np.mean(x))\ndx\nplt.figure(figsize = (12,5))\nsns.barplot(x = list(dx.index) , y = dx.values) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_corpus(data):\n    corpus = []\n    for text in data.posts.str.split():\n        for i in text:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequency of top 15 words in Posts","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"corpus = create_corpus(dataset)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n    else:\n        dic[word] = 1\n\ntop = sorted(dic.items() , key = lambda x:x[1] , reverse = True)[:15]\n\nplt.figure(figsize = (15,5))\nx , y = zip(*top)\nsns.barplot(list(x) , list(y)).set(title = 'Frequency of Top 15 words in posts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now look each of these most frequent words category wise","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def category_top15(dataset,category):\n    dx = dataset[dataset.type == category]\n    corpus = create_corpus(dx)\n    dic = defaultdict(int)\n    for word in corpus:\n        if word in stop:\n            dic[word] += 1\n        else:\n            dic[word] = 1\n\n    top = sorted(dic.items() , key = lambda x:x[1] , reverse = True)[:15]\n\n    plt.figure(figsize = (15,5))\n    x , y = zip(*top)\n    sns.barplot(list(x) , list(y)).set(title = 'Frequency of Top 15 words in this '+ category)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"categories = dataset.type.unique()\nfor i in categories:\n    category_top15(dataset,i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\ncorpus = create_corpus(dataset)\ndic = defaultdict(int)\npunctuation = string.punctuation\n\nfor word in corpus:\n    if word in punctuation:\n        dic[word] += 1\n        \ntop = sorted(dic.items() , key = lambda x:x[1] , reverse = True)[:15]\n\nx,y = zip(*top)\nsns.barplot(list(x) , list(y)).set(title = 'Barplot for top 15 punctuation in posts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_top_tweet_bigrams(corpus,n = None):\n    vec = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_of_words = bag_of_words.sum(axis = 0)\n    words_freq = [(word,sum_of_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq , key = lambda x:x[1] , reverse = True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(dataset.posts)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef combine_text(text):\n    return ' '.join(text)\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    no_punc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(no_punc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = combine_text(remove_stopwords)\n    \n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"dataset['clean_posts'] = dataset['posts'].apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"mbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition', \n        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling', \n        'J':'Judging', 'P': 'Perceiving'}\ndataset['description'] = dataset.type.apply(lambda x:' '.join([mbti[l] for l in list(x)]))\ndataset['clean_text_length'] = dataset.clean_posts.apply(lambda x:len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def Word_Cloud(dataset , category):\n    fig , ax1 = plt.subplots(1 , 1 , figsize = [26,8])\n    dx = dataset[dataset.type == category]['clean_posts']\n    wordcloud1 = WordCloud(background_color = 'black' , width = 600 , height = 400).generate(\" \".join(dx))\n    ax1.imshow(wordcloud1)\n    ax1.axis('off')\n    ax1.set_title('Wordcloud for posts '+ category , fontsize = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"for i in categories:\n    Word_Cloud(dataset,i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split training data and test data in 80-20 ratio","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndx = dataset[['clean_posts','type']]\nencoder = LabelEncoder()\ndx['type_enc'] = encoder.fit_transform(dx.type)\ndx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"category = list(encoder.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_data = dx.iloc[:6940,:]\ntest_data = dx.iloc[6940:,]\ntrain_data.shape,test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_data = train_data.dropna()\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train_data.clean_posts)\ntest_vectors = count_vectorizer.transform(test_data.clean_posts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(min_df = 2, max_df = 0.5, ngram_range = (1 , 2))\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_data.clean_posts)\ntest_tfidf = tfidf_vectorizer.transform(test_data.clean_posts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(class_weight='balanced' , C = 0.005)\nscore = model_selection.cross_val_score(clf , train_vectors , train_data['type_enc'] , cv = 5 , scoring = 'f1_micro')\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"clf_xgb = xgb.XGBClassifier(max_depth = 7 , n_estimators = 200 , colsample_bytree = 0.8 , subsample = 0.8 , nthread = 10 , learning_rate = 0.1)\nscores = model_selection.cross_val_score(clf_xgb , train_vectors , train_data['type_enc'] , cv = 5 , scoring = 'f1_micro')\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"clf_xgb.fit(train_vectors , train_data.type_enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"y_pred = clf_xgb.predict(test_vectors)\ny_test = test_data.type_enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"clf_xgb.predict(test_vectors)\ncm = confusion_matrix(y_pred,y_test)\n\nplt.figure(figsize = (20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(category); ax.yaxis.set_ticklabels(category);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}