{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://bit.ly/2VnXWr2\" width=\"100\" align=\"left\">"},{"metadata":{},"cell_type":"markdown","source":"# Final project: NLP to predict Myers-Briggs Personality Type"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nYou'll first need to download glove.6B.100d.txt from https://nlp.stanford.edu/projects/glove/ and save it to the following\npath (NLP-to-predict-Myers-Briggs-Personality-Type/glove_data/glove.6B/)of your local version of my project's reopsitory\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:33.22884Z","start_time":"2020-06-04T12:18:28.704686Z"},"trusted":true},"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nimport pickle as pkl\n\nfrom sklearn.model_selection import train_test_split\n\nimport keras.metrics\nfrom keras import regularizers, initializers, optimizers, callbacks\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.utils import class_weight\nfrom keras.layers import *\nfrom keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-29T18:09:27.528161Z","start_time":"2020-05-29T18:09:27.525169Z"}},"cell_type":"markdown","source":"## 4. Model building and evaluation: Deep Learning"},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.080411Z","start_time":"2020-06-04T12:18:33.2301Z"},"trusted":true},"cell_type":"code","source":"spacy_nlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.08629Z","start_time":"2020-06-04T12:18:34.082265Z"},"trusted":true},"cell_type":"code","source":"stopwords = spacy.lang.en.stop_words.STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.194761Z","start_time":"2020-06-04T12:18:34.088251Z"},"trusted":true},"cell_type":"code","source":"MAX_NB_WORDS = 100000    # max no. of words for tokenizer\nMAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence), including padding\nVALIDATION_SPLIT = 0.2   # data for validation (not used in training)\nEMBEDDING_DIM = 100      # embedding dimensions for word vectors (word2vec/GloVe)\nGLOVE_DIR = \"../input/nlp-to-predict-myers-briggs-personality-type/NLP-to-predict-Myers-Briggs-Personality-Type/glove_data/glove.6B/glove.6B.100d.txt\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Types"},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.459053Z","start_time":"2020-06-04T12:18:34.196755Z"},"trusted":false},"cell_type":"code","source":"mbti_df_clean = pd.read_pickle(\"../input/2-mbti-preprocessing/mbti_clean_text.pkl\")\nresult_umap_types  = pd.read_csv(\"../input/2-mbti-preprocessing/result_umap_types.csv\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.474421Z","start_time":"2020-06-04T12:18:34.460585Z"},"trusted":false},"cell_type":"code","source":"mbti_df_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.583661Z","start_time":"2020-06-04T12:18:34.476009Z"},"trusted":false},"cell_type":"code","source":"result_umap_types.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.671154Z","start_time":"2020-06-04T12:18:34.586635Z"},"cell_style":"center","trusted":false},"cell_type":"code","source":"neg, pos = np.bincount(result_umap_types[\"enfj\"])\ntotal = neg + pos\nprint('Total: {}\\n    enfj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"enfp\"])\ntotal = neg + pos\nprint('Total: {}\\n    enfp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"entj\"])\ntotal = neg + pos\nprint('Total: {}\\n    entj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"entp\"])\ntotal = neg + pos\nprint('Total: {}\\n    entp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"esfj\"])\ntotal = neg + pos\nprint('Total: {}\\n    esfj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"esfp\"])\ntotal = neg + pos\nprint('Total: {}\\n    esfp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"estj\"])\ntotal = neg + pos\nprint('Total: {}\\n    estj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"estp\"])\ntotal = neg + pos\nprint('Total: {}\\n    estp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"infj\"])\ntotal = neg + pos\nprint('Total: {}\\n    infj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"infp\"])\ntotal = neg + pos\nprint('Total: {}\\n    infp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"intj\"])\ntotal = neg + pos\nprint('Total: {}\\n    intj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"intp\"])\ntotal = neg + pos\nprint('Total: {}\\n    intp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"isfj\"])\ntotal = neg + pos\nprint('Total: {}\\n    isfj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"isfp\"])\ntotal = neg + pos\nprint('Total: {}\\n    isfp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"istj\"])\ntotal = neg + pos\nprint('Total: {}\\n    istj: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_types[\"istp\"])\ntotal = neg + pos\nprint('Total: {}\\n    istp: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.805246Z","start_time":"2020-06-04T12:18:34.673979Z"},"cell_style":"center","trusted":false},"cell_type":"code","source":"labels_dict = {0: 190 , 1: 675, 2: 231, 3: 685, 4: 42, 5: 48, 6: 39, 7: 89, 8: 1470,\n                9: 1832, 10: 1091, 11: 1304, 12: 166, 13: 271, 14: 205, 15: 337}","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.900249Z","start_time":"2020-06-04T12:18:34.806139Z"},"trusted":false},"cell_type":"code","source":"def create_class_weight(labels_dict):\n    total = 8675\n    keys = labels_dict.keys()\n    class_weight = dict()\n\n    for key in keys:\n        score = math.log(total/float(labels_dict[key]))\n        class_weight[key] = score if score > 1.0 else 1.0\n\n    return class_weight\n\nclass_weights = create_class_weight(labels_dict)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:34.972064Z","start_time":"2020-06-04T12:18:34.902086Z"},"cell_style":"center","trusted":false},"cell_type":"code","source":"labels = [\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\", \"infj\", \"infp\", \"intj\", \"intp\", \"isfj\", \n          \"isfp\", \"istj\", \"istp\"]\ny = result_umap_types[labels].values\nX = mbti_df_clean[\"posts_clean\"]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:35.069809Z","start_time":"2020-06-04T12:18:34.973567Z"},"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:35.161618Z","start_time":"2020-06-04T12:18:35.071767Z"},"trusted":false},"cell_type":"code","source":"X_train = list(X_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:35.254815Z","start_time":"2020-06-04T12:18:35.162791Z"},"trusted":false},"cell_type":"code","source":"texts = [line for line in X_train] ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:38.442295Z","start_time":"2020-06-04T12:18:35.255812Z"},"trusted":false},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:40.24187Z","start_time":"2020-06-04T12:18:38.443534Z"},"trusted":false},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Vocabulary size:', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:40.401113Z","start_time":"2020-06-04T12:18:40.243849Z"},"trusted":false},"cell_type":"code","source":"data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)\n\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:40.411614Z","start_time":"2020-06-04T12:18:40.402617Z"},"trusted":false},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = y_train[indices]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:40.493773Z","start_time":"2020-06-04T12:18:40.413609Z"},"trusted":false},"cell_type":"code","source":"num_validation_samples = int(VALIDATION_SPLIT*data.shape[0])\nx_train = data[: -num_validation_samples]\ny_train = labels[: -num_validation_samples]\nx_val = data[-num_validation_samples: ]\ny_val = labels[-num_validation_samples: ]\n\nprint ((x_train.shape),(y_train.shape),(x_val.shape),(y_val.shape))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:40.605288Z","start_time":"2020-06-04T12:18:40.495773Z"},"trusted":false},"cell_type":"code","source":"print('Number of entries in each category:')\nprint('training: ', y_train.sum(axis=0))\nprint('validation: ', y_val.sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:40.699039Z","start_time":"2020-06-04T12:18:40.607283Z"},"trusted":false},"cell_type":"code","source":"print('Tokenized sentences: \\n', data[10])\nprint('One hot label: \\n', labels[10])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:56.674354Z","start_time":"2020-06-04T12:18:40.700259Z"},"trusted":false},"cell_type":"code","source":"embeddings_index = {}\nf = open(GLOVE_DIR, encoding=\"UTF-8\")\nprint('Loading GloVe from:', GLOVE_DIR,'...', end='')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\nf.close()\nprint(\"Done.\\n Proceeding with Embedding Matrix...\", end=\"\")\n\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint(\" Completed!\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:56.96379Z","start_time":"2020-06-04T12:18:56.678339Z"},"trusted":false},"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_layer = Embedding(len(word_index) + 1,\n                           EMBEDDING_DIM,\n                           weights = [embedding_matrix],\n                           input_length = MAX_SEQUENCE_LENGTH,\n                           trainable=False,\n                           name = 'embeddings')\nembedded_sequences = embedding_layer(sequence_input)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:57.619714Z","start_time":"2020-06-04T12:18:56.965829Z"},"trusted":false},"cell_type":"code","source":"x = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\npreds = Dense(16, activation=\"softmax\")(x)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:18:57.871407Z","start_time":"2020-06-04T12:18:57.620971Z"},"trusted":false},"cell_type":"code","source":"opt = keras.optimizers.Adam()\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss = 'categorical_crossentropy',\n             optimizer=opt,\n             weighted_metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:21:08.81296Z","start_time":"2020-06-04T12:18:57.873465Z"},"trusted":false},"cell_type":"code","source":"print('Training progress:')\nhistory = model.fit(x_train, y_train, epochs = 15, batch_size=32, validation_data=(x_val, y_val), verbose=2, \n                    class_weight=class_weights)\nkeras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:21:08.824792Z","start_time":"2020-06-04T12:21:08.814955Z"},"trusted":false},"cell_type":"code","source":"hist_df = pd.DataFrame(history.history) \nhist_df.to_csv(\"types_hist_df.csv\")\ntypes_hist_df = hist_df\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:21:09.111704Z","start_time":"2020-06-04T12:21:08.998013Z"},"trusted":false},"cell_type":"code","source":"types_hist_df['val_f1'] = ((types_hist_df[\"val_precision\"]*types_hist_df[\"val_recall\"])/\n                           (types_hist_df[\"val_precision\"]+types_hist_df[\"val_recall\"]))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:21:09.227692Z","start_time":"2020-06-04T12:21:09.11284Z"},"trusted":false},"cell_type":"code","source":"types_hist_df = types_hist_df[['val_loss', 'loss', 'val_accuracy', 'accuracy', 'val_precision', 'precision',\n                  'val_recall', 'recall', 'val_f1']]\ntypes_hist_df","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:40:00.553257Z","start_time":"2020-06-04T12:40:00.206356Z"},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nplt.plot(types_hist_df[['val_loss', 'val_accuracy', 'val_precision', 'val_recall', 'val_f1']])\n\nplt.title('Training and Validation: Types')\nplt.xlabel('Epochs')\nplt.ylabel('Metrics')\nplt.legend(['val_loss', 'val_accuracy', 'val_precision', 'val_recall', 'val_f1'])\nplt.savefig(\"types_history.png\")\n\nsns.set_context(\"talk\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using dimensions"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.nicepng.com/png/detail/148-1486992_discover-the-most-powerful-ways-to-automate-your.png\" width=\"1000\"> "},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:21:10.50135Z","start_time":"2020-06-04T12:21:10.498358Z"},"trusted":false},"cell_type":"code","source":"#raise SystemExit(\"Here it comes another quite consuming memory process. You should better not start it till everything else has itereated propperly\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:00.227296Z","start_time":"2020-06-04T12:45:00.068953Z"},"trusted":false},"cell_type":"code","source":"mbti_df_clean = pd.read_pickle(\"../input/2-mbti-preprocessing/mbti_clean_text.pkl\")\nresult_umap_dimensions  = pd.read_csv(\"../input/2-mbti-preprocessing/result_umap_dimensions.csv\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:02.296877Z","start_time":"2020-06-04T12:45:02.285883Z"},"trusted":false},"cell_type":"code","source":"mbti_df_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:04.152015Z","start_time":"2020-06-04T12:45:04.140991Z"},"trusted":false},"cell_type":"code","source":"result_umap_dimensions.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:06.133702Z","start_time":"2020-06-04T12:45:06.12575Z"},"cell_style":"center","trusted":false},"cell_type":"code","source":"neg, pos = np.bincount(result_umap_dimensions[\"i-e\"])\ntotal = neg + pos\nprint('Total: {}\\n    i-e: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_dimensions[\"n-s\"])\ntotal = neg + pos\nprint('Total: {}\\n    n-s: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_dimensions[\"t-f\"])\ntotal = neg + pos\nprint('Total: {}\\n    t-f: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n\nneg, pos = np.bincount(result_umap_dimensions[\"j-p\"])\ntotal = neg + pos\nprint('Total: {}\\n    j-p: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:14.007482Z","start_time":"2020-06-04T12:45:13.987998Z"},"cell_style":"center","trusted":false},"cell_type":"code","source":"labels_dict = {0: 1999 , 1: 1997, 2: 4694, 3: 5241}","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:16.107974Z","start_time":"2020-06-04T12:45:16.10199Z"},"trusted":false},"cell_type":"code","source":"def create_class_weight(labels_dict):\n    total = 8675\n    keys = labels_dict.keys()\n    class_weight = dict()\n\n    for key in keys:\n        score = math.log(total/float(labels_dict[key]))\n        class_weight[key] = score if score > 1.0 else 1.0\n\n    return class_weight\n\nclass_weights = create_class_weight(labels_dict)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:17.921754Z","start_time":"2020-06-04T12:45:17.917126Z"},"trusted":false},"cell_type":"code","source":"labels = [\"i-e\", \"n-s\", \"t-f\", \"j-p\"]\ny = result_umap_dimensions[labels].values\nX = mbti_df_clean[\"posts_clean\"]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:19.668413Z","start_time":"2020-06-04T12:45:19.660407Z"},"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:21.249013Z","start_time":"2020-06-04T12:45:21.244417Z"},"trusted":false},"cell_type":"code","source":"X_train = list(X_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:22.896975Z","start_time":"2020-06-04T12:45:22.892975Z"},"trusted":false},"cell_type":"code","source":"texts = [line for line in X_train] ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:27.319466Z","start_time":"2020-06-04T12:45:24.668893Z"},"trusted":false},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:32.85841Z","start_time":"2020-06-04T12:45:31.204929Z"},"trusted":false},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Vocabulary size:', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:35.890268Z","start_time":"2020-06-04T12:45:35.740895Z"},"trusted":false},"cell_type":"code","source":"data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)\n\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:38.445604Z","start_time":"2020-06-04T12:45:38.437625Z"},"trusted":false},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = y_train[indices]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:40.737521Z","start_time":"2020-06-04T12:45:40.732899Z"},"trusted":false},"cell_type":"code","source":"num_validation_samples = int(VALIDATION_SPLIT*data.shape[0])\nx_train = data[: -num_validation_samples]\ny_train = labels[: -num_validation_samples]\nx_val = data[-num_validation_samples: ]\ny_val = labels[-num_validation_samples: ]\n\nprint ((x_train.shape),(y_train.shape),(x_val.shape),(y_val.shape))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:45.825558Z","start_time":"2020-06-04T12:45:45.821122Z"},"trusted":false},"cell_type":"code","source":"print('Number of entries in each category:')\nprint('training: ', y_train.sum(axis=0))\nprint('validation: ', y_val.sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:45:49.569218Z","start_time":"2020-06-04T12:45:49.564176Z"},"trusted":false},"cell_type":"code","source":"print('Tokenized sentences: \\n', data[10])\nprint('One hot label: \\n', labels[10])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:46:10.442584Z","start_time":"2020-06-04T12:45:54.845696Z"},"trusted":false},"cell_type":"code","source":"embeddings_index = {}\nf = open(GLOVE_DIR, encoding=\"UTF-8\")\nprint('Loading GloVe from:', GLOVE_DIR,'...', end='')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\nf.close()\nprint(\"Done.\\n Proceeding with Embedding Matrix...\", end=\"\")\n\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint(\" Completed!\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:46:19.439073Z","start_time":"2020-06-04T12:46:19.236927Z"},"trusted":false},"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_layer = Embedding(len(word_index) + 1,\n                           EMBEDDING_DIM,\n                           weights = [embedding_matrix],\n                           input_length = MAX_SEQUENCE_LENGTH,\n                           trainable=False,\n                           name = 'embeddings')\nembedded_sequences = embedding_layer(sequence_input)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:46:26.974804Z","start_time":"2020-06-04T12:46:26.468982Z"},"trusted":false},"cell_type":"code","source":"x = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\npreds = Dense(4, activation=\"softmax\")(x)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:46:31.310901Z","start_time":"2020-06-04T12:46:31.108934Z"},"trusted":false},"cell_type":"code","source":"opt = keras.optimizers.Adam()\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss = 'categorical_crossentropy',\n             optimizer=opt,\n             weighted_metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:47:46.757769Z","start_time":"2020-06-04T12:46:37.445948Z"},"trusted":false},"cell_type":"code","source":"print('Training progress:')\nhistory = model.fit(x_train, y_train, epochs = 10, batch_size=32, validation_data=(x_val, y_val), verbose=2, \n                    class_weight=class_weights)\nkeras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:51:34.580312Z","start_time":"2020-06-04T12:51:34.572981Z"},"trusted":false},"cell_type":"code","source":"hist_df = pd.DataFrame(history.history) \nhist_df.to_csv(\"dimensions_hist_df.csv\")\ndimensions_hist_df = hist_df\ndimensions_hist_df","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:52:01.650123Z","start_time":"2020-06-04T12:52:01.644139Z"},"trusted":false},"cell_type":"code","source":"dimensions_hist_df['val_f1'] = ((dimensions_hist_df[\"val_precision\"]*dimensions_hist_df[\"val_recall\"])/\n                                (dimensions_hist_df[\"val_precision\"]+dimensions_hist_df[\"val_recall\"]))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:52:05.252658Z","start_time":"2020-06-04T12:52:05.23421Z"},"trusted":false},"cell_type":"code","source":"dimensions_hist_df = dimensions_hist_df[['val_loss', 'loss', 'val_accuracy', 'accuracy', 'val_precision', 'precision',\n                  'val_recall', 'recall', 'val_f1']]\ndimensions_hist_df","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-04T12:52:37.88164Z","start_time":"2020-06-04T12:52:37.525498Z"},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nplt.plot(dimensions_hist_df[['val_accuracy', 'val_precision', 'val_recall', 'val_f1']])\n\nplt.title('Training and Validation: Dimensions')\nplt.xlabel('Epochs')\nplt.ylabel('Metrics')\nplt.legend(['val_accuracy', 'val_precision', 'val_recall', 'val_f1'])\nplt.savefig(\"dimensions_history.png\")\n\nsns.set_context(\"talk\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"types = types_hist_df[\"val_f1\"].max()\ndimensions = dimensions_hist_df[\"val_f1\"].max()\n\nprint(\"Best F1 Scores for both models, the one using types and the one using dimensions are of\" ,types, \"and\" ,dimensions, \" respectively, still much lower than the scores obtained using ML models.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best F1 Scores for both models, the one using types and the one using dimensions are of 0.205942 and 0.130249 respectively, still much lower than the scores obtained using ML models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}