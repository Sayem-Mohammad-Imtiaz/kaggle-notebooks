{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Helper function to plot training vs test data accuracy\n\ndef plot_training_vs_test(training, test):\n    objects = ('Training', 'Test')\n    performance = [training, test]\n    y_pos = np.arange(len(objects))\n    \n    plt.bar(y_pos,performance, align='center', alpha=0.5)\n    plt.xticks(y_pos, objects)\n    axes = plt.gca()\n    axes.set_ylim([0,1])\n    axes.tick_params(axis='x', colors='white')\n    axes.tick_params(axis='y', colors='white')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Speed Dating - Classification Model Comparisons To Predict\n\nThis notebook will use a few of the common models we have learned through this course to try and predict wether or not an individual will or will not like another person after the initial first speed date.\n\nData is taken from: https://www.kaggle.com/annavictoria/speed-dating-experiment\nData was produced by Columbia Business School professors Ray Fisman and Sheena Iyengar. They performed an experiment having students go on a 4 minute date and rate the person they dated on 6 attributes:\n    - Attractiveness\n    - Sincerity\n    - Intelligence\n    - Fun\n    - Ambition\n    - Shared Interests\n\nThe experiment also followed up with some of these inidividuals to get information after the fact to, to see if their opinions on dating has changed or not, or if they continued dating any of the people they met on the speed date round, for the sake of this document I will only be focusing on the speed date portion, and if models are able to predict based on the initial meet.\n\nTesting:\n    - Logistic Regression\n    - SVMs\n    - Decision Trees"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn as sk\nimport seaborn as sns\nfrom sklearn import model_selection, metrics, linear_model, datasets, feature_selection, preprocessing\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv(\"../input/speed-dating-experiment/Speed Dating Data.csv\", encoding=\"ISO-8859-1\")\nmanualtest = pd.read_csv(\"../input/manualdata/Manual_Test.csv\", encoding=\"ISO-8859-1\")\ndo_manual_test = True\ngenderToCompare = int(input(\"Enter what gender to model for (0 = female  1 = male  * = both): \"))\n\nall_fields_to_extract = [\n    'int_corr',\n    'samerace',\n    'attr',\n    'sinc',\n    'intel',\n    'fun',\n    'amb',\n    'shar',\n    'like',\n    'age',\n    'field_cd',\n    'race',\n    'income',\n    'imprace',\n    'imprelig',\n    'age_o',\n    'dec'\n]\nfeatures_to_include = ['int_corr', \n                       'samerace',\n                       'attr' ,\n                       'sinc',\n                       'intel',\n                       'fun',\n                       'amb',\n                       'shar',\n                       'age',\n                       'field_cd',\n                       'race',\n                       'imprace',\n                       'agediff'\n                      ]   \n#                       'imprelig'                       \n    \nif(genderToCompare == 0):\n    df = df.loc[df.gender == 0, :].loc[:, all_fields_to_extract]\n    print('female chosen')\nelif(genderToCompare == 1):\n    df = df.loc[df.gender == 1, :].loc[:, all_fields_to_extract]\n    print('male chosen')\nelse:\n    df = df.loc[:,all_fields_to_extract]\n    print('both chosen')\n   \ndf['agediff'] = df['age_o'] - df['age']\ndf['income'] = df['income'].str.replace(',', '')\nmanualtest['income'] = manualtest['income'].str.replace(',', '')\nmanualtest['agediff'] = manualtest['age_o'] - manualtest['age']\n\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean Data\n\nEither drop nan records or average the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace([np.inf, -np.inf], np.nan)\n#Average out the NaNs\n#df = df.fillna(df.mean())\n#Drop NaN records\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See top features"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_rank = feature_selection.mutual_info_classif(df[features_to_include], df['dec'])\nfeature_rank_df = pd.DataFrame(list(zip(features_to_include, feature_rank)), columns=['Feature', 'Score'])\nfeature_rank_df.sort_values(by='Score', ascending = False).head()\n#sns.pairplot(df, hue='dec')\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(df.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trn, X_tst, Y_trn, Y_tst = train_test_split(df[features_to_include], df['dec'], test_size=0.4)\nprint('Size of training: ', len(Y_trn))\nprint('Size of testing: ', len(Y_tst))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = sk.linear_model.LogisticRegression()\nlogreg.fit(X_trn[features_to_include],Y_trn)\n\nprint('accuracy on training data',round(logreg.score(X_trn[features_to_include], Y_trn),2),'%')\nprint('accuracy on test data',round(logreg.score(X_tst[features_to_include], Y_tst),2),'%')\n\nplot_training_vs_test(logreg.score(X_trn[features_to_include], Y_trn), logreg.score(X_tst[features_to_include], Y_tst))\n\nif(do_manual_test):\n    preds = logreg.predict(manualtest[features_to_include])\n    print(preds[0] == manualtest.dec[0])\n    print('Prediction of manual data: ')\n    print(preds[0])\n    print('Actual: ')\n    print(manualtest.dec[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM"},{"metadata":{},"cell_type":"markdown","source":"### Linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nSVM_model = LinearSVC()\nSVM_model.fit(X_trn[features_to_include], Y_trn)\n\nprint('accuracy on training data',round(SVM_model.score(X_trn[features_to_include], Y_trn),2),'%')\nprint('accuracy on test data',round(SVM_model.score(X_tst[features_to_include], Y_tst),2),'%')\n\nplot_training_vs_test(SVM_model.score(X_trn[features_to_include], Y_trn), SVM_model.score(X_tst[features_to_include], Y_tst))\n\nif(do_manual_test):\n    preds = SVM_model.predict(manualtest[features_to_include])\n    print(preds[0] == manualtest.dec[0])\n    print('Prediction of manual data: ')\n    print(preds[0])\n    print('Actual: ')\n    print(manualtest.dec[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kernal SVM\n\n### Radial Basis Function Mapping"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nkernel_SVM_model = SVC(kernel='rbf')\nkernel_SVM_model.fit(X_trn[features_to_include], Y_trn)\n\nprint('accuracy on training data',round(kernel_SVM_model.score(X_trn[features_to_include], Y_trn),2),'%')\nprint('accuracy on test data',round(kernel_SVM_model.score(X_tst[features_to_include], Y_tst),2),'%')\n\nplot_training_vs_test(kernel_SVM_model.score(X_trn[features_to_include], Y_trn), kernel_SVM_model.score(X_tst[features_to_include], Y_tst))\n\nif(do_manual_test):\n    preds = kernel_SVM_model.predict(manualtest[features_to_include])\n    print(preds[0] == manualtest.dec[0])\n    print('Prediction of manual data: ')\n    print(preds[0])\n    print('Actual: ')\n    print(manualtest.dec[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Poly"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\n#kernel_SVM_model = SVC(kernel='poly')\n#kernel_SVM_model.fit(X_trn[features_to_include], Y_trn)\n\n#print('accuracy on training data',round(kernel_SVM_model.score(X_trn[features_to_include], Y_trn),2),'%')\n#print('accuracy on test data',round(kernel_SVM_model.score(X_tst[features_to_include], Y_tst),2),'%')\n\n#plot_training_vs_test(kernel_SVM_model.score(X_trn[features_to_include], Y_trn), kernel_SVM_model.score(X_tst[features_to_include], Y_tst))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### sigmoid"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nkernel_SVM_model = SVC(kernel='sigmoid', C=3.0, coef0=1.0, probability=True)\nkernel_SVM_model.fit(X_trn[features_to_include], Y_trn)\n\nprint('accuracy on training data',round(kernel_SVM_model.score(X_trn[features_to_include], Y_trn),2),'%')\nprint('accuracy on test data',round(kernel_SVM_model.score(X_tst[features_to_include], Y_tst),2),'%')\n\nplot_training_vs_test(kernel_SVM_model.score(X_trn[features_to_include], Y_trn), kernel_SVM_model.score(X_tst[features_to_include], Y_tst))\n\nif(do_manual_test):\n    preds = kernel_SVM_model.predict(manualtest[features_to_include])\n    print(preds[0] == manualtest.dec[0])\n    print('Prediction of manual data: ')\n    print(preds[0])\n    print('Actual: ')\n    print(manualtest.dec[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nDT_model = tree.DecisionTreeClassifier(max_depth=5, min_samples_leaf=50)\nDT_model.fit(X_trn[features_to_include], Y_trn)\n\nprint('accuracy on training data',round(DT_model.score(X_trn[features_to_include], Y_trn),2),'%')\nprint('accuracy on test data',round(DT_model.score(X_tst[features_to_include], Y_tst),2),'%')\n\nplot_training_vs_test(DT_model.score(X_trn[features_to_include], Y_trn), DT_model.score(X_tst[features_to_include], Y_tst))\n\nif(do_manual_test):\n    preds = DT_model.predict(manualtest[features_to_include])\n    print(preds[0] == manualtest.dec[0])\n    print('Prediction of manual data: ')\n    print(preds[0])\n    print('Actual: ')\n    print(manualtest.dec[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#bestScore = 0.0\n#bestX = 1\n#bestY = 1\n#for x in range(1,100):\n#    for y in range(1,15):\n#        random_forest_model = RandomForestClassifier(n_estimators=x, max_depth=y)\n#        random_forest_model.fit(X_trn[features_to_include], Y_trn)\n#        if(round(random_forest_model.score(X_tst[features_to_include], Y_tst),2) > bestScore):\n#            bestScore = round(random_forest_model.score(X_tst[features_to_include], Y_tst),2)\n#            bestX = x\n#            bestY = y\n#print('best estimators: ', bestX, ' best max depth: ', bestY)\n\nrandom_forest_model = RandomForestClassifier(n_estimators=5, max_depth=3)\nrandom_forest_model.fit(X_trn[features_to_include], Y_trn)\nprint('accuracy on training data',round(random_forest_model.score(X_trn[features_to_include], Y_trn),2),'%')\nprint('accuracy on test data',round(random_forest_model.score(X_tst[features_to_include], Y_tst),2),'%')\n\nplot_training_vs_test(random_forest_model.score(X_trn[features_to_include], Y_trn), random_forest_model.score(X_tst[features_to_include], Y_tst))\n\ndf_feat_importances = pd.DataFrame(list(zip(features_to_include,random_forest_model.feature_importances_)), columns=['Feature','Importance'])\ndf_feat_importances.sort_values(by='Importance', inplace=True)\nplt.figure(figsize=[6,8])\nplt.barh(df_feat_importances['Feature'],df_feat_importances['Importance'])\n\nif(do_manual_test):\n    preds = random_forest_model.predict(manualtest[features_to_include])\n    print(preds[0] == manualtest.dec[0])\n    print('Prediction of manual data: ')\n    print(preds[0])\n    print('Actual: ')\n    print(manualtest.dec[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosted Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbm_model = GradientBoostingClassifier(n_estimators=50, max_depth=8, min_samples_leaf=75)\ngbm_model.fit(X_trn[features_to_include], Y_trn)\n\nprint('accuracy on training data',round(gbm_model.score(X_trn[features_to_include], Y_trn),2),'%')\nprint('accuracy on test data',round(gbm_model.score(X_tst[features_to_include], Y_tst),2),'%')\n\nplot_training_vs_test(gbm_model.score(X_trn[features_to_include], Y_trn), gbm_model.score(X_tst[features_to_include], Y_tst))\n\ndf_feat_importances_gbm = pd.DataFrame(list(zip(features_to_include,gbm_model.feature_importances_)), columns=['Feature','Importance'])\ndf_feat_importances_gbm.sort_values(by='Importance', inplace=True)\nplt.figure(figsize=[6,8])\nplt.barh(df_feat_importances_gbm['Feature'],df_feat_importances_gbm['Importance'])\n\nif(do_manual_test):\n    preds = gbm_model.predict(manualtest[features_to_include])\n    print(preds[0] == manualtest.dec[0])\n    print('Prediction of manual data: ')\n    print(preds[0])\n    print('Actual: ')\n    print(manualtest.dec[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nsc = StandardScaler()\nX = sc.fit_transform(df.drop('dec', axis=1))\ny = df['dec'].values\ny_cat = to_categorical(y)\n\n\nX_trn, X_tst, Y_trn, Y_tst = train_test_split(X, y_cat, test_size=0.2)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(17,), activation='tanh'))\nmodel.add(Dense(32, activation='tanh'))\nmodel.add(Dense(32, activation='tanh'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(Adam(lr=0.05),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_trn, Y_trn, epochs=20, verbose=2, validation_split=0.1)\n\n\ny_pred = model.predict(X_tst)\n\ny_test_class = np.argmax(y_tst, axis=1)\ny_pred_class = np.argmax(y_pred, axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\naccuracy_score(y_test_class, y_pred_class)\nprint(classification_report(y_test_class, y_pred_class))\nconfusion_matrix(y_test_class, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}