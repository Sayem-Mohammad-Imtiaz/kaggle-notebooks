{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using a Neural Network on the Breast Cancer Wisconsin Dataset\n\nLet's note that I'm a student in one of Microsoft AI School in France and that I am experimenting one of my first Neural Network on Keras.\n\nI did try first to use the MLP Classifier from sklearn [in this Notebook](https://www.kaggle.com/mizujou/breast-cancer-mlpclassifier-by-mizujou), and I want to compare the results with a simple NN with Keras.\n\n---\n\n# 1. Import Librairies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basics\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nimport IPython\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport kerastuner as kt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\n\n# Graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\"Numpy: \" + np.__version__)\nprint(\"Tensorflow: \" + tf.__version__)\nprint(\"Keras: \" + keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 2. Loading the Dataset\n\nAttributes information:\n* Diagnosis (0 = malignant, 1 = benign)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X, y = load_breast_cancer(return_X_y=True)\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 3. Split the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data, train_targets, test_targets = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=0, \n    shuffle=True,\n    stratify=y\n)\n\ntrain_data.shape, train_targets.shape, test_data.shape, test_targets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.DataFrame(train_data)\ntrain_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean',\n    'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',\n    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se',\n    'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se',\n    'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n    'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst',\n    'symmetry_worst', 'fractal_dimension_worst'"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train_df[:], diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 4. HyperTuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_builder(hp):\n  model = keras.Sequential()\n  model.add(keras.layers.BatchNormalization(input_shape=[30]))\n\n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n  hp_units = hp.Int('units', min_value=2, max_value=30, step=2)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  model.add(keras.layers.Dense(1, activation='sigmoid'))\n\n  # Tune the learning rate for the optimizer \n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]) \n\n  model.compile(\n      optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n      loss=keras.losses.BinaryCrossentropy(), \n      metrics=['binary_accuracy']\n  )\n\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner = kt.Hyperband(\n    model_builder,\n    objective='val_binary_accuracy', \n    max_epochs=10,\n    factor=3,\n)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClearTrainingOutput(tf.keras.callbacks.Callback):\n  def on_train_end(*args, **kwargs):\n    IPython.display.clear_output(wait = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"tuner.search(\n    train_data, train_targets, \n    epochs=1500, \n    validation_split=0.1, # We set the size of our validation set here\n    callbacks=[ClearTrainingOutput()]\n)\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hps.get('units')} and the optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 5. Creation of the model\n\n1. We will start by trying to recreate the architecture of the best model using MLPClassifier and see the performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=(30,)),\n    layers.Dense(14, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001), \n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"history = model.fit(\n    train_data, train_targets,\n    validation_split=0.1,\n    batch_size=None,\n    epochs=500,\n    callbacks=[early_stopping],\n    verbose=1, # hide the output because we have so many epochs\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n---\n\n# 6. Performance on the Test Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_data, test_targets, verbose=2)\n\nprint('\\nTest accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 8. Conclusion\n\n* We successfully find the same Accuracy with Keras and our previous [MLP Classifier version](https://www.kaggle.com/mizujou/breast-cancer-mlpclassifier-by-mizujou) which is **95,61%**\n* Once again, the point of this notebook wasn't to find the best model using Keras but more to have a first approach using Keras with a simple architecture of a Neural Network\n\n### Update\n\n* After launching the notebook again, I decided to change some things like the size of the validation split since it is a small dataset which even made the model generalize better and finish now with a **98%** accuracy on the Test Set.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}