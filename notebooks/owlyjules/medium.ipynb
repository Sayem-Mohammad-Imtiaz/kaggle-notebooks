{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\n\nfrom scipy.sparse import coo_matrix, vstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\n\nimport nltk\nfrom collections import Counter\nimport itertools\n\n## --------------------------------------\n\nclass Vocab:\n    def __init__(self, itos, unk_index):\n        self._itos = itos\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]\n\nclass TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()\n\nclass SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size\n        \n    def tokenize(self, text):\n        return nltk.tokenize.word_tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform(self, texts):\n        result = []\n        for text in texts:\n            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in texts]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tokenized_texts:\n            tokens = ['<S>'] + tokens + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n\ndef features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([[example.input_ids for example in list_of_features]], dtype=torch.long)\n    labels_tensor = torch.tensor([[example.label_id for example in list_of_features]], dtype=torch.long)\n    return text_tensor, labels_tensor\n\n## --------------------------------------\n\ndef pad(token_ids, max_seq_len, pad_index):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    \n    return ids\n\ndef get_data():\n    dataframe = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv', encoding = 'ISO-8859-1')\n    data = dataframe.values\n\n    return data\n\ndef remove_unlabeled(x):\n    indices = [i for i in range(x.shape[0]) if x[i, 3] != 'pos' and x[i, 3] != 'neg']\n    return np.delete(x, indices, 0)\n\ndef unrandomize():\n    seed = 1448\n    \n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n\nclass TextClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_size, num_filters, num_classes, window_sizes = (3, 4, 5)):\n        super(TextClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        \n        self.dropout_big = torch.nn.Dropout(p = 0.4)\n        self.dropout_small = torch.nn.Dropout(p = 0.05)\n\n        self.convs = nn.ModuleList([\n            nn.Conv1d(1, num_filters, [window_size, emb_size], padding = (window_size - 1, 0)) for window_size in window_sizes\n        ])\n\n        self.fc = nn.Linear(num_filters * len(window_sizes), num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout_big(x)\n\n        x = torch.unsqueeze(x, 1)\n        xs = []\n        for i in range(len(self.convs)):\n            conv = self.convs[i]\n            \n            x2 = F.relu(conv(x))\n            x2 = torch.squeeze(x2, -1)\n            x2 = F.max_pool1d(x2, x2.size(2))\n            \n            if i == 1:\n                x2 = self.dropout_small(x2)\n            \n            xs.append(x2)\n        x = torch.cat(xs, 2)\n\n        x = x.view(x.size(0), -1)\n        logits = self.fc(x)\n        \n        return logits\n\n# class TextClassifier(nn.Module):\n#     def __init__(self, vocab_size, emb_size, num_filters, num_classes, window_sizes=(3, 4, 5)):\n#         super(TextClassifier, self).__init__()\n\n#         self.embedding = nn.Embedding(vocab_size, emb_size)\n        \n#         self.dropout = torch.nn.Dropout(p = 0.5)\n\n#         self.convs = nn.ModuleList([\n#             nn.Conv2d(1, num_filters, [window_size, emb_size], padding = (window_size - 1, 0)) for window_size in window_sizes\n#         ])\n\n#         self.fc = nn.Linear(num_filters * len(window_sizes), num_classes)\n\n#     def forward(self, x):\n#         x = self.embedding(x)\n\n#         x = torch.unsqueeze(x, 1)\n#         xs = []\n#         for conv in self.convs:\n#             x2 = F.relu(conv(x))\n#             x2 = torch.squeeze(x2, -1)\n#             x2 = F.max_pool1d(x2, x2.size(2))\n#             xs.append(x2)\n#         x = torch.cat(xs, 2)\n\n#         x = x.view(x.size(0), -1)\n#         logits = self.fc(x)\n        \n#         return logits\n\ndef loaderify(data, train):\n    batch_size = 64 if train else 1\n    shuffle = True\n    num_workers = 2\n    \n    loader = torch.utils.data.DataLoader(data, batch_size = batch_size, shuffle = shuffle, num_workers = num_workers)\n    \n    return loader\n\ndef get_loaders(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n    train_data = torch.from_numpy(X_train)\n    val_data = torch.from_numpy(X_val)\n    test_data = torch.from_numpy(X_test)\n    \n    train_labels = torch.tensor(Y_train)\n    val_labels = torch.tensor(Y_val)\n    test_labels = torch.tensor(Y_test)\n    \n    train_ds = torch.utils.data.TensorDataset(train_data, train_labels)\n    val_ds = torch.utils.data.TensorDataset(val_data, val_labels)\n    test_ds = torch.utils.data.TensorDataset(test_data, test_labels)\n    \n    train_loader = loaderify(train_ds, train = True)\n    val_loader = loaderify(val_ds, train = False)\n    test_loader = loaderify(test_ds, train = False)\n    \n    return [train_loader, val_loader, test_loader]\n\ndef fix_shape_neural(X_train, X_val, X_test):\n    X_train_new = np.vstack(X_train)\n    X_val_new = np.vstack(X_val)\n    X_test_new = np.vstack(X_test)\n    \n    return [X_train_new, X_val_new, X_test_new]\n\ndef train_cnn(convnet, loader, val_loader, epochs = 1):\n    criterion = nn.CrossEntropyLoss()\n#     optimizer = optim.SGD(convnet.parameters(), lr = 0.01, momentum = 0.9)\n    optimizer = optim.Adam(convnet.parameters(), lr = 0.0003)\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        for i, data in enumerate(loader, 0):\n            inputs, labels = data\n\n            optimizer.zero_grad()\n\n            outputs = convnet(inputs.cuda())\n            loss = criterion(outputs, labels.cuda())\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            \n            every_n = 100\n            if i % every_n == every_n - 1:\n                val_loss = get_val_loss(convnet, val_loader)\n                \n                print('[%d, %5d] loss: %.3f | val_loss: %.3f' % (epoch + 1, i + 1, running_loss / every_n, val_loss))\n                running_loss = 0.0\n\ndef get_val_loss(convnet, val_loader):\n    criterion = nn.CrossEntropyLoss()\n    \n    running_loss = 0.0\n    for i, data in enumerate(val_loader, 0):\n        inputs, labels = data\n\n        outputs = convnet(inputs.cuda())\n        loss = criterion(outputs, labels.cuda())\n\n        running_loss += loss.item()\n    \n    return running_loss / len(val_loader)\n\ndef get_predictions(convnet, data_loader):\n    y_true = []\n    y_pred = []\n    \n    for data in data_loader:\n        d, labels = data\n        d = d.cuda()\n        labels = labels.cuda()\n        \n        outputs = convnet(d)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true += labels.tolist()\n        y_pred += predicted.tolist()\n    \n    return [y_true, y_pred]\n\ndef print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n    \"\"\"pretty print for confusion matrixes\"\"\"\n    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n    empty_cell = \" \" * columnwidth\n    \n    # Begin CHANGES\n    fst_empty_cell = (columnwidth-3)//2 * \" \" + \"t/p\" + (columnwidth-3)//2 * \" \"\n    \n    if len(fst_empty_cell) < len(empty_cell):\n        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n    # Print header\n    print(\"    \" + fst_empty_cell, end=\" \")\n    # End CHANGES\n    \n    for label in labels:\n        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n        \n    print()\n    # Print rows\n    for i, label1 in enumerate(labels):\n        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n        for j in range(len(labels)):\n            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n            if hide_zeroes:\n                cell = cell if float(cm[i, j]) != 0 else empty_cell\n            if hide_diagonal:\n                cell = cell if i != j else empty_cell\n            if hide_threshold:\n                cell = cell if cm[i, j] > hide_threshold else empty_cell\n            print(cell, end=\" \")\n        print()\n\ndef print_metrics(convnet, test_loader, classes):\n    y_true, y_pred = get_predictions(convnet, test_loader)\n    \n    print('\\nclassification report:')\n    print(metrics.classification_report(y_true, y_pred, target_names = classes))\n    \n    y_true = [classes[a] for a in y_true]\n    y_pred = [classes[a] for a in y_pred]\n    \n    cm = metrics.confusion_matrix(y_true, y_pred, labels = classes)\n    print('\\nconfusion matrix:')\n    print_cm(cm, classes)\n\ndef evaluate(convnet, test_loader):\n    y_true, y_pred = get_predictions(convnet, test_loader)\n    \n    correct = (np.array(y_true) == np.array(y_pred)).sum()\n    total = len(y_true)\n    evaluation = correct / total\n    \n    return evaluation\n\ndef preprocess_neural(data):\n    data = remove_unlabeled(data)\n\n    dev_data = data[data[:, 1] == 'train']\n    test_data = data[data[:, 1] == 'test']\n    train_data, val_data = train_test_split(dev_data, test_size = 0.05)\n    \n    X_train = train_data[:, 2]\n    Y_train = train_data[:, 3]\n    \n    X_val = val_data[:, 2]\n    Y_val = val_data[:, 3]\n    \n    X_test = test_data[:, 2]\n    Y_test = test_data[:, 3]\n    \n    max_seq_len = 200\n    max_vocab_size = 10000\n    \n    text2id = SimpleTextTransformer(max_vocab_size)\n    \n    print('Learning vocab...')\n    \n    X_train = text2id.fit_transform(X_train)\n    X_train = np.array([pad(v, max_seq_len, text2id.pad_index) for v in X_train])\n    \n    X_val = text2id.transform(X_val)\n    X_val = np.array([pad(v, max_seq_len, text2id.pad_index) for v in X_val])\n    \n    X_test = text2id.transform(X_test)\n    X_test = np.array([pad(v, max_seq_len, text2id.pad_index) for v in X_test])\n    \n    Y_train = np.array([1 if Y_train[i] == 'pos' else 0 for i in range(Y_train.shape[0])])\n    Y_val = np.array([1 if Y_val[i] == 'pos' else 0 for i in range(Y_val.shape[0])])\n    Y_test = np.array([1 if Y_test[i] == 'pos' else 0 for i in range(Y_test.shape[0])])\n    \n    classes = ['neg', 'pos']\n    vocab_size = len(text2id.vocab)\n    \n    return [X_train, Y_train, X_val, Y_val, X_test, Y_test, classes, vocab_size]\n\ndef train_neural(train_loader, val_loader, test_loader, classes, vocab_size):\n    emb_size = 300\n    num_filters = 128\n    num_classes = 2\n    \n    epochs = 5\n\n    clf = TextClassifier(vocab_size, emb_size, num_filters, num_classes)\n    clf.cuda()\n    clf.train()\n    \n    print('Training...')\n    \n    train_cnn(clf, train_loader, val_loader, epochs)\n    \n    clf.eval()\n    \n#     val_loss = get_val_loss(clf, val_loader)\n#     print('\\nval_loss: {:.3f}\\n'.format(val_loss))\n    \n    print_metrics(clf, test_loader, classes)\n    \n    score = evaluate(clf, val_loader)\n    print('\\nCNN score: {:.3f}\\n'.format(score))\n    \ndef main():\n    unrandomize()\n    \n    data = get_data()\n    X_train, Y_train, X_val, Y_val, X_test, Y_test, classes, vocab_size = preprocess_neural(data)\n    train_loader, val_loader, test_loader = get_loaders(X_train, Y_train, X_val, Y_val, X_test, Y_test)\n    train_neural(train_loader, val_loader, test_loader, classes, vocab_size)\n\nmain()\nprint('done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}