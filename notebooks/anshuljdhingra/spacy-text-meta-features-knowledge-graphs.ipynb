{"cells":[{"metadata":{"trusted":true,"_uuid":"6b9c1dde0b2464ff6778d75d84efd3ba2ef21848"},"cell_type":"markdown","source":"\n## spaCy : Faster Natural Language Processing Toolkit\n\n<br>\n\nIn this kernel, I have shared two use-cases, that can be used for different purposes: \n\n- 1. Building a Meta Feature Extractor Module for Text Data   \n- 2. Building a Basic Knowledge Graph using spaCy\n\n\n### Quick Intro\n\nSpacy is written in cython language, (C extension of Python designed to give C like performance to the python program). Hence is a quite fast library. spaCy provides a concise API to access its methods and properties governed by trained machine (and deep) learning models. For a detailed information, one can refer to my blog on spaCy that I wrote in 2017 :\n\nhttps://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/\n\n\nDifferent types of features can be generated from the text data. For example - \n\nCount Features : Word counts, Character Counts, Punctuation Counts, Stopwords. \nNLP Attributes : Part of speech tags distribution, grammar relations as features, topic models \nWord Vectors : TF-IDF, Count Vectors, Word Embeddings  \n\n### Uses of meta features of text:  \n\n1. Information Extraction   \n&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Social Media  \n&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Chats & Conversations  \n&nbsp;&nbsp;&nbsp;&nbsp; 1.3 News / Articles     \n2. Machine Learning Models  \n&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Classification / Regression Models     \n&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Recommendation Models     \n&nbsp;&nbsp;&nbsp;&nbsp; 2.3. Building Knowledge Graphs  \n\n\nIn this kernel, I have shared a framework that uses spacy which can be used to automatically different types of features from a given text. Importing the required libraries. \n"},{"metadata":{"trusted":true,"_uuid":"85600930b325b2bc99c604b7535cf298e475c247"},"cell_type":"code","source":"from collections import Counter \nimport pandas as pd\nimport spacy \nnlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b23103283c0efb1de3b67f44d0da64853fce5787"},"cell_type":"markdown","source":"Next, lets define the class that will use spacy functions to generate features . In the class, initialize some important variables and lists. "},{"metadata":{"_uuid":"f97c8c7c3a2dd72fd56f053c59081398d4012d36","_execution_state":"idle","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"feats = ['char_count', 'word_count', 'word_count_cln',\n       'stopword_count', '_NOUN', '_VERB', '_ADP', '_ADJ', '_DET', '_PROPN',\n       '_INTJ', '_PUNCT', '_NUM', '_PRON', '_ADV', '_PART', '_amod', '_ROOT',\n       '_punct', '_advmod', '_auxpass', '_nsubjpass', '_ccomp', '_acomp',\n       '_neg', '_nsubj', '_aux', '_agent', '_det', '_pobj', '_prep', '_csubj',\n       '_nummod', '_attr', '_acl', '_relcl', '_dobj', '_pcomp', '_xcomp',\n       '_cc', '_conj', '_mark', '_prt', '_compound', '_dep', '_advcl',\n       '_parataxis', '_poss', '_intj', '_appos', '_npadvmod', '_predet',\n       '_case', '_expl', '_oprd', '_dative', '_nmod']\n\nclass AutomatedTextFE:\n    def __init__(self):\n        self.pos_tags = ['NOUN', 'VERB', 'ADP', 'ADJ', 'DET', 'PROPN', 'INTJ', 'PUNCT',\\\n                         'NUM', 'PRON', 'ADV', 'PART']\n        self.dep_tags = ['amod', 'ROOT', 'punct', 'advmod', 'auxpass', 'nsubjpass',\\\n                         'ccomp', 'acomp', 'neg', 'nsubj', 'aux', 'agent', 'det', 'pobj',\\\n                         'prep', 'csubj', 'nummod', 'attr', 'acl', 'relcl', 'dobj', 'pcomp', \\\n                         'xcomp', 'cc', 'conj', 'mark', 'prt', 'compound', 'dep', 'advcl',\\\n                         'parataxis', 'poss', 'intj', 'appos', 'npadvmod', 'predet', 'case',\\\n                         'expl', 'oprd', 'dative', 'nmod']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7fcde2feeae77b919d0ca2ca8f053bc9bc71855"},"cell_type":"markdown","source":"Adding a text cleaning function that removes stopwords, punctuations, performs normalization (lemmatization) and lower cases the words. "},{"metadata":{"trusted":true,"_uuid":"ecf6976c6ab9b2bea87bab7aa48ee9a605450c34"},"cell_type":"code","source":"def _spacy_cleaning(doc):\n    toks = [t for t in doc if (t.is_stop == False)]\n    toks = [t for t in toks if (t.is_punct == False)]\n    words = [t.lemma_ for token in toks]\n    return \" \".join(words)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0de615940556d7906b02f985d7bc2a36ed8d1dfd"},"cell_type":"markdown","source":"Next, lets write the function to perform feature engineering. This function will extract following:\n\n- word counts, char counts, stopword counts   \n- part of speech tags and their counts  \n- dependency relations among words and their counts   "},{"metadata":{"trusted":true,"_uuid":"b7513a1f85631147587d9678c2308ef7cf93f1e4"},"cell_type":"code","source":"def _spacy_features(df):\n    df[\"clean_text\"] = df[c].apply(lambda x : _spacy_cleaning(x))\n    df[\"char_count\"] = df[textcol].apply(len)\n    df[\"word_count\"] = df[c].apply(lambda x : len([_ for _ in x]))\n    df[\"word_count_cln\"] = df[\"clean_text\"].apply(lambda x : len(x.split()))\n    df[\"stopword_count\"] = df[c].apply(lambda x : len([_ for _ in x if _.is_stop]))\n    df[\"pos_tags\"] = df[c].apply(lambda x : dict(Counter([_.head.pos_ for _ in x])))\n    df[\"dep_tags\"] = df[c].apply(lambda x : dict(Counter([_.dep_ for _ in x])))\n    return df ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3870e1e4a52f8cb59f0254979c1328ec8294de34"},"cell_type":"markdown","source":"Next, lets wrap this functions in the main class"},{"metadata":{"trusted":true,"_uuid":"572b91cd3c32bc1a15fd16982e9216428812b468"},"cell_type":"code","source":"class AutomatedTextFE:\n    def __init__(self, df, textcol):\n        self.df = df\n        self.textcol = textcol\n        self.c = \"spacy_\" + textcol\n        self.df[self.c] = self.df[self.textcol].apply( lambda x : nlp(x))\n        \n        self.pos_tags = ['NOUN', 'VERB', 'ADP', 'ADJ', 'DET', 'PROPN', 'INTJ', 'PUNCT',\\\n                         'NUM', 'PRON', 'ADV', 'PART']\n        self.dep_tags = ['amod', 'ROOT', 'punct', 'advmod', 'auxpass', 'nsubjpass',\\\n                         'ccomp', 'acomp', 'neg', 'nsubj', 'aux', 'agent', 'det', 'pobj',\\\n                         'prep', 'csubj', 'nummod', 'attr', 'acl', 'relcl', 'dobj', 'pcomp', \\\n                         'xcomp', 'cc', 'conj', 'mark', 'prt', 'compound', 'dep', 'advcl',\\\n                         'parataxis', 'poss', 'intj', 'appos', 'npadvmod', 'predet', 'case',\\\n                         'expl', 'oprd', 'dative', 'nmod']\n        \n    def _spacy_cleaning(self, doc):\n        tokens = [token for token in doc if (token.is_stop == False)\\\n                  and (token.is_punct == False)]\n        words = [token.lemma_ for token in tokens]\n        return \" \".join(words)\n        \n    def _spacy_features(self):\n        self.df[\"clean_text\"] = self.df[self.c].apply(lambda x : self._spacy_cleaning(x))\n        self.df[\"char_count\"] = self.df[self.textcol].apply(len)\n        self.df[\"word_count\"] = self.df[self.c].apply(lambda x : len([_ for _ in x]))\n        self.df[\"word_count_cln\"] = self.df[\"clean_text\"].apply(lambda x : len(x.split()))\n        \n        self.df[\"stopword_count\"] = self.df[self.c].apply(lambda x : \n                                                          len([_ for _ in x if _.is_stop]))\n        self.df[\"pos_tags\"] = self.df[self.c].apply(lambda x :\n                                                    dict(Counter([_.head.pos_ for _ in x])))\n        self.df[\"dep_tags\"] = self.df[self.c].apply(lambda x :\n                                                    dict(Counter([_.dep_ for _ in x])))\n        \n    def _flatten_features(self):\n        for key in self.pos_tags:\n            self.df[\"_\" + key] = self.df[\"pos_tags\"].apply(lambda x : \\\n                                                           x[key] if key in x else 0)\n        \n        for key in self.dep_tags:\n            self.df[\"_\" + key] = self.df[\"dep_tags\"].apply(lambda x : \\\n                                                           x[key] if key in x else 0)\n                \n    def generate_features(self):\n        self._spacy_features()\n        self._flatten_features()\n        self.df = self.df.drop([self.c, \"pos_tags\", \"dep_tags\", \"clean_text\"], axis=1)\n        return self.df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9886961f1222adac518698d52e160888fad6c903"},"cell_type":"markdown","source":"Finally, create a wrapper function which can be used to call the specific functions for feature generation."},{"metadata":{"trusted":true,"_uuid":"f720050c0f6df106a516a674cbca0e16b89a6a07"},"cell_type":"code","source":"def spacy_features(df, tc):\n    fe = AutomatedTextFE(df, tc)\n    return fe.generate_features()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"864ba036830e3225141dccbbb9b1fddac16577c0"},"cell_type":"markdown","source":"Let's look at some examples: \n\n### Ted Talk Transcripts Dataset "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"890f4043c01ebe552f4a8cdada8f66374d9fcfe7"},"cell_type":"code","source":"path = \"../input/ted-talks/transcripts.csv\"\ndf = pd.read_csv(path)[:10]\ntextcol = \"transcript\"\n\nfeats_df = spacy_features(df, textcol)\nfeats_df[[textcol] + feats].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e97af825c09b52f31cd03e5e59c4a62dce766ab"},"cell_type":"markdown","source":"### Seinfeld Chronicles Dataset"},{"metadata":{"trusted":true,"_uuid":"e5310f8b42ed0de2a5e5a77f62820d17de77cb26"},"cell_type":"code","source":"path = \"../input/seinfeld-chronicles/scripts.csv\"\ndf = pd.read_csv(path)[:10]\ntextcol = \"Dialogue\"\n\nfeats_df = spacy_features(df, textcol)\nfeats_df[[textcol] + feats].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56cdec5c70d72fc837d597407ea6582e34e15781"},"cell_type":"markdown","source":"So these features can be passed to ML model for better classification, or can be used in recommendation engines to improve recommendations. Other use-cases can be to improve the search results, chat bot responses, and better information reterival.\n\n### Basics of Knowledge Graphs using spaCy\n\nIn this section, I have explained the basics of building knowledge graphs using spaCy. \n\nFirst, lets understand what are knoweldge graphs. \n\n- What are knowlege graphs ? \n> Knowledge stored in a graph form. The knowledge is captured in entities, attributes, relationships. The Nodes represents entities, NodeLabels represents attributes, and Edges represents Relationships. \n\n- Example:  \n> Chris Nolan (Director, Producer, person) ---> born in  ----> London (place) ---> Director of  ----> Interstellar (Movie) ---> shooted in  -----> Iceland (place)  \n\n- Source of information for building knowledge graphs: \n> Structured Text: Wikipedia, Dbpedia  \n> Unstructured Text: Social Media, Blogs, Images, Videos, Audios \n\n#### Main ideas for building knowlege graphs\n\n- Entity Extraction   \nIn this step, the aim is to extract right entities from the text data. spaCy provides NER (Named Entity Recognition) which can be used for this purpose.  \n\n- Relationship Extraction    \nIn this step, the aim is to identify the relationship between the sentences / entities. Again, by using spaCy one can extract the grammar relations between two words / entities.  \n\n- Relationship Linking    \nThe hard part of knowlege graphs is to identify what kind of relationship exists between the two entities. The idea is to add the contextual sense to the relationship. \n\nLet's look at very high level implementation of this idea using spacy. Lets load a news dataset. "},{"metadata":{"trusted":true,"_uuid":"7e30860a91ef34a6d07427110f1d6ce64993bf8f"},"cell_type":"code","source":"path = \"../input/news-aggregator-dataset/uci-news-aggregator.csv\"\ndf = pd.read_csv(path)[:1500]\ndf[\"spacy_title\"] = df[\"TITLE\"].apply(lambda x : nlp(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2099872afd539fa28784a066f2ce0c06c42a2585"},"cell_type":"markdown","source":"Now, we will define a grammar pattern / part of speech pattern to identify what type of relations we want to extract from the data. \n\nLet's we are interested in finding an action relation between two named entities. so we can define a pattern using part of speech tags as : \n\nProper Noun - Verb - Proper Noun"},{"metadata":{"trusted":true,"_uuid":"a04ef934585f47980ad78a4168a9301172d98776"},"cell_type":"code","source":"pos_chain_1 = \"NNP-VBZ-NNP\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98847a394b96a3abfc71e8d9ea28ed77f27d4b0a"},"cell_type":"markdown","source":"Using spaCy, we can now iterate in text and identify what are the relevant triplets (governer, relation, dependent) or in other terms, what are the entities and relations."},{"metadata":{"trusted":true,"_uuid":"fd80efd1e651449ee42847f5859feadea27a15ee"},"cell_type":"code","source":"df[\"named_entities\"] = df[\"spacy_title\"].apply(lambda x : x.ents)\nfor i, r in df.iterrows():\n    pos_chain = \"-\".join([d.tag_ for d in r['spacy_title']])\n    if pos_chain_1 in pos_chain:\n        if len(r[\"named_entities\"]) == 2:\n            print (r[\"TITLE\"])\n            print (r[\"named_entities\"])\n            print ()\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1d96806e871e80c0fc0f9316d33db3714fdedfc"},"cell_type":"markdown","source":"So from these examples, one can see different entities and relations for example: \n\n- Honda --- **restructures** ---> US operations  \n- Carl Icahn --- **slams** ---> eBay CEO\n- Google --- **confirms** ---> Android SDK \n- GM --- **hires** ---> Lehman Brothers \n\nReferences : https://kgtutorial.github.io/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}