{"cells":[{"metadata":{"_uuid":"1aa4d7cbbf4659a8c248bbe76bf2c20149246851"},"cell_type":"markdown","source":"**Data_Dictionary**\n\n1. Elevation = Elevation in meters.\n2. Aspect = Aspect in degrees azimuth.\n3. Slope = Slope in degrees.\n4. Horizontal_Distance_To_Hydrology = Horizontal distance to nearest surface water features.\n5. Vertical_Distance_To_Hydrology = Vertical distance to nearest surface water features.\n6. Horizontal_Distance_To_Roadways = Horizontal distance to nearest roadway.\n7. Hillshade_9am = Hill shade index at 9am, summer solstice. Value out of 255.\n8. Hillshade_Noon = Hill shade index at noon, summer solstice. Value out of 255.\n9. Hillshade_3pm = Hill shade index at 3pm, summer solstice. Value out of 255.\n10. Horizontal_Distance_To_Fire_Point = sHorizontal distance to nearest wildfire ignition points.\n11. Wilderness_Area1 = Rawah Wilderness Area\n12. Wilderness_Area2 = Neota Wilderness Area\n13. Wilderness_Area3 = Comanche Peak Wilderness Area\n14. Wilderness_Area4 = Cache la Poudre Wilderness Area\n\n**Soil_Type1 to Soil_Type40 [Total 40 Types]**\n\n**Cover_TypeForest Cover Type designation. Integer value between 1 and 7, with the following key:**\n1. Spruce/Fir\n2.  Lodgepole Pine\n3.  Ponderosa Pine\n4.  Cottonwood/Willow\n5.  Aspen\n6.  Douglas-fir\n7.  Krummholz"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Let's import necessary dependencies \nimport pandas as pd\nimport warnings\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c248de4aa7f54b9b7eeda85eac99714e54913cce","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4a231267092b379f0c20580291324835170bef9","trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\n\n#Read data for analysis\ndata=pd.read_csv('../input/covtype.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8d538112d3f282c00a7deff1c845264c1ee2626"},"cell_type":"markdown","source":" **Explore Data Dimension and count of values without any sneak peek in Data**"},{"metadata":{"_uuid":"28446ffa0321536790ffaa341aea3b3a071ca9f2","trusted":true},"cell_type":"code","source":"print('Data Dimension:')\nprint('Number of Records:', data.shape[0])\nprint('Number of Features:', data.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2de393acf34166d57281f0106f88d318ea1a1dc7","trusted":true},"cell_type":"code","source":"#Names of columns\nprint('Feature Names')\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c3ed1f7d237e31643ab315dc83ce9a4eb2f0d01"},"cell_type":"markdown","source":"**Looks like we got many binary independent features. Good!**\n**Now let us understand the data type of each features**"},{"metadata":{"_uuid":"a70d60142e4e31ef198110e196aa98a83f762124","trusted":true},"cell_type":"code","source":"#A huge list!\nprint(data.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8092bb8682ac9bc9e8ea4d543d0af6c902f88963","trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(6,4))\nsns.countplot(y=data.dtypes ,data=data)\nplt.xlabel(\"Data Type Count\")\nplt.ylabel(\"Data types\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb32704fb4136db53a337c00c32aaf9ee58f60fb"},"cell_type":"markdown","source":"1. **So we have complete Numeric Data, Even Better!!**\n2. **Also there doesn't seem to be any missing value. Good work at Data Collection**"},{"metadata":{"_uuid":"6e0a239f00047a33673a2516919e495d75345425","trusted":true},"cell_type":"code","source":"#Let's check for missing values once again\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eba9bc9d7e547a885bc1cca7d7dd9f712d83a039"},"cell_type":"markdown","source":"**We forgot to check the Data distribution for each feature. Spend some good time here. Lot's of inferences I believe**"},{"metadata":{"_uuid":"c01d0b87307dab2429a1ebb9cfc16768272c2563","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae1de4d26af51cfa77c99589acb234a731eb737c"},"cell_type":"markdown","source":"**#Inferences:**\n1. Few of the features looks skewed, we'll see those later.\n2. No missing Values (We say this for the third time :p)\n3. Wilderness Area and Soil Type are one hot coded.\n4. Scales are different over the whole data, hence might need to scale for some required algorithms."},{"metadata":{"_uuid":"f396cc5ff494b93afe8b4b6e5469357e81f06ab9"},"cell_type":"markdown","source":"**Skewness**\n>The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero. \n>Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. \n>By skewed left, it means that the left tail is long relative to the right tail. Similarly, skewed right means that the right tail is long relative to the left tail."},{"metadata":{"_uuid":"056d617a3263c7a1e9481135db3db5b875643961","trusted":true},"cell_type":"code","source":"print('Skewness of the below features:')\nprint(data.skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a361cb8a3282500c195ed8634903181241768dd1"},"cell_type":"code","source":"skew=data.skew()\nskew_df=pd.DataFrame(skew,index=None,columns=['Skewness'])\nplt.figure(figsize=(15,7))\nsns.barplot(x=skew_df.index,y='Skewness',data=skew_df)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"545f5d1e9145dc7be70f9633bfacd943c5d2ddf4"},"cell_type":"markdown","source":"**#Inferences:**\n> Some of the Variables are heavily skewed hence need to be corrected or transformed  on a later stage. "},{"metadata":{"_uuid":"1e63404074cf4bfa7de169585c33a8acb69638f5"},"cell_type":"markdown","source":"**How about the class balance? We'll see**"},{"metadata":{"_kg_hide-output":false,"_uuid":"27e817978b808cc8825e3ab69199028998bb5a96","trusted":true},"cell_type":"code","source":"class_dist=data.groupby('Cover_Type').size()\nclass_label=pd.DataFrame(class_dist,columns=['Size'])\nplt.figure(figsize=(8,6))\nsns.barplot(x=class_label.index,y='Size',data=class_label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27ae50bd9ce75e3b6d1ad7ca123c6ff05363e03d"},"cell_type":"markdown","source":"> But I'm interested in percentwise distribution of each class. Let's check"},{"metadata":{"_uuid":"27e817978b808cc8825e3ab69199028998bb5a96","trusted":true},"cell_type":"code","source":"for i,number in enumerate(class_dist):\n    percent=(number/class_dist.sum())*100\n    print('Cover_Type',class_dist.index[i])\n    print('%.2f'% percent,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6aec286d5b147df0b177191c40f54eebec16d43"},"cell_type":"markdown","source":"**#Inferences:**\n1.  Cover_Type 1 and 2 i.e **Spruce/Fir** and **Lodgepole Pine** seems to dominate the area. \n2.  Also the Cover_Type 4 i.e **Cottonwood/Willow** is minimal compare to the rest"},{"metadata":{"_uuid":"d99b3e456576d1992800d6d65e878a45ee81ef3e"},"cell_type":"markdown","source":"**Oh common let us check the data atleast, enough with size and dimension**"},{"metadata":{"_uuid":"fb3da20c95716478a1e6f641a6daca53eda86da5","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d0ec24ba66556a084987646695152c7bd74dcce"},"cell_type":"markdown","source":">Nice! Now, Let's convert the whole data into few Mini datasets. I'll make use of it in plots\n* cont_data - Data without binary features i.e continuous features\n* binary_Data - Data having all binary features [Wilderness Areas + Soil Types]\n* wilderness_Data - Binary Wilderness Areas\n* Soil_Data - Binary Soil Types"},{"metadata":{"_uuid":"6d30d7fa96858100cb279ddb7a022c07e1304b5d","trusted":true},"cell_type":"code","source":"cont_data=data.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']\n\nbinary_data=data.loc[:,'Wilderness_Area1':'Soil_Type40']\n\nWilderness_data=data.loc[:,'Wilderness_Area1': 'Wilderness_Area4']\n\nSoil_data=data.loc[:,'Soil_Type1':'Soil_Type40']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0dba7292e6b469286816d1aeff90ceb455c3178"},"cell_type":"markdown","source":"\n**I want to see the number of  values counts within each features, mainly for the Binary types**"},{"metadata":{"_uuid":"dbde0796f7f7f0bf5b6ca5faa22ca109cd352d08","trusted":true},"cell_type":"code","source":"#Iterate via columns of data having only binary features\nfor col in binary_data:\n    count=binary_data[col].value_counts()\n    print(col,count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de68aa24b94a5499d970dc111b1a15e167a325f2"},"cell_type":"markdown","source":"**#Inferences:**\n> **This tells me lots of valuable insights. Mostly regarding the soil types. Wanna know? Ok, let me not hide it from you**.\n* It's Just that, there are some of the Soil types which consists of very few counts.  \n* Statistically speaking, for half a million records, balance number per soil type (total 40 in number) is 581012/40 = 14.5k\n* Whereas, here we see a different figure. I know that data need not be balanced all the times. But may be we can get rid of really small size features. Isn't it?\n* Let me list down those along with there size. I'm displaying the Soil type having less than 1000 occurence size"},{"metadata":{"_uuid":"b576e27da307517838ed6741c8508570f71ff740","trusted":true},"cell_type":"code","source":"print('Soil Type',' Occurence_count')\nfor col in binary_data:\n    count=binary_data[col].value_counts()[1] #considering all one's among 1 and 0's in each soil type\n    if count < 1000:\n        print(col,count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c8b005f8919c5b7be1b54411360dfe0fbd91860"},"cell_type":"markdown","source":"* I know this will make more sense in a visual such as bar graph right? I'm excited to see it too. But let's infer more from the numbers as of now. \n* We'll do plottings once we start with Bivariate and Multivariate analysis. \n* We'll see if we need to really drop the above soil types. \n* We can only confirm on it if it is not aligned (give any relation) to our target variable i.e Cover_Type. So, please wait, do not conclude. Climax is yet to come :D"},{"metadata":{"_uuid":"671a69114a215e0e1bde23325d7b6bf37ada2b46"},"cell_type":"markdown","source":"**Let's get started with plots based EDA (Exploratory Data Analysis) **\n*  Fun begins here, am I right?\n* Data Distribution of features via Histograms. Although I love box plots more than histograms, we'll use boxplot to check distribution with respect to categorical variable. In our case that is Cover_Type, having 7 different category of classes."},{"metadata":{"_uuid":"cef9801666851dc8a3e79c88e5916cd058f9d2a0","trusted":true},"cell_type":"code","source":"# data_num = data.select_dtypes([np.int, np.float]) #If you need to select only numeric features. \n#Here we already have all numeric Data.\n\nfor i, col in enumerate(cont_data.columns):\n    plt.figure(i)\n    sns.distplot(cont_data[col])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fa9c8b73a7e8053d0bc19d9909359ef008e5f5b"},"cell_type":"markdown","source":"* > The above plots more or less tells us about the skewness that we saw earlier. Let's dig down into Bivariate and Multivariate Analysis\n* > Let's check for distribution with respect to our target. This is where magic happens!"},{"metadata":{"_uuid":"60865c704c31262f0a1919984a10c371c0bd2508"},"cell_type":"markdown","source":"* > Here, First i want to check the shape of continous features with respect to the target class. Hence I'll use the continuous_data (cont_data) and plot a boxplot against target. \n* > You can also look at violinplot here, It's visually appealing. "},{"metadata":{"_uuid":"28b0a53df98b09d9ff5fe7427d042db3030544ae","trusted":true},"cell_type":"code","source":"# %%time\ndata['Cover_Type']=data['Cover_Type'].astype('category') #To convert target class into category\n\nfor i, col in enumerate(cont_data.columns):\n    plt.figure(i,figsize=(8,4))\n    sns.boxplot(x=data['Cover_Type'], y=col, data=data, palette=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5527d099d4e9b436a7224bd7cfbfca04b048697"},"cell_type":"markdown","source":"*  Plots looks cool right? What's Even more cool you know?\n*  The insights. Let's figure out very general insights\n*  There are couple of features which shows not much of variance with respect to classes\n*  And features such as 'Elevation', 'slope' and 'horizontal distance to road_ways does a good job"},{"metadata":{"_uuid":"611cc005d1ba2665ccc13df4bf4d53c68357742c"},"cell_type":"markdown","source":"> Let's do something similar for our binary features. This time we'll use countplot."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"026c5430f1e2d76da2e53b91949e81303d7fa616","trusted":true},"cell_type":"code","source":"%%time\nfor i, col in enumerate(binary_data.columns):\n    plt.figure(i,figsize=(6,4))\n    sns.countplot(x=col, hue=data['Cover_Type'] ,data=data, palette=\"rainbow\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4af3dd3eb77700491615e664bde069328259b87f"},"cell_type":"markdown","source":"* > So the plot does justice to the distribution which each class but I want to have a single feature having Soil_Type corresponding to each row. \n* > Let's see if I can do it.  This will help me to visualize it better, instead of counting 0's and 1's in each one hot coded Soil types."},{"metadata":{"_uuid":"c8bf550289be6b393c68a8dd5006021c629490e0","trusted":true},"cell_type":"code","source":"%%time\n#If someone can help me with function to reverse one hot coding, please let me know in comment. I know this is not the robust way.\ndef rev_code(row):\n    for c in Soil_data.columns:\n        if row[c]==1:\n            return c  \n\ndata['Soil_Type']=Soil_data.apply(rev_code, axis=1) #Time consuming","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b13373d2ba5ec6baaaa57c54b9afe80e32e19a52"},"cell_type":"markdown","source":"> I'll do the same for Wilderness Area"},{"metadata":{"_uuid":"ae266ab5a58001b165670e2477e5196362ed261a","trusted":true},"cell_type":"code","source":"%%time\ndef rev_code(row):\n    for c in Wilderness_data.columns:\n        if row[c]==1:\n            return c  \n\ndata['Wilderness_Type']=Wilderness_data.apply(rev_code, axis=1) #Time consuming","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c4d1b1505cd77f8288a226f593f650529912387"},"cell_type":"markdown","source":"> Yup! It's done. Looks like we have a desired single Soil_Type and Wilderness_Type feature. Let's now use count plot against our Target Cover_Type"},{"metadata":{"_uuid":"254bf19bacf98f657ee426e20274b3eed84d20d0","trusted":true},"cell_type":"code","source":"%%time\nplt.figure(figsize=(16,8))\nsns.countplot(x='Wilderness_Type', hue='Cover_Type',data=data, palette=\"rainbow\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f36598f67a4eef65616196682df3d2dfe4097f5","trusted":true},"cell_type":"code","source":"%%time\nplt.figure(figsize=(16,8))\nsns.countplot(x='Soil_Type', hue='Cover_Type',data=data, palette=\"rainbow\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92fe920a395066bddd09e1a32321c23519f788cb"},"cell_type":"markdown","source":"* >Above two plots tells  us the count of trees in each class considering Wilderness and Soil Type.\n* >Soil_Type plot is not very clear since it's  too vast. So let's go by the number. We'll see how many and what type of Cover_Type we have under each soil Type"},{"metadata":{"_uuid":"b1a94e34616decd479da57562c1160e201148952","trusted":true},"cell_type":"code","source":"soil_counts = []\nfor num in range(1,41):\n    col = ('Soil_Type' + str(num))\n    this_soil = data[col].groupby(data['Cover_Type'])\n    totals = []\n    for value in this_soil.sum():\n        totals.append(value)\n    total_sum = sum(totals)\n    soil_counts.append(total_sum)\n    print(\"Total Trees in Soil Type {0}: {1}\".format(num, total_sum))\n    percentages = [ (total*100 / total_sum) for total in totals]\n    print(\"{0}\\n\".format(percentages))\nprint(\"Number of trees in each soil type:\\n{0}\".format(soil_counts))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2218fa8afa6140b2dd3118eefb05b3e40d1c41e"},"cell_type":"markdown","source":"**Did we check the co-relation??**\n * > No we didn't. This is something that I usually check first. No, problem. it's never too late.\n * > Let's better vizualise it via heatmap. All in one!"},{"metadata":{"_uuid":"be8c8b4eb171c74fca69a7bf1e0ef0682c246a08","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.heatmap(cont_data.corr(),cmap='magma',linecolor='white',linewidths=1,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bbc24881ed9b3afc6b273f4551e3f32241d7d10"},"cell_type":"markdown","source":"* >Couple of features are have a good amount of co-relation. Guess which one? I'll tell you.\n* >  Hillshade_9am ~ Hillshade_3pm and Aspect ~ Hillshade_3pm"},{"metadata":{"_uuid":"7d7e6149ea19d2b34eebd61ec735151d71d392f4","trusted":true},"cell_type":"code","source":"g = sns.PairGrid(cont_data)\ng.map(plt.scatter)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"052943bc469edeef5ffe603e375773668d6130e8"},"cell_type":"markdown","source":"* > This gives us the relation and its shape with respect to other features. Various inferences can be drwan out.\n* > Pairgrid plot is just awesome. And it's even more awesome when it's combined with KDE clusters. \n* > But for considerably heavy data, its time consuming. Be aware before running the below plot."},{"metadata":{"_uuid":"5519a2c95757982d4b9cdfeec6dd3605cb8c07a0","trusted":true},"cell_type":"code","source":"# %%time\n# g = sns.PairGrid(cont_data)\n# g.map_diag(plt.hist)\n# g.map_upper(sns.kdeplot)\n# g.map_lower(sns.kdeplot)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b54c06bd227e324931d6d02bef47cfafc1bf6fc"},"cell_type":"markdown","source":"* > There's lot of scope for Data Viz. as far this dataset is concerned. My objective was a surface walkthrough the dataset. I would roll out new versions on this part by part. \n* >Let's now wind it up by Data Modelling. Another Excitement, right?"},{"metadata":{"_uuid":"fc23b55e7317cdda4dac2c26de0158b379d9e0a6"},"cell_type":"markdown","source":"**Data Modelling**"},{"metadata":{"_uuid":"be8305ee10ee4b5facf6abfbaacafe6476948ca3"},"cell_type":"markdown","source":"* X = Input or independent variables\n* y=  Target variable ('Cover_Type')"},{"metadata":{"_uuid":"6a2b6699b55b41042e877d4363e3175cdb3c7ed4","trusted":true},"cell_type":"code","source":"X=data.loc[:,'Elevation':'Soil_Type40']\ny=data['Cover_Type']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fbfa3b573fc3aa58c9a6ba9fb647a549a08fa6a"},"cell_type":"markdown","source":">Let us take a step to remove the features with low Std deviation as demonstrated earlier. \n>Also I'll remove one of the co-related variable"},{"metadata":{"_uuid":"9e4e088c48c0bf52ce1f45a8494d47c565805044","trusted":true},"cell_type":"code","source":"#Features to be removed before the model\nrem=['Hillshade_3pm','Soil_Type7','Soil_Type8','Soil_Type14','Soil_Type15',\n     'Soil_Type21','Soil_Type25','Soil_Type28','Soil_Type36','Soil_Type37']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d8c007c9976926b1f1226cac6a932bb561e909f","trusted":true},"cell_type":"code","source":"#Remove the unwanted features\nX.drop(rem, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"543c845b245fb48bdaea6f0e1cb5eec10cae07ba","trusted":true},"cell_type":"code","source":"#Splitting the data into  train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58215d4477a96dbb0386d1097e73afc78f67d733"},"cell_type":"markdown","source":"* ** I have tried various Classification algorithms out of which KNN served the best.**\n* ** Algorithms such as RandomForest and DecisionTree are doing a decent job here. So please explore.**"},{"metadata":{"_uuid":"d4c634bad923190233c65cd54dd71cad05ae25b9","trusted":true},"cell_type":"code","source":"%%time\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,7)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe44b85c5df770a14ccbb0b90f72da658dac0c24"},"cell_type":"markdown","source":"> Let's visualize the change in accuracies with respect to train and test data at different neighbors "},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"1e3395d10408f7351b806fdca7131470ebed120f","trusted":true},"cell_type":"code","source":"#Generate plot\nplt.figure(figsize=(10,6))\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e391ba0dfe0c5811a00a339a040f036487c1213"},"cell_type":"markdown","source":"> Neighbor value = 5 yeilds the best result. Let's go by that for now. "},{"metadata":{"_uuid":"a5a060a18848c522ca5609a42552e5aea100ede8","trusted":true},"cell_type":"code","source":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(n_neighbors=5) #Using Eucledian distance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffaad4771a1056b28705a14705c1c53879839715","trusted":true},"cell_type":"code","source":"#Fit the model\nknn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7e81e2fe6bb2b7565605a97e07312a6b152dca7","trusted":true},"cell_type":"code","source":"#Get accuracy. Note: In case of classification algorithms score method represents accuracy.\nAccuracy=knn.score(X_test,y_test)\nprint('KNN Accuracy:',Accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94a92a0773d9341776b0ae37ce32f549e4e09ce4"},"cell_type":"markdown","source":"**Not bad. KNN works great here. Lazy learner is doing a good work at differentiating a CoverType. **"},{"metadata":{"_uuid":"49b6572a1c145fb30763e88ac0433c5aa613b2d6"},"cell_type":"markdown","source":"**I'll put the accuracies obtained by various other classification techniques. Try to enhance it more via Cross Validation may be.**\n**Let me know in comment if you manage to raise your accuracies. **\n> **I'm gonna do it too. Those are my next steps, such as CV, more insights, Feature engineering etc. I'll roll out updates part by part**"},{"metadata":{"_uuid":"22a7d409b3d1326f52c288ed54af273d9c7bacef","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import scipy.stats as ss\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"319f975de7d9ae014f56bf271754995872a237c2","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\nMLA = []\nZ = [LinearSVC() , DecisionTreeClassifier() , LogisticRegression() , GaussianNB() ,RandomForestClassifier() , \n     GradientBoostingClassifier()]\nX = [\"LinearSVC\" , \"DecisionTreeClassifier\" , \"LogisticRegression\" , \"GaussianNB\" ,\"RandomForestClassifier\" , \n     \"GradientBoostingClassifier\"]\n\nfor i in range(0,len(Z)):\n    model = Z[i]\n    model.fit( X_train , y_train )\n    pred = model.predict(X_test)\n    MLA.append(accuracy_score(pred , y_test))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"568a816c887c1ffad49f8db7743c8d2c7c59fb5a"},"cell_type":"code","source":"d = { \"Algorithm\" : X, \"Accuracy\" : MLA }\n\ndfm = pd.DataFrame(d)\ndfm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5ab39c24eef7bff7345251d78ea1395cf6ad21c"},"cell_type":"markdown","source":"*  **Try to surpass these accuracies. **\n*  **My objective was to 'Get to know' the Forest Cover Type Dataset for which I tried to articulate it step by step.**\n*  **If you liked it, please let me know with a upvote, It serves a Motivation. Good Luck and Thank you for spending your time here! :)**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}