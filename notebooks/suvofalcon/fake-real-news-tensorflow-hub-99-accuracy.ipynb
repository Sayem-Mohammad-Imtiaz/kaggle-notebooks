{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-07T06:07:27.805062Z","iopub.execute_input":"2021-08-07T06:07:27.805553Z","iopub.status.idle":"2021-08-07T06:07:27.828068Z","shell.execute_reply.started":"2021-08-07T06:07:27.80545Z","shell.execute_reply":"2021-08-07T06:07:27.827108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Library imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:27.829307Z","iopub.execute_input":"2021-08-07T06:07:27.829724Z","iopub.status.idle":"2021-08-07T06:07:34.555498Z","shell.execute_reply.started":"2021-08-07T06:07:27.829681Z","shell.execute_reply":"2021-08-07T06:07:34.554798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset and validate the data load\n\nWe will load the individual dataset, create a target attribute which will indicate 1 if the news is fake. Combine both the dataframes and create the combined dataframe for modelling","metadata":{}},{"cell_type":"code","source":"# Load the fake and real news datasets\nfake_news = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\nfake_news[\"fake\"] = 1\n\n# Load the real news \nreal_news = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nreal_news[\"fake\"] = 0\n\n# We will join the two dataframes and create the combined one for modelling\n\nnews = pd.concat([fake_news, real_news])\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:34.556824Z","iopub.execute_input":"2021-08-07T06:07:34.557203Z","iopub.status.idle":"2021-08-07T06:07:37.464779Z","shell.execute_reply.started":"2021-08-07T06:07:34.557174Z","shell.execute_reply":"2021-08-07T06:07:37.46376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for any null values\nnews.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:37.466044Z","iopub.execute_input":"2021-08-07T06:07:37.466441Z","iopub.status.idle":"2021-08-07T06:07:37.487055Z","shell.execute_reply.started":"2021-08-07T06:07:37.466401Z","shell.execute_reply":"2021-08-07T06:07:37.486115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the data structure\nnews.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:37.488424Z","iopub.execute_input":"2021-08-07T06:07:37.488843Z","iopub.status.idle":"2021-08-07T06:07:37.52308Z","shell.execute_reply.started":"2021-08-07T06:07:37.488801Z","shell.execute_reply":"2021-08-07T06:07:37.522403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis and Data Visualizations","metadata":{}},{"cell_type":"code","source":"# Explore the target variable\nsns.countplot(x='fake', data=news)\nprint(\"Distributions...\")\nprint(news['fake'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:37.524216Z","iopub.execute_input":"2021-08-07T06:07:37.524478Z","iopub.status.idle":"2021-08-07T06:07:37.826857Z","shell.execute_reply.started":"2021-08-07T06:07:37.524452Z","shell.execute_reply":"2021-08-07T06:07:37.825846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore 2 texts for the fake dataset\nnews[news['fake'] == 1]['text'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:37.828289Z","iopub.execute_input":"2021-08-07T06:07:37.828673Z","iopub.status.idle":"2021-08-07T06:07:37.855497Z","shell.execute_reply.started":"2021-08-07T06:07:37.828633Z","shell.execute_reply":"2021-08-07T06:07:37.854796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore 2 textx for the real news\nnews[news['fake'] == 0]['text'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:37.857308Z","iopub.execute_input":"2021-08-07T06:07:37.857757Z","iopub.status.idle":"2021-08-07T06:07:37.867099Z","shell.execute_reply.started":"2021-08-07T06:07:37.857727Z","shell.execute_reply":"2021-08-07T06:07:37.866387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore the Subject column\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='subject', data=news, hue='fake')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:37.868475Z","iopub.execute_input":"2021-08-07T06:07:37.86877Z","iopub.status.idle":"2021-08-07T06:07:38.139538Z","shell.execute_reply.started":"2021-08-07T06:07:37.868735Z","shell.execute_reply":"2021-08-07T06:07:38.138832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### We will create a new columns called Month and Year from Date and analyse whether fake or real news has some correlation with Month or Year in the timeline","metadata":{}},{"cell_type":"code","source":"news['date'] = pd.to_datetime(news['date'], errors='coerce')\nnews['Year'] = news['date'].dt.year\nnews['Month'] = news['date'].dt.month\n\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:38.14083Z","iopub.execute_input":"2021-08-07T06:07:38.141129Z","iopub.status.idle":"2021-08-07T06:07:38.396978Z","shell.execute_reply.started":"2021-08-07T06:07:38.141102Z","shell.execute_reply":"2021-08-07T06:07:38.396158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the impact of Year on the target variable\nsns.countplot(x='Year', data=news, hue='fake')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:38.397954Z","iopub.execute_input":"2021-08-07T06:07:38.398194Z","iopub.status.idle":"2021-08-07T06:07:38.594149Z","shell.execute_reply.started":"2021-08-07T06:07:38.39817Z","shell.execute_reply":"2021-08-07T06:07:38.593162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All news in the year 2015 in the dataset is a fake news. So this attribute has a level which perfectly distributes the target variable.","metadata":{}},{"cell_type":"code","source":"# Check the impact of Month on the target variable\nsns.countplot(x='Month', data=news, hue='fake')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:38.597135Z","iopub.execute_input":"2021-08-07T06:07:38.59741Z","iopub.status.idle":"2021-08-07T06:07:38.870114Z","shell.execute_reply.started":"2021-08-07T06:07:38.597383Z","shell.execute_reply":"2021-08-07T06:07:38.869107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows an interesting pattern - The number of fake news is higher till month 8, post which the number of real news increases drastically. Which essentially means if the month is <= 8, the probability of fake news is higher. ","metadata":{}},{"cell_type":"markdown","source":"#### We will combine the title and text column","metadata":{}},{"cell_type":"code","source":"news['text'] = news['title'] + news['text']\nnews.drop(labels=['title'], axis=1, inplace=True)\n\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:38.87142Z","iopub.execute_input":"2021-08-07T06:07:38.871865Z","iopub.status.idle":"2021-08-07T06:07:39.05597Z","shell.execute_reply.started":"2021-08-07T06:07:38.871835Z","shell.execute_reply":"2021-08-07T06:07:39.055038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the final data\n\nWe will remove the subject attribute - Since it perfectly distributes the target variable\nWe will remove the Year attribute - This also has a clear division for the target variable\nWe will remove the Month Attribute - This also has a very clear approach of demarcating the target variable\n\nFor now we will just go ahead with the \"text attribute\"","metadata":{}},{"cell_type":"code","source":"news.drop(labels=['subject', 'date', 'Year', 'Month'], axis=1, inplace=True)\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.057175Z","iopub.execute_input":"2021-08-07T06:07:39.057455Z","iopub.status.idle":"2021-08-07T06:07:39.072839Z","shell.execute_reply.started":"2021-08-07T06:07:39.057427Z","shell.execute_reply":"2021-08-07T06:07:39.071824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Test Split","metadata":{}},{"cell_type":"code","source":"# We will shuffle the dataframe and extract the feature and label\n\nnews = news.sample(frac=1)\nfeature_text = news['text']\ntarget = news['fake']","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.074065Z","iopub.execute_input":"2021-08-07T06:07:39.074492Z","iopub.status.idle":"2021-08-07T06:07:39.086816Z","shell.execute_reply.started":"2021-08-07T06:07:39.074427Z","shell.execute_reply":"2021-08-07T06:07:39.085986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform the split\nfeatures_train, features_test, target_train, target_test = train_test_split(feature_text, target, test_size=0.3, \n                                                                            random_state=101)\n\n# We will further split the training set into validatoion to evaluate the Neural Network training\nfeatures_train, features_val, target_train, target_val = train_test_split(features_train, target_train, test_size=0.3, \n                                                                            random_state=101)\n\nprint(\"Training Features shape: \", features_train.shape)\nprint(\"Training Target shape: \", target_train.shape)\n\nprint(\"Validation Features shape: \", features_val.shape)\nprint(\"Validation Target shape: \", target_val.shape)\n\nprint(\"Test Features shape: \", features_test.shape)\nprint(\"Training Target shape: \", target_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.08817Z","iopub.execute_input":"2021-08-07T06:07:39.088617Z","iopub.status.idle":"2021-08-07T06:07:39.11343Z","shell.execute_reply.started":"2021-08-07T06:07:39.088573Z","shell.execute_reply":"2021-08-07T06:07:39.112503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First 10 training samples\nfeatures_train[: 10]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.114639Z","iopub.execute_input":"2021-08-07T06:07:39.114926Z","iopub.status.idle":"2021-08-07T06:07:39.121571Z","shell.execute_reply.started":"2021-08-07T06:07:39.114897Z","shell.execute_reply":"2021-08-07T06:07:39.12066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First 10 training classes (target)\ntarget_train[: 10]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.122785Z","iopub.execute_input":"2021-08-07T06:07:39.123083Z","iopub.status.idle":"2021-08-07T06:07:39.135916Z","shell.execute_reply.started":"2021-08-07T06:07:39.123049Z","shell.execute_reply":"2021-08-07T06:07:39.134869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build and Train the Neural Network Model","metadata":{}},{"cell_type":"code","source":"# Define some global Model Constants\n\nINPUT_SHAPE = []\n\nOUTPUT_UNITS = 1\nHIDDEN_UNITS_SINGLE = 16\nHIDDEN_UNITS_DEEP = 8\nACTIVATION_HIDDEN = tf.keras.activations.relu\nACTIVATION_OUTPUT = tf.keras.activations.sigmoid\nLEARNING_RATE = 1e-3\nOPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\nLOSS_FUNCTION = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nL2_REGULARIZER = tf.keras.regularizers.L2(0.001)\nDROPOUT_RATE = 0.2\n\nEPOCHS = 3","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.137282Z","iopub.execute_input":"2021-08-07T06:07:39.137563Z","iopub.status.idle":"2021-08-07T06:07:39.153939Z","shell.execute_reply.started":"2021-08-07T06:07:39.137538Z","shell.execute_reply":"2021-08-07T06:07:39.152719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the Model Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"# Define the Metrics - These are the metrics we will evaluate during training\n\nMETRICS = [tf.keras.metrics.TruePositives(name='tp'),\n          tf.keras.metrics.FalsePositives(name='fp'),\n          tf.keras.metrics.TrueNegatives(name='tn'),\n          tf.keras.metrics.FalseNegatives(name='fn'), \n          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n          tf.keras.metrics.Precision(name='precision'),\n          tf.keras.metrics.Recall(name='recall'),\n          tf.keras.metrics.AUC(name='auc')]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.155202Z","iopub.execute_input":"2021-08-07T06:07:39.155635Z","iopub.status.idle":"2021-08-07T06:07:39.237919Z","shell.execute_reply.started":"2021-08-07T06:07:39.155605Z","shell.execute_reply":"2021-08-07T06:07:39.23703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### In order to perform text processing, we would be using a pre-trained embedding layer from tensorflow-hub\nWe will create a Keras Layer that uses tensorflow hub model to embed sentences\n\n#### The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The model that we are using (google/nnlm-en-dim128/2) splits the sentence into tokens, embeds each token and then combines the embedding.","metadata":{}},{"cell_type":"code","source":"model_embeddings = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\nhub_layer = hub.KerasLayer(model_embeddings, input_shape=INPUT_SHAPE, dtype=tf.string, trainable=True)\n\n# We will use it in the first two samples and check\nhub_layer(features_train[:2])","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:39.238996Z","iopub.execute_input":"2021-08-07T06:07:39.239287Z","iopub.status.idle":"2021-08-07T06:07:51.102655Z","shell.execute_reply.started":"2021-08-07T06:07:39.239244Z","shell.execute_reply":"2021-08-07T06:07:51.101706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build and Compile the Model","metadata":{}},{"cell_type":"code","source":"# Defining a function which will build and compile the model\n\n'''\nThis will build and compile a model with one hidden layer and 16 neurons\n'''\ndef make_model(metrics=METRICS, output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    model = tf.keras.Sequential()\n    model.add(hub_layer)\n    model.add(tf.keras.layers.Dense(units=HIDDEN_UNITS_SINGLE, activation=ACTIVATION_HIDDEN))\n    model.add(tf.keras.layers.Dense(units=OUTPUT_UNITS, activation=ACTIVATION_OUTPUT))\n    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=metrics)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:51.103765Z","iopub.execute_input":"2021-08-07T06:07:51.104061Z","iopub.status.idle":"2021-08-07T06:07:51.110771Z","shell.execute_reply.started":"2021-08-07T06:07:51.104033Z","shell.execute_reply":"2021-08-07T06:07:51.109099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to plot training loss vs validation loss\n\n'''\nThis function will take a epoch model from training a neural network\nWill plot training loss vs validation loss\n'''\n\ndef plotTrainLossVsValLoss(epochs_history):\n    plt.figure(figsize=(12, 8))\n    loss_train = epochs_history.history['loss']\n    loss_val = epochs_history.history['val_loss']\n\n    plt.figure(figsize=(12, 8))\n\n    loss_train = epochs_history.history['loss']\n    loss_val = epochs_history.history['val_loss']\n\n    epochs = range(1, (EPOCHS + 1))\n    plt.plot(epochs, loss_train, 'g', label='Training loss')\n    plt.plot(epochs, loss_val, 'b', label='Validation loss')\n    plt.title('Training Loss vs Validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:51.115056Z","iopub.execute_input":"2021-08-07T06:07:51.115424Z","iopub.status.idle":"2021-08-07T06:07:51.124254Z","shell.execute_reply.started":"2021-08-07T06:07:51.115392Z","shell.execute_reply":"2021-08-07T06:07:51.12332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to plot training accuracy vs validation accuracy\n\n'''\nThis function will take a epoch model from training a neural network\nWill plot training accuracy vs validation accuracy\n'''\n\ndef plotTrainAccuracyVsValAccuracy(epochs_history):\n    plt.figure(figsize=(12, 8))\n\n    loss_train = epochs_history.history['accuracy']\n    loss_val = epochs_history.history['val_accuracy']\n\n    epochs = range(1, (EPOCHS + 1))\n    plt.plot(epochs, loss_train, 'g', label='Training Accuracy')\n    plt.plot(epochs, loss_val, 'b', label='Validation Accuracy')\n    plt.title('Training Accuracy vs Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:51.12554Z","iopub.execute_input":"2021-08-07T06:07:51.12582Z","iopub.status.idle":"2021-08-07T06:07:51.13976Z","shell.execute_reply.started":"2021-08-07T06:07:51.125763Z","shell.execute_reply":"2021-08-07T06:07:51.138726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to plot the confusion matrix\n# Let us visualize the Confusion Matrix and detail out some key metrices including classification report\n\n'''\nThis function will plot the confusion matrix \nThis will also display various performance metrices\n'''\ndef plot_cm(labels, predictions, threshold=0.5):\n    cm = confusion_matrix(labels, predictions > threshold)\n    plt.figure(figsize=(5, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title(\"Confusion Matrix %0.2f\" %threshold)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"Actual Label\")\n    \n    print('True Negatives: ', cm[0][0])\n    print('Incorrectly Detected (False Positives): ', cm[0][1])\n    print('Missed (False Negatives): ', cm[1][0])\n    print('True Positives: ', cm[1][1])\n    print('Total Transactions: ', np.sum(cm[1]))\n    print(\"\\n\")\n    print(\"F1-Score\")\n    print(f1_score(target_test, target_predictions > 0.5))\n    print(\"\\n\")\n    print(\"Accuracy Score\")\n    print(accuracy_score(target_test, target_predictions > threshold))\n    print(\"\\n\")\n    print(\"Classification Report\")\n    print(classification_report(target_test, target_predictions > 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T07:19:26.735522Z","iopub.execute_input":"2021-08-07T07:19:26.735892Z","iopub.status.idle":"2021-08-07T07:19:26.744083Z","shell.execute_reply.started":"2021-08-07T07:19:26.735859Z","shell.execute_reply":"2021-08-07T07:19:26.742877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets build the model and see the mmodel summary\n\nsimplemodel = make_model()\nsimplemodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:51.153074Z","iopub.execute_input":"2021-08-07T06:07:51.153518Z","iopub.status.idle":"2021-08-07T06:07:51.592951Z","shell.execute_reply.started":"2021-08-07T06:07:51.153476Z","shell.execute_reply":"2021-08-07T06:07:51.59206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will now train the model on training and validation data\n# Now use the function to plot the confusion matrix\n\nstart = datetime.now()\nepochs_history_simple = simplemodel.fit(features_train, target_train, epochs=EPOCHS,\n                          validation_data=(features_val, target_val),\n                          verbose=1)\nend = datetime.now()\nprint(f\"The training of simple model completed in time - {end - start}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:07:51.594039Z","iopub.execute_input":"2021-08-07T06:07:51.594301Z","iopub.status.idle":"2021-08-07T07:07:51.179773Z","shell.execute_reply.started":"2021-08-07T06:07:51.594274Z","shell.execute_reply":"2021-08-07T07:07:51.17813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check Performance Graphs","metadata":{}},{"cell_type":"code","source":"# Plot training loss vs validation loss\nplotTrainLossVsValLoss(epochs_history=epochs_history_simple)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T07:16:07.174153Z","iopub.execute_input":"2021-08-07T07:16:07.174673Z","iopub.status.idle":"2021-08-07T07:16:07.410109Z","shell.execute_reply.started":"2021-08-07T07:16:07.174628Z","shell.execute_reply":"2021-08-07T07:16:07.40893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Training accuracy vs Validation accuracy\nplotTrainAccuracyVsValAccuracy(epochs_history=epochs_history_simple)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T07:16:18.10146Z","iopub.execute_input":"2021-08-07T07:16:18.101811Z","iopub.status.idle":"2021-08-07T07:16:18.297647Z","shell.execute_reply.started":"2021-08-07T07:16:18.101767Z","shell.execute_reply":"2021-08-07T07:16:18.29673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Validation","metadata":{}},{"cell_type":"code","source":"# Let us run the predictions\ntarget_predictions = simplemodel.predict(features_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T07:16:42.996214Z","iopub.execute_input":"2021-08-07T07:16:42.996556Z","iopub.status.idle":"2021-08-07T07:16:45.457551Z","shell.execute_reply.started":"2021-08-07T07:16:42.996526Z","shell.execute_reply":"2021-08-07T07:16:45.456777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now use the function to plot the confusion matrix\nplot_cm (target_test, target_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T07:19:31.110966Z","iopub.execute_input":"2021-08-07T07:19:31.111312Z","iopub.status.idle":"2021-08-07T07:19:31.446769Z","shell.execute_reply.started":"2021-08-07T07:19:31.111283Z","shell.execute_reply":"2021-08-07T07:19:31.445809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}