{"cells":[{"metadata":{},"cell_type":"markdown","source":"**[SQL Home Page](https://www.kaggle.com/learn/intro-to-sql)**\n\n---\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hypothesis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Project ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\nfrom google.cloud import bigquery\n\ndata_dropout = pd.read_csv('../input/indian-school-education-statistics/dropout-ratio-2012-2015.csv')\ndata_gross = pd.read_csv('../input/indian-school-education-statistics/gross-enrollment-ratio-2013-2016.csv')\ndata_comps = pd.read_csv('../input/indian-school-education-statistics/percentage-of-schools-with-comps-2013-2016.csv')\ndata_electricity = pd.read_csv('../input/indian-school-education-statistics/percentage-of-schools-with-electricity-2013-2016.csv')\ndata_water_facility = pd.read_csv('../input/indian-school-education-statistics/percentage-of-schools-with-water-facility-2013-2016.csv')\ndata_boys_toilet = pd.read_csv('../input/indian-school-education-statistics/schools-with-boys-toilet-2013-2016.csv')\ndata_girls_toilet = pd.read_csv('../input/indian-school-education-statistics/schools-with-girls-toilet-2013-2016.csv')\n\n\nprint(data_dropout.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sqlite3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conn = sqlite3.connect(':memory:')\ncur = conn.cursor()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dropout.to_sql(name='dropout', con=conn)\n# cur.execute('SELECT * FROM dropout')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SQL command to create a table in the database \nsql_command = \"\"\"SELECT * FROM dropout\"\"\"\n  \n# execute the statement \ncur.execute(sql_command) \n\n# store all the fetched data in the ans variable \nans = cur.fetchall() \n\n#for x in ans:\n#  print(x)\n\ndf = pd.DataFrame (ans,columns=['Id','State_UT','year','Primary_Boys','Primary_Girls','Primary_Total','Upper Primary_Boys','Upper Primary_Girls', 'Upper Primary_Total', 'Secondary _Boys', 'Secondary _Girls', 'Secondary _Total', 'HrSecondary_Boys', 'HrSecondary_Girls', 'HrSecondary_Total'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# close the connection \ncur.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://) Import the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Use some of the data aggregation functions, regression or other methods to test your hypothesis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Prediction using Regression ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe first test of your new data exploration skills uses data describing crime in the city of Chicago.\n\nBefore you get started, run the following cell. It sets up the automated feedback system to review your answers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[](http://)Use the next code cell to fetch the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"chicago_crime\" dataset\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List all the tables in the \"hacker_news\" dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there are four!)\nfor table in tables:  \n    print(table.table_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct a reference to the \"global_air_quality\" table\ntable_ref = dataset_ref.table(\"global_air_quality\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview the first five entries in the \"by\" column of the \"global_air_quality\" table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        ORDER BY timestamp DESC\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the query\nquery_job = client.query(query)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# API request - run the query, and return a pandas DataFrame\nus_cities = query_job.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_cities.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What five cities have the most measurements?\nus_cities.city.value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"query_improved = \"\"\"\n        SELECT city, \n               EXTRACT(DAYOFWEEK FROM timestamp) AS Year\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        GROUP BY year\n        ORDER BY city \n        \"\"\"\n# Set up the query\nquery_job = client.query(query)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query, job_config=safe_config)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\npopular_comments.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exercises\n\n### 1) Count tables in the dataset\n\nHow many tables are in the Chicago Crime dataset?","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Write the code you need here to figure out the answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_tables = ____  # Store the answer as num_tables and then run this cell\n\n# Check your answer\nq_1.check()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a hint or the solution, uncomment the appropriate line below.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#q_1.hint()\n#q_1.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Explore the table schema\n\nHow many columns in the `crime` table have `TIMESTAMP` data?","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Write the code to figure out the answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_timestamp_fields = ____ # Put your answer here\n\n# Check your answer\nq_2.check()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a hint or the solution, uncomment the appropriate line below.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#q_2.hint()\n#q_2.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) Create a crime map\n\nIf you wanted to create a map with a dot at the location of each crime, what are the names of the two fields you likely need to pull out of the `crime` table to plot the crimes on a map?","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Write the code here to explore the data so you can find the answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fields_for_plotting = [____, ____] # Put your answers here\n\n# Check your answer\nq_3.check()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a hint or the solution, uncomment the appropriate line below.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#q_3.hint()\n#q_3.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thinking about the question above, there are a few columns that appear to have geographic data. Look at a few values (with the `list_rows()` command) to see if you can determine their relationship.  Two columns will still be hard to interpret. But it should be obvious how the `location` column relates to `latitude` and `longitude`.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Scratch space for your code","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keep going\n\nYou've looked at the schema, but you haven't yet done anything exciting with the data itself. Things get more interesting when you get to the data, so keep going to **[write your first SQL query](https://www.kaggle.com/dansbecker/select-from-where).**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---\n**[SQL Home Page](https://www.kaggle.com/learn/intro-to-sql)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161314) to chat with other Learners.*","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}