{"cells":[{"metadata":{},"cell_type":"markdown","source":"* # Churn Modeling, Classification Task"},{"metadata":{},"cell_type":"markdown","source":"### Description of the problem"},{"metadata":{},"cell_type":"markdown","source":"The data set contains details of customers of a bank.\nThe target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer. This variable is labeled in the data as 'Exited'\n"},{"metadata":{},"cell_type":"markdown","source":"### Loading the required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport random\nfrom numpy import mean\nfrom pprint import pprint\n\n\n# Visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.gridspec as gridspec\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Modelization\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scikitplot.estimators import plot_feature_importances\nimport category_encoders as ce\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.model_selection import train_test_split\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom eli5.sklearn.explain_weights import explain_decision_tree, explain_rf_feature_importance\nimport xgboost as xgb\nfrom eli5.xgboost import explain_weights_xgboost\n\n# Warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option(\"display.max_columns\",200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting the working directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data (Churn data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/churn-modelling/Churn_Modelling.csv\")\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{},"cell_type":"markdown","source":"### Structure of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variables structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dimensionality(data):\n    print(\"The dataset has\", data.shape[0], \" observations, and \", data.shape[1], \"columns\")\n\ndimensionality(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variable_types(data):\n    print(data.dtypes)\n    print(\"\\nThere are\", sum(data.dtypes==\"object\"), \"qualitative variables and\", sum(data.dtypes==\"int64\") + sum(data.dtypes=='float64'), \"quantitative variables\")\n    \nvariable_types(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the numerical variables, distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_by_dtype(data, data_type):\n    \"\"\"filter a dataframe by columns with a certain data_type\"\"\"\n    col_names = data.dtypes[data.dtypes == data_type].index\n    return data[col_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Filtering the numerical variables\ndata_numerical = pd.concat([filter_by_dtype(data, 'int64'), filter_by_dtype(data, 'float64')],axis=1)\ndata_numerical.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Plot Distibutions ###\ngraph_1 = plt.figure(figsize = (15,20))\nax = graph_1.gca()\ndata_numerical.hist(ax = ax, bins = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the numerical variables distribution we can see that the first 3: 'Age', 'CreditScore', and 'Balance' present a semi-normal distribution, whereas the other variables like 'RowNumber' and  'Ternure' present a similar distribution among theirselves"},{"metadata":{},"cell_type":"markdown","source":"From the graph we can also observe, that we have to drop the RowNumber column and the Id, that is a variable that is repeated with the index of the row"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(data.Surname.value_counts().sort_index().head())\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.Surname.unique())\ndata.Surname.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the table, we can see that Surname is going to be very difficult to encode as it presents 2932 unique surnames distributed among the 10,000 observations. Additionally, there is repeated surnames"},{"metadata":{},"cell_type":"markdown","source":"A priori we can observe that some variables are not useful for our analysis, these ones are: CustomerId as thet are just an id identification with similar values, Surname, as encoding will be a nightmare, and because we have people with same Lastnames, that will be hard to interpret later on in the model, and RowNumber, as this information we already have it as the row indeces "},{"metadata":{},"cell_type":"markdown","source":"### Dropping unuseful variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the categorical variables, correlation matrix"},{"metadata":{},"cell_type":"markdown","source":"For this, we need to encode our categorical variables, a priori we know that these variables were: Surname, Geography and Gender. Additionally the dataset presents variables that are already coded in binary form as: HasCrCard, IsActiveMember and Exited, teh last one of particular interest for us as it will be regarded as our target variable.\nLet's verify the 3 remaining 'object' type variables: Surname, Geography and Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.Geography.value_counts().sort_index())\nprint(\"\\n\")\nprint(data.Gender.value_counts().sort_index())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data visualization of our class variables (7)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 9))\nplt.subplot(2,2,1)\nsns.countplot((data.NumOfProducts), palette='colorblind')\n\nplt.subplot(2,2,2)\nsns.countplot(data.IsActiveMember)\n\nplt.subplot(2,2,3)\nsns.countplot(data.Exited)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see some of our 'numerical' variables are actually class variables, so we will need to encode them after the train/test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 9))\n\nplt.subplot(2,2,1)\nsns.countplot((data.Geography), palette='colorblind')\n\nplt.subplot(2,2,2)\nsns.countplot(data.Gender)\n\nplt.subplot(2,2,3)\nsns.countplot(data.Tenure)\n\nplt.subplot(2,2,4)\nsns.countplot(data.HasCrCard)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph, we can see that the Geography variable displays only 3 countries: France, Germany and Spain.\n\nFinally, the variable Gender, as expected presents 2 levels, Female and Male that we will need to encode into 0 and 1's\n"},{"metadata":{},"cell_type":"markdown","source":"### Correlation of the variables, heatmap\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def variabletype(data):\n    colname=data.columns\n    coltype=data.dtypes\n    variabletype=[]\n    for i in data:\n        if (data[i].nunique()>11) and (data[i].dtype=='int64' or data[i].dtype=='float64'):\n            variabletype.append('Continuous')\n        else:\n            variabletype.append('Class')\n    #variabletype\n    dict={'ColumnName':colname,\n         'Column_dtype':coltype,\n          'Variable_Type':variabletype}\n    return pd.DataFrame(dict)\ndf1=variabletype(data)\ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_matrix(data):\n    sns.set(style=\"dark\", palette='colorblind')\n    corr = data.corr('spearman')\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    f, ax = plt.subplots(figsize=(11, 9))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    return sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n                square=True, annot = corr.round(2), linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Spearman correlation was selected, because eventhough all our variables are manually encoded, their data_type is still object.\nIn this scenario we can say that:\n\nCorrelations are very low among our variables, being the higuest 0.32 between age and the fact if the client left the bank. Actually, this is our fist insight, as one first educated guess will be that younger the clients have a higuer the probability to change among banks as their mentality is more flexible.\nThe second 'high' positive correlation we have is 0.11 between the balance and the explanatory variable.\nFinally, on the negative correlations we have -0.32 between balance and number of products, which translate into less number of products higuer balance of our client."},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will be performing data transformations"},{"metadata":{},"cell_type":"markdown","source":"### Variable creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Salary to Balance Ratio\ndata['BalanceToSalaryRatio']=data.EstimatedSalary/data.Balance\n\n# Score to Balance Ratio\ndata['ScoreToBalance']=data.CreditScore/data.Balance\n\n# age to Salary ratio\ndata['SalaryToAge']=data.Age/data.EstimatedSalary\n\n# Products to Balance\ndata['ProductsToBalance']=data.NumOfProducts/data.Balance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Replacing the 'inf values' to zero"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.replace([np.inf, -np.inf], 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Test split"},{"metadata":{},"cell_type":"markdown","source":"Based on various articles that mention the 'data lickage', to follow is performed the train, test split before the feature generation and standarization"},{"metadata":{},"cell_type":"markdown","source":"##### Rule of thumb: Split your dataset into train and test sets before you do any processing to the data. Otherwise there might be data leakage that makes model evaluation overly optimistic. One of the common causes of data leakage is due to train-test split after data processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data[data.columns.difference(['Exited'])]\ny=data['Exited']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42\n                                                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dimensionality(X_train))\nprint(dimensionality(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The dataset has' , y_train.shape[0], 'observations')\nprint('The dataset has' , y_test.shape[0], 'observations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values treatment and imputation"},{"metadata":{},"cell_type":"markdown","source":"#### Identification of the missing values, and their proportion in the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.isnull().sum())\nprint(X_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training data set does not present any missing values, which is very surprisingly if we consider the score of this data in Kaggle"},{"metadata":{},"cell_type":"markdown","source":"### Binary encoding for the categorical variables \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"numCols=[]\nlevCols=[]\nresponse=[]\nfor i in X_train.columns:\n    if (X_train[i].dtype=='int64' or X_train[i].dtype=='float64'):\n        numCols.append(i)\n    else:\n        levCols.append(i)\n        \nprint(numCols)\nprint(levCols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in numCols:\n    if (i=='NumOfProducts' or i=='HasCrCard' or i=='IsActiveMember' or i=='Tenure' ):\n        levCols.append(i)\n\nelements_to_remove= ['NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Tenure', 'Exited']\nfor j in elements_to_remove:\n    if j in numCols:\n        numCols.remove(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numCols)\nprint(levCols)\nresponse = 'Exited'\nresponse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary Encoder"},{"metadata":{},"cell_type":"markdown","source":"We decided to use Binary encoding to avoid overpopulating the dataset of columns as One Hot Encoding does. The practical difference is that Binary encoding results in only log(base 2)ⁿ features, whereas in the One Hot Encoding we woould be creating as much columns as categories."},{"metadata":{},"cell_type":"markdown","source":"#### Training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder=ce.BinaryEncoder(cols=levCols)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data_encoded=encoder.fit_transform(X_train[levCols])\nX_train=pd.concat([X_train, data_encoded], axis=1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test dataset"},{"metadata":{},"cell_type":"markdown","source":"On test data we only 'transform' not fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_encoded2=encoder.transform(X_test[levCols])\nX_test=pd.concat([X_test, data_encoded2], axis=1)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standarization of numerical, continuous variables"},{"metadata":{},"cell_type":"markdown","source":"To run some of our algorithms, the data standarization is required, so as follows, we perform it on the \"numCols\" variables"},{"metadata":{},"cell_type":"markdown","source":"As we are going to use  decision trees, and ensemble version of it (Gradient Boosting Tree, Random Forest etc), we do not need to normalize the data because the tree based algorithms try to find intervals separating the regions. Insisting on it, would probably you would run into numeric issues.\n__However__, because we will be aplying other algorithms and we have performed the exercses before without standarization, we have decided to comply with this requirement"},{"metadata":{},"cell_type":"markdown","source":"### Manual encoding of the categorical variables"},{"metadata":{},"cell_type":"markdown","source":"As pre-step of the scaling processing we will be manually encoding the level variables"},{"metadata":{},"cell_type":"markdown","source":"#### Training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gender\nX_train['Gender'].replace(to_replace='Male', value='0', regex=True, inplace=True)\nX_train['Gender'].replace(to_replace='Female', value='1', regex=True, inplace=True)\n#Geography\nX_train['Geography'].replace(to_replace='Spain', value='0', regex=True, inplace=True)\nX_train['Geography'].replace(to_replace='Germany', value='1', regex=True, inplace=True)\nX_train['Geography'].replace(to_replace='France', value='2', regex=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Gender'] = X_train.Gender.astype(int)\nX_train['Geography'] = X_train.Geography.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gender\nX_test['Gender'].replace(to_replace='Male', value='0', regex=True, inplace=True)\nX_test['Gender'].replace(to_replace='Female', value='1', regex=True, inplace=True)\n#Geography\nX_test['Geography'].replace(to_replace='Spain', value='0', regex=True, inplace=True)\nX_test['Geography'].replace(to_replace='Germany', value='1', regex=True, inplace=True)\nX_test['Geography'].replace(to_replace='France', value='2', regex=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['Gender'] = X_test.Gender.astype(int)\nX_test['Geography'] = X_test.Geography.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Satandarization: Using the train mean on the test set by Standard Scaler"},{"metadata":{},"cell_type":"markdown","source":"##### Satandarization __on__ Numeric Variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1= X_train.copy()\nX_train2= X_train.copy()\nX_test1= X_test.copy()\nX_test2=  X_test.copy()\ncolumns_to_standarize=['Age', 'Balance', 'BalanceToSalaryRatio', 'CreditScore', 'EstimatedSalary',\n                      'ProductsToBalance', 'SalaryToAge', 'ScoreToBalance']\n#X_train2[columns_to_standarize] = X_train2[columns_to_standarize].apply(lambda x: (x-x.mean()/x.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX_train2 = sc.fit_transform(X_train2[columns_to_standarize])\nX_test2 = sc.transform(X_test2[columns_to_standarize])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2=pd.DataFrame(X_train2, columns=columns_to_standarize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test2=pd.DataFrame(X_test2, columns=columns_to_standarize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1.reset_index(drop=True, inplace=True)\nX_test1.reset_index(drop=True, inplace=True)\nX_train2.reset_index(drop=True, inplace=True)\nX_test2.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1.drop(['Age', 'Balance', 'BalanceToSalaryRatio', 'CreditScore', 'EstimatedSalary',\n                      'ProductsToBalance', 'SalaryToAge', 'ScoreToBalance'],axis=1, inplace=True)\nX_test1.drop(['Age', 'Balance', 'BalanceToSalaryRatio', 'CreditScore', 'EstimatedSalary',\n                      'ProductsToBalance', 'SalaryToAge', 'ScoreToBalance'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1=pd.concat([X_train1, X_train2], axis=1)\nmissing_data(X_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test1=pd.concat([X_test1, X_test2], axis=1)\nmissing_data(X_test1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Final train and test datasets standarized"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_standarized=X_train1.copy()\nX_test_standarized=X_test1.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Oversampling the standarized dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state= 42)\nX_train_standarized_upsampled, y_train_standarized_upsampled = ros.fit_sample(X_train_standarized, y_train)\ny_vals_, counts_ = np.unique(y_train, return_counts=True)\ny_vals_ros_, counts_ros_ = np.unique(y_train_standarized_upsampled, return_counts=True)\nprint(' Classes in the train set originally were:',dict(zip(y_vals_, counts_)),'\\n',\n      'Classes in the rebalanced train set are now:',dict(zip(y_vals_ros_, counts_ros_)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test_standarized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection by Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"features=X_train.columns\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 700, num = 10)]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(start=10, stop=100, num = 10)]\n# Creating the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth}\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n# First creating the base model to tune\nrandom_forest = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrandom_forest_random = RandomizedSearchCV(estimator = random_forest, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrandom_forest_random.fit(X_train[features], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluation of the random search"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_random_tunned = RandomForestClassifier(max_depth=40, n_estimators=700, random_state=42)\nrandom_forest_random_tunned.fit(X_train, y_train)\nplot_feature_importances(random_forest_random_tunned, feature_names=features, figsize=(40, 20));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the Random Forest classifier we would be tempted to include the features: we will include the 20 of them and compare them with the xgboost classifier\n    - age (definitely)\n    - SalaryToAge\n    - EstimatedSalary \n    - CreditScore\n    - Balance\n    - ScoreToBalance\n    - BalanceToSalaryRatio\n    - ProductsToBalance\n    - Num of Products\n    - Tenure\n    - NumOfProducts_2     \n    - NumOfProducts_1\n    - NumOfProducts_0\n    - Geography_2\n    - Geography\n    - Tenure_4\n    - IsActiveMember_1\n    - IsActiveMember\n    - IsActiveMember_0\n    - Tenure_2"},{"metadata":{},"cell_type":"markdown","source":"From the previous, we are pleased to see that the ratios generated are in the top features importances and we will be keeping them. However, there is some overlapping among the original variables and the encoded variables, so we will be dropping the original variables and taking just the encoded."},{"metadata":{},"cell_type":"markdown","source":"#### Train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Xgboost feature importance by permutation"},{"metadata":{},"cell_type":"markdown","source":"#### Hyperameter tuning"},{"metadata":{},"cell_type":"markdown","source":"In K-Fold CV, we further will split our training set into K number of subsets, aka folds.\n\nIn K = 5, the first iteration we train on the first four folds and evaluate on the fifth. The second time we train on the first, second, third, and fifth fold and evaluate on the fourth and so on.."},{"metadata":{},"cell_type":"markdown","source":"For hyperparameter tuning, we perform many iterations of the entire K-Fold CV process, each time using different model settings. We then compare all of the models, select the best one, train it on the full training set, and then evaluate on the testing set. "},{"metadata":{},"cell_type":"markdown","source":"Parameters to test in the hypertuning: According to literature, the most important settings are the number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features) and max_depth = max number of levels in each decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To use RandomizedSearchCV, we first need to create a parameter grid to sample from during the fitting:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 700, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(start=10, stop=100, num = 10)]\n# Creating the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth}\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n# First creating the base model to tune\nxg_boost = xgb.XGBClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nxg_boost_random = RandomizedSearchCV(estimator = xg_boost, param_distributions = random_grid, n_iter = 20, \n                                     cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nxg_boost_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_boost_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluation of the Random search"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_boost_random_tunned = xgb.XGBClassifier(n_estimators=50, max_depth=20, random_state=42)\nxg_boost_random_tunned.fit(X_train, y_train)\nexplain_weights_xgboost(xg_boost_random_tunned)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the Xgboost classifier we would be tempted to include the features:\n    \n    - NumOfProducts_2\n    - NumOfProducts\n    - IsActiveMember\n    - Age\n    - Geography_2\n    - ProductsToBalance\n    - ScoreToBalance\n    - Balance\n    - BalanceToSalaryRatio\n    - SalaryToAge\n    - EstimatedSalary\n    - Gender\n    - Geography\n    - HasCrCard\n    - CreditScore\n    - Tenure_2\n    - Tenure_3\n    - Tenure_4\n    - Tenure\n    - Tenure_1"},{"metadata":{},"cell_type":"markdown","source":"### Selecting the variables and discarding those overlapped by the Binary encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"overlapping_variables=['Gender', 'Geography', 'HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Tenure']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Age', 'Balance', 'BalanceToSalaryRatio', 'CreditScore',\n       'EstimatedSalary', 'ProductsToBalance', 'SalaryToAge', 'ScoreToBalance',\n        'Gender_0', 'Gender_1', 'Geography_0', 'Geography_1',\n       'Geography_2', 'HasCrCard_0', 'HasCrCard_1', 'IsActiveMember_0',\n       'IsActiveMember_1', 'NumOfProducts_0', 'NumOfProducts_1',\n       'NumOfProducts_2', 'Tenure_0', 'Tenure_1', 'Tenure_2', 'Tenure_3',\n       'Tenure_4']\nlen(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Verifying on the Random Forest Classifier, feature importance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_selected_features = RandomForestClassifier(max_depth=40, n_estimators=122, random_state=42)\nrandom_forest_selected_features.fit(X_train[features], y_train)\nplot_feature_importances(random_forest_selected_features, feature_names=features, figsize=(40, 20));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_boost_random_selected_features = xgb.XGBClassifier(n_estimators=50, max_depth=20, random_state=42)\nxg_boost_random_selected_features.fit(X_train[features], y_train)\nexplain_weights_xgboost(xg_boost_random_selected_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Base model"},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression, no upsampling"},{"metadata":{},"cell_type":"markdown","source":"#### Creating a table for model comparisons"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names =  ['Model', 'Precision', 'Recall', 'F1-score', 'Accuracy']\nmodel_comparison = pd.DataFrame(columns = col_names)\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# logistic regression object \nlr = LogisticRegression() \n# Training the model on train set \nlr.fit(X_train[features], y_train) \n# Predicting on test set\npredictions_logistic_regression = lr.predict(X_test[features]) \n\nprint(classification_report(y_test, predictions_logistic_regression)) \n\n#Extracting the results\nlogistic_regression_report = classification_report(y_test, predictions_logistic_regression, output_dict=True )\nprecision_logistic_regression =  logistic_regression_report['macro avg']['precision'] \nrecall_logistic_regression = logistic_regression_report['macro avg']['recall']    \nf1_score_logistic_regression = logistic_regression_report['macro avg']['f1-score']\naccuracy_logistic_regression = logistic_regression_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix of Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_logistic_regression = confusion_matrix(y_test, predictions_logistic_regression)\n\nsns.heatmap(cm_logistic_regression, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{},"cell_type":"markdown","source":"To do so, local validation will be selected. The argument behind this, is  that if we use always the test set for validation, it can lead to overfitting. \n\nSo its better to do cross validate on more segments of unseen data'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression_cv(X_train, y_train, features, k):\n    train_roc_auc, test_roc_auc, iteration = [], [], []\n    i = 1\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    for train, test in kf.split(X_train.index.values):\n        # Logistic regression model    \n        lr = LogisticRegression()        \n        lr.fit(X_train.iloc[train][features],y_train.iloc[train]) \n        #Predictions on train and test set from cross val\n        preds_train=lr.predict(X_train.iloc[train][features])\n        preds_test = lr.predict(X_train.iloc[test][features])\n        train_roc_auc.append(roc_auc_score(y_train.iloc[train], preds_train))\n        test_roc_auc.append(roc_auc_score(y_train.iloc[test], preds_test))\n        iteration.append(i)\n        i+=1  \n    columns = {'Iteration': iteration, 'Train ROC AUC': train_roc_auc, 'Test ROC AUC': test_roc_auc}\n    results = pd.DataFrame.from_dict(columns)\n    results2 = results.drop(['Iteration'], axis=1)\n    results2.boxplot()\n    results.loc[len(results)] = [\"Mean\", np.mean(train_roc_auc), np.mean(test_roc_auc)]\n    display(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression_cv(X_train, y_train, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression_result= ['logistic regression', precision_logistic_regression, recall_logistic_regression, f1_score_logistic_regression,\n                                 accuracy_logistic_regression]\nmodel_comparison.loc[len(model_comparison)] = logistic_regression_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Base model: Logistic regression,  upsampling"},{"metadata":{},"cell_type":"markdown","source":"### Checking for imbalanced data"},{"metadata":{},"cell_type":"markdown","source":"Important Note. Always split into test and train sets BEFORE trying oversampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets"},{"metadata":{},"cell_type":"markdown","source":"In order to prevent the future algorithms to be biassed towards our categorical variable: Exited. To follow is presented the analysis to see if efectively, we are dealing with an imbalanced dataset"},{"metadata":{},"cell_type":"markdown","source":"### Upsampling"},{"metadata":{},"cell_type":"markdown","source":"For this, we will be using Random Oversampling, that according to documentation is a replacement sampling that can be used to  get a balanced representation of each class in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y_train)\ny_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the histogram, we may see that the data accounts for an imbalance, as the Exited variable, yes represents only the 2% of the dataset"},{"metadata":{},"cell_type":"markdown","source":"Being the actual proportion in the train set 3.901 times to 1. Reason by which we need to resample our target variable"},{"metadata":{},"cell_type":"markdown","source":"In this case we will take integrally the Exited ('No') observations and we will be oversampling the ('Yes') observations in the datset, because according to literature undersampling is more suitable for a million of observations dataset, whereas in this case, we are dealing with thousands of observations, the literature suggest oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing by 0's and 1's\ny_train_0 = y_train[y_train == 0]\ny_train_1 = y_train[y_train== 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=42)\nX_train_res, y_train_res = ros.fit_sample(X_train, y_train)\ny_vals, counts = np.unique(y_train, return_counts=True)\ny_vals_ros, counts_ros = np.unique(y_train_res, return_counts=True)\nprint(' Classes in the train set originally were:',dict(zip(y_vals, counts)),'\\n',\n      'Classes in the rebalanced train set now are:',dict(zip(y_vals_ros, counts_ros)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression object with over sampling\nlr2 = LogisticRegression() \n# Training the model on train set \nlr2.fit(X_train_res[features], y_train_res) \n# Predicting on test set\npredictions_logistic_regression_up = lr2.predict(X_test[features]) \n\n# print classification report \nprint(classification_report(y_test, predictions_logistic_regression_up)) \n\n#Extracting the results\nlogistic_regression_report_up = classification_report(y_test, predictions_logistic_regression_up, output_dict=True )\nprecision_logistic_regression_up =  logistic_regression_report_up['macro avg']['precision'] \nrecall_logistic_regression_up = logistic_regression_report_up['macro avg']['recall']    \nf1_score_logistic_regression_up = logistic_regression_report_up['macro avg']['f1-score']\naccuracy_logistic_regression_up = logistic_regression_report_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model’s ability to classify all destinations (accuracy) decreased with oversampling. On the other hand, the model’s ability to classify the minority class (recall) improved "},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix of Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_logistic_regression_up = confusion_matrix(y_test, predictions_logistic_regression_up)\n\nsns.heatmap(cm_logistic_regression_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression_cv(X_train_res, y_train_res, features, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression_result_up= ['logistic regression upsampled', precision_logistic_regression_up, \n                             recall_logistic_regression_up, f1_score_logistic_regression_up,\n                            accuracy_logistic_regression_up]\nmodel_comparison.loc[len(model_comparison)] = logistic_regression_result_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression with standarization and upsampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_standarized_upsampled\n#y_train_standarized_upsampled\n#X_test_standarized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression object with over sampling\nlr2 = LogisticRegression() \n# Training the model on train set \nlr2.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled) \n# Predicting on test set\npredictions_logistic_regression_std_up = lr2.predict(X_test_standarized[features]) \n\n# print classification report \nprint(classification_report(y_test, predictions_logistic_regression_std_up)) \n\n#Extracting the results\nlogistic_regression_report_std_up = classification_report(y_test, predictions_logistic_regression_std_up, output_dict=True )\nprecision_logistic_regression_std_up =  logistic_regression_report_std_up['macro avg']['precision'] \nrecall_logistic_regression_std_up = logistic_regression_report_std_up['macro avg']['recall']    \nf1_score_logistic_regression_std_up = logistic_regression_report_std_up['macro avg']['f1-score']\naccuracy_logistic_regression_std_up = logistic_regression_report_std_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Logistic regression standarized and upsampled"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_logistic_regression_std_up = confusion_matrix(y_test, predictions_logistic_regression_std_up)\n\nsns.heatmap(cm_logistic_regression_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression_cv(X_train_standarized_upsampled, y_train_standarized_upsampled, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression_result_std_up= ['logistic regression standarized upsampled', precision_logistic_regression_std_up, \n                                    recall_logistic_regression_std_up, f1_score_logistic_regression_std_up,\n                                 accuracy_logistic_regression_std_up]\nmodel_comparison.loc[len(model_comparison)] = logistic_regression_result_std_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Trees"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{},"cell_type":"markdown","source":"4 parameters:\n\n- max_features\n- max_depth\n- min_samples_split\n- min_samples_leaf\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the parameter grid\n# Number of features to consider at every split\nmax_features = list(range(1,X_train.shape[1]))\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(start= 10, stop= 100, num = 10)]\n#min_samples_split\nmin_samples_split=[int(x) for x in np.linspace(start = 0.1, stop = 10, num = 10)]     \n#min_samples_leaf                       \nmin_samples_leaf=[int(x) for x in np.linspace(start = 0.1, stop = 10, num = 10)]                      \n                           \n# Creating the random grid\nrandom_grid = {\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf\n              }\n#pprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\ndecision_tree_classifier = DecisionTreeClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\ndecision_tree_classifier_random = RandomizedSearchCV(estimator = decision_tree_classifier, \n                                                     param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, \n                                                     random_state=42, n_jobs = -1)\n# Fit the random search model\ndecision_tree_classifier_random.fit(X_train[features], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_classifier_random.best_params_\nbest_parameters = decision_tree_classifier_random.best_params_\npd.DataFrame(best_parameters.values(),best_parameters.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The model: Decision tree classifier tunned"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Defining the model:\ndecision_tree_classifier_tunned = DecisionTreeClassifier(min_samples_split= 10,\n                                                         min_samples_leaf= 10,\n                                                         max_features= 19,\n                                                         max_depth= 10, \n                                                         random_state=42)\n# Training the model:\ndecision_tree_classifier_tunned.fit(X_train[features], y_train)\n\n# Predicting on test set:\ntree_predictions = decision_tree_classifier_tunned.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, tree_predictions)) \n\n#Extracting the results\ndecision_tree_report = classification_report(y_test, tree_predictions, output_dict=True )\nprecision_decision_tree =  decision_tree_report['macro avg']['precision'] \nrecall_decision_tree = decision_tree_report['macro avg']['recall']    \nf1_score_decision_tree = decision_tree_report['macro avg']['f1-score']\naccuracy_decision_tree = decision_tree_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Decision Tree\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_decision_tree = confusion_matrix(y_test, tree_predictions)\n\nsns.heatmap(cm_decision_tree, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decision_tree_classifier_cv(X_train, y_train, features, k):\n    train_roc_auc, test_roc_auc, iteration = [], [], []\n    i = 1\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    for train, test in kf.split(X_train.index.values):\n        # Model    \n        decision_tree_classifier = DecisionTreeClassifier(min_samples_split= 10,\n                                                         min_samples_leaf= 10,\n                                                         max_features= 19,\n                                                         max_depth= 10, \n                                                         random_state=42)       \n        decision_tree_classifier.fit(X_train.iloc[train][features],y_train.iloc[train]) \n        #Predictions on train and test set from cross val\n        preds_train=decision_tree_classifier.predict(X_train.iloc[train][features])\n        preds_test = decision_tree_classifier.predict(X_train.iloc[test][features])\n        train_roc_auc.append(roc_auc_score(y_train.iloc[train], preds_train))\n        test_roc_auc.append(roc_auc_score(y_train.iloc[test], preds_test))\n        iteration.append(i)\n        i+=1  \n    columns = {'Iteration': iteration, 'Train ROC AUC': train_roc_auc, 'Test ROC AUC': test_roc_auc}\n    results = pd.DataFrame.from_dict(columns)\n    results2 = results.drop(['Iteration'], axis=1)\n    results2.boxplot()\n    results.loc[len(results)] = [\"Mean\", np.mean(train_roc_auc), np.mean(test_roc_auc)]\n    display(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_classifier_cv(X_train, y_train, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_result= ['decision tree', precision_decision_tree, \n                                    recall_decision_tree, f1_score_decision_tree,\n                                 accuracy_decision_tree]\nmodel_comparison.loc[len(model_comparison)] = decision_tree_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Trees with upsampling"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First create the base model to tune\ndecision_tree_classifier_up = DecisionTreeClassifier()\n# Using the random grid to search for best hyperparameters\ndecision_tree_classifier_random_up = RandomizedSearchCV(estimator = decision_tree_classifier_up, \n                                    param_distributions = random_grid, n_iter = 100, cv = 3, \n                                    verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\ndecision_tree_classifier_random_up.fit(X_train_res[features], y_train_res)\n\ndecision_tree_best_parameters_up = decision_tree_classifier_random_up.best_params_\npd.DataFrame(decision_tree_best_parameters_up.values(),decision_tree_best_parameters_up.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model:\ndecision_tree_classifier_up = DecisionTreeClassifier(min_samples_split= 6,\n                                                         min_samples_leaf= 1,\n                                                         max_features= 21,\n                                                         max_depth= 40, \n                                                         random_state=42)\n# Training the model:\ndecision_tree_classifier_up.fit(X_train_res[features], y_train_res)\n\n# Predicting on test set:\ntree_predictions_up = decision_tree_classifier_up.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, tree_predictions_up)) \n\n#Extracting the results\ndecision_tree_report_up = classification_report(y_test, tree_predictions_up, output_dict=True )\nprecision_decision_tree_up =  decision_tree_report_up['macro avg']['precision'] \nrecall_decision_tree_up = decision_tree_report_up['macro avg']['recall']    \nf1_score_decision_tree_up = decision_tree_report_up['macro avg']['f1-score']\naccuracy_decision_tree_up = decision_tree_report_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Decision Tree upsampled\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_decision_tree_up = confusion_matrix(y_test, tree_predictions_up)\n\nsns.heatmap(cm_decision_tree_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_classifier_cv(X_train_res, y_train_res, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_result_up= ['decision tree upsampled', precision_decision_tree_up, \n                                    recall_decision_tree_up, f1_score_decision_tree_up,\n                                    accuracy_decision_tree_up]\n\nmodel_comparison.loc[len(model_comparison)] = decision_tree_result_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree with standarization and upsampling"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First create the base model to tune\ndecision_tree_classifier_std_up = DecisionTreeClassifier()\n\n# Using the random grid to search for best hyperparameters\ndecision_tree_classifier_random_std_up = RandomizedSearchCV(estimator = decision_tree_classifier_std_up, \n                                    param_distributions = random_grid, n_iter = 100, cv = 3, \n                                    verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\ndecision_tree_classifier_random_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\ndecision_tree_best_parameters_std_up = decision_tree_classifier_random_std_up.best_params_\npd.DataFrame(decision_tree_best_parameters_std_up.values(),decision_tree_best_parameters_std_up.keys(),\n             columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model\ndecision_tree_classifier_std_up = DecisionTreeClassifier(min_samples_split= 6,\n                                                         min_samples_leaf= 1,\n                                                         max_features= 21,\n                                                         max_depth= 40,\n                                                         random_state=42)\n# Training the model with oversampling\ndecision_tree_classifier_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\n# Predicting on test set:\ntree_pred_std_up = decision_tree_classifier_std_up.predict(X_test_standarized[features])\n\n# Printing the classification report \nprint(classification_report(y_test, tree_pred_std_up)) \n\n#Extracting the results\ndecision_tree_report_std_up = classification_report(y_test, tree_pred_std_up, output_dict=True )\nprecision_decision_tree_std_up =  decision_tree_report_std_up['macro avg']['precision'] \nrecall_decision_tree_std_up = decision_tree_report_std_up['macro avg']['recall']    \nf1_score_decision_tree_std_up = decision_tree_report_std_up['macro avg']['f1-score']\naccuracy_decision_tree_std_up = decision_tree_report_std_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Decision Tree standarized and upsampled"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_decision_tree_std_up = confusion_matrix(y_test, tree_pred_std_up)\n\nsns.heatmap(cm_decision_tree_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_classifier_cv(X_train_standarized_upsampled, y_train_standarized_upsampled, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_result_std_up= ['decision tree standarized upsampled', precision_decision_tree_std_up, \n                                    recall_decision_tree_std_up, f1_score_decision_tree_std_up,\n                                    accuracy_decision_tree_std_up]\n\nmodel_comparison.loc[len(model_comparison)] = decision_tree_result_std_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest classifier"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the parameter grid\n\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 700, num = 10)]\n# Number of features to consider at every split\nmax_features = list(range(1,X_train[features].shape[1]))\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(start= 10, stop= 100, num = 10)]\n#min_samples_split\nmin_samples_split=[int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]     \n#min_samples_leaf                       \nmin_samples_leaf=[int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]                      \n                           \n# Creating the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf\n              }\n#pprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using  random grid to search for best hyperparameters\n# First create the base model to tune\nrandom_forest_classifier = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrandom_forest_classifier_random = RandomizedSearchCV(estimator = random_forest_classifier, \n                                                     param_distributions = random_grid, n_iter = 100, \n                                                     cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrandom_forest_classifier_random.fit(X_train[features], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier_random.best_params_\nbest_parameters = random_forest_classifier_random.best_params_\npd.DataFrame(best_parameters.values(),best_parameters.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The model: Random forest classifier tunned"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Defining the model:\nrandom_forest_classifier_tunned = RandomForestClassifier(n_estimators=627, min_samples_split=8, min_samples_leaf=7,\n                                                         max_features= 20, max_depth= 70,  random_state=42)\n# Training the model:\nrandom_forest_classifier_tunned.fit(X_train[features], y_train)\n\n# Predicting on test set:\nrandom_tree_pred= random_forest_classifier_tunned.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, random_tree_pred)) \n\n#Extracting the results\nrandom_tree_report = classification_report(y_test, random_tree_pred, output_dict=True )\nprecision_random_tree =  random_tree_report['macro avg']['precision'] \nrecall_random_tree = random_tree_report['macro avg']['recall']    \nf1_score_random_tree = random_tree_report['macro avg']['f1-score']\naccuracy_random_tree = random_tree_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_random_tree = confusion_matrix(y_test, random_tree_pred)\n\nsns.heatmap(cm_random_tree, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_forest_classifier_cv(X_train, y_train, features, k):\n    train_roc_auc, test_roc_auc, iteration = [], [], []\n    i = 1\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    for train, test in kf.split(X_train.index.values):\n        # Model    \n        random_forest_classifier = RandomForestClassifier(n_estimators=627, min_samples_split=8, min_samples_leaf=7,\n                                                         max_features= 20, max_depth= 70,  random_state=42)\n        random_forest_classifier.fit(X_train.iloc[train][features],y_train.iloc[train]) \n        #Predictions on train and test set from cross val\n        preds_train=random_forest_classifier.predict(X_train.iloc[train][features])\n        preds_test = random_forest_classifier.predict(X_train.iloc[test][features])\n        train_roc_auc.append(roc_auc_score(y_train.iloc[train], preds_train))\n        test_roc_auc.append(roc_auc_score(y_train.iloc[test], preds_test))\n        iteration.append(i)\n        i+=1  \n    columns = {'Iteration': iteration, 'Train ROC AUC': train_roc_auc, 'Test ROC AUC': test_roc_auc}\n    results = pd.DataFrame.from_dict(columns)\n    results2 = results.drop(['Iteration'], axis=1)\n    results2.boxplot()\n    results.loc[len(results)] = [\"Mean\", np.mean(train_roc_auc), np.mean(test_roc_auc)]\n    display(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier_cv(X_train, y_train, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_tree_result = ['random forest', precision_random_tree, \n                              recall_random_tree, f1_score_random_tree,\n                              accuracy_random_tree]\n\nmodel_comparison.loc[len(model_comparison)] = random_tree_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest classifier with upsampling"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n# First create the base model to tune\nrandom_forest_classifier_up = RandomForestClassifier()\n\nrandom_forest_classifier_random_up = RandomizedSearchCV(estimator = random_forest_classifier_up, \n                                                     param_distributions = random_grid, \n                                                     n_iter = 100, cv = 3, verbose=2, random_state=42, \n                                                     n_jobs = -1)\n\n# Fit the random search model\nrandom_forest_classifier_random_up.fit(X_train_res[features], y_train_res)\n\nrandom_forest_best_parameters_up = random_forest_classifier_random_up.best_params_\npd.DataFrame(random_forest_best_parameters_up.values(),random_forest_best_parameters_up.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model:\nrandom_forest_classifier_oversampled = RandomForestClassifier(n_estimators=555, min_samples_split=3, min_samples_leaf=1,\n                                       max_features= 3, max_depth= 90,  random_state=42)\n\n# Training the model with oversampling:\nrandom_forest_classifier_oversampled.fit(X_train_res[features], y_train_res)\n\n# Predicting on test set:\nrandom_tree_pred_up =random_forest_classifier_oversampled.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, random_tree_pred_up)) \n\n#Extracting the results\nrandom_tree_report_up = classification_report(y_test, random_tree_pred_up, output_dict=True )\nprecision_random_tree_up= random_tree_report_up['macro avg']['precision'] \nrecall_random_tree_up   = random_tree_report_up['macro avg']['recall']    \nf1_score_random_tree_up = random_tree_report_up['macro avg']['f1-score']\naccuracy_random_tree_up = random_tree_report_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Random Forest upsampled"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_random_tree_up = confusion_matrix(y_test, random_tree_pred_up)\n\nsns.heatmap(cm_random_tree_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier_cv(X_train_res, y_train_res, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_tree_result_up= ['random forest upsampled', precision_random_tree_up, \n                              recall_random_tree_up, f1_score_random_tree_up,\n                              accuracy_random_tree_up]\n\nmodel_comparison.loc[len(model_comparison)] = random_tree_result_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest classifier with standarization and oversampling"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n# First create the base model to tune\nrandom_forest_classifier_std_up = RandomForestClassifier()\n\nrandom_forest_classifier_random_std_up = RandomizedSearchCV(estimator = random_forest_classifier_std_up, \n                                                     param_distributions = random_grid, \n                                                     n_iter = 100, cv = 3, verbose=2, random_state=42, \n                                                     n_jobs = -1)\n\n# Fit the random search model\nrandom_forest_classifier_random_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\nrandom_forest_best_parameters_std_up = random_forest_classifier_random_std_up.best_params_\npd.DataFrame(random_forest_best_parameters_std_up.values(),random_forest_best_parameters_std_up.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model:\nrandom_forest_classifier_std_up = RandomForestClassifier(n_estimators=555, min_samples_split=3, min_samples_leaf=1,\n                                       max_features= 3, max_depth= 90,  random_state=42)\n\n# Training the model with oversampling and standarization:\nrandom_forest_classifier_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\n# Predicting on test set:\nrandom_tree_pred_std_up=random_forest_classifier_std_up.predict(X_test_standarized[features])\n\n# Printing the classification report \nprint(classification_report(y_test, random_tree_pred_std_up)) \n\n#Extracting the results\nrandom_tree_report_std_up = classification_report(y_test, random_tree_pred_std_up, output_dict=True )\nprecision_random_tree_std_up= random_tree_report_std_up['macro avg']['precision'] \nrecall_random_tree_std_up   = random_tree_report_std_up['macro avg']['recall']    \nf1_score_random_tree_std_up = random_tree_report_std_up['macro avg']['f1-score']\naccuracy_random_tree_std_up= random_tree_report_std_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Random Tree standarized and upsampled"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_random_tree_std_up = confusion_matrix(y_test, random_tree_pred_std_up)\n\nsns.heatmap(cm_random_tree_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier_cv(X_train_standarized_upsampled, y_train_standarized_upsampled, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_tree_result_std_up= ['random forest standarized upsampled', precision_random_tree_std_up, \n                        recall_random_tree_std_up, f1_score_random_tree_std_up,\n                        accuracy_random_tree_std_up]\n\nmodel_comparison.loc[len(model_comparison)] = random_tree_result_std_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Suport Vector Machines- SVM"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel = ['linear', 'rbf', 'poly']\n\n# Creating the random grid\nrandom_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}  \n\n#pprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n# Using  random grid to search for best hyperparameters\n# First create the base model to tune\nsvm_classifier = SVC(random_state=42)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrandom_svm = RandomizedSearchCV(estimator = svm_classifier, param_distributions = random_grid, n_iter = 100, \n                                cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrandom_svm.fit(X_train[features], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_svm.best_params_\nbest_parameters = random_svm.best_params_\npd.DataFrame(best_parameters.values(),best_parameters.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The model: SVC classifier tunned"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Defining the model: SVM\nsvm_tunned = SVC(gamma=1, C=0.1 ,random_state=42)\n\n# Training the model:\nsvm_tunned.fit(X_train[features], y_train)\n\n# Predicting on test set:\nsvm_pred=svm_tunned.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, svm_pred)) \n\n#Extracting the results\nsvm_report = classification_report(y_test, svm_pred, output_dict=True )\nprecision_svm= svm_report['macro avg']['precision'] \nrecall_svm   = svm_report['macro avg']['recall']    \nf1_score_svm = svm_report['macro avg']['f1-score']\naccuracy_svm= svm_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_svm = confusion_matrix(y_test, svm_pred)\n\nsns.heatmap(cm_svm, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def svm_cv(X_train, y_train, features, k):\n    train_roc_auc, test_roc_auc, iteration = [], [], []\n    i = 1\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    for train, test in kf.split(X_train.index.values):\n        # Model    \n        svm_classifier = SVC(gamma=1, C=0.1 ,random_state=45)\n        svm_classifier.fit(X_train.iloc[train][features], y_train.iloc[train]) \n        #Predictions on train and test set from cross val\n        preds_train= svm_classifier.predict(X_train.iloc[train][features])\n        preds_test = svm_classifier.predict(X_train.iloc[test][features])\n        train_roc_auc.append(roc_auc_score(y_train.iloc[train], preds_train))\n        test_roc_auc.append(roc_auc_score(y_train.iloc[test], preds_test))\n        iteration.append(i)\n        i+=1  \n    columns = {'Iteration': iteration, 'Train ROC AUC': train_roc_auc, 'Test ROC AUC': test_roc_auc}\n    results = pd.DataFrame.from_dict(columns)\n    results2 = results.drop(['Iteration'], axis=1)\n    results2.boxplot()\n    results.loc[len(results)] = [\"Mean\", np.mean(train_roc_auc), np.mean(test_roc_auc)]\n    display(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_cv(X_train, y_train, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_result= ['svm', precision_svm, recall_svm, f1_score_svm, accuracy_svm]\n\nmodel_comparison.loc[len(model_comparison)] = svm_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM with upsampling"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n\n# First create the base model to tune\nsvm_classifier_up = SVC(random_state=42)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrandom_svm_up = RandomizedSearchCV(estimator = svm_classifier_up, param_distributions = random_grid, n_iter = 100, \n                                cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrandom_svm_up.fit(X_train_res[features], y_train_res)\n\nrandom_svm_best_parameters_up = random_svm_up.best_params_\npd.DataFrame(random_svm_best_parameters_up.values(),random_svm_best_parameters_up.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model:\nsvm_up = SVC(gamma=1, C=1 ,random_state=42)\n\n# Training the model with upsampling:\nsvm_up.fit(X_train_res[features], y_train_res)\n\n# Predicting on test set:\nsvm_pred_up =svm_up.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, svm_pred_up)) \n\n#Extracting the results\nsvm_report_up = classification_report(y_test, svm_pred_up, output_dict=True )\nprecision_svm_up = svm_report_up['macro avg']['precision'] \nrecall_svm_up   = svm_report_up['macro avg']['recall']    \nf1_score_svm_up = svm_report_up['macro avg']['f1-score']\naccuracy_svm_up= svm_report_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for SVM upsampled"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_svm_up = confusion_matrix(y_test, svm_pred_up)\n\nsns.heatmap(cm_svm_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_cv(X_train_res, y_train_res, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_result_up= ['svm upsampled', precision_svm_up, recall_svm_up, f1_score_svm_up, accuracy_svm_up]\n\nmodel_comparison.loc[len(model_comparison)] = svm_result_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM with standarized and upsampled"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n\n# First create the base model to tune\nsvm_classifier_std_up = SVC(random_state=42)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrandom_svm_std_up = RandomizedSearchCV(estimator = svm_classifier_std_up, param_distributions = random_grid, n_iter = 100, \n                                cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrandom_svm_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\nrandom_svm_best_parameters_std_up = random_svm_std_up.best_params_\npd.DataFrame(random_svm_best_parameters_std_up.values(),\n             random_svm_best_parameters_std_up.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model:\nsvm_classifier_std_up = SVC(gamma=1, C=10 ,random_state=42)\n\n# Training the model with oversampling:\nsvm_classifier_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\n# Predicting:\nsvm_pred_std_up=svm_classifier_std_up.predict(X_test_standarized[features])\n\n# Printing the classification report \nprint(classification_report(y_test, svm_pred_std_up)) \n\n#Extracting the results\nsvm_report_std_up = classification_report(y_test, svm_pred_std_up, output_dict=True )\nprecision_svm_std_up = svm_report_std_up['macro avg']['precision'] \nrecall_svm_std_up   = svm_report_std_up['macro avg']['recall']    \nf1_score_svm_std_up = svm_report_std_up['macro avg']['f1-score']\naccuracy_svm_std_up= svm_report_std_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for SVM standarized and upsampled"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_svm_std_up = confusion_matrix(y_test, svm_pred_std_up)\n\nsns.heatmap(cm_svm_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_cv(X_train_standarized_upsampled, y_train_standarized_upsampled, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_result_std_up= ['svm standarized upsampled', precision_svm_std_up, \n                    recall_svm_std_up, f1_score_svm_std_up, accuracy_svm_std_up]\n\nmodel_comparison.loc[len(model_comparison)] = svm_result_std_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XG-boost "},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters done before on feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model: Xg-boost\nxg_boost_tunned= xgb.XGBClassifier(n_estimators=50, max_depth=20, random_state=42)\n\n#Training the model:\nxg_boost_tunned.fit(X_train[features], y_train)\n\n#Predicting:\nxg_boost_pred=xg_boost_tunned.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, xg_boost_pred)) \n\n#Extracting the results\nxg_boost_report = classification_report(y_test, xg_boost_pred, output_dict=True )\nprecision_xg_boost = xg_boost_report['macro avg']['precision'] \nrecall_xg_boost   = xg_boost_report['macro avg']['recall']    \nf1_score_xg_boost = xg_boost_report['macro avg']['f1-score']\naccuracy_xg_boost= xg_boost_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for XgBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_xg_boost = confusion_matrix(y_test, xg_boost_pred)\n\nsns.heatmap(cm_xg_boost, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_xg_boost_cv(X_train, y_train, features, k):\n    train_roc_auc, test_roc_auc, iteration = [], [], []\n    i = 1\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    for train, test in kf.split(X_train.index.values):\n        # Model    \n        xg_boost_ = xgb.XGBClassifier(n_estimators=50, max_depth=20, random_state=45)\n        xg_boost_.fit(X_train.iloc[train][features],y_train.iloc[train]) \n        #Predictions on train and test set from cross val\n        preds_train=xg_boost_.predict(X_train.iloc[train][features])\n        preds_test = xg_boost_.predict(X_train.iloc[test][features])\n        train_roc_auc.append(roc_auc_score(y_train.iloc[train], preds_train))\n        test_roc_auc.append(roc_auc_score(y_train.iloc[test], preds_test))\n        iteration.append(i)\n        i+=1  \n    columns = {'Iteration': iteration, 'Train ROC AUC': train_roc_auc, 'Test ROC AUC': test_roc_auc}\n    results = pd.DataFrame.from_dict(columns)\n    results2 = results.drop(['Iteration'], axis=1)\n    results2.boxplot()\n    results.loc[len(results)] = [\"Mean\", np.mean(train_roc_auc), np.mean(test_roc_auc)]\n    display(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_xg_boost_cv(X_train, y_train, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_boost_result= ['XgBoost', precision_xg_boost, \n                    recall_xg_boost, f1_score_xg_boost, accuracy_xg_boost]\n\nmodel_comparison.loc[len(model_comparison)] = xg_boost_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Xg-boost with upsampling"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 700, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(start=10, stop=100, num = 10)]\n# Creating the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth}\n#pprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n# First creating the base model to tune\nxg_boost_up = xgb.XGBClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nxg_boost_random_up = RandomizedSearchCV(estimator = xg_boost_up, param_distributions = random_grid, n_iter = 20, \n                                     cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nxg_boost_random_up.fit(X_train_res, y_train_res)\n\nxg_boost_up_best_parameters_up = xg_boost_random_up.best_params_\npd.DataFrame(xg_boost_up_best_parameters_up.values(),xg_boost_up_best_parameters_up.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model: Xg-boost\nxg_boost_tunned_up= xgb.XGBClassifier(n_estimators=411, max_features='sqrt' ,max_depth=10, random_state=42)\n\n#Training the model:\nxg_boost_tunned_up.fit(X_train_res[features], y_train_res)\n\n#Predicting:\nxg_boost_pred_up=xg_boost_tunned_up.predict(X_test[features])\n\n# Printing the classification report \nprint(classification_report(y_test, xg_boost_pred_up)) \n\n#Extracting the results\nxg_boost_report_up = classification_report(y_test, xg_boost_pred_up, output_dict=True )\nprecision_xg_boost_up = xg_boost_report_up['macro avg']['precision'] \nrecall_xg_boost_up   = xg_boost_report_up['macro avg']['recall']    \nf1_score_xg_boost_up = xg_boost_report_up['macro avg']['f1-score']\naccuracy_xg_boost_up = xg_boost_report_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for XgBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_xg_boost_up = confusion_matrix(y_test, xg_boost_pred_up)\n\nsns.heatmap(cm_xg_boost_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_xg_boost_cv(X_train_res, y_train_res, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_boost_result_up= ['XgBoost upsampled', precision_xg_boost_up, \n                    recall_xg_boost_up, f1_score_xg_boost_up, accuracy_xg_boost_up]\n\nmodel_comparison.loc[len(model_comparison)] = xg_boost_result_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Xg-boost standarized with upsampling"},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the random grid to search for best hyperparameters\n\n# First creating the base model to tune\nxg_boost_std_up = xgb.XGBClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nxg_boost_random_std_up = RandomizedSearchCV(estimator = xg_boost_std_up, param_distributions = random_grid, n_iter = 20, \n                                     cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nxg_boost_random_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\nxg_boost_best_parameters_std_up = xg_boost_random_std_up.best_params_\npd.DataFrame(xg_boost_best_parameters_std_up.values(),xg_boost_best_parameters_std_up.keys(),columns=['Tuned Parameters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model: Xg-boost\nxg_boost_tunned_std_up= xgb.XGBClassifier(n_estimators=338, max_features='sqrt', max_depth=90, random_state=42)\n\n#Training the model:\nxg_boost_tunned_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\n#Predicting:\nxg_boost_pred_std_up=xg_boost_tunned_up.predict(X_test_standarized[features])\n\n# Printing the classification report \nprint(classification_report(y_test, xg_boost_pred_std_up)) \n\n#Extracting the results\nxg_boost_report_std_up = classification_report(y_test, xg_boost_pred_std_up, output_dict=True )\nprecision_xg_boost_std_up = xg_boost_report_std_up['macro avg']['precision'] \nrecall_xg_boost_std_up  = xg_boost_report_std_up['macro avg']['recall']    \nf1_score_xg_boost_std_up = xg_boost_report_std_up['macro avg']['f1-score']\naccuracy_xg_boost_std_up = xg_boost_report_std_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for XgBoost\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_xg_boost_std_up = confusion_matrix(y_test, xg_boost_pred_std_up)\n\nsns.heatmap(cm_xg_boost_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the model by cross_validation on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_xg_boost_cv(X_train_standarized_upsampled, y_train_standarized_upsampled, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_boost_result_std_up= ['XgBoost standarized upsampled', precision_xg_boost_std_up, \n                    recall_xg_boost_std_up, f1_score_xg_boost_std_up, accuracy_xg_boost_std_up]\n\nmodel_comparison.loc[len(model_comparison)] = xg_boost_result_std_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"Neural networks are brain-inspired that consist of input and output layers, and a hidden layer consisting of units that transform the input into something the output layer can use."},{"metadata":{},"cell_type":"markdown","source":"ANNs have three layers interconnected. \n- The first layer consists of input neurons that send data on to the second layer\n- The second layer, similarly sends the output neurons to the third layer"},{"metadata":{},"cell_type":"markdown","source":"ANN is rarely used for predictive modelling. The reason is that it usually tries to over-fit the relationship. ANN is generally used in cases where what has happened in past is repeated almost exactly in same way."},{"metadata":{},"cell_type":"markdown","source":"Note that:\n__A Neural Network without Activation function would simply be a Linear regression Model__"},{"metadata":{},"cell_type":"markdown","source":"##### Keras library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense #Dense module is for the layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this we will be using 'adam' that is a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. "},{"metadata":{},"cell_type":"markdown","source":"I will be adding a callback following classmates advice, previously the results of the Ann were :\n\n Ann\t0.398750\t0.500000\t0.443672\t0.7975\n\nSo Lets see if it will improve the results"},{"metadata":{},"cell_type":"markdown","source":"aka: an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n# Set callback functions to early stop training and save the best model so far\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In here wee wanted to monitor the test loss at each epoch. After the test loss has not improved after two epochs, the training will be interrupted\n\nNote: Since we set patience=2, we won’t get the best model, but the model two epochs after the best model. Therefore, optionally, we can include a second operation, ModelCheckpoint which saves the model to a file after every checkpoint "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\n# Initializing the model\nann = Sequential()\n\n# Adding the input layer and the first hidden layer\nann.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 25))\n\n\n# Adding the second hidden layer\nann.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n\n# Adding the output layer\nann.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN | means applying Stochastic Gradient Descent on ann\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Training the model:\nann.fit(X_train[features], y_train, batch_size = 10, epochs = 100, verbose = 0, callbacks=callbacks)\n\n#Predicting\ny_pred_ann= ann.predict(X_test[features])\n\n# Printing the classification report \n#As y_pred_ann is continuous and our target variable is binary, to follow the results are compared to boolean values:\ny_pred_ann_bin=(y_pred_ann>0.5)\n\nprint(classification_report(y_test, y_pred_ann_bin)) \n\n#Extracting the results\nann_report = classification_report(y_test, y_pred_ann_bin, output_dict=True )\nprecision_ann = ann_report['macro avg']['precision'] \nrecall_ann  = ann_report['macro avg']['recall']    \nf1_score_ann = ann_report['macro avg']['f1-score']\naccuracy_ann = ann_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Ann"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_ann = confusion_matrix(y_test, y_pred_ann_bin)\n\nsns.heatmap(cm_xg_boost_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_ann)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.title('ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the ANN on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ann_cv(X_train, y_train, features, k):\n    train_roc_auc, test_roc_auc, iteration = [], [], []\n    i = 1\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    for train, test in kf.split(X_train.index.values):\n        \n        ann_ = Sequential()\n        # Model    \n        ann_.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 25))\n\n        # Adding the second hidden layer\n        ann_.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n        # Adding the output layer\n        ann_.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n        \n        #Predictions on train and test set from cross val\n        preds_train=ann_.predict(X_train.iloc[train][features])\n        preds_test = ann_.predict(X_train.iloc[test][features])\n        train_roc_auc.append(roc_auc_score(y_train.iloc[train], preds_train))\n        test_roc_auc.append(roc_auc_score(y_train.iloc[test], preds_test))\n        iteration.append(i)\n        i+=1  \n    columns = {'Iteration': iteration, 'Train ROC AUC': train_roc_auc, 'Test ROC AUC': test_roc_auc}\n    results = pd.DataFrame.from_dict(columns)\n    results2 = results.drop(['Iteration'], axis=1)\n    results2.boxplot()\n    results.loc[len(results)] = [\"Mean\", np.mean(train_roc_auc), np.mean(test_roc_auc)]\n    display(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_cv(X_train, y_train, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_result= ['Ann', precision_ann, recall_ann, f1_score_ann, accuracy_ann]\n\nmodel_comparison.loc[len(model_comparison)] = ann_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural network with Oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing the model\nann_up = Sequential()\n\n# Adding the input layer and the first hidden layer\nann_up.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 25))\n\n# Adding the second hidden layer\nann_up.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nann_up.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN | means applying Stochastic Gradient Descent on ann\nann_up.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Training the model:\nann_up.fit(X_train_res[features], y_train_res, batch_size = 10, epochs = 100, verbose = 0, callbacks=callbacks)\n\n#Predicting\ny_pred_ann_up= ann_up.predict(X_test[features])\n\n# Printing the classification report \n#As y_pred_ann is continuous and our target variable is binary, to follow the results are compared to boolean values:\ny_pred_ann_bin_up=(y_pred_ann_up>0.5)\n\nprint(classification_report(y_test, y_pred_ann_bin_up)) \n\n#Extracting the results\nann_report_up = classification_report(y_test, y_pred_ann_bin_up, output_dict=True )\nprecision_ann_up = ann_report_up['macro avg']['precision'] \nrecall_ann_up  = ann_report_up['macro avg']['recall']    \nf1_score_ann_up = ann_report_up['macro avg']['f1-score']\naccuracy_ann_up = ann_report_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Ann"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_ann_up = confusion_matrix(y_test, y_pred_ann_bin_up)\n\nsns.heatmap(cm_ann_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the ANN in the train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_cv(X_train_res, y_train_res, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_result_up= ['Ann upsampled', precision_ann_up, recall_ann_up, f1_score_ann_up, accuracy_ann_up]\n\nmodel_comparison.loc[len(model_comparison)] = ann_result_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural networks standarized and upsampled"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing the model\nann_std_up = Sequential()\n\n# Adding the input layer and the first hidden layer\nann_std_up.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 25))\n\n# Adding the second hidden layer\nann_std_up.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nann_std_up.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN | means applying Stochastic Gradient Descent on ann\nann_std_up.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Training the model:\nann_std_up.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled, batch_size = 10, epochs = 100, verbose = 0, callbacks=callbacks)\n\n#Predicting\ny_pred_ann_std_up= ann_std_up.predict(X_test_standarized[features])\n\n# Printing the classification report \n#As y_pred_ann is continuous and our target variable is binary, to follow the results are compared to boolean values:\ny_pred_ann_bin_std_up=(y_pred_ann_std_up>0.5)\n\nprint(classification_report(y_test, y_pred_ann_bin_std_up)) \n\n#Extracting the results\nann_report_std_up = classification_report(y_test, y_pred_ann_bin_std_up, output_dict=True )\nprecision_ann_std_up = ann_report_std_up['macro avg']['precision'] \nrecall_ann_std_up  = ann_report_std_up['macro avg']['recall']    \nf1_score_ann_std_up = ann_report_std_up['macro avg']['f1-score']\naccuracy_ann_std_up = ann_report_std_up['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Ann"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_ann_std_up = confusion_matrix(y_test, y_pred_ann_bin_std_up)\n\nsns.heatmap(cm_ann_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_ann_std_up)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.title('ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing the stability of the ANN on train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_cv(X_train_standarized_upsampled, y_train_standarized_upsampled, features, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_result_std_up= ['Ann standarized upsampled', precision_ann_std_up, recall_ann_std_up, \n                    f1_score_ann_std_up, accuracy_ann_std_up]\n\nmodel_comparison.loc[len(model_comparison)] = ann_result_std_up\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Improving our ANN"},{"metadata":{},"cell_type":"markdown","source":"#### Tuning our ANN "},{"metadata":{},"cell_type":"markdown","source":"In this section we will be trying values for the batch size. According to kaggle advice this are likely to be powers of 2, so we will be trying 9, 25 and 32"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuning the ANN\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\ndef build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 25))\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier)\nparameters = {'batch_size': [9 , 25, 32],\n              'epochs': [100, 200],\n              'optimizer': ['adam', 'rmsprop']}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search = grid_search.fit(X_train[features], y_train, verbose = 0)\nbest_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best parameters after tuning are: {}'.format(best_parameters))\nprint('Best accuracy after tuning is: {}'.format(best_accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble boosting model: AdaBoost "},{"metadata":{},"cell_type":"markdown","source":"Note: AdaBoost uses Decision Tree Classifier as default Classifier, but in this case we will try to contrast the results obtained by running AdaBoost with Suport Vector Classifier that in general was outperformed. \nEventhough AdaBoost is considerably slower than his brother XgBoost, we wanted to try AdaBoost because it is not prone to overfitting\n\nAs SVC doesnt account for probabilities required for AdaBoost the turnaround  is to define  probabilities and use a linear kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# Defining the model: Ada-Boost\nsvc=SVC(probability=True, kernel='linear')\nada_boost = AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1,random_state=42)\n\n#Training the model:\nada_boost.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\n#Predicting:\nada_boost_pred=ada_boost.predict(X_test_standarized[features])\n\n# Printing the classification report \nprint(classification_report(y_test, ada_boost_pred)) \n\n#Extracting the results\nada_boost_report = classification_report(y_test, ada_boost_pred, output_dict=True )\nprecision_ada_boost = ada_boost_report['macro avg']['precision'] \nrecall_ada_boost  = ada_boost_report['macro avg']['recall']    \nf1_score_ada_boost = ada_boost_report['macro avg']['f1-score']\naccuracy_ada_boost = ada_boost_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_ada_boost = confusion_matrix(y_test, ada_boost_pred)\n\nsns.heatmap(cm_ada_boost, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_boost_result= ['AdaBoost standarized upsampled', precision_ada_boost, recall_ada_boost, \n                    f1_score_ada_boost, accuracy_ada_boost]\n\nmodel_comparison.loc[len(model_comparison)] = ada_boost_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble model: Weighted Average"},{"metadata":{},"cell_type":"markdown","source":"As averaging models presents the flaw that most of the time one model has more predictive power than another and disregards this, weighted average models give  more weight to the best models on the final predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, clone, RegressorMixin\nclass WeightedAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models, weights):\n        self.models = models\n        self.weights = weights\n        assert sum(self.weights)==1\n        \n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    \n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.sum(predictions*self.weights, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model: Weighted Average\nweighted_average_ = WeightedAveragedModels([decision_tree_classifier_up, svm_classifier_std_up, xg_boost_tunned_up],\n                                           [0.4, 0.4, 0.2])\n\n#Training the model:\nweighted_average_.fit(X_train_standarized_upsampled[features], y_train_standarized_upsampled)\n\n#Predicting:\nweighted_average_pred = weighted_average_.predict(X_test_standarized[features])\n\n#As weighted_average_pred is continuous and our target variable is binary, to follow the results are compared to boolean values:\nweighted_average_pred_bin=(weighted_average_pred>0.5)\n\n\n# Printing the classification report \nprint(classification_report(y_test, weighted_average_pred_bin)) \n\n#Extracting the results\nweighted_average_report = classification_report(y_test, weighted_average_pred_bin, output_dict=True )\nprecision_weighted_average = weighted_average_report['macro avg']['precision'] \nrecall_weighted_average  = weighted_average_report['macro avg']['recall']    \nf1_score_weighted_average = weighted_average_report['macro avg']['f1-score']\naccuracy_weighted_average = weighted_average_report['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix for Weighted Average"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_weighted_average = confusion_matrix(y_test, weighted_average_pred_bin)\n\nsns.heatmap(cm_weighted_average, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feeding the model comparison table"},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_average_result= ['Weighted Average standarized upsampled', precision_weighted_average, recall_weighted_average, \n                    f1_score_weighted_average, accuracy_weighted_average]\n\nmodel_comparison.loc[len(model_comparison)] = weighted_average_result\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model performance summary"},{"metadata":{},"cell_type":"markdown","source":"### Multiplot confusion matrix comparisons"},{"metadata":{"trusted":true},"cell_type":"code","source":"multiplot = plt.figure(figsize = (18,18))\n\nm_1 = multiplot.add_subplot(3,3,1)\nm_1.set_title(\"Logistic Regression\")\nsns.heatmap(cm_logistic_regression, annot = True, fmt = 'd')\n\nm_2 = multiplot.add_subplot(3,3,2)\nm_2.set_title(\"Logistic Regression upsampled\")\nsns.heatmap(cm_logistic_regression_up, annot = True, fmt = 'd')\n\nm_3 = multiplot.add_subplot(3,3,3)\nm_3.set_title(\"Logistic Regression upsampled standarized\")\nsns.heatmap(cm_logistic_regression_std_up, annot = True, fmt = 'd')\n\nm_4 = multiplot.add_subplot(3,3,4)\nm_4.set_title(\"Decision Trees\")\nsns.heatmap(cm_decision_tree, annot = True, fmt = 'd')\n\nm_5 = multiplot.add_subplot(3,3,5)\nm_5.set_title(\"Decision Trees upsampled\")\nsns.heatmap(cm_decision_tree_up, annot = True, fmt = 'd')\n\nm_6 = multiplot.add_subplot(3,3,6)\nm_6.set_title(\"Decision Trees standarized upsampled\")\nsns.heatmap(cm_decision_tree_std_up, annot = True, fmt = 'd')\n\nm_7 = multiplot.add_subplot(3,3,7)\nm_7.set_title(\"Random Forest\")\nsns.heatmap(cm_random_tree, annot = True, fmt = 'd')\n\nm_8 = multiplot.add_subplot(3,3,8)\nm_8.set_title(\"Random Forest upsampled\")\nsns.heatmap(cm_random_tree_up, annot = True, fmt = 'd')\n\nm_9 = multiplot.add_subplot(3,3,9)\nm_9.set_title(\"Random Forest standarized upsampled\")\nsns.heatmap(cm_random_tree_std_up, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiplot = plt.figure(figsize = (18,18))\n\nm_10 = multiplot.add_subplot(3,3,1)\nm_10.set_title(\"SVM\")\nsns.heatmap(cm_svm, annot = True, fmt = 'd')\n\nm_11 = multiplot.add_subplot(3,3,2)\nm_11.set_title(\"SVM upsampled\")\nsns.heatmap(cm_svm_up, annot = True, fmt = 'd')\n\nm_12 = multiplot.add_subplot(3,3,3)\nm_12.set_title(\"SVM standarized upsampled\")\nsns.heatmap(cm_svm_std_up, annot = True, fmt = 'd')\n\nm_13 = multiplot.add_subplot(3,3,4)\nm_13.set_title(\"XgBoost\")\nsns.heatmap(cm_xg_boost, annot = True, fmt = 'd')\n\nm_14 = multiplot.add_subplot(3,3,5)\nm_14.set_title(\"XgBoost upsampled\")\nsns.heatmap(cm_xg_boost_up, annot = True, fmt = 'd')\n\nm_15 = multiplot.add_subplot(3,3,6)\nm_15.set_title(\"XgBoost upsampled standarized\")\nsns.heatmap(cm_xg_boost_std_up, annot = True, fmt = 'd')\n\nm_16 = multiplot.add_subplot(3,3,7)\nm_16.set_title(\"Ann\")\nsns.heatmap(cm_ann, annot = True, fmt = 'd')\n\nm_17 = multiplot.add_subplot(3,3,8)\nm_17.set_title(\"Ann upsampled\")\nsns.heatmap(cm_ann_up, annot = True, fmt = 'd')\n\nm_18 = multiplot.add_subplot(3,3,9)\nm_18.set_title(\"Ann standarized upsampled\")\nsns.heatmap(cm_ann_std_up, annot = True, fmt = 'd')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusions"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_comparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After tunning the ANN the resulting parameters are:\nBest parameters after tuning are: {'batch_size': 9, 'epochs': 100, 'optimizer': 'adam'}\n\nFor an accuracy of : 0.796\n\nSince our ANN reaches same accuracy we decided not to re-train the ANN with those parameters.\n\n"},{"metadata":{},"cell_type":"markdown","source":"As conclusions, we can state that te best performing algorithm according to accuracy is random forest on its base form, followed by the same model but upsampled and XgBoost, also as base model.\nThis 3 models present the best accuracy and predict the target variable without focusing on the people who stayed in the bank."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_comparison.sort_values(by='Accuracy', ascending=False, na_position='first')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regarding the ensemble methods, AdaBoost was tremendously outperformed by XgBoost. To follow a ensemble comparation of the models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"multiplot = plt.figure(figsize = (18,18))\n\ne_1 = multiplot.add_subplot(3,3,1)\ne_1.set_title(\"Ensemble: AdaBoost std and upsampled with SVC as base estimator\")\nsns.heatmap(cm_ada_boost, annot = True, fmt = 'd')\n\ne_2 = multiplot.add_subplot(3,3,2)\ne_2.set_title(\"Ensemble: Weighted Average std and upsampled\")\nsns.heatmap(cm_weighted_average, annot = True, fmt = 'd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model comparisons according to accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_models=[accuracy_logistic_regression, accuracy_logistic_regression_up, accuracy_logistic_regression_std_up, \n                 accuracy_decision_tree, accuracy_decision_tree, accuracy_decision_tree_up, accuracy_decision_tree_std_up,\n                accuracy_random_tree, accuracy_random_tree_up, accuracy_random_tree_std_up, accuracy_svm, accuracy_svm_up,\n                accuracy_svm_up, accuracy_svm_std_up, accuracy_xg_boost, accuracy_xg_boost_up, accuracy_xg_boost_std_up,\n                accuracy_ann, accuracy_ann_up, accuracy_ann_std_up, accuracy_ada_boost, accuracy_weighted_average]\nlabels=['accuracy_logistic_regression', 'accuracy_logistic_regression_up', 'accuracy_logistic_regression_std_up', \n                 'accuracy_decision_tree', 'accuracy_decision_tree', 'accuracy_decision_tree_up', 'accuracy_decision_tree_std_up',\n                'accuracy_random_tree', 'accuracy_random_tree_up', 'accuracy_random_tree_std_up', 'accuracy_svm, accuracy_svm_up',\n                'accuracy_svm_up', 'accuracy_svm_std_up', 'accuracy_xg_boost', 'accuracy_xg_boost_up', 'accuracy_xg_boost_std_up',\n                'accuracy_ann', 'accuracy_ann_up', 'accuracy_ann_std_up', 'accuracy_ada_boost', 'accuracy_weighted_average']\ny_pos = np.arange(len(accuracy_models))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt; plt.rcdefaults()\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.barh(y_pos, accuracy_models, align='center', alpha=0.5)\nplt.yticks(y_pos, labels)\nplt.ylabel('Accuracy performance')\nplt.title('ML algorithms')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also worth mentioning that, results followed the theory behind them as the algorithms which performed high accuracy  without the standarization are the decision rule based algorithms, namely the decision trees and their ensemble versions."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}