{"cells":[{"metadata":{},"cell_type":"markdown","source":"# League of Legends game result prediction\n## Goal:predicting result with data of first 10 minutes game.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. [Data Checking](#01)\n2. [Deeper look at blue team features](#02)\n3. [Feature Engineering](#03)\n4. [Machine learning preprocessing](#04)\n5. [Gaussian Naive Bayes](#05)\n > [5.1 Optimize Gaussian Naive Bayes](#051)\n\n > [5.2 Visualise the comparison of different parameters](#052)\n \n > [5.3 Gaussian Naive bayes with hyperparameter](#053)\n\n6. [KNN](#06)\n > [6.1 Optimize KNN](#061)\n\n > [6.2 Score of different parameters of KNN](#062)\n\n > [6.3 Visualise the comparison of different parameters](#063)\n \n > [6.4 KNN with hyperparameters](#064)\n\n7. [Decision Tree](#07)\n >[7.1 Optimize Decision Tree](#071)\n \n >[7.2 Visualise the comparison of different parameters](#072)\n \n >[7.3 Decision Tree with hyperparameters](#073)\n \n8. [Comparison of different algorithm](#08)\n9. [Confusion matrices of GaussianNB, KNN, Decision Tree with hyperparameters](#09)\n10. [Conclusion](#10)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(This is not a tutorial)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries and Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ngames = pd.read_csv(\"../input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv\")\ngames.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=01></a>\n# 1. Data Checking","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Missing value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"games.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Infinite value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"games[games==np.inf]=np.nan\ngames.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! no missing value and infinite value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"games.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=02></a>\n# 2. Deeper look at blue team features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"games_blue=games.iloc[:,1:21] #drop ID and red team feature\ngames_blue.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation between blue team features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax=plt.subplots(figsize=(18,8))\nsns.heatmap(games_blue.corr(), annot=True, linewidth=0.5, fmt='.1f', ax=ax,cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=03></a>\n# 3. Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For win or loose is a relative issue, simply using the data of one team doesn't make sense. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding \"relative\" features\ngames_df = games_blue\nblueVision = games['blueWardsPlaced']-games['redWardsDestroyed']\nredVision = games['redWardsPlaced']-games['blueWardsDestroyed']\nblueKdRatio = games_df['blueKills']/games_df['blueDeaths']\nredKdRatio = games['redKills']/games['redDeaths']\ngames_df['blueRedKdDiff']= blueKdRatio - redKdRatio\ngames_df['blueVisionDiff']= blueVision - redVision","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"variables explanation:\n* blue(red)Vision: Sight that blue(red) actually have.\n* blue(red)KdRatio: blue(red)Kills/blue(red)Kills","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## x and y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=games_df.drop(['blueWins','blueWardsPlaced','blueWardsDestroyed','blueKills','blueDeaths','blueGoldPerMin',\n                'blueTotalExperience','blueTotalGold'],axis=1)\ny=games_df.blueWins","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handle negative values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since negative values can affect the result of machine learning after standardize, we have to handle them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Move absolute of neagative values to a new features\nx['rGD_n']=0\nx['rGD_p']=0\nfor i in range(9879):\n    if(x.loc[i,'blueGoldDiff']<0):\n        x.loc[i,'rGD_n']=abs(x.loc[i,'blueGoldDiff'])\n    elif(x.loc[i,'blueGoldDiff']>=0):\n            x.loc[i,'rGD_p']=x.loc[i,'blueGoldDiff']\n            \n\nx['blueKdDiff_p']=0\nx['blueKdDiff_n']=0\n\nfor i in range(9879):\n    if(x.loc[i,'blueRedKdDiff']<0):\n        x.loc[i,'blueKdDiff_n']=abs(x.loc[i,'blueRedKdDiff'])\n    elif(x.loc[i,'blueRedKdDiff']>=0):\n            x.loc[i,'blueKdDiff_p']=x.loc[i,'blueRedKdDiff']\n\nx['blueVD_n']=0\nx['blueVD_p']=0\n\nfor i in range(9879):\n    if(x.loc[i,'blueVisionDiff']<0):\n        x.loc[i,'blueVD_n']=abs(x.loc[i,'blueVisionDiff'])\n    elif(x.loc[i,'blueVisionDiff']>=0):\n            x.loc[i,'blueVD_p']=x.loc[i,'blueVisionDiff']\n\nx=x.drop(['blueGoldDiff','blueExperienceDiff','blueRedKdDiff','blueVisionDiff'], axis=1)\n\n#check data\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Infinite value in x","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check infinity value\nx[x==np.inf]=np.nan\nx.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill nan with max value in there column\nx.fillna(x.max(axis=0), inplace=True)\nx.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=04></a>\n# 4. Machine learning preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Train/Test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size = 0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardize features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#for train/test split\nx_train = (x_train - x_train.min(axis=0)) / (x_train.max(axis=0) - x_train.min(axis=0))\nx_test = (x_test - x_test.min(axis=0)) / (x_test.max(axis=0) - x_test.min(axis=0))\n#for cross fold validation\nX = (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Algorithms","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Gaussian Naive Bayes\n2. KNN\n3. Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#used to store accuracies of each algorithms\nacc={}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=05></a>\n# 5. Gaussian Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#naive bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\nacc_gnb1=gnb.score(x_test,y_test)*100\nacc['GaussianNB_brfore_tuning'] = acc_gnb1\nprint('Accuracy of GNB:{:.2f}%'.format(acc_gnb1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=051></a>\n## 5.1 Optimize Gaussian Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nnp.random.seed(999)\n\nnb_classifier = GaussianNB()\n\nparams_NB = {'var_smoothing': np.logspace(0,-9, num=150)}\n\ngs_NB = GridSearchCV(estimator=nb_classifier, \n                     param_grid=params_NB, \n                     cv=3,\n                     verbose=1, \n                     scoring='accuracy')\n\ngs_NB.fit(X, y);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_NB.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_NB.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=052></a>\n## 5.2 Visualise the comparison of different parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results_NB = pd.DataFrame(gs_NB.cv_results_['params'])\nresults_NB['test_score'] = gs_NB.cv_results_['mean_test_score']\n\nplt.plot(results_NB['var_smoothing'], results_NB['test_score'], marker = '.')    \nplt.xlabel('Var. Smoothing')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"NB Performance Comparison\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=053></a>\n## 5.3 Gaussian Naive bayes with hyperparameter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb2 = GaussianNB(var_smoothing = 0.003338027673990301)\ngnb2.fit(x_train, y_train)\nacc_gnb2=gnb2.score(x_test,y_test)*100\nacc['GaussianNB_after_tuning'] = acc_gnb2\nprint('Accuracy of GNB after tuning:{:.2f}%'.format(acc_gnb2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=06></a>\n# 6. KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1, p=2)\n\nknn.fit(x_train, y_train)\nacc_knn1 = knn.score(x_test, y_test)*100\nacc['KNN_before_tuning'] = acc_knn1\nprint('Accuracy of knn:{:.2f}%'.format(acc_knn1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=061></a>\n## 6.1 Optimize KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params_KNN = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7], \n              'p': [1, 2, 5]}\n\nfrom sklearn.model_selection import GridSearchCV\n\ngs_KNN = GridSearchCV(estimator=KNeighborsClassifier(), \n                      param_grid=params_KNN, \n                      cv=3,\n                      verbose=1,  # verbose: the higher, the more messages\n                      scoring='accuracy', \n                      return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_KNN.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_KNN.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_KNN.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_KNN.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_KNN = pd.DataFrame(gs_KNN.cv_results_['params'])\nresults_KNN['test_score'] = gs_KNN.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=062></a>\n## 6.2 Score of different parameters of KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results_KNN['metric'] = results_KNN['p'].replace([1,2,5], [\"Manhattan\", \"Euclidean\", \"Minkowski\"])\nresults_KNN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=063></a>\n## 6.3 Visualise the comparison of different parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"ggplot\")\n\nfor i in [\"Manhattan\", \"Euclidean\", \"Minkowski\"]:\n    temp = results_KNN[results_KNN['metric'] == i]\n    plt.plot(temp['n_neighbors'], temp['test_score'], marker = '.', label = i)\n    \nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"KNN Performance Comparison\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=064></a>\n## 6.4 KNN with hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn2 = KNeighborsClassifier(n_neighbors=7, p=1)\n\nknn2.fit(x_train, y_train)\nacc_knn2 = knn2.score(x_test, y_test)*100\nacc['KNN_after_tuning'] = acc_knn2\nprint('Accuracy of knn after tuning:{:.2f}%'.format(acc_knn2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=07></a>\n# 7. Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nacc_dt1 = dt.score(x_test,y_test)*100\nacc['Decision_tree_before_tuning'] = acc_dt1\nprint('Accuracy of Decision tree before tuning:{:.2f}%'.format(acc_dt1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=071></a>\n## 7.1 Optimize Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_classifier = DecisionTreeClassifier(random_state=999)\n\nparams_DT = {'criterion': ['gini', 'entropy'],\n             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8],\n             'min_samples_split': [2, 3]}\n\ngs_DT = GridSearchCV(estimator=df_classifier, \n                     param_grid=params_DT, \n                     cv=3,\n                     verbose=1, \n                     scoring='accuracy')\n\ngs_DT.fit(X, y);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_DT.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_DT.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=072></a>\n## 7.2 Visualise the comparison of different parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results_DT = pd.DataFrame(gs_DT.cv_results_['params'])\nresults_DT['test_score'] = gs_DT.cv_results_['mean_test_score']\nresults_DT.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['gini', 'entropy']:\n    temp = results_DT[results_DT['criterion'] == i]\n    temp_average = temp.groupby('max_depth').agg({'test_score': 'mean'})\n    plt.plot(temp_average, marker = '.', label = i)\n    \n    \nplt.legend()\nplt.xlabel('Max Depth')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"DT Performance Comparison\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=073></a>\n## 7.3 Decision Tree with hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt2 = DecisionTreeClassifier(criterion = 'gini', max_depth=4, min_samples_split =2)\ndt2.fit(x_train,y_train)\nacc_dt2 = dt2.score(x_test,y_test)*100\nacc['Decision_tree_after_tuning'] = acc_dt2\nprint('Accuracy of Decision tree after tuning:{:.2f}%'.format(acc_dt2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=08></a>\n# 8. Comparison of different algorithm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_df = pd.DataFrame(acc.items(), columns=['Algorithm', 'acc_score'])\nacc_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,8))\nax = sns.barplot(x='Algorithm', y='acc_score', data = acc_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=09></a>\n# 9. Confusion matrices of GaussianNB, KNN, Decision Tree with hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_NB = gnb2.predict(x_test)\ny_KNN = knn2.predict(x_test)\ny_DT = dt2.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_nb = confusion_matrix(y_test,y_NB)\ncm_knn = confusion_matrix(y_test,y_KNN)\ncm_dt = confusion_matrix(y_test,y_DT)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix of gaussianNB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix of KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix of Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm_dt,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=10></a>\n# 10. Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With optimized Gaussian Naive bayes we have almost 74% test accuracy, which is somewhat good result, for this is only first 10 minutes of the game(the average game time is around 35 minutes).\n* How to improve the test accuracy?\n 1. Maybe use other models to train this data, such as SVM, Random forest, or unsupervised algorithms.\n 2. Select different features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'm new to data science, still need to learn a lot.\nDon't be hesitate to share your advice or thought on this kernel, any opinion is a treasure.\nThank you!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}