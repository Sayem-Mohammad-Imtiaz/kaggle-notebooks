{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\"> A comparison of different classifiers’ accuracy & performance for high-dimensional data</h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Problem formulation</h2>\n\nThe **EEG Brainwave Dataset** contains electronic brainwave signals from an EEG headset and is in temporal format.\n\nThe challenge is: **Can we predict emotional sentiment from brainwave readings?**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Import Packages</h2>","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.listdir('../input')\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, Dropout, Bidirectional\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brainwave_df = pd.read_csv('../input/emotions.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Peek of Data</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"brainwave_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Dimensions of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"brainwave_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Descriptive Statistics</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"brainwave_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Class Distribution</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x=brainwave_df.label, color='blue')\nplt.title('Emotional sentiment class distribution')\nplt.ylabel('Class Counts')\nplt.xlabel('Class Label')\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Correlation Between Attributes</h2>\nCorrelation refers to the relationship between two variables and how they may or may not change together.\n\nThe most common method for calculating correlation is [Pearson’s Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient), that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df = brainwave_df['label']\nbrainwave_df.drop('label', axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = brainwave_df.corr(method='pearson')\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Skew of Univariate Distributions</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skew = brainwave_df.skew()\nskew","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">RandomForest Classifier</h2>\n\n`RandomForest` is a tree & bagging approach-based ensemble classifier. It will automatically reduce the number of features by its probabilistic entropy calculation approach.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\n\npl_random_forest = Pipeline(steps=[('random_forest', RandomForestClassifier())])\nscores = cross_val_score(pl_random_forest, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for RandomForest : ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Logistic Regression Classifier</h2>\n\n`Logistic Regression` is a linear classifier and works in same way as linear regression.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\n\npl_log_reg = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores = cross_val_score(pl_log_reg, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression: ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Principal Component Analysis (PCA)</h2>\n\nPCA can transform original low level variables to a higher dimensional space and thus reduce the number of required variables. All co-linear variables get clubbed together. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaled_df = scaler.fit_transform(brainwave_df)\npca = PCA(n_components = 30)\npca_vectors = pca.fit_transform(scaled_df)\nfor index, var in enumerate(pca.explained_variance_ratio_):\n    print(\"Explained Variance ratio by Principal Component \", (index+1), \" : \", var)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(pca.explained_variance_ratio_)\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,8))\nsns.scatterplot(x=pca_vectors[:, 0], y=pca_vectors[:, 1], hue=label_df)\nplt.title('Principal Components vs Class distribution', fontsize=16)\nplt.ylabel('Principal Component 2', fontsize=16)\nplt.xlabel('Principal Component 1', fontsize=16)\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Logistic Regression classifier with these two PCs</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npl_log_reg_pca = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('pca', PCA(n_components = 2)),\n                             ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores = cross_val_score(pl_log_reg_pca, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression with 2 Principal Components: ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Take all 10 PCs</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npl_log_reg_pca_10 = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('pca', PCA(n_components = 10)),\n                             ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores = cross_val_score(pl_log_reg_pca_10, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression with 10 Principal Components: ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Artificial Neural Network Classifier (ANN)</h2>\n\nAn ANN classifier is non-linear with automatic feature engineering and dimensional reduction techniques. `MLPClassifier` in scikit-learn works as an ANN. But here also, basic scaling is required for the data.[](http://)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npl_mlp = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('mlp_ann', MLPClassifier(hidden_layer_sizes=(1275, 637)))])\nscores = cross_val_score(pl_mlp, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for ANN : ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Linear Support Vector Machines Classifier (SVM)</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npl_svm = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('pl_svm', LinearSVC())])\nscores = cross_val_score(pl_svm, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Linear SVM : ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Extreme Gradient Boosting Classifier (XGBoost)</h2>\n\nXGBoost is a boosted tree based ensemble classifier. Like ‘RandomForest’, it will also automatically reduce the feature set. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npl_xgb = Pipeline(steps=\n                  [('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])\nscores = cross_val_score(pl_xgb, brainwave_df, label_df, cv=10)\nprint('Accuracy for XGBoost Classifier : ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.array(brainwave_df).shape\nX = np.array(brainwave_df)\n# X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\nY = np.array(label_df)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\nX_train.shape, X_test.shape, Y_train.shape, Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train = np.resize(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.resize(X_test, (X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_enc = LabelEncoder()\nY_train = label_enc.fit_transform(Y_train)\nY_test = label_enc.transform(Y_test)\nY_train.shape, Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(120, activation='relu', input_shape=(1, 2548)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(X_test, Y_test, verbose=0)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM with Dropout","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lstmdrop = Sequential()\nlstmdrop.add(LSTM(100))\nlstmdrop.add(Dropout(0.2))\nlstmdrop.add(Dense(1, activation='sigmoid'))\nlstmdrop.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_lstm = lstmdrop.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_dropout = lstmdrop.evaluate(X_test, Y_test)\nscores_dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacked LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_stack = Sequential()\nlstm_stack.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(1, 2548)))\nlstm_stack.add(LSTM(100, activation='relu', return_sequences=True))\nlstm_stack.add(LSTM(50, activation='relu', return_sequences=True))\nlstm_stack.add(LSTM(25, activation='relu'))\nlstm_stack.add(Dense(20, activation='relu'))\nlstm_stack.add(Dense(10, activation='relu'))\nlstm_stack.add(Dense(1, activation='sigmoid'))\nlstm_stack.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_stacked = lstm_stack.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_stacked = lstm_stack.evaluate(X_test, Y_test)\nscores_stacked","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONV + LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conv = Sequential()\nconv.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nconv.add(MaxPooling1D(pool_size=2, padding='same'))\nconv.add(LSTM(100))\nconv.add(Dense(1, activation='sigmoid'))\nconv.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_conv = conv.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_conv = conv.evaluate(X_test, Y_test)\nscores_conv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacked LSTM + Conv ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_stack = Sequential()\nconv_stack.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nconv_stack.add(MaxPooling1D(pool_size=2, padding='same'))\nconv_stack.add(LSTM(100, activation='relu', return_sequences=True))\nconv_stack.add(LSTM(50, activation='relu', return_sequences=True))\nconv_stack.add(LSTM(25, activation='relu'))\nconv_stack.add(Dense(20, activation='relu'))\nconv_stack.add(Dense(10, activation='relu'))\nconv_stack.add(Dense(1, activation='sigmoid'))\nconv_stack.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_conv_stack = conv_stack.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_conv_stack = conv_stack.evaluate(X_test, Y_test)\nscores_conv_stack","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BiLSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bi = Sequential()\nbi.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1, 2548)))\nbi.add(Dense(1))\nbi.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_bi = bi.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_bi = bi.evaluate(X_test, Y_test)\nscores_bi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Histories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history_lstm.history.keys())\n# summarize history for accuracy\nplt.plot(history_lstm.history['acc'])\nplt.plot(history_lstm.history['val_acc'])\nplt.title('LSTM/Dropout Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_lstm.history['loss'])\nplt.plot(history_lstm.history['val_loss'])\nplt.title('LSTM/Dropout Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history_stacked.history.keys())\n# summarize history for accuracy\nplt.plot(history_stacked.history['acc'])\nplt.plot(history_stacked.history['val_acc'])\nplt.title('Stacked LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_stacked.history['loss'])\nplt.plot(history_stacked.history['val_loss'])\nplt.title('Stacked LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history_conv.history.keys())\n# summarize history for accuracy\nplt.plot(history_conv.history['acc'])\nplt.plot(history_conv.history['val_acc'])\nplt.title('Convolutions + LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_conv.history['loss'])\nplt.plot(history_conv.history['val_loss'])\nplt.title('Convolutions + LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history_conv_stack.history.keys())\n# summarize history for accuracy\nplt.plot(history_conv_stack.history['acc'])\nplt.plot(history_conv_stack.history['val_acc'])\nplt.title('Convolutions + Stacked LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_conv_stack.history['loss'])\nplt.plot(history_conv_stack.history['val_loss'])\nplt.title('Convolutions + Stacked LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history_bi.history.keys())\n# summarize history for accuracy\nplt.plot(history_bi.history['acc'])\nplt.plot(history_bi.history['val_acc'])\nplt.title('Bidirectional LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_bi.history['loss'])\nplt.plot(history_bi.history['val_loss'])\nplt.title('Bidirectional LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}