{"cells":[{"metadata":{},"cell_type":"markdown","source":"读取数据"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\n#读取数据\ndata_dir = \"../input\"\ndf = pd.read_csv(data_dir + '/spam.csv', encoding = 'latin-1')\n\n#拆分数据为训练集与测试集\ndata_train, data_test, labels_train, labels_test = train_test_split(df.v2, df.v1, test_size=0.2, random_state=0)\n\n#print('拆分过后的每个邮件内容')\nprint(data_train[:10])\n#print(\"拆分过后每个邮件是否是垃圾邮件\")\nprint(labels_train[0:10])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"建立词汇表，统计两个类目下总计词数"},{"metadata":{"trusted":true},"cell_type":"code","source":"#用一个dictionary保存词汇，给每个词汇赋唯一id\n\ndef GetVocabulary(data):\n    vocab_dict = {}\n    wid = 0\n    for document in data:\n        words = document.split()\n        for word in words:\n            word = word.lower()\n            if word not in vocab_dict:\n                vocab_dict[word] = wid\n                wid += 1\n    return vocab_dict\n\n#用训练集建立词汇表\nvocab_dict = GetVocabulary(data_train)\nprint('Number of all the unique words:' + str(len(vocab_dict.keys())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"把文章变成词向量\nlen与vocab_dict一致"},{"metadata":{"trusted":true},"cell_type":"code","source":"#把文本变成向量的表示形式，以便计算\n\ndef Document2Vector(vocab_dict, data):\n    word_vector = np.zeros(len(vocab_dict.keys()))\n    words = data.split()\n    for word in words:\n        word = word.lower()\n        if word in vocab_dict:\n            word_vector[vocab_dict[word]] += 1 \n    return word_vector\n\n#解释向量输出例子\nexample = Document2Vector(vocab_dict, \"we are good good\")\nprint(example)\nprint(example[vocab_dict['we']],example[vocab_dict['are']],example[vocab_dict['good']])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train_matrix append vector\nvector[i] len与 vocab_dict一致"},{"metadata":{"trusted":true},"cell_type":"code","source":"#把训练集df变成向量形式\ntrain_matrix = []\nfor document in data_train.values:\n    word_vector = Document2Vector(vocab_dict, document)\n    train_matrix.append(word_vector)\n\nprint(len(train_matrix))\nprint(train_matrix[:10])\nprint(np.array(train_matrix).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"naive bayes训练，得到训练集每个词概率"},{"metadata":{"trusted":true},"cell_type":"code","source":"#训练计算两个概率：\n#1.词在每个分类下的概率 P（email/spam)\n#2. 每个分类的概率 P（spam)\n\ndef NaiveBayes_train(train_matrix, labels_train):\n    num_docs = len(train_matrix)\n    num_words = len(train_matrix[0])\n    spam_word_counter = np.ones(num_words)\n    ham_word_counter = np.ones(num_words)\n    \n    ham_total_count = 0\n    spam_total_count = 0\n    \n    spam_count = 0\n    ham_count = 0\n    \n    for i in range(num_docs):\n        if i%500 == 0:\n            print(\"train on the doc id: \" + str(i))\n        \n        if labels_train[i] == 'ham':\n            ham_word_counter += train_matrix[i]\n            ham_total_count += sum(train_matrix[i])\n            ham_count += 1\n        else: \n            spam_word_counter += train_matrix[i]\n            spam_total_count += sum(train_matrix[i])\n            spam_count += 1\n            \n#对概率取log\n\n    p_spam_vector = np.log(spam_word_counter/(spam_total_count + num_words)) #注意在分母也加上平滑部分\n    p_ham_vector = np.log(ham_word_counter/(ham_total_count + num_words))\n    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs)\n                         \n#train\n\np_spam_vector, p_spam, p_ham_vector, p_ham =NaiveBayes_train(train_matrix, labels_train.values)\n                      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"进行测试集预测"},{"metadata":{"trusted":true},"cell_type":"code","source":"#对测试集进行预测，计算随机emil单词两个分类下的概率，选择较大者作为分类结果\n\ndef Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham):\n\n    spam = sum(test_word_vector * p_spam_vector) + p_spam\n    ham = sum(test_word_vector * p_ham_vector) + p_ham\n    if spam > ham:\n        return 'spam'\n    else:\n        return 'ham' \n\npredictions = []\ni = 0\nfor document in data_test.values:\n    if i%100 == 0:\n        print('test training on doc:' + str(i))\n    i += 1\n    test_word_vector = Document2Vector(vocab_dict, document)\n    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n    predictions.append(ans)\n    \nprint(len(predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"检测模型"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nprint(accuracy_score(labels_test, predictions))\nprint(classification_report(labels_test, predictions))\nprint(confusion_matrix(labels_test, predictions))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}