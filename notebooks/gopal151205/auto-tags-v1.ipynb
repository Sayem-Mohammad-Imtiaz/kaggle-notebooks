{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport nltk\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['Questions.csv', 'Tags.csv', 'Answers.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#ques = pd.read_csv('../input/stacksample/Questions.csv', encoding='ISO-8859-1')\nques = pd.read_csv('../input/Questions.csv', encoding='ISO-8859-1')\ntags = pd.read_csv('../input/Tags.csv', encoding='ISO-8859-1')\nans = pd.read_csv('../input/Answers.csv', encoding='ISO-8859-1')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c3460b633455179c5adac3da057fdc855f81167"},"cell_type":"code","source":"ques.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"681c32c1404d2e321c689220784685c3e42f1338"},"cell_type":"code","source":"ans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d54f5f1524d1eb0a8b574814e46452ad0c81cc6"},"cell_type":"code","source":"tags.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9e91226a6448159f9e5c48d24bf71af5c655f26"},"cell_type":"code","source":"print('Ques shape: ', ques.shape)\nprint('Ans shape: ', ans.shape)\nprint('Tags shape: ', tags.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5968cd98c803d5fb6fa592550d17784e4376eb57","scrolled":true},"cell_type":"code","source":"import collections\nimport math\nans_per_question = collections.Counter(ans['ParentId'])\n#{quesId: ansCount}\n# { 406760: 408, 100420: 100, 40480: 69, 490420: 67, 226970: 55, 202750: 51, 17054000: 49 }\n# ans_per_question.most_common() =>> [(406760, 408), (38210, 316), (23930, 129), (100420, 100)]\nquesId,nosAnswers = zip(*ans_per_question.most_common())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ce3498d9febd96bee22ea45e113b022e226f7ed"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nN=20\nplt.bar(range(N), nosAnswers[:N], align='center', alpha=0.5)\nplt.ylabel('Number of Answers per Question')\nplt.xlabel('Question Id')\nplt.title('Distribution of Answers per question')\nplt.text(3,400,\"Avegrage answers per question \"+str(math.ceil((np.mean(nosAnswers)))))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d1fc48b52bdba394670556298b2313000515be8"},"cell_type":"code","source":"ans_freq_counter = collections.Counter(ans_per_question.values())\n\nans_count,nosQuestions = zip(*ans_freq_counter.most_common())\nN=10\nplt.bar(ans_count[:N], nosQuestions[:N], align='center', alpha=0.5)\nplt.ylabel('Number of Questions')\nplt.xlabel('Answer count')\nplt.title('Questions vs Their Answer count')\nplt.text(5,500000,\"Avegrage answers per question \"+str(math.ceil((np.mean(nosAnswers)))))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5526048cb53a37974170e2889a8ffe4c6129fde"},"cell_type":"code","source":"tags_per_question = collections.Counter(tags['Id'])\n# { 406760: 8, 100420: 7, 40480: 6, 490420: 5, 226970: 5, 202750: 4, 17054000: 2 }\n# ans_per_question.most_common() =>> [(406760, 8), (38210, 7), (23930, 6), (100420, 2)]\ntags_freq_counter = collections.Counter(tags_per_question.values())\n\ntags_count,nosQuestions = zip(*tags_freq_counter.most_common())\nN=10\nplt.bar(tags_count[:N], nosQuestions[:N], align='center', alpha=0.5)\nplt.ylabel('Number of Questions')\nplt.xlabel('Tags count')\nplt.title('Questions vs Their tags count')\nplt.text(2,340000,\"Avegrage Tags per question \"+str(math.ceil((np.mean(tags_count)))))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4045ae21f2845769b27b74d98799ed13a51a06a5"},"cell_type":"code","source":"print('Popular tags')\ntagCount =  collections.Counter(list(tags['Tag']))\ntagName,freq = zip(*tagCount.most_common(15))\nplt.bar(tagName, freq )\nplt.xticks(rotation='vertical')\nplt.ylabel('Tag Count')\nplt.xlabel('Tags name')\nplt.title('Tags vs tags count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52f3bf4783968bfaf99a2cd6a11132bfab038545"},"cell_type":"code","source":"import datetime\nques['datetime'] = pd.to_datetime(ques['CreationDate'])\nques.set_index('datetime', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"845665d898f30665822c2256eab37ae11ebd1167"},"cell_type":"code","source":"weeklyQues = ques.resample('M').count()\nweeklyQues.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"add96679119191b9604deb43d901686e2e85d75e"},"cell_type":"code","source":"weeklyQues['datetime'] = weeklyQues.index\nweeklyQues.plot(x='datetime', y='Title', kind='line', lw=0.75, c='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcaf17d4a526d9c4e8afe832342ee82ca1d254df"},"cell_type":"code","source":"#tags.info()\ntags['Tag'] = tags['Tag'].astype(str)\ngrouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"093f597b6915de9c9632864c4c6310c7888a11f2","scrolled":true},"cell_type":"code","source":"grouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags})\ngrouped_tags_final.head()\ngrouped_tags.reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5095c90bda25acdffeaedc7ee20bdbf1f1a3f67"},"cell_type":"code","source":"ques.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dd4148239268e972f35f5386eac52344ff277aa"},"cell_type":"code","source":"score_gt_5 = ques['Score'] >= 5\nques = ques[score_gt_5]\nques.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e428ca119e79bce6c40e29014808f8d74a3108dc"},"cell_type":"code","source":"ques.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10b7c66c1f2ce4062f12299e5a11a55aff691492"},"cell_type":"code","source":"ques.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac7bfea869e4de847bea9b2809644c0c61bf329b"},"cell_type":"code","source":"merged_ques = ques.merge(grouped_tags_final, on='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b45244ae4a7066cd736a3bfc89b5518ee083ada0"},"cell_type":"code","source":"merged_ques.drop(columns=['Id', 'Score'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d67b13fa5fba3e0aea9d45ffe8d698fefe357df6"},"cell_type":"code","source":"merged_ques.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques['Tags'] = merged_ques['Tags'].apply(lambda x: x.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flat_list = [item for sublist in merged_ques['Tags'].values for item in sublist]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords = nltk.FreqDist(flat_list)\n\nkeywords = nltk.FreqDist(keywords)\n\nfrequencies_words = keywords.most_common(100)\ntags_features = [word[0] for word in frequencies_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_common(tags):\n    tags_filtered = []\n    for i in range(0, len(tags)):\n        if tags[i] in tags_features:\n            tags_filtered.append(tags[i])\n    return tags_filtered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques['Tags'] = merged_ques['Tags'].apply(lambda x: most_common(x))\nmerged_ques['Tags'] = merged_ques['Tags'].apply(lambda x: x if len(x)>0 else None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques.dropna(subset=['Tags'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nimport unicodedata\nimport spacy\nimport nltk\nimport re\n\nnlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\nstopword_list = stopwords.words('english')\n\ntokenizer = ToktokTokenizer()\nlemmatizer = WordNetLemmatizer()\nlemma=WordNetLemmatizer()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strip_html_tags(text):\n    return BeautifulSoup(text, \"html.parser\").get_text()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_characters(text):\n    pattern = r'[^a-zA-z0-9#\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_stemmer(text):\n    ps = nltk.porter.PorterStemmer()\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nltk2wn_tag(nltk_tag):\n  if nltk_tag.startswith('J'):\n    return wordnet.ADJ\n  elif nltk_tag.startswith('V'):\n    return wordnet.VERB\n  elif nltk_tag.startswith('N'):\n    return wordnet.NOUN\n  elif nltk_tag.startswith('R'):\n    return wordnet.ADV\n  else:          \n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_text(sentence):\n  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n  res_words = []\n  for word, tag in wn_tagged:\n    if tag is None:            \n      res_words.append(word)\n    else:\n      res_words.append(lemmatizer.lemmatize(word, tag))\n  return \" \".join(res_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_contractions(text):\n    text = text.lower()\n    text = re.sub(r\"ain't\", \"is not \", text)\n    text = re.sub(r\"aren't\", \"are not \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"can't've\", \"cannot have \", text)\n    text = re.sub(r\"'cause\", \"because \", text)\n    text = re.sub(r\"could've\", \"could have \", text)\n    text = re.sub(r\"couldn't\", \"could not \", text)\n    text = re.sub(r\"couldn't've\", \"could not have \", text)\n    text = re.sub(r\"didn't\", \"did not \", text)\n    text = re.sub(r\"doesn't\", \"does not \", text)\n    text = re.sub(r\"don't\", \"do not \", text)\n    text = re.sub(r\"hadn't\", \"had not \", text)\n    text = re.sub(r\"hadn't've\", \"had not have \", text)\n    text = re.sub(r\"hasn't\", \"has not \", text)\n    text = re.sub(r\"haven't\", \"have not \", text)\n    text = re.sub(r\"he'd\", \"he would \", text)\n    text = re.sub(r\"he'd've\", \"he would have \", text)\n    text = re.sub(r\"he'll\", \"he will \", text)\n    text = re.sub(r\"he'll've\", \"he he will have \", text)\n    text = re.sub(r\"he's\", \"he is \", text)\n    text = re.sub(r\"how'd\", \"how did \", text)\n    text = re.sub(r\"how'd'y\", \"how do you \", text)\n    text = re.sub(r\"how'll\", \"how will \", text)\n    text = re.sub(r\"how's\", \"how is \", text)\n    text = re.sub(r\"I'd\", \"I would \", text)\n    text = re.sub(r\"I'd've\", \"I would have \", text)\n    text = re.sub(r\"I'll\", \"I will \", text)\n    text = re.sub(r\"I'll've\", \"I will have \", text)\n    text = re.sub(r\"I'm\", \"I am \", text)\n    text = re.sub(r\"I've\", \"I have \", text)\n    text = re.sub(r\"i'd\", \"i would \", text)\n    text = re.sub(r\"i'd've\", \"i would have \", text)\n    text = re.sub(r\"i'll\", \"i will \", text)\n    text = re.sub(r\"i'll've\", \"i will have \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"i've\", \"i have \", text)\n    text = re.sub(r\"isn't\", \"is not \", text)\n    text = re.sub(r\"it'd\", \"it would \", text)\n    text = re.sub(r\"it'd've\", \"it would have \", text)\n    text = re.sub(r\"it'll\", \"it will \", text)\n    text = re.sub(r\"it'll've\", \"it will have \", text)\n    text = re.sub(r\"it's\", \"it is \", text)\n    text = re.sub(r\"let's\", \"let us \", text)\n    text = re.sub(r\"ma'am\", \"madam \", text)\n    text = re.sub(r\"mayn't\", \"may not \", text)\n    text = re.sub(r\"might've\", \"might have \", text)\n    text = re.sub(r\"mightn't\", \"might not \", text)\n    text = re.sub(r\"mightn't've\", \"might not have \", text)\n    text = re.sub(r\"must've\", \"must have \", text)\n    text = re.sub(r\"mustn't\", \"must not \", text)\n    text = re.sub(r\"mustn't've\", \"must not have \", text)\n    text = re.sub(r\"needn't\", \"need not \", text)\n    text = re.sub(r\"needn't've\", \"need not have \", text)\n    text = re.sub(r\"o'clock\", \"of the clock \", text)\n    text = re.sub(r\"oughtn't\", \"ought not \", text)\n    text = re.sub(r\"oughtn't've\", \"ought not hav \", text)\n    text = re.sub(r\"shan't\", \"shall not \", text)\n    text = re.sub(r\"sha'n't\", \"shall not \", text)\n    text = re.sub(r\"shan't've\", \"shall not have \", text)\n    text = re.sub(r\"she'd\", \"she would \", text)\n    text = re.sub(r\"she'd've\", \"she would have \", text)\n    text = re.sub(r\"she'll\", \"she will \", text)\n    text = re.sub(r\"she'll've\", \"she will have \", text)\n    text = re.sub(r\"she's\", \"she is \", text)\n    text = re.sub(r\"should've\", \"should have \", text)\n    text = re.sub(r\"shouldn't\", \"should not \", text)\n    text = re.sub(r\"shouldn't've\", \"should not have \", text)\n    text = re.sub(r\"so've\", \"so have \", text)\n    text = re.sub(r\"so have\", \"so as \", text)\n    text = re.sub(r\"that'd\", \"that would \", text)\n    text = re.sub(r\"that'd've\", \"that would have \", text)\n    text = re.sub(r\"that's\", \"that is \", text)\n    text = re.sub(r\"there'd\", \"there would \", text)\n    text = re.sub(r\"there'd've\", \"there would have \", text)\n    text = re.sub(r\"there's\", \"there is \", text)\n    text = re.sub(r\"they'd\", \"they would \", text)\n    text = re.sub(r\"they'd've\", \"they would have \", text)\n    text = re.sub(r\"they'll\", \"they will \", text)\n    text = re.sub(r\"they'll've\", \"they will have \", text)\n    text = re.sub(r\"they're\", \"they are \", text)\n    text = re.sub(r\"they've\", \"they have \", text)\n    text = re.sub(r\"to've\", \"to have \", text)\n    text = re.sub(r\"wasn't\", \"was not \", text)\n    text = re.sub(r\"we'd\", \"we would \", text)\n    text = re.sub(r\"we'd've\", \"we would have \", text)\n    text = re.sub(r\"we'll\", \"we will \", text)\n    text = re.sub(r\"we'll've\", \"we will have \", text)\n    text = re.sub(r\"we're\", \"we are \", text)\n    text = re.sub(r\"we've\", \"we have \", text)\n    text = re.sub(r\"weren't\", \"were not \", text)\n    text = re.sub(r\"what'll\", \"what will \", text)\n    text = re.sub(r\"what'll've\", \"what will have \", text)\n    text = re.sub(r\"what're\", \"what are \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"when's\", \"when is \", text)\n    text = re.sub(r\"when've\", \"what have \", text)\n    text = re.sub(r\"where'd\", \"where did \", text)\n    text = re.sub(r\"where's\", \"where is \", text)\n    text = re.sub(r\"where've\", \"where have \", text)\n    text = re.sub(r\"who'll\", \"who will \", text)\n    text = re.sub(r\"who'll've\", \"who will have \", text)\n    text = re.sub(r\"who's\", \"who is \", text)\n    text = re.sub(r\"who've\", \"who have \", text)\n    text = re.sub(r\"why's\", \"why is \", text)\n    text = re.sub(r\"why've\", \"why have \", text)\n    text = re.sub(r\"will've\", \"will have \", text)\n    text = re.sub(r\"won't\", \"will not \", text)\n    text = re.sub(r\"won't've\", \"will not have \", text)\n    text = re.sub(r\"would've\", \"would have \", text)\n    text = re.sub(r\"wouldn't\", \"would not \", text)\n    text = re.sub(r\"wouldn't've\", \"would not have \", text)\n    text = re.sub(r\"y'all\", \"you all \", text)\n    text = re.sub(r\"y'all'd\", \"you all would \", text)\n    text = re.sub(r\"y'all'd've\", \"you all would have \", text)\n    text = re.sub(r\"y'all're\", \"you all are \", text)\n    text = re.sub(r\"y'all've\", \"you all have \", text)\n    text = re.sub(r\"you'd\", \"you would \", text)\n    text = re.sub(r\"you'd've\", \"you would have \", text)\n    text = re.sub(r\"you'll\", \"you will \", text)\n    text = re.sub(r\"you'll've\", \"you will have \", text)\n    text = re.sub(r\"you're\", \"you are \", text)\n    text = re.sub(r\"you've\", \"you have \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\'\\n\", \" \", text)\n    text = re.sub(r\"\\'\\xa0\", \" \", text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strip_list_noempty(mylist):\n    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n    return [item for item in newlist if item != '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_punct(text): \n    words=tokenizer.tokenize(text)\n    punctuation_filtered = []\n    regex = re.compile('[%s]' % re.escape(punct))\n    remove_punctuation = str.maketrans(' ', ' ', punct)\n    for w in words:\n        if w in tags_features:\n            punctuation_filtered.append(w)\n        else:\n            punctuation_filtered.append(regex.sub('', w))\n  \n    filtered_list = strip_list_noempty(punctuation_filtered)\n        \n    return ' '.join(map(str, filtered_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_stopwords(text):\n    stop_words = set(stopword_list)\n    words = tokenizer.tokenize(text)\n    filtered = [w for w in words if not w in stop_words]\n    return ' '.join(map(str, filtered))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_corpus(corpus):\n\n    normalized_corpus = []\n    \n    # normalize each document in the corpus\n    for doc in corpus:\n        # strip HTML\n        doc = strip_html_tags(doc)\n        \n        # remove accented characters\n        doc = remove_accented_chars(doc)\n        \n        # lowercase the text\n        doc = doc.lower()\n            \n        # expand contraction\n        doc = expand_contractions(doc)\n        \n        #clean punctuations\n        doc = clean_punct(doc)\n    \n        # remove stopwords\n        doc = remove_stopwords(doc)\n\n        # lemmatize text\n        doc = lemmatize_text(doc)\n        \n        # remove special characters \n            \n        # insert spaces between special characters to isolate them\n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n        doc = remove_special_characters(doc)\n            \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        \n        normalized_corpus.append(doc)\n\n    return normalized_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques['norm_title'] = normalize_corpus(merged_ques['Title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_ques['norm_body'] = normalize_corpus(merged_ques['Body'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_topics = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = merged_ques['norm_body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_train = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n                                       max_features=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TF_IDF_matrix = vectorizer_train.fit_transform(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50,random_state=11).fit(TF_IDF_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"--------------------------------------------\")\n        print(\"Topic %d:\" % (topic_idx))\n        print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n        print(\"--------------------------------------------\")\n        \n\nno_top_words = 10\ndisplay_topics(lda, vectorizer_train.get_feature_names(), no_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = merged_ques['norm_body']\nX2 = merged_ques['norm_title']\ny = merged_ques['Tags']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmultilabel_binarizer = MultiLabelBinarizer()\ny_bin = multilabel_binarizer.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\",\n                                       max_features=1000)\n\nvectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\",\n                                       max_features=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX1_tfidf = vectorizer_X1.fit_transform(X1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX2_tfidf = vectorizer_X2.fit_transform(X2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\nX_tfidf = hstack([X1_tfidf,X2_tfidf])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X1', X1.shape)\nprint('X2', X2.shape)\nprint('y_bin', y_bin.shape)\nprint('X_tfidf', X_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_jacard(y_true,y_pred):\n    '''\n    see https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n    '''\n    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n    \n    return jacard.mean()*100\n\ndef print_score(y_pred, clf):\n    print(\"Clf: \", clf.__class__.__name__)\n    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n    print(\"---\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = DummyClassifier()\nsgd = SGDClassifier()\nlr = LogisticRegression()\nmn = MultinomialNB()\nsvc = LinearSVC()\nperceptron = Perceptron()\npac = PassiveAggressiveClassifier()\n\nfor classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print_score(y_pred, classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlpc = MLPClassifier()\nmlpc.fit(X_train, y_train)\n\ny_pred = mlpc.predict(X_test)\n\nprint_score(y_pred, mlpc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\n\nprint_score(y_pred, rfc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'estimator__C':[1,10,100,1000]\n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = OneVsRestClassifier(LinearSVC())\nCV_svc = model_selection.GridSearchCV(estimator=svc, param_grid=param_grid, cv= 5, verbose=10, scoring=make_scorer(avg_jacard,greater_is_better=True))\nCV_svc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV_svc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = CV_svc.best_estimator_\nbest_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model.predict(X_test)\nprint_score(y_pred, best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(y_train.shape[1]):\n    print(multilabel_binarizer.classes_[i])\n    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_top10(feature_names, clf, class_labels):\n    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(\"--------------------------------------------\")\n        print(\"%s: %s\" % (class_label,\n              \" \".join(feature_names[j] for j in top10)))\n        print(\"--------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = vectorizer_X1.get_feature_names() + vectorizer_X2.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_top10(feature_names, best_model, multilabel_binarizer.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nvectorX1_pkl = open('vectorX1.pickle', 'ab')\nvectorX2_pkl = open('vectorX2.pickle', 'ab')\nmlc_pkl = open('mlc.pickle', 'ab')\n      \n# source, destination \npickle.dump(vectorizer_X1, vectorX1_pkl)\npickle.dump(vectorizer_X2, vectorX2_pkl)\npickle.dump(best_model, mlc_pkl)\n\nvectorX1_pkl.close()\nvectorX2_pkl.close()\nmlc_pkl.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x1_ = pd.Series(['android'])\n#x2_ = pd.Series(['activity'])\n\n#x1_tfidf = vectorizer_X1.transform(x1_)\n#x2_tfidf = vectorizer_X2.transform(x2_)\n#x_tfidf = hstack([x1_tfidf,x2_tfidf])\n\n#y__pred = best_model.predict(x_tfidf)\n#y__pred = [for i in y__pred[0]]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}