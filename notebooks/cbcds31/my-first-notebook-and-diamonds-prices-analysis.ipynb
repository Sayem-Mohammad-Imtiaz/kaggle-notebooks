{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's read the data from .csv first."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfilepath = '../input/diamonds/diamonds.csv'\ndiamond_data = pd.read_csv(filepath)\n# Drop the first column since it has indexes we don't need\ndiamond_data.drop(diamond_data.columns[0], axis=1, inplace=True)\n\nnum_cols = diamond_data.select_dtypes(exclude=['object']).columns\ncat_cols = np.setdiff1d(diamond_data.columns, num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunately, there are no NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in diamond_data.columns:\n    isnull = diamond_data.loc[diamond_data[feature].isnull()].shape[0]\n    print(feature + \" - \" + str(isnull))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond_data[num_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A plenty of entries have zero length, width or depth, it seems not so real."},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond_data.drop(diamond_data[diamond_data.x == 0].index, inplace=True)\ndiamond_data.drop(diamond_data[diamond_data.y == 0].index, inplace=True)\ndiamond_data.drop(diamond_data[diamond_data.z == 0].index, inplace=True)\ndiamond_data.reindex()\ndiamond_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond_data[cat_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a pretty big difference between prices..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot(diamond_data.price, kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there's no correspondence between depth and table features and price. I'll try to do some feature engineering to make a use of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nsns.scatterplot(x=diamond_data['carat'], y=diamond_data['price'], ax=ax1)\nsns.scatterplot(x=diamond_data['depth'], y=diamond_data['price'], ax=ax2)\nsns.scatterplot(x=diamond_data['table'], y=diamond_data['price'], ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot relationships between categorical data and price."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nsns.stripplot(x=diamond_data['clarity'], y=diamond_data['price'], ax=ax1)\nsns.stripplot(x=diamond_data['color'], y=diamond_data['price'], ax=ax2)\nsns.stripplot(x=diamond_data['cut'], y=diamond_data['price'], ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It took a plenty of time to figure out how to plot a heatmap from categorical data but it worth it âœŠ"},{"metadata":{"trusted":true},"cell_type":"code","source":"clarity_cut = diamond_data[['clarity', 'cut']]\nclarity_cut_concat = clarity_cut['clarity'].map(str) + '_' + clarity_cut['cut'].map(str)\ncl_cut_counts = clarity_cut.assign(concat=clarity_cut_concat).groupby(['clarity', 'cut']).concat.count()\n\nclarity_color = diamond_data[['clarity', 'color']]\nclarity_color_concat = clarity_color['clarity'].map(str) + '_' + clarity_color['color'].map(str)\ncl_col_counts = clarity_color.assign(concat=clarity_color_concat).groupby(['clarity', 'color']).concat.count()\n\ncolor_cut = diamond_data[['color', 'cut']]\ncolor_cut_concat = color_cut['color'].map(str) + '_' + color_cut['cut'].map(str)\ncol_cut_counts = color_cut.assign(concat=color_cut_concat).groupby(['color', 'cut']).concat.count()\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\nsns.heatmap(data=cl_cut_counts.unstack(), ax=ax1)\nsns.heatmap(data=cl_col_counts.unstack(), ax=ax2)\nsns.heatmap(data=col_cut_counts.unstack(), ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll do label encoding by hand because the order of features is important."},{"metadata":{"trusted":true},"cell_type":"code","source":"clarity_dict = {'FL': 11,'IF': 10, 'VVS1': 9, 'VVS2': 8, 'VS1': 7,\n                'VS2': 6, 'SI1': 5, 'SI2': 4, 'I1': 3, 'I2': 2, 'I3': 1}\ncolor_dict = {'D': 7, 'E': 6, 'F': 5, 'G': 4, 'H': 3, 'I': 2, 'J': 1}\ncut_dict = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n\ndiamond_data['clarity'] = diamond_data['clarity'].map(clarity_dict)\ndiamond_data['cut'] = diamond_data['cut'].map(cut_dict)\ndiamond_data['color'] = diamond_data['color'].map(color_dict)\ndiamond_data[['clarity', 'cut', 'color']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to add some generic numerical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond_data['xyz'] = diamond_data['x'] * diamond_data['y'] * diamond_data['z']\nxyz_carat = diamond_data['xyz'] * diamond_data['carat']\ndiamond_data['xyz_carat'] = xyz_carat.map(np.log)\n\ndepth_table = diamond_data['depth'] * diamond_data['table']\ndiamond_data['depth_table'] = depth_table.map(np.log)\n\ncarat_depth = diamond_data['depth'] * diamond_data['carat']\ndiamond_data['carat_depth'] = carat_depth.map(np.log)\n\ncarat_table = diamond_data['carat'] * diamond_data['table']\ndiamond_data['carat_table'] = carat_table.map(np.log)\n\ndiamond_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like there's a relationship between new features and diamond price but plots look kinda same so I guess I'll drop some of these features laterðŸ¤”"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nsns.scatterplot(x=diamond_data['xyz_carat'], y=diamond_data['price'], ax=ax1)\nsns.scatterplot(x=diamond_data['carat_table'], y=diamond_data['price'], ax=ax2)\nsns.scatterplot(x=diamond_data['carat_depth'], y=diamond_data['price'], ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll combine categorical features, later it will be more clear which ones are useful. It's better to fit LaberEncoder only on training data, but I don't know does it matter in my case, to be fair ðŸ˜ƒ"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nfrom sklearn.preprocessing import LabelEncoder\n\ninteractions = pd.DataFrame(index=diamond_data.index)\nfor col1, col2 in itertools.combinations(cat_cols, 2):\n    col_name = col1 + '_' + col2\n    interaction = diamond_data[col1].map(str) + '_' + diamond_data[col2].map(str)\n    encoder = LabelEncoder()\n    interactions[col_name] = encoder.fit_transform(interaction)\n    \ndiamond_data = diamond_data.join(interactions)\ndiamond_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split data into train, validation and test datasets."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def train_valid_test_split(data, train_percent, valid_percent):\n    np.random.seed(6)\n    perm = np.random.permutation(diamond_data.index)\n    n = len(data.index)\n    train_end = int(train_percent * n)\n    valid_end = int(valid_percent * n) + train_end\n    train = data.loc[perm[:train_end]]\n    valid = data.loc[perm[train_end:valid_end]]\n    test = data.loc[perm[valid_end:]]\n    return train, valid, test\n\ntrain, valid, test = train_valid_test_split(diamond_data, 0.7, 0.2)\ntrain_X = train.drop('price', axis=1)\ntrain_y = train.price\nvalid_X = valid.drop('price', axis=1)\nvalid_y = valid.price\ntest_X = test.drop('price', axis=1)\ntest_y = test.price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll try running different models with default parameters to see which one performs best. Then I'll do some hyperparameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\ndef evaluate_model(model, train_X, train_y, valid_X, valid_y):\n    model.fit(train_X, train_y)\n    predictions = model.predict(valid_X)\n    return mean_absolute_error(valid_y, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nmodels = [('DecisionTreeRegressor', DecisionTreeRegressor()),\n          ('RandomForestRegressor', RandomForestRegressor()),\n          ('LinearRegression', LinearRegression()),\n          ('XGBRegressor', XGBRegressor())]\n\nfor model_name, model in models:\n    print('mae for ' + model_name + \": \", end='')\n    print(evaluate_model(model, train_X, train_y, valid_X, valid_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try selecting useful features to improve model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\nparams = [6, 8, 10, 12, 14, 15, 16, 17]\nfor k in params:\n    selector = SelectKBest(f_regression, k=k)\n    X_new = selector.fit_transform(train_X, train_y)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new),\n                                     index=train_X.index, columns=train_X.columns)\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    print('mae for k = {}: '.format(k), end='')\n    print(evaluate_model(model, train_X[selected_columns], train_y, valid_X[selected_columns], valid_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll leave 15 features because mae was a little bit lower with this parameter. I have a feeling that I can improve results by creating more generic features but at the moment I don't have much experience to figure out how to get more of them and which ones will be better."},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = SelectKBest(f_regression, k=15)\nX_new = selector.fit_transform(train_X, train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new),\n                                     index=train_X.index, columns=train_X.columns)\nselected_columns = selected_features.columns[selected_features.var() != 0]\n    \ntrain_X = train_X[selected_columns]\nvalid_X = valid_X[selected_columns]\ntest_X = test_X[selected_columns]\nselected_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to tune the regressor itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = range(5, 96, 15)\n\nfor num in n_estimators:\n    print('mae for n_estimators = {}: '.format(num), end='')\n    print(evaluate_model(RandomForestRegressor(n_estimators=num),train_X, train_y, valid_X, valid_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\\*I tried changing params such as min_samples_split and min_samples_leaf but mae was higher\\*"},{"metadata":{},"cell_type":"markdown","source":"Uh, my pulse is raising... Final testing on test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('mae on test data: ')\nprint(evaluate_model(RandomForestRegressor(n_estimators=50),train_X, train_y, test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yep, it could be better but could be worse either ðŸ¤£"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}