{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_pickle(\"/kaggle/input/handling-imbalanced-data-eda-small-fe/df_for_use.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9,9))\ncorr = df.corr()\nsns.heatmap(corr, cmap='RdBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM Without Outlier Elimination","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\n\nX_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### LightGBM without Outlier Elimination\n\n\nstart = time.time()\n\nlgbm_clf = LGBMClassifier(n_estimators = 3000, random_state = 2020)\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = 'auc' , eval_set = evals, verbose = 50)\nlgbm_cpu_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nlgbm_cpu_runtime = time.time() - start\n\nprint( 'LightGBM_cpu_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(lgbm_cpu_roc_score ,lgbm_cpu_runtime ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM (Apply Outlier Elimination)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_outlier(df= None, column = None, weight = 5.0):\n    #Extract column data with Bad Loan only, get 1/4 percentile and 3/4 percentile through np.percentile\n    \n    bad_loan = df[df['loan_condition_cat']==1][column]\n    quantile_25 = np.percentile(bad_loan.values,25)\n    quantile_75 = np.percentile(bad_loan.values,75)\n    \n    #calculate IQR, multiply with 3, get min,max value\n    \n    iqr = quantile_75  - quantile_25\n    iqr_weight = iqr*weight\n    lowest_val = quantile_25 - iqr_weight\n    highest_val = quantile_25 + iqr_weight\n    \n    #fix outlier which is bigger than max, smaller than min\n    \n    outlier_index = bad_loan[(bad_loan < lowest_val) | (bad_loan > highest_val)].index\n    return outlier_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_index = get_outlier (df = df , column = 'recoveries', weight = 5.0)\nprint ( \"Outlier index :\", outlier_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preprocessed_df(df=None):\n    df_copy = df.copy()\n    amount_n = np.log1p(df['loan_amount'])\n    df_copy.insert(0, 'Amount_Scaled', amount_n)\n    df_copy.drop(['loan_amount'], axis=1, inplace=True)\n    # 이상치 데이터 삭제하는 로직 추가\n    outlier_index = get_outlier(df=df_copy, column='recoveries', weight=5.0)\n    df_copy.drop(outlier_index, axis=0, inplace=True)\n    return df_copy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = get_preprocessed_df(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_copy.drop('loan_condition_cat', axis=1)\ny = df_copy['loan_condition_cat']\n\n\nX_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstart = time.time()\n\nlgbm_clf = LGBMClassifier(n_estimators = 3000, random_state = 2020)\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = 'auc' , eval_set = evals, verbose = 50)\nlgbm_outlier_eliminated_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nlgbm_outlier_eliminated_runtime = time.time() - start\n\nprint( 'LightGBM_outlier_eliminated_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(lgbm_outlier_eliminated_roc_score ,lgbm_outlier_eliminated_runtime ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( 'LightGBM_cpu_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(lgbm_cpu_roc_score ,lgbm_cpu_runtime ))\nprint( 'LightGBM_outlier_eliminated_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(lgbm_outlier_eliminated_roc_score ,lgbm_outlier_eliminated_runtime ))\n\n\n### Negative Effect on Model\n#### Opinion : Since DATA is not so skewed, so many columns are designated as outlier and removed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}