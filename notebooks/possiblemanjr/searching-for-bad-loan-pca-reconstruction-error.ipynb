{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score \nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report \nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport shap\nimport joblib\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold , cross_val_score\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_pickle('../input/searching-for-bad-loan-data-preprocessing/df_pp.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Loan_status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['Loan_status'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Loan_status')\nax[0].set_ylabel('')\nsns.countplot('Loan_status',data=df,ax=ax[1])\nax[1].set_title('Loan_status')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fastcluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Libraries\n\n\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n\nfrom sklearn import preprocessing as pp\n#from sklearn import impute.SimpleImputer as pp\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport fastcluster\nfrom scipy.cluster.hierarchy import dendrogram, cophenet, fcluster\nfrom scipy.spatial.distance import pdist\nimport os, sys, gc, warnings, random, datetime\nwarnings.filterwarnings('ignore')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # for min_max scaling\n# from mlxtend.preprocessing import minmax_scaling\n# df = minmax_scaling(df ,columns =df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing as pp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Loan_status', axis=1)\n# x_dd = X.copy()\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import MinMaxScaler\n# minMaxScaler = MinMaxScaler()\n# minMaxScaler.fit(X)\n# X_scaled = minMaxScaler.transform(X)\n\n# X= pd.DataFrame(X_scaled, columns=x_dd.columns)\n# X = reduce_mem_usage(X)\ny = df['Loan_status']\n\nfrom sklearn import preprocessing as pp\nfeaturesToScale = X.columns\nsX = pp.MinMaxScaler(copy=True)\nX.loc[:,featuresToScale] = sX.fit_transform(X[featuresToScale])\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def anomalyScores(originalDF, reducedDF):\n    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n    loss = pd.Series(data=loss,index=originalDF.index)\n    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotResults(trueLabels, anomalyScores, returnPreds = False):\n    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n    preds.columns = ['trueLabel', 'anomalyScore']\n    precision, recall, thresholds = \\\n        precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])\n    average_precision = \\\n        average_precision_score(preds['trueLabel'],preds['anomalyScore'])\n    \n    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    \n    plt.title('Precision-Recall curve: Average Precision = \\\n    {0:0.2f}'.format(average_precision))\n\n    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n                                     preds['anomalyScore'])\n    areaUnderROC = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic: \\\n    Area under the curve = {0:0.2f}'.format(areaUnderROC))\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    if returnPreds==True:\n        return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatterPlot(xDF, yDF, algoName):\n    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n               data=tempDF, fit_reg=False)\n    ax = plt.gca()\n    ax.set_title(\"Separation of Observations using \"+algoName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normal PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.decomposition import PCA\n\n\nn_components = range(20,32)\n\nfor i in n_components:\n    \n    print('Number of N_components : ', i)\n    whiten = False\n    random_state = 2020\n\n    pca = PCA(n_components=i, whiten=whiten, \\\n              random_state=random_state)\n\n    X_train_PCA = pca.fit_transform(X_train)\n    X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n\n    X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\n    X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n                                       index=X_train.index)\n    \n    anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\n    preds = plotResults(y_train, anomalyScoresPCA, True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"whiten = False\nrandom_state = 2020\n\npca = PCA(n_components=26, whiten=whiten, \\\n              random_state=random_state)\n\nX_train_PCA = pca.fit_transform(X_train)\nX_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n\nX_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\nX_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n                                       index=X_train.index)\nscatterPlot(X_train_PCA, y_train, \"PCA\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\npreds = plotResults(y_train, anomalyScoresPCA, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.sort_values(by=\"anomalyScore\",ascending=False,inplace=True)\ncutoff = 46467\npredsTop = preds[:cutoff]\nprint(\"Precision: \",np.round(predsTop. \\\n            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))\nprint(\"Recall: \",np.round(predsTop. \\\n            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))\nprint(\"Bad Loan Caught out of 46467 cases:\", predsTop.trueLabel.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sparse PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import SparsePCA\n\nn_components = 26\nalpha = 0.0001\nrandom_state = 2020\nn_jobs = -1\n\n# sparsePCA = SparsePCA(n_components=n_components, \\\n#                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n\nsparsePCA = SparsePCA(n_components=n_components, \\\n                alpha=alpha, random_state=random_state, n_jobs=n_jobs,normalize_components='deprecated')\n\nsparsePCA.fit(X_train.loc[:,:])\nX_train_sparsePCA = sparsePCA.transform(X_train)\nX_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=X_train.index)\n\nscatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sparsePCA_inverse = np.array(X_train_sparsePCA). \\\n    dot(sparsePCA.components_) + np.array(X_train.mean(axis=0))\nX_train_sparsePCA_inverse = \\\n    pd.DataFrame(data=X_train_sparsePCA_inverse, index=X_train.index)\n\nanomalyScoresSparsePCA = anomalyScores(X_train, X_train_sparsePCA_inverse)\npreds = plotResults(y_train, anomalyScoresSparsePCA, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 커널 PCA\nfrom sklearn.decomposition import KernelPCA\n\nn_components = 26\nkernel = 'rbf'\ngamma = None\nfit_inverse_transform = True\nrandom_state = 2020\nn_jobs = 1\n\nkernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n                gamma=gamma, fit_inverse_transform= \\\n                fit_inverse_transform, n_jobs=n_jobs, \\\n                random_state=random_state)\n\nkernelPCA.fit(X_train.iloc[:2000])\nX_train_kernelPCA = kernelPCA.transform(X_train)\nX_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA, \\\n                                 index=X_train.index)\n\nX_train_kernelPCA_inverse = kernelPCA.inverse_transform(X_train_kernelPCA)\nX_train_kernelPCA_inverse = pd.DataFrame(data=X_train_kernelPCA_inverse, \\\n                                         index=X_train.index)\n\nscatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalyScoresKernelPCA = anomalyScores(X_train, X_train_kernelPCA_inverse)\npreds = plotResults(y_train, anomalyScoresKernelPCA, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 가우시안 랜덤 투영\nfrom sklearn.random_projection import GaussianRandomProjection\n\nn_components = 26\neps = None\nrandom_state = 2020\n\nGRP = GaussianRandomProjection(n_components=n_components, \\\n                               eps=eps, random_state=random_state)\n\nX_train_GRP = GRP.fit_transform(X_train)\nX_train_GRP = pd.DataFrame(data=X_train_GRP, index=X_train.index)\n\nscatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_GRP_inverse = np.array(X_train_GRP).dot(GRP.components_)\nX_train_GRP_inverse = pd.DataFrame(data=X_train_GRP_inverse, \\\n                                   index=X_train.index)\n\nanomalyScoresGRP = anomalyScores(X_train, X_train_GRP_inverse)\npreds = plotResults(y_train, anomalyScoresGRP, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 희소 랜덤 투영\n\nfrom sklearn.random_projection import SparseRandomProjection\n\nn_components = 26\ndensity = 'auto'\neps = .01\ndense_output = True\nrandom_state = 2018\n\nSRP = SparseRandomProjection(n_components=n_components, \\\n        density=density, eps=eps, dense_output=dense_output, \\\n                                random_state=random_state)\n\nX_train_SRP = SRP.fit_transform(X_train)\nX_train_SRP = pd.DataFrame(data=X_train_SRP, index=X_train.index)\n\nscatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_SRP_inverse = np.array(X_train_SRP).dot(SRP.components_.todense())\nX_train_SRP_inverse = pd.DataFrame(data=X_train_SRP_inverse, index=X_train.index)\n\nanomalyScoresSRP = anomalyScores(X_train, X_train_SRP_inverse)\nplotResults(y_train, anomalyScoresSRP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 미니-배치 사전 학습\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\n\nn_components = 7\nalpha = 1\nbatch_size = 200\nn_iter = 10\nrandom_state = 2020\n\nminiBatchDictLearning = MiniBatchDictionaryLearning( \\\n    n_components=n_components, alpha=alpha, batch_size=batch_size, \\\n    n_iter=n_iter, random_state=random_state)\n\nminiBatchDictLearning.fit(X_train)\nX_train_miniBatchDictLearning = \\\n    miniBatchDictLearning.fit_transform(X_train)\nX_train_miniBatchDictLearning = \\\n    pd.DataFrame(data=X_train_miniBatchDictLearning, index=X_train.index)\n\nscatterPlot(X_train_miniBatchDictLearning, y_train, \\\n            \"Mini-batch Dictionary Learning\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_miniBatchDictLearning_inverse = \\\n    np.array(X_train_miniBatchDictLearning). \\\n    dot(miniBatchDictLearning.components_)\n\nX_train_miniBatchDictLearning_inverse = \\\n    pd.DataFrame(data=X_train_miniBatchDictLearning_inverse, \\\n                 index=X_train.index)\n\nanomalyScoresMiniBatchDictLearning = anomalyScores(X_train, \\\n    X_train_miniBatchDictLearning_inverse)\npreds = plotResults(y_train, anomalyScoresMiniBatchDictLearning, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 독립 성분 분석\n\nfrom sklearn.decomposition import FastICA\n\nn_components = 26\nalgorithm = 'parallel'\nwhiten = True\nmax_iter = 200\nrandom_state = 2020\n\nfastICA = FastICA(n_components=n_components, \\\n    algorithm=algorithm, whiten=whiten, max_iter=max_iter, \\\n    random_state=random_state)\n\nX_train_fastICA = fastICA.fit_transform(X_train)\nX_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=X_train.index)\n\nX_train_fastICA_inverse = fastICA.inverse_transform(X_train_fastICA)\nX_train_fastICA_inverse = pd.DataFrame(data=X_train_fastICA_inverse, \\\n                                       index=X_train.index)\n\nscatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalyScoresFastICA = anomalyScores(X_train, X_train_fastICA_inverse)\nplotResults(y_train, anomalyScoresFastICA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 테스트 셋에 PCA 적용\nX_test_PCA = pca.transform(X_test)\nX_test_PCA = pd.DataFrame(data=X_test_PCA, index=X_test.index)\n\nX_test_PCA_inverse = pca.inverse_transform(X_test_PCA)\nX_test_PCA_inverse = pd.DataFrame(data=X_test_PCA_inverse, \\\n                                  index=X_test.index)\n\nscatterPlot(X_test_PCA, y_test, \"PCA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalyScoresPCA = anomalyScores(X_test, X_test_PCA_inverse)\npreds = plotResults(y_test, anomalyScoresPCA, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 테스트 셋에 독립 성분 분석 적용\nX_test_fastICA = fastICA.transform(X_test)\nX_test_fastICA = pd.DataFrame(data=X_test_fastICA, index=X_test.index)\n\nX_test_fastICA_inverse = fastICA.inverse_transform(X_test_fastICA)\nX_test_fastICA_inverse = pd.DataFrame(data=X_test_fastICA_inverse, \\\n                                      index=X_test.index)\n\nscatterPlot(X_test_fastICA, y_test, \"Independent Component Analysis\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalyScoresFastICA = anomalyScores(X_test, X_test_fastICA_inverse)\nplotResults(y_test, anomalyScoresFastICA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n#K-Means Clustering with two clusters\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(X_train)\n\nclusters = kmeans.predict(X_train)\n\n#'cluster_df' will be used as a DataFrame\n#to assist in the visualization\ncluster_df = pd.DataFrame()\n\ncluster_df['cluster'] = clusters\ncluster_df['class'] = y\n\nsns.factorplot(col='cluster', y=None, x='class', data=cluster_df, kind='count', order=[1,0], palette=([\"#7d069b\",\"#069b15\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}