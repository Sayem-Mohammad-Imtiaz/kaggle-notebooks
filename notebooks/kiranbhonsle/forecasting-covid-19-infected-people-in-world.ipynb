{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2>Forecasting The Number Of People Infected With Coronavirus in the World</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Importing necessary libraries</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.ar_model import AR\nfrom datetime import datetime\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport math\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom statsmodels.tsa.seasonal import seasonal_decompose","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf = pd.read_csv('../input/novel-corona-virus-2019-dataset/time_series_covid_19_confirmed.csv')\ncor_inf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Data Preprocessing for the covid-19 dataset</h3>"},{"metadata":{},"cell_type":"markdown","source":"<p>The valuable features what we are hunting for is <li>Country names.</li><li>Count of\n    infected people</li><li> Dates</li> </br>At which they were affected in the country.\n    As we are not focused on predicting on indiviual country as it might be \n    bias on some country as a result the prediction might be very large or too small\n    which may be act as outlier.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop lat long and province/state columns\ncor_inf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p> Drop the <b>Province/State</b>,<b> Latitude</b> and <b> Longitude</b> columns  as they make the data to narrow and data for those column might be missing for some countries </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf.drop(labels = ['Province/State','Lat', 'Long'],axis = 1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Reshaping the data to proceed with Forecasting</h3>"},{"metadata":{},"cell_type":"markdown","source":"<p>Now we want to reshape the data as per the requirement i.e make index as dates,and column\nname as country names\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cor_inf['Country/Region'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>To get the total infected people per day in the world we sum the count of infected people from all the countries and group them as per dates</p>\n<p>But before we saw that in the country/Region column we have duplicated data as a result\nwe need to first sum all the country and then get the count of all the people infected </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#group by data based upon country since the countries name are repeated more than once \ncor_inf = cor_inf.groupby(['Country/Region']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf.loc['China'].tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshape the data as per the time series analysis\ncor_inf_re = pd.DataFrame()\nfor i in range(0,len(cor_inf)):\n    cor_inf_re[cor_inf.index[i]] = cor_inf.iloc[i].values\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(cor_inf.index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf_re.index = cor_inf.columns[:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf_re.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def total_infected_sum():\n    count = []\n    for i in range(0,len(cor_inf_re)):\n        count.append(sum(cor_inf_re.iloc[i].values))\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf_re['Total infected'] = total_infected_sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf_re.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Now only we need to convert index datatype i.e object to datetime </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def parser(date):\n    date = datetime.strptime(date,'%m/%d/%y')\n    date  = str(date.day) + '-' + str(date.month) + '-' + str(date.year)\n    print(date)\n    return datetime.strptime(date,'%d-%m-%Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert str to datetime in index \ntimestamp = []\nfor i in range(0,len(cor_inf_re)):\n    timestamp.append(parser(cor_inf_re.index[i]))\ncor_inf_re.index = timestamp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Sum all the count of infected people of each country to get the total infected people per date\n</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_inf_re.to_csv('./covid_19_confirmed.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparing for time series\ninfected_people = cor_inf_re['Total infected']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#column for infected per day\ndiff = []\ndiff.append(cor_inf_re['Total infected'][0])\nfor i in range(0,len(cor_inf_re['Total infected']) - 1):\n    diff.append(cor_inf_re['Total infected'][i+1] - cor_inf_re['Total infected'][i])\n\ncor_inf_re['Infected_per_Day'] = diff","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Visualization of dataset</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualization\nplt.xlabel('dates')\nplt.ylabel('infected people')\ninfected_people.plot(figsize = (11,5),marker='o')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the statistical part of the data\ninfected_people.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to check if there is an trend or seasonality\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(infected_people)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.trend.plot(figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.seasonal.plot(figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#autocorrelation graph\nplot_acf(infected_people)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pacf(infected_people)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"infec_one = infected_people.diff(periods=1)\ninfec_one = infec_one[1:]\nplot_acf(infec_one)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = infected_people.iloc[:-8]\ntest = infected_people.iloc[-8:]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"<h2>Exponential Smoothing</h2>"},{"metadata":{},"cell_type":"markdown","source":"Single, Double and Triple Exponential Smoothing can be implemented in Python using the ExponentialSmoothing Statsmodels class.\n\nFirst, an instance of the ExponentialSmoothing class must be instantiated, specifying both the training data and some configuration for the model.\n\nSpecifically, you must specify the following configuration parameters:\n\n<li>trend: The type of trend component, as either “add” for additive or “mul” for multiplicative. Modeling the trend can be disabled by setting it to None.</li>\n<li>damped: Whether or not the trend component should be damped, either True or False.</li>\n<li>seasonal: The type of seasonal component, as either “add” for additive or “mul” for multiplicative. Modeling the seasonal component can be disabled by setting it to None.</li>\n<li>seasonal_periods: The number of time steps in a seasonal period, e.g. 12 for 12 months in a yearly seasonal structure (more here).\n</li>\n\nThe model can then be fit on the training data by calling the fit() function.\n\nThis function allows you to either specify the smoothing coefficients of the exponential smoothing model or have them optimized. By default, they are optimized (e.g. optimized=True). These coefficients include:\n\nsmoothing_level (alpha): the smoothing coefficient for the level.\nsmoothing_slope (beta): the smoothing coefficient for the trend.\nsmoothing_seasonal (gamma): the smoothing coefficient for the seasonal component.\ndamping_slope (phi): the coefficient for the damped trend."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExponentialSmoothing(train,trend = \"mul\",seasonal_periods=7,seasonal=\"add\").fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(start = 50 ,end= 57 )\n#predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,4))\npredictions.plot(c ='r',marker = 'o',markersize=10,linestyle='--')\ntest.plot(marker = 'o',markersize=10,linestyle='--')\nprint(\"root mean squared error : \",math.sqrt(mean_squared_error(test,predictions)))\nprint(\"mean absolute error : \",mean_absolute_error(test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>SARIMAX model</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SARIMAX(train,order = (4,2,1),trend='t',seasonal_order=(2, 2, 1, 14))\nmodel_fit = model.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model_fit.predict(start = 53,end=60)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,4))\nplt.plot(predictions,'r',marker = 'o',markersize=10,linestyle='--')\nplt.plot(test,marker = 'o',markersize=10,linestyle='--')\nprint(\"root mean squared error : \",math.sqrt(mean_squared_error(test,predictions)))\nprint(\"mean absolute error : \",mean_absolute_error(test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>As a result I go over a sarima model as i seen it perform better although the prediction\nas per the real data was really close and it didn't ovefit the model which in the case of exponenetial smoothning\n</p>"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"predictions = model_fit.predict(start = 50,end=67)\npredictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p> as per the result it might be that after the end of march, it will affect to 450000\npeople all over the world so precaution and prevention is a first priority of evry human\nbeing</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":4}