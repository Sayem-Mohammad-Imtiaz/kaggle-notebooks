{"cells":[{"metadata":{"_cell_guid":"f9e8d76b-48f8-a70f-c5d1-7ebca7b410bc","_uuid":"0ae3589fb8ddd8221b47b059b18c5c7cb607bf36"},"cell_type":"markdown","source":"# INTRODUCTION\n\nIn a nutshell, I will be using a Principal Component Analysis (PCA) based approach to analysing the TMDB dataset and then implementing some KMeans clustering to provide visualisations of any related clusters I find in the dataset. This notebook will purely be an exploratory and hopefully concise enough attempt to explain the idea of PCA as well as using a clustering method (KMeans) to extract meaningful relations out of it. "},{"metadata":{"_cell_guid":"cd3102d3-e469-d89e-9242-91dc661c6da1","_uuid":"77ea142b72ce6e64113b2a29a2f84ed8c3607e34","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA # Principal Component Analysis module\nfrom sklearn.cluster import KMeans # KMeans clustering \nimport matplotlib.pyplot as plt # Python defacto plotting library\nimport seaborn as sns # More snazzy plotting library\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"078167f9-c9e1-5342-41cb-1c48ea0c6683","_uuid":"ade4283468172a587856df8f58a7ffd7f9004b1e"},"cell_type":"markdown","source":"Let's import the movie dataset with imagination. The dataset will be called `tmdb_movie` and let's inspect the first 5 rows of the dataframe with .head()"},{"metadata":{"_cell_guid":"546ba765-b576-0cf5-599e-fcd2904f67ca","_uuid":"4dfca8e15bb81548cb9f2fa0a1c5a9f3e69c677e","trusted":true},"cell_type":"code","source":"tmdb_movie = pd.read_csv('../input/tmdb_5000_movies.csv')\ntmdb_credits = pd.read_csv('../input/tmdb_5000_credits.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e205aad347171fce1fb1200045305ee8850206c"},"cell_type":"code","source":"tmdb_movie.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bc979d954019a42e1fcf62444ab29e18e6203a8"},"cell_type":"code","source":"tmdb_movie.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97757a5c-5ca0-81a1-3b33-5770a5020dde","_uuid":"b9f821f09fc871b44e9b2c07af9d51489648ea77"},"cell_type":"markdown","source":"# 1. DATA FILTERING AND CLEANSING\n**Filtering for Numerical values only**\n\nAs observed from the dataframe above, some columns contain numbers while others, words. Let's do some filtering to extract only the numbered columns and not the ones with words."},{"metadata":{"_cell_guid":"0657540e-6bcf-1478-d7d0-7b0c45319e72","_uuid":"e19d8aff4bf4b053abd149ff8c28ef68ce54030e","trusted":true},"cell_type":"code","source":"str_list = [] # empty list to contain columns with strings (words)\nfor colname, colvalue in tmdb_movie.iteritems():\n    if type(colvalue[1]) == str:\n         str_list.append(colname)\n# Get to the numeric columns by inversion            \nnum_list = tmdb_movie.columns.difference(str_list)         ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76b83f87-fad7-3705-2d93-a70c3e720471","_uuid":"12b9653f436da02087098f4b9ba60f470759f032"},"cell_type":"markdown","source":"Now create a new dataframe (movie_num) containing just the numbers as such : "},{"metadata":{"_cell_guid":"1de96952-a2f1-3683-762e-796204e051e5","_uuid":"187bc52a290bc2a68f97788953669beee65198fb","trusted":true},"cell_type":"code","source":"movie_num = tmdb_movie[num_list]\n#del movie # Get rid of movie df as we won't need it now\nmovie_num.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d350b89e6923daa325eed57bdb69c590fe915cc"},"cell_type":"code","source":"movie_num.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d78ec8c9-a805-415e-e60a-5a29a7c5b173","_uuid":"30f5719655dde9320eb66a48aba2c902666e3917"},"cell_type":"markdown","source":"**Removal of Null values**"},{"metadata":{"_cell_guid":"09af4f73-2186-4065-c3b4-6692f58af427","_uuid":"479007fdcfeeb364cc3ad54c4cd8fe33500797dc"},"cell_type":"markdown","source":"In here, I will just do the naive thing of replacing these NaNs with zeros as such:"},{"metadata":{"_cell_guid":"28b274dc-8837-2930-3997-41b255c0eaef","_uuid":"7b5040a527256fb646b92e0c7cd52d6b040d3e75","trusted":true},"cell_type":"code","source":"movie_num = movie_num.fillna(value=0, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"90e67a0e-44ef-3c86-842e-f058066fc554","_uuid":"3bcb1aaa9d27e38254bb97dc89d0cd4bc20717c3"},"cell_type":"markdown","source":"**Standardisation** \n\nFinally we mentioned that we have to find some sort of way to standardise the data and for this, we use sklearn's StandardScaler."},{"metadata":{"_cell_guid":"f6ca79a8-ae86-fbde-4aac-88d1f01522c8","_uuid":"c69374294731f94d222b6cc532a39038473b9e10","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = movie_num.values\n# Data Normalization\nX_std = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba3faf11-dde4-44e0-7a05-5186e6443a15","_uuid":"f28478f26c19054845ec5971560efc2cd4445176"},"cell_type":"markdown","source":"Let's look at some hexbin visualisations first to get a feel for how the correlations between the different features compare to one another. In the hexbin plots, the lighter in color the hexagonal pixels, the more correlated one feature is to another."},{"metadata":{"_cell_guid":"6261a0c0-7ca5-f9ce-7dab-ef0ae9c4c7ac","_uuid":"986bfb8952cac020157d3a960f04c42e1b71e0d5","trusted":true},"cell_type":"code","source":"tmdb_movie.plot(y= 'vote_average', x ='runtime',kind='hexbin',gridsize=35, sharex=False, colormap='cubehelix', title='Hexbin of vote_average and runtime',figsize=(12,8))\ntmdb_movie.plot(y= 'vote_average', x ='revenue',kind='hexbin',gridsize=45, sharex=False, colormap='cubehelix', title='Hexbin of vote_average and revenue',figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"78c22e8b-e484-ce35-a7ef-5fb0a68540e4","_uuid":"78d11b2a8fb57461384509d1210b6e415e552290"},"cell_type":"markdown","source":"Anyway now - time for the customary heatmap per the tradition of most notebooks on Principal Component Analysis. The heatmap is generated to visually show how strongly correlated the values of the dataframe's columns are to one another. "},{"metadata":{"_cell_guid":"b56dc8a1-3de8-ce99-e1f9-ffe2576c45c4","_uuid":"fd93b0e4a857ae2e400e9df727f1523875bc1dad","trusted":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\nplt.title('Pearson Correlation of Movie Features')\n# Draw the heatmap using seaborn\nsns.heatmap(movie_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db2c9e61-5234-fa73-3493-24714456b474","_uuid":"c5fa4e97067ab515c165569f6fbe1070d411a219"},"cell_type":"markdown","source":"As we can see from the heatmap, there are regions (features) where we can see quite positive linear correlations amongst each other, given the darker shade of the colours - top left-hand corner and bottom right quarter. This is a good sign as it means we may be able to find linearly correlated features for which we can perform PCA projections on."},{"metadata":{"_cell_guid":"8d36d73d-2998-30b6-2140-ab5347855bc7","_uuid":"c7b3b46211af8b8b9557cc2ef754eb25182bc7fb"},"cell_type":"markdown","source":"# 2. EXPLAINED VARIANCE MEASURE"},{"metadata":{"_cell_guid":"045a6d1d-9b80-da80-361b-b2269e4917eb","_uuid":"b57e744b00e07abd56f033de5082ea7c0dc85103","trusted":true},"cell_type":"code","source":"# Calculating Eigenvectors and eigenvalues of Cov matirx\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f711388a64a1e301f9c56851461d020411a1b841"},"cell_type":"code","source":"mean_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b59d356c58c84644ca7389aa6c8af198b51a0b64"},"cell_type":"code","source":"cov_mat","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d16f75f-6e57-7d08-e617-4ff7ca883496","_uuid":"83201e8bbceecefb55a93d7e6b3c6f19beca5849"},"cell_type":"markdown","source":"Now having obtained the eigenvalues and eigenvectors, we will group them together by creating a list of eigenvalue, eigenvector tuples. Following on from this we will sort the list  in order of Highest eigenvalue to lowest eigenvalue and then use the eigenvalues to calculate both the individual explained variance and the cumulative explained variance for visualisation."},{"metadata":{"_cell_guid":"d64d8e0a-4dc6-4648-4380-9413cfdf6af7","_uuid":"bc0b7a92a3226930678e2fb26d5a0237376daec2","trusted":true},"cell_type":"code","source":"# Create a list of (eigenvalue, eigenvector) tuples\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort from high to low\neig_pairs.sort(key = lambda x: x[0], reverse= True)\n\n# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\ncum_var_exp = np.cumsum(var_exp) # Cumulative explained variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"320f6e250958b18fb52813d480fb6100328a4a4b"},"cell_type":"code","source":"cum_var_exp","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cdd5b6e-e08e-5048-d40b-6e697dc40326","_uuid":"da6ad23aa1a0163e8ebff60f7a469fb41d2dc0cd"},"cell_type":"markdown","source":"Now time to plot the explained variance graphs to see how our contributions look like. The cumulative explained variance is visualised in a blue step-plot while the individual explained variance is plotted via green bar charts as follows: "},{"metadata":{"_cell_guid":"192d6a57-09a6-167c-4afe-670d3603ffad","_uuid":"bc7b022a5a4b065274d4fbb8bad4b3083d378125","trusted":true},"cell_type":"code","source":"# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \nplt.figure(figsize=(10, 5))\nplt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\nplt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecf9189fa6fbf5b6127f3a39f1dd21927b017c27"},"cell_type":"code","source":"movie_num.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b18d6a4-a85b-58e8-76ab-68aac4189fac","_uuid":"9df4ae25c1feada6e415a13d616cd9bf816ecd0d"},"cell_type":"markdown","source":"# 3. PRINCIPAL COMPONENT ANALYSIS \nHaving roughly identified how many components/dimensions we would like to project on, let's now implement sklearn's PCA module. \n\nThe first line of the code contains the parameters \"n_components\" which states how many PCA components we want to project the dataset onto. Since we are going implement PCA with 7 components, therefore we set n_components = 7.  \n\nThe second line of the code calls the \"fit_transform\" method, which fits the PCA model with the standardised movie data X_std and applies the dimensionality reduction on this dataset. "},{"metadata":{"_cell_guid":"1dab4fa0-96a7-b5ac-1571-0a516ff6288b","_uuid":"c8169d430c4ba47647058d51cbccf2af80c5981b","trusted":true},"cell_type":"code","source":"pca = PCA(n_components=7)\nx_9d = pca.fit_transform(X_std)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2e21012-2f37-4ad7-5299-178a9f887dee","_uuid":"b7c92c6e0801eeabc950a3d4db954b524cc9ed49"},"cell_type":"markdown","source":"Awesome. Having now applied our specific PCA model with the movie dataset, let's visualise the first 2 projection components as a 2D scatter plot to see if we can get a quick feel for the underlying data. "},{"metadata":{"_cell_guid":"4d30a956-ccbf-ddfd-df0e-f1234b6c98f0","_uuid":"57765809549b0ef4816fbaa85a9eb19695113c81","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9,7))\nplt.scatter(x_9d[:,0],x_9d[:,1], c='goldenrod',alpha=0.5)\nplt.ylim(-10,30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b97cf478-6cb5-dee9-a742-91b3c011812e","_uuid":"12ecb0368da8a2bcdeb220a832ac44c384b7a0a5"},"cell_type":"markdown","source":" # 4. VISUALISATIONS WITH KMEANS CLUSTERING\nA simple KMeans will now be applied to the PCA projection data. Each cluster will be visualised with a different colour so hopefully we will be able to pick out clusters by eye. \n\nTo start off, we set up a KMeans clustering with sklearn's KMeans() and call the \"fit_predict\" method to compute cluster centers and predict cluster indices for the first and third PCA projections (to see if we can observe any appreciable clusters). We then define our own colour scheme and plot the scatter diagram as follows:"},{"metadata":{"_cell_guid":"4ff3c5bc-0c2f-3040-4c15-c25e848e68a7","_uuid":"6f24d4481e12b7f1720d6da16bbdd8ffbc44aaa0","trusted":true},"cell_type":"code","source":"# Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters=3)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_9d)\n\n# Define our own color map\nLABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_9d[:,0],x_9d[:,2], c= label_color, alpha=0.5) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"788f46470e310de41e270879aa4b3119cd752244"},"cell_type":"code","source":"# Set a 4 KMeans clustering\nkmeans = KMeans(n_clusters=4)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_9d)\n\n# Define our own color map\nLABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b',3:'y'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_9d[:,0],x_9d[:,3], c= label_color, alpha=0.5) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac18827d04027f396a1f2e86118a08f533970045"},"cell_type":"code","source":"# Set a 5 KMeans clustering\nkmeans = KMeans(n_clusters=5)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_9d)\n\n# Define our own color map\nLABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b',3:'y',4:'m'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_9d[:,0],x_9d[:,4], c= label_color, alpha=0.5) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab9e26967ea9d9b781499ed7cc354f4d0750d0e5"},"cell_type":"code","source":"# Set a 6 KMeans clustering\nkmeans = KMeans(n_clusters=6)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_9d)\n\n# Define our own color map\nLABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b',3:'y',4:'m',5:'c'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_9d[:,0],x_9d[:,5], c= label_color, alpha=0.5) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b00d8fe04ee91e04d7ae10991fc5444ef3f9aee"},"cell_type":"code","source":"# Set a 7 KMeans clustering\nkmeans = KMeans(n_clusters=7)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_9d)\n\n# Define our own color map\nLABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b',3:'y',4:'m',5:'c',6:'k'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_9d[:,0],x_9d[:,6], c= label_color, alpha=0.5) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"64dc05c3-c4bc-ecf5-0dcd-99ec1f109bbd","_uuid":"6866c8a599153b8745b74d4aabf4b9994d3107e6"},"cell_type":"markdown","source":"Pairplot automatically plots all the features in the dataframe  in pairwise manner. I will pairplot the first 3 projections against one another and the resultant plot is given below:"},{"metadata":{"_cell_guid":"e66e6d3d-c5d1-4b2d-3335-7908afcb4b9e","_uuid":"524a7b6f94237d6e7a7e92024f31b62a7e524d3e","trusted":true},"cell_type":"code","source":"# Create a temp dataframe from our PCA projection data \"x_9d\"\ndf = pd.DataFrame(x_9d)\ndf = df[[0,1,2]] # only want to visualise relationships between first 3 projections\ndf['X_cluster'] = X_clustered","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f04b3784-396a-b1b1-fc46-a637da0d3524","_uuid":"53a380c9a89d4b530117df22e1a10c91c2cbdd2f","trusted":true},"cell_type":"code","source":"# Call Seaborn's pairplot to visualize our KMeans clustering on the PCA projected data\nsns.pairplot(df, hue='X_cluster', palette= 'Dark2', diag_kind='kde',size=1.85)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"73575ddf-bdac-85fd-1e38-8a6757f1dc3e","_uuid":"c93edd75d5c0627db5997c3ef9252556890d4d41","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_is_fork":false,"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"_change_revision":0,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}