{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Notebook to compare performance between Neural Network and Random Forest\n\nHigh-level summary:\n    1. RF essentially creates many decision-trees, and then picks the value that the majority of the trees produce (i.e. a 'forest' of trees). Weighting is visible.\n    2. Neural Network (Multi-layer Perceptron) has multiple layers, paths and activation functions neurons (i.e. a 'web' or 'network' of 'nodes') - which may in parallel traverse multiple paths. The middle layers are hidden and a 'black box'. It is feed forward (and doesn't go backwards).\n    \nVisually:\n\nRandom Forest:\nCourtesy of: https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n\n![Random Forest](https://miro.medium.com/max/925/1*i0o8mjFfCn-uD79-F1Cqkw.png)\n\nNeural Network:\nCourtesy of: https://cs231n.github.io/neural-networks-1/\n\n![Neural Network](https://cs231n.github.io/assets/nn1/neural_net2.jpeg)"},{"metadata":{},"cell_type":"markdown","source":"# Firstly, let's get the dataset and do some EDA, data integration and wrangling\nCourtesy of: https://www.kaggle.com/rounakbanik/pokemon"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pandas_profiling\n\n#Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import data\npokemon = pd.read_csv('../input/pokemon/pokemon.csv')\n\npokemon.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next let's do some EDA\npokemon.profile_report(style={'full_width':True})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pokemon.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pokemon.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see the following characteristics:\n1. 588 different categories\n2. 731 pokemon total\n3. 70 legendary pokemon\n\nAs categorical data, you would not expect any correlation etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will drop abilities, for simplicity purposes\npokemon.drop(columns='abilities', inplace=True)\n\n# Drop missing values\npokemon = pokemon.dropna(axis=1) #Rows with NaN\n\npokemon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next we need to split feature vs labels/targets\n# We will arrayise the features\n\n# Labels are the values we want to predict - in this case, whether a pokemon is legendary\nlabels = np.array(pokemon['is_legendary'])\n\n# Remove the labels from the features\nfeatures = pokemon.drop('is_legendary', axis = 1) # axis 1 refers to the columns\nfeature_names = list(pokemon.drop('is_legendary', axis = 1).columns) # Get feature names\n\n# While data is already in sparse matrix for many aspects, one-hot encoding required for remaining string values\npokemon_preprocessed = pd.get_dummies(features)\n\n# Convert to numpy array\nfeatures = np.array(pokemon_preprocessed)\n\npokemon_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create training vs test data\n# We'll use a 70/30 split\nfrom sklearn.model_selection import train_test_split\n\ntrain_features, test_features, train_labels, test_labels = train_test_split(\n    features\n    ,labels\n   ,test_size=0.30 #30%\n   ,random_state=42 #seed used by the random number generator\n)\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NN are sensitive to feature scaling, so we'll scale our data - only train data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaler.fit(train_features)\n\n# Now apply the transformations to the data:\ntrain_features = scaler.transform(train_features)\ntest_features = scaler.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First let's do it with NN - we won't use any tweaking - just use data as is"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(solver='lbfgs' #‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n                    ,alpha=1e-5 #L2 penalty (regularization term) parameter.\n                    ,hidden_layer_sizes=(2,100) #We will pick 100 hidden layers, each 2 activation functions wide \n                    ,random_state=1 #seed used by the random number generator\n                   )\n\n# Fit training data to NN model\nmodel_NN = clf.fit(train_features, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict using test data with NN model \npredict_NN = model_NN.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate results - show confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nclass_names = ['Is Legendary', 'Is not Legendary']\ncm = confusion_matrix(predict_NN, test_labels)\n\n# Reconvert back to DF\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n\n# Create seaborn heatmap plot\nfig_NN = plt.figure() \n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n\nheatmap = sns.heatmap(df_cm\n                      , annot=True\n                      , fmt=\"d\",\n                      cmap=\"Blues\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14, color='black')\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14, color='black')\n\nfig_NN.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy:\")\nprint(accuracy_score(test_labels, predict_NN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_labels, predict_NN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Next, let's do it with RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100 # Number of trees in forest\n                             ,max_depth=5 # Number of levels of tree\n                             ,random_state=0 #seed used by the random number generator\n                            )\n\n# Fit training data to RF model\nmodel_RF = clf.fit(train_features, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_RF = model_RF.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unlike NN, in RF we can peek under the hood\n# Print out 1st tree in the forest\nfrom sklearn.tree import export_graphviz\nfrom graphviz import Source\nfrom IPython.display import Image\n\nexport_graphviz(\n    model_RF.estimators_[0]\n    ,out_file='1_tree_limited.dot'\n    ,feature_names=(pokemon_preprocessed.columns) # Grab feature names, minus the label\n    ,class_names = ['Is Legendary', 'Is not Legendary']\n   ,filled = True\n    )\n\n!dot -Tpng 1_tree_limited.dot -o 1_tree_limited.png -Gdpi=600\nImage(filename = '1_tree_limited.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print out 10th tree in the forest\nexport_graphviz(\n    model_RF.estimators_[9]\n    ,out_file='10_tree_limited.dot'\n    ,feature_names=(pokemon_preprocessed.columns) # Grab feature names, minus the label\n    ,class_names = ['Is Legendary', 'Is not Legendary']\n   ,filled = True\n    )\n\n!dot -Tpng 10_tree_limited.dot -o 10_tree_limited.png -Gdpi=600\nImage(filename = '10_tree_limited.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate results - show confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nclass_names = ['Is Legendary', 'Is not Legendary']\ncm = confusion_matrix(predict_RF, test_labels)\n\n# Reconvert back to DF\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n\n# Create seaborn heatmap plot\nfig_RF = plt.figure() \n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n\nheatmap = sns.heatmap(df_cm\n                      , annot=True\n                      , fmt=\"d\",\n                      cmap=\"Blues\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14, color='black')\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14, color='black')\n\nfig_RF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy:\")\nprint(accuracy_score(test_labels, predict_RF))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_labels, predict_RF))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final conclusion\nRF performs better with tabular format in this circumstance and has better explainability (you can see the decision trees).\n\nThe Confusion matrices are as follows:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"NN accuracy: \")\nprint(round(accuracy_score(predict_NN, test_labels) * 100, 2), '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_NN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RF accuracy: \")\nprint(round(accuracy_score(predict_RF, test_labels) * 100, 2) ,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_RF","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, the data is very skewered and therefore these results are questionable -\n70 out of 801 Pokemon are legendary (per EDA profile before)\n731 out of 801 Pokemon are not legendary (per EDA profile before)\n\nTherefore, a baseline model of just only guessing 'Is not Legendary' would already achieve a 91.3% accuracy."},{"metadata":{},"cell_type":"markdown","source":"# In conclusion, a baseline model of just blindly guessing No would be better than both ML models..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}