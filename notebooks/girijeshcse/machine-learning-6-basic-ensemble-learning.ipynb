{"cells":[{"metadata":{"_uuid":"d69cc3abfd92ef495f93158d7cebc3e9da990fbf"},"cell_type":"markdown","source":"Welcome to the **6th tutorial** in this tutorial series! In the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests), we talked about Random Forest. Random Forest consists of an ensemble of decision trees, and is one popular example of **ensemble learning**. \n\nThis tutorial is the first part of the two-part tutorial on ensemble learning, covering basic concepts and techniques such as <font color='blue'> bagging and pasting</font>. The next tutorial is the second part, covering advanced and powerful techniques such as <font color='blue'>boosting and stacking</font>. As one of the most popular boosting techniques on Kaggle, <font color='blue'>XGBoost</font> will also be covered in the next tutorial. However, if you are new to ensemble learning, it is best to start from this tutorial. Understanding the basic concepts and techniques of ensemble learning is the foundation for understanding the advanced techniques.\n\nHere is a list of my previous tutorials, if you are interested:\n* [Machine Learning 1 - Regression, Gradient Descent](https://www.kaggle.com/fengdanye/machine-learning-1-regression-gradient-descent)  \n* [Machine Learning 2 Regularized LM, Early Stopping](https://www.kaggle.com/fengdanye/machine-learning-2-regularized-lm-early-stopping)  \n* [Machine Learning 3 Logistic and Softmax Regression](https://www.kaggle.com/fengdanye/machine-learning-3-logistic-and-softmax-regression)\n* [Machine Learning 4 Support Vector Machine](https://www.kaggle.com/fengdanye/machine-learning-4-support-vector-machine)\n* [Machine Learning 5 Random Forests](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests)\n--------------------------------"},{"metadata":{"_uuid":"9bd0ee658f0bdc5ddb47d8825f028fbd9b7a7ee6"},"cell_type":"markdown","source":"**Table of Content**\n* Ensemble Learning Classification\n    * Introduction\n        * Voting rules\n    * Ensemble of different classifiers\n    * Ensemble of same classifiers\n        * Random sampling of training instances\n            * Bagging\n                * oob score\n            * Pasting\n            * Example\n        * Random sampling of features\n            * Example\n        * Random thresholds - Extra Trees\n            * Example\n    * Summary of performance\n\n* Ensemble Learning Regression\n    * Introduction\n    * Example - BaggingRegressor\n        * oob score\n    * Example - RandomForestRegressor\n    * Example - ExtraTreesRegressor\n    * Summary of performance"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8098b6bc34421d8cf294a27eb2239380f3ab00dc"},"cell_type":"code","source":"plt.rc('axes', lw = 1.5)\nplt.rc('xtick', labelsize = 14)\nplt.rc('ytick', labelsize = 14)\nplt.rc('xtick.major', size = 5, width = 3)\nplt.rc('ytick.major', size = 5, width = 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"468b6ecb1261311c64760c2fc28094a80c3d47c5"},"cell_type":"markdown","source":"# Ensemble Learning Classification\n## Introduction\nIn the previous tutorials, we have introduced several families of classifiers - logistic regression, SVM, decision trees, etc. We mostly focused on using one classifier of one family to classify a sample $\\vec{x}$. However, we can also combine the knowledge of multiple classifiers and make a \"consensus\" decision from each classifier's own decision. Usually this method performs better (e.g. better generalization to test set, less overfitting) than a single classifier, and we saw that in the last tutorial with the example of a Random Forest. The technique of classifying instances based on multiple classifiers' decisions is called **Ensemble learning classification**."},{"metadata":{"_uuid":"2531542c0caae3c7f7defb1066644a3e5b1e4a90"},"cell_type":"markdown","source":"### Voting rules\nNow, let's imagine we have five classifiers in the ensemble and we are predicting a class for a single instance $\\vec{x}$. Each classifier will make a decision on its own. In this case, the ensemble will predict the class based on **majority of votes**:\n<img src=\"https://imgur.com/eAruUj3.png\" width=\"600px\"/>\nAs you can see, since four classifiers predicted \"Class 2\", and only one classifier predicted \"Class 1\", the ensemble decides the final prediction is \"Class 2\". This voting rule is sometimes called **hard voting**.\n\nNow, remember that many classifiers we have introducted not only predict a class, but also provide prediction probabilities for each class. If all classifiers in the ensemble have prediction probabilities, we can also use the so-called **soft voting** rule:\n<img src=\"https://imgur.com/ud382N9.png\" width=\"600px\"/>\nHere, the probabilities for each class are averaged over all classifiers in the ensemble, and the class that has the highest average probability is predicted. **In many cases, soft voting performs better than hard voting**, since it takes in more information and gives higher weight to highly confident predictions (i.e. predictions with high probability). As introduced in the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests), for example, Scikit-learn's Random Forest classifier uses soft voting by default.\n\n#### Side note on how to obtain prediction probability in SVM (only if you are interested)\nSVM is one classifier that, by default, does not produce prediction probability. However, you can force a probability calculation in Scikit-learn's SVC by setting \"probability\" to *True*. You can then call the predict_proba() function to obtain class probablities. To understand how the probabilities are obtained, you can read the [documentation](https://scikit-learn.org/stable/modules/svm.html#scores-and-probabilities), as well as this [stack overflow answer](https://stackoverflow.com/questions/15111408/how-does-sklearn-svm-svcs-function-predict-proba-work-internally). The basic idea is as follows:\n* First, train SVM in a cross validation manner. For each fold, there is a training set and a hold-out set. The $\\vec{w}$ and $b$ are obtained through training on the training set, and then $\\vec{w}\\cdot \\vec{x}+b$ is calculated on the hold out set. If this is a five-fold cross validation (which is the case for Scikit-learn's SVC), then you will have 5 hold-out sets, the union of which is the whole data set. Note that in the aforementioned links,  $\\vec{w}\\cdot \\vec{x}+b$ is represented by $f$.\n* The union of the $\\vec{w}\\cdot \\vec{x}+b$ calculated on the hold out sets will be used to train a logistic sigmoid function: $P(y=1|f)=\\frac{1}{1+exp(Af+B)}$, where $f=\\vec{w}\\cdot \\vec{x}+b$ on the hold-out sets. $P(y=1|f)>0.5$ predicts $y=1$, and $P(y=1|f)\\leq0.5$ predicts $y=0$. During training, parameter $A$ and $B$ will be optimized to minimize the cross-entropy loss function. This is basically a logistic regression on the SVM scores.\n* Now that we have $A$ and $B$, SVM will be re-trained on the entire data set. For a given instance $\\vec{x}$, the re-trained SVM will produce a $f=\\vec{w}\\cdot \\vec{x}+b$ value, and then $P(y=1|f)=\\frac{1}{1+exp(Af+B)}$ will produce the probability.  \n\nThis method of conducting logistic regression on SVM scores, is called **Platt scaling**. If you are interested in learning more about this method, you can read Platt's paper “[Probabilistic outputs for SVMs and comparisons to regularized likelihood methods](http://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf)\". The multiclass case is extended by Wu, Lin and Weng, “[Probability estimates for multi-class classification by pairwise coupling](https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf)”, JMLR 5:975-1005, 2004.\n\nNote that asking SVM to calculate prediction probabilities will significantly slow down the training speed since there is a 5-fold cross validation."},{"metadata":{"trusted":true,"_uuid":"c31402c21c4a729c78465ff43344e9aee81a0a21"},"cell_type":"markdown","source":"When we talk about an ensemble of classfiers, there are two possibilities: \n* An ensemble of different classifiers, which, for example, can contain logistic regression classifiers, SVM classifiers, and decision trees, all at the same time.\n* An ensemble of the same type of classifiers, such as a Random Forest.\n\n<img src=\"https://imgur.com/mlRpzUf.png\" width=\"400px\"/>\n\nWe will first talk about ensemble of different classifiers."},{"metadata":{"_uuid":"8d5d1eaa4e0bda3d6ce6912d1c5014b920cb475d"},"cell_type":"markdown","source":"## Ensemble of different classifiers\nLet's build a simple ensemble of different classifiers as follows:\n<img src=\"https://imgur.com/3G6054b.png\" width=\"400px\"/>\nIn this ensemble, there are only three classifiers: **a Random Forest classifier, a SVM classifier, and a logistic regression classifier**. Note that random forest itself is an ensemble learning classifier. Again, we will be using the Red Wine Quality dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"wineData['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dfcf73095040c202022a7c4c1fab097ca94e891"},"cell_type":"code","source":"wineData = pd.read_csv('../input/winequality-red.csv')\n\n\nwineData['category'] = wineData['quality'] >= 7\nX = wineData[wineData.columns[0:11]].values\ny = wineData['category'].values.astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"543e92e1089cadade01e3e2c6b0f18ba418241ad"},"cell_type":"code","source":"wineData.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81ca6d7ae11c6a87581a10e69fed37d0b4a9d624"},"cell_type":"markdown","source":"The wine quality is binarized into either \"good\" ($y=1$, quality>=7) or \"not good\" ($y=0$, quality<7). The input $X$ consists of 11 features such as fixed acidity and pH. We will then split the data set into a trianing set and a test set:"},{"metadata":{"trusted":true,"_uuid":"d4d02ec201de7585de91ec12e250113835035b38"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\nprint('X train size: ', X_train.shape)\nprint('y train size: ', y_train.shape)\nprint('X test size: ', X_test.shape)\nprint('y test size: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae301acc6790883fd0b869029f8691c9813a59eb"},"cell_type":"markdown","source":"Again, the \"random_state\" is set to guarantee a repeatable result for this tutorial. In practice, you should remove the random_state argument.  \nTo construct a voting classifier, we use scikit-learn's **VotingClassifier**. Note that we set the voting rule to \"soft\". This is because only soft voting produces prediction probabilities, and we need the probilities to plot ROC curve."},{"metadata":{"trusted":true,"_uuid":"5b0bb62a2ebd7a43ed11bb5d119087d39c8b00da"},"cell_type":"code","source":"# Below, random_state is only used to guarantee repeatable result for the tutorial. \nrfClf = RandomForestClassifier(n_estimators=500, random_state=0) # 500 trees. \nsvmClf = SVC(probability=True, random_state=0) # force a probability calculation\nlogClf = LogisticRegression(random_state=0)\n\nclf = VotingClassifier(estimators = [('rf',rfClf), ('svm',svmClf), ('log', logClf)], voting='soft') # construct the ensemble classifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a06ff72735d63b48f3e581744b4e71573775b035"},"cell_type":"markdown","source":"**To construct the ensemble classifier, all you need to do is to specify what estimators you want to include in VotingClassifier()**. In this case we have \"estimators = [('rf',rfClf), ('svm',svmClf), ('log', logClf)]\". Now, we train the classifier on the training set:"},{"metadata":{"trusted":true,"_uuid":"33fdfdadbee74271b70017ab18ed19a7217ac469"},"cell_type":"code","source":"clf.fit(X_train, y_train) # train the ensemble classifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c478d9a70e38f9a4a48917d533ffbc97c928b02b"},"cell_type":"markdown","source":"And then examine how the classifier performs on the test set:"},{"metadata":{"trusted":true,"_uuid":"66611c824b4b69dad556e0e5ebcc4bb635f980c2"},"cell_type":"code","source":"from sklearn.metrics import precision_score, accuracy_score\ny_true, y_pred = y_test, clf.predict(X_test)\nprint('precision on the test set: ', precision_score(y_true, y_pred))\nprint('accuracy on the test set: ', accuracy_score(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"667d999fc11100312b4eef5bee28db9c8df5ca4a"},"cell_type":"markdown","source":"We have achieved a precision of 68.2% and an accuracy of 87.7%. Note that in the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests), we achieved 58.3% precision and 87.7% accuracy with a single Random Forest classifier. Though the accuracy has not improved, precision has improved considerably with the use of an ensemble classifier. Remeber that precision = TP/(TP+FP). An improvement in precision indicates that among all predicted positivies, the proportion of true positives has increased.\n\nOf course, there are more measures you can calculate than just precision and accuracy. You can find the definition of a full list of performance measures on this [Wikipedia page](https://en.wikipedia.org/wiki/Confusion_matrix).\n\nNow, let's plot the ROC curve and calculate AUC:"},{"metadata":{"trusted":true,"_uuid":"6d30621dc535c931fa07390a27ddae1b612438c7"},"cell_type":"code","source":"from sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\nphat = clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7a64b4db3446be54fdf565b5f627ee58d76e8cc"},"cell_type":"code","source":"plt.subplots(figsize=(8,6))\nfpr, tpr, thresholds = roc_curve(y_test, phat)\nplt.plot(fpr, tpr)\nx = np.linspace(0,1,num=50)\nplt.plot(x,x,color='lightgrey',linestyle='--',marker='',lw=2,label='random guess')\nplt.legend(fontsize = 14)\nplt.xlabel('False positive rate', fontsize = 18)\nplt.ylabel('True positive rate', fontsize = 18)\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1688e67838e8a30781c9fa92b5d3e58010f92f0f"},"cell_type":"code","source":"print('AUC is: ', auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c14099809fbe6e22aae808b726c5ffd60dd3efa7"},"cell_type":"markdown","source":"This AUC (0.914) is a slight improvement to the AUC calculated from a single Random Forest classifier (0.909) in the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests/).  For a introduction to ROC curve and AUC, see my [previous tutorial](https://www.kaggle.com/fengdanye/machine-learning-3-logistic-and-softmax-regression)."},{"metadata":{"_uuid":"caa8c2f2b0011f0e931516334d2db60227d73ad8"},"cell_type":"markdown","source":"## Ensemble of same classifiers\n**Random Forest is one of the most popular examples of an ensemble of same classifiers**. In a random forest, each tree has the same hyperparameters (e.g. max_depth and min_samples_leaf), but is trained on a bootstrap of the training set. If max_features is smaller than one, trees in a random forest are also split on randomly sampled subset of features. In the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests), we have presented an example of training and using a random forest.\n\n**Now, let's jump out of the context of random forests. Let's imagine a more general case, where we simply have an ensemble of the same type of classifiers**. This can be an ensemble of SVMs, an ensemble of logistic regression classifiers, an ensemble of decision trees, or even an ensemble of random forest classifiers. **For the ensemble to perform well, we would want the individual classifiers to be as independent as possible**. In the previous section, this is taken care of by having different types of classifiers. In the case where the calssifiers are of same type, we can achieve so by introducing randomness for each classifier. This tutorial will talk about three ways to do that:\n1. Randomly sample training instances for each classifier\n2. Randomly sample features that are used to train each classifier\n3. Specifically for decision tress - use random threshold for each feature (as compared to finding the best threshold)  \n\nWe will go over them one by one."},{"metadata":{"_uuid":"10f7c67672687e508ed84b0c0ac0f8d39b48c777"},"cell_type":"markdown","source":"### Random sampling of training instances\n#### Bagging\nBagging refers to the method of randomly sampling training instances *with replacement*. In statistics, sampling with replacement is also called *bootstrapping*. **The term \"with replacement\" means that after one instance is taken randomly from the training set, a replacement of this instance is put into the training set. When the next instance is selected, there is a chance that this next instance selected is the same as the previous instance selected**. Here is a simple example of bagging:\n<img src=\"https://imgur.com/XA7mf26.png\" width=\"400px\"/>\nAs you can see, the same instance can appear multiple times in the subsample. This is the characteristic of the bagging method."},{"metadata":{"_uuid":"e8b0cf6873d1072c6e5acef6bb31a952365bc87b"},"cell_type":"markdown","source":"##### oob score\nDuring bagging, each subsample is used to train one classifier. For each classifier, the samples that are *not* seen during training is called **out-of-bag instances**, or **oob instances**:\n<img src=\"https://imgur.com/ssSY5Kj.png\" width=\"500px\"/>\nThese oob instances can be used to evaluate the performance of the classifiers, since they serve the same function as a test set - a dataset that is not seen during training. To evaluate an oob score using the bagging method, we use Scikit-learn's **BaggingClassifier**, and set oob_score=True. From the [source code of BaggingClassifier](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/ensemble/bagging.py#L430) (line 583-618), we can see that the oob score is calculated as follows:\n* For each trained classifier, locate its corresponding oob instances.\n* Use the trained classifier to predict on its oob instances\n    * If the classifier has predict_proba(), predict the probabilities of each class for each oob instance\n    * If the classifier does not have predict_proba(), predict which class each oob instance belongs to\n* Do this for all classifiers in the ensemble\n* For each instance in the whole training set, locate the classifiers for which this instance is an oob instance. For convenience, I will call these classifiers the \"oob classifiers\" for this instance.\n    * If the classifier has predict_proba(), predict the class of this instance to be the one that has the highest average probability across all \"oob classifiers\" for this instance.\n    * If the classifier does not have predict_proba(), predict the class of this instance to be the one that has the majority of votes from all \"oob classifiers\" for this instance.\n* Do this for all instances in the whole training set. For convenience, let's name the prediction on the whole training set as y_oob.\n* **The final oob score is the accuracy score of the above prediction: accuracy_score(y_true, y_oob)**. Both y_true and y_oob have a size of m, where m is the total number of training instances.\n\n**To summary, the BaggingClassifier's oob socre gives you an estimation of the accuracy of the ensemble classifier on a test set**. At the end of this section, we will show an example of how to use BaggingClassifier and obtain an oob score."},{"metadata":{"_uuid":"e9e27053e8d03a72efaea28800d57f1748a3afdc"},"cell_type":"markdown","source":"#### Pasting\nPasting refers to the method of randomly sampling training instances *without replacement*. This means that, in a certain subsample, the same instance can only appear at most once:\n<img src=\"https://imgur.com/5ZMvOoL.png\" width=\"400px\"/>\nScikit-learn's BaggingClassifier can also perform pasting if the argument *bootstrap* is set to *False*."},{"metadata":{"_uuid":"8d5f9be017c9ce3495338b6552e0ccfa9c27ef82"},"cell_type":"markdown","source":"#### Example\nIn this example, we will compare the performance of a single logistic regression classifier with that of an ensemble of logistic regression classifiers. Let's first read the data:"},{"metadata":{"trusted":true,"_uuid":"415ec6e8828e578780b5e4fb463ce022d03628e8"},"cell_type":"code","source":"wineData = pd.read_csv('../input/winequality-red.csv')\n\nwineData['category'] = wineData['quality'] >= 7\n\nX = wineData[wineData.columns[0:11]].values\ny = wineData['category'].values.astype(np.int)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\nprint('X train size: ', X_train.shape)\nprint('y train size: ', y_train.shape)\nprint('X test size: ', X_test.shape)\nprint('y test size: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c01e7599c2e2438bd3b2e3a6b514759245c4a660"},"cell_type":"markdown","source":"**Let's first use a single logistic regression classifier and see how it performs in terms of precision, accuracy and AUC**. In [tutorial 3](https://www.kaggle.com/fengdanye/machine-learning-3-logistic-and-softmax-regression), we introduced logistic regression and did very similar training/predicting, but we did not do a train-test split in that tutorial. Here, we will do the same training/predicting, but with a train-test split.\n\nDon't forget, the first step is to standardize the training data:"},{"metadata":{"trusted":true,"_uuid":"e378458a6df02dab04f2279b16ae1f2296637bef"},"cell_type":"code","source":"scaler = StandardScaler()\nX_train_stan = scaler.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e8bde8a7f370ffecdb774aa3840ca4fb1726876"},"cell_type":"markdown","source":"Then we train a logisitc regression classifier on the standardized training set and evalute its performance on the test set:"},{"metadata":{"trusted":true,"_uuid":"9cd845e50ffb002a72bd1e6b0bc482ae59d3a130"},"cell_type":"code","source":"logReg = LogisticRegression(random_state=0, solver='lbfgs') # random_state is only set to guarantee for repeatable result for the tutorial\nlogReg.fit(X_train_stan, y_train)\n\nX_test_stan = scaler.transform(X_test) # don't forget this step!\ny_pred = logReg.predict(X_test_stan)\n\nprint('precision on the test set: ', precision_score(y_test, y_pred))\nprint('accuracy on the test set: ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8268c6398a893f7f651845019918a00604465e33"},"cell_type":"code","source":"phat = logReg.predict_proba(X_test_stan)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, phat)\n\nplt.subplots(figsize=(8,6))\nplt.plot(fpr, tpr)\nx = np.linspace(0,1,num=50)\nplt.plot(x,x,color='lightgrey',linestyle='--',marker='',lw=2,label='random guess')\nplt.legend(fontsize = 14)\nplt.xlabel('False positive rate', fontsize = 18)\nplt.ylabel('True positive rate', fontsize = 18)\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d93e716db12b75ff93245f1deb4fa78ab3906ac1"},"cell_type":"code","source":"print('AUC is: ', auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f0a21aa22eff970e33905d26e3212ad799616eb"},"cell_type":"markdown","source":"**Now, let's use an ensemble of 500 logistic regression classifiers with the bagging method**. To do this, we will use Scikit-learn's **BaggingClassifier**:"},{"metadata":{"trusted":true,"_uuid":"970c909dda3703b52d631ec24dc0934dbe2c86e2"},"cell_type":"code","source":"bagClf = BaggingClassifier(LogisticRegression(random_state=0, solver='lbfgs'), n_estimators = 500, oob_score = True, random_state = 90)\n# again, random_state is only set to guarantee repeatable result for this tutorial.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"945be8c7f442b4d541cc5b4a4d6c8192cbc7bfa4"},"cell_type":"markdown","source":"Note that by default, BaggingClassifier has arguments *max_samples=1.0* and *bootstrap=True*. **This means that this ensemble classifier is drawing subsamples with replacement (i.e. bagging), and the subsample size is equal to the size of the whole training set**. We have also set the argument *oob_score* to *True* for out-of-bag accuracy estimation.\n\nNow let's train and evaluate the ensemble classifier:"},{"metadata":{"trusted":true,"_uuid":"0b2544d4b884c8f786c736c520a5917b97f37959"},"cell_type":"code","source":"bagClf.fit(X_train_stan, y_train)\nprint(bagClf.oob_score_) # The oob score is an estimate of the accuracy of the ensemble classifier, as introduced earlier. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e24da95cb4a66bae3b0f2b1f3cd830ecea9591"},"cell_type":"code","source":"y_pred = bagClf.predict(X_test_stan)\nphat = bagClf.predict_proba(X_test_stan)[:,1]\n\nprint('precision on the test set: ', precision_score(y_test, y_pred))\nprint('accuracy on the test set: ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a48fd959480b9a3cad87782fc46bab92019d0fa3"},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, phat)\nplt.subplots(figsize=(8,6))\nplt.plot(fpr, tpr)\nx = np.linspace(0,1,num=50)\nplt.plot(x,x,color='lightgrey',linestyle='--',marker='',lw=2,label='random guess')\nplt.legend(fontsize = 14)\nplt.xlabel('False positive rate', fontsize = 18)\nplt.ylabel('True positive rate', fontsize = 18)\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b9a649051968ef3f51c4b34917626866ea6f7ba"},"cell_type":"code","source":"print('AUC is: ', auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"678d1d9f7e7c0b9406f062e7aa6b612827b7a474"},"cell_type":"markdown","source":"The ensemble classifier has slight improvement compared to a single logistic regression classifier, but it is no match to the performance of the simple three-classifier (SVM + random-forest + logistic) ensemble we used earlier. This ensemble of 500 logisitc regression classifiers also performs worse than the Random Forest we used in the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests) which has 500 decision trees. **One possible reason is that this ensemble of 500 logistic regression classifiers only randomly samples the training instances, but not the features, whereas the random forest randomly samples both training instances and features (with the *max_feature* argument). This may lead to a lack of independence between individual classifiers in the current ensemble, thus the slightly worse performance**. \n\nIn the next section, we will talk about randomly sampling the input features."},{"metadata":{"_uuid":"9520375fbd36ce5c042918d6da49a1b1d8b10315"},"cell_type":"markdown","source":"### Random sampling of features\nTo further increase the independence between individual classifiers, we can **train each classifier on a different random subset of features**. In the Wine Quality dataset, we have 11 input features: "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d6714e8091f22d2fa31a135827c3933678cf1b83"},"cell_type":"code","source":"wineData.head(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f4ff48549a84ec61d422a933715fb01e06c9443"},"cell_type":"markdown","source":"Random sampling *with replacement* of the above features with a sample size of 3 will look like this:\n<img src=\"https://imgur.com/aeRjeR6.png\" width=\"500px\"/>\nAgain, if sampling *without replacement*, then there won't be repeated features in the subsample.  **To do random sampling of features, we can again use BaggingClassifier**. The two key arguments are *bootstrap_features* and *max_features*. The *bootstrap_features* decides whether to sample with replacement, and *max_features* decides the proportion of features to draw from the input features. You can set, for example, *bootstrap_features = True* and *max_features = 1.0* to draw a bootstrap sample of the features with size equal the total number of features.\n\nRandom sampling features without random sampling the training instances is called the **Random Subspaces** method. This corresponds to *bootstrap = False, max_samples = 1.0*, and *bootstrap_features = True* and/or *max_features < 1.0*. Of course, features and training instances can be sampled at the same time. In this case, the method is called **Random Patches. In the Random Patches method, each classifier is trained on its own corresponding subsamples of training instances and features**. The oob score is calculated as described earlier. The only additional detail is that when each classifier makes predictions on its oob samples, the classifier only uses a subset of features."},{"metadata":{"_uuid":"26b9ebfd10ed317588ecc41ffb0a60dd83b154a5"},"cell_type":"markdown","source":"#### Example\nNow, let's use the ensemble of 500 logisitc regression classifiers again, but this time randomly **sample both training instances and features**:"},{"metadata":{"trusted":true,"_uuid":"ba4065cfb82fab4043beea996e33cdbe25f9caf2"},"cell_type":"code","source":"bagClf = BaggingClassifier(LogisticRegression(random_state=0, solver='lbfgs'), n_estimators = 500, \n                           bootstrap_features = True, max_features = 1.0, oob_score = True, random_state = 90)\n# Notice that bootstrap_features is set to True.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c88b10c151772af335d85338fab56cb41583f4a9"},"cell_type":"code","source":"bagClf.fit(X_train_stan, y_train)\nprint(bagClf.oob_score_) # The oob score is an estimate of the accuracy of the ensemble classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e40e722f603a1bcab0d28a73d5c389de16d035d"},"cell_type":"code","source":"y_pred = bagClf.predict(X_test_stan)\nphat = bagClf.predict_proba(X_test_stan)[:,1]\n\nprint('precision on the test set: ', precision_score(y_test, y_pred))\nprint('accuracy on the test set: ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77e117f1d18c1d8d987e84be408145ed49d3654e"},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, phat)\nplt.subplots(figsize=(8,6))\nplt.plot(fpr, tpr)\nx = np.linspace(0,1,num=50)\nplt.plot(x,x,color='lightgrey',linestyle='--',marker='',lw=2,label='random guess')\nplt.legend(fontsize = 14)\nplt.xlabel('False positive rate', fontsize = 18)\nplt.ylabel('True positive rate', fontsize = 18)\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9290d6d3f2f322b46e6e5f0ef90267b61f15552e"},"cell_type":"code","source":"print('AUC is: ', auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a15680c74cd8bc15cf1284340564e2c0a3404e46"},"cell_type":"markdown","source":"With the use of Random Patches method, the precision of the prediction has considerably increased (0.54 -> 0.61), but the accuracy and AUC have stayed more or less the same."},{"metadata":{"_uuid":"e234d3c3964d5b42f72d80895760eb937ba00e68"},"cell_type":"markdown","source":"### Random thresholds - Extra Trees\nIn the specific case of decision trees, we can further randomize individual trees by introducing random thresholds at each node. The trees are called **Extremely Randomized Trees (or Extra Trees)**. The Extra Trees Classifier work similarly as a Random Forest Classifier. It searches a random subset of the features for the best split of each node. **The difference is that a Random Forest searches for the best (feature, threshold) combination at each split, whereas in Extra Trees each candidate feature's threshold is drawn at random (one threshold per feature) and the best among them is selected (see [documentation](https://scikit-learn.org/stable/modules/ensemble.html#forest))**. You can also take a look at the [source code for random splitter](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_splitter.pyx#L652), which is [used by Extra Trees](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/tree/tree.py#L1146) (totally optional, only if you are interested). The random splitter basically works by drawing one feature at a time, choosing a random threshold for this feature, and evaluating the split's performance. If the split is better than the previous best, than the split becomes the current best. The loop finishes when *max_features* have been drawn (without replacement), or when the number of drawn features is below *max_features* but the remaining features are all constants. That's why the random splitter is said to \"choose the best random split\".\n\n**Also note that, by default, Scikit-learn's RandomForestClassifier has bootstrap = True, whereas ExtraTreesClassifier has bootstrap = False. This means that ExtraTressClassifier uses the full training instances with no random sampling** (both RandomForestClassifier and ExtraTreesClassifier has sample size = full training set size, therefore a bootstrap=False means the full training instances are used).\n\n#### Example\nNow, let's try out the **ExtraTreesClassifier** on the Wine Quality dataset. Just like what we did in the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests), we will use **GridSearchCV** to search for the best hyperparameters. GridSearchCV is introduced in my [4th tutorial](https://www.kaggle.com/fengdanye/machine-learning-4-support-vector-machine)."},{"metadata":{"trusted":true,"_uuid":"514ef462c0ea99e96a966454a041518ed8826ac8"},"cell_type":"code","source":"tuned_parameters = {'n_estimators':[500],'n_jobs':[-1], 'max_features': [0.5,0.6,0.7,0.8,0.9,1.0], \n                    'max_depth': [10,11,12,13,14],'min_samples_leaf':[1,10,100],'random_state':[0]} \n\nclf = GridSearchCV(ExtraTreesClassifier(), tuned_parameters, cv=5, scoring='roc_auc')\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3b28a2d7b19fe679b0b6ff47ec69467ebd86678"},"cell_type":"code","source":"print('The best model is: ', clf.best_params_)\nprint('This model produces a mean cross-validated score (auc) of', clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f87c99360511c06d4b0515ff0f159d2cedbf72a"},"cell_type":"markdown","source":"GridSesearchCV has determined the best model to be the one with *max_depth = 13*, *max_features = 0.9*, and *min_samples_leaf = 1*. Now let's see how the Extra Trees classifier performs on the test set:"},{"metadata":{"trusted":true,"_uuid":"a5bcd9537e6ab676aa3cff3a0f579262282a085a"},"cell_type":"code","source":"y_pred = clf.predict(X_test)\nprint('precision on the evaluation set: ', precision_score(y_test, y_pred))\nprint('accuracy on the evaluation set: ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3d04a129e19a3db868a986250f778527c465eea"},"cell_type":"code","source":"phat = clf.predict_proba(X_test)[:,1]\nplt.subplots(figsize=(8,6))\nfpr, tpr, thresholds = roc_curve(y_test, phat)\nplt.plot(fpr, tpr)\nx = np.linspace(0,1,num=50)\nplt.plot(x,x,color='lightgrey',linestyle='--',marker='',lw=2,label='random guess')\nplt.legend(fontsize = 14)\nplt.xlabel('False positive rate', fontsize = 18)\nplt.ylabel('True positive rate', fontsize = 18)\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3f8843ce47046143f67906e0c81f94ba0adfcb4"},"cell_type":"code","source":"print('AUC is: ', auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d407acc20b449c8bd076c88046af50cd6c7e0d8c"},"cell_type":"markdown","source":"This Extra Trees classifier has by far the best accuracy and AUC score on the test set! "},{"metadata":{"_uuid":"c6500015ea878acdc95b362f1d199084c32a3215"},"cell_type":"markdown","source":"## Summary of performance\nIn this tutorial and the [last tutorial](http://https://www.kaggle.com/fengdanye/machine-learning-5-random-forests), we have trained various ensemble learning classifiers on the Wine Quality dataset. Since we have always split the train/test set with *random_state=42*, all these classifiers were trained on the same data and tested on the same data. This makes it possible for us to compare the performance of different ensemble classifiers. The *random_state* set for the classifiers also affect the performance, of course, and usually in practice *random_state* is generated at random (i.e. *random_state = None*). But for the purpose of repeatable results, I have to set a specific *random_state* value in this tutorial.\n\nHere is the summary table of the performance of different ensemble classifiers on the Red Wine Quality dataset:"},{"metadata":{"_uuid":"6bb6a01a3d68e0ecd7e1ebb97158c03890cc0af7"},"cell_type":"markdown","source":"| Classifier name | Precision on test set   | Accuracy on test set | AUC on test set | Comments |\n|------|------|------|------|------|\n|   Random Forest with 500 trees (with GridSearchCV)  | 0.58 | 0.88 | 0.91 | from the [last tutorial](https://www.kaggle.com/fengdanye/machine-learning-5-random-forests) |\n|RF-SVM-logistic classifier | **0.68** | 0.88 | 0.91| simple ensemble of three different classifiers |\n| 500 logisitic classifiers with bagging | 0.54 | 0.87 | 0.88| bagging method - randomly sample training instances|\n| 500 logisitic classifiers with Random Patches | 0.61 | 0.87 | 0.88| Random Patches - randomly sample both training instances and features|\n|Extra Trees with 500 trees (with GridSearchCV)| 0.62 | **0.89** | **0.93**| Extra trees use best random split|"},{"metadata":{"_uuid":"bf82c5dc27583ba15eae40674fe56047556807b1"},"cell_type":"markdown","source":"The best score has been bolded. Some discussion:\n* If you care the most about precision, then the RF-SVM-logisitc classifier is the best. If you care the most about accuracy or AUC, then Extra Trees classifier is the best.\n* Random Patches method improves the 500 logisitc classifiers in terms of precision.\n* Extra Trees classifier performs better than the Random Forest classifier.\n* Adding SVM and logisitc classifier to a Random Forest classifier improves precision considerably. "},{"metadata":{"_uuid":"9b2e073467acef53e2208177bd9f30d3220b89ef"},"cell_type":"markdown","source":"# Ensemble Learning Regression\n## Introduction\nEnsemble learning regression works similarly to classification. However, in regression, it is more common to use an ensemble of the same type of regressors than using different types of regressors. Scikit-learn does not have an equivalence of VotingClassifier() for regression, therefore to build an ensemble with different types of regressors you will have to put in extra work. On the other hand, building an ensemble of the same type of regressors is quick and easy. Scikit-learn provides classes such as **BaggingRegressor, RandomForestRegressor, and ExtraTreesRegressor** for such purposes. \n\nIn the following sections, I will show one example for each type of regressor. **This time, we will view the Red Wine Quality dataset as a regression problem. The inputs will be the 11 features, and the output will be the quality of the wine (0-10)**. And instead of evaluating precision, accuracy and AUC, we will evaluate the **r2 score** of the prediction. The r2 score is also often called \"coefficient of determination\" or shows up as \"R squared\" in many statistical packages. You can find the definition of r2 score on [this Wikipedia page](https://en.wikipedia.org/wiki/Coefficient_of_determination). The closer the r2 score to 1.0, the better.\n\nLet's first read the data:"},{"metadata":{"trusted":true,"_uuid":"ec441f4474ad93934c1070c9a591108807880f99"},"cell_type":"code","source":"wineData = pd.read_csv('../input/winequality-red.csv')\nwineData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7d6bc1d30ab4cc530532d3252a38baa819f001f"},"cell_type":"code","source":"X = wineData[wineData.columns[0:11]].values\ny = wineData['quality'].values.astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beaba3476dee246ef2211d299d4328c81ca01511"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)\n\nprint('X train size: ', X_train.shape)\nprint('y train size: ', y_train.shape)\nprint('X test size: ', X_test.shape)\nprint('y test size: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5b8e7b7a8ecf0255b69203067dce0510d133339"},"cell_type":"markdown","source":"## Example - BaggingRegressor\nLet's start with Scikit-learn's **BaggingRegressor**.  BaggingRegressor works very similar to BaggingClassifier. By default, BaggingRegressor trains each regressor in the ensemble on a bootstrap sample of the training instances. Additionally, we will set *bootstrap_features = True* to also do bootstrapping on input features.\n### oob score\n Like BaggingClassifier, BaggingRegressor can calculate an oob score.  The oob score is calculated as follows (see [source code](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/ensemble/bagging.py#L991)):\n* For each trained regressor, locate its corresponding oob instances.\n* Use the trained regressor to predict on its oob instances. If feature sampling is enabled, then only the selected features will be used to make predictions.\n* Do this for all regressors in the ensemble\n* For each instance in the whole training set, locate the regressors for which this instance is an oob instance. For convenience, I will call these regressors the \"oob regressors\" for this instance. Predict the value for this instance to be the average predicted values across all \"oob regressors\" for this instance.\n* Do this for all instances in the whole training set. For convenience, let's name the prediction on the whole training set as y_oob.\n* **The final oob score is the r2 score of the above prediction: r2_score(y_true, y_oob)**. Both y_true and y_oob have a size of m, where m is the total number of training instances.\n\n**The BaggingRegressor's oob socre gives you an estimation of the r2 score of the ensemble regressor on a test set**. "},{"metadata":{"trusted":true,"_uuid":"d22cf10f3b954386f4d192818671910095079763"},"cell_type":"code","source":"# Standardize the data\nscaler = StandardScaler()\nX_train_stan = scaler.fit_transform(X_train)\nX_test_stan = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5af4e802272d27d2af97f6987c2ee8909f188f14"},"cell_type":"code","source":"# use an ensemble of 500 linear regressors. Use Random Patches method.\nbagReg = BaggingRegressor(LinearRegression(), n_estimators = 500, \n                           bootstrap_features = True, max_features = 1.0, oob_score = True, random_state = 0)\n\nbagReg.fit(X_train_stan, y_train)\nprint(\"oob score is: \", bagReg.oob_score_) # The oob score is an estimate of the r2 score of the ensemble classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f4c1bac02269a849089791bd0e19a62280fe101"},"cell_type":"code","source":"from sklearn.metrics import r2_score\ny_pred = bagReg.predict(X_test_stan)\nprint(\"The r2 score on the test set is: \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51ac86806b04c9b595146bf5a533eeea9a571eca"},"cell_type":"markdown","source":"## Example - RandomForestRegressor"},{"metadata":{"_uuid":"1e4bc66e37b4eaccae9ad6e858ca6503e9057a36"},"cell_type":"markdown","source":"Now let's try out **RandomForestRegressor**. Unlike RandomForestClassifier, RandomForestRegressor by default searches the full set of features at each split. But here, we will use GridSearchCV to search for an optimal combination of *max_features*, *max_depth* and *min_samples_leaf*."},{"metadata":{"trusted":true,"_uuid":"0df67746cfd82d9195adba289ad98d8d2eb092d0"},"cell_type":"code","source":"tuned_parameters = {'n_estimators':[500],'n_jobs':[-1], 'max_features': [0.5,0.6,0.7,0.8,0.9,1.0], \n                    'max_depth': [16,20,24],'min_samples_leaf':[1,10,100],'random_state':[0]} \n\nreg = GridSearchCV(RandomForestRegressor(), tuned_parameters, cv=5, scoring='r2')\nreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b14b988dc3424f8a6d894ed60e650b0330348bd6"},"cell_type":"code","source":"print('The best model is: ', reg.best_params_)\nprint('This model produces a mean cross-validated score (r2) of', reg.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8390ce54d31e102e6ff121ab290b7e364eaf043e"},"cell_type":"code","source":"y_pred = reg.predict(X_test)\nprint(\"The r2 score on the test set is: \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07578056ac3d479bb1c51611f3d8610c911d8b2e"},"cell_type":"markdown","source":"## Example - ExtraTreesRegressor\nSimilar to the Random Forest case, **ExtraTreesRegressor** differs from ExtraTreesClassifier in that the regressor by default considers all features to select the best split. But again, we will use GridSearchCV to search for the best combination of *max_features*, *max_depth* and *min_samples_leaf*."},{"metadata":{"trusted":true,"_uuid":"cf51d440872993e7a8b1c363d47e498b56246c0d"},"cell_type":"code","source":"tuned_parameters = {'n_estimators':[500],'n_jobs':[-1], 'max_features': [0.5,0.6,0.7,0.8,0.9,1.0], \n                    'max_depth': [20,24,28],'min_samples_leaf':[1,10,100],'random_state':[0]} \n\nreg = GridSearchCV(ExtraTreesRegressor(), tuned_parameters, cv=5, scoring='r2')\nreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3e16a4b890b1fab25128199aa10a878762acb16"},"cell_type":"code","source":"print('The best model is: ', reg.best_params_)\nprint('This model produces a mean cross-validated score (r2) of', reg.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d3e5aabb31d21562a8c20f2ca136b4b888c6f57"},"cell_type":"code","source":"y_pred = reg.predict(X_test)\nprint(\"The r2 score on the test set is: \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b46337216577aea4fa2c8e30b49e04cba01c136"},"cell_type":"markdown","source":"## Summary of performance\n| Classifier name | r2 score | Comments |\n|------|------|------|------|------|\n| 500 linear regressors with Random Patches | 0.35 | BaggingRegressor() |\n| Random Forest with 500 trees (with GridSearchCV)  | 0.48| RandomForestRegressor() |\n| Extra Trees with 500 trees (with GridSearchCV)| **0.51** | ExtraTreesRegressor() |"},{"metadata":{"_uuid":"56e4ab9aa7e3e9e42a56ebebbc17202a4975e3c9"},"cell_type":"markdown","source":"* The ensemble linear regressors perform the worst. This is expected since the linear assumption probably is not sufficient for this problem.\n* Both Random Forest and Extra Trees perform much better than the ensemble linear regressors, with Extra Trees slightly better than Random Forest.\n* However, no regressor achieves a satisfying r2 score. Let's plot the y_test and y_pred from the Extra Trees regressor:"},{"metadata":{"trusted":true,"_uuid":"6c5b14ca22ff822136fbeddbc64a4e741dfaf58f"},"cell_type":"code","source":"plt.plot(y_test,y_pred, linestyle='',marker='o')\nplt.xlabel('true y values', fontsize = 14)\nplt.ylabel('predicited y values', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"630100b54e24d0709fe1d3ddf7d697744a60e3cf"},"cell_type":"markdown","source":"In the next tutorial, we will talk about advanced ensemble learning techniques such as boosting and stacking. We will revisit the classification and regression tasks that we have done in this tutorial and see if the advanced techniques can push the performance forward. I hope you find this tutorial useful, and please let me know if you have any questions or comments. See you next time!\n\n-------------------\n1st version published: 12/31/2018"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}