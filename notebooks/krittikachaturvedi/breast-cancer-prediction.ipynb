{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include = \"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"Unnamed: 32\"],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diag_group = df.groupby(\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of rows diagnosed malignant: \", diag_group.get_group(\"M\").shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of rows diagnosed benign: \",diag_group.get_group(\"B\").shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"id\"],axis = 1)\n#Encoding categorical data values\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf.diagnosis = le.fit_transform(df.diagnosis)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding correlation between the features\nimport seaborn as sns\ncorr = df.corr()\nsns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill true value array with shape of corr.shape[0]\n#print(np.full(corr.shape[0],True, dtype=bool))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare correlation between the features and remove features that have correlation higher than 0.9\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = df.columns[columns]\ndf = df[selected_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will calculate p-value for every predictor variable in regression model for given dataset and we can conclude from \n#p-value that if p-value is greater than 0.05 then that feature don't have any significant contribution to predict \n#cancer type . Hence the feature will be removed.\n\nX = df.iloc[:,1:]\nY = df.iloc[:,0]\nimport statsmodels.api as sm\nmod = sm.OLS(Y,X)\nfii = mod.fit()\np_values = fii.summary2().tables[1]['P>|t|']\np_values = pd.DataFrame({'feature': p_values.index, 'PVal': p_values.values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.05\nfor i in range(len(p_values)):\n    if p_values.iloc[i].PVal > threshold:\n        df = df.drop(p_values.iloc[i].feature, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers  Identification"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#radius_mean\nsns.boxplot(x = df['concavity_mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier = df[df['concavity_mean'] > 0.25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['concavity_mean'] < 0.25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['concavity_mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['radius_mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier = df[df['radius_mean'] > 21]\ndf = df[df['radius_mean'] < 21]\nsns.boxplot(x = df['radius_mean'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Application"},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the dataset\nX = df.drop('diagnosis', axis = 1)\nY = df.diagnosis\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.6, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression Algorithm \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nclf = LogisticRegression(random_state = 0)\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\nclf_scores = []\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nlist1 = []\nfor neighbors in range(1,7):\n    clf = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    list1.append(accuracy_score(Y_test,Y_pred))\nplt.plot(list(range(1,7)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors = 6)\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor c in [0.5,0.6,0.7,0.8,0.9,1.0]:\n    clf = SVC(C = c, random_state=0, kernel = 'rbf')\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    list1.append(accuracy_score(Y_test,Y_pred))\nplt.plot([0.5,0.6,0.7,0.8,0.9,1.0], list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC(C = 0.8, random_state=0, kernel = 'rbf')\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,10):\n    clf = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    list1.append(accuracy_score(Y_test,Y_pred))\n#print(mylist)\nplt.plot(list(range(2,10)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(max_leaf_nodes = 8, random_state=0, criterion='entropy')\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nx = clf_scores\ny = [\"LogisticRegression\", \"KNN\", \"SVM\", \"DecisionTree\"]\nplt.bar( y,x,color=['aqua', 'coral', 'gold', 'orchid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}