{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n\nThis Notebook introduces use of BERT for tokenization task in a solution for text classification.\n\nThe dataset used here is [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)","metadata":{}},{"cell_type":"markdown","source":"# Analysis preparation","metadata":{}},{"cell_type":"markdown","source":"## Install and import libraries","metadata":{}},{"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport re\nimport random\nimport math\n\ntry:\n    %tensorflow_version 2.x\nexcept Exception as ex:\n    print(ex)\n    pass\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Input data","metadata":{}},{"cell_type":"code","source":"movie_reviews = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\nprint(f\"Null: {movie_reviews.isnull().values.any()}\")\nprint(f\"shape: {movie_reviews.shape}\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"We prepare the text for the classification. The text preprocessing includes the following:\n* Remove html tags;  \n* Remove punctuations and numbers;  \n* Remove single character words;  \n* Remove multiple spaces.  ","metadata":{}},{"cell_type":"code","source":"TAG_RE = re.compile(r'<[^>]+>')\ndef remove_tags(text):\n    \"\"\"\n    Remove html tags\n    \"\"\"\n    return TAG_RE.sub('', text)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(sen):\n    \"\"\"\n    Remove html tags\n    Remove punctuations and numbers\n    Remove single character words\n    Remove multiple spaces\n    \"\"\"\n    # Removing html tags\n    sentence = remove_tags(sen)\n\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We apply the preprocessing to all sentences.","metadata":{}},{"cell_type":"code","source":"reviews = []\nsentences = list(movie_reviews['review'])\nfor sen in sentences:\n    reviews.append(preprocess_text(sen))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(movie_reviews.columns.values)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie_reviews.sentiment.unique()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We transform the target feature, from {'positive', 'negative'} to {1, 0}","metadata":{}},{"cell_type":"code","source":"y = movie_reviews['sentiment']\ny = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check text and target features","metadata":{}},{"cell_type":"code","source":"print(f\"Review sample:\\n {reviews[10]}\")\nprint(f\"Review sentiment: {y[10]}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Tokenization using BERT","metadata":{}},{"cell_type":"markdown","source":"We use `BertTokenizer` (BERT uncased) from bert.\nWe initialize BertTokenizer with vocabulary file and option to lower case.","metadata":{}},{"cell_type":"code","source":"BertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n                            trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check tokenizer","metadata":{}},{"cell_type":"code","source":"print(tokenizer.tokenize(\"don't try to be so sentimental or so judgemental\"))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"don't try to be so sentimental or so judgemental\")))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aplly tokenizer to data","metadata":{}},{"cell_type":"code","source":"def tokenize_reviews(text_reviews):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_reviews = [tokenize_reviews(review) for review in reviews]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After tokenization, we add to the transformed data as well the length for each review.","metadata":{}},{"cell_type":"code","source":"reviews_with_len = [[review, y[i], len(review)]\n                 for i, review in enumerate(tokenized_reviews)]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(reviews_with_len)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sort reviews on length.","metadata":{}},{"cell_type":"code","source":"reviews_with_len.sort(key=lambda x: x[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nbatched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(iter(batched_dataset))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-test split\n\n\nWe split the dataset in train-test, reserving 10% of the data for test, 90% in used for train.","metadata":{}},{"cell_type":"code","source":"TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\nTEST_BATCHES = TOTAL_BATCHES // 10\nbatched_dataset.shuffle(TOTAL_BATCHES)\ntest_data = batched_dataset.take(TEST_BATCHES)\ntrain_data = batched_dataset.skip(TEST_BATCHES)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define model","metadata":{}},{"cell_type":"code","source":"class TextClassificationModel(tf.keras.Model):\n    \n    def __init__(self,\n                 vocabulary_size,\n                 embedding_dimensions=128,\n                 cnn_filters=50,\n                 dnn_units=512,\n                 model_output_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name=\"text_model\"):\n        super(TextClassificationModel, self).__init__(name=name)\n        \n        self.embedding = layers.Embedding(vocabulary_size,\n                                          embedding_dimensions)\n        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=2,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=3,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=4,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        \n        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if model_output_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=model_output_classes,\n                                           activation=\"softmax\")\n    \n    def call(self, inputs, training):\n        l = self.embedding(inputs)\n        l_1 = self.cnn_layer1(l) \n        l_1 = self.pool(l_1) \n        l_2 = self.cnn_layer2(l) \n        l_2 = self.pool(l_2)\n        l_3 = self.cnn_layer3(l)\n        l_3 = self.pool(l_3) \n        \n        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n        concatenated = self.dense_1(concatenated)\n        concatenated = self.dropout(concatenated, training)\n        model_output = self.last_dense(concatenated)\n        \n        return model_output","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"VOCAB_LENGTH = len(tokenizer.vocab)\nEMB_DIM = 200\nCNN_FILTERS = 100\nDNN_UNITS = 256\nOUTPUT_CLASSES = 2\n\nDROPOUT_RATE = 0.2\n\nNB_EPOCHS = 5","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_model = TextClassificationModel(vocabulary_size=VOCAB_LENGTH,\n                        embedding_dimensions=EMB_DIM,\n                        cnn_filters=CNN_FILTERS,\n                        dnn_units=DNN_UNITS,\n                        model_output_classes=OUTPUT_CLASSES,\n                        dropout_rate=DROPOUT_RATE)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if OUTPUT_CLASSES == 2:\n    text_model.compile(loss=\"binary_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"accuracy\"])\nelse:\n    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"sparse_categorical_accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_model.fit(train_data, epochs=NB_EPOCHS)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation using test set","metadata":{}},{"cell_type":"code","source":"results = text_model.evaluate(test_data)\nprint(f\"Test evaluation results: {results}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}