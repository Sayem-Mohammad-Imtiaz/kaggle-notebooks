{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.014594,"end_time":"2020-08-07T20:09:40.810659","exception":false,"start_time":"2020-08-07T20:09:40.796065","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Tensorflow/Keras/GPU for Chinese MNIST Prediction</font></center></h1>\n\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Prepare the data analysis</a>   \n- <a href='#3'>Data exploration</a>   \n- <a href='#4'>Characters classification</a>       \n- <a href='#5'>Conclusions</a>       \n"},{"metadata":{"_uuid":"a8e77ace65f04c89a878bf18249e4d8e23fec996","papermill":{"duration":0.012542,"end_time":"2020-08-07T20:09:40.836833","exception":false,"start_time":"2020-08-07T20:09:40.824291","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\n\nThe objective of the Kernel is to take us through the steps of a machine learning analysis.   \n\n\nWe will use a dataset with adnotated images of Chinese numbers, handwritten by a number of 100 volunteers, each providing a number of 10 samples, each sample with a complete set of 15 Chinese characters for numbers.\n\nThe Chinese characters are the following:\n* 零 - for 0  \n* 一 - for 1\n* 二 - for 2  \n* 三 - for 3  \n* 四 - for 4  \n* 五 - for 5  \n* 六 - for 6  \n* 七 - for 7  \n* 八 - for 8  \n* 九 - for 9  \n* 十 - for 10\n* 百 - for 100\n* 千 - for 1000\n* 万 - for 10 thousands\n* 亿 - for 100 millions\n\n\n\nWe start by preparing the analysis (load the libraries and the data), continue with an Exploratory Data Analysis (EDA).\n\nWe follow then with features engineering and preparation for creation of a model. The dataset is split in training, validation and test set. \n\nWe run a model using Tensorflow through Keras interface, with GPU acceleration, using as well Dropouts, variable learning speed and early stoping based on variation of validation error accuracy.\n\nAt the end, we use the best model to predict for the test set.\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>  "},{"metadata":{"_uuid":"4e97555eb77978a29a51c41f39cec67136b18157","papermill":{"duration":0.012218,"end_time":"2020-08-07T20:09:40.861797","exception":false,"start_time":"2020-08-07T20:09:40.849579","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# <a id='2'>Prepare the data analysis</a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n\n"},{"metadata":{"_uuid":"cb2e73fe056a3dda7eb48eeac2facf0c441816d1","papermill":{"duration":0.01234,"end_time":"2020-08-07T20:09:40.886804","exception":false,"start_time":"2020-08-07T20:09:40.874464","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# <a id='21'>Load packages</a>\n\nWe load the packages used for the analysis.\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"af08260bfbe163f9132f39d09627899bbc4c1dae","execution":{"iopub.execute_input":"2020-08-07T20:09:40.923713Z","iopub.status.busy":"2020-08-07T20:09:40.923016Z","iopub.status.idle":"2020-08-07T20:09:51.294918Z","shell.execute_reply":"2020-08-07T20:09:51.294151Z"},"papermill":{"duration":10.395493,"end_time":"2020-08-07T20:09:51.29505","exception":false,"start_time":"2020-08-07T20:09:40.899557","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\nimport tensorflow_addons as tfa\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also set a number of parameters for the data and model."},{"metadata":{"_kg_hide-input":true,"_uuid":"a2082fb1e56fc6cfc91d40820b905267bc1ca468","execution":{"iopub.execute_input":"2020-08-07T20:09:51.3284Z","iopub.status.busy":"2020-08-07T20:09:51.327572Z","iopub.status.idle":"2020-08-07T20:09:51.33057Z","shell.execute_reply":"2020-08-07T20:09:51.329925Z"},"papermill":{"duration":0.02261,"end_time":"2020-08-07T20:09:51.33069","exception":false,"start_time":"2020-08-07T20:09:51.30808","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"IMAGE_PATH = '..//input//chinese-mnist//data//data//'\nIMAGE_WIDTH = 64\nIMAGE_HEIGHT = 64\nIMAGE_CHANNELS = 1\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nVAL_SIZE = 0.2\nCONV_2D_DIM_1 = 16\nCONV_2D_DIM_2 = 16\nCONV_2D_DIM_3 = 32\nCONV_2D_DIM_4 = 64\nMAX_POOL_DIM = 2\nKERNEL_SIZE = 3\nBATCH_SIZE = 32\nNO_EPOCHS = 50\nDROPOUT_RATIO = 0.5\nPATIENCE = 5\nVERBOSE = 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"307f656565365ff05faf226e5a447875dd0dfead","papermill":{"duration":0.012243,"end_time":"2020-08-07T20:09:51.355989","exception":false,"start_time":"2020-08-07T20:09:51.343746","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n# <a id='22'>Load the data</a>  \n\nLet's see first what data files do we have in the root directory."},{"metadata":{"_kg_hide-input":true,"_uuid":"9f1df6658b17558179d8a9016f544410de16c354","execution":{"iopub.execute_input":"2020-08-07T20:09:51.386466Z","iopub.status.busy":"2020-08-07T20:09:51.385812Z","iopub.status.idle":"2020-08-07T20:09:51.396165Z","shell.execute_reply":"2020-08-07T20:09:51.395466Z"},"papermill":{"duration":0.02758,"end_time":"2020-08-07T20:09:51.396292","exception":false,"start_time":"2020-08-07T20:09:51.368712","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"os.listdir(\"..//input//chinese-mnist\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"241b8735a85a25e16421fda8c35bc3d3c69e7ea8","papermill":{"duration":0.012554,"end_time":"2020-08-07T20:09:51.421745","exception":false,"start_time":"2020-08-07T20:09:51.409191","status":"completed"},"tags":[]},"cell_type":"markdown","source":"There is a dataset file and a folder with images.  \n\nLet's load the dataset file first."},{"metadata":{"_kg_hide-input":true,"_uuid":"d7b9f11a014428e56e422d97a5b3ef70efec007e","execution":{"iopub.execute_input":"2020-08-07T20:09:51.454208Z","iopub.status.busy":"2020-08-07T20:09:51.453147Z","iopub.status.idle":"2020-08-07T20:09:51.475687Z","shell.execute_reply":"2020-08-07T20:09:51.474939Z"},"papermill":{"duration":0.041137,"end_time":"2020-08-07T20:09:51.475823","exception":false,"start_time":"2020-08-07T20:09:51.434686","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data_df=pd.read_csv('..//input//chinese-mnist//chinese_mnist.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22b3984ccc3e29daaf77a796d9d7966cd798e1a8","papermill":{"duration":0.012421,"end_time":"2020-08-07T20:09:51.501438","exception":false,"start_time":"2020-08-07T20:09:51.489017","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's glimpse the data. First, let's check the number of columns and rows."},{"metadata":{"_kg_hide-input":true,"_uuid":"535f3f9cea3b26428bec3ede4ed49009bdb91889","execution":{"iopub.execute_input":"2020-08-07T20:09:51.532948Z","iopub.status.busy":"2020-08-07T20:09:51.531954Z","iopub.status.idle":"2020-08-07T20:09:51.535764Z","shell.execute_reply":"2020-08-07T20:09:51.536226Z"},"papermill":{"duration":0.022162,"end_time":"2020-08-07T20:09:51.53641","exception":false,"start_time":"2020-08-07T20:09:51.514248","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b4405ddcce03ee722f05234d508188997817f8d","papermill":{"duration":0.012557,"end_time":"2020-08-07T20:09:51.562194","exception":false,"start_time":"2020-08-07T20:09:51.549637","status":"completed"},"tags":[]},"cell_type":"markdown","source":"There are 15000 rows and 5 columns. Let's look to the data."},{"metadata":{"_kg_hide-input":true,"_uuid":"4d326f747f0a14580b20c2e034e6c3368edcd18b","execution":{"iopub.execute_input":"2020-08-07T20:09:51.605005Z","iopub.status.busy":"2020-08-07T20:09:51.604346Z","iopub.status.idle":"2020-08-07T20:09:51.613556Z","shell.execute_reply":"2020-08-07T20:09:51.61287Z"},"papermill":{"duration":0.038128,"end_time":"2020-08-07T20:09:51.613686","exception":false,"start_time":"2020-08-07T20:09:51.575558","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data_df.sample(100).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c97047b17cda76e346e444229485ac91ec966423","papermill":{"duration":0.013516,"end_time":"2020-08-07T20:09:51.640502","exception":false,"start_time":"2020-08-07T20:09:51.626986","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The data contains the following values:  \n\n* suite_id - each suite corresponds to a set of handwritten samples by one volunteer;  \n* sample_id - each sample wil contain a complete set of 15 characters for Chinese numbers;\n* code - for each Chinese character we are using a code, with values from 1 to 15;\n* value - this is the actual numerical value associated with the Chinese character for number;  \n* character - the Chinese character;  \n\nWe index the files in the dataset by forming a file name from suite_id, sample_id and code. The pattern for a file is as following:\n\n> \"input_{suite_id}_{sample_id}_{code}.jpg\""},{"metadata":{"_uuid":"55dd26f919decca9d67daec9895a5d9e11f1d28b","papermill":{"duration":0.012864,"end_time":"2020-08-07T20:09:51.666722","exception":false,"start_time":"2020-08-07T20:09:51.653858","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n# <a id='3'>Data exploration</a>  \n\n\n\nLet's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. \n"},{"metadata":{"_uuid":"14443450ba96e12ad8e18ce4dd1779f18d5f914b","papermill":{"duration":0.012823,"end_time":"2020-08-07T20:09:51.692734","exception":false,"start_time":"2020-08-07T20:09:51.679911","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id='31'>Check for missing data</a>  \n\nLet's create a function that check for missing data in the dataset."},{"metadata":{"_kg_hide-input":true,"_uuid":"4544dd470d743c54f815faaee863038ad5e8398f","execution":{"iopub.execute_input":"2020-08-07T20:09:51.729596Z","iopub.status.busy":"2020-08-07T20:09:51.72686Z","iopub.status.idle":"2020-08-07T20:09:51.765828Z","shell.execute_reply":"2020-08-07T20:09:51.76514Z"},"papermill":{"duration":0.060032,"end_time":"2020-08-07T20:09:51.765951","exception":false,"start_time":"2020-08-07T20:09:51.705919","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(data_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cb0410e8b9afd75ac7b50d0489d90eda6e1b109","papermill":{"duration":0.012988,"end_time":"2020-08-07T20:09:51.79229","exception":false,"start_time":"2020-08-07T20:09:51.779302","status":"completed"},"tags":[]},"cell_type":"markdown","source":"There is no missing (null) data in the dataset. Still it might be that some of the data labels are misspelled; we will check this when we will analyze each data feature."},{"metadata":{"_uuid":"1fbab44688fb2ab073aac8f964e534f90ce1dfff","papermill":{"duration":0.013024,"end_time":"2020-08-07T20:09:51.818796","exception":false,"start_time":"2020-08-07T20:09:51.805772","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='32'>Explore image data</a>  \n\nLet's also check the image data. First, we check how many images are stored in the image folder."},{"metadata":{"_kg_hide-input":true,"_uuid":"46f15681887fa82ab13224e52df69d91119fc9ad","execution":{"iopub.execute_input":"2020-08-07T20:09:51.85147Z","iopub.status.busy":"2020-08-07T20:09:51.850595Z","iopub.status.idle":"2020-08-07T20:09:51.999006Z","shell.execute_reply":"2020-08-07T20:09:51.998386Z"},"papermill":{"duration":0.166601,"end_time":"2020-08-07T20:09:51.999147","exception":false,"start_time":"2020-08-07T20:09:51.832546","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"image_files = list(os.listdir(IMAGE_PATH))\nprint(\"Number of image files: {}\".format(len(image_files)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68523860593e9a64059b51d40a316454e6937a68","papermill":{"duration":0.013103,"end_time":"2020-08-07T20:09:52.026367","exception":false,"start_time":"2020-08-07T20:09:52.013264","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's also check that each line in the dataset has a corresponding image in the image list.  \nFirst, we will have to compose the name of the file from the indexes."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-07T20:09:52.058656Z","iopub.status.busy":"2020-08-07T20:09:52.05795Z","iopub.status.idle":"2020-08-07T20:09:52.061233Z","shell.execute_reply":"2020-08-07T20:09:52.060619Z"},"papermill":{"duration":0.021652,"end_time":"2020-08-07T20:09:52.061376","exception":false,"start_time":"2020-08-07T20:09:52.039724","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def create_file_name(x):\n    \n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-07T20:09:52.09392Z","iopub.status.busy":"2020-08-07T20:09:52.093262Z","iopub.status.idle":"2020-08-07T20:09:52.834502Z","shell.execute_reply":"2020-08-07T20:09:52.833383Z"},"papermill":{"duration":0.759655,"end_time":"2020-08-07T20:09:52.834697","exception":false,"start_time":"2020-08-07T20:09:52.075042","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data_df[\"file\"] = data_df.apply(create_file_name, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"457cd17212904bb96f86ec1770cbdbefc5ffb395","execution":{"iopub.execute_input":"2020-08-07T20:09:52.878138Z","iopub.status.busy":"2020-08-07T20:09:52.877358Z","iopub.status.idle":"2020-08-07T20:09:52.880765Z","shell.execute_reply":"2020-08-07T20:09:52.881212Z"},"papermill":{"duration":0.030408,"end_time":"2020-08-07T20:09:52.881409","exception":false,"start_time":"2020-08-07T20:09:52.851001","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"file_names = list(data_df['file'])\nprint(\"Matching image names: {}\".format(len(set(file_names).intersection(image_files))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b20cd791ede3f23d0c9275aafc75827b9424df4","papermill":{"duration":0.013486,"end_time":"2020-08-07T20:09:52.908457","exception":false,"start_time":"2020-08-07T20:09:52.894971","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's also check the image sizes."},{"metadata":{"_kg_hide-input":true,"_uuid":"64f4416a8e20197d60f7dbc9dd41a5e73049bfd0","execution":{"iopub.execute_input":"2020-08-07T20:09:52.941441Z","iopub.status.busy":"2020-08-07T20:09:52.940499Z","iopub.status.idle":"2020-08-07T20:09:52.94367Z","shell.execute_reply":"2020-08-07T20:09:52.943156Z"},"papermill":{"duration":0.021537,"end_time":"2020-08-07T20:09:52.943792","exception":false,"start_time":"2020-08-07T20:09:52.922255","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def read_image_sizes(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    return list(image.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c72a7d125efb51e00a58554692dbd99adc74b55","execution":{"iopub.execute_input":"2020-08-07T20:09:52.977811Z","iopub.status.busy":"2020-08-07T20:09:52.977099Z","iopub.status.idle":"2020-08-07T20:10:20.440036Z","shell.execute_reply":"2020-08-07T20:10:20.439377Z"},"papermill":{"duration":27.482666,"end_time":"2020-08-07T20:10:20.440168","exception":false,"start_time":"2020-08-07T20:09:52.957502","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = np.stack(data_df['file'].apply(read_image_sizes))\ndf = pd.DataFrame(m,columns=['w','h'])\ndata_df = pd.concat([data_df,df],axis=1, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-07T20:10:20.481248Z","iopub.status.busy":"2020-08-07T20:10:20.480289Z","iopub.status.idle":"2020-08-07T20:10:20.485364Z","shell.execute_reply":"2020-08-07T20:10:20.484664Z"},"papermill":{"duration":0.031369,"end_time":"2020-08-07T20:10:20.485489","exception":false,"start_time":"2020-08-07T20:10:20.45412","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2b88af0c239ca3d9e37e159889836a4f38913c8","papermill":{"duration":0.01395,"end_time":"2020-08-07T20:10:20.513971","exception":false,"start_time":"2020-08-07T20:10:20.500021","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id='33'>Suites</a>  \n\nLet's check the suites of the images. For this, we will group by `suite`."},{"metadata":{"_kg_hide-input":true,"_uuid":"6f1c39d0398275215f92f61542544132a0d574a0","execution":{"iopub.execute_input":"2020-08-07T20:10:20.552206Z","iopub.status.busy":"2020-08-07T20:10:20.551433Z","iopub.status.idle":"2020-08-07T20:10:20.556355Z","shell.execute_reply":"2020-08-07T20:10:20.555749Z"},"papermill":{"duration":0.027877,"end_time":"2020-08-07T20:10:20.556477","exception":false,"start_time":"2020-08-07T20:10:20.5286","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(f\"Number of suites: {data_df.suite_id.nunique()}\")\nprint(f\"Samples: {data_df.sample_id.unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd421a7d1872af204c26588d1a15eaddca08a396","papermill":{"duration":0.013884,"end_time":"2020-08-07T20:10:20.584461","exception":false,"start_time":"2020-08-07T20:10:20.570577","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We have 100 suites, each with 10 samples. This means a total of 15K images with Chinese characters."},{"metadata":{"_uuid":"c2a5e2401b418f1723c859ee9e0b4ad5071e4a82","papermill":{"duration":0.01388,"end_time":"2020-08-07T20:10:20.612504","exception":false,"start_time":"2020-08-07T20:10:20.598624","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# <a id='4'>Characters classification</a>\n\nOur objective is to use the images that we investigated until now to correctly identify the Chinese numbers (characters).   \n\nWe have a unique dataset and we will have to split this dataset in **train** and **test**. The **train** set will be used for training a model and the test will be used for testing the model accuracy against new, fresh data, not used in training.\n\n"},{"metadata":{"_uuid":"e8c0a6df4bb85bcdf90f7c908decab07304d660f","papermill":{"duration":0.013817,"end_time":"2020-08-07T20:10:20.640532","exception":false,"start_time":"2020-08-07T20:10:20.626715","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id='40'>Split the data</a>  \n\nFirst, we split the whole dataset in train and test. We will use **random_state** to ensure reproductibility of results. We also use **stratify** to ensure balanced train/validation/test sets with respect of the labels. \n\nThe train-test split is **80%** for training set and **20%** for test set.\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"352d452d5212d8c9eff074f11820b03a0d44387b","execution":{"iopub.execute_input":"2020-08-07T20:10:20.674301Z","iopub.status.busy":"2020-08-07T20:10:20.672984Z","iopub.status.idle":"2020-08-07T20:10:20.681811Z","shell.execute_reply":"2020-08-07T20:10:20.681155Z"},"papermill":{"duration":0.02728,"end_time":"2020-08-07T20:10:20.681946","exception":false,"start_time":"2020-08-07T20:10:20.654666","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=data_df[\"code\"].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"856060cc500db00e472b7755c91aba20c953a5f6","papermill":{"duration":0.013843,"end_time":"2020-08-07T20:10:20.70994","exception":false,"start_time":"2020-08-07T20:10:20.696097","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Next, we will split further the **train** set in **train** and **validation**. We want to use as well a validation set to be able to measure not only how well fits the model the train data during training (or how well `learns` the training data) but also how well the model is able to generalize so that we are able to understands not only the bias but also the variance of the model.  \n\nThe train-validation split is **80%** for training set and **20%** for validation set."},{"metadata":{"_kg_hide-input":true,"_uuid":"83d0be04ae5a4ad5834631bf18e21917d6313bcd","execution":{"iopub.execute_input":"2020-08-07T20:10:20.792734Z","iopub.status.busy":"2020-08-07T20:10:20.79206Z","iopub.status.idle":"2020-08-07T20:10:20.798292Z","shell.execute_reply":"2020-08-07T20:10:20.797636Z"},"papermill":{"duration":0.074293,"end_time":"2020-08-07T20:10:20.798435","exception":false,"start_time":"2020-08-07T20:10:20.724142","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=train_df[\"code\"].values)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"0dcaa8c2c5423ab8fc2898d4a4aa937801592c2c","papermill":{"duration":0.013803,"end_time":"2020-08-07T20:10:20.826467","exception":false,"start_time":"2020-08-07T20:10:20.812664","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's check the shape of the three datasets."},{"metadata":{"_kg_hide-input":true,"_uuid":"8247f70b4deb4600fe322f004733234ed37617f0","execution":{"iopub.execute_input":"2020-08-07T20:10:20.861599Z","iopub.status.busy":"2020-08-07T20:10:20.860807Z","iopub.status.idle":"2020-08-07T20:10:20.865892Z","shell.execute_reply":"2020-08-07T20:10:20.865304Z"},"papermill":{"duration":0.025101,"end_time":"2020-08-07T20:10:20.866012","exception":false,"start_time":"2020-08-07T20:10:20.840911","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(\"Train set rows: {}\".format(train_df.shape[0]))\nprint(\"Test  set rows: {}\".format(test_df.shape[0]))\nprint(\"Val   set rows: {}\".format(val_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee768c083f40fcbd109425182bc55ce86173b69d","papermill":{"duration":0.013954,"end_time":"2020-08-07T20:10:20.894386","exception":false,"start_time":"2020-08-07T20:10:20.880432","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We are now ready to start building our first model."},{"metadata":{"_uuid":"d76e822dd76565d29fcfed323cb034939f307581","papermill":{"duration":0.014261,"end_time":"2020-08-07T20:10:20.923053","exception":false,"start_time":"2020-08-07T20:10:20.908792","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id='41'>Build the model</a>    \n\n\nNext step in our creation of a predictive model.  \n\nLet's define few auxiliary functions that we will need for creation of our models.\n\nA function for reading images from the image files, scale all images to 100 x 100 x 3 (channels)."},{"metadata":{"_kg_hide-input":true,"_uuid":"f80b4e20e98ce5bf328fba3a22457c4a994de06b","execution":{"iopub.execute_input":"2020-08-07T20:10:20.957996Z","iopub.status.busy":"2020-08-07T20:10:20.957281Z","iopub.status.idle":"2020-08-07T20:10:20.960391Z","shell.execute_reply":"2020-08-07T20:10:20.959742Z"},"papermill":{"duration":0.023116,"end_time":"2020-08-07T20:10:20.960514","exception":false,"start_time":"2020-08-07T20:10:20.937398","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def read_image(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    image = skimage.transform.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT, 1), mode='reflect')\n    return image[:,:,:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e396e0cd23633af2169e4d50985f1987654205a9","papermill":{"duration":0.013964,"end_time":"2020-08-07T20:10:20.988843","exception":false,"start_time":"2020-08-07T20:10:20.974879","status":"completed"},"tags":[]},"cell_type":"markdown","source":"A function to create the dummy variables corresponding to the categorical target variable."},{"metadata":{"_kg_hide-input":true,"_uuid":"0f7a2146ca93aef9367ecd64300980005d89911b","execution":{"iopub.execute_input":"2020-08-07T20:10:21.023803Z","iopub.status.busy":"2020-08-07T20:10:21.02284Z","iopub.status.idle":"2020-08-07T20:10:21.026166Z","shell.execute_reply":"2020-08-07T20:10:21.025547Z"},"papermill":{"duration":0.023137,"end_time":"2020-08-07T20:10:21.026292","exception":false,"start_time":"2020-08-07T20:10:21.003155","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def categories_encoder(dataset, var='character'):\n    X = np.stack(dataset['file'].apply(read_image))\n    y = pd.get_dummies(dataset[var], drop_first=False)\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"b40c205d5189b23cbbc4ef0cda8798721d504ff9","papermill":{"duration":0.014169,"end_time":"2020-08-07T20:10:21.055376","exception":false,"start_time":"2020-08-07T20:10:21.041207","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's populate now the train, val and test sets with the image data and create the  dummy variables corresponding to the categorical target variable, in our case `subspecies`."},{"metadata":{"_kg_hide-input":true,"_uuid":"70acefcd6dc5d494b1c7db6dc90bae5f8c856d94","execution":{"iopub.execute_input":"2020-08-07T20:10:21.090594Z","iopub.status.busy":"2020-08-07T20:10:21.089629Z","iopub.status.idle":"2020-08-07T20:10:55.249319Z","shell.execute_reply":"2020-08-07T20:10:55.249886Z"},"papermill":{"duration":34.180192,"end_time":"2020-08-07T20:10:55.250096","exception":false,"start_time":"2020-08-07T20:10:21.069904","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X_train, y_train = categories_encoder(train_df)\nX_val, y_val = categories_encoder(val_df)\nX_test, y_test = categories_encoder(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5093354dc9c7f0510ba54a254690db45e38d0bcc","papermill":{"duration":0.014154,"end_time":"2020-08-07T20:10:55.279273","exception":false,"start_time":"2020-08-07T20:10:55.265119","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now we are ready to start creating our model.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv2D(CONV_2D_DIM_1, kernel_size=KERNEL_SIZE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS), activation='relu', padding='same'))\nmodel.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\nmodel.add(MaxPool2D(MAX_POOL_DIM))\nmodel.add(Dropout(DROPOUT_RATIO))\nmodel.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\nmodel.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\nmodel.add(Dropout(DROPOUT_RATIO))\nmodel.add(Flatten())\nmodel.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"046612eda2408801ded03cc6a5e2357a99298969","execution":{"iopub.execute_input":"2020-08-07T20:10:55.536197Z","iopub.status.busy":"2020-08-07T20:10:55.535375Z","iopub.status.idle":"2020-08-07T20:10:55.544655Z","shell.execute_reply":"2020-08-07T20:10:55.545136Z"},"papermill":{"duration":0.029315,"end_time":"2020-08-07T20:10:55.545308","exception":false,"start_time":"2020-08-07T20:10:55.515993","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0149020f748dc9eaa89ebf732829031d6d9d35a2","papermill":{"duration":0.015031,"end_time":"2020-08-07T20:10:55.575855","exception":false,"start_time":"2020-08-07T20:10:55.560824","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We are using the predefined epoch number for this experiment (50 steps).\n\nThe modes uses 2 Convolutional layers, followed by a MaxPool, a Dropout, then another 2 Convolutional layers and a Dropout. Then follows a Flatten and a Dense layer. We compile the model with **Adam** optimizer and use a categorical crossentropy loss functions. The metric used is accuracy.\n\nWe are using as well a learning function with variable learning rate (depends on the epoch number). \n\nAt each training epoch, we evaluate the validation error and, based on its evolution, we decide if we stop the training or continue (with a prededined `patience` factor - i.e. we only stop if validation is not improving for a certain number of steps (we set the patience to 5 steps). If at a certain step the validation error is improving, we save the current model. We then will load the best model and use it for prediction of test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** (x+NO_EPOCHS))\nearlystopper = EarlyStopping(monitor='loss', patience=PATIENCE, verbose=VERBOSE)\ncheckpointer = ModelCheckpoint('best_model.h5',\n                                monitor='val_accuracy',\n                                verbose=VERBOSE,\n                                save_best_only=True,\n                                save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-07T20:10:55.61515Z","iopub.status.busy":"2020-08-07T20:10:55.614209Z","iopub.status.idle":"2020-08-07T20:11:52.972263Z","shell.execute_reply":"2020-08-07T20:11:52.972845Z"},"papermill":{"duration":57.381381,"end_time":"2020-08-07T20:11:52.973032","exception":false,"start_time":"2020-08-07T20:10:55.591651","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_model  = model.fit(X_train, y_train,\n                  batch_size=BATCH_SIZE,\n                  epochs=NO_EPOCHS,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[earlystopper, checkpointer, annealer])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5837b3e9cb131fab2c58f4b9de92f147f48b59ae","papermill":{"duration":0.046111,"end_time":"2020-08-07T20:11:53.066967","exception":false,"start_time":"2020-08-07T20:11:53.020856","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='42'>Model evaluation</a> \n\n\nLet's start by plotting the loss error for the train and validation set. \nWe define a function to visualize these values."},{"metadata":{"_kg_hide-input":true,"_uuid":"a87e3beea44a87a806893b798a38d26904d10718","execution":{"iopub.execute_input":"2020-08-07T20:11:53.174105Z","iopub.status.busy":"2020-08-07T20:11:53.173163Z","iopub.status.idle":"2020-08-07T20:11:54.228966Z","shell.execute_reply":"2020-08-07T20:11:54.228416Z"},"papermill":{"duration":1.115302,"end_time":"2020-08-07T20:11:54.229093","exception":false,"start_time":"2020-08-07T20:11:53.113791","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def create_trace(x,y,ylabel,color):\n        trace = go.Scatter(\n            x = x,y = y,\n            name=ylabel,\n            marker=dict(color=color),\n            mode = \"markers+lines\",\n            text=x\n        )\n        return trace\n    \ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['accuracy']\n    val_acc = hist['val_accuracy']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = list(range(1,len(acc)+1))\n    #define the traces\n    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n                                                             'Training and validation loss'))\n    #add traces to the figure\n    fig.append_trace(trace_ta,1,1)\n    fig.append_trace(trace_va,1,1)\n    fig.append_trace(trace_tl,1,2)\n    fig.append_trace(trace_vl,1,2)\n    #set the layout for the figure\n    fig['layout']['xaxis'].update(title = 'Epoch')\n    fig['layout']['xaxis2'].update(title = 'Epoch')\n    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n    #plot\n    iplot(fig, filename='accuracy-loss')\n\nplot_accuracy_and_loss(train_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='43'>Prediction of test set</a> "},{"metadata":{"_uuid":"a77c0127288f090233e70831e6b909c1618efa35","papermill":{"duration":0.052348,"end_time":"2020-08-07T20:11:54.334909","exception":false,"start_time":"2020-08-07T20:11:54.282561","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\nLet's continue by evaluating the **test** set **loss** and **accuracy**. We will use here the test set."},{"metadata":{},"cell_type":"markdown","source":"### Predict using last epoch model"},{"metadata":{"_kg_hide-input":true,"_uuid":"1f54e33fcba0e3054d364f35a22f69ef350e8e0d","execution":{"iopub.execute_input":"2020-08-07T20:11:54.451558Z","iopub.status.busy":"2020-08-07T20:11:54.450812Z","iopub.status.idle":"2020-08-07T20:11:55.696802Z","shell.execute_reply":"2020-08-07T20:11:55.696207Z"},"papermill":{"duration":1.30894,"end_time":"2020-08-07T20:11:55.696935","exception":false,"start_time":"2020-08-07T20:11:54.387995","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46cd7e86e92ac2ec484f0c38c451465cc16a2736","papermill":{"duration":0.052022,"end_time":"2020-08-07T20:11:55.802159","exception":false,"start_time":"2020-08-07T20:11:55.750137","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's check also the test accuracy per class."},{"metadata":{"_kg_hide-input":true,"_uuid":"55e96cfdaf488df5bc3d5511fa062563926227ad","execution":{"iopub.execute_input":"2020-08-07T20:11:55.915583Z","iopub.status.busy":"2020-08-07T20:11:55.914891Z","iopub.status.idle":"2020-08-07T20:11:55.917961Z","shell.execute_reply":"2020-08-07T20:11:55.917306Z"},"papermill":{"duration":0.063255,"end_time":"2020-08-07T20:11:55.918074","exception":false,"start_time":"2020-08-07T20:11:55.854819","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def test_accuracy_report(model):\n    predicted = model.predict(X_test)\n    test_predicted = np.argmax(predicted, axis=1)\n    test_truth = np.argmax(y_test.values, axis=1)\n    print(metrics.classification_report(test_truth, test_predicted, target_names=y_test.columns)) \n    test_res = model.evaluate(X_test, y_test.values, verbose=0)\n    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"68c06c36ae2f89e070c878bf5f660e765d23878b","execution":{"iopub.execute_input":"2020-08-07T20:11:56.029217Z","iopub.status.busy":"2020-08-07T20:11:56.028565Z","iopub.status.idle":"2020-08-07T20:11:58.539083Z","shell.execute_reply":"2020-08-07T20:11:58.53837Z"},"papermill":{"duration":2.568244,"end_time":"2020-08-07T20:11:58.539245","exception":false,"start_time":"2020-08-07T20:11:55.971001","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"test_accuracy_report(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict using best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_optimal = model\nmodel_optimal.load_weights('best_model.h5')\nscore = model_optimal.evaluate(X_test, y_test, verbose=0)\nprint(f'Best validation loss: {score[0]}, accuracy: {score[1]}')\n\ntest_accuracy_report(model_optimal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>Conclusions</a>  \n \n \nTraining uses 64% of the total data (9,600 / 15,000 images), validation 16% of the total images (2,400 / 15,000) and test 3,000 images (20% of the total number of images).\n\nTensorflow/Keras with GPU, with 2 set of Convolutional layers, MaxPool and Dropout, using as well a variable learning rate, periodic saving best model and early stopping and then using best model for testing, resulted in 97% accuracy for testing set.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}