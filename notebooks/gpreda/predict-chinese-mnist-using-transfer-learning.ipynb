{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center><font size=\"6\">Predict Chinese MNIST using Transfer Learnig</font></center></h1>\n\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n - Objectives\n - Data\n- <a href='#2'>Prepare the data analysis</a>   \n - Load packages\n - Load the data\n - Image suites\n- <a href='#3'>Data exploration</a>   \n - Check for missing data \n - Explore image data\n- <a href='#4'>Characters classification</a>       \n - Split the data\n - Build the model\n - Model evaluation\n - Predicton of test set\n      * Prediction using last epoch model\n      * Prediction using best model","metadata":{}},{"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\n\n## Objectives\n\nThere are two objectives for this Kernel.\n\nFirst objective is to take us through the steps of a machine learning analysis.   \n\nThe second objective is to demonstrate how we can use transfer learning to train a model for image classification.\n\n## Data\n\nWe will use a dataset with adnotated images of Chinese numbers, handwritten by a number of 100 volunteers, each providing a number of 10 samples, each sample with a complete set of 15 Chinese characters for numbers.\n\nThe Chinese characters are the following:\n* 零 - for 0  \n* 一 - for 1\n* 二 - for 2  \n* 三 - for 3  \n* 四 - for 4  \n* 五 - for 5  \n* 六 - for 6  \n* 七 - for 7  \n* 八 - for 8  \n* 九 - for 9  \n* 十 - for 10\n* 百 - for 100\n* 千 - for 1000\n* 万 - for 10 thousands\n* 亿 - for 100 millions\n\n\n\nWe start by preparing the analysis (load the libraries and the data), continue with an Exploratory Data Analysis (EDA).\n\nWe follow then with features engineering and preparation for creation of a model. The dataset is split in training, validation and test set. \n\nWe run a model using Tensorflow through Keras interface, with GPU acceleration, using as well Dropouts, variable learning speed and early stoping based on variation of validation error accuracy.\n\nAt the end, we use the best model to predict for the test set.\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>  ","metadata":{"_uuid":"a8e77ace65f04c89a878bf18249e4d8e23fec996","papermill":{"duration":0.014437,"end_time":"2020-08-17T07:42:21.673551","exception":false,"start_time":"2020-08-17T07:42:21.659114","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <a id='2'>Prepare the data analysis</a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n\n","metadata":{"_uuid":"4e97555eb77978a29a51c41f39cec67136b18157","papermill":{"duration":0.013804,"end_time":"2020-08-17T07:42:21.701429","exception":false,"start_time":"2020-08-17T07:42:21.687625","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Load packages\n\nWe load the packages used for the analysis.\n","metadata":{"_uuid":"cb2e73fe056a3dda7eb48eeac2facf0c441816d1","papermill":{"duration":0.013784,"end_time":"2020-08-17T07:42:21.729458","exception":false,"start_time":"2020-08-17T07:42:21.715674","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport tensorflow as tf","metadata":{"_kg_hide-input":true,"_uuid":"af08260bfbe163f9132f39d09627899bbc4c1dae","papermill":{"duration":9.280135,"end_time":"2020-08-17T07:42:31.023643","exception":false,"start_time":"2020-08-17T07:42:21.743508","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:20.602444Z","iopub.execute_input":"2021-06-26T16:22:20.602726Z","iopub.status.idle":"2021-06-26T16:22:27.963402Z","shell.execute_reply.started":"2021-06-26T16:22:20.602699Z","shell.execute_reply":"2021-06-26T16:22:27.962458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also set a number of parameters for the data and model.","metadata":{"papermill":{"duration":0.014009,"end_time":"2020-08-17T07:42:31.053339","exception":false,"start_time":"2020-08-17T07:42:31.03933","status":"completed"},"tags":[]}},{"cell_type":"code","source":"NO_EPOCHS = 10\nNUM_CLASSES = 15\nIMAGE_WIDTH = 64\nIMAGE_HEIGHT = 64\nIMAGE_CHANNELS = 1\nIMAGE_PATH = '..//input//chinese-mnist//data//data//'\nRESNET_WEIGHTS_PATH = '/kaggle/input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:22:27.968799Z","iopub.execute_input":"2021-06-26T16:22:27.971104Z","iopub.status.idle":"2021-06-26T16:22:27.978254Z","shell.execute_reply.started":"2021-06-26T16:22:27.971039Z","shell.execute_reply":"2021-06-26T16:22:27.977497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_STATE = 42\nTEST_SIZE = 0.2\nVAL_SIZE = 0.2","metadata":{"_kg_hide-input":true,"_uuid":"a2082fb1e56fc6cfc91d40820b905267bc1ca468","papermill":{"duration":0.026595,"end_time":"2020-08-17T07:42:31.095187","exception":false,"start_time":"2020-08-17T07:42:31.068592","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:27.983059Z","iopub.execute_input":"2021-06-26T16:22:27.985724Z","iopub.status.idle":"2021-06-26T16:22:27.992024Z","shell.execute_reply.started":"2021-06-26T16:22:27.985684Z","shell.execute_reply":"2021-06-26T16:22:27.99103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## Load the data  \n\nLet's see first what data files do we have in the root directory.","metadata":{"_uuid":"307f656565365ff05faf226e5a447875dd0dfead","papermill":{"duration":0.013863,"end_time":"2020-08-17T07:42:31.123799","exception":false,"start_time":"2020-08-17T07:42:31.109936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"os.listdir(\"..//input//chinese-mnist\")","metadata":{"_kg_hide-input":true,"_uuid":"9f1df6658b17558179d8a9016f544410de16c354","papermill":{"duration":0.026671,"end_time":"2020-08-17T07:42:31.16452","exception":false,"start_time":"2020-08-17T07:42:31.137849","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:27.996614Z","iopub.execute_input":"2021-06-26T16:22:27.999226Z","iopub.status.idle":"2021-06-26T16:22:28.01854Z","shell.execute_reply.started":"2021-06-26T16:22:27.999186Z","shell.execute_reply":"2021-06-26T16:22:28.017619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a dataset file and a folder with images.  \n\nLet's load the dataset file first.","metadata":{"_uuid":"241b8735a85a25e16421fda8c35bc3d3c69e7ea8","papermill":{"duration":0.014103,"end_time":"2020-08-17T07:42:31.192924","exception":false,"start_time":"2020-08-17T07:42:31.178821","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_df=pd.read_csv('..//input//chinese-mnist//chinese_mnist.csv')","metadata":{"_kg_hide-input":true,"_uuid":"d7b9f11a014428e56e422d97a5b3ef70efec007e","papermill":{"duration":0.048266,"end_time":"2020-08-17T07:42:31.255239","exception":false,"start_time":"2020-08-17T07:42:31.206973","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.024431Z","iopub.execute_input":"2021-06-26T16:22:28.026572Z","iopub.status.idle":"2021-06-26T16:22:28.235219Z","shell.execute_reply.started":"2021-06-26T16:22:28.02653Z","shell.execute_reply":"2021-06-26T16:22:28.234451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's glimpse the data. First, let's check the number of columns and rows.","metadata":{"_uuid":"22b3984ccc3e29daaf77a796d9d7966cd798e1a8","papermill":{"duration":0.014494,"end_time":"2020-08-17T07:42:31.284233","exception":false,"start_time":"2020-08-17T07:42:31.269739","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_df.shape","metadata":{"_kg_hide-input":true,"_uuid":"535f3f9cea3b26428bec3ede4ed49009bdb91889","papermill":{"duration":0.022781,"end_time":"2020-08-17T07:42:31.321442","exception":false,"start_time":"2020-08-17T07:42:31.298661","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.238385Z","iopub.execute_input":"2021-06-26T16:22:28.238719Z","iopub.status.idle":"2021-06-26T16:22:28.246008Z","shell.execute_reply.started":"2021-06-26T16:22:28.238691Z","shell.execute_reply":"2021-06-26T16:22:28.245081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 15000 rows and 5 columns. Let's look to the data.","metadata":{"_uuid":"5b4405ddcce03ee722f05234d508188997817f8d","papermill":{"duration":0.014952,"end_time":"2020-08-17T07:42:31.35114","exception":false,"start_time":"2020-08-17T07:42:31.336188","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_df.sample(100).head()","metadata":{"_kg_hide-input":true,"_uuid":"4d326f747f0a14580b20c2e034e6c3368edcd18b","papermill":{"duration":0.037352,"end_time":"2020-08-17T07:42:31.403105","exception":false,"start_time":"2020-08-17T07:42:31.365753","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.247824Z","iopub.execute_input":"2021-06-26T16:22:28.248188Z","iopub.status.idle":"2021-06-26T16:22:28.269035Z","shell.execute_reply.started":"2021-06-26T16:22:28.248151Z","shell.execute_reply":"2021-06-26T16:22:28.268044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data contains the following values:  \n\n* suite_id - each suite corresponds to a set of handwritten samples by one volunteer;  \n* sample_id - each sample wil contain a complete set of 15 characters for Chinese numbers;\n* code - for each Chinese character we are using a code, with values from 1 to 15;\n* value - this is the actual numerical value associated with the Chinese character for number;  \n* character - the Chinese character;  \n\nWe index the files in the dataset by forming a file name from suite_id, sample_id and code. The pattern for a file is as following:\n\n> \"input_{suite_id}_{sample_id}_{code}.jpg\"","metadata":{"_uuid":"c97047b17cda76e346e444229485ac91ec966423","papermill":{"duration":0.014082,"end_time":"2020-08-17T07:42:31.431505","exception":false,"start_time":"2020-08-17T07:42:31.417423","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n# <a id='3'>Data exploration</a>  \n\n\n\nLet's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. \n","metadata":{"_uuid":"55dd26f919decca9d67daec9895a5d9e11f1d28b","papermill":{"duration":0.013706,"end_time":"2020-08-17T07:42:31.460074","exception":false,"start_time":"2020-08-17T07:42:31.446368","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Check for missing data \n\nLet's create a function that check for missing data in the dataset.","metadata":{"_uuid":"14443450ba96e12ad8e18ce4dd1779f18d5f914b","papermill":{"duration":0.014465,"end_time":"2020-08-17T07:42:31.489061","exception":false,"start_time":"2020-08-17T07:42:31.474596","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(data_df)","metadata":{"_kg_hide-input":true,"_uuid":"4544dd470d743c54f815faaee863038ad5e8398f","papermill":{"duration":0.067419,"end_time":"2020-08-17T07:42:31.571455","exception":false,"start_time":"2020-08-17T07:42:31.504036","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.270563Z","iopub.execute_input":"2021-06-26T16:22:28.27109Z","iopub.status.idle":"2021-06-26T16:22:28.307519Z","shell.execute_reply.started":"2021-06-26T16:22:28.271049Z","shell.execute_reply":"2021-06-26T16:22:28.306694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing (null) data in the dataset. Still it might be that some of the data labels are misspelled; we will check this when we will analyze each data feature.","metadata":{"_uuid":"3cb0410e8b9afd75ac7b50d0489d90eda6e1b109","papermill":{"duration":0.017553,"end_time":"2020-08-17T07:42:31.605533","exception":false,"start_time":"2020-08-17T07:42:31.58798","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## Explore image data  \n\nLet's also check the image data. First, we check how many images are stored in the image folder.","metadata":{"_uuid":"1fbab44688fb2ab073aac8f964e534f90ce1dfff","papermill":{"duration":0.01546,"end_time":"2020-08-17T07:42:31.638523","exception":false,"start_time":"2020-08-17T07:42:31.623063","status":"completed"},"tags":[]}},{"cell_type":"code","source":"image_files = list(os.listdir(IMAGE_PATH))\nprint(\"Number of image files: {}\".format(len(image_files)))","metadata":{"_kg_hide-input":true,"_uuid":"46f15681887fa82ab13224e52df69d91119fc9ad","papermill":{"duration":0.306034,"end_time":"2020-08-17T07:42:31.960509","exception":false,"start_time":"2020-08-17T07:42:31.654475","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.308723Z","iopub.execute_input":"2021-06-26T16:22:28.309049Z","iopub.status.idle":"2021-06-26T16:22:28.702152Z","shell.execute_reply.started":"2021-06-26T16:22:28.309015Z","shell.execute_reply":"2021-06-26T16:22:28.701219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also check that each line in the dataset has a corresponding image in the image list.  \nFirst, we will have to compose the name of the file from the indexes.","metadata":{"_uuid":"68523860593e9a64059b51d40a316454e6937a68","papermill":{"duration":0.016067,"end_time":"2020-08-17T07:42:31.991945","exception":false,"start_time":"2020-08-17T07:42:31.975878","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_file_name(x):\n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","metadata":{"papermill":{"duration":0.022647,"end_time":"2020-08-17T07:42:32.029941","exception":false,"start_time":"2020-08-17T07:42:32.007294","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.703516Z","iopub.execute_input":"2021-06-26T16:22:28.704077Z","iopub.status.idle":"2021-06-26T16:22:28.709339Z","shell.execute_reply.started":"2021-06-26T16:22:28.704034Z","shell.execute_reply":"2021-06-26T16:22:28.708016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df[\"file\"] = data_df.apply(create_file_name, axis=1)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.047181,"end_time":"2020-08-17T07:42:33.091877","exception":false,"start_time":"2020-08-17T07:42:32.044696","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.710994Z","iopub.execute_input":"2021-06-26T16:22:28.711488Z","iopub.status.idle":"2021-06-26T16:22:28.920714Z","shell.execute_reply.started":"2021-06-26T16:22:28.711449Z","shell.execute_reply":"2021-06-26T16:22:28.919667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_names = list(data_df['file'])\nprint(\"Matching image names: {}\".format(len(set(file_names).intersection(image_files))))","metadata":{"_kg_hide-input":true,"_uuid":"457cd17212904bb96f86ec1770cbdbefc5ffb395","papermill":{"duration":0.032292,"end_time":"2020-08-17T07:42:33.139604","exception":false,"start_time":"2020-08-17T07:42:33.107312","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.922011Z","iopub.execute_input":"2021-06-26T16:22:28.922362Z","iopub.status.idle":"2021-06-26T16:22:28.933789Z","shell.execute_reply.started":"2021-06-26T16:22:28.922321Z","shell.execute_reply":"2021-06-26T16:22:28.932651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image suites \n\nLet's check the suites of the images. For this, we will group by `suite`.","metadata":{"_uuid":"a2b88af0c239ca3d9e37e159889836a4f38913c8","papermill":{"duration":0.014816,"end_time":"2020-08-17T07:43:13.057171","exception":false,"start_time":"2020-08-17T07:43:13.042355","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"Number of suites: {data_df.suite_id.nunique()}\")\nprint(f\"Samples: {data_df.sample_id.unique()}\")","metadata":{"_kg_hide-input":true,"_uuid":"6f1c39d0398275215f92f61542544132a0d574a0","papermill":{"duration":0.032824,"end_time":"2020-08-17T07:43:13.105585","exception":false,"start_time":"2020-08-17T07:43:13.072761","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.935616Z","iopub.execute_input":"2021-06-26T16:22:28.936351Z","iopub.status.idle":"2021-06-26T16:22:28.946481Z","shell.execute_reply.started":"2021-06-26T16:22:28.93631Z","shell.execute_reply":"2021-06-26T16:22:28.945469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 100 suites, each with 10 samples.","metadata":{"_uuid":"fd421a7d1872af204c26588d1a15eaddca08a396","papermill":{"duration":0.015976,"end_time":"2020-08-17T07:43:13.138063","exception":false,"start_time":"2020-08-17T07:43:13.122087","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <a id='4'>Characters classification</a>\n\nOur objective is to use the images that we investigated until now to correctly identify the Chinese numbers (characters).   \n\nWe have a unique dataset and we will have to split this dataset in **train** and **test**. The **train** set will be used for training a model and the test will be used for testing the model accuracy against new, fresh data, not used in training.\n\n","metadata":{"_uuid":"c2a5e2401b418f1723c859ee9e0b4ad5071e4a82","papermill":{"duration":0.016285,"end_time":"2020-08-17T07:43:13.170531","exception":false,"start_time":"2020-08-17T07:43:13.154246","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Split the data\n\nFirst, we split the whole dataset in train and test. We will use **random_state** to ensure reproductibility of results. We also use **stratify** to ensure balanced train/validation/test sets with respect of the labels. \n\nThe train-test split is **80%** for training set and **20%** for test set.\n","metadata":{"_uuid":"e8c0a6df4bb85bcdf90f7c908decab07304d660f","papermill":{"duration":0.015216,"end_time":"2020-08-17T07:43:13.201489","exception":false,"start_time":"2020-08-17T07:43:13.186273","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=data_df[\"code\"].values)","metadata":{"_kg_hide-input":true,"_uuid":"352d452d5212d8c9eff074f11820b03a0d44387b","papermill":{"duration":0.039166,"end_time":"2020-08-17T07:43:13.255753","exception":false,"start_time":"2020-08-17T07:43:13.216587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.948321Z","iopub.execute_input":"2021-06-26T16:22:28.948775Z","iopub.status.idle":"2021-06-26T16:22:28.968245Z","shell.execute_reply.started":"2021-06-26T16:22:28.948726Z","shell.execute_reply":"2021-06-26T16:22:28.967635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will split further the **train** set in **train** and **validation**. We want to use as well a validation set to be able to measure not only how well fits the model the train data during training (or how well `learns` the training data) but also how well the model is able to generalize so that we are able to understands not only the bias but also the variance of the model.  \n\nThe train-validation split is **80%** for training set and **20%** for validation set.","metadata":{"_uuid":"856060cc500db00e472b7755c91aba20c953a5f6","papermill":{"duration":0.015183,"end_time":"2020-08-17T07:43:13.286316","exception":false,"start_time":"2020-08-17T07:43:13.271133","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=train_df[\"code\"].values)","metadata":{"_kg_hide-input":true,"_uuid":"83d0be04ae5a4ad5834631bf18e21917d6313bcd","papermill":{"duration":0.035503,"end_time":"2020-08-17T07:43:13.374943","exception":false,"start_time":"2020-08-17T07:43:13.33944","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.970252Z","iopub.execute_input":"2021-06-26T16:22:28.970839Z","iopub.status.idle":"2021-06-26T16:22:28.986239Z","shell.execute_reply.started":"2021-06-26T16:22:28.970801Z","shell.execute_reply":"2021-06-26T16:22:28.98564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the shape of the three datasets.","metadata":{"_kg_hide-input":true,"_uuid":"0dcaa8c2c5423ab8fc2898d4a4aa937801592c2c","papermill":{"duration":0.015014,"end_time":"2020-08-17T07:43:13.405103","exception":false,"start_time":"2020-08-17T07:43:13.390089","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"Train set rows: {}\".format(train_df.shape[0]))\nprint(\"Test  set rows: {}\".format(test_df.shape[0]))\nprint(\"Val   set rows: {}\".format(val_df.shape[0]))","metadata":{"_kg_hide-input":true,"_uuid":"8247f70b4deb4600fe322f004733234ed37617f0","papermill":{"duration":0.026846,"end_time":"2020-08-17T07:43:13.447104","exception":false,"start_time":"2020-08-17T07:43:13.420258","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.987574Z","iopub.execute_input":"2021-06-26T16:22:28.987948Z","iopub.status.idle":"2021-06-26T16:22:28.993422Z","shell.execute_reply.started":"2021-06-26T16:22:28.987912Z","shell.execute_reply":"2021-06-26T16:22:28.992602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now ready to start building our first model.","metadata":{"_uuid":"ee768c083f40fcbd109425182bc55ce86173b69d","papermill":{"duration":0.014997,"end_time":"2020-08-17T07:43:13.477395","exception":false,"start_time":"2020-08-17T07:43:13.462398","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Build the model\n\n\nNext step in our creation of a predictive model.  \n\nLet's define few auxiliary functions that we will need for creation of our models.\n\nA function for reading images from the image files, scale all images to 100 x 100 x 3 (channels).","metadata":{"_uuid":"d76e822dd76565d29fcfed323cb034939f307581","papermill":{"duration":0.015327,"end_time":"2020-08-17T07:43:13.50807","exception":false,"start_time":"2020-08-17T07:43:13.492743","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def read_image(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    image = skimage.transform.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), mode='reflect')\n    return image[:,:]","metadata":{"_kg_hide-input":true,"_uuid":"f80b4e20e98ce5bf328fba3a22457c4a994de06b","papermill":{"duration":0.024961,"end_time":"2020-08-17T07:43:13.549005","exception":false,"start_time":"2020-08-17T07:43:13.524044","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:28.995143Z","iopub.execute_input":"2021-06-26T16:22:28.99552Z","iopub.status.idle":"2021-06-26T16:22:29.002667Z","shell.execute_reply.started":"2021-06-26T16:22:28.995483Z","shell.execute_reply":"2021-06-26T16:22:29.001671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A function to create the dummy variables corresponding to the categorical target variable.","metadata":{"_uuid":"e396e0cd23633af2169e4d50985f1987654205a9","papermill":{"duration":0.015351,"end_time":"2020-08-17T07:43:13.579496","exception":false,"start_time":"2020-08-17T07:43:13.564145","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def categories_encoder(dataset, var='character'):\n    X = np.stack(dataset['file'].apply(read_image))\n    # we just copy the B&W image 3 times to create the RGB equivalent\n    X = np.repeat(X[..., np.newaxis], 3, -1)\n    y = pd.get_dummies(dataset[var], drop_first=False)\n    return X, y","metadata":{"_kg_hide-input":true,"_uuid":"0f7a2146ca93aef9367ecd64300980005d89911b","papermill":{"duration":0.025442,"end_time":"2020-08-17T07:43:13.620741","exception":false,"start_time":"2020-08-17T07:43:13.595299","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:29.004383Z","iopub.execute_input":"2021-06-26T16:22:29.004916Z","iopub.status.idle":"2021-06-26T16:22:29.012454Z","shell.execute_reply.started":"2021-06-26T16:22:29.004874Z","shell.execute_reply":"2021-06-26T16:22:29.011535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's populate now the train, val and test sets with the image data and create the  dummy variables corresponding to the categorical target variable, in our case `subspecies`.","metadata":{"_kg_hide-input":true,"_uuid":"b40c205d5189b23cbbc4ef0cda8798721d504ff9","papermill":{"duration":0.015577,"end_time":"2020-08-17T07:43:13.651687","exception":false,"start_time":"2020-08-17T07:43:13.63611","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, y_train = categories_encoder(train_df)\nX_val, y_val = categories_encoder(val_df)\nX_test, y_test = categories_encoder(test_df)","metadata":{"_kg_hide-input":true,"_uuid":"70acefcd6dc5d494b1c7db6dc90bae5f8c856d94","papermill":{"duration":36.816334,"end_time":"2020-08-17T07:43:50.483568","exception":false,"start_time":"2020-08-17T07:43:13.667234","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:22:29.014156Z","iopub.execute_input":"2021-06-26T16:22:29.014644Z","iopub.status.idle":"2021-06-26T16:24:11.991665Z","shell.execute_reply.started":"2021-06-26T16:22:29.014608Z","shell.execute_reply":"2021-06-26T16:24:11.990705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"train: {X_train.shape}, {y_train.shape}; valid: {X_val.shape}, {y_val.shape}; test: {X_test.shape}, {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:24:11.992959Z","iopub.execute_input":"2021-06-26T16:24:11.993286Z","iopub.status.idle":"2021-06-26T16:24:12.001598Z","shell.execute_reply.started":"2021-06-26T16:24:11.993252Z","shell.execute_reply":"2021-06-26T16:24:12.000699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = X_train[1234,:]\nprint(x1.shape)\nplt.imshow(x1[:,:,0])\nplt.show()\nplt.imshow(x1[:,:,1])\nplt.show()\nplt.imshow(x1[:,:,2])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:24:12.003027Z","iopub.execute_input":"2021-06-26T16:24:12.003556Z","iopub.status.idle":"2021-06-26T16:24:12.399552Z","shell.execute_reply.started":"2021-06-26T16:24:12.003513Z","shell.execute_reply":"2021-06-26T16:24:12.39863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to start creating our model.  \nWe will use the <a href=\"https://keras.io/api/applications/resnet/\">ResNet50</a> model from Keras library.\n**ResNet50** (short for Residual Networks) is a classic neural network used as a backbone for many computer vision tasks. This model was the winner of **ImageNet** challenge in **2015**. The fundamental breakthrough with ResNet was it allowed us to train extremely deep neural networks successfully.","metadata":{"_uuid":"5093354dc9c7f0510ba54a254690db45e38d0bcc","papermill":{"duration":0.015991,"end_time":"2020-08-17T07:43:50.517555","exception":false,"start_time":"2020-08-17T07:43:50.501564","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(tf.keras.applications.ResNet50(include_top=False, pooling='max', weights='imagenet', input_shape=(64,64,3)))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n# ResNet-50 model is already trained, should not be trained\nmodel.layers[0].trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:25:40.298638Z","iopub.execute_input":"2021-06-26T16:25:40.298997Z","iopub.status.idle":"2021-06-26T16:25:42.914985Z","shell.execute_reply.started":"2021-06-26T16:25:40.298966Z","shell.execute_reply":"2021-06-26T16:25:42.914208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-input":true,"_uuid":"046612eda2408801ded03cc6a5e2357a99298969","papermill":{"duration":0.036022,"end_time":"2020-08-17T07:43:54.638221","exception":false,"start_time":"2020-08-17T07:43:54.602199","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:25:43.861624Z","iopub.execute_input":"2021-06-26T16:25:43.861952Z","iopub.status.idle":"2021-06-26T16:25:43.887787Z","shell.execute_reply.started":"2021-06-26T16:25:43.861923Z","shell.execute_reply":"2021-06-26T16:25:43.886786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are using the predefined epoch number for this experiment (50 steps).\n\nWe are using as well a learning function with variable learning rate (depends on the epoch number). \n\nAt each training epoch, we evaluate the validation error and, based on its evolution, we decide if we stop the training or continue (with a prededined `patience` factor - i.e. we only stop if validation is not improving for a certain number of steps (we set the patience to 5 steps). If at a certain step the validation error is improving, we save the current model. We then will load the best model and use it for prediction of test set.","metadata":{"_uuid":"0149020f748dc9eaa89ebf732829031d6d9d35a2","papermill":{"duration":0.015724,"end_time":"2020-08-17T07:43:54.670642","exception":false,"start_time":"2020-08-17T07:43:54.654918","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nPATIENCE = 10\nVERBOSE = 1\nBATCH_SIZE = 128\n\nannealer = LearningRateScheduler(lambda x: 1e-2 * 0.99 ** (x+NO_EPOCHS))\nearlystopper = EarlyStopping(monitor='loss', patience=PATIENCE, verbose=VERBOSE)\ncheckpointer = ModelCheckpoint('best_model.h5',\n                                monitor='val_accuracy',\n                                verbose=VERBOSE,\n                                save_best_only=True,\n                                save_weights_only=True)","metadata":{"papermill":{"duration":0.02634,"end_time":"2020-08-17T07:43:54.713001","exception":false,"start_time":"2020-08-17T07:43:54.686661","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:26:03.393769Z","iopub.execute_input":"2021-06-26T16:26:03.394112Z","iopub.status.idle":"2021-06-26T16:26:03.402049Z","shell.execute_reply.started":"2021-06-26T16:26:03.394081Z","shell.execute_reply":"2021-06-26T16:26:03.401146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:26:10.064565Z","iopub.execute_input":"2021-06-26T16:26:10.064909Z","iopub.status.idle":"2021-06-26T16:26:10.087368Z","shell.execute_reply.started":"2021-06-26T16:26:10.064879Z","shell.execute_reply":"2021-06-26T16:26:10.08662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model  = model.fit(X_train, y_train,\n                  batch_size=BATCH_SIZE,\n                  epochs=100,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[earlystopper, checkpointer, annealer])","metadata":{"papermill":{"duration":92.666389,"end_time":"2020-08-17T07:45:27.395516","exception":false,"start_time":"2020-08-17T07:43:54.729127","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:26:12.420512Z","iopub.execute_input":"2021-06-26T16:26:12.420864Z","iopub.status.idle":"2021-06-26T16:30:42.77667Z","shell.execute_reply.started":"2021-06-26T16:26:12.420835Z","shell.execute_reply":"2021-06-26T16:30:42.775645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='42'>Model evaluation</a> \n\n\nLet's start by plotting the loss error for the train and validation set. \nWe define a function to visualize these values.","metadata":{"_uuid":"5837b3e9cb131fab2c58f4b9de92f147f48b59ae","papermill":{"duration":0.103671,"end_time":"2020-08-17T07:45:27.599691","exception":false,"start_time":"2020-08-17T07:45:27.49602","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_trace(x,y,ylabel,color):\n        trace = go.Scatter(\n            x = x,y = y,\n            name=ylabel,\n            marker=dict(color=color),\n            mode = \"markers+lines\",\n            text=x\n        )\n        return trace\n    \ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['accuracy']\n    val_acc = hist['val_accuracy']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = list(range(1,len(acc)+1))\n    #define the traces\n    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n                                                             'Training and validation loss'))\n    #add traces to the figure\n    fig.append_trace(trace_ta,1,1)\n    fig.append_trace(trace_va,1,1)\n    fig.append_trace(trace_tl,1,2)\n    fig.append_trace(trace_vl,1,2)\n    #set the layout for the figure\n    fig['layout']['xaxis'].update(title = 'Epoch')\n    fig['layout']['xaxis2'].update(title = 'Epoch')\n    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,3])\n    #plot\n    iplot(fig, filename='accuracy-loss')\n\nplot_accuracy_and_loss(train_model)","metadata":{"_kg_hide-input":true,"_uuid":"a87e3beea44a87a806893b798a38d26904d10718","papermill":{"duration":1.036379,"end_time":"2020-08-17T07:45:28.737931","exception":false,"start_time":"2020-08-17T07:45:27.701552","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:30:46.464922Z","iopub.execute_input":"2021-06-26T16:30:46.46524Z","iopub.status.idle":"2021-06-26T16:30:46.549846Z","shell.execute_reply.started":"2021-06-26T16:30:46.46521Z","shell.execute_reply":"2021-06-26T16:30:46.548979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='43'>Prediction of test set</a> ","metadata":{"papermill":{"duration":0.109691,"end_time":"2020-08-17T07:45:28.960493","exception":false,"start_time":"2020-08-17T07:45:28.850802","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\nLet's continue by evaluating the **test** set **loss** and **accuracy**. We will use here the test set.","metadata":{"_uuid":"a77c0127288f090233e70831e6b909c1618efa35","papermill":{"duration":0.111906,"end_time":"2020-08-17T07:45:29.184734","exception":false,"start_time":"2020-08-17T07:45:29.072828","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Prediction using last epoch model","metadata":{"papermill":{"duration":0.106642,"end_time":"2020-08-17T07:45:29.400253","exception":false,"start_time":"2020-08-17T07:45:29.293611","status":"completed"},"tags":[]}},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"_kg_hide-input":true,"_uuid":"1f54e33fcba0e3054d364f35a22f69ef350e8e0d","papermill":{"duration":0.584648,"end_time":"2020-08-17T07:45:30.09239","exception":false,"start_time":"2020-08-17T07:45:29.507742","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:30:53.869608Z","iopub.execute_input":"2021-06-26T16:30:53.870053Z","iopub.status.idle":"2021-06-26T16:30:55.555741Z","shell.execute_reply.started":"2021-06-26T16:30:53.870011Z","shell.execute_reply":"2021-06-26T16:30:55.554709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check also the test accuracy per class.","metadata":{"_uuid":"46cd7e86e92ac2ec484f0c38c451465cc16a2736","papermill":{"duration":0.10955,"end_time":"2020-08-17T07:45:30.312189","exception":false,"start_time":"2020-08-17T07:45:30.202639","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def test_accuracy_report(model):\n    predicted = model.predict(X_test)\n    test_predicted = np.argmax(predicted, axis=1)\n    test_truth = np.argmax(y_test.values, axis=1)\n    print(metrics.classification_report(test_truth, test_predicted, target_names=y_test.columns)) \n    test_res = model.evaluate(X_test, y_test.values, verbose=0)\n    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])","metadata":{"_kg_hide-input":true,"_uuid":"55e96cfdaf488df5bc3d5511fa062563926227ad","papermill":{"duration":0.118537,"end_time":"2020-08-17T07:45:30.538414","exception":false,"start_time":"2020-08-17T07:45:30.419877","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:30:59.046006Z","iopub.execute_input":"2021-06-26T16:30:59.046324Z","iopub.status.idle":"2021-06-26T16:30:59.05307Z","shell.execute_reply.started":"2021-06-26T16:30:59.046294Z","shell.execute_reply":"2021-06-26T16:30:59.051699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy_report(model)","metadata":{"_kg_hide-input":true,"_uuid":"68c06c36ae2f89e070c878bf5f660e765d23878b","papermill":{"duration":0.81342,"end_time":"2020-08-17T07:45:31.493388","exception":false,"start_time":"2020-08-17T07:45:30.679968","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:31:01.593107Z","iopub.execute_input":"2021-06-26T16:31:01.593429Z","iopub.status.idle":"2021-06-26T16:31:04.972971Z","shell.execute_reply.started":"2021-06-26T16:31:01.593396Z","shell.execute_reply":"2021-06-26T16:31:04.970546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction using best model","metadata":{"papermill":{"duration":0.110311,"end_time":"2020-08-17T07:45:31.715575","exception":false,"start_time":"2020-08-17T07:45:31.605264","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_optimal = model\nmodel_optimal.load_weights('best_model.h5')\nscore = model_optimal.evaluate(X_test, y_test, verbose=0)\nprint(f'Best validation loss: {score[0]}, accuracy: {score[1]}')\n\ntest_accuracy_report(model_optimal)","metadata":{"papermill":{"duration":1.110682,"end_time":"2020-08-17T07:45:32.936223","exception":false,"start_time":"2020-08-17T07:45:31.825541","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-26T16:31:04.97476Z","iopub.execute_input":"2021-06-26T16:31:04.975353Z","iopub.status.idle":"2021-06-26T16:31:09.279022Z","shell.execute_reply.started":"2021-06-26T16:31:04.97531Z","shell.execute_reply":"2021-06-26T16:31:09.276509Z"},"trusted":true},"execution_count":null,"outputs":[]}]}