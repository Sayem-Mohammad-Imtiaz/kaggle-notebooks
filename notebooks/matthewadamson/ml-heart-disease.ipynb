{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Disease Identification"},{"metadata":{},"cell_type":"markdown","source":"import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,plot_confusion_matrix\nfrom sklearn import tree\n\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.pyplot as plt\nimport copy\nfrom sklearn.model_selection import KFold\nimport time\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read dataset and split into training and testing sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\n\nX = data.drop('target',axis=1)\ny = data['target']\n\n#Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Samples:\",len(y))\nprint(\"No disease:\",list(y).count(0))\nprint(\"Disease:\",list(y).count(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def r2(a,b):\n    this_correlation = np.corrcoef(a, b)[0,1]\n    this_r2 = this_correlation**2\n    return this_r2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"function to plot confusion matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion(title, model, X_train, y_train, X_test, y_test):\n    svm_confusion_matrix = plot_confusion_matrix(model, X_train, y_train,\n                      display_labels=['No Heart Disease','Heart Disease'],\n                      cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Training Set)\")\n    plt.show()\n\n    svm_confusion_matrix = plot_confusion_matrix(model, X_test, y_test,\n                          display_labels=['No Heart Disease','Heart Disease'],\n                          cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Testing Set)\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"function to draw learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 1, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Predictive Accuracy\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True,verbose=2)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n    print(\"CV Scores:\",test_scores_mean)\n\n    return plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"First, Create and display the untuned model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree = tree.DecisionTreeClassifier(max_depth=6, \n                                  criterion='gini',\n                                  min_samples_leaf=1,\n                                  min_samples_split=2,\n                                  random_state=0)\ndecision_tree.fit(X_train, y_train)\n\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(decision_tree, \"Initial Decision Tree Learning Rate\", \n                    X_train, y_train, axes=[axes], ylim=(0.4, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gridsearch decision tree hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_params = {\n    \"criterion\":['gini','entropy'],\n    \"max_depth\":range(3,15),\n    \"min_samples_leaf\":range(1,15),\n    \"min_samples_split\":range(1,12),\n    \n}\ndecision_tree = tree.DecisionTreeClassifier()\n\n\ngrid = GridSearchCV(decision_tree,\n                    param_grid = dt_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 11, 'min_samples_split': 2}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the Decision tree algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\ndecision_tree = tree.DecisionTreeClassifier(max_depth=4, \n                                  criterion='entropy',\n                                  min_samples_leaf=11,\n                                  min_samples_split=2,\n                                  class_weight={0: 1, 1: 3},\n                                  random_state=0)\ndecision_tree.fit(X_train, y_train)\nprint(\"Training time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Decision Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show the Decision Tree learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(decision_tree, \"Final Decision Tree Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.4, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show the Decision Tree confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion(\"Decision Tree\", decision_tree, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network"},{"metadata":{},"cell_type":"markdown","source":"Create and display a network with estimated parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(13,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=500,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Initial Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tune the Neural Network hyperparameters with GridSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_params = {\n    \"hidden_layer_sizes\":[(13,),(),(13,6),(6,)],\n    \"activation\":['identity', 'logistic', 'tanh', 'relu'],\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"verbose\":[True],\n    \"max_iter\":range(20,800,50)\n}\n\nnn = MLPClassifier()\ngrid = GridSearchCV(nn,\n                    param_grid = nn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'activation': 'identity', 'hidden_layer_sizes': (6,), 'max_iter': 120, 'solver': 'lbfgs', 'verbose': True}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nnn = MLPClassifier(activation='identity',\n                   hidden_layer_sizes=(6,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=54,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Train r2 score:',r2(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Neural Network Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate the Neural Network accuracy by iterations."},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_scores_train = [];\nnn_scores = [];\nfor i in range(20):\n    nn = MLPClassifier(activation='identity',\n                   hidden_layer_sizes=(),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=i*6+1,\n                   random_state=1,\n                   early_stopping=True)\n    \n    nn.fit(X_train,y_train);\n    y_test_pred = nn.predict(X_test)\n    nn_scores_train.append(accuracy_score(y_train, y_train_pred))\n    nn_scores.append(accuracy_score(y_test, y_test_pred))\n    print(\"I\",i)\nprint(\"Training Scores:\",nn_scores_train)\nprint(\"Testing Scores:\",nn_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,201,10),nn_scores_train,marker='o',label=\"Training Set\")\nplt.plot(range(1,201,10),nn_scores,marker='o',label=\"Testing Set\")\nplt.xlabel('Network Iterations')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Neural Network Accuracy by Iteration Number\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot Neural Network learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Final Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot Neural Network confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion(\"Neural Network\", nn, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boosting"},{"metadata":{},"cell_type":"markdown","source":"Create estimated model"},{"metadata":{"trusted":true},"cell_type":"code","source":"booster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=3, \n                                  criterion='gini',\n                                  min_samples_leaf=1,\n                                  min_samples_split=2,\n                                  class_weight={0: 1, 1: 1},\n                                  random_state=0),\n    n_estimators=100,\n    learning_rate=0.1,\n    random_state=0) \nbooster.fit(X_train, y_train)\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"Initial ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 1.05),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run Adaptive Boosting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nbooster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=5, \n                                  criterion='entropy',\n                                  min_samples_leaf=11,\n                                  min_samples_split=2,\n                                  class_weight={0: 1, 1: 3},\n                                  random_state=0),\n    n_estimators=500,\n    learning_rate=0.75,\n    random_state=0) \nbooster.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Boosted Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Boosted Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate model accuracy by iteration"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\ntrain_accuracies = [];\ntest_accuracies = [];\nfor i in range(1,1001,50):\n    booster = AdaBoostClassifier(\n        tree.DecisionTreeClassifier(max_depth=5, \n                                      criterion='entropy',\n                                      min_samples_leaf=11,\n                                      min_samples_split=2,\n                                      class_weight={0: 1, 1: 3},\n                                      random_state=0),\n        n_estimators=i,\n        learning_rate=0.75,\n        random_state=0) \n    booster.fit(X_train, y_train)\n    y_test_pred = booster.predict(X_test)\n    y_train_pred = booster.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n    print(i,end=\" \")\nprint(\"Training Scores:\",train_accuracies)\nprint(\"Testing Scores:\",test_accuracies)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Draw Boosting Accuracy by estimator count"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,1001,50),train_accuracies,marker='o',label=\"Training Set\")\nplt.plot(range(1,1001,50),test_accuracies,marker='o',label=\"Testing Set\")\nplt.xlabel('Estimators')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Adaptive Boosting Accuracy by Estimator Count\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Draw chart for Ada Booster learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 1.05),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Draw decision tree for Adaptive Boosting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion(\"Boosted Decision Tree\", booster, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM"},{"metadata":{},"cell_type":"markdown","source":"Use GridSearch to tune Poly kernel SVM hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_params1 = {\n    \"kernel\":['poly'],\n    \"degree\":range(1,9),\n    \"C\":[4000,6000,8000, 12000,16000]\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\nprint(grid.best_score_)\n#{'C': 8000, 'degree': 2, 'kernel': 'poly'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_params1 = {\n    \"kernel\":['rbf'],\n    \"gamma\":['scale','auto']\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'gamma': 'scale', 'kernel': 'rbf'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nsvm = SVC(kernel='rbf',\n          gamma='scale',\n          random_state=0)\nsvm.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"\\nUsing RBF Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"RBF Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.4, 0.9),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion(\"SVM, Using RBF Kernel\", svm, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estimate poly kernel:"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(kernel='poly',\n          C=1,\n          degree=3,\n          random_state=0)\nsvm.fit(X_train, y_train)\n\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Initial Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.8),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nsvm = SVC(kernel='poly',\n          C=8000,\n          degree=2,\n          class_weight={0: 1, 1: 5},\n          random_state=0)\nsvm.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion(\"SVM, Using Poly Kernel\", svm, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_params = {\n    \"n_neighbors\":range(1,10),\n    \"leaf_size\":range(1,15),\n    \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n}\n\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn,\n                    param_grid = knn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 3}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estimate KNN model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=10,\n                           algorithm='auto',\n                           leaf_size=10)\nknn.fit(X_train, y_train)\n\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint('KNN Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"Initial KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.85),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nknn = KNeighborsClassifier(n_neighbors=3,\n                           algorithm='auto',\n                           leaf_size=1)\nknn.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\n\nstart_time = time.time();\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"K = 4\")\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"KNN Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.95),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion(\"K Nearest Neighbors\", knn, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}