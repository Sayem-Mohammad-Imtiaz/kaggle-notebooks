{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b5a9f482f6f979dffc66dca67620191b3abc7cc"},"cell_type":"code","source":"#Importing the dataset\ndataset=pd.read_csv('../input/glass.csv')\nX=dataset.iloc[:,:-1].values\ny=dataset.iloc[:,9].values\ny=np.resize(y,(len(y),1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cac46289323e014b9df5b8b931e311798301c8e5"},"cell_type":"code","source":"# Encoding categorical data\n# Encoding the Dependent Variable\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder(categorical_features = [0])\ny = onehotencoder.fit_transform(y).toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57d0c47b81e9fcd9a8943afd21d2be54cdf8c920"},"cell_type":"code","source":"#Splitting the dataset into training and testing dataset\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 , random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ae10932a7e2d62e39442f78a9e4a0103279f98e"},"cell_type":"code","source":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f78257c9b1f84b994daa78f659ceb50414ec8b14"},"cell_type":"code","source":"#Design Matrix & Some necessary initializations\nX_train=np.concatenate((np.ones((len(X_train),1)),X_train),axis=1)\nX_test=np.concatenate((np.ones((len(X_test),1)),X_test),axis=1)\ntheta=np.zeros((len(X_train[0]),6))\niterations=2400\nalpha=0.03\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68a2037ae726201d4d344612c6675e5280a7919d"},"cell_type":"code","source":"#Gradient Descent\ndef Gradient_Descent(X,y,iterations,alpha,theta):\n    m=len(X)\n    for j in range(0,6):\n        for i in range(0,iterations):\n            hx=sigmoid(X@theta[:,j])\n            delta=X.T@(hx-y[:,j])\n            theta[:,j]=theta[:,j]-(alpha/m)*delta\n    return theta\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f4d95d35197d7e288251d93e6c4eabe42a5f089"},"cell_type":"code","source":"#Sigmoid Function\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6f9386f62d9d51e1747b24d648ae9f80482c6e1"},"cell_type":"code","source":"#Decoding values y_test which were encoded using OneHotEncoder & calcultaing which class has the maximum probability for each of the test sample\ndef one_hot_decoder(y):\n    Y= np.zeros((len(y),1))\n    for i in range(len(y)):\n        x=np.argmax(y[i,:])+1\n        if x>3:\n            x=x+1\n        Y[i][0]=x\n    return Y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e36638c4273285c43727cf6ff9a96ea53db8ece0"},"cell_type":"code","source":"#Calculating predicted values\ndef cal_y_pred_val(X_test, theta):\n    y_pred=np.zeros((len(X_test), len(theta[0])))\n    for i in range(len(theta[0])):\n        y_pred[:,i]=sigmoid(X_test@theta[:,i])\n    return y_pred    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f4e2beef622c4c00709b422bdcf32eed8381244"},"cell_type":"code","source":"#Training model\ntheta=Gradient_Descent(X_train,y_train,iterations,alpha,theta)    \nprint('The theta calculated for the training set by Gradient Descent is as follows:\\n')\nprint(theta)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"741ccdb5899f2ea145d55161acd7de9386254ab0"},"cell_type":"code","source":"#Testing model\ny_pred=cal_y_pred_val(X_test, theta)\ny_pred_decoded=one_hot_decoder(y_pred)\ny_test=one_hot_decoder(y_test)\ny_pred_decoded=y_pred_decoded.astype(int)\ny_test=y_test.astype(int)\nprint('Predicted values for test data:')\nprint(y_pred_decoded)\ny_comparison=np.concatenate((y_test,y_pred_decoded),axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c8d2df8642754b680ca1f485665a7a04f93d46a"},"cell_type":"code","source":"print('Comparison between test values & predicted values for test data:')\nprint(y_comparison)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36b3ede3084d140d7bd3ba83505c55fcf363d529"},"cell_type":"code","source":"#Calculating the mean squared error and mean absolute percentage error \nfrom sklearn.metrics import mean_squared_error\nm_squared_errror=mean_squared_error(y_test, y_pred_decoded)\n\ndef mean_absolute_percentage_error(y_test, y_pred): \n    y_test, y_pred = np.array(y_test), np.array(y_pred)\n    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n\nm_absolute_percentage=mean_absolute_percentage_error(y_test, y_pred_decoded)\nprint('The mean squared error = {0:.2f} \\nThe mean absolute percentage error = {1:.2f}%\\n'.format(m_squared_errror,m_absolute_percentage))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}