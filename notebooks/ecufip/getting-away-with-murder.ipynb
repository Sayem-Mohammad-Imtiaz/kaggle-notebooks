{"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}},"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"1c1cd369685bc46480bb01d5372f9f1ea206cbfb","_cell_guid":"7441f59e-8cd8-4fa0-ad2d-59861dc56067"},"source":"# Getting Away with Murder\n<i>What factors impact the probability of a homicide being solved in the US, based on 1980-2014 data?<i/>"},{"cell_type":"markdown","metadata":{"_uuid":"460fcd4f2e65ffb7c9b68159400eb91f5fbe0d28","_cell_guid":"de6975d0-39dd-4792-abe2-584679eb3625"},"source":"## 1. Overview\nThis is an analysis of the US Homicide dataset. The goal is to develop a model that will predict whether or not a homicide was solved based on certain factors of that homicide. Steps will include data cleansing, exploratory analysis and the creation of categorical models (KNN, decision tree, random forest, logistic regression). \n\nIt is designed as a high level analysis outlining the process and how the models work, hence there are certain obvious areas for improvement, notably feature selection. This should still provide a good starting point for more detailed analysis into this."},{"cell_type":"markdown","metadata":{"_uuid":"de0d02db26179c83812ca6df34a61276f9941fe8","_cell_guid":"27d976ea-1a59-48df-8b12-688f30387054"},"source":"## 2. Data import/ initial cleansing"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2df3659a9a8578ac322b4f8cded70d38ab82a501","_cell_guid":"2b373a6f-3c6f-4e48-819f-35cb2e0968af","collapsed":true},"source":"# load modules\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# sklearn\nfrom sklearn import metrics, dummy, grid_search, cross_validation, neighbors\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# seaborn\nimport seaborn as sns\n\n# for decision tree visualisation\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\n# import pydotplus as pydot\n\nnp.random.seed(1)\n\n# get data, change to lower case and remove spaces in column heads\nmurders = pd.read_csv('../input/database.csv', low_memory=False, index_col=0)\nmurders.columns = map(str.lower, murders.columns)\nmurders.columns = [x.strip().replace(' ', '_') for x in murders.columns]"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9d7fdba68ed5deaa088a229dc5864b8460bc9905","_cell_guid":"e6f9341c-74d1-4568-972f-9fee659222de","collapsed":true},"source":"# checking what the data looks like\nprint('shape:', murders.shape)\nprint('data types:', murders.dtypes)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"be3a1033aebc39e4c6dee828aeca456a32392d1b","_cell_guid":"50b00b7a-9f8b-410c-90fe-78bee8889dcf","collapsed":true},"source":"# changed dependend variable to an integer for future analysis\nmurders['crime_solved'] = (murders['crime_solved'] == 'Yes').astype(int)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"51296785cf18dbc6a44595ec04daf6bf07965c5a","_cell_guid":"e76e5fdf-1cf9-4174-b465-8fcdfb6ea995","collapsed":true},"source":"# check for null values\nmurders.isnull().sum()"},{"cell_type":"markdown","metadata":{"_uuid":"efb806e67cf2beacfbea012e30a285ca1c33aaa2","_cell_guid":"3833fe83-d818-4284-99ea-b2eb65b12c93"},"source":"It is possible to remove all of the columns relating to the perpetrator, as these will not be known for unsolved crimes."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a6022499620385af4dee951e3ea6fd811db6d4e7","_cell_guid":"3a7c8f96-295d-4e11-83e4-6e29b583b66a","collapsed":true},"source":"perpetrator_columns = [col for col in murders.columns if 'perpetrator' in col]\nperpetrator_columns.append('relationship')\nmurders = murders.drop(perpetrator_columns, axis=1)"},{"cell_type":"markdown","metadata":{"_uuid":"c4856c3a1da0e70eefa47bf619b02a58dd6a7b66","_cell_guid":"cce44761-5071-44c1-87dc-906518e1690f"},"source":"It is unlikely that where the data comes from will have an impact on whether the crime is solved, so 'record_source' can be removed, and it is unclear what 'incident' means so this can also be removed."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"603988e14bd530660a283c105bd452cf8091639a","_cell_guid":"2a1e7b4f-ad35-473c-830a-16d9878ca784","collapsed":true},"source":"murders = murders.drop(['incident','record_source'], axis=1)"},{"cell_type":"markdown","metadata":{"_uuid":"afa9a14752a014d116bd0aae035788c0e91e0649","_cell_guid":"a382a714-6ee5-4cc9-85eb-7ad13a339410"},"source":"To simplify this analysis, there will be no focus on the city or agency name as there are a large number of unique values. These are all related to the geographic location. In a future iteration of this study, it may be possible to group agencies (e.g. by number of murders) and assess their impact."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c27cd3207725a3f1313434c92c019e5c6d658877","_cell_guid":"6fcec933-72c2-408a-8583-db3dfbc49593","collapsed":true},"source":"for col in ['agency_name', 'agency_code', 'city']:\n    print('{} unique values: {}'.format(col, len(murders[col].unique())))\n\nmurders = murders.drop(['agency_code', 'agency_name', 'city'], axis=1)"},{"cell_type":"markdown","metadata":{"_uuid":"fc69bfcef8ada31bdb859e8d1fe3ff11935a950d","_cell_guid":"370a812c-0970-4e9d-bb8c-d3766fbdd883"},"source":"It is interesting to see that there is both a race and an ethnicity variable - it seems 'ethnicity' focuses purely on whether a victim is Hispanic or not!"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"27b2e8a6e97c4e151bdf843e70cd10ccea7561c1","_cell_guid":"952d0383-55d8-4f94-a3c2-eb0e4458be6a","collapsed":true},"source":"for col in ['victim_race', 'victim_ethnicity']:\n    print('{} unique values: {}'.format(col, murders[col].unique()))"},{"cell_type":"markdown","metadata":{"_uuid":"72b43edf920f4a5315913ac1391d1f03ba8d2098","_cell_guid":"18cbb3fb-5389-48ab-9ed1-3896cc2fb65d"},"source":"'crime_type' only refers to whether a crime is 'Manslaughter by Negligence' or not, which adds little value. Since negligence is not really comparable to other homicides and these account for a very small proportion of the total crimes, these instances will be removed along with the column."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"191901d2b0716af918242a12cf977b7d382f5027","_cell_guid":"21921a6f-0246-4d42-8ed1-db9a00bcb6f3","collapsed":true},"source":"print(murders['crime_type'].value_counts())\nmurders = murders.drop(murders.index[murders['crime_type'] == 'Manslaughter by Negligence'])\nmurders = murders.drop('crime_type', axis = 1)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"feae9fb896702e0a2f565bac7ea4addecdd85789","_cell_guid":"90796c70-fe70-4a35-9929-011ed08a3163","collapsed":true},"source":"# dataset to use\nmurders.head()"},{"cell_type":"markdown","metadata":{"_uuid":"76d907f9e8ae8139c3a3e88272485cb3d6333513","_cell_guid":"d38a331e-c328-4c3c-a609-a58c6206d314"},"source":"## 3. Exploratory Analysis"},{"cell_type":"markdown","metadata":{"_uuid":"0006e041f5885559df595b6e90f76b100848aa8f","_cell_guid":"682607f7-a0e8-4cc4-9ca9-e5ef40b7189d"},"source":"### 3.1 The Variables"},{"cell_type":"markdown","metadata":{"_uuid":"9bd365be92497535bf2a70341c57be623425faf1","_cell_guid":"a6d5b008-cdee-4df0-b05b-39c94627eb2e"},"source":"To start with, a look at victim age. It seems that the frequency of murders is high for babies but falls over childhood. It then rises to peak at the age of 20 before tailing off towards old age. The 'solved rate' is highest in childhood and lower for adults, especially young adults in their early 20s. What happens at the top end of the range looks strange as there is a spike at 99 and a number of victims with age = 998. This would indicate some sort of irregular recording system (e.g. NaN is recorded as 99/ 998) so all crimes where age > 98 will be dropped from the dataset."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e5c55eae027d00af9060c106cafb1331f33c4622","_cell_guid":"f75473ea-2066-4afa-90b3-7687e38ec706","collapsed":true},"source":"pd.set_option('display.mpl_style', 'default')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.size'] = 10\n\nage_dist_solved = murders.groupby('victim_age')['crime_solved'].mean()\nage_dist = murders.groupby('victim_age')['crime_solved'].size()\nages = np.sort(murders['victim_age'].unique())\n\nfig, ax = plt.subplots(nrows=1,ncols=2, figsize=(16, 10))\n\nfigure = plt.subplot(2,2,1)\nage_dist.plot(kind='bar', color = '#a11f0c')\nplt.title('Number of Homicides by Victim Age', fontsize = 16)\nplt.ylabel('Number of homicides')\nplt.xticks(range(0, len(ages), 5), ages[range(0, len(ages), 5)])\n\nfigure = plt.subplot(2,2,2)\nage_dist_solved.plot(kind='bar', color = '#e73f0b')\nplt.title('Proportion of Homicides Solved by Victim Age', fontsize = 16)\nplt.ylabel('Proportion solved')\n_ = plt.xticks(range(0, len(ages), 5), ages[range(0, len(ages), 5)])\n\n# _ assigned to stop the final array of xticks being the output and printing out"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"22d40847f3f066cbb0449fdfed2a358934f25cff","_cell_guid":"3d0db890-1a9f-4129-9675-3375fef556fe","collapsed":true},"source":"murders = murders.drop(murders.index[murders['victim_age'] >98])"},{"cell_type":"markdown","metadata":{"_uuid":"61c532262554c66cb9b5c5b504fca43896be4b56","_cell_guid":"6edb049a-c84d-4106-b5a6-3614e66202a8"},"source":"Now to observe the practical details of the crimes. Agency type, state and weapon seem to be the variables with the most variation in solved rates. There are also a number of values that have exceedingly low frequency, particularly for agency type, state and weapon. \n\nSince this is designed to be a very high level analysis, any value with a frequency of lower than 1,200 (c.0.2% of dataset) for agency type, state and weapon will be removed from the dataset. Conclusions made from these variables will have very limited application and may distract the focus away from more important trends. \n\nAlthough one could argue it is not ideal to simply remove some states, this study is focused on broad trends. A glance at the bar charts would suggest that states with a low number of homicides have a better solved rate. A further iteration of this study could be to group states by their number of homicides, geographic location, or even the homicide rates (importing population information). For now, it is easier just to remove this data."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f87f18f5cde9a997cb19886efbf2d957cca2981d","_cell_guid":"0d1b7f00-0b81-4482-9f71-09801f96f639","collapsed":true},"source":"for column in ['agency_type', 'state', 'year', 'month', 'weapon']:\n    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(16, 10))\n    \n    dist_solved = murders.groupby(column)['crime_solved'].mean()\n    dist = murders.groupby(column)['crime_solved'].size()\n    \n    figure = plt.subplot(2,2,1)\n    dist.plot(kind='bar', color= '#003b46')\n    plt.title('Number of Homicides by {}'.format(column), fontsize = 16)\n    plt.ylabel('Number of homicides')\n\n    figure = plt.subplot(2,2,2)\n    dist_solved.plot(kind='bar', color = '#07575b')\n    plt.title('Proportion of Homicides Solved by {}'.format(column), fontsize = 16)\n    _ = plt.ylabel('Proportion solved')"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"16d4a75c4feae0ffbd4d90e10026a5ce985a9360","_cell_guid":"973bdd34-d009-4609-a4c1-84c5adbcb550","collapsed":true},"source":"print('dropped:')\nfor col in ['agency_type', 'weapon', 'state']:\n    for val in murders[col].unique():\n        if(len(murders[murders[col] == val]) < 1200):\n            print('{}: {}, frequency: {}'.format(col, val,\\\n                    len(murders[murders[col] == val])))\n            murders = murders.drop(murders.index[murders[col] == val])"},{"cell_type":"markdown","metadata":{"_uuid":"48c8e62b1b9003e6b8312f0919de0d63685f8db2","_cell_guid":"6df82be1-08ec-4e81-86c5-f6b3c87e1cfb"},"source":"Finally to consider the details of the victim. There look to be some significant differences in the solved rate for gender, race, ethnicity and victim count. There are also a number of 'unknown' values in sex, race and ethnicity. For race and gender, their frequency is negligible so these homicides will be removed from the dataset. For ethnicity, the number is significant, so they will remain. For the purposes of analysis, a dummy variable will be created only for those who positively identify as Hispanic (i.e. 'Unknown' and 'Non-Hispanic' will be grouped together. With regards to the number of victims, for the purposes of analysis, homicides will be categorised as either 'mass' or not, where mass indicates more than 1 victim ('victim_count' > 0). "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9659dd0f960af17a84ea198754712f0445b9bc99","_cell_guid":"ff3a831b-55be-41e0-9e5d-d9443b6614b3","collapsed":true},"source":"for column in ['victim_sex','victim_race', 'victim_ethnicity', 'victim_count']:\n    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(16, 10))\n    \n    dist_solved = murders.groupby(column)['crime_solved'].mean()\n    dist = murders.groupby(column)['crime_solved'].size()\n    \n    figure = plt.subplot(2,2,1)\n    dist.plot(kind='bar', color= '#2e4600')\n    plt.title('Number of Homicides by {}'.format(column), fontsize = 16)\n    plt.ylabel('Number of homicides')\n\n    figure = plt.subplot(2,2,2)\n    dist_solved.plot(kind='bar', color = '#486b00')\n    plt.title('Proportion of Homicides Solved by {}'.format(column), fontsize = 16)\n    _ = plt.ylabel('Proportion solved')"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"08fc6dd63f3294600fead9060a4cadc3bcbc8256","_cell_guid":"345ce133-a975-41d3-97ad-a78da3a7d91c","collapsed":true},"source":"murders = murders.drop(murders.index[murders['victim_sex'] == 'Unknown'])\nmurders = murders.drop(murders.index[murders['victim_race'] == 'Unknown'])"},{"cell_type":"markdown","metadata":{"_uuid":"56ff6156207d79b9d59c3f2118acad8c0c9120b6","_cell_guid":"430160bb-a82b-459e-8988-0ba12910b4f2"},"source":"### 3.2 Creating Dummy Variables"},{"cell_type":"markdown","metadata":{"_uuid":"90f58777dab9eb79590f5f89a60cb6c6abc67fd0","_cell_guid":"6896f852-d4b4-4088-972a-e853c143dc8f"},"source":"Dummies will be created for all categorical data. For the 'year' variable, it will be grouped by decade so as not to have too many dummies. For 'weapon', all types of gun related crimes will be grouped together. "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"42dfb4316a039640dbf5ed12f036df3b59baeeca","_cell_guid":"c0510d81-67af-45f0-b224-3f6048d4228a","collapsed":true},"source":"# get dummies for main variables\nmurders = murders.join(pd.get_dummies(murders['agency_type'], prefix = 'agency'))\nmurders = murders.join(pd.get_dummies(murders['state'], prefix = 'state'))\nmurders = murders.join(pd.get_dummies(murders['month'],prefix='mon'))\nmurders = murders.join(pd.get_dummies(murders['victim_sex']))\nmurders = murders.join(pd.get_dummies(murders['victim_race'], prefix='vic_rac'))\nmurders = murders.join(pd.get_dummies(murders['weapon'], prefix='weapon'))\n\n# change to lowercase/ remove spaces\nmurders.columns = map(str.lower, murders.columns)\nmurders.columns = [x.strip().replace(' ', '_') for x in murders.columns]\n\n# assign dummies for more than 1 victim (Victim Count is 0 if just 1 victim)\nmurders['mass'] = (murders['victim_count'] > 0)\n\n# group values into decades and create a decade_vars dataframe\nmurders['1980s'] = (murders['year'] < 1990) & (murders['year'] >= 1980)\nmurders['1990s'] = (murders['year'] < 2000) & (murders['year'] >= 1990)\nmurders['2000s'] = (murders['year'] < 2010) & (murders['year'] >= 2000)\nmurders['2010s'] = (murders['year'] < 2020) & (murders['year'] >= 2010)\n\n# create a dummy for Hispanic\nmurders['hispanic'] = murders['victim_ethnicity'] == 'Hispanic' \n\n# group all gun related crime into 1 dummy\nmurders['weapon_any_gun'] = (murders['weapon_rifle'] == True)\\\n| (murders['weapon_shotgun'] == True)\\\n| (murders['weapon_handgun'] == True)\\\n| (murders['weapon_gun'] == True)\\\n| (murders['weapon_firearm'] == True)\n\n# drop the original gun dummies from the main dataset\nmurders = murders.drop(['weapon_rifle','weapon_shotgun', 'weapon_handgun','weapon_gun','weapon_firearm'], axis=1)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e4b8f6c5f1f5227fd55946f6e6496a053456e0d4","_cell_guid":"d7437da5-5739-4d4f-ae4f-248aef8c43ff","collapsed":true},"source":"# drop the original columns that have been 'dummified'\nmurders = murders.drop(['agency_type', 'victim_sex',\\\n                        'victim_race', 'victim_ethnicity', 'state',\\\n                        'weapon', 'victim_count', 'month', 'year'], axis=1)"},{"cell_type":"markdown","metadata":{"_uuid":"b4b11c79a375a44831f7b8efaa3538cf85521671","_cell_guid":"67811518-ceb4-490b-bd02-4072cd476358"},"source":"The following variables will be removed as 'baselines'. These are the ones with the largest number of associated instances:\n\n* Agency: agency_municipal_police \n* State: state_california\n* Month: mon_july\n* Gender: male\n* Race: vic_rac_white\n* Weapon: weapon_any_gun\n* Decade: 1990s"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b81cc746df6438b0e52745f80987f024fdcf9098","_cell_guid":"d4e6c3b3-08f2-4228-a1d7-64a4f455dd9a","collapsed":true},"source":"murders = murders.drop(['agency_municipal_police', 'state_california', 'mon_july','male',\\\n                'vic_rac_white', 'weapon_any_gun', '1990s'], axis = 1)"},{"cell_type":"markdown","metadata":{"_uuid":"d76736bdf6385281a5258d299adc097d7653890a","_cell_guid":"0fdbcf7a-0f5f-473d-9fa6-f84d3aecf66c"},"source":"### 3.3 Correlation Matrix"},{"cell_type":"markdown","metadata":{"_uuid":"49629d06a60e16feddc401acb0d939c02fe36b8a","_cell_guid":"ed495750-84bd-4dd6-a830-3a881fff6efd","collapsed":true},"source":"The correlation matrix below corroborates what was determined above regarding the correlation between a crime being solved and weapon, gender, race and agency type. There are some weak correlations between the independent variables, such as race and state, state and agency type and weapon and victim age. These all seem reasonable and none of them seem strong enough for multicolinearity to be an issue."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0aeecc8e87a44cc4c865587e2d3663d4bde33950","_cell_guid":"5611d091-a44e-4bcd-bd3e-aa5cc8caf356","collapsed":true},"source":"cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nmurders = murders[murders.columns].astype(float)\n\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(murders.corr(), cmap=cmap)"},{"cell_type":"markdown","metadata":{"_uuid":"3bb7ebc7fcfbb6db68cdaa67146cf1466af0ee19","_cell_guid":"ca893b6e-1a35-4eeb-9a3d-e09321e210e8"},"source":"## 4. Modelling"},{"cell_type":"markdown","metadata":{"_uuid":"0dc370ebc981d6a866a039afcea82cf7e6bc01c3","_cell_guid":"a47eb997-2f20-408b-a1c4-acf187bdd6b7"},"source":"### 4.1 Train and Test Datasets\nThe model creation will be fairly basic, focusing more on optimising the parameters than feature selection. This project is more to demonstrate how the models work as opposed to creating the best predictive model.\n\n4 types of model will be created:\n\n1. KNN neighbours\n2. Decision tree\n3. Random forest\n4. Logistic regression\n\nAll of the selected features will be used in each model. Each type of analysis will consist of:\n1. Using grid search on a sample to determine which the best parameters would be - this will be quicker than using the full dataset\n2. Use these parameters to create a model using a train dataset and then test it on a test dataset\n\nFor this reason, the dataset will need to be split into a train and test set and a sample will be taken. 30,000 is a large sample that represents around 5% of the dataset.\n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c008584024b69c826d8111af95e449bec7047648","_cell_guid":"674c87be-717a-477d-b12b-ee31b35f7861","collapsed":true},"source":"# split into a train and test set\ntrain, test = train_test_split(murders, test_size = 0.3)\n\n# create a sample and check solved rate similar to population\nsample_murders = murders.sample(n=30000)\nsample_murders['crime_solved'].value_counts(normalize = True)"},{"cell_type":"markdown","metadata":{"_uuid":"c0bf6ac0e25301ff910b1951f3a8aa13b7182703","_cell_guid":"2166544c-a90d-4172-bea4-e947b68dd337"},"source":"### 4.2 Benchmarks and scoring\n\nIf it was the case that the predictors had no impact on the dependent variable, it would be rational to assume that any given homicide would be solved, given that 70.2% of the total were solved. Hence this 'model' will be the benchmark against which other models can be judged.  This would have the following metrics (based on the value counts for the dependent variable below):\n\n--- | Predicted positive | Predicted negative\n---| ---| ---\n<b>Actually positive</b>| 429,704 (TP) | 0 (FN)\n<b>Actually negative</b>| 182,175 (FP) | 0 (TN)\n\n\n<br>\nFPR = FP/(FP + TN) = 182,175/182,175 = 100%\n\nRecall (TPR) = TP/P = 429,704/429,704 = 100%\n\nPrecision = TP/(TP+FP) = 429,704/(429,704 + 182,175) = 70.23%\n\nAccuracy = (TP+TN)/Size of dataset = 429,705/611,879 = 70.23%\n\nSince this study has the goal of improving information, failing to identify a true positive is no worse than incorrectly identifying a negative result as positive (i.e. a false positive). This means that <b>accuracy</b> is the a more appropriate measure of model performance than recall or precision. The target to beat is 70.23%.\n\nThe issue with accuracy is that, due to the high solved rate, the accuracy associated with guessing all murders were solved is already fairly high and hence hard to improve upon. Furthermore, if the goal of the model were to change from accuracy maximisation to a target that combines both precision and recall, the random assignment model starts to look shakier. \nA better model would be able to make predictions with a lower FPR whilst minimizing the sacrifice in TPR. A good measure for a model's ability to do this is the <b>area under the curve</b> metric (AUC). Therefore both accuracy and AUC will be considered when determining how good the models are. A model that relies on guessing should have an AUC score of 50%."},{"cell_type":"markdown","metadata":{"_uuid":"011d69050417fe718d2cf67597a8af686aa8b6b3","_cell_guid":"0fcf6739-4071-416b-a478-e593d04f989c"},"source":"### 4.3 KNN\nA grid search using 2, 5, 10, 20, 50, 100 and 150 neighbours shows that 100 neighbours produces the best accuracy and AUC scores, although the accuracy score is in line with the 70.2% benchmark. The accuracy starts to flatten out at just below 70% with around 50 neighbours, which could suggest that at this point the number of neighbours makes proximity irrelevant and it is more or less equivalent to comparing to the entire sample."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"31b11aad536d48fff8cac21f5691b3a3b405bd21","_cell_guid":"aac64e38-8b31-4399-a656-fa75295be3f7","collapsed":true},"source":"X = sample_murders.drop('crime_solved', axis=1)\ny = sample_murders['crime_solved']\n\nn = [2,5,10,20,50,100,150]\n\ngs = grid_search.GridSearchCV(\n    estimator=neighbors.KNeighborsClassifier(weights = 'uniform'),\n    param_grid={'n_neighbors': n},\n    cv=cross_validation.KFold(len(sample_murders), n_folds = 3),\n)\ngs.fit(X, y)\nprint('Best accuracy: {}, {}'.format(gs.best_score_, gs.best_params_))\nknn_acc_scores = gs.grid_scores_\n\ngs = grid_search.GridSearchCV(\n    estimator=neighbors.KNeighborsClassifier(weights = 'uniform'),\n    param_grid={'n_neighbors': n},\n    cv=cross_validation.KFold(len(sample_murders), n_folds = 3),\n    scoring = 'roc_auc'\n)\ngs.fit(X, y)\nprint('Best AUC: {}, {}'.format(gs.best_score_, gs.best_params_))\nknn_auc_scores = gs.grid_scores_"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3f63b903889d8b7c3fa427ea1e5417ab6b56fc6b","_cell_guid":"78251a92-3057-4329-9beb-cd60e68664b5","collapsed":true},"source":"plt.plot(n,[s[1] for s in  knn_acc_scores], label = 'Accuracy')\nplt.plot(n,[s[1] for s in  knn_auc_scores], label = 'AUC')\nplt.title('KNN: Score vs. Number of Neighbours')\nplt.ylim(0.5,0.8)\nplt.ylabel('Score')\nplt.xlabel('Number of neighbours')\nplt.legend(loc = 4)"},{"cell_type":"markdown","metadata":{"_uuid":"647137859ca092040f65c952304c8cd948f70026","_cell_guid":"821a8319-b27d-4126-9c76-afeff4c1b6ab"},"source":"For KNN, the sample will be split into a train and test dataset as opposed to using the train and test dataset created above. This is because KNN takes a particularly long time to run. The accuracy is around the same as the benchmark but the AUC marks an improvement. "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2be6e3728c438627737fe5ef5211d55869cb4dbb","_cell_guid":"ca6a2e9e-c632-471c-8e3f-d5d79f48aa02","collapsed":true},"source":"sample_train, sample_test = train_test_split(sample_murders, test_size = 0.3)\n\nX = sample_train.drop('crime_solved', axis=1)\ny = sample_train['crime_solved']\n\nX_test = sample_test.drop('crime_solved', axis=1)\ny_test = sample_test['crime_solved']\n\nmodel = neighbors.KNeighborsClassifier(weights = 'uniform', n_neighbors = 100)\n\nmodel.fit(X,y)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test).T[1]\n\nprint('Accuracy: {}, AUC: {}'.format(metrics.accuracy_score(y_test, predictions),\\\n                                     metrics.roc_auc_score(y_test, probabilities)))"},{"cell_type":"markdown","metadata":{"_uuid":"067f02b6b2e1716714c5f3542e408f354d939005","_cell_guid":"1491301d-3219-4f3b-bf34-514fad5befa9"},"source":"### 4.4 Decision Tree"},{"cell_type":"markdown","metadata":{"_uuid":"6a0d0281dc34e4f33589de97f1f8f8bff4162d0f","_cell_guid":"ce901885-6fc6-46ee-84bc-f596285880c9"},"source":"With this model, a visualisation can be created using a very basic decision tree (depth 3). This is helpful in understanding what the decision tree is actually doing. "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a0b2336d2a811b0a3e28329a2551e230775f9700","_cell_guid":"9c3d1c0f-3a18-4055-a146-1e75cd007fa2","collapsed":true},"source":"#to be run when I can figure out how to import pydotplus\nmodel = DecisionTreeClassifier(max_depth=3)\n\nX = sample_murders.drop('crime_solved', axis = 1)\ny = sample_murders['crime_solved']\n\nmodel.fit(X, y)\nprint(model.score(X,y))\n\n## pydotplus not importing to Kaggle\n# create an output file object\n# dot_data = StringIO() \n\n# export_graphviz(model, \n#             out_file = dot_data,  \n#             filled = True, \n#             rounded = True,\n#             special_characters = True,\n#             feature_names = X.columns)  \n\n# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n# Image(graph.create_png())"},{"cell_type":"markdown","metadata":{"_uuid":"2d7d67a4a7192c2f679def6fcf3d939a7ab21d22","_cell_guid":"42ea1032-d307-4a2a-b8e8-7e6474b8fe0c"},"source":"To optimise the model, depths of 1-20 will be tested in increments of 2. The model performs best under both accuracy and AUC when the max depth is 9."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"39cf4fe6af7e169c32449f6727740173d12562d3","_cell_guid":"6614cc18-2152-4f77-9bd5-588adc752c8f","collapsed":true},"source":"X = sample_murders.drop('crime_solved', axis=1)\ny = sample_murders['crime_solved']\n\ndepths = list(range(1,20, 2))\n\ngs = grid_search.GridSearchCV(\n    estimator=DecisionTreeClassifier(),\n    param_grid={'max_depth': depths},\n    cv=cross_validation.KFold(len(X), n_folds = 3),\n    scoring = 'accuracy'\n)\ngs.fit(X, y)\nprint('Best accuracy: {}, {}'.format(gs.best_score_, gs.best_params_))\ndt_acc_scores = gs.grid_scores_\n\ngs = grid_search.GridSearchCV(\n    estimator=DecisionTreeClassifier(),\n    param_grid={'max_depth': depths},\n    cv=cross_validation.KFold(len(X), n_folds = 3),\n    scoring = 'roc_auc'\n)\ngs.fit(X, y)\nprint('Best AUC: {}, {}'.format(gs.best_score_, gs.best_params_))\ndt_auc_scores = gs.grid_scores_"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e86790d93eca3912d8fb13d78fcbf8f39f3dad40","_cell_guid":"f18fd49c-86b2-4db4-9527-896168ddf1d1","collapsed":true},"source":"plt.plot(depths,[s[1] for s in  dt_acc_scores], label = 'Accuracy')\nplt.plot(depths,[s[1] for s in  dt_auc_scores], label = 'AUC')\nplt.title('Decision Tree: Score vs. Max Depth')\nplt.ylabel('Score')\nplt.xlabel('Max depth')\nplt.legend(loc = 4)"},{"cell_type":"markdown","metadata":{"_uuid":"595604300fafdafba21d02d4f80e94805ee02583","_cell_guid":"eced4270-c29b-4600-8369-8a28ac6628c5"},"source":"The accuracy and AUC  are not far off what was observed with the KNN model."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d4396240c7ceae1ea74cef8f0c6f5be67f0a23d6","_cell_guid":"d42e957b-886d-4c5c-98b5-a89effbae82d","collapsed":true},"source":"X_test = test.drop('crime_solved', axis=1)\ny_test = test['crime_solved']\n\nmodel = DecisionTreeClassifier(max_depth=9)\nmodel.fit(X,y)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test).T[1]\n\nprint('Accuracy: {}, AUC: {}'.format(metrics.accuracy_score(y_test, predictions),\\\n                                     metrics.roc_auc_score(y_test, probabilities)))"},{"cell_type":"markdown","metadata":{"_uuid":"6e215ead5057252ccdf9cbef79258608e5ba32a0","_cell_guid":"3326289e-f8d1-4d38-9a3d-a5ffee265f23"},"source":"### 4.5 Random Forest\nFor the random forest model there are 2 parameters that will be tested. The number of trees to use (n_estimators) and the max depth of each tree. The optimal parameters are using 100 estimators and a max depth of 20."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"839dcb17c248e184eda23d7e8ce1a6f74c01968b","_cell_guid":"24d41de6-58a0-41ea-8847-8ec1df173cc3","collapsed":true},"source":"X = sample_murders.drop('crime_solved', axis = 1)\ny = sample_murders['crime_solved']\n\nfor scoring in ['accuracy', 'roc_auc']:\n    gs = grid_search.GridSearchCV(\n        estimator=RandomForestClassifier(),\n        param_grid={'n_estimators': [5, 10, 20, 50, 100], 'max_depth': [10, 20, 30, 50, 100, 200]},\n        cv=cross_validation.KFold(len(X), n_folds = 3, shuffle = True),\n        scoring = scoring\n    )\n    gs.fit(X, y)\n    print('best {}: {}, {}'.format(\\\n         scoring, gs.best_score_, gs.best_params_))"},{"cell_type":"markdown","metadata":{"_uuid":"00506348cd6e7c53919b8b75d1facee75fb132da","_cell_guid":"97afebd1-578d-4cc3-ae19-ae0bc0d53795"},"source":"Using these parameters on the train data creates a model with an accuracy of 71.8% and an AUC of 70.7%. This is a notable improvement on previous models. The AUC can also be illustrated visually on the ROC curve."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bec086cff08196e631ef8250f5d8c5dc27c7637d","_cell_guid":"148f6fea-d501-4a3d-a091-070d1a6b5a83","collapsed":true},"source":"X = train.drop('crime_solved', axis = 1)\ny = train['crime_solved']\n\nX_test = test.drop('crime_solved', axis = 1)\ny_test = test['crime_solved']\n\nmodel = RandomForestClassifier(n_estimators = 100, max_depth = 20)    \nmodel.fit(X, y)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test).T[1]\n\nprint('Accuracy: {}, AUC: {}'.format(metrics.accuracy_score(y_test, predictions),\\\n                                     metrics.roc_auc_score(y_test, probabilities)))\n\n# create a dummy model array\nmodel_dum = dummy.DummyClassifier()\nmodel_dum.fit(X, y)\nprobability_0 = model_dum.predict_proba(X_test).T[1]\n\n# plot ROC curve\nax = plt.subplot(111)\nvals = metrics.roc_curve(y_test, probability_0)\nax.plot(vals[0], vals[1])\nvals = metrics.roc_curve(y_test, probabilities)\nax.plot(vals[0], vals[1])\n_ = ax.set(title='ROC curve', ylabel='TPR', xlabel='FPR', xlim=(0, 1), ylim=(0, 1))"},{"cell_type":"markdown","metadata":{"_uuid":"222a2352bd3d1677eeabaaf86718cc82be48bd07","_cell_guid":"0895be6d-3e44-47a0-81a7-2f36e16d13fe"},"source":"According to this model, the most important factors in determining whether a crime is solved is the age of the victim, whether it took place in NY state, if a knife was used as a weapon, if the victim was a female and if the victim was black. Whilst the 'direction' of the impact isn't clear from this, it can be inferred from the exploratory analysis."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b4fcca94420abe0bca38ade5cd13bad486faf419","_cell_guid":"774e7d69-1289-49ce-8e41-8341643e145f","collapsed":true},"source":"imp_features = pd.DataFrame({'importance': model.feature_importances_, 'feature': X.columns})\\\n.sort_values(by = 'importance', ascending = True)\nimp_features.tail(10).plot(kind = 'barh')\nplt.title('Most important features', fontsize = 16)\nplt.ylabel('Feature')\n_ = plt.yticks(range(0, 10), imp_features['feature'].tail(10))"},{"cell_type":"markdown","metadata":{"_uuid":"a8578e14fa106cb20f5a181f1ff9e94e7fb0d745","_cell_guid":"0d680df6-ff82-4bb8-aaa6-e50f0e7dcdc3"},"source":"### 4.6 Logistic Regression"},{"cell_type":"markdown","metadata":{"_uuid":"ffa87b693744318c6194ba66e13370d703f37635","_cell_guid":"ab30dd98-c6e2-467a-b704-8ef3c60cb6b0"},"source":"The best parameters for a logistic regression were using l2 regularisation with a penalty of 1."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e047e11a6c1b532b325e83047e7de3027afb8bb0","_cell_guid":"1689a16b-3592-4508-b0a2-de0114306233","collapsed":true},"source":"X = sample_murders.drop('crime_solved', axis = 1)\ny = sample_murders['crime_solved']\n\nfor scoring in ['accuracy', 'roc_auc']:\n    gs = grid_search.GridSearchCV(\n        estimator=LogisticRegression(),\n        param_grid={'C': [10**i for i in range(-8, 9, 4)], 'penalty': ['l1', 'l2']},\n        cv=cross_validation.KFold(n=len(X),n_folds=3),\n        scoring = scoring\n    )\n    gs.fit(X, y)\n    print('best {}: {}, {}'.format(scoring, gs.best_score_, gs.best_params_))"},{"cell_type":"markdown","metadata":{"_uuid":"90102626be8cdfe276aad25fc26606bbdd3003d1","_cell_guid":"603c51e0-aec8-48c9-9efc-b8c715942bc7"},"source":"In actual fact, using either of these parameter sets doesn't make a huge difference on either so the one that maximises AUC will be used. This model has an accuracy of 70.7% and an AUC of 67.6%. This is not as good as the random forest but better than the other models."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a2118280dc2b60c1a715c736ec3234538a7a1f27","_cell_guid":"580f7711-415f-43a9-8b77-cac3922d1805","collapsed":true},"source":"X = train.drop('crime_solved', axis = 1)\ny = train['crime_solved']\n\nX_test = test.drop('crime_solved', axis = 1)\ny_test = test['crime_solved']\n\nmodel = LogisticRegression(penalty = 'l2', C = 1)  \nmodel.fit(X, y)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test).T[1]\n\nprint('Accuracy: {}, AUC: {}'.format(metrics.accuracy_score(y_test, predictions),\\\n                                     metrics.roc_auc_score(y_test, probabilities)))"},{"cell_type":"markdown","metadata":{"_uuid":"4a5c22852ebe1e8c70630d6377f2512bced6bf44","_cell_guid":"b98185b4-f7de-4f9f-9273-d625ab298081"},"source":"This model can be used to make predications, for example a 20 year old black, hispanic (according to the dataset they do exist) male strangulation victim who was killed in DC in October in the 1990s and whose killing was investigated by county police would only have an 11.6% chance of their homicide being solved. Note that 1990s is the default and County Police as an agency was dropped after the first logistic regression, as its impact was not significantly different from that of the default (Municipal Police). These do not need to be included in the predictive model."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"aee8b6d2cd5578d980031021ffcf3e8b4d363173","_cell_guid":"a74de914-c03c-44f3-aac8-96c0c5cb0fef","collapsed":true},"source":"new_victim_dict = {'victim_age': 20, 'hispanic':1, 'state_district_of_columbia': 1,\\\n                   'mon_october': 1, 'vic_rac_black': 1, 'weapon_strangulation': 1}\n\nnew_victim_arr = [0 for i in range(len(X.columns))]\n\nfor i in new_victim_dict:\n     new_victim_arr[X.columns.get_loc(i)] = new_victim_dict.get(i)\n        \nmodel.predict_proba([new_victim_arr])[0][1]"},{"cell_type":"markdown","metadata":{"_uuid":"827c2b329f6a32201bbacf5a3d245a08b7b7377d","_cell_guid":"67773052-2221-4a21-a368-974a171f91f1"},"source":"A 2 year old white, non-hispanic female who was killed using drugs in North Carolina in February in the 1980s as part of a mass murder and whose killing was investigated by state police would have a 95.9% chance of having their homicide solved. Note that white is the default."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ce1e8903087daf6408e8c23ba62b3a05e9eea41f","_cell_guid":"eb7bf60a-271c-4d27-a578-eda7c303339c","collapsed":true},"source":"new_victim_dict = {'victim_age': 2, 'state_north_carolina': 1, '1980s':1,\\\n                   'mon_february': 1, 'weapon_drugs': 1, 'agency_state_police': 1, 'mass' : 1}\n\nnew_victim_arr = [0 for i in range(len(X.columns))]\n\nfor i in new_victim_dict:\n     new_victim_arr[X.columns.get_loc(i)] = new_victim_dict.get(i)\n        \nmodel.predict_proba([new_victim_arr])[0][1]"},{"cell_type":"markdown","metadata":{"_uuid":"fa2d5fd72f35cbd356d9cc136a9c9d6ce4bff901","_cell_guid":"280dded6-f248-4304-9c51-89db2fb259b1"},"source":"## 5. Conclusions\nThe below is a summary of all the models created. Random forest with 100 estimators and a max depth of 20 is the best of these."},{"cell_type":"markdown","metadata":{"_uuid":"cce263057fa21c25ad06c41b082bebccc78c1501","_cell_guid":"0f15c7d4-2fec-4871-b008-14c6ba88924b","collapsed":true},"source":"--- | Parameters | Accuracy score | ROC AUC score\n---| ---| ---\n<b>KNN</b>| Neighbours: 100 | 70.8% | 61.4%\n<b>Decision tree</b>| Max depth: 11 | 70.6% | 65.2%\n<b>Random forest</b>| No. estimators: 100<br> Max depth: 20| 71.8% | 70.7%\n<b>Logistic regression</b>| Regularization: l2<br> Penalty: 1 | 70.7% | 67.7%"},{"cell_type":"markdown","metadata":{"_uuid":"c0a5e38e779a2fc16cbafe89b43256dea432a12d","_cell_guid":"dd622d9b-54c7-45aa-9d0f-164c60bf23fd"},"source":"A more detailed analysis could consider which parameters to use and could develop these models further. It could also focus more on the contribution of individual variables, perhaps in a more focused way."}],"nbformat_minor":1}