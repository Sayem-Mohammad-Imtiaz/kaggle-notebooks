{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Data Inspectation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Acknowledgements\n\nThe data is originally from the article Hotel Booking Demand Datasets, written by Nuno Antonio, Ana Almeida, and Luis Nunes for Data in Brief, Volume 22, February 2019.https://www.sciencedirect.com/science/article/pii/S2352340918315191\n\nThe data was downloaded and cleaned by Thomas Mock and Antoine Bichat for #TidyTuesday during the week of February 11th, 2020.https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md\n\nraw data: https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\n\nTask: cluster cancel and non- cancel  \n\nReference:\nhttps://www.kaggle.com/jessemostipak/hotel-booking-demand/notebooks\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/hotel-booking-demand/hotel_bookings.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls = df.isnull().sum()\nnulls[nulls > 0]\n#currently only 4 columns has missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace('Undefined', np.NaN)\n# there are few coloumns have Undefined value instead of NaN,\n#replace 'Undefined' to NaN in the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate the total missing values across the whole dataset\ndf.isnull().sum().sum()/(len(df.index)*31)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing for missing values"},{"metadata":{},"cell_type":"markdown","source":"#### (1) Drop unhelpful columns & rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"percentage = df.isnull().sum()/ len(df)\npercentage.sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#company has 94.4% missing values, not helpful, drop the column\ndf.drop(['company'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reservation_status_date contain a lot of variety \ndf.drop(['reservation_status_date'], axis=1, inplace=True)  # objects & 926 varieties \n# reservation_status includes 'Canceled' feature \n#By keeping reservation_status in data,\n# it is possible to achieve 100% accuracy rate because that feature is direct way to predict cancellations\n# so, drop the reservation_status coumns\ndf.drop(['reservation_status'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (2) inspect all the columns unqiue values for replacing the missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#transform column to binary value\ndf['hotel'] = df['hotel'].map({'Resort Hotel':0, 'City Hotel':1}).astype(int)\n\ndf['arrival_date_month'] = df['arrival_date_month'].map({'January':1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7,\n                                                            'August':8, 'September':9, 'October':10, 'November':11, 'December':12}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since country colomn has high varity data,so need to transfer to numerical data\n# transfer to catergorical data first, then transfer to numeric data\ndf['country'] = df['country'].astype('category')\ndf['country'] = df['country'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inspect data again\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new colomns: 'is_family' , 'deposit_given', 'total_nights'\ndef family_check(df):\n    if ((df['adults'] > 0) & (df['children'] > 0)):\n        val = 1\n    elif ((df['adults'] > 0) & (df['babies'] > 0)):\n        val = 1\n    else:\n        val = 0\n    return val\n\ndef deposit(df):\n    if ((df['deposit_type'] == 'No Deposit') | (df['deposit_type'] == 'Refundable')):\n        return 0\n    else:\n        return 1\n    \ndef previous_cancellations_check(df):\n    if df['previous_cancellations'] == 0:\n        return 0\n    else:\n        return 1\n    \ndef previous_bookings_not_canceled_check(df):\n    if df['previous_bookings_not_canceled'] == 0:\n        return 1\n    else:\n        return 0    \n    \ndef booking_changed_check(df):\n    if df['booking_changes'] == 0:\n        return 0\n    else:\n        return 1    \n    \ndef feature(df):\n    # create new column 'is_family' base on 'adults', 'children', 'babies'\n    df['is_family'] = df.apply(family_check, axis = 1)\n    # create new column 'deposit_given' base on 'deposit_type'\n    df['deposit_given'] = df.apply(deposit, axis=1)\n    df['previous_cancelled'] = df.apply(previous_cancellations_check, axis=1)\n    df['previous_bookings_not_canceled_check'] = df.apply(previous_bookings_not_canceled_check, axis=1)\n    df['booking_changed'] = df.apply(booking_changed_check, axis=1)\n    # create new column 'total_nights' base on 'stays_in_weekend_nights' and 'stays_in_week_nights'\n    df['total_nights'] = df['stays_in_weekend_nights']+ df['stays_in_week_nights']\n    df['booking_times'] = df['previous_cancellations'] + df['previous_bookings_not_canceled']\n    return df\n\ndf = feature(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since we create 'deposit_given' column from 'deposit_type', so can drop 'deposit_type'\ndf.drop(['deposit_type'], axis=1, inplace=True)\ndf.drop(['previous_cancellations'], axis=1, inplace=True)\ndf.drop(['previous_bookings_not_canceled'], axis=1, inplace=True)\ndf.drop(['booking_changes'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the correlation in the dataset with \"is_canceled\"\ncorr_matrix = df.corr()\ncorr_matrix['is_canceled'].sort_values(ascending=False)\n# can see 'arrival_date_month','arrival_date_week_number','arrival_date_year',\n#          'children','stays_in_week_nights','arrival_date_day_of_month','total_nights'\n# have very low correlation(< 0.01%) with 'is_cancelled', whcih is not significant, \n# so it a good idea to drop these columns for further prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the colomns with low correlation(< 0.01%) \n\ndf.drop(['arrival_date_month','arrival_date_week_number',\n          'stays_in_week_nights','children','arrival_date_year','total_nights',\n        'arrival_date_day_of_month'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(data = df, columns = ['meal', 'market_segment', 'distribution_channel',\n                                            'reserved_room_type', 'assigned_room_type', 'customer_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inspect data\ndf.shape\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (3). Data Preprocessing for ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature set and targer set\nX = df.drop('is_canceled', axis = 1)\ny = df['is_canceled']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (4) Data scaling"},{"metadata":{},"cell_type":"markdown","source":"##### Choosing scaling\n##### use MixMax scaling in order to normalize the data set. The data in our data set are spread across a wide range of values, which might result in various features affecting the final result more than the other feature. MixMax scaling reduces this effect by re-scaling the data to a specificed range of values, in this case 0-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data scalering - StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Classification"},{"metadata":{},"cell_type":"markdown","source":"####  Evaluation strategy: The training set score and test set scores are close, the highest test set score among all models is better"},{"metadata":{},"cell_type":"markdown","source":"###  (1) KNN Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GridsearchCV and cross validation searching for KNN hypterparameter\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_neighbors': np.arange(1, 11)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, param_grid, cv=5,return_train_score = False,n_jobs = -1)# return_train_score = True,\nknn_cv.fit(X_train, y_train)\nknn_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nknn_cv_result = pd.DataFrame(knn_cv.cv_results_)\nknn_cv_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#k=6 is the best hyperparameter, applied this value in the model\nclf = KNeighborsClassifier(n_neighbors=6,n_jobs = -1)\nclf.fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(clf.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(clf.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(knn_cv.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(knn_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (2) Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\ngrid={\"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000], \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression(solver='liblinear')\nlogreg_cv=GridSearchCV(logreg,grid,cv=5,return_train_score = False ,n_jobs = -1)\nlogreg_cv.fit(X_train,y_train)\nlogreg_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nlogreg_cv_result = pd.DataFrame(logreg_cv.cv_results_)\nlogreg_cv_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use best parameter C value \nlogreg1 = LogisticRegression(C=10, penalty = 'l1',solver='liblinear',random_state=0).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg1.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg1.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(logreg_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (3) Linear Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"#select best hyperparameter #slow\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import LinearSVC\nCs = [0.1, 1, 10, 100]\nparam_grid = {'C': Cs}\nlinearSVC = GridSearchCV(LinearSVC(max_iter=500000), param_grid, cv=5,return_train_score = False,n_jobs = -1)\nlinearSVC.fit(X_train, y_train)\nlinearSVC.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nlinearSVC_result = pd.DataFrame(linearSVC.cv_results_)\nlinearSVC_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use best parameter C value \nsvm = LinearSVC(C=10).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svm.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svm.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(linearSVC.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(linearSVC.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (4) Kerenilzed Support Vector Machine (rbf, poly, and linear)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear hyperparameter selection \nfrom sklearn.svm import SVC\nCs = [0.01, 0.1, 1, 10, 100]\nparam_grid = {'C': Cs}\nkerenl_lin = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5,return_train_score = False,n_jobs = -1)\nkerenl_lin.fit(X_train, y_train)\nprint(\"The best classifier is: \", kerenl_lin.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV(cv=5, estimator=SVC(kernel='linear'), n_jobs=-1,\n#              param_grid={'C': [0.01, 0.1, 1, 10, 100]})\n# The best classifier is:  {'C': 100}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nkerenl_lin_result = pd.DataFrame(kerenl_lin.cv_results_)\nkerenl_lin_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use best parameter C value \nsvc = SVC(kernel='linear', C=100,gamma='auto').fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svc.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svc.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(kerenl_lin.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(kerenl_lin.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training set score: 0.797\n# Test set score: 0.799\n# Best parameters: {'C': 100}\n# Best cross-validation score: 0.7962","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rbf hypterparameter selection \nC_range = [0.001, 0.01, 0.1, 1, 10, 100]\nparam_grid = {'C': C_range}\nkernel_rbf = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5,return_train_score = False, n_jobs = -1)\nkernel_rbf.fit(X_train, y_train)\nprint(\"The best classifier is: \", kernel_rbf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n#              param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]})\n# The best classifier is:  SVC(C=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nkernel_rbf_result = pd.DataFrame(kernel_rbf.cv_results_)\nkernel_rbf_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use best parameter C value \nsvc = SVC(kernel='rbf', C=100,gamma='auto').fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svc.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svc.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(kernel_rbf.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(kernel_rbf.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training set score: 0.822\n# Test set score: 0.825\n# Best parameters: {'C': 100}\n# Best cross-validation score: 0.8232","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#poly hyperparameter selection \nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nCs = [0.1, 1, 10, 100]\nparam_grid = {'C': Cs}\nkernel_poly = GridSearchCV(SVC(kernel='poly'), param_grid, cv=3,return_train_score = False,n_jobs = -1)\nkernel_poly.fit(X_train, y_train)\nprint(\"The best classifier is: \", kernel_poly.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV(cv=3, estimator=SVC(kernel='poly'), n_jobs=-1,\n#              param_grid={'C': [0.1, 1, 10, 100]})\n# The best classifier is:  {'C': 100}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nkernel_poly_result = pd.DataFrame(kernel_poly.cv_results_)\nkernel_poly_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use best parameter C value \nsvc = SVC(kernel = 'poly',C=100,gamma='auto').fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svc.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svc.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(kernel_poly.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(kernel_poly.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training set score: 0.815\n# Test set score: 0.817\n# Best parameters: {'C': 100}\n# Best cross-validation score: 0.8228","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (5) Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree hyperparameter\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters={'min_samples_split' : range(10,500,20),'max_depth': range(1,20,2)}\nclf_tree=DecisionTreeClassifier()\ngrid_search=GridSearchCV(clf_tree,parameters, cv=10,return_train_score = False,n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint(\"The best classifier is: \", grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV(cv=10, estimator=DecisionTreeClassifier(), n_jobs=-1,\n#              param_grid={'max_depth': range(1, 20, 2),\n#                          'min_samples_split': range(10, 500, 20)})\n# The best classifier is:  {'max_depth': 19, 'min_samples_split': 10}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ngrid_search_result = pd.DataFrame(grid_search.cv_results_)\ngrid_search_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use best parameters values \nclf_tree=DecisionTreeClassifier(max_depth=19,min_samples_split=10).fit(X_train, y_train)\nprint(\"Training clf_treeset score: {:.3f}\".format(clf_tree.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf_tree.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(grid_search.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training clf_treeset score: 0.888\n# Test set score: 0.849\n# Best parameters: {'max_depth': 19, 'min_samples_split': 10}\n# Best cross-validation score: 0.8478","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (6) Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# random forest hyperparameter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters={'min_samples_split' : range(10,500,20),'max_depth': range(1,20,2)}\nclf_treeR=RandomForestClassifier()\ngrid_searchR=GridSearchCV(clf_treeR,parameters, cv=10,return_train_score = False,n_jobs = -1)\ngrid_searchR.fit(X_train, y_train)\nprint(\"The best classifier is: \", grid_searchR.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ngrid_searchR_result = pd.DataFrame(grid_searchR.cv_results_)\ngrid_searchR_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use best parameters values \nclf_treeR=RandomForestClassifier(max_depth=19,min_samples_split=10).fit(X_train, y_train)\nprint(\"Training clf_treeset score: {:.3f}\".format(clf_treeR.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf_treeR.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid_searchR.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(grid_searchR.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Find the best model"},{"metadata":{},"cell_type":"markdown","source":"##### 1. Knn: train 0.873, test 0.835, Best cross-validation score 0.8286"},{"metadata":{},"cell_type":"markdown","source":"##### 2. Logistic Regression:  train  0.794,  test 0.796, Best cross-validation score 0.7935"},{"metadata":{},"cell_type":"markdown","source":"##### 3. Linear Support Vector Machine : train 0.794, test 0.795, Best cross-validation score: 0.7932"},{"metadata":{},"cell_type":"markdown","source":"##### 4.Kerenilzed Support Vector Machine (rbf, poly, and linear):   \n##### (1) Linear train 0.797, test 0.799, Best cross-validation score: 0.7962 \n##### (2) Rbf train 0.822, test 0.825, Best cross-validation score: 0.8232\n##### (3) Poly train 0.815 test 0.817, Best cross-validation score: 0.8228"},{"metadata":{},"cell_type":"markdown","source":"#### 5. decision tree: train 0.888  test 0.849, Best cross-validation score: 0.8478"},{"metadata":{},"cell_type":"markdown","source":"#### 6. random forest: train 0.881  test 0.864, Best cross-validation score: 0.8617"},{"metadata":{},"cell_type":"markdown","source":"### Random forest model has the highest training and test score. In addition, it also has the highest Best cross-validation score-- 0.8617. Therefore, Random forest model is the best option"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score\n\ny_pred = grid_searchR.predict(X_test)\nprint('accuracy_score: ', accuracy_score(y_test, y_pred))\nprint('roc_auc_score: ', roc_auc_score(y_test, grid_searchR.predict_proba(X_test)[:,1]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}