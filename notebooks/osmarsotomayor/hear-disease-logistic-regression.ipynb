{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Pre-procesamiento de los datos"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\"\"\"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n\ndata=pd.read_csv(\"../input/heart-disease-prediction-using-logistic-regression/framingham.csv\") \ndata.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# exploramos la forma del data set\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# verificamos si hay valores nulos con la funcion isnull\npd.isnull(data[\"male\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# glucose tiene valores nulos, con las funciones ravel y sum \npd.isnull(data[\"glucose\"]).ravel().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tambien podemos usar esta funcion para hallar los valores nulos\nlen(data[pd.isnull(data.glucose)]) # 388\n# como es un tipo de dato que nos dice los niveles de glucosa lo mejor seria rellenarlos con el promedio con la funcion fillna\ndata[\"glucose\"]=data[\"glucose\"].fillna(data[\"glucose\"].mean()) \n# comprobamos que elos valores nulos del data set ahora son cero \nlen(data[pd.isnull(data.glucose)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observamos la cantidad de datos nulos en educacion y como no tiene descripción y no es muy importante para el caso de estudio la eliminaremos \npd.isnull(data[\"education\"]).ravel().sum() \ndata=data.drop([\"education\"],axis=1) \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# la variable cigsPerDay tambien presenta datos faltantes, por la naturaleza los remplazaremos con el promedio\ndata[\"cigsPerDay\"]=data[\"cigsPerDay\"].fillna(data[\"cigsPerDay\"].mean())\npd.isnull(data[\"cigsPerDay\"]).ravel().sum() #conprobamos que ya no haya valores faltantes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# como la varible BPmeds puede tener una gran significancia y solo son 53 observaciones eliminaremos los valores nulos del dataset\n# eliminaremos las filas con valores faltantes de la columnas BPMeads #https://interactivechaos.com/es/python/scenario/eliminacion-de-filas-en-un-dataframe-pandas-partir-de-una-condicion\npd.isnull(data[\"BPMeds\"]).ravel().sum()\ndf = data[data.BPMeds.isnull()] #este proceso se llma indexar para que con drop podamos eliminar solo los valores nulos de una fila\ndf.shape # aqui vermos el dataset con los valores nulos\ndf.index # optenemos el indice\ndata.drop(data[data.BPMeds.isnull()].index, inplace = True) # aqui ya usamos drop y elimonamos las filas con na de la columna BPmeds\ndata.shape # vemos la nueva cantidad de datos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# la variable toChol al ser los niveles de colesterol, continua remplazaremos los faltantes con la media\ndata[\"totChol\"]= data[\"totChol\"].fillna(data[\"totChol\"].mean())\npd.isnull(data[\"totChol\"]).ravel().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BMI de igual manera al ser el indice de masa corporal lo remplazamos por la media\n\ndata[\"BMI\"]=data[\"BMI\"].fillna(data[\"BMI\"].mean()) \npd.isnull(data[\"BMI\"]).ravel().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aqui usamos el metodo foward fill que es llenar la obs faltante con la inmediata anteriror \ndata[\"heartRate\"]= data[\"heartRate\"].fillna(method= \"ffill\") \npd.isnull(data[\"heartRate\"]).ravel().sum()\n\n#con este terminamos de quitar los valores nulos del dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analisi exploratorio de los datos "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n# Iniciamos el analisis explorando la columna male par ver si puede afectar a las enfermedades cardiacas, pienso que la 0 es de hombres y 1 de mujeres\ndata[\"male\"].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Podemos hacer primeramente graficas cruzadas para ver quien se enferma más \n%matplotlib inline \npd.crosstab(data.male,data.TenYearCHD).plot(kind=\"bar\")\nplt.xlabel(\"Sexo\")\nplt.ylabel(\"Riesgo de enfermedad\")\nplt.title(\"Riesgo por sexo\")\n\n# este pequeño analisis arroja que es más probable que siendo mujer adquiera una enfermedad cardiaca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analisamos si el ser o no FUMADORES puede afectar y servir para el modelo \ntabla_con_smoke= pd.crosstab(data.currentSmoker,data.TenYearCHD)\ntabla_con_smoke.astype(\"float\").div(tabla_con_smoke.sum(axis=1),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\npd.crosstab(data.currentSmoker,data.TenYearCHD).plot(kind=\"bar\")\nplt.xlabel(\"Fumador\")\nplt.ylabel(\"Riesgo\")\n# sorprendentemente no nos dice mucho si es oh  no fumador ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data[\"cigsPerDay\"])\nplt.xlabel(\"cigarros por día\")\nplt.ylabel(\"Personas del estudio\")\n\n# analizando el histograma podemo concluir que el hecho de que fumen no representa un aumento significativo de enfermedades cardiacas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tabla=pd.crosstab(data.BPMeds, data.TenYearCHD)\ntabla.astype(float).div(tabla.sum(axis=1),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# analizamos ahora la variable bpmeds que es si el pacient tiene medicacion via intravenosa \npd.crosstab(data.BPMeds, data.TenYearCHD).plot(kind=\"bar\")\nplt.xlabel(\"Medicación intravenosa\")\nplt.title(\"Riesgo de enfermedad con respecto a medicación \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#graficamos haciendo proporcionales las barras para poder ver mejor si hay mas casos si tienes medicación intravenosa, al parecer sí\n\ntable1=pd.crosstab(data.BPMeds, data.TenYearCHD)\ntable1.div(table1.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)\nplt.xlabel(\"Medicacion intravenosa 0= no, 1=sí\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ahora revisaremos prevalent stroke para ver si existio un para anteriro \ndata[\"prevalentStroke\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prevalent hyp, si el paciente tiene hipertension \npd.crosstab(data.prevalentHyp,data.TenYearCHD).plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table3=pd.crosstab(data.prevalentHyp,data.TenYearCHD)\ntable3.div(table3.sum(axis=1).astype(float),axis=0).plot(kind=\"bar\",stacked=True) \n# la hipertension definitivamente cuenta para las enfermedades del corazon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ahora analizaremos diabetes \ntable4=pd.crosstab(data.diabetes,data.TenYearCHD)\ntable4.div(table4.sum(1),axis=0).plot(kind=\"bar\",stacked=True) \nplt.title(\"Enfermedades cardiovasculares con respecto a diabetes\") \n\n# aqui tambien analizamos la diabetes y arroja resultado tambien positivo ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corresponde el turno de colesterol(tot chol) y con este el de las variables continuas \ndata[\"totChol\"].hist() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table= pd.crosstab(data.totChol,data.TenYearCHD)\ntable.div(table.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nComo la variable tochol es continua procedere a transformarla en una varible categorica que se dividira de la siguiente manera, si es mayor o no a 240 pues segun articulos esto se considera alto y \npuede traer concecuancias para la salud de y enfermedades cardivasculares \n\nhttps://medlineplus.gov/spanish/pruebas-de-laboratorio/niveles-de-colesterol/\n\"\"\"\n# usaremos la funcion tu cut para esto\ncategoria= pd.cut(data.totChol,bins=[0,240,696],labels=[0,1]) # cut es como lo cortara y label el valor que se le asgnaran\ncategoria\ndata.insert(6,\"Colesterol\",categoria) # 9 es la posicion de la nueva columna, despues el nombre y por ultimo la columna que agregaremos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table= pd.crosstab(data.Colesterol,data.TenYearCHD)\ntable.div(table.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table= pd.crosstab(data.Colesterol,data.TenYearCHD)\ntable.div(table.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ahora analizaremos la varible Sys blood presurr\ndata[\"sysBP\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table= pd.crosstab(data.sysBP,data.TenYearCHD)\ntable.div(table.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ahora analizaremos la varible dia blood presurr\ndata[\"diaBP\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table= pd.crosstab(data.diaBP,data.TenYearCHD)\ntable.div(table.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  vemos ahora el indice de masa corporal \ndata[\"BMI\"].hist() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#como el resultado tranformaremos tambien la varible colesterol para dividrilo pasarlo a uno con más riesgo \ndata[\"BMI\"].mean() # 25.84 \ndata[\"BMI\"].max() # 56.8\ncategoria= pd.cut(data.BMI,bins=[0,27.5,56.8],labels=[0,1]) # cut es como lo cortara y label el valor que se le asgnaran\ncategoria\ndata.insert(12,\"IMC\",categoria)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table=pd.crosstab(data.IMC,data.TenYearCHD)\ntable.div(table.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## vemos la relacion ahora con los latidos \ndata[\"heartRate\"].hist()  \n\n# la variable no dice demasiado así que no se evaluara","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table=pd.crosstab(data.glucose,data.TenYearCHD)\ntable.div(table.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Selección de variables del modelo **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ahora quitamos la variable TenYears pues es nuestra variable a predecir\ndata_vars= data.columns.values.tolist()\nY= [\"TenYearCHD\"]\nX= [v for v in data_vars if v not in Y]\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importamos paquetes necesarios \n\nfrom sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr= LogisticRegression() # cargamos el modelo a una variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr= LogisticRegression() # cargamos el modelo a una variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# se usaran 12 variables para el modelo \nn=12\nrfe= RFE(lr,n) #Cargamos las regresion lineal  y el numero de variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe= rfe.fit(data[X],data[Y].values.ravel()) # pasamos para el modelo las variables del vector X como independientes e Y como dependiente\n# values.ravel es para hacer el vector Y tipo fila porque si no se pasa como vector columna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rfe.support_) # imprimimos u a especie de vector con los resultados de que variables estaran en el modelo ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rfe.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hacemos un zip para crizar los datos de las columnas con los de rfe.support\nz=zip(data_vars,rfe.support_)\nprint(z) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(z)\ncols=[\"male\",\"age\",\"BPMeds\",\"prevalentHyp\",\"diabetes\",\"Colesterol\",\"sysBP\",\"diaBP\",\"BMI\",\"IMC\",\"heartRate\",\"glucose\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# si esta linea no se corre da este error #TypeError: cannot perform reduce with flexible type quiza porque Y no esta bien cargado o porque se carga como un tipo distindto de dato\nX=data[cols]\nY=data[\"TenYearCHD\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **implementacion del modelo con Sciki-learn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nlogistic_model2=linear_model.LogisticRegression()\nlogistic_model2.fit(X,Y) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_model2.score(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creamos un data frame cruzando el zip de la columna x y las cruzamos con la prediccion\npd.DataFrame(list(zip(X.columns,np.transpose(logistic_model2.coef_)))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cros validation "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vamos a relizar el modelo con scik learn  pero con las varibles de entrenamiento \nlm=linear_model.LogisticRegression()\nlm.fit(X_train,Y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Asignamos en una variable las probabilidades que seran necesarias para el modelo\nprobabilidades=lm.predict_proba(X_test)\n#CARGAMOS paquetes para la validación\nfrom IPython.display import display, Math, Latex\ndisplay(Math(r'Y_p=\\begin{cases}0& si\\ p\\leq0.5\\\\1&si\\ p >0.05\\end{cases}')) # como el programa clasifica las pro por defecto\nprobabilidades","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sacamos la predicción \npredicción=lm.predict(X_test) \nprobabilidades[:,1] \n# podemos ajustar el modelo menos estricto que 0.05, creamos un nuevo array de probabilidades \nprops=probabilidades[:,1] # para obtener solo la segunda columna\nprob_df=pd.DataFrame(props) \ntreshold=0.1\nprob_df[\"predicción\"]=np.where(prob_df[0] > treshold,1,0)\n# esta linea dice si la probabilidad en la columna cero es mayor a threshold se catagolara como propenso es decir 1 y si no sera ceros es decir no propenso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculamos cuantos caen como propensos, recurda que son los datos ya de predicción no de entrenamiento\npd.crosstab(prob_df.predicción,columns=\"counts\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lo vemos en porcentaje \n784/len(prob_df) *100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vamos ahora a verificar la eficacia del modelo \nfrom sklearn import metrics\nmetrics.accuracy_score(Y_test,predicción)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.cienciadedatos.net/documentos/30_cross-validation_oneleaveout_bootstrap\n\"\"\"\nExisten varios tipos de clasificaciones pero ahora utilizaremos el K-fold-cross validation\n\"\"\"\nfrom sklearn.model_selection import cross_val_score\n#scoring es la presicion de lo que necesitamos del modelo, cv es el numero de k que se dividira el modelo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores= cross_val_score(linear_model.LogisticRegression(),X,Y, scoring=\"accuracy\",cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aqui ya hacemos predicciones con nuestros datos de entrenamiento \nscores\nscores.mean() # el promedio de predicción de el modelo ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  **Matrices de confución**  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# le agregaremos un columna que seran los casos reales \nprob_df[\"Real\"]=list(Y_test) \nprob_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#optenemos la matriz de confusion \nmc=pd.crosstab(prob_df.predicción,prob_df.Real)\nmc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sacamos el verdadero positivo\nTP=mc[1][1]\nTP # 158\n# verdadero negativo\nTN=mc[0][0] \nTN # 439\n# falso negativo\nFP=mc[1][0]\nFP # 33\n# falso negativo\nFN=mc[0][1]\nFN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\noptemenmos las metricas de la matriz de confusion iniciando con la exactitud o accuracy que es la proporcion \nde predicciones de enfermedad que elmodelo calsifico correctamente \n\"\"\"\nAcuurracy= (TP+TN)/(TP+TN+FP+FN)\nAcuurracy *100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La precisión también se conoce como valor predictivo positivo y es la proporción de instancias relevantes entre las instancias recuperadas. En otras palabras, responde a la pregunta \"¿Qué proporción de identificaciones positivas fue realmente correcta?\"\nPresición=TP/(TP+FP)\nPresición*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sensibilidad VP/(VP+FN),  Es la proporción de casos positivos que fueron correctamente identificadas por el algoritmo. \nsensibilidad= TP/(TP+FN) \nsensibilidad # 0.20153061224489796\n# Especificidad También conocida como la Tasa de Verdaderos Negativos, (“true negative rate”) o TN. Se trata de los casos negativos que el algoritmo ha clasificado correctamente.  Expresa cuan bien puede el modelo detectar esa clase. \nrecall= TN/(TN+FP)\nrecall*100\n\n# IDENTIFICAMOS los sanos entre los sanos","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}