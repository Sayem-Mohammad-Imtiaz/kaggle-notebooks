{"cells":[{"metadata":{"_uuid":"487e59d2748294785deef4586a4634a77b28c818"},"cell_type":"markdown","source":"## Introduction\nThanks Kaggle bot, I'll take the tour from here. We have five sets of data prefixed **gamer*N*-** (from five computer gamers) each containing:\n\n- A diary of annotations in **annotations.csv** including:\n  - ['sleep-2-peak' reaction time](https://sleep-2-peak.com/) each hour\n  - caffeine and food ingress and egress\n  - self-assessment [Stanford sleepiness scale (1-7)](https://web.stanford.edu/~dement/sss.html) each hour\n- A red light transmission PPG time-series sampled at approx 100Hz\n  - this spans two files, **ppg-2000-01-01.csv** and **ppg-2000-01-02.csv**, each about 12hrs long"},{"metadata":{"_uuid":"8c04cb612041538ed0bd1a73df05b60469e28c78"},"cell_type":"markdown","source":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`.  [Here is a superb catalogue of plots and their statistical usefulness.](https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python)"},{"metadata":{"_kg_hide-input":false,"_uuid":"7cada04d8edf4950f04fa329b0488a5a3d741671","trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f37c9ce1ef1f49be7babc915596265cc58fe111"},"cell_type":"markdown","source":"There are 5 x (1 + 2) = 15 csv files in the dataset:\n"},{"metadata":{"_kg_hide-input":false,"_uuid":"d6fa1849c650e1cdd64e886b5b37c19fa71b56e4","trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"949d6562848085170f93e5f04360055ab8cf74f8"},"cell_type":"markdown","source":"### (\n\nThe next hidden code cells are Kaggle's default functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code. We're not using these for this dataset."},{"metadata":{"_kg_hide-input":true,"_uuid":"2ee08ee8d24a2c7855b4d9808733ec4da238dd6a","trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"246ba1e37cc2458e21789548879b6039db60d78c","trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"98d790bd24a35ef3a95585838f20c4863eb20c04","trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### )"},{"metadata":{"_uuid":"5fc9d683a66af16a3f90d8c5acf42abda33ee4e4"},"cell_type":"markdown","source":"## Pick a gamer 'gamer[1-5]'"},{"metadata":{"trusted":true,"_uuid":"16c58f0d487f541b60197989790a280bebd78cc3"},"cell_type":"code","source":"gamerID = 'gamer5'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ee852886674a5903e3ddef9ba90470475b05350"},"cell_type":"markdown","source":"### Let's load the annotations file: ../input/gamer*N*-annotations.csv"},{"metadata":{"_kg_hide-input":false,"_uuid":"1252a010030e11ecbae40e3bad913c6e67bfa926","trusted":true},"cell_type":"code","source":"dateCols = ['Datetime']\nanots = pd.read_csv('../input/' + gamerID + '-annotations.csv', parse_dates=dateCols)\nanots.dataframeName = gamerID + '-annotations'\nanots.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6feffd9a607e75c3a083d2504c2331e915e61031"},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"_uuid":"a26c8ff0aed417a4943ab3719201e0d953f70d5d","trusted":true},"cell_type":"code","source":"anots.head(30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3331a7add55e037c799a56823b3f1f8dce230960"},"cell_type":"markdown","source":"Hopefully there will be some correlation between the Stanford Sleepiness Self-Assessment and the Reaction Time Test results:"},{"metadata":{"trusted":true,"_uuid":"8c6effb4f124d1154cc5ed5fcec672cd694bb387"},"cell_type":"code","source":"#dfxy = anots.pivot(index='Datetime', columns='Event') # Unpack key,value pairs as columns with time x-axis\n#dfxy.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53a353cbe90a081d851a50cc26f70a68a0203dd1"},"cell_type":"markdown","source":"Compare self-assessment with measured reaction times:"},{"metadata":{"_kg_hide-input":false,"_uuid":"130f596256b60325d01369c3e3f6a0205f6a819a","trusted":true},"cell_type":"code","source":"sss = anots[anots.Event == \"Stanford Sleepiness Self-Assessment (1-7)\"].drop('Event', axis=1).copy()\nsss['SelfAssess'] = sss['Value'].map(lambda x: float(x))\n\nrt = anots[anots.Event == \"Sleep-2-Peak Reaction Time (ms)\"].drop('Event', axis=1).copy()\nrt['ReactTime'] = rt['Value'].map(lambda x: float(x))\n\ndiary = anots[anots.Event == \"Diary Entry (text)\"].drop('Event', axis=1).copy()\n\nfatigueplot = plt.figure(figsize=(7,4), dpi= 150)\naxsa = fatigueplot.add_subplot(1,1,1)\naxsa.set_title('Sleepiness of ' + gamerID + ' through episode')\naxsa.set_xlabel('Time of day')\nplt.xticks(rotation=90)\naxsa.set_xlim(pd.Timestamp('2000-01-01 11:00'), pd.Timestamp('2000-01-02 11:00:00'))\naxsa.xaxis.set_major_locator(mpl.dates.MinuteLocator())\naxsa.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d %H:%M'))\n#axsa.xaxis.set_minor_formatter(mpl.dates.DateFormatter('%H:%M'))\naxsa.set_ylabel('sleepiness self assessment (1-7)', color='b')\naxsa.set_ylim(0.9,7.1)\naxrt = axsa.twinx()\naxrt.set_ylabel('reaction time (ms)', color='r')\n\naxsa.plot('Datetime', 'SelfAssess', 'b-', data=sss)\naxrt.plot('Datetime', 'ReactTime', 'r-', data=rt)\n\nfor item in diary.iterrows():\n    s = item[1]\n    axsa.axvline(s.Datetime, linewidth=0.2, color='g')\n    axsa.text(s.Datetime, -1.0, s.Value, rotation=90, fontsize='xx-small',\n              color='g', alpha=0.5, horizontalalignment='right')\n\n#fatigueplot.autofmt_xdate()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fatigueplot.savefig('./'+ gamerID +'-annotations.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeafc0d6d7fe01e2d50cab847a5fba77784d76d2"},"cell_type":"markdown","source":"### Let's check the PPG time-series files: ../input/gamer*N*-ppg-2000-01-01.csv"},{"metadata":{"_kg_hide-input":false,"_uuid":"56bbb2126c868cd27e43b212d0a8b30c72dbc324","trusted":true},"cell_type":"code","source":"nRowsRead = 2000 # specify 'None' if want to read whole file\n# gamer1-ppg-2000-01-01.csv has 2,996,500 rows (about 12hrs) in reality\n# but we are only previewing the first 2000 rows\nppg = pd.read_csv('../input/' + gamerID + '-ppg-2000-01-01.csv', delimiter=',', nrows = nRowsRead)\nppg.dataframeName = gamerID + '-ppg-2000-01-01.csv'\nppg.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7fb6a953c48fa20c34d1190689c3f64b1c426c6"},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"_uuid":"45e1194eb00441f529ffcc735793758985593443","trusted":true},"cell_type":"code","source":"ppg.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d0ff4efa0a38f3fb6148661515e5f875e3919e6"},"cell_type":"markdown","source":"Note that the microsecond timestamps jump in steps of a few tens of microseconds even though the samples are nominally about 10,000 microseconds apart. In fact timestamps are even more messed up at the start of the file, so let's first just plot them as if they are just an equi-spaced array of samples.\n\nThis should be a nice simple time-series showing each pulse:"},{"metadata":{"_kg_hide-input":false,"_uuid":"4884860e426adcd446626d0e45b0560341179638","trusted":true},"cell_type":"code","source":"ts = plt.figure(figsize=(7,4), dpi= 150)\nax = ts.add_subplot(1,1,1)\nax.set_title('PPG time series for ' + gamerID)\nax.set_xlabel('Sample')\nax.set_xticklabels([])\nax.set_ylabel('Red transmission', color='r')\nax.plot('Time', 'Red_Signal', 'r-', data=ppg)\nts.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plots the samples side-by side, so the step-changes of the timestamps are not significant. Let's look at how the timestamp steps affect the detail:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ppg['timestamp'] = pd.to_datetime(ppg['Time'])\n\nppg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_t_axes(data_range):\n    \"\"\"\n    plots time series as a resampled series above the time-stamped version\n\n    Parameters\n    ----------\n    data_range : pandas.dataframe\n        Pandas dataframe with columns 'Red_signal' for y-axis and 'timestamp' for x-axis\n    \"\"\"\n    ts, (ax, tsax) = plt.subplots(2, figsize=(7,4), dpi= 150)\n    ts.suptitle('PPG time series')\n    ax.set_title('at constant sample rate:')\n    ax.set_ylabel('Red tx', color='r')\n    ax.xaxis.set_visible(False)\n    ax.plot('Time', 'Red_Signal', 'r-', data=data_range)\n    \n    tsax.set_title('and using timestamps:')\n    tsax.set_xlabel('Timestamp Time')\n    plt.xticks(rotation = 90)\n    tsax.set_ylabel('Red tx', color='r')\n    tsax.plot('timestamp', 'Red_Signal', 'r-', data=data_range)\n    ts.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_t_axes(ppg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So for the first 800 samples at the beginning of data capture Raspberry Pi Linux serial port buffering makes funny things happen with the timestamps in the Time column. It does look like it settles down after a couple of seconds though, so if we ignore the first 700 samples it looks better:"},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_t_axes(ppg[1000:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's zoom in to look at the quantisation noise that the jumping timestamp causes close-up:"},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_t_axes(ppg[1000:1100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For signal processing purposes we should probably treat it as a series of equi-spaced samples and calculate the exact sample rate from a lengthy sample to determine accurate R-R intervals. All the statistical methods work only with time-series resampled at a constant samping rate anyway."},{"metadata":{"_uuid":"e78b7c35d958d84373d383c9f105e97ae2b4a433"},"cell_type":"markdown","source":"### Finally check 3rd file: ../input/gamer*N*-ppg-2000-01-02.csv"},{"metadata":{"_kg_hide-input":false,"_uuid":"b6cd99e389b9d54e654fe436098f194839f9f98a","trusted":true},"cell_type":"code","source":"nRowsRead = 2000 # specify 'None' if want to read whole file\n# gamer1-ppg-2000-01-02.csv has 3,177,175 rows (approx 12hrs) in reality,\n# but we are only loading/previewing the first 2000 rows\nppg2 = pd.read_csv('../input/' + gamerID + '-ppg-2000-01-02.csv', delimiter=',', nrows = nRowsRead)\nppg2.dataframeName = gamerID + '-ppg-2000-01-02.csv'\nppg2['timestamp'] = pd.to_datetime(ppg2['Time'])\nppg2.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79bd976ee456b2ffb03f4508243e132be9a014f7"},"cell_type":"markdown","source":"This is just the continuation of the first file after midnight. There's also the same first 1000 samples garbage timestamps issue as there was at the beginning of data capture in the previous day's file:"},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_t_axes(ppg2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" And 1ms jumps in the timestamp are still best ignored on an already noisy signal:"},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_t_axes(ppg2[1000:1200])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51cce898bfd646bfb231167c4236c476ab24e25f"},"cell_type":"markdown","source":"It's not a bad signal and getting accurate R-R intervals good enough for Heart Rate Variability analysis should be achievable."}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}