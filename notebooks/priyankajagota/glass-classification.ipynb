{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ***Glass Classification by Machine learning***"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from PIL import ImageTk, Image  \nimage = Image.open(\"../input/glassss/Untitledw21.png\")\nimage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 1 -: Importing Libraries and Data**# "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd # Importing pandas for data manipulation and analysis\nimport numpy as np # Importing python linear algebra library to do work with arrays\nfrom sklearn.model_selection import train_test_split # to split the data into train and test sets\nimport seaborn as sns # to import a python library to create alluring and communicative plots and graphs\nimport matplotlib.pyplot as plt # to import the ploting library of python language\nfrom sklearn.preprocessing import StandardScaler #Importing the Standard Sclaer from sklearn.preprocessing\nfrom sklearn.model_selection import GridSearchCV # Import grid search to choose the best parameters of model\nimport warnings # to import warnings\nwarnings.filterwarnings('ignore') # to import warnings as 'ignore'\nfrom sklearn.metrics import accuracy_score # Import accuracy score function of python that helps to evaluate models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/glass/glass.csv\") # to import csv file as data frame\nprint(df) # to see the dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 2 -: Data Preprocessing**# "},{"metadata":{},"cell_type":"markdown","source":"##  *i)Checking for missing data*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking For Null values in our datasets and then removing the same.\npd.DataFrame(df.isna().sum()) #This will give the snapshot if me have any null values in our dataset.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Analysis -: Data contains no missing values, which is a good thing !"},{"metadata":{},"cell_type":"markdown","source":" ## **ii)Checking Inconsistency in the data values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Analysis -: As we can see frome the above output, that the data is pretty much consistence as data type of all values of a particular feature is similar!\n> ### For example, all the values in \"Type\" column have int64 as data type."},{"metadata":{},"cell_type":"markdown","source":" ## **iii)Checking duplicate values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to find the row that has occured more than once in a dataset\nduplicate_rows = df[df.duplicated()]\nprint(duplicate_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates() # drop duplicate values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Analysis -: It is important to remove duplicate rows in order to avoid bais in dataset. As our dataset contains some duplicate rows, therefore, I have removed those rows to abstain any partiality in data"},{"metadata":{},"cell_type":"markdown","source":"# **Part 3 -: Exploratory Data Analysis**# "},{"metadata":{},"cell_type":"markdown","source":" ## **i)Univariant Analysis of Numerical Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing numeric variables using seaborn\nsns.set(font_scale=1.5)\nsns.set_style(style='darkgrid')\nf, axes = plt.subplots(3,3,figsize=(25,25))\nsns.distplot( df[\"RI\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#0000ff\"),color=\"black\", ax=axes[0, 0])\nsns.distplot( df[\"Na\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#00cc00\"),color=\"black\", ax=axes[0, 1])\nsns.distplot( df[\"Mg\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#e68a00\"),color=\"black\", ax=axes[0, 2])\nsns.distplot( df[\"Al\"] , hist_kws=dict(edgecolor=\"k\", linewidth=2,color=\"#992600\"),color=\"black\", ax=axes[1, 0])\nsns.distplot( df[\"Si\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#e600ac\"),color=\"black\", ax=axes[1, 1])\nsns.distplot( df[\"K\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"skyblue\"),color=\"black\", ax=axes[1, 2])\nsns.distplot( df[\"Ca\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2,color='orange'), color=\"black\", ax=axes[2, 0])\ndf['Ba'].plot.hist(color=['olive'],edgecolor=\"k\", linewidth=2,ax=axes[2, 1],title='Ba')\nsns.distplot( df[\"Fe\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"red\"),color=\"black\", ax=axes[2, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Analysis -:i) RI -: The graph shows multimodal and right skewed distribution. It depicts that the value of refractive index for most of samples of different elements is in between 1.515 to 1.520.\n> ### ii)Na,Al and Fe has Unimodal distribution with the highest values lying in the range of 12 - 14, 1 - 2, -0.1 to 0.1 respectively.\n> ### iii)However,Ca,K,Si,Mg have bimodal distribution.Whereas, distribution of Ba is right skewed."},{"metadata":{},"cell_type":"markdown","source":" ## **ii)Univariant Analysis of Categorical Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing categorical features\ndf['Type'].value_counts().plot.bar(color=['olive','skyblue','red','orange','pink','blue'],title='Type of glass',edgecolor=\"k\", linewidth=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Analysis -:According to the above output, 2nd type of glass has occured most frequently in the dataset."},{"metadata":{},"cell_type":"markdown","source":" ## **iii)Multivariant analysis of all the features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create correlation matrix\ncorrMatrix = round(df.corr(),1)\nplt.figure(figsize=(16,11))\n# to plot the matrix as heat map\nsns.heatmap(corrMatrix,annot=True,cmap='seismic',linewidths=2,linecolor='black')\nplt.title(\"Heatmap Correlation of Heart Failure Prediction\", fontsize = 23)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Analysis -:According to the correlation matrix, we can find out some interesting facts :\n> ### i) Calcium and refractive index are positively correlated to each other,which means on increasing the value of one, other will increase somewhat linearly.\n> ### ii) However, Type and Mg are negatively correlated to each other which demonstrate that both are nearly inversely proportional to each other."},{"metadata":{},"cell_type":"markdown","source":"# **Part 4 -: Splitting and Scaling of Data**# "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Type', axis=1).values #Feature datasets for the purpose of calculation.\ny = df['Type'].values #Target data sets for the purpose of calculations.\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=42,shuffle=True)#Splitting the data into train and test sets.\npd.DataFrame(X_train) # to have a look at trained dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating an object of Scaler\nscaler = StandardScaler()\n\n#Fitting the training features\nscaler.fit(X_train)\n\n#transforming the train features\nX_train_scaled = scaler.transform(X_train)\n\n#transforming the test features\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 5 -: Models**#"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create two separate train and test lists\ntrain_accuracies=[]\ntest_accuracies=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## **i)Support Vector Classification (SVC) Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Part 1 -: SELECT THE BEST HYPERPARMETERS WITH HELP OF GRID SEARCH\nfrom sklearn.svm import SVC \n# defining parameter range \nHyper_parameters = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \nGridSearch_svc = GridSearchCV(estimator = SVC(),\n                               param_grid = Hyper_parameters,\n                               cv = 15,\n                               n_jobs = -1)\nGridSearch_svc.fit(X_train_scaled, y_train)\n\nprint(\"Best hyperparameters for model:\"+str(GridSearch_svc.best_params_))\nprint(\"Best estimator for model:\"+str(GridSearch_svc.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Part 2 -: Build a model with the help of best parameters\nsvc = SVC(C=100, gamma=0.1,kernel='rbf',probability=True)\nsvc.fit(X_train_scaled, y_train)\n# to predict the target values\npred_svc_test = svc.predict(X_test_scaled)\npred_svc_train = svc.predict(X_train_scaled)\ntrain_accuracy_svc=accuracy_score(y_train,pred_svc_train)*100\ntest_accuracy_svc=accuracy_score(y_test,pred_svc_test)*100\ntrain_accuracies.append(train_accuracy_svc)\ntest_accuracies.append(test_accuracy_svc)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_svc_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_svc_test)*100) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## **ii)Random Forest Classifier Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Part 1 -: Hypertuning of parameters\nfrom sklearn.ensemble import RandomForestClassifier\n#The structure that Scikit-learn needs to run Grid search\nparam_grid={'max_depth':[3,4,5],\n           'max_leaf_nodes':[10,15,20],\n            'min_samples_leaf':[10,15,20,25]}\nfrom sklearn.model_selection import GridSearchCV\n#applying GridSearch on a Decisiontree classifier with a 3 different parameters:\ngrid_search = GridSearchCV(RandomForestClassifier(n_estimators=11,random_state=573),param_grid,cv=10,return_train_score=True)\ngrid_search.fit(X_train_scaled,y_train)\nprint(\"Best parameters:\"+str(grid_search.best_params_))\nprint(\"Best estimator:\"+str(grid_search.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Part 2 -: Build a model with the help of best parameters\nrf = RandomForestClassifier(max_depth=5, max_leaf_nodes=10, min_samples_leaf=10,\n                       n_estimators=11, random_state=573)\nrf.fit(X_train_scaled, y_train)\n# to predict the target values\npred_rf_test = rf.predict(X_test_scaled)\npred_rf_train = rf.predict(X_train_scaled)\ntrain_accuracy_rf=accuracy_score(y_train,pred_rf_train)*100\ntest_accuracy_rf=accuracy_score(y_test,pred_rf_test)*100\ntrain_accuracies.append(train_accuracy_rf)\ntest_accuracies.append(test_accuracy_rf)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_rf_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_rf_test)*100) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## **iii)XG Boost Classifier Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Part 1 -: Hypertuning of parameters\n#XGBoost\nfrom xgboost import XGBClassifier\nxg=XGBClassifier(random_state=573)\n\n#List Hyperparameters that we want to tune.\n\nparameter_grid_xg={'learning_rate':[0.05, 0.10, 0.15, 0.20],'max_depth':[3,4,5],'gamma':[ 0.0, 0.1, 0.2 , 0.3]}\ngridsearch_xg = GridSearchCV(xg, parameter_grid_xg,cv=15)\ngridsearch_xg.fit(X_train_scaled, y_train);\n\n#Get best hyperparameters\ngridsearch_xg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Part 2 -: Build a model with the help of best parameters\nxg =XGBClassifier(gamma=0.2,learning_rate=0.05,max_depth=4,random_state=573)\nxg.fit(X_train_scaled, y_train)\n# to predict the target values\npred_xg_test = xg.predict(X_test_scaled)\npred_xg_train = xg.predict(X_train_scaled)\ntrain_accuracy_xg=accuracy_score(y_train,pred_xg_train)*100\ntest_accuracy_xg=accuracy_score(y_test,pred_xg_test)*100\ntrain_accuracies.append(train_accuracy_xg)\ntest_accuracies.append(test_accuracy_xg)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_xg_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_xg_test)*100) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 6 -: Compare the models**#"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of labels of build models\nlabel = ['SVC','Random Forest','XG Boost']\nprint(label)\n\n#checking the train and test accuracies for all the parameter values\ntrain_accuracy = [round(num, 2) for num in train_accuracies]\nprint(\"Train Accuracies \"+str(train_accuracy))\n\ntest_accuracy = [round(num, 2) for num in test_accuracies]\nprint(\"\\nTest Accuracies \"+str(test_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy dataframe\nAcc_df = pd.DataFrame({'Model':label,'Train Accuracy(%)': train_accuracy,'Test Accuracy(%)': test_accuracy})\n# Plot the heat map for the dataframe\nAcc_df.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Analysis -: As per above output, I choose XG Boost as the best model as it's Train and Test accuracies are more than other models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}