{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Modules & Helpful Functions","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install --upgrade neural_structured_learning --user","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings; warnings.filterwarnings('ignore')\nimport numpy as np,pylab as pl,pandas as pd\nimport sys,h5py,urllib,zipfile\nimport tensorflow as tf\nimport neural_structured_learning as nsl\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def display_10img(X,y,s):\n    fig,ax=pl.subplots(figsize=(10,3),nrows=2,ncols=5,\n                       sharex=True,sharey=True)\n    ax=ax.flatten()\n    for i in range(10):\n        ax[i].imshow(X[i].reshape(s,s),cmap=pl.cm.Greens)\n        ax[i].set_title(y[i])\n    ax[0].set_xticks([]); ax[0].set_yticks([])\n    pl.tight_layout()\ndef prepro(x_train,y_train,x_test,y_test,n_class):\n    n=int(len(x_test)/2)    \n    x_valid,y_valid=x_test[:n],y_test[:n]\n    x_test,y_test=x_test[n:],y_test[n:]\n    cy_train=to_categorical(y_train,n_class) \n    cy_valid=to_categorical(y_valid,n_class)\n    cy_test=to_categorical(y_test,n_class)\n    df=pd.DataFrame([[x_train.shape,x_valid.shape,x_test.shape],\n                     [y_train.shape,y_valid.shape,y_test.shape],\n                     [cy_train.shape,cy_valid.shape,cy_test.shape]],\n                    columns=['train','valid','test'],\n                    index=['images','labels','encoded labels'])\n    display(df)\n    return [[x_train,x_valid,x_test],\n            [y_train,y_valid,y_test],\n            [cy_train,cy_valid,cy_test]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data\n### #1","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"digits=datasets.load_digits(n_class=10)\nX1,y1=digits.data,digits.target\nX_train1,X_test1,y_train1,y_test1=\\\ntrain_test_split(X1,y1,test_size=.2,random_state=1)\ndisplay_10img(X_train1,y_train1,8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_train2,y_train2),(X_test2,y_test2)=mnist.load_data()\nX_train2=X_train2.reshape(-1,784).astype('float32')\nX_test2=X_test2.reshape(-1,784).astype('float32')\ny_train2=y_train2.astype('int32')\ny_test2=y_test2.astype('int32')\n[[X_train2,X_valid2,X_test2],\n [y_train2,y_valid2,y_test2],\n [cy_train2,cy_valid2,cy_test2]]=\\\nprepro(X_train2,y_train2,X_test2,y_test2,10)\ndisplay_10img(X_train2,y_train2,28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fpath='../input/classification-of-handwritten-letters/'\nf=h5py.File(fpath+'LetterColorImages2.h5','r')\nkeys=list(f.keys()); rn=10**3\nletters=u'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\nX3=np.array(f[keys[1]])/255\nX3=1-np.dot(X3[...,:3],[.299,.587,.114])\ny3=np.array(f[keys[2]])-1\nX_train3,X_test3,y_train3,y_test3=\\\ntrain_test_split(X3,y3,test_size=.2,random_state=1)\nX_train3=X_train3.reshape(-1,32*32).astype('float32')\nX_test3=X_test3.reshape(-1,32*32).astype('float32')\ny_train3=y_train3.astype('int32')\ny_test3=y_test3.astype('int32')\n[[X_train3,X_valid3,X_test3],\n [y_train3,y_valid3,y_test3],\n [cy_train3,cy_valid3,cy_test3]]=\\\nprepro(X_train3,y_train3,X_test3,y_test3,33)\ndisplay_10img(X_train3,y_train3,32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Networks\n### #1","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def trainNN(X,y,D,K,h,a,r,step_size,epochs):\n    W1=a*np.random.randn(D,h)\n    b1=np.zeros((1,h))\n    W2=a*np.random.randn(h,K)\n    b2=np.zeros((1,K))\n    # gradient descent loop\n    num_examples=X.shape[0]\n    for i in range(epochs): \n        # evaluate class scores\n        hidden_layer=np.maximum(0,np.dot(X,W1)+b1) # ReLU activation\n        scores=np.dot(hidden_layer,W2)+b2  \n        # compute the class probabilities\n        exp_scores=np.exp(scores)\n        probs=exp_scores/np.sum(exp_scores,axis=1,keepdims=True)   \n        # compute the loss: average cross-entropy loss and regularization\n        corect_logprobs=-np.log(probs[range(num_examples),y])\n        data_loss=np.sum(corect_logprobs)/num_examples\n        reg_loss=.5*r*np.sum(W1**2)+.5*r*np.sum(W2**2)\n        loss=data_loss+reg_loss\n        if (i+1)%500==0:\n            print(\"iteration %d: loss %f\"%(i+1,loss))  \n        # compute the gradient on scores\n        dscores=probs\n        dscores[range(num_examples),y]-=1\n        dscores/=num_examples  \n        # backpropate the gradient to the parameters\n        # first backprop into parameters W2 and b2\n        dW2=np.dot(hidden_layer.T,dscores)\n        db2=np.sum(dscores,axis=0,keepdims=True)\n        # next backprop into hidden layer\n        dhidden=np.dot(dscores,W2.T)\n        # backprop the ReLU non-linearity\n        dhidden[hidden_layer<=0]=0\n        # finally into W,b\n        dW1=np.dot(X.T,dhidden)\n        db1=np.sum(dhidden,axis=0,keepdims=True)  \n        # add regularization gradient contribution\n        dW2+=r*W2; dW1+=r*W1  \n        # perform a parameter update\n        W1+=-step_size1*dW1; b1+=-step_size1*db1\n        W2+=-step_size1*dW2; b2+=-step_size1*db2\n    return [W1,b1,W2,b2]\ndef predict(X,W1,b1,W2,b2):\n    hidden_layer=np.maximum(0,np.dot(X,W1)+b1)\n    scores=np.dot(hidden_layer,W2)+b2\n    return np.argmax(scores,axis=1)\ndef accuracy(y,py): return np.mean(py==y)*100","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"D1=64 # dimensionality\nK1=10 # number of classes\nh1=320 # size of hidden layer\n# hyperparameters\na1=1e-3; step_size1=1e-2; epochs1=2500\nr1=1e-4 # regularization strength\n[W11,b11,W12,b12]=\\\ntrainNN(X_train1,y_train1,D1,K1,h1,\n        a1,r1,step_size1,epochs1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"py_train1=predict(X_train1,W11,b11,W12,b12)\nacc_train1=accuracy(y_train1,py_train1)\nprint('Digits. Train accuracy: %.2f'%acc_train1)\npy_test1=predict(X_test1,W11,b11,W12,b12)\nacc_test1=accuracy(y_test1,py_test1)\nprint('Digits. Test accuracy: %.2f'%acc_test1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=.001; epochs=12; hidden1=256; hidden2=256\nbatch_size=128; display_step=1; \nn_inputs=784; n_classes=10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def cat_accuracy(predictions,labels):\n    return (100.0*np.sum(np.argmax(predictions,1)==\\\n           np.argmax(labels,1))/predictions.shape[0])\ndef mlp(x):\n    layer1=tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n    layer2=tf.add(tf.matmul(layer1,weights['W2']),biases['b2'])\n    return tf.matmul(layer2,weights['out'])+biases['out']\ngraph=tf.Graph()\nwith graph.as_default():\n    X=tf.compat.v1.placeholder(\"float32\",[None,n_inputs])\n    y=tf.compat.v1.placeholder(\"int32\",[None,n_classes])\n    vX=tf.constant(X_test2)\n    weights={'W1':tf.Variable(\\\n                  tf.compat.v1.random_normal([n_inputs,hidden1])),\n             'W2':tf.Variable(\\\n                  tf.compat.v1.random_normal([hidden1,hidden2])),\n             'out':tf.Variable(\\\n                   tf.compat.v1.random_normal([hidden2,n_classes]))}\n    biases={'b1':tf.Variable(\\\n                 tf.compat.v1.random_normal([hidden1])),\n            'b2':tf.Variable(\\\n                 tf.compat.v1.random_normal([hidden2])),\n            'out':tf.Variable(\\\n                  tf.compat.v1.random_normal([n_classes]))}\n    logits=mlp(X); vlogits=mlp(vX)\n    loss=tf.reduce_mean(\\\n    tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=lr)\n    train_opt=optimizer.minimize(loss)\n    train_predictions=tf.nn.softmax(logits)\n    test_predictions=tf.nn.softmax(vlogits)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"with tf.compat.v1.Session(graph=graph) as sess:\n    tf.compat.v1.global_variables_initializer().run()\n    for epoch in range(epochs):\n        avg_loss=0.; avg_acc=0.\n        total_batch=int(X_train2.shape[0]/batch_size)\n        for i in range(total_batch):\n            offset=(i*batch_size)%(X_train2.shape[0]-batch_size)\n            batch_X=X_train2[offset:(offset+batch_size)]\n            batch_y=cy_train2[offset:(offset+batch_size)]\n            _,l,batch_py=sess.run([train_opt,loss,train_predictions],\n                                  feed_dict={X:batch_X,y:batch_y})\n            avg_loss+=l/total_batch\n            avg_acc+=cat_accuracy(batch_py,batch_y)/total_batch\n        if epoch%display_step==0:\n            print(\"Epoch: %04d\"%(epoch+1),\n                  \"loss={:.9f}\".format(avg_loss),\n                  \"accuracy={:.3f}\".format(avg_acc))\n    print(\"Test accuracy: %.3f%%\"%\\\n    cat_accuracy(test_predictions.eval(),cy_test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# data 2\nbatch_size=64; img_size=28; n_class=10; epochs=7","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"base_model=tf.keras.Sequential([\n    tf.keras.Input((img_size,img_size),name='input'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128,activation=tf.nn.relu),\n    tf.keras.layers.BatchNormalization(),    \n    tf.keras.layers.Dense(256,activation=tf.nn.relu),\n    tf.keras.layers.Dense(n_class,activation=tf.nn.softmax)\n])\nadv_config=nsl.configs\\\n.make_adv_reg_config(multiplier=.2,adv_step_size=.05)\nadv_model=nsl.keras\\\n.AdversarialRegularization(base_model,adv_config=adv_config)\nadv_model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train2=tf.data.Dataset.from_tensor_slices(\n    {'input':X_train2,'label':y_train2}).batch(batch_size)\nvalid2=tf.data.Dataset.from_tensor_slices(\n    {'input':X_valid2,'label':y_valid2}).batch(batch_size)\nvalid_steps=X_valid2.shape[0]//batch_size\nadv_model.fit(train2,validation_data=valid2,verbose=2,\n              validation_steps=valid_steps,epochs=epochs);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores2=adv_model.evaluate({'input':X_test2,'label':y_test2})\nprint(\"Test accuracy: %.1f\"%(scores2[2]*100.))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data3\nbatch_size=128; img_size=32; n_class=33; epochs=100","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"base_model=tf.keras.models.Sequential([\n        tf.keras.Input((img_size,img_size),name='input'),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128,activation='relu'),\n        tf.keras.layers.BatchNormalization(),    \n        tf.keras.layers.Dense(256,activation='relu'),\n        tf.keras.layers.BatchNormalization(),    \n        tf.keras.layers.Dense(512,activation='relu'),\n        tf.keras.layers.BatchNormalization(),   \n        tf.keras.layers.Dense(1024,activation='relu'),\n        tf.keras.layers.Dense(33,activation='softmax')\n    ])\nadv_config=nsl.configs\\\n.make_adv_reg_config(multiplier=.2,adv_step_size=.05)\nadv_model=nsl.keras\\\n.AdversarialRegularization(base_model,adv_config=adv_config)\nadv_model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train3=tf.data.Dataset.from_tensor_slices(\n    {'input':X_train3,'label':cy_train3}).batch(batch_size)\nvalid3=tf.data.Dataset.from_tensor_slices(\n    {'input':X_valid3,'label':cy_valid3}).batch(batch_size)\nvalid_steps=X_valid3.shape[0]//batch_size\nadv_model.fit(train3,validation_data=valid3,verbose=2,\n              validation_steps=valid_steps,epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores3=adv_model.evaluate({'input':X_test3,'label':cy_test3})\nprint(\"Test accuracy: %.1f\"%(scores3[2]*100.))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}