{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nfrom keras.preprocessing.text import Tokenizer\n# Packages for modeling\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers\n\nTweet= pd.read_csv(\"../input/Tweets.csv\")\nTweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there's 14650 rows and 15 columns of data. There's a lot of missing data within some of these columns and there's a lot of information here we don't need so let's reduce the dataframe to the columns of interest, and rename them and set the dates as the index."},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet = Tweet.loc[: , ['airline_sentiment', \n                         'airline_sentiment_confidence',\n                         'negativereason',\n                         'negativereason_confidence',              \n                         'name',\n                         'text',\n                         'tweet_coord',\n                         'tweet_created',\n                         'airline']]\nTweet = Tweet.set_index('tweet_created')\nTweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect the airlines by grouping the data by airlines and indexing each of their ratings"},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet.groupby('airline')['airline_sentiment'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets regroup our dataframe and extract the lower level details of each airlines ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet.groupby(['airline','airline_sentiment']).count().iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's create a graph by plots the total number of each tweet rating (positive,negative, or neutral)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = Tweet.groupby(['airline','airline_sentiment']).count().iloc[:,0].unstack(0).plot(kind = 'bar', title = 'Airline Ratings via Twitter')\nax.set_xlabel('Ratings')\nax.set_ylabel('Ratings Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore what percentage of negative, neutral and positive tweets to the total tweets for each airline."},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentages(df, rating = 'negative'):\n    if rating == 'negative':\n        i = 0\n        column = 'Percent Negative Ratings'\n    elif rating == 'neutral':\n        i = 1\n        column = 'Percent Neutral Ratings'\n    elif rating == 'positive':\n        i = 2\n        column = 'Percent Positive Ratings'\n        \n    #Count of all tweet ratings for each airline (negative, neutral, positive)\n    each_airline_ratings_counts = df.groupby(['airline','airline_sentiment']).count().iloc[:,0]\n    #Rating tweet total index for each airline:\n    #American i\n    #Delta i + 3\n    #southwest i + 6\n    #US Airways i + 9\n    #United i + 12\n    #Virgin i + 15\n\n    #Count of total tweets about an airline\n    total_airline_ratings_counts = df.groupby(['airline'])['airline_sentiment'].count()\n    #Airline index in total tweets:\n    #American 0\n    #Delta 1\n    #Southwest 2\n    #US Airways 3\n    #United 4\n    #Virgin 5\n\n\n    #Create a dictionary of percentage of rating tweets = (each_airline_ratings_counts / total_airline_ratings_counts)\n    tweet_ratings_dict = {'American':each_airline_ratings_counts[i] / total_airline_ratings_counts[0],\n                'Delta':each_airline_ratings_counts[i + 3] / total_airline_ratings_counts[1],\n                'Southwest': each_airline_ratings_counts[i + 6] / total_airline_ratings_counts[2],\n                'US Airways': each_airline_ratings_counts[i + 9] / total_airline_ratings_counts[3],\n                'United': each_airline_ratings_counts[i + 12] / total_airline_ratings_counts[4],\n                'Virgin': each_airline_ratings_counts[i + 15] / total_airline_ratings_counts[5]}\n\n    #make a dataframe from the dictionary\n    percent_tweet_ratings = pd.DataFrame.from_dict(tweet_ratings_dict, orient = 'index')\n    \n    #have to manually set column name when using .from_dict() method\n    percent_tweet_ratings.columns = [column]\n        \n    return percent_tweet_ratings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's compile our percentages and plot them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a df called negative that contains the percent negatives by calling the function above\npercent_negative_ratings = percentages(Tweet, 'negative')\n\n#Create a df called neutral that contains the percent neutrals by calling the function above\npercent_neutral_ratings = percentages(Tweet, 'neutral')\n\n#Create a df called positive that contains the percent positives by calling the function above\npercent_positive_ratings= percentages(Tweet, 'positive')\n\ndef merging_airlines_ratings_dataframes(x,y,z):\n\n    concatenate_airlines_ratings_dataframes = pd.concat([x,y,z], axis = 1)\n    return concatenate_airlines_ratings_dataframes\n\n#concatenate all 3 dataframes of percent ratings\npercent_ratings_dataframes_concatenated = merging_airlines_ratings_dataframes(percent_neutral_ratings, percent_negative_ratings, percent_positive_ratings)\nprint(percent_ratings_dataframes_concatenated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#graph all of airlines ratings dataframes\nax = percent_ratings_dataframes_concatenated.plot(kind = 'bar', stacked = True, rot = 0, figsize = (15,6))\n#set x label\nax.set_xlabel('Airlines')\n#set y label\nax.set_ylabel('Percentages')\n#move the legend to the bottom of the graph\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n          fancybox=True, shadow=True, ncol=5)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the below code shown when we explored those particular locations in the airline column and tweet text column we found that Delta was tagged as the Airline being referenced in the tweet. But when we look at the actual text we can see it was really Jet Blue being referenced . "},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_column = list(Tweet.reset_index().iloc[6750:6755,8])\ntweet_text_column = list(Tweet.reset_index().iloc[6750:6755,6])\n\nfor pos, item in enumerate(airline_column):\n    print('Airline as entered: ' + str(item))\n    print('The tweet text: ')\n    print(tweet_text_column[pos], '\\n''\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now must get rid of the 'Airline' column in the dataframe, parse all the tweet text and pullout the proper airline being referenced in all Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet = Tweet.iloc[:,0:6]\nTweet.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"parse all the tweet text and pullout the proper airline being referenced in all Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n\n# We create new column called 'Airline'Then we extract the right airline from the tweet text by applying regular expression function to the 'text' column\nTweet['Airline'] = Tweet.text.apply(lambda x: re.findall('\\@\\w+', x)[0])\n\n#get all unique twitter tags and the count for how many times it appears in the column\ntwitter_text_tags = np.unique(Tweet.Airline, return_counts = True)\n\n#compile twitter_text_tags so that it lists the unique tag and its total count side by side instead of 2 seperate arrays\ntwitter_tags_count = list(zip(twitter_text_tags[0],twitter_text_tags[1]))\ntwitter_tags_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we will compile all the airlines referenced into a list then we need to refine our regular expression search, looking not only for the airlines, but ignoring the camel case so that all spellings of an airline are equal"},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_companies_list = ['@virginamerica','@united','@southwestair','@americanair','@jetblue','@usairways']\n    \n# We compile a regex search to seperate out only the airline tag and ignoring other users tags in the text\n# We are ignoring case, or capitaliztion  in order to negate all the uniquess we encountered in the list above\nairlines = re.compile('|'.join(airline_companies_list), re.IGNORECASE)\n    \n#We apply the compiled regex search and remove the twitter tag '@'\n#for example, the following code takes @AmericanAir and returns AmericanAir\nTweet['Airline'] = Tweet.Airline.apply(lambda x: np.squeeze(re.findall(airlines, x))).str.split('@').str[1]\nprint(list(Tweet.Airline.head(10)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We will filter the data frame and find the index locations of those tweets where the airline didn't come first but came a users name instead then we display all '@' tags that was referenced in the tweet text."},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet_df_without_airline_first = Tweet.reset_index()\nTweet_df_rows_without_airline_first = Tweet_df_without_airline_first[Tweet_df_without_airline_first.Airline.isnull()].text.apply(lambda x: re.findall('\\@\\w+', x))\nTweet_df_rows_without_airline_first","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we find those locations that did not have an airline in place, and manually set the values for these locations in the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset the index of our dataframe\nTweet = Tweet.reset_index()\n\n#compile a list of index locations of the tweets that return null and set their airline value to the appropriate\n#airline referenced in the tweet\nunited = [737,868,1088,4013]\nsouthwest = [4604,5614,5615,6136,6362]\njetblue = [6796,6811,6906]\nusairways = [7330, 8215,10243,10517,10799,10864,10874,10876,11430]\namerican = [11159,12222,12417,12585,13491,13979]\ndelta = [12038, 12039]\nTweet.set_value(united,'Airline','united')\nTweet.set_value(southwest,'Airline','southwestair')\nTweet.set_value(jetblue,'Airline','jetblue')\nTweet.set_value(usairways,'Airline','usairways')\nTweet.set_value(american,'Airline','americanair')\nTweet.set_value(delta,'Airline','delta')\n    \n#Since all airlines tweets are camel case in different orders, make all airlines uppercase so they are all equal\nTweet.Airline = Tweet.Airline.apply(lambda x: x.upper())\n    \n#create a dictionary to map the all uppercase airlines to the proper naming convention\nTweet_map_airline = {'AMERICANAIR':'American Airlines',\n                'JETBLUE':'Jet Blue',\n                'SOUTHWESTAIR':'Southwest Airlines',\n                'UNITED': 'United Airlines',\n                'USAIRWAYS': 'US Airways',\n                'VIRGINAMERICA':'Virgin Airlines',\n                'DELTA':'Delta Airlines'}\n    \n#map the uppercase airlines to the proper naming convention\nTweet.Airline = Tweet.Airline.map(Tweet_map_airline)\n\n#display our new airlines!!!\nnp.unique(Tweet.Airline)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's filter our data down to those ratings that have only greater than 0.51 rating confidence percentages.let's convert the date and time to be just date and convert it's type from **'string'** to be as pandas **'datetime'**."},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet_conf_df = Tweet[Tweet.airline_sentiment_confidence >= 0.51 ]\n#create a copy of our original dataframe and reset the index\ndate = Tweet_conf_df.reset_index()\n#convert the Date column to pandas datetime\ndate.tweet_created = pd.to_datetime(date.tweet_created)\n#Reduce the dates in the date column to only the date and no time stamp using the 'dt.date' method\ndate.tweet_created = date.tweet_created.dt.date\nTweet_conf_df = date\nprint(Tweet_conf_df.info())\nTweet_conf_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore again the ratings types of airline(negative, neutral, positive)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df_test = Tweet_conf_df.groupby(['Airline','airline_sentiment']).count().iloc[:,0]\ntweet_df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore again  what percentage of negative, neutral and positive tweets to the total tweets for each airline."},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentages(df, rating = 'negative'):\n    if rating == 'negative':\n        i = 0\n        column = 'Percent Negative Ratings'\n    elif rating == 'neutral':\n        i = 1\n        column = 'Percent Neutral Ratings'\n    elif rating == 'positive':\n        i = 2\n        column = 'Percent Positive Ratings'\n        \n    #Count of all tweet ratings for each airline (negative, neutral, positive), remove Delta since it only has 2 entries total\n    each_airline_ratings_counts = df[df.Airline != 'Delta Airlines'].groupby(['Airline','airline_sentiment']).count().iloc[:,0]\n    #Rating tweet total index for each airline:\n    #American i\n    #Jet Blue i + 3\n    #southwest i + 6\n    #US Airways i + 9\n    #United i + 12\n    #Virgin i + 15\n\n    #Count of total tweets about an airline\n    total_airline_ratings_counts = df[df.Airline != 'Delta Airlines'].groupby(['Airline'])['airline_sentiment'].count()\n    #Airline index in total tweets:\n    #American 0\n    #Jet Blue 1\n    #Southwest 2\n    #US Airways 3\n    #United 4\n    #Virgin 5\n\n    #Create a dictionary of percentage of rating tweets = (each_airline_ratings_counts / total_airline_ratings_counts)\n    tweet_ratings_dict = {'American':each_airline_ratings_counts[i] / total_airline_ratings_counts[0],\n                'Jet Blue':each_airline_ratings_counts[i + 3] / total_airline_ratings_counts[1],\n                'Southwest': each_airline_ratings_counts[i + 6] / total_airline_ratings_counts[2],\n                'US Airways': each_airline_ratings_counts[i + 9] / total_airline_ratings_counts[3],\n                'United': each_airline_ratings_counts[i + 12] / total_airline_ratings_counts[4],\n                'Virgin': each_airline_ratings_counts[i + 15] / total_airline_ratings_counts[5]}\n\n    #make a dataframe from the dictionary\n    percent_tweet_ratings = pd.DataFrame.from_dict(tweet_ratings_dict, orient = 'index')\n    \n    #have to manually set column name when using .from_dict() method\n    percent_tweet_ratings.columns = [column]\n        \n    return percent_tweet_ratings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compile our percentages and plot them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a df called negative that contains the percent negatives by calling the function above\npercent_negative_ratings = percentages(Tweet_conf_df, 'negative')\n\n#Create a df called neutral that contains the percent neutrals by calling the function above\npercent_neutral_ratings = percentages(Tweet_conf_df, 'neutral')\n\n#Create a df called positive that contains the percent positives by calling the function above\npercent_positive_ratings= percentages(Tweet_conf_df, 'positive')\n\n\n#concatenate all 3 dataframes of percent ratings\npercent_ratings_dataframes_concatenated = merging_airlines_ratings_dataframes(percent_neutral_ratings, percent_negative_ratings, percent_positive_ratings)\nprint(percent_ratings_dataframes_concatenated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#graph all of airlines ratings dataframes\nax = percent_ratings_dataframes_concatenated.plot(kind = 'bar', stacked = True, rot = 0, figsize = (15,6))\n#set x label\nax.set_xlabel('Airlines')\n#set y label\nax.set_ylabel('Percentages')\n#move the legend to the bottom of the graph\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n          fancybox=True, shadow=True, ncol=5)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's extract all the negative reasons aggregate them for each airline then plotting the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet_conf_df_ngreason = Tweet_conf_df.reset_index().loc[:,['Airline','negativereason']].dropna().groupby(['Airline','negativereason']).size()\nTweet_conf_df_ngreason.unstack(0).plot(kind = 'bar', figsize = (15,6), rot = 70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's how many of each ratings that every airline got for each date"},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet_conf_day_df = Tweet_conf_df.groupby(['tweet_created','Airline','airline_sentiment']).size()\nTweet_conf_day_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tweet_conf_day_df = Tweet_conf_day_df.reset_index()\n#Remove delta since it only has 2 entries\nTweet_conf_day_df = Tweet_conf_day_df[Tweet_conf_day_df.Airline != 'Delta Airlines']\n#filter to only negative ratings\nTweet_conf_day_df = Tweet_conf_day_df[Tweet_conf_day_df.airline_sentiment == 'negative'].reset_index()\nTweet_conf_day_df = Tweet_conf_day_df.iloc[:,1:5]\n#groupby and plot data\nax2 = Tweet_conf_day_df.groupby(['tweet_created','Airline']).sum().unstack().plot(kind = 'bar', figsize = (15,6), rot = 70)\nlabels = ['American Airlines','Jet Blue','Southwest Airlines','US Airways','United Airlines','Virgin Airlines']\nax2.legend(labels = labels)\nax2.set_xlabel('Date')\nax2.set_ylabel('Negative Tweets')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we wiil make Word CLoud visualization for the negative ratings. the negative words with the most frequency in the data will appear with the biggest size in the wordcloud image"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will filter the data to be the data with the negative ratings\nTweet_text_cloud_df=Tweet_conf_df[Tweet_conf_df['airline_sentiment']=='negative']\nwords = ' '.join(Tweet_text_cloud_df['text'])\n#we will remove the links , tags and RT from the text\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\n#then we will visualize the cleaned data by word cloud visualizations\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                     ).generate(cleaned_word)\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we wiil make Word CLoud visualization for the positive ratings. the positive words with the most frequency in the data will appear with the biggest size in the wordcloud image"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will filter the data to be the data with the positive ratings\nTweet_text_cloud_df=Tweet_conf_df[Tweet_conf_df['airline_sentiment']=='positive']\nwords = ' '.join(Tweet_text_cloud_df['text'])\n#we will remove the links , tags and RT from the text\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\n#then we will visualize the cleaned data by word cloud visualizations\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                     ).generate(cleaned_word)\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we wiil make Word CLoud  for the neutral ratings. the neutral words with the most frequency in the data will appear with the biggest size in the wordcloud image"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will filter the data to be the data with the neutral ratings\nTweet_text_cloud_df=Tweet_conf_df[Tweet_conf_df['airline_sentiment']=='neutral']\n#we will remove the links , tags and RT from the text\nwords = ' '.join(Tweet_text_cloud_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\n#then we will visualize the cleaned data by word cloud visualizations\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                     ).generate(cleaned_word)\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's filter tweets text by applying tweet_to_words function to 'text' column \ndef tweet_to_words(tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words ))\n\nTweet_conf_df.text = Tweet_conf_df.text.apply(lambda x: tweet_to_words(x))\n# We will code te values in the 'airline_sentiment' column to be numeric values\nTweet_conf_df['airline_sentiment'] = Tweet_conf_df['airline_sentiment'].replace('negative', 0)\nTweet_conf_df['airline_sentiment'] = Tweet_conf_df['airline_sentiment'].replace('neutral', 1)\nTweet_conf_df['airline_sentiment'] = Tweet_conf_df['airline_sentiment'].replace('positive', 2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer(analyzer = \"word\")\n## Create sparse matrix from the vectorizer\ndt_features= vect.fit_transform(Tweet_conf_df['text'])\ntext_transformed = pd.DataFrame(dt_features.toarray(), columns=vect.get_feature_names())\nX_train, X_test, y_train, y_test = train_test_split(text_transformed, Tweet_conf_df['airline_sentiment'], test_size=0.2, random_state=456)\n# Train a logistic regression\nlog_reg = LogisticRegression(C=1.0, dual=True, penalty=\"l2\").fit(X_train, y_train)\n# Predict the labels\ny_predicted = log_reg.predict(X_test)\n\nprint('our score is:',  log_reg.score(X_test,y_test))\n# Print accuracy score and confusion matrix on test set\nprint('Accuracy on the test set: ', accuracy_score(y_test, y_predicted))\nprint(confusion_matrix(y_test, y_predicted)/len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_predicted, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's predict the airline sentiment by **Deep learning** "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(Tweet_conf_df.text, Tweet_conf_df.airline_sentiment, test_size=0.1, random_state=37)\ntk = Tokenizer(num_words= 10000,\n               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n               lower=True,\n               split=\" \")\ntk.fit_on_texts(X_train_d)\n\nX_train_seq = tk.texts_to_sequences(X_train_d)\nX_test_seq = tk.texts_to_sequences(X_test_d)\n\ndef one_hot_seq(seqs, nb_features = 10000):\n    ohs = np.zeros((len(seqs), nb_features))\n    for i, s in enumerate(seqs):\n        ohs[i, s] = 1.\n    return ohs\n\nX_train_oh = one_hot_seq(X_train_seq)\nX_test_oh = one_hot_seq(X_test_seq)\n\nle = LabelEncoder()\ny_train_le = le.fit_transform(y_train_d)\ny_test_le = le.transform(y_test_d)\ny_train_oh = to_categorical(y_train_le)\ny_test_oh = to_categorical(y_test_le)\n\nX_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_model = models.Sequential()\ndrop_model.add(layers.Dense(64, kernel_initializer = 'uniform', activation='relu', input_shape=(10000,)))\ndrop_model.add(layers.Dropout(0.5))\ndrop_model.add(layers.Dense(64,kernel_initializer = 'uniform', activation='relu'))\ndrop_model.add(layers.Dropout(0.5))\ndrop_model.add(layers.Dense(3, activation='softmax'))\n\nprint(drop_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\ndrop_model.fit(X_train_rest,y_train_rest, batch_size = 64, nb_epoch = 10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}