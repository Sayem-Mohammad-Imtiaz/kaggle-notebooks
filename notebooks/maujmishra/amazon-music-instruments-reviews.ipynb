{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Amazon Musical Instruments Reviews","metadata":{}},{"cell_type":"markdown","source":"Webportals like Bhuvan get vast amount of feedback from the users. To go through all the feedback's can be a tedious job. You have to categorize opinions expressed in feedback forums. This can be utilized for feedback management system. We Classification of individual comments/reviews.and we also determining overall rating based on individual comments/reviews. So that company can get a complete idea on feedback's provided by customers and can take care on those particular fields. This makes more loyal Customers to the company, increase in business , fame ,brand value ,profits.","metadata":{}},{"cell_type":"markdown","source":"**Attributes in the dataset**\n\n1. reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n2. asin - ID of the product, e.g. 0000013714\n3. reviewerName - name of the reviewer\n4. helpful - helpfulness rating of the review, e.g. 2/3\n5. reviewText - text of the review\n6. overall - rating of the product\n7. summary - summary of the review\n8. unixReviewTime - time of the review (unix time)\n9. reviewTime - time of the review (raw)","metadata":{}},{"cell_type":"markdown","source":"**Task**\n\nClassify reviews as positive, negative, and neutral based on the attributes above.","metadata":{}},{"cell_type":"markdown","source":"**Libraries Required**","metadata":{}},{"cell_type":"code","source":"import re # Regular Expressions\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:47.342388Z","iopub.execute_input":"2021-09-21T10:05:47.342676Z","iopub.status.idle":"2021-09-21T10:05:47.351686Z","shell.execute_reply.started":"2021-09-21T10:05:47.342647Z","shell.execute_reply":"2021-09-21T10:05:47.350575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the dataset\ndf = pd.read_csv(\"../input/amazon-music-reviews/Musical_instruments_reviews.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:47.602274Z","iopub.execute_input":"2021-09-21T10:05:47.602777Z","iopub.status.idle":"2021-09-21T10:05:47.843108Z","shell.execute_reply.started":"2021-09-21T10:05:47.602744Z","shell.execute_reply":"2021-09-21T10:05:47.842263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing and analysis the dataset**","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:48.181999Z","iopub.execute_input":"2021-09-21T10:05:48.182817Z","iopub.status.idle":"2021-09-21T10:05:48.191274Z","shell.execute_reply.started":"2021-09-21T10:05:48.182768Z","shell.execute_reply":"2021-09-21T10:05:48.190019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:48.512145Z","iopub.execute_input":"2021-09-21T10:05:48.513258Z","iopub.status.idle":"2021-09-21T10:05:48.539079Z","shell.execute_reply.started":"2021-09-21T10:05:48.513217Z","shell.execute_reply":"2021-09-21T10:05:48.538209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:48.782268Z","iopub.execute_input":"2021-09-21T10:05:48.782614Z","iopub.status.idle":"2021-09-21T10:05:48.78957Z","shell.execute_reply.started":"2021-09-21T10:05:48.782584Z","shell.execute_reply":"2021-09-21T10:05:48.788902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"# I will modify overall column of the dataset to start with\n# In the overall column the ratings (given by the user) is given.\n# I will modify the column - 4 and 5 as positive rating, 1 and 2 as negative rating and 3 as neutral \n# Function to modify overall column\n\ndef change_score(rating):\n    if rating < 3:\n        return 0\n    elif rating > 3:\n        return 2\n    else:\n        return 1\n\ndf_score = df[\"overall\"]\ndf_score = df_score.map(change_score)\ndf[\"overall\"] = df_score","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:49.322274Z","iopub.execute_input":"2021-09-21T10:05:49.322773Z","iopub.status.idle":"2021-09-21T10:05:49.342657Z","shell.execute_reply.started":"2021-09-21T10:05:49.322722Z","shell.execute_reply":"2021-09-21T10:05:49.341606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:49.582371Z","iopub.execute_input":"2021-09-21T10:05:49.583273Z","iopub.status.idle":"2021-09-21T10:05:49.599706Z","shell.execute_reply.started":"2021-09-21T10:05:49.58321Z","shell.execute_reply":"2021-09-21T10:05:49.598722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counting occurences of positive, negative and neutral reviews\ndf[\"overall\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:49.862416Z","iopub.execute_input":"2021-09-21T10:05:49.862906Z","iopub.status.idle":"2021-09-21T10:05:49.872791Z","shell.execute_reply.started":"2021-09-21T10:05:49.862847Z","shell.execute_reply":"2021-09-21T10:05:49.87192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The dataset has imbalanced classes, which I will fix later","metadata":{}},{"cell_type":"code","source":"# The helpful column of the dataframe gives value - [x,y]\n# out of 'y' people 'x' found the corresponding review helpful\n# so people who found the review helful = x, and people who found the review \"not helpful\" = y-x (total people voted on the review - people who voted helful)\n# I will seprate the x and y values in different columns\n\n# Since helpful columns have values in object form, I will have to convert all the values to python list\ndef convert_to_list(str_lst):\n    str_ = str_lst.strip(\"[]\").replace(\",\",\" \")\n    lst = str_.split()\n    lst_to_int = list(map(int, lst))\n    return lst_to_int\n        \ndef total_rating(lst_rating):\n    return lst_rating[1] # y\n\ndef helpful_rating(lst_rating):\n    return lst_rating[0] # x\n\ndf[\"helpful\"] = df[\"helpful\"].map(convert_to_list) # \"[x,y]\" -> [x,y]\ndf[\"total_ratings\"] = df[\"helpful\"].map(total_rating) # y\ndf[\"helpful\"] = df[\"helpful\"].map(helpful_rating) # x\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:50.422394Z","iopub.execute_input":"2021-09-21T10:05:50.422893Z","iopub.status.idle":"2021-09-21T10:05:50.479081Z","shell.execute_reply.started":"2021-09-21T10:05:50.422861Z","shell.execute_reply":"2021-09-21T10:05:50.478317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for duplicate rows in columns - [\"reviewerName\", \"reviewText\", \"unixReviewTime\"]\n\nprint(\"Number rows having common values of [reviewerName, reviewText, unixReviewTime] =\", df[df.duplicated(subset=[\"reviewerName\", \"reviewText\", \"unixReviewTime\"])].shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:50.74209Z","iopub.execute_input":"2021-09-21T10:05:50.742385Z","iopub.status.idle":"2021-09-21T10:05:50.764185Z","shell.execute_reply.started":"2021-09-21T10:05:50.742356Z","shell.execute_reply":"2021-09-21T10:05:50.763228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are no duplicates in the dataset","metadata":{}},{"cell_type":"code","source":"# Checking in columns helpful and total_ratings if helpful > total_ratings\n# If any row follows above condition, I will remove it\n\nprint(\"Number of rows, in which helpful > total_ratings =\",df[df[\"helpful\"] > df[\"total_ratings\"]].shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:51.28236Z","iopub.execute_input":"2021-09-21T10:05:51.283218Z","iopub.status.idle":"2021-09-21T10:05:51.290906Z","shell.execute_reply.started":"2021-09-21T10:05:51.283165Z","shell.execute_reply":"2021-09-21T10:05:51.289963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are no rows which satisfy helpful > total_ratings ","metadata":{}},{"cell_type":"code","source":"# Checking for null values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:51.852126Z","iopub.execute_input":"2021-09-21T10:05:51.852483Z","iopub.status.idle":"2021-09-21T10:05:51.869985Z","shell.execute_reply.started":"2021-09-21T10:05:51.852447Z","shell.execute_reply":"2021-09-21T10:05:51.86917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review text is the most important column for classification\n# I will remove columns having null review text\n# Missing values of reviewerName column doesn't matter, because reveiwerName doesn't contribute in determining polarity of the review\ndf.drop(df[df[\"reviewText\"].isnull()].index, axis=0, inplace=True)\ndf.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:52.202247Z","iopub.execute_input":"2021-09-21T10:05:52.203323Z","iopub.status.idle":"2021-09-21T10:05:52.213117Z","shell.execute_reply.started":"2021-09-21T10:05:52.203264Z","shell.execute_reply":"2021-09-21T10:05:52.212179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Preprocessing\n\nIn Text Preprcessing, I will-\n* Remove HTML tags in the review text\n* Remove special characters from the text (#, ! etc.)\n* Convert the text in lowercase\n* Removal of stop words\n* Applying Stemming to the text","metadata":{}},{"cell_type":"code","source":"# Removing HTML tags\ndef remove_html(text):\n    html_pattern = re.compile(\"<.*?>\")\n    text = re.sub(html_pattern, \" \", text) # Substitute HTML tag with space\n    return text\n\n# Removing special characters\ndef remove_spl_char(text):\n    text = re.sub(r\"[?|!|.|,|)|(|\\|/|#|\\'|\\\"]\", r\"\", text) # All the special characters removed\n    return text\n\n# Converting to lowercase\ndef in_lowercase(text):\n    text = text.lower()\n    return text\n\n# Removing stop words\nstop_words = set(stopwords.words(\"english\")) # List of all the stop words\n\ndef remove_stopwords(text):\n    filtered_text_lst = []\n    text_lst = text.split()\n    for word in text_lst:\n        if word not in stop_words:\n            filtered_text_lst.append(word)\n        else:\n            continue\n    filtered_word = \" \".join(filtered_text_lst)\n    return filtered_word\n\nstem = PorterStemmer()\ndef stemming(text):\n    stemmed_txt_lst = []\n    text_lst = text.split()\n    for word in text_lst:\n        stemmed_word = stem.stem(word)\n        stemmed_txt_lst.append(stemmed_word)\n    stemmed_txt_lst = \" \".join(stemmed_txt_lst)\n    return stemmed_txt_lst\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:53.787577Z","iopub.execute_input":"2021-09-21T10:05:53.788322Z","iopub.status.idle":"2021-09-21T10:05:53.807101Z","shell.execute_reply.started":"2021-09-21T10:05:53.788229Z","shell.execute_reply":"2021-09-21T10:05:53.806074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(text):\n    rem_html_txt = remove_html(text) # Remove HTML\n    rem_spl_char_txt = remove_spl_char(rem_html_txt) # Remove Special Characters\n    lowercase_txt = in_lowercase(rem_spl_char_txt) # Conversion in lowercase\n    rem_stopwords_txt = remove_stopwords(lowercase_txt) # Remove stopwords\n    stemmed_txt = stemming(text)\n    final_txt = stemmed_txt\n    return final_txt\n\ndf[\"final_review\"] = df[\"reviewText\"].map(text_preprocessing)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:05:54.342283Z","iopub.execute_input":"2021-09-21T10:05:54.342937Z","iopub.status.idle":"2021-09-21T10:06:17.617269Z","shell.execute_reply.started":"2021-09-21T10:05:54.342894Z","shell.execute_reply":"2021-09-21T10:06:17.615359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Before Text Preprocessing- \", \"\\n\")\nprint(df[\"reviewText\"][0], \"\\n\")\nprint(\"After Text Preprocessing- \", \"\\n\")\nprint(df[\"final_review\"][0])","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:17.620691Z","iopub.execute_input":"2021-09-21T10:06:17.621386Z","iopub.status.idle":"2021-09-21T10:06:17.632239Z","shell.execute_reply.started":"2021-09-21T10:06:17.621323Z","shell.execute_reply":"2021-09-21T10:06:17.631225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Avg Word2Vec","metadata":{}},{"cell_type":"markdown","source":"* I will use word2vec to convert text values to vector\n* I have sufficient training examples to construct W2V","metadata":{}},{"cell_type":"code","source":"reviews = []\ndef construct_reviews_lst(review):\n    review_split = review.split()\n    reviews.append(review_split)\ndf[\"final_review\"].map(construct_reviews_lst)   \n\nprint(df[\"final_review\"].iloc[0]) # Before\nreviews[0] # After","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:17.634229Z","iopub.execute_input":"2021-09-21T10:06:17.634712Z","iopub.status.idle":"2021-09-21T10:06:17.880974Z","shell.execute_reply.started":"2021-09-21T10:06:17.634657Z","shell.execute_reply":"2021-09-21T10:06:17.880029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing W2V model\nw2v_model = Word2Vec(reviews, vector_size=50, min_count=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:17.883139Z","iopub.execute_input":"2021-09-21T10:06:17.884051Z","iopub.status.idle":"2021-09-21T10:06:22.536143Z","shell.execute_reply.started":"2021-09-21T10:06:17.884006Z","shell.execute_reply":"2021-09-21T10:06:22.535058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Avg W2V for each review\ndef avg_w2v(reviews):\n    text_vector = []\n    for review in reviews:\n        review_vec_sum = np.zeros(50)\n        num_words = 0\n        for word in review:\n            try:\n                word_vec = w2v_model.wv[word]\n                review_vec_sum += word_vec\n                num_words += 1\n            except:\n                pass\n        avg_review_vector = review_vec_sum / num_words\n        text_vector.append(avg_review_vector)\n    return text_vector\n\ntext_vector = np.array(avg_w2v(reviews)) # Text Vector of all the reviews","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:22.537565Z","iopub.execute_input":"2021-09-21T10:06:22.537842Z","iopub.status.idle":"2021-09-21T10:06:26.545239Z","shell.execute_reply.started":"2021-09-21T10:06:22.537806Z","shell.execute_reply":"2021-09-21T10:06:26.544349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[\"final_review\"][0])\n\nprint(\"\\n\\nVector Representation of above text - \")\ntext_vector[0] # Text Vector of review","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:26.547203Z","iopub.execute_input":"2021-09-21T10:06:26.547825Z","iopub.status.idle":"2021-09-21T10:06:26.557324Z","shell.execute_reply.started":"2021-09-21T10:06:26.547789Z","shell.execute_reply":"2021-09-21T10:06:26.556041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now, I will create final dataset which will contain all features, which will be used for classification","metadata":{}},{"cell_type":"code","source":"# Feature name for text vector\ndef create_feature_names():\n    text_features = []\n    for index in range(1,51):\n        feature_name = \"text-feature-\"+ str(index)\n        text_features.append(feature_name)\n    return text_features\ntext_features = create_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:26.558889Z","iopub.execute_input":"2021-09-21T10:06:26.559192Z","iopub.status.idle":"2021-09-21T10:06:26.569967Z","shell.execute_reply.started":"2021-09-21T10:06:26.559162Z","shell.execute_reply":"2021-09-21T10:06:26.568688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing dataframe from 'text_vector' variable\n\ndef create_df_txt_vec(text_vector):\n    df_text_lst = []\n    for vector in text_vector:\n        vector_reshape = np.reshape(vector ,(50, 1)).T\n        df_vector = pd.DataFrame(vector_reshape, columns=text_features)\n        df_text_lst.append(df_vector)\n    df_text = pd.concat(df_text_lst, ignore_index=True)\n    return df_text\n\ndf_text = create_df_txt_vec(text_vector)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:26.571847Z","iopub.execute_input":"2021-09-21T10:06:26.572096Z","iopub.status.idle":"2021-09-21T10:06:29.760297Z","shell.execute_reply.started":"2021-09-21T10:06:26.572069Z","shell.execute_reply":"2021-09-21T10:06:29.759367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting featrures 'helpful' and 'total_ratings' from the original dataframe, because they might affect the prediction\n# Selecting 'overall' attribute for output\n\ndf_final = pd.concat([df_text, df[\"helpful\"], df[\"total_ratings\"], df[\"overall\"]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:29.762011Z","iopub.execute_input":"2021-09-21T10:06:29.762326Z","iopub.status.idle":"2021-09-21T10:06:29.772034Z","shell.execute_reply.started":"2021-09-21T10:06:29.76229Z","shell.execute_reply":"2021-09-21T10:06:29.771018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:29.774594Z","iopub.execute_input":"2021-09-21T10:06:29.774828Z","iopub.status.idle":"2021-09-21T10:06:29.807628Z","shell.execute_reply.started":"2021-09-21T10:06:29.774803Z","shell.execute_reply":"2021-09-21T10:06:29.806709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Fitting","metadata":{}},{"cell_type":"code","source":"df_features = df_final.drop(\"overall\", axis=1)\ndf_target = df_final[\"overall\"]\n\ndf_features_columns = df_features.columns\ndf_features_scaled = StandardScaler().fit_transform(df_features)\ndf_features_scaled = pd.DataFrame(df_features_scaled, columns=df_features_columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:42.993783Z","iopub.execute_input":"2021-09-21T10:06:42.994073Z","iopub.status.idle":"2021-09-21T10:06:43.024829Z","shell.execute_reply.started":"2021-09-21T10:06:42.994043Z","shell.execute_reply":"2021-09-21T10:06:43.023951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(df_features_scaled, df_target, test_size=0.25, train_size=0.75)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:45.027491Z","iopub.execute_input":"2021-09-21T10:06:45.027933Z","iopub.status.idle":"2021-09-21T10:06:45.03755Z","shell.execute_reply.started":"2021-09-21T10:06:45.027902Z","shell.execute_reply":"2021-09-21T10:06:45.036735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for imbalanced dataset\ndf_final[\"overall\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:46.342397Z","iopub.execute_input":"2021-09-21T10:06:46.342889Z","iopub.status.idle":"2021-09-21T10:06:46.351097Z","shell.execute_reply.started":"2021-09-21T10:06:46.342847Z","shell.execute_reply":"2021-09-21T10:06:46.350229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, The positive reviews are 9015, neutral reviews are 772 and negative reviews are 467\n# I will use combination of undersampling and oversampling to match the classes\n\n# Creating Pipeline to perform SMOTE and UnderSampling techniques\noversampling_smote = SMOTE(sampling_strategy={1:5000, 0:5000})\nundersampling = RandomUnderSampler(sampling_strategy={2:5000})\npipeline = Pipeline([('under', undersampling), ('smote', oversampling_smote)])\ndf_train_resampled = pipeline.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:47.622141Z","iopub.execute_input":"2021-09-21T10:06:47.622856Z","iopub.status.idle":"2021-09-21T10:06:47.742139Z","shell.execute_reply.started":"2021-09-21T10:06:47.622815Z","shell.execute_reply":"2021-09-21T10:06:47.741292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df_train_resampled[0] # Resampled X_train \ny_train = df_train_resampled[1] # Resampled y_train\n\nX_train, y_train = shuffle(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:48.12226Z","iopub.execute_input":"2021-09-21T10:06:48.122564Z","iopub.status.idle":"2021-09-21T10:06:48.137648Z","shell.execute_reply.started":"2021-09-21T10:06:48.122532Z","shell.execute_reply":"2021-09-21T10:06:48.136522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"C = [0.001 ,0.01 ,0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # Values of hyperparameter C\n\ncv_f1_mean= []\n\nfor value in C:\n    model = LogisticRegression(C = value, solver=\"sag\", max_iter=5000)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1_micro\")\n    cv_f1_mean.append(np.mean(scores))\n    \nmax_f1 = max(cv_f1_mean)\nindex_max_f1 = cv_f1_mean.index(max_f1)\n\nprint(\"Optimal value of hyperparameter C: \" + str(C[index_max_f1]))\nprint(\"F1 score at optimal C: \" + str(max_f1))","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:40:44.552742Z","iopub.execute_input":"2021-09-21T08:40:44.55303Z","iopub.status.idle":"2021-09-21T08:49:58.606202Z","shell.execute_reply.started":"2021-09-21T08:40:44.552999Z","shell.execute_reply":"2021-09-21T08:49:58.605019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression(C = 6, solver=\"sag\", max_iter=5000) # Calculating test accuracy of model\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:49:58.607723Z","iopub.execute_input":"2021-09-21T08:49:58.608143Z","iopub.status.idle":"2021-09-21T08:50:07.32164Z","shell.execute_reply.started":"2021-09-21T08:49:58.608097Z","shell.execute_reply":"2021-09-21T08:50:07.320839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of LogisticRegression model on test set: \" + str(acc_test) + \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:50:07.323095Z","iopub.execute_input":"2021-09-21T08:50:07.323466Z","iopub.status.idle":"2021-09-21T08:50:07.339563Z","shell.execute_reply.started":"2021-09-21T08:50:07.323437Z","shell.execute_reply":"2021-09-21T08:50:07.338594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KNN","metadata":{}},{"cell_type":"code","source":"K = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # Values of hyperparameter K\n\ncv_f1_mean= []\n\nfor value in K:\n    model = KNeighborsClassifier(n_neighbors=value)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1_micro\")\n    cv_f1_mean.append(np.mean(scores))\n    \nmax_f1 = max(cv_f1_mean)\nindex_max_f1 = cv_f1_mean.index(max_f1)\n\nprint(\"Optimal value of hyperparameter K: \" + str(K[index_max_f1]))\nprint(\"F1 score at optimal K: \" + str(max_f1))","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:50:07.340962Z","iopub.execute_input":"2021-09-21T08:50:07.341476Z","iopub.status.idle":"2021-09-21T08:55:04.436296Z","shell.execute_reply.started":"2021-09-21T08:50:07.341433Z","shell.execute_reply":"2021-09-21T08:55:04.43553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=1) # Calculating test accuracy of model\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:55:04.437518Z","iopub.execute_input":"2021-09-21T08:55:04.438358Z","iopub.status.idle":"2021-09-21T08:55:04.551904Z","shell.execute_reply.started":"2021-09-21T08:55:04.438317Z","shell.execute_reply":"2021-09-21T08:55:04.551089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of 1-NN model on test set: \" + str(acc_test) + \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:55:04.552988Z","iopub.execute_input":"2021-09-21T08:55:04.553795Z","iopub.status.idle":"2021-09-21T08:55:10.554112Z","shell.execute_reply.started":"2021-09-21T08:55:04.553753Z","shell.execute_reply":"2021-09-21T08:55:10.552979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"code","source":"C = [0.001 ,0.01 ,0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # Values of hyperparameter C\n\ncv_f1_mean= []\n\nfor value in C:\n    model = SVC(C = value,kernel=\"rbf\")\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1_micro\")\n    cv_f1_mean.append(np.mean(scores))\n    \nmax_f1 = max(cv_f1_mean)\nindex_max_f1 = cv_f1_mean.index(max_f1)\n\nprint(\"Optimal value of hyperparameter C: \" + str(C[index_max_f1]))\nprint(\"F1 score at optimal C: \" + str(max_f1))","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:55:10.555422Z","iopub.execute_input":"2021-09-21T08:55:10.555731Z","iopub.status.idle":"2021-09-21T09:17:58.135034Z","shell.execute_reply.started":"2021-09-21T08:55:10.555699Z","shell.execute_reply":"2021-09-21T09:17:58.133997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVC(C = 15,kernel=\"rbf\") # Calculating test accuracy of model\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:17:58.136424Z","iopub.execute_input":"2021-09-21T09:17:58.137236Z","iopub.status.idle":"2021-09-21T09:18:11.769411Z","shell.execute_reply.started":"2021-09-21T09:17:58.137199Z","shell.execute_reply":"2021-09-21T09:18:11.768481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of SVM model on test set: \" + str(acc_test) + \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:18:11.770749Z","iopub.execute_input":"2021-09-21T09:18:11.771078Z","iopub.status.idle":"2021-09-21T09:18:12.995614Z","shell.execute_reply.started":"2021-09-21T09:18:11.771036Z","shell.execute_reply":"2021-09-21T09:18:12.99458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gaussian Naive Nayes","metadata":{}},{"cell_type":"code","source":"model = GaussianNB()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:18:12.996779Z","iopub.execute_input":"2021-09-21T09:18:12.997008Z","iopub.status.idle":"2021-09-21T09:18:13.021522Z","shell.execute_reply.started":"2021-09-21T09:18:12.996983Z","shell.execute_reply":"2021-09-21T09:18:13.020607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of GaussianNB model on test set: \" + str(acc_test) + \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:18:13.022942Z","iopub.execute_input":"2021-09-21T09:18:13.023724Z","iopub.status.idle":"2021-09-21T09:18:13.036033Z","shell.execute_reply.started":"2021-09-21T09:18:13.023677Z","shell.execute_reply":"2021-09-21T09:18:13.035134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:06:52.002423Z","iopub.execute_input":"2021-09-21T10:06:52.002719Z","iopub.status.idle":"2021-09-21T10:07:02.92564Z","shell.execute_reply.started":"2021-09-21T10:06:52.00269Z","shell.execute_reply":"2021-09-21T10:07:02.924655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of Random Forest model on test set: \" + str(acc_test) + \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:07:08.422175Z","iopub.execute_input":"2021-09-21T10:07:08.422466Z","iopub.status.idle":"2021-09-21T10:07:08.506626Z","shell.execute_reply.started":"2021-09-21T10:07:08.422435Z","shell.execute_reply":"2021-09-21T10:07:08.505544Z"},"trusted":true},"execution_count":null,"outputs":[]}]}