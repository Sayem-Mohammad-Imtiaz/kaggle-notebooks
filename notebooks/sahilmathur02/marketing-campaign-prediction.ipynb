{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Table of content\n1. Import Required Libraries\n2. Load Data\n3. Data Information\n4. Check for null values\n5. EDA\n    1. Check Outliers\n    2. Univariant Analysis\n    3. Bivariant Analysis\n    4. Multivariant Analysis\n    5. Correlation\n6. Encode Categorical Variables\n7. Spliting Data\n8. Base Line Models\n    1. Logistic Regression\n    2. XGBoost Classifier\n    3. Random Forest Classifier\n    4. Gradient Boosting Classifier\n    5. Stacking Classifier\n9. Balancing target variable\n10. Feature Selection\n11. Dimentionality Reduction\n12. Hyper Parameter Tuning\n13. Best Model","metadata":{}},{"cell_type":"markdown","source":"# Import Required Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n\n# use below code to convert the 'exponential' values to float\nnp.set_printoptions(suppress=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/arketing-campaign/marketing_campaign.csv', sep=';')\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping ID Column beacause we dont id column for predictions\ndf.drop('ID', axis=1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Information","metadata":{}},{"cell_type":"code","source":"# Shape of Dataset\nprint('Data contains', df.shape[0], 'rows and', df.shape[1], 'columns')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset information about value count and variable data type\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Numerical Data Description\ndf.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorical Data Description\ndf.describe(include='O').T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check for null values","metadata":{}},{"cell_type":"code","source":"# Check for null values in the dataset\ndf.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only income column contains null values","metadata":{}},{"cell_type":"markdown","source":"# Filling Null Values","metadata":{}},{"cell_type":"code","source":"def fill_na(frame):\n    for i in frame.columns:\n        if(((frame[i].isnull().sum() / len(frame))*100) <= 30) & (frame[i].dtype == 'int64'):\n            frame[i] = frame[i].fillna(frame[i].median())\n            \n        elif(((frame[i].isnull().sum() / len(frame))*100) <= 30) & (frame[i].dtype == 'O'):\n            frame[i] = frame[i].fillna(frame[i].mode()[0])\n            \n        elif(((frame[i].isnull().sum() / len(frame))*100) <= 30) & (frame[i].dtype == 'float64'):\n            frame[i] = frame[i].fillna(frame[i].median())\n            \nfill_na(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"# 1. Check Outliers","metadata":{}},{"cell_type":"code","source":"def detect_outliers(frame):\n    for i in frame.columns:\n        if(frame[i].dtype == 'int64'):\n            sns.boxplot(frame[i])\n            plt.show()\n            \n        elif(frame[i].dtype == 'float64'):\n            sns.boxplot(frame[i])\n            plt.show()\n            \ndetect_outliers(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Univariant Analysis","metadata":{}},{"cell_type":"code","source":"def univariant(frame):\n    for i in frame.columns:\n        if(frame[i].dtype == 'int64'):\n            print(i)\n            sns.distplot(x=frame[i])\n            plt.show()\n                \n        elif(frame[i].dtype == 'float64'):\n            print(i)\n            sns.distplot(x=frame[i])\n            plt.show()\n            \nunivariant(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Response variable seperately because our target variable(Class) is int and we have to treat it like object this time\nsns.countplot(df['Response'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our target variable(Response) is not balanced ","metadata":{}},{"cell_type":"markdown","source":"# 3. Multivariant Analysis","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Correlation","metadata":{}},{"cell_type":"code","source":"# Check correlation between variables\nplt.figure(figsize=(30,25))\nsns.heatmap(df.corr(), annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting dt_Customer into datetime64 data type\ndf['Dt_Customer'] = df['Dt_Customer'].astype('datetime64')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating two new columns Date_customer and Month_customer from Dt_Customer column\ndf['Date_Customer'] = df['Dt_Customer'].dt.day\ndf['Month_Customer'] = df['Dt_Customer'].dt.month\ndf['Year_Customer'] = df['Dt_Customer'].dt.year","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can drop Dt_Customer column\ndf.drop('Dt_Customer', axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode Categorical Variables","metadata":{}},{"cell_type":"code","source":"def encode(dataframe):\n    lec = LabelEncoder()\n    for j in dataframe.columns:\n        if(dataframe[j].dtype == 'object'):\n            dataframe[j] = lec.fit_transform(dataframe[j])\n            \nencode(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split data into train and test","metadata":{}},{"cell_type":"code","source":"x = df.drop('Response', axis=1)\ny = df['Response']\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets Build Models","metadata":{}},{"cell_type":"markdown","source":"# Base Line Models","metadata":{}},{"cell_type":"markdown","source":"# 1. Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(max_iter=10000)\nlr.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_pred = lr.predict(X_test)\nprint(classification_report(Y_test, lr_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier()\nxgb.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_pred = xgb.predict(X_test)\nprint(classification_report(Y_test, xgb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pred = rf.predict(X_test)\nprint(classification_report(Y_test, rf_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"gb = GradientBoostingClassifier()\ngb.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_pred = gb.predict(X_test)\nprint(classification_report(Y_test, gb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(Y_test, gb_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Stacking Classifier","metadata":{}},{"cell_type":"code","source":"estimators = [('xgb', XGBClassifier()),\n             ('rf', RandomForestClassifier()),\n             ('gb', GradientBoostingClassifier())]\nstack = StackingClassifier(estimators=estimators)\nstack.fit(X_train, Y_train)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_pred = stack.predict(X_test)\nprint(classification_report(Y_test, stack_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From all my base line models Gradient Boosting Classifier gives best results","metadata":{}},{"cell_type":"markdown","source":"# Balancing the target variable","metadata":{}},{"cell_type":"code","source":"smote = SMOTETomek()\nx_train, y_train = smote.fit_resample(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building models again using new training sets ","metadata":{}},{"cell_type":"markdown","source":"# 1. Logistic Regression","metadata":{}},{"cell_type":"code","source":"slr = LogisticRegression(max_iter=10000)\nslr.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slr_pred = slr.predict(X_test)\nprint(classification_report(Y_test, slr_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"sxgb = XGBClassifier()\nsxgb.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sxgb_pred = sxgb.predict(X_test)\nprint(classification_report(Y_test, sxgb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"srf = RandomForestClassifier()\nsrf.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"srf_pred = srf.predict(X_test)\nprint(classification_report(Y_test, srf_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"sgb = GradientBoostingClassifier()\nsgb.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgb_pred = sgb.predict(X_test)\nprint(classification_report(Y_test, sgb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sstack = StackingClassifier(estimators=estimators)\nsstack.fit(x_train, y_train)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sstack_pred = sstack.predict(X_test)\nprint(classification_report(Y_test, sstack_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Models after balancing the target variable gives good results. But if i them compare with base line models then base line model of Gradient Boosting Classifier give highest accuracy. So i further build my model with Gradient Boosting Classifier base line.","metadata":{}},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"th = np.sort(gb.feature_importances_)\nl = []\nfor g in th:\n    select = SelectFromModel(gb, threshold = g, prefit = True)\n    x_Train = select.transform(X_train)\n    model = GradientBoostingClassifier()\n    model.fit(x_Train, Y_train)\n    x_Test = select.transform(X_test)\n    y_pred = model.predict(x_Test)\n    accuracy = accuracy_score(Y_test, y_pred)\n    print('Threshold:', g, 'Model Score:', accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp = pd.DataFrame(rf.feature_importances_)\nimp.index = X_train.columns\nimp[imp[0] < 0.017037885998921535]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(['Z_CostContact', 'Z_Revenue'], axis=1)\nX_test = X_test.drop(['Z_CostContact', 'Z_Revenue'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building model after feature selection","metadata":{}},{"cell_type":"code","source":"fgb = GradientBoostingClassifier()\nfgb.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fgb_pred = fgb.predict(X_test)\nprint(classification_report(Y_test, fgb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(Y_test, fgb_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After feature selection i am getting same accuracy. So i further build model using base line","metadata":{}},{"cell_type":"markdown","source":"# Dimentionality Reduction","metadata":{}},{"cell_type":"code","source":"# First i check how many components we want\n# For this first i am initializing the pca\npca = PCA()\n# Fitting the training set in pca\npca.fit(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now check number of components\npca.explained_variance_ratio_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above our 99.97% data covers in 1 principal component","metadata":{}},{"cell_type":"code","source":"# Creating pca with n_components = 15\nPca = PCA(n_components=15)\n# Fitting the training data\nX_Train = Pca.fit_transform(X_train)\nX_Test = Pca.fit_transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building models after applying pca\npgb = GradientBoostingClassifier()\npgb.fit(X_Train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pgb_pred = pgb.predict(X_Test)\nprint(classification_report(Y_test, pgb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper Parameter Tuning","metadata":{}},{"cell_type":"code","source":"grid = {\n    'learning_rate' : [0.2, 0.3, 0.4, 0.5],\n    'n_estimators' : [300, 500, 700, 900],\n    'min_samples_split' : [3, 4, 5, 6],\n    'max_depth' : [2, 3, 4, 5],\n    'loss' : ['deviance', 'exponential']\n}\nrandom_cv = RandomizedSearchCV(estimator=gb,\n                              param_distributions=grid,\n                              n_iter=20,\n                              n_jobs=-1,\n                              cv=5,\n                              verbose=7,\n                              random_state=10,\n                              scoring='accuracy')\nrandom_cv.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_cv.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hgb = GradientBoostingClassifier(learning_rate=0.5, loss='exponential', max_depth=2,\n                           min_samples_split=4, n_estimators=300)\nhgb.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hgb_pred = hgb.predict(X_test)\nprint(classification_report(Y_test, hgb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# My Best Model\nMy Best model is Gradient Boosting Classifier after Hyper Parameter tuning","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}