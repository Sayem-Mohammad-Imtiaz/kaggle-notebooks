{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"21ce0e8b-93c7-e6bf-61dd-0d468d540c8a"},"source":"This is an attempt to predict **IMDB score** based on the selected fields:\n\n * duration (float64)\n * director_facebook_likes (float64)\n * actor_1_facebook_likes (float64)\n * actor_2_facebook_likes (float64)\n * actor_3_facebook_likes (float64)\n * genres (26 multi-hot)\n * cast_total_facebook_likes (float64)\n * budget (float64)\n * title_year (float64)\n * content_rating (18 one-hot)\n * language (48 one-hot)\n\nThe **RMSE** for each method (score error range +-):\n \n * LinearRegressor (something's wrong, as the error on cross-validation is unreal)\n * DecisionTreeRegressor -  1.27 (~13%)\n * RandomForestRegressor - 0.94 (~9%)\n * NN (101x512x1024x1) - 1.24 (~12%)\n * NN (101x512x1024x1) - 100k steps - 1.15 (~11%)\n * NN (101x512x1) - 1.20 (~12%)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0a9ac8b-07a0-f57d-99f4-92dbc66c8752"},"outputs":[],"source":"import pandas as pd\n\nMOVIE_DATASET_PATH = '../input/movie_metadata.csv'\n\ndef load_movie_data(path=MOVIE_DATASET_PATH):\n    return pd.read_csv(path)\n\nmovies = load_movie_data()\nmovies.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"65a7571b-9e57-452d-1acd-d8af5d420398"},"source":"The data has null values, we'll fill those:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0eb806fe-d9f6-43a2-14de-acf3edaa6703"},"outputs":[],"source":"obj_cols = ['color', 'director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'genres', 'movie_title', 'plot_keywords', \n                          'movie_imdb_link', 'country', 'language', 'content_rating']\nnum_cols = [x for x in list(movies.columns.values) if x not in obj_cols]\n\nmovies_num = movies[num_cols]\nmovies_obj = movies[obj_cols]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12bece70-e2ba-877e-b2b1-a2aa47942204"},"outputs":[],"source":"for col in obj_cols:\n    movies[col].fillna('', inplace=True)\n    \nfor col in num_cols:\n    median = movies_num[col].median()\n    movies[col].fillna(median, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"04c3458b-5ab2-7a53-a20a-4b5d2a47b2a0"},"source":"Let's transform the useful string columns to numerical values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d574b39f-1327-12fc-13a8-996d84b7477a"},"outputs":[],"source":"genres = ['History', 'Reality-TV', 'Family', 'Adventure', 'Romance', 'Film-Noir', 'Music', 'War', 'Crime', 'Thriller', 'Drama', 'Sport', 'Game-Show', 'Documentary', 'News', 'Biography', 'Comedy', 'Short', 'Animation', 'Horror', 'Action', 'Fantasy', 'Mystery', 'Sci-Fi', 'Western', 'Musical']\ncontent_ratings = movies['content_rating'].unique().tolist()\nlanguages = movies['language'].unique().tolist()\nmovies['genres'] = movies['genres'].apply(lambda x: [genres.index(o) for o in x.split('|')])\nmovies['content_rating'] = movies['content_rating'].apply(lambda x: content_ratings.index(x))\nmovies['language'] = movies['language'].apply(lambda x: languages.index(x))\nmovies.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2f00e88-3a0e-cd6d-3dc8-b2580ede5f38"},"source":"I will extract the data that will be used in training. I will try to predict the IMDB score based on the following inputs:\n\n * duration (float64)\n * director_facebook_likes (float64)\n * actor_1_facebook_likes (float64)\n * actor_2_facebook_likes (float64)\n * actor_3_facebook_likes (float64)\n * genres (26 multi-hot)\n * cast_total_facebook_likes (float64)\n * budget (float64)\n * title_year (float64)\n * content_rating (18 one-hot)\n * language (48 one-hot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63297ea0-a205-fe30-9866-0691002cf793"},"outputs":[],"source":"num_attribs = ['duration', 'director_facebook_likes', 'actor_1_facebook_likes', 'actor_2_facebook_likes',\n               'actor_3_facebook_likes', 'cast_total_facebook_likes', 'budget', 'title_year']\ncat_attribs = ['content_rating', 'language']\nmulti_cat_attribs = ['genres']"},{"cell_type":"markdown","metadata":{"_cell_guid":"07760354-a5e5-1197-a2ac-eb810c2c85c9"},"source":"We have 3 non-numerical fields that need to be transformed. For each one-hot encoded I'll create N columns filled with 0 (not matching) and 1 (matching). "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98a7b96a-1e99-64f2-6c8e-cd080691aad9"},"outputs":[],"source":"data = movies[num_attribs + ['genres']].copy(deep=True)\n\nfor x in cat_attribs:\n    num = len(movies[x].unique())\n    for i in range(num):\n        data[x + '_' + str(i)] = (movies[x] == i).astype(int)\n\ndef calculate(s):\n    row = dict()\n    for x in range(len(genres)):\n        row['genre_' + str(x)] = 1 if x in s['genres'] else 0\n    return pd.Series(row)\n\ndata = data.merge(data.apply(calculate, axis=1), left_index=True, right_index=True)\ndata.drop('genres', axis=1, inplace=True)\n    \ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a811f82e-c499-c6c2-21b3-91233720fadf"},"outputs":[],"source":"scores = movies['imdb_score'].copy(deep=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a2b6d2b-72c7-356a-eb6a-6782fd325894"},"outputs":[],"source":"# Scaling inputs\nfor x in data:\n    m = data[x].max() * 1.0\n    data[x] = data[x].apply(lambda x: x / m)\n\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5464463-f5e3-8220-ae0b-78f7f94bc903"},"outputs":[],"source":"# Scaling outputs\nmax_score = 10.0\nscores = scores.apply(lambda x: x / max_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20dc03b9-9cc5-cf86-859b-62615a645a6f"},"outputs":[],"source":"combined = data.copy(deep=True)\ncombined['score'] = scores\n\ncorr_matrix = combined.corr()\ncorr_matrix[\"score\"].sort_values(ascending=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"49104748-da41-3a93-c932-c36005197961"},"source":"Let's try different fitters :)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"670f9e37-3833-5ef4-9d11-da9b9fb5677d"},"outputs":[],"source":"from sklearn.model_selection import cross_val_score\nimport numpy as np\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94aa46d5-800a-73f4-712b-94553e3c85d9"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(data, scores)\n\nlin_scores = cross_val_score(lin_reg, data, scores,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8a32adc-2f22-9ffc-1f24-fb8eaeff61cc"},"outputs":[],"source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(data, scores)\n\ntree_scores = cross_val_score(tree_reg, data, scores,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)\ndisplay_scores(tree_rmse_scores)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cbde8b6e-fac1-758f-2725-30ed35d610da"},"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(data, scores)\n\nforest_scores = cross_val_score(forest_reg, data, scores,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)"},{"cell_type":"markdown","metadata":{"_cell_guid":"694e060f-a5b8-4d97-f487-35e839ea7ee6"},"source":"Let's try some NN with tensorflow"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9f74cd2-47b8-406e-45df-1cbe0f12ce0a"},"outputs":[],"source":"import tensorflow as tf\nimport math"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b4ab135-2b53-6f40-e17e-fe6ca57c4e66"},"outputs":[],"source":"npinputs = data.as_matrix()\nnpoutputs = np.asarray([[x] for x in scores.as_matrix()])\n\nprint(npinputs)\nprint(npoutputs)\n\nsplit = 4600\ntrain_dataset = npinputs[:split, :]\ntrain_labels = npoutputs[:split, :]\nvalid_dataset = npinputs[split:, :]\nvalid_labels = npoutputs[split:, :]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e36b5b32-7e01-acf7-3934-3bba64149cf5"},"outputs":[],"source":"BATCH_SIZE = 64\n\nINPUT_SIZE = len(npinputs[0])\nOUTPUT_SIZE = 1\nHIDDEN_LAYERS = [512, 1024]\n\ndef accuracy(predictions, labels):\n    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) /\n            predictions.shape[0])\n\ndef model(inputs):\n    # First hidden\n    with tf.name_scope('hidden1'):\n        weights = tf.Variable(tf.truncated_normal(\n            [INPUT_SIZE, HIDDEN_LAYERS[0]], stddev=1.0 /\n            math.sqrt(float(HIDDEN_LAYERS[0]))))\n        biases = tf.Variable(tf.zeros([HIDDEN_LAYERS[0]]), dtype=tf.float32)\n        hidden = tf.nn.relu(tf.matmul(inputs, weights) + biases)\n        \n    with tf.name_scope('hidden2'):\n        weights = tf.Variable(tf.truncated_normal(\n            [HIDDEN_LAYERS[0], HIDDEN_LAYERS[1]], stddev=1.0 /\n            math.sqrt(float(HIDDEN_LAYERS[1]))))\n        biases = tf.Variable(tf.zeros([HIDDEN_LAYERS[1]]), dtype=tf.float32)\n        hidden = tf.nn.relu(tf.matmul(hidden, weights) + biases)\n\n    with tf.name_scope('output'):\n        weights = tf.Variable(tf.truncated_normal(\n            [HIDDEN_LAYERS[1], OUTPUT_SIZE], stddev=1.0 /\n            math.sqrt(float(HIDDEN_LAYERS[1]))), dtype=tf.float32)\n        biases = tf.Variable(tf.zeros([OUTPUT_SIZE]), dtype=tf.float32)\n        output = tf.matmul(hidden, weights) + biases\n\n    logits = output\n\n    return logits\n\ndef floss(logits, outputs):\n    return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(logits, outputs))))\n\n\ndef foptimizer(loss):\n    return tf.train.GradientDescentOptimizer(0.001).minimize(loss)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64664035-2ec2-e975-1094-48566cef0a8c"},"outputs":[],"source":"graph = tf.Graph()\nwith graph.as_default():\n    # Placeholders\n    inputs = tf.placeholder(tf.float32, shape=[\n        None, INPUT_SIZE])\n    outputs = tf.placeholder(tf.float32, shape=[None, OUTPUT_SIZE])\n\n    logits = model(inputs)\n\n    loss = floss(logits, outputs)\n    optimizer = foptimizer(loss)\n    preds = tf.nn.softmax(logits)\n\nnum_steps = 5001\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    for step in range(num_steps):\n        offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n        batch_data = train_dataset[offset:(offset + BATCH_SIZE), :]\n        batch_labels = train_labels[offset:(offset + BATCH_SIZE), :]\n        feed_dict = {inputs: batch_data, outputs: batch_labels}\n        _, l, predictions = session.run(\n            [optimizer, loss, preds], feed_dict=feed_dict)\n        if (step % 500 == 0):\n            print('Minibatch loss at step %d: %f' % (step, l))\n            feed_dict = {inputs: valid_dataset, outputs: valid_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, preds], feed_dict=feed_dict)\n            print('Validation loss at step %d: %f' % (step, l))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af2fdf7a-7005-9dd0-6fff-8ee8ee6620ea"},"outputs":[],"source":"for i, genre in enumerate(genres):\n    print (i, genre)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}