{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/charts1/MachL.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\">Classification Algorithms</h1>\n\nIn Machine Learning, classification is a **supervised learning approach**, which can be thought of as a means of categorizing or \"classifying\" some unknown items into a discrete set of \"classes.\"Classification attempts to learn the relationship between a set of feature variables and a target variable of interest.<br>\nThe target attribute in classification is a categorical variable with discrete values.So, how does classification and classifiers work?\nGiven a set of training data points, along with the target labels, classification determines the class label for an unlabeled test case.<br>\nLet’s explain this with an example.<br>\nA good sample of classification is the **loan default prediction** .Suppose a bank is concerned about the potential for loans not to be repaid.If previous loan default data can be used to predict which customers are likely to have problems repaying loans, these \"bad risk\" customers can either have their loan application declined or offered alternative products.<br><br>\n\nThe goal of a loan default predictor is to use existing loan default data, which is information about the customers (such as age, income, education, etc.), to build a classifier, pass a new customer or potential future defaulter to the model, and then label it (i.e. the\ndata points) as \"Defaulter\" or \"Not Defaulter\", or for example, 0 or 1.This is how a classifier predicts an unlabeled test case.<br>\n**Please notice that this specific example was about a binary classifier with two values.** <br>\nWe can also build classifier models for both binary classification and multi-class classification.For example, imagine that you collected data about a set of patients, all of whom suffered from the same illness.During their course of treatment, each patient responded to one of three medications.You can use this labeled dataset, with a classification algorithm, to build a classification model.\nThen you can use it to find out which drug might be appropriate for a future patient with the same illness.<br>\nAs you can see, it is a sample of multi-class classification.<br><br>\n\nClassification has different business use cases as well, for example: <br>\nTo predict the category to which a customer belongs; For Churn detection, where we predict whether a customer switches to another provider or brand; Or to predict whether or not a customer responds to a particular advertising campaign.\nData classification has several applications in a wide variety of industries.Essentially, many problems can be expressed as associations between feature and target variables, especially when labeled data is available.This provides a broad range of applicability for classification.<br><br>\nFor example, classification can be used for \n* email filtering\n* speech recognition\n* handwriting\n* recognition\n* bio-metric identification\n* document classification, and much more.<br>\nHere we have the types of classification algorithms in machine learning.<br><br>\n<ul>\n    <li><a href=\"#knn\" style=\"color:violet;\">1. K-Nearest Neighbors Algorithm</a></li>\n    <li><a href=\"#svm\" style=\"color:violet;\" >2. Support Machine Vector Algorithm & Naive Bayes</a></li>\n    <li><a href=\"#dec\" style=\"color:violet;\">3. Decision Tree Classification</a></li>\n    <li><a href=\"#rand\" style=\"color:violet;\">4. Random Forest Classification</a></li>\n    <li><a href=\"#eval\" style=\"color:violet;\">5. Evaluation Of Classification Models</a></li>\n    <li><a href=\"#kmeans\" style=\"color:violet;\">6. K-Means Clustering </a></li>\n    <li><a href=\"#hier\" style=\"color:violet;\">7. Hierarchial Clustering </a></li>\n    <li><a href=\"#pca\" style=\"color:violet;\">8. Principle Component Analysis ( PCA ) </a></li>\n    <li><a href=\"#kfold\" style=\"color:violet;\">9. Model Selection & K-Fold Cross Validation ,Grid Search Cross Validation</a></li>\n    \n    \n</ul>\n\n"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"knn\"> 1. K-Nearest Neighbors Algorithm </h1> \n<p>\n Imagine that a telecommunications provider has segmented its customer base by service usage patterns, categorizing the customers into four groups.If demographic data can be used to predict group membership, the company can customize offers for individual prospective customers.This is a classification problem.<br>\nThat is, given the dataset, with predefined labels, we need to build a model to be used to predict the class of a new or unknown case.<br>\nThe example focuses on using demographic data, such as region, age, and marital status, to predict usage patterns.\nThe target field, called custcat, has four possible values that correspond to the four customer groups, as follows: Basic Service, E-Service, Plus Service, and Total Service.<br>\nOur objective is to build a classifier, for example using the rows 0 to 7, to predict\nthe class of row 8.<br><br>\n    \n\nNow, let’s define the k-nearest neighbors.The k-nearest-neighbors algorithm is a classification algorithm that takes a bunch of labelled points and uses them to learn how to label other points.<br>\nThis algorithm classifies cases based on their similarity to other cases.In k-nearest neighbors, data points that are near each other are said to be “neighbors.”<br>\nK-nearest neighbors is based on this paradigm: “Similar cases with the same class labels are near each other.”Thus, the distance between two cases is a measure of their dissimilarity.There are different ways to calculate the similarity, or conversely, the distance or\ndissimilarity of two data points.\nFor example, this can be done using **Euclidian distance**.<br>\nNow, let’s see how the k-nearest neighbors algorithm actually works.In a classification problem, the k-nearest neighbors algorithm works as follows:<br>\n<p style=\"color:purple;\" >\n1. Pick a value for K. <br>\n2. Calculate the distance from the new case (holdout from each of the cases in the dataset). <br>\n3. Search for the K observations in the training data that are ‘nearest’ to the measurements of the unknown data point. <br>\n4. Predict the response of the unknown data point using the most popular response value from the K nearest neighbors. <br>\n</p>\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K nearest neighbour algorithm\n#First we are going to observe our dataset,\ndata=pd.read_csv(\"../input/classification/data.csv\")\n#drop unnecessary columns from our dataset\ndata.drop(['id','Unnamed: 32'],axis=1,inplace=True)\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As it can be seen from the dataset in diagnosis column we have two types of label,\n#melignant =M -->Bad Tumor Type\n#benign =B -->Good Tumor Type\n\nM=data[ data['diagnosis']=='M' ]\nB=data[ data['diagnosis']=='B' ]\n\n#scatter plot of radius mean-texture_means\n\nplt.scatter( M.radius_mean,M.texture_mean,color=\"red\",label=\"bad\" )\nplt.scatter( B.radius_mean,B.texture_mean,color='green',label='Good')\n\nplt.xlabel('radius mean')\nplt.ylabel('texture mean')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we should convet our data type to integer,so let's make list comprehension\n\ndata.diagnosis=[ 1 if each=='M' else 0 for each in data.diagnosis ]\n\n#dependent variable is diagnosis column\ny=data.diagnosis.values\n#and the rest of the data is called independents,x s ,features that\n#affects the dependent variable y,diagnosis\nx_data=data.drop(['diagnosis'],axis=1) #features\n\n#normalization\n#we should make normalization because of the anormal differences between \n#our values in the columns of the dataset\n\nx=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n\n#train and test splitting\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n\n#knn model,import the required library\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier( n_neighbors=8 ) #n_neighbors=k\nknn.fit( x_train,y_train ) #train our model\nprediction=knn.predict(x_test) #test our model\n\nprint(\"{} nn score: {}\".format(8,knn.score(x_test,y_test))) #accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find k value\n\nscore_list=[]\n\n#let's try different numbers of n_neighbors and see the \n#changing results according to it,here we'll try\n#the numbers between 1-15,and append the results \n#to the array,finally we'll plot it to see better\n\nfor each in range(1,15):\n    knn2=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    current_score=knn2.score( x_test,y_test )\n    score_list.append( current_score )\nplt.plot( range(1,15),score_list )\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\n\n#when  knn has the highest value k takes the value of 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here I can say that if we choose 4 it'll give us best \n#result,since it hast the highest accuracy value in graph\n\n#our previous accuracy is nearly 0.96 means 96%\n#now I'm gonna change it to the 4 and we'll see better \n#accuracy results\n\nknn3=KNeighborsClassifier(n_neighbors=4)\nknn3.fit(x_train,y_train)\ncurrent_score=knn3.score( x_test,y_test )\n\ncurrent_score\n#see it's better,so try and find the best!\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"svm\"> 2. Support Machine Vector Algorithm (SVM )</h1> \n"},{"metadata":{},"cell_type":"markdown","source":"SVM is a supervised algorithm that classifies cases by finding a separator.\n<p style=\"color:purple;\" >\n1.Mapping data to a **high-dimensional** feature space so that dataset can be categorized even when the data otherwise linearly separable.<br>\n 2.Then a separator is the estimator for the data.( Finding a separator )\n</p>\n\nThe data points will fall into two different categories and it should'nt be represent a separable non-linear dataset otherwise these two categories can be separated by a curve not a line.And data can be in the three-dimensional space then we should use hyperplanes as a separator.<br><br>\n\n**Data Transformation**<br>\n\nIf our data point are not linearly separable,for example a simple straightforward line(1 dimensional-1 feature) then we must transform our data.For example, [x,x'2] ,and now since x^2( two dimensional-2 feature) is a curve we can separate the data point more easily.<br>\n\nMapping data into a higher space is called Kernelling.( types are Linear,Polynomial,RBF,Sigmoid )<br><br>\n\nHow do we find the right and optimized separator after transformation ? <br>\n\nSVM s are based on the idea of finding a hyperplane that best divides the dataset into two classes,you can think of the hyperplane as a line that linearly separates these two classes.<br>\nOne reasnonable choice is the best hyperplane is the one that represents the largest separation or margin between the two classes.\n<br><br>\n\nSo the goal is to choose a hyperplane with as big margin as possible.Examples closest to the hyperplane are support vectors.Only support vectors are enough to achive our goal and other training examples can be ignored.We try to find the hyperplane in such a way that it has the maximum distance to support vectors.<br><br>\n<p style=\"color:purple;\" >\nSupport Vector 1 --> w.T+b=1<br>\nHyperplane --> w.T+b=0<br>\nSupport Vector 2 --> w.T+b=-1<br>\n</p>\nThe output of the algorithm is the parameters w and b for the line of hyperplane.You can make classificaitons using this estimated line.<br>\n<br>\nAfter test,if the equation returns a value greater than 0 then the data point belongs to first class which is above the line and vice-versa,<br><br>\n\n**Advantages:**<br>\n\n* Accurate in high-dimensional spaces\n* Memory efficient<br>\n\n**Disadvntages:**<br>\n\n* Prone to over-fitting( if the number of features is greater than the number of samples )\n* No probability estimation\n* Small datasets <br>\n\n**When to use**: <br>\n\n* Image recognition\n* text category assinment\n* detecting spam\n* sentiment analysis\n* gene expression classification\n* regressionioutlier detection and clustering\n\n\n\n\n\n\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#again train and test splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1)\n#by chaning the size of test and size data we can change accuracy val.\nfrom sklearn.svm import SVC\nfrom warnings import simplefilter\n\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nsvm=SVC( random_state=1 )\nsvm.fit( x_train,y_train )\n\nprint('accuracy of svm algorithm: ',svm.score( x_test,y_test ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(x_train,y_train)\n\nprint('accuracy of naive bayes algorithm: ',svm.score( x_test,y_test ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/charts1/chart.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"dec\"> 3. Decision Tree Classification</h1> <br>\n\nA decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.<br><br>\n\nDecision Tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. Its training time is faster compared to the neural network algorithm. The time complexity of decision trees is a function of the number of records and number of attributes in the given data. The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. Decision trees can handle high dimensional data with good accuracy.<br><br>\n\nHow does the Decision Tree algorithm work?<br><br>\n\nThe basic idea behind any decision tree algorithm is as follows:<br>\n\nSelect the best attribute using Attribute Selection Measures(ASM) to split the records.\nMake that attribute a decision node and breaks the dataset into smaller subsets.\nStarts tree building by repeating this process recursively for each child until one of the condition will match:<br>\n\n* All the tuples belong to the same attribute value.\n* There are no more remaining attributes.\n* There are no more instances."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load libraries\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# load dataset\npima = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\npima['Pregnancies']=pima['Pregnancies'].astype('float')\npima.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split dataset in features and target variable\nfeature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n\ny = pima.Outcome # Target variable\nX = pima.drop(['Outcome'],axis=1) # Features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/charts1/chart2.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"rand\"> 4. Random Forest Classification</h1> <br>"},{"metadata":{},"cell_type":"markdown","source":"**Ensemble Methods**<br>\nEnsemble methods are algorithms that combine multiple algorithms into a single predictive model in order to decrease variance, decrease bias, or improve predictions.<br>\n\nEnsemble methods are usually broken into two categories:<br>\n\nParallel: An ensemble method where the models that make up the building blocks of the larger methods are generated independent of each other (i.e., they can be trained/generated as trivially parallel problems applied to the dataset).<br>\nSequential: An ensemble methods where the learners are generated in a sequential order and are dependent on each other (i.e., they can only be trained one at a time, as the next model will require information from the training upstream of it).<br>\nThe random forest algorithm relies on a parallel ensemble method called \"bagging\" to generate its weak classifiers.<br>\n\n**Bagging**<br>\nBagging is a colloquial term for bootstrap aggregation. Bootstrap aggregation is a method that allows us to decrease the variance of an estimate by averaging multiple estimates that are measured from random subsamples of a population.<br>\n\n**Bootstrap Sampling**<br>\nThe first portion of bagging is the application of bootstrap sampling to obtain subsets of the data. These subsets are then fed into one model that will comprise the final ensemble method. This is a straightforward process, given a set of observation data, n observations are selected at random and with replacement to form the subsample. This subsample is what is then fed into the machine learning algorithm of choice to train the model.<br>\n\n**Aggregation**<br>\nAfter all of the models have been built, their outputs must be aggregated into a single coherent prediction for the larger model. In the case of a classifier model, this is usually just a winners take all strategy—whichever category receives the most votes is the final outcome predicted. In the case of a regression problem, a simple average of predicted outcome values is used.<br>\n\n**Feature Bagging**<br>\nFeature bagging (or the random subspace method) is a type of ensemble method that is applied to the features (columns) of a dataset instead of to the observations (rows). It is used as a method of reducing the correlation between features by training base predictors on random subsets of features instead of the complete feature space each time.<br>\n\n**The Random Forest** <br>\nBased on what was previously covered in decision trees and ensemble methods, it should come as little surprise as to where the random forest gets its name or how they’re constructed at a high-level, but let’s go over it anyways.\n\nA random forest is comprised of a set of decision trees, each of which is trained on a random subset of the training data. These trees predictions can then be aggregated to provide a single prediction from a series of predictions.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree     import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n\nbc = load_breast_cancer()\nX = bc.data\ny = bc.target\n\n# Create our test/train split\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n\n\n## build our models\ndecision_tree = DecisionTreeClassifier()\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\n## Train the classifiers\ndecision_tree.fit(X_train, y_train)\nrandom_forest.fit(X_train, y_train)\n\n# Create Predictions\ndt_pred = decision_tree.predict(X_test)\nrf_pred = random_forest.predict(X_test)\n\n# Check the performance of each model\nprint('Decision Tree Model')\nprint(classification_report(y_test, dt_pred, target_names=bc.target_names))\n\nprint('Random Forest Model')\nprint(classification_report(y_test, rf_pred, target_names=bc.target_names))\n\n#Graph our confusion matrix\n\ndt_cm = confusion_matrix(y_test, dt_pred)\nrf_cm = confusion_matrix(y_test, rf_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, we’re able to increase the number of correctly predicted benign tumors and decrease the number of benign tumors that are predicted as malignant. By using a random forest, we can more accurately predict the state of a tumor, potentially decreasing the amount of unneeded procedures performed on patients and decreasing patient stress about their diagnosis.<br>\n\nAt this point, you’d usually start investigating hyperparameter tuning. This is a crucial part of the modeling process in order to ensure that your model is optimal. "},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"eval\"> 5. Evaluation Of Classification Models</h1> <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/classification/data.csv')\n\ndata.drop(['id','Unnamed: 32'],axis=1,inplace=True)\n\n#diagnosis type cannot be object it must be categorical or integer \n#convert them into integer with list comprehension\n\ndata.diagnosis=[ 1 if each=='M' else 0 for each in data.diagnosis ]\n\ny=data.diagnosis.values\nx_data=data.drop(['diagnosis'],axis=1) #features\n\n#normalization\n\nimport numpy as np\nx=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n\n#train and test data splitting\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier( n_estimators=100,random_state=1 )\nrf.fit(x_train,y_train)\n\nprint(\"Random Forest Classification score: \",rf.score(x_test,y_test))\n#estimator how many trees inside of it\n#which subsample will you use every time random_state indicates\n\ny_pred=rf.predict(x_test)\ny_true=y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidth=0.5,linecolor='red',fmt='.0f',ax=ax)\nplt.xlabel('y_pred')\nplt.ylabel('y_true')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.<br>\n<p  style=\"color:purple;\" >\n    <b>true positives (TP):</b> These are cases in which we predicted yes (they have the disease), and they do have the disease.<br>\n    <b> true negatives (TN):</b> We predicted no, and they don't have the disease.<br>\n    <b> false positives (FP):</b> We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")<br>\n    <b>false negatives (FN):</b> We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n</p>\n"},{"metadata":{},"cell_type":"markdown","source":"**Accuracy:** <br>\nOverall, how often is the classifier correct?<br>\n(TP+TN)/total <br>\n**Misclassification Rate:**<br>\nOverall, how often is it wrong?<br>\n(FP+FN)/total <br>\nequivalent to 1 minus Accuracy also known as \"Error Rate\"<br>\n**True Positive Rate:**<br> \nWhen it's actually yes, how often does it predict yes?<br>\nTP/actual yes<br>\nalso known as \"Sensitivity\" or \"Recall\"<br>\n**False Positive Rate:** <br>When it's actually no, how often does it predict yes?<br>\nFP/actual no = 10/60 = 0.17<br>\n**True Negative Rate:** <br>When it's actually no, how often does it predict no?<br>\nTN/actual no = 50/60 = 0.83<br>\nequivalent to 1 minus False Positive Rate<br>\nalso known as \"Specificity\"<br>\n**Precision:** <br>When it predicts yes, how often is it correct?\nTP/predicted yes = 100/110 = 0.91<br>\n**Prevalence:**<br> How often does the yes condition actually occur in our sample?<br>\nactual yes/total = 105/165 = 0.64<br><br>\nA couple other terms are also worth mentioning:<br>\n\n**Null Error Rate:** <br>This is how often you would be wrong if you always predicted the majority class. (In our example, the null error rate would be 60/165=0.36 because if you always predicted yes, you would only be wrong for the 60 \"no\" cases.) This can be a useful baseline metric to compare your classifier against. However, the best classifier for a particular application will sometimes have a higher error rate than the null error rate, as demonstrated by the Accuracy Paradox.<br>\n**Cohen's Kappa:** This is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. In other words, a model will have a high Kappa score if there is a big difference between the accuracy and the null error rate. (More details about Cohen's Kappa.)<br>\n**F Score:** This is a weighted average of the true positive rate (recall) and precision. (More details about the F Score.)<br>\n**ROC Curve:** This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class. (More details about ROC Curves.)<br>"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"kmeans\"> 6. K-means Clustering</h1> <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#create dataset using gaussian variable\n#class 1\nx1=np.random.normal(25,5,1000) #avg=25,sigma=5,total points=1000 (25-30 arasında 1000 tane değer)\ny1=np.random.normal(25,5,1000)\n\n#create dataset using gaussian variable\n#class 2\nx2=np.random.normal(55,5,1000) #avg=25,sigma=5,total points=1000 (25-30 arasında 1000 tane değer)\ny2=np.random.normal(60,5,1000)\n\n#create dataset using gaussian variable\n#class 3\nx3=np.random.normal(55,5,1000) #avg=25,sigma=5,total points=1000 (25-30 arasında 1000 tane değer)\ny3=np.random.normal(15,5,1000)\n\nx=np.concatenate((x1,x2,x3),axis=0) #yukardan aşağı birleştirdik 3000 tane değer elde ettik\ny=np.concatenate((y1,y2,y3),axis=0) #yukardan aşağı birleştirdik 3000 tane değer elde ettik\ndictionary={\"x\":x,\"y\":y}\n\ndata=pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1,color='black') #we give color black to all cause it will be unsupervised learning\nplt.scatter(x2,y2,color='black') #implementation,remove color='black to see classification'\nplt.scatter(x3,y3,color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()#concatenated data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss=[]\n#to find the optimum value of k we try all k values in for loop\n#according to the elbow rule we'll decide the k value\nfor k  in range(1,15):\n    kmeans=KMeans(n_clusters=k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot( range(1,15),wcss )\nplt.xlabel('number of k(cluster value)')\nplt.ylabel('wcss')\nplt.show()\n#most optimum k value is 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Choosing K** <br>\nThe algorithm described above finds the clusters and data set labels for a particular pre-chosen K. To find the number of clusters in the data, the user needs to run the K-means clustering algorithm for a range of K values and compare the results. In general, there is no method for determining exact value of K, but an accurate estimate can be obtained using the following techniques.<br>\n\nOne of the metrics that is commonly used to compare results across different values of K is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of K is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine K.<br>\n\nA number of other techniques exist for validating K, including cross-validation, information criteria, the information theoretic jump method, the silhouette method, and the G-means algorithm. In addition, monitoring the distribution of data points across groups provides insight into how the algorithm is splitting the data for each K."},{"metadata":{"trusted":true},"cell_type":"code","source":"#so lest's choose k=3 and see the model\n\nkmeans2=KMeans(n_clusters=3)\nclusters=kmeans2.fit_predict(data)\n\nprint(clusters[:20])\n#print(clusters[:50])\n# we have in the labels 0,1 and 2 's iside of them\n#it assigned some labels to each group of data inside of it\n\ndata[\"label\"]=clusters #clusterları dataya ekliyoruz\n#ekledim clusterları görsellestire\nplt.scatter( data.x[data.label==0],data.y[data.label==0],color='red')\nplt.scatter( data.x[data.label==1],data.y[data.label==1],color='blue')\nplt.scatter( data.x[data.label==2],data.y[data.label==2],color='green')\n#see successfully classified data,they are differentited from each other\n#and let's see that centroids\n\n#kmeans2.cluster_centers_ is a two dimensional array\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color='yellow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"hier\"> 7. Hierarchial Clustering</h1> <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#create dataset using gaussian variable\n#class 1\nx1=np.random.normal(25,5,100) #avg=25,sigma=5,total points=1000 (25-30 arasında 1000 tane değer)\ny1=np.random.normal(25,5,100)\n\n#create dataset using gaussian variable\n#class 2\nx2=np.random.normal(55,5,100) #avg=25,sigma=5,total points=1000 (25-30 arasında 1000 tane değer)\ny2=np.random.normal(60,5,100)\n\n#create dataset using gaussian variable\n#class 3\nx3=np.random.normal(55,5,100) #avg=25,sigma=5,total points=1000 (25-30 arasında 1000 tane değer)\ny3=np.random.normal(15,5,100)\n\nx=np.concatenate((x1,x2,x3),axis=0) #yukardan aşağı birleştirdik 3000 tane değer elde ettik\ny=np.concatenate((y1,y2,y3),axis=0) #yukardan aşağı birleştirdik 3000 tane değer elde ettik\ndictionary={\"x\":x,\"y\":y}\n\ndata=pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1) #we give color black to all cause it will be unsupervised learning\nplt.scatter(x2,y2) #implementation,remove color='black to see classification'\nplt.scatter(x3,y3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we'll draw dendogram\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nmerg=linkage( data,method='ward') #clusterların içindeki varianceları küçültür,yayılımları minimize eder\ndendrogram(merg,leaf_rotation=90)\nplt.xlabel('data points')\nplt.ylabel('euclidian distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"hier\"> 8. Principle Component Analysis (PCA)</h1> <br>"},{"metadata":{},"cell_type":"markdown","source":"Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\nSo to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\n\niris=load_iris()\n#convert it to dat frame\ndata=iris.data # numpy array\nfeature_names=iris.feature_names\ny=iris.target\n\ndf=pd.DataFrame(data,columns=feature_names)\ndf['class']=y #0-1-2 sınıflarımız var\n\nx=data\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n#datamızın featurelarını azaltmaya çalışıyoruz\n#reduce features into 2,normalize=whitten\npca=PCA( n_components=2,whiten=True )\npca.fit(x)\n\n#boyutu düsürcek modeli ettik,matemaksiksel hesaplamaları yaptık\nx_pca=pca.transform(x)\n#uygulamak için trnsform etmeliyiz\nprint('variance ratio: ',pca.explained_variance_ratio_)\nprint('sum: ',sum(pca.explained_variance_ratio_))\n# %97(sum) sini datanın hala kaybetmedik","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pca ile 2d görselleştirme yapacağız\ndf['p1']=x_pca[:,0]\ndf['p2']=x_pca[:,1]\n#p1 ve p2 bizim reduction sonucunda elde ettiğimiz featurelar \n#bunları dataframe e ekliyoruz\n\ncolor=[\"red\",\"green\",\"blue\"]\n\nimport matplotlib.pyplot as plt\n\nfor each in range(3):\n    plt.scatter(df[ df['class']==each ].p1,df[ df['class']==each ].p2,color=color[each],label=iris.target_names[each] )\nplt.legend()\nplt.show()\n#versicolor ve virginica arasında biraz karışma var ama yinede iyi şekilde\n#birbirlerinden ayrılmışalr,featurelar azaltınca veri kaybı yaşamışız anlamına gelir","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\" id=\"hier\"> 9. Model Selection & K-Fold Cross Validation</h1> <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nimport numpy as np\n\niris=load_iris()\n#convert it to dat frame\nx=iris.data\ny=iris.target\n#normalization\nx=( x-np.min(x))/(np.max(x)-np.min(x))\n#train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n\n#knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3)\n#en yakın 3 komşuya bakıyoruz\n#3 tane accuracy değeri buluyoruz\naccuracies=cross_val_score( estimator=knn,X=x_train, y=y_train,cv=10 )\n#train datamızı 10 a böldük her seferinde ir kaçını train diğerlerini validation olarak kullandık\nprint('Accuracy values are: ',accuracies)\n\nprint('average accuracy: ',np.mean(accuracies))\nprint('average std: ',np.std(accuracies))\n\nknn.fit(x_train,y_train)\nprint('test accuracy: ',knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\n#grid in içine tune etmek istediğimiz parametreyi yazıyoruz\ngrid={'n_neighbors':np.arange(1,50)}\nknn=KNeighborsClassifier()\n#öncesinde n_neighbors u elimizle seçiyorduk\n#ama şimdi GridSearchCV ile optimum değeri bulduruyoruz\n#daha sonra knn ye atayıp knn_cv değerini belirlemek\n#için kullanıyoruz \nknn_cv=GridSearchCV( knn,grid,cv=10 )\nknn_cv.fit(x,y)\n\nprint(\"tuned hyperprarameter K:\",knn_cv.best_params_)\nprint(\"the best accuracy score according to \\nthe tuned parameter: \",knn_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.linear_model import LogisticRegression\n\n\n#grid search cv with logistic regression\nx=x[:100,:]\ny=y[:100]\n\n\n#C parametresi regularization parametresi dir.Fazla yüksek\n#olursa overfit olur model datayı ezberler,çok düşük olursa underfit\n#olur ondada model datayı iyi öğrenemez\n#l1 ve l2 loss functionlardır lasso ve ridge\ngrid={'C':np.logspace(-3,3,7),'penalty':['l1','l2']}\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x,y)\nprint('accuracy',logreg_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us separate them x_train and y_train in fit()\nx=x[:100,:]\ny=y[:100]\n\n#normalization\nx=( x-np.min(x))/(np.max(x)-np.min(x))\n#train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n\ngrid={'C':np.logspace(-3,3,7),'penalty':['l1','l2']}\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x_train,y_train)\nprint('accuracy',logreg_cv.best_score_)\n\n#bu değerlerden yeni bir log_reg modeli oluştur\n\nlogreg2=LogisticRegression()\nlogreg2.fit(x_train,y_train)\nprint('score2: ',logreg2.score(x_test,y_test))\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}