{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/A_DeviceMotion_data/A_DeviceMotion_data\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Load Data\nThe dataset includes time-series data generated by accelerometer and gyroscope sensors (attitude, gravity, userAcceleration, and rotationRate). It is collected with an iPhone 6s kept in the participant's front pocket.\n<br><br>\nThe data is organized in multiple folders. Each folder contains 24 data files (one for each subject) for a particular acitivty. <br>\ndws: Walking Downstairs\n<br>\njog: Jogging\n<br>\nsit: Sitting\n<br>\nstd: Standing \n<br>\nups: Walking Upstairs\n<br>\nwlk: Walking"},{"metadata":{"trusted":true,"_uuid":"67c46f9f03366106e52a69633acdb28ca3dc5f02"},"cell_type":"code","source":"folders = glob('../input/A_DeviceMotion_data/A_DeviceMotion_data/*_*')\nfolders = [s for s in folders if \"csv\" not in s]\ndf_all_list = []\nactivity_codes = {'dws':0,'jog':1,'sit':2,'std':3,'ups':4,'wlk':5}\nactivity_types = list(activity_codes.keys())\n\nfor j in folders:\n    #print('j',j)\n    csv = glob(j + '/*')\n    for i in csv:\n        df = pd.read_csv(i)\n        df['activity'] = activity_codes[j[49:52]]\n        df['sub_num'] = i[len(j)+5:-4]\n        expnum = np.zeros(df.shape[0])\n        df_all_list.append(df)\ndf_all = pd.concat(df_all_list,axis=0)\ndf_all = df_all.drop('Unnamed: 0',axis=1)\nprint(df_all.shape)\nprint(df_all.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d27a0b129524e50ffbe4e5230caf39ceb7c42a4"},"cell_type":"markdown","source":"### Visualization\nLet's visualize one timeseries per acitivty"},{"metadata":{"trusted":true,"_uuid":"b102ebdf2e5441040f5cf0517581e8bf7fbb1912"},"cell_type":"code","source":"for act in activity_types:\n    plt.subplot('61'+str(activity_codes[act]))\n    plt.subplots_adjust(hspace=1.0)\n    df = df_all[(df_all['sub_num']=='1') & (df_all['activity']==activity_codes[act])]\n    plt.title(act)\n    plt.plot(df['userAcceleration.z'][:400])\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eec0c3f4e5b11bc8876fc47497ed7895d7c0012a"},"cell_type":"markdown","source":"### Data Preprocessing \nFor convolution neural network the input data needs to be in a particular format. We will organize data windows of 400 datapoints with 12 channels (one channel per reading)."},{"metadata":{"trusted":true,"_uuid":"b163425848b637691f8a373bade58a78af506c04"},"cell_type":"code","source":"segment_size = 400\ndata_all_x_list = []\ndata_all_y_list = []\nfor j in folders:\n    #print('j',j)\n    csv = glob(j + '/*')\n    for i in csv:\n        df = pd.read_csv(i)\n        df = df.drop('Unnamed: 0',axis=1)\n        win_count = int(df.shape[0]/segment_size)\n        data_x = np.zeros((win_count,segment_size,df.shape[1]))\n        data_y = np.zeros(win_count)\n        for c in range(win_count):\n            start_idx = c*segment_size\n            end_idx = start_idx + segment_size\n            data_x[c,:,:] = df[start_idx:end_idx].values\n            data_y[:] = activity_codes[j[49:52]]\n        data_all_x_list.append(data_x)\n        data_all_y_list.append(data_y)\ndata_all_x = np.concatenate(data_all_x_list,axis=0)\ndata_all_y = np.concatenate(data_all_y_list,axis=0)\ndata_all_y = data_all_y.astype(int)\nprint(data_all_x.shape)\nprint(data_all_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3c3823fc6f5c105b53ca40d5a174ca2844ed600"},"cell_type":"markdown","source":"### Convolution Neural Network\nFollowing the architecture we will be training for this problem.\n<br>\n<a href=\"https://imgur.com/K9YozCL\"><img src=\"https://i.imgur.com/K9YozCL.jpg\" title=\"source: imgur.com\" /></a>\n<br>\nThe activation used is <b> relu</b>\n<br> the probabilities for each class is calculated applying <b> SoftMax</b> on last layer (Logit)"},{"metadata":{"trusted":true,"_uuid":"70f7d12362497cf2716db327d42eb14f27eea3a4"},"cell_type":"code","source":"def cnn_model_fn(features,labels,mode):\n    conv1 = tf.layers.conv1d(inputs=features,\n                             filters=32,\n                             kernel_size=5,\n                             padding='same',\n                             data_format='channels_last',\n                             activation=tf.nn.relu)\n    print('conv1.shape',conv1.shape)\n    pool1 = tf.layers.max_pooling1d(inputs=conv1,pool_size=2,strides=2)\n    print('pool1.shape',pool1.shape)\n    \n    conv2 = tf.layers.conv1d(inputs=pool1,\n                             filters=64,\n                             kernel_size=5,\n                             padding='same',\n                             data_format='channels_last',\n                             activation=tf.nn.relu)\n    print('conv2.shape',conv2.shape)\n    pool2 = tf.layers.max_pooling1d(inputs=conv2,pool_size=2,strides=2)\n    print('pool2.shape',pool2.shape)\n    \n    pool2_flat = tf.reshape(pool2,[-1,100*64])  \n    dense1 = tf.layers.dense(inputs=pool2_flat,units=500,activation=tf.nn.relu)\n    \n    dropput = tf.layers.dropout(inputs=dense1,rate=0.1,training=(mode==tf.estimator.ModeKeys.TRAIN))\n    \n    dense2 =  tf.layers.dense(inputs=dropput,units=100,activation=tf.nn.relu)\n    \n    logits = tf.layers.dense(inputs=dense2,units=6)\n    \n    predictions = { 'classes':tf.arg_max(logits,dimension=1),\n                 'probabilites': tf.nn.softmax(logits,name = 'softmax_tensor')}\n    \n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions)\n    \n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,logits=logits)\n    \n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode=mode,loss=loss,train_op=train_op)\n    \n    eval_metric_op = {'accuracy': tf.metrics.accuracy(labels=labels,predictions=predictions['classes'])}\n    return tf.estimator.EstimatorSpec(mode=mode,loss=loss,eval_metric_ops=eval_metric_op)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34c9f1da418c75e76cea3b34e08e021cd63d33b1"},"cell_type":"code","source":"train_x,test_x,train_y,test_y = train_test_split(data_all_x,data_all_y,test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a47a8a9fec5a495a890d42f3b70678784c785b6"},"cell_type":"code","source":"har_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,model_dir='/tmp/har_classifer_model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1abdb5e600ed763dfbd602c4e5403e0dbec0c04f"},"cell_type":"code","source":"train_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x = train_x,\n    y = train_y,\n    batch_size=10,\n    num_epochs=None,\n    shuffle=True)\n\n\nhar_classifier.train(\n    input_fn=train_input_fn,\n    steps=20000)\n    #hooks=[logging_hook])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fd91e2b836d9de46037c25863b9f57247149412"},"cell_type":"code","source":"test_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x = test_x,\n    y = test_y,\n    num_epochs=1,\n    shuffle=False)\n\neval_results = har_classifier.evaluate(input_fn=test_input_fn)\nprint(eval_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fecb93c2c345868499376218b047e0c1aebe72b8"},"cell_type":"markdown","source":"### Results\nThe accuracy achieved on Test set is <b> more then 96% </b>"},{"metadata":{"trusted":true,"_uuid":"d1caf9231eb0cc4ae4528b8fd724fa8744023797"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}