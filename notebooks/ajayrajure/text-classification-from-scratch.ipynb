{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content</h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"##Library-and-Data\" role=\"tab\" aria-controls=\"profile\">#Library and Data<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Reading-Data\" role=\"tab\" aria-controls=\"messages\">Reading Data<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#Logistic-Regression-Classifier\" role=\"tab\" aria-controls=\"settings\">Logistic Regression Classifier<span class=\"badge badge-primary badge-pill\">3</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Support-Vector-Classifier\" role=\"tab\" aria-controls=\"settings\">Support Vector Classifier<span class=\"badge badge-primary badge-pill\">4</span></a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Multinomial-Naive-Bayes-Classifier\" role=\"tab\" aria-controls=\"settings\">Multinomial Naive Bayes Classifier<span class=\"badge badge-primary badge-pill\">5</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Bernoulli-Naive-Bayes-Classifier\" role=\"tab\" aria-controls=\"settings\">Bernoulli Naive Bayes Classifier<span class=\"badge badge-primary badge-pill\">6</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Gradient-Boost-Classifier\" role=\"tab\" aria-controls=\"settings\">Gradient Boost Classifier<span class=\"badge badge-primary badge-pill\">7</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#XGBoost-Classifier\" role=\"tab\" aria-controls=\"settings\">XGBoost Classifier<span class=\"badge badge-primary badge-pill\">8</span></a>  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Stochastic-Gradient-Descent\" role=\"tab\" aria-controls=\"settings\">Stochastic Gradient Descent<span class=\"badge badge-primary badge-pill\">9</span></a>\n     <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Decision-Tree\" role=\"tab\" aria-controls=\"settings\">Decision Tree<span class=\"badge badge-primary badge-pill\">10</span></a>\n     <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Random-Forest-Classifier\" role=\"tab\" aria-controls=\"settings\">Random Forest Classifier<span class=\"badge badge-primary badge-pill\">11</span></a>\n     <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#KNN-Classifier\" role=\"tab\" aria-controls=\"settings\">KNN Classifier<span class=\"badge badge-primary badge-pill\">12</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#LSTM\" role=\"tab\" aria-controls=\"settings\">LSTM<span class=\"badge badge-primary badge-pill\">12</span></a>\n    "},{"metadata":{},"cell_type":"markdown","source":"# Library and Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping\n\n\nimport nltk\nimport nltk as nlp\nimport string\nimport re\ntrue = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fake['target'] = 'fake'\ntrue['target'] = 'true'\nnews = pd.concat([fake, true]).reset_index(drop = True)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(news['text'], news.target, test_size=0.2, random_state=2020)\n\npipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LogisticRegression())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(news['text'], news.target, test_size=0.2, random_state=2020)\n\npipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LinearSVC())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multinomial Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', MultinomialNB())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bernoulli Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', BernoulliNB())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=55))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', XGBClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=2020))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stochastic Gradient Descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', SGDClassifier())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = 10, \n                                           splitter='best', \n                                           random_state=2020))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', RandomForestClassifier())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = news.text\nY = news.target\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\nmax_words = 500\nmax_len = 75\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\ndef RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model\nmodel = RNN()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model \nplot_model(model, to_file='model1.png')\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(sequences_matrix,Y_train,batch_size=256,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\naccr = model.evaluate(test_sequences_matrix,Y_test)\nprint('Accuracy: {:0.2f}'.format(accr[1]))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}