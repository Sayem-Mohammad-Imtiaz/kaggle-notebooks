{"cells":[{"metadata":{},"cell_type":"markdown","source":"Created by a [TransUnion](https://www.transunion.com/) data scientist that believes that information can be used to **change our world for the better**. #InformationForGood","execution_count":null},{"metadata":{"id":"UWCJ5rdyo0Ea"},"cell_type":"markdown","source":"# Task 2 What do we know about COVID-19 risk factors?\n\n\n***Task Details***\n\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n\nSpecifically, we want to know what the literature reports about:\n\nData on potential risks factors\n\n- Smoking, pre-existing pulmonary disease\n- Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n- Neonates and pregnant women\n- Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n- Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n- Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n- Susceptibility of populations\n- Public health mitigation measures that could be effective for control","execution_count":null},{"metadata":{"id":"Gh8HJyM8phc1"},"cell_type":"markdown","source":"# Step1: Import Data\n- metadata.csv\n- Remove articles that were published before November 2019","execution_count":null},{"metadata":{"id":"Iv_PGGGwP-SU","outputId":"ac280de8-d3e4-4741-e219-5f1ec75dc84a","trusted":true},"cell_type":"code","source":"import re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\nimport operator\nimport string\nimport time\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom collections import defaultdict\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nnltk.download('stopwords')\nnltk.download('punkt')\neng_stopwords = set(stopwords.words(\"english\"))\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"id":"9idLqwEfP2zI","trusted":true},"cell_type":"code","source":"import io\ndata = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"tHXjoQhtQJhk","outputId":"8b62d867-3471-4fb5-cce4-89fbaebb4e8b","trusted":true},"cell_type":"code","source":"print(\"Data shape of metadata.csv: \", data.shape)\n# remove articles that were published before November 2019\nmeta_data = data.loc[data[\"publish_time\"] >= \"2019-11-01\"]\n# only keep title, abstracts, doi and url\nmeta_data = meta_data[[\"cord_uid\", \"title\", \"abstract\", \"doi\", \"url\"]]\nmeta_data = meta_data.reset_index(drop=True)\nprint(\"New data shape: \", meta_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"tm3pstSNuxYl","trusted":true},"cell_type":"code","source":"# clean abstract\ndef clean_abstract(text):\n\n  text = text.lower()\n  word_len = len(\"abstract\")\n  if text[:word_len] == \"abstract\":\n    text = text[word_len:]\n  if \"risk factor\" in text:\n    text = text.replace(\"risk factor\", \"riskfactor\")\n  elif \"risk factors\" in text:\n    text = text.replace(\"risk factors\", \"riskfactor\")\n  return text","execution_count":null,"outputs":[]},{"metadata":{"id":"VqlWHs7xqw4Q","outputId":"09805bce-5b50-40e2-ed89-dbe7e97f0237","trusted":true},"cell_type":"code","source":"#check na\nmeta_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"DxIYiNvSqw69","outputId":"aa6e518d-b07b-4154-9c9a-46b2278bf549","trusted":true},"cell_type":"code","source":"# remove rows with NAs in title and abstract\ncomplete_cases = meta_data.dropna(subset=['title', 'abstract'])\nprint(complete_cases.isna().sum())\nprint(\"Data shape: \", complete_cases.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"CK44bF-7zcGa","trusted":true,"collapsed":true},"cell_type":"code","source":"# clean abstracts\nabstracts = complete_cases['abstract']\ncleaned_abstract = [clean_abstract(text) for text in abstracts]\ncomplete_cases.abstract = cleaned_abstract","execution_count":null,"outputs":[]},{"metadata":{"id":"-r04rjSWmOQk"},"cell_type":"markdown","source":"\n# Step 2: Preprocessing\n- Remove all punctuations, numbers and all other non-alphabets.\n- Convert all texts to lower case.\n- Word Tokenization\n- Remove stopwords\n- Stemming\n\n","execution_count":null},{"metadata":{"id":"xodLdHh0qw9T","trusted":true},"cell_type":"code","source":"def tokenize(text):\n    '''\n    Convert the text corpus to lower case, remove all punctuations and numbers which lead to\n    a final cleaned corpus with only tokens where all characters in the string are alphabets.\n    '''\n    # convert the text to lower case and replace all new line characters by an empty string\n    lower_text = text.lower().replace('\\n', ' ')\n    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    punct_text = lower_text.translate(table)\n    # use NLTK's word tokenization to tokenize the text \n    # remove numbers and empty tokens, only keep tokens with all characters in the string are alphabets\n    tokens = [word for word in word_tokenize(punct_text) if word.isalpha()]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"id":"yoyTMytSrJ4E","trusted":true},"cell_type":"code","source":"def remove_stopwords(word_list, sw=stopwords.words('english')):\n    \"\"\" \n    Filter out all stop words from the text corpus.\n    \"\"\"\n    # It is important to keep words like no and not. Since the meaning of the text will change oppositely\n    # if they are removed.\n    if 'not' in sw:\n        sw.remove('not')\n    if 'no' in sw:\n        sw.remove('no')\n    \n    cleaned = []\n    for word in word_list:\n        if word not in sw:\n            cleaned.append(word)\n    return cleaned","execution_count":null,"outputs":[]},{"metadata":{"id":"t1iJpJQbrOyT","trusted":true},"cell_type":"code","source":"def stem_words(word_list):\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in word_list]\n    text = \" \".join(stemmed_words)\n    return stemmed_words, text","execution_count":null,"outputs":[]},{"metadata":{"id":"e1uLwiHdrYp5","trusted":true},"cell_type":"code","source":"def preprocess(text):\n    \"\"\"\n    Combine all preprocess steps together.\n    Clean each text into tokenized stemmed word list and also return concatenated string\n    \"\"\"\n    tokenized = tokenize(text)\n    stopword_removed = remove_stopwords(tokenized)\n    tokenized_str, cleaned_str = stem_words(stopword_removed)\n    return stopword_removed, tokenized_str, cleaned_str","execution_count":null,"outputs":[]},{"metadata":{"id":"1nAkoOReuhMP","trusted":true},"cell_type":"code","source":"# clean abstracts\nabstracts = complete_cases[\"abstract\"].values.tolist()\ntokenized_abstracts = []\nstr_abstracts = []\n# create a word dictionary to store all words and their stemmed results\nword_dict_abstract = {}\nfor abstract in abstracts:\n  result = preprocess(abstract)\n  tokenized = result[0]\n  stemmed = result[1]\n  for i in range(0,len(stemmed)):\n    if stemmed[i] not in word_dict_abstract:\n      word_dict_abstract[stemmed[i]] = tokenized[i]\n  tokenized_abstracts.append(stemmed)\n  str_abstracts.append(result[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Unique Words in Abstracts:\", len(word_dict_abstract))","execution_count":null,"outputs":[]},{"metadata":{"id":"xKDTB9fHuhTs","trusted":true},"cell_type":"code","source":"complete_cases[\"tokenized_abstract\"] = tokenized_abstracts\ncomplete_cases[\"cleaned_abstracts\"] = str_abstracts","execution_count":null,"outputs":[]},{"metadata":{"id":"_n4ygjy9uhR5","outputId":"8e950aad-84b9-4d9c-b129-0169a2f4dec0","trusted":true},"cell_type":"code","source":"# cleaned dataset\ncomplete_cases.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"gj6M1A6hncF4"},"cell_type":"markdown","source":"# Step 3: Find Related Articles\n1. Topic Modeling: Latent Dirichlet Allocation (LDA)\n    \n    Reference: https://www.kaggle.com/ktattan/lda-and-document-similarity\n2. Keyword Search","execution_count":null},{"metadata":{"id":"Mtrqxo8F4-9O","trusted":true},"cell_type":"code","source":"import gensim\nimport itertools\nimport random\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nfrom scipy.stats import entropy","execution_count":null,"outputs":[]},{"metadata":{"id":"VRTaVT8YcUpI","trusted":true},"cell_type":"code","source":"# 20 topics, on abstracts\ndef train_model_lda(data):\n    \n    num_topics = 20\n    chunksize = 300\n    dictionary = corpora.Dictionary(data['tokenized_abstract'])\n    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized_abstract']]\n\n    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                   alpha=1e-3, eta=0.5e-3, chunksize=chunksize, minimum_probability=0.0, passes=2)\n    return dictionary, corpus, lda","execution_count":null,"outputs":[]},{"metadata":{"id":"_Vbk1QqmcUyL","trusted":true},"cell_type":"code","source":"np.random.seed(2020)\ndictionary, corpus, lda = train_model_lda(complete_cases)","execution_count":null,"outputs":[]},{"metadata":{"id":"iItOey6nenMu","outputId":"30699adb-c4f0-4908-f49d-0b8779259ac1","trusted":true},"cell_type":"code","source":"for i in range(0,20):\n    print(\"--------Topic: \", i, \"--------\")\n    topic = lda.show_topic(topicid=i, topn=50)\n    # use word dictionary to translate stemmed words back to original words\n    print([word_dict_abstract[word[0]] for word in topic])\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{"id":"8ph-GlCKnV2a","trusted":true},"cell_type":"code","source":"# read in task description, the task description was edited in order to cover more keywords\ntask_description = '''\nSmoking, chronic and pre-existing pulmonary disease\nCo-infections and co-morbidities\nNeonates, pregnant women, transmission during pregnancy \nAge and sex/gender difference, women, men\nSocio-economic, psychological, behavioral and environmental factors\nTransmission dynamics\nSeverity of disease, fatality, mortality and high-risk patient groups\nSusceptibility\nPublic health mitigation measures\n'''","execution_count":null,"outputs":[]},{"metadata":{"id":"m0NwXQtKn8G9","trusted":true},"cell_type":"code","source":"task_complete_w, tokenized_list, cleaned_desc = preprocess(task_description)\nbow = dictionary.doc2bow(tokenized_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"4GnYXa-3h4nf","outputId":"48d28498-e8ab-40f7-cf02-a1f84dc2fd9f","trusted":true},"cell_type":"code","source":"doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n# bar plot of topic distribution of this document\nfig, ax = plt.subplots(figsize=(12,6));\n# the histogram of the data\npatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)\nax.set_xlabel('Topic ID', fontsize=15)\nax.set_ylabel('Topic Distribution', fontsize=15)\nax.set_title(\"Topic Distribution of Task Description\", fontsize=20)\nax.set_xticks(np.linspace(0,19,20))\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"iHPsr3luiLxK","outputId":"701350c6-4364-45be-c1fb-df8ee864c3b0","trusted":true},"cell_type":"code","source":"for i in doc_distribution.argsort()[-5:][::-1]:\n    print(i, [word_dict_abstract[item[0]] for item in lda.show_topic(topicid=i, topn=50)], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"B2F6ckL96sb4","trusted":true},"cell_type":"code","source":"def assign_article_topics(text):\n    \"\"\"\n    Find the top 6 topics that are most relevant to each abstract in data.\n    \"\"\"\n    bow = dictionary.doc2bow(text)\n    doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n    top6_topics = doc_distribution.argsort()[-6:][::-1]\n    return list(top6_topics)","execution_count":null,"outputs":[]},{"metadata":{"id":"BZStEZFE8b5a","trusted":true},"cell_type":"code","source":"top6_topics = []\nfor abstract in list(complete_cases[\"tokenized_abstract\"]):\n    top6_topics.append(assign_article_topics(abstract))\ncomplete_cases['top6_topics'] = top6_topics","execution_count":null,"outputs":[]},{"metadata":{"id":"BcT8yaPN8rm7","outputId":"9aca8ff7-a5e4-4e61-ab29-d61769415371","trusted":true},"cell_type":"code","source":"complete_cases.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Topic Distribution of all the abstracts we have in data.","execution_count":null},{"metadata":{"id":"qGgpsyzt99J-","outputId":"1028afde-55d5-42bb-baf6-75a5fe873c8e","trusted":true},"cell_type":"code","source":"# topic distribution of all abstracts in data\nfrom collections import Counter\n\ntopics_total = list(itertools.chain.from_iterable(complete_cases['top6_topics']))\ncount = Counter(topics_total)\ndf = pd.DataFrame.from_dict(count, orient='index')\nax = df.sort_index().plot(kind='bar', legend=None, title=\"Topic Distribution Frequency Plot\")\nax.set_xlabel(\"Topic ID\")\nax.set_ylabel(\"Frequency Count\")","execution_count":null,"outputs":[]},{"metadata":{"id":"2E-06zvuti0Q","outputId":"3f29082f-5b74-419c-bbe8-2c177fad8625","trusted":true},"cell_type":"code","source":"# Most popular 5 topics in data\nsorted_count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse = True)}\nfor i in list(sorted_count.keys())[:5]:\n    print(i, [word_dict_abstract[item[0]] for item in lda.show_topic(topicid=i, topn=20)], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"_oTEua-TC3zW","outputId":"652a8aa1-0a50-4f3d-8c75-441256cb976b","trusted":true},"cell_type":"code","source":"target = list(doc_distribution.argsort()[-5:][::-1])\ntarget = sorted(target)\nprint(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the related abstracts found using LDA:","execution_count":null},{"metadata":{"id":"WhZUhaqHHGYa","outputId":"acb7c31a-7f65-4882-dc5b-68986419e1d2","trusted":true},"cell_type":"code","source":"match_4 = complete_cases[[len(set(item).intersection(set(target)))>=4 for item in complete_cases['top6_topics'].tolist()]]\nmatch_4_title_abs = match_4.drop([\"top6_topics\"], axis = 1)\n\n# filter out articles that are not related to COVID-19 or general infectious diseases\ncov19_names = [\"ncov\", \"covid-19\", \"coronavirus\", \"sars-cov-2\"]\nrelated = []\nfor i in range(0, match_4_title_abs.shape[0]):\n  r = any([(keyword in match_4_title_abs[\"title\"].tolist()[i].lower()) or (keyword in match_4_title_abs[\"abstract\"].tolist()[i].lower()) for keyword in cov19_names])\n  related.append(r)\n\n# relevant target articles\ntarget_data = match_4_title_abs[related]\n# evaluate results\nsample = target_data.sample(5, random_state=2)\nfor i in range(0,sample.shape[0]):\n  print(\"----------Article\", i, \"----------\")\n  print(\"Title: \", \"\\n\", sample['title'].tolist()[i])\n  print(\"Abstracts: \", \"\\n\", sample['abstract'].tolist()[i]) \n  print()\n  print()","execution_count":null,"outputs":[]},{"metadata":{"id":"IcURzw_BOW_t"},"cell_type":"markdown","source":"# Step3-2: Final Target Articles\n- Use keyword search to find more related articles\n- Filter out all articles that are not related to COVID-19\n","execution_count":null},{"metadata":{"id":"aCmXdXJtcAxd","trusted":true},"cell_type":"code","source":"keyword = [\"smok\", \"preexisting\", \"pre-existing\", \"chronic\", \"underlying\", # smoking, pre-existing pulmonary disease\n           \"co-infection\", \"coinfection\", \"co-morbidities\", \"comorbidities\", # co-infections\n           \"neonat\", \"pregnancy\", \"pregnant\", \"newborn\", \"uterine\", \"infant\", # neorates and pregnant women\n           \"socioeconomic\", \"socio-economic\", # socio-economic\n           \"susceptibility\", \"environmental\", \"psychological\", \"stress\", \"mental\", \"frustration\", # mental\n           \"mitigation measures\", #government mitigation measures\n           \"riskfactor\", \"riskfactors\"\n]","execution_count":null,"outputs":[]},{"metadata":{"id":"ynSLVUIkw5xk","trusted":true},"cell_type":"code","source":"found = []\ncomplete_cases = complete_cases.reset_index(drop=True)\nfor i in range(0, complete_cases.shape[0]):\n  r = any([(word in complete_cases[\"title\"][i].lower()) or (word in complete_cases[\"abstract\"][i].lower()) for word in keyword])\n  found.append(r)","execution_count":null,"outputs":[]},{"metadata":{"id":"dVw4Nf6by4Gw","trusted":true},"cell_type":"code","source":"# keyword search results\nkeyword_search_output = complete_cases[found].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check related abstracts found using keyword search:","execution_count":null},{"metadata":{"id":"5x8pAEnAyxtp","outputId":"00d4bb05-31e5-435d-f137-1b4f209187eb","trusted":true},"cell_type":"code","source":"# filter out articles that are not related to covid-19 from keyword search results\nrelated_2 = []\nfor i in range(0, keyword_search_output.shape[0]):\n  r = any([(keyword in keyword_search_output[\"title\"][i].lower()) or (keyword in keyword_search_output[\"abstract\"][i].lower()) for keyword in cov19_names])\n  related_2.append(r)\nkeyword_found = keyword_search_output[related_2].reset_index(drop=True)\n\n# evaluate results\nsample = keyword_found.sample(5, random_state=0)\nfor i in range(0,sample.shape[0]):\n  print(\"----------Article\", i, \"----------\")\n  print(\"Title: \", \"\\n\", sample['title'].tolist()[i])\n  print(\"Abstracts: \", \"\\n\", sample['abstract'].tolist()[i]) \n  print()\n  print()","execution_count":null,"outputs":[]},{"metadata":{"id":"prvZsM5f6OCP","trusted":true},"cell_type":"code","source":"keyword_search_ids = set(keyword_found['cord_uid'].tolist())\nlda_search_ids = set(target_data['cord_uid'].tolist())\nall_found = keyword_search_ids.union(lda_search_ids)\nfinal_articles = complete_cases[[uid in list(all_found) for uid in complete_cases['cord_uid']]]","execution_count":null,"outputs":[]},{"metadata":{"id":"o8TInQDxe_ll"},"cell_type":"markdown","source":"# Step 4: Text Summarization\n- Summarize each selected abstract using ***skip-thoughts*** \n\n    Reference: \n* https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1\n* https://github.com/ryankiros/skip-thoughts\n\n- Why Text Summarization:\n\n\n>> In the end, we want to find key senteces that are most relevant to each sub-task, not just relevant abstracts/articles.\n\n>> So we want to keep each abstract clear and concise. \n\n**Skip-Thoughts**\n\n\n*  Similar to sent2vec, skip-thoughts learns to encode input sentences into a fixed-dimensional vector representation. \n*  Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation for each sentence in the input\n*  The decoder is expected to generate the previous and next sentences, word by word. \n*  The encoder-decoder network is trained to minimize the sentence reconstruction loss.\n*  These learned representations are embeddings of semantically similar sentences are closer to each other in vector space.\n\n\n\n---\n\n### **Pipeline**\n\n**Step 1** Preprocessing\n\n- To lower case, remove punctuations, remove stop words.... (Complete)\n\n**Step 2** Skip-thoughts Encoder\n- Sentences are encoded into fixed-dimensional vector representation\n\n**Step 3** Clustering\n- The encoded sentences are clustered (K-Means)\n\n**Step 4** Summarization\n- The sentences corresponding to sentence embeddings that are closest to the cluster centers are chosen and combined as final summary\n\n","execution_count":null},{"metadata":{"id":"QZXXX3EwYWAy","outputId":"f550415b-9232-4bf5-9bdd-519e587fd2a4","trusted":true},"cell_type":"code","source":"#!pip install numpy==1.16.1\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\nfrom keras.models import load_model\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min","execution_count":null,"outputs":[]},{"metadata":{"id":"9kWJ4Y0Od4K8","outputId":"d3ae7fb1-3d88-46cc-b94e-7585320a982f","trusted":true},"cell_type":"code","source":"# clone the skip-thoughts github repository\n# !git clone https://github.com/ryankiros/skip-thoughts","execution_count":null,"outputs":[]},{"metadata":{"id":"J-HCIbe4cfmk","outputId":"be4a4a26-87fb-41b1-cf98-067b7c4efd93","trusted":true},"cell_type":"code","source":"# pre-trained models and word embeddings (wikipedia data)\n#!mkdir ../input/skipthoughtspackage/skip-thoughts-master/models\n#!wget -P ../input/skipthoughtspackage/skip-thoughts-master/models http://www.cs.toronto.edu/~rkiros/models/dictionary.txt\n#!wget -P ../input/skipthoughtspackage/skip-thoughts-master/models http://www.cs.toronto.edu/~rkiros/models/utable.npy\n#!wget -P ../input/skipthoughtspackage/skip-thoughts-master/models http://www.cs.toronto.edu/~rkiros/models/btable.npy\n#!wget -P ../input/skipthoughtspackage/skip-thoughts-master/models http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz\n#!wget -P ../input/skipthoughtspackage/skip-thoughts-master/models http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl\n#!wget -P ../input/skipthoughtspackage/skip-thoughts-master/models http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz\n#!wget -P ../input/skipthoughtspackage/skip-thoughts-master/models http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl","execution_count":null,"outputs":[]},{"metadata":{"id":"j4dy7El4YRbZ","outputId":"a03b5a86-7860-466d-9b4d-870585e24187","trusted":true},"cell_type":"code","source":"# import os\n# os.chdir('../../')\n# !pwd\n# import skipthoughts","execution_count":null,"outputs":[]},{"metadata":{"id":"ptMQvW1zbYEU","trusted":true},"cell_type":"code","source":"def skipthought_encode(text):\n    \"\"\"\n    Sentences are encoded into fixed-dimensional vector representation\n    \"\"\"\n    \n    enc_text = [None]*len(text)\n    cum_sum_sentences = [0]\n    sent_count = 0\n    for txt in text:\n        sent_count += len(txt)\n        cum_sum_sentences.append(sent_count)\n        all_sentences = [sent for txt in text for sent in txt]\n    \n    print('Loading pre-trained models...')\n    model = skipthoughts.load_model()\n    encoder = skipthoughts.Encoder(model)\n    print('Encoding sentences...')\n    enc_sentences = encoder.encode(all_sentences, verbose=False)\n\n    for i in range(len(text)):\n        begin = cum_sum_sentences[i]\n        end = cum_sum_sentences[i+1]\n        enc_text[i] = enc_sentences[begin:end]\n    return enc_text","execution_count":null,"outputs":[]},{"metadata":{"id":"r563GN0rf9aX","trusted":true},"cell_type":"code","source":"def MakeSummary(texts):\n    \"\"\"\n    Produce final summary of each text.\n    \"\"\"\n    n_topics = len(texts)\n    summary = [None]*n_topics\n    print('Starting to encode...')\n    # sentence encoding...\n    enc_texts= skipthought_encode(texts)\n    print('Encoding Finished')\n    # perform K-Means clustering\n    for i in range(n_topics):\n        enc_txt = enc_texts[i]\n        # number of clusters\n        n_clusters = int(np.ceil(len(enc_txt)**0.5))\n        kmeans = KMeans(n_clusters=n_clusters, random_state=1)\n        kmeans = kmeans.fit(enc_txt)\n        avg = []\n        closest = []\n        for j in range(n_clusters):\n            idx = np.where(kmeans.labels_ == j)[0]\n            avg.append(np.mean(idx))\n        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,\\\n                                                   enc_txt)\n        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n        summary[i] = ' '.join([texts[i][closest[idx]] for idx in ordering])\n    print('Clustering Finished')\n\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"id":"nfYlmWEA9mL4","outputId":"c417c6b3-c68e-47d3-dbc2-4e7fe1b3fe4b","trusted":true},"cell_type":"code","source":"all_abstracts = final_articles['abstract'].tolist()\ntokenized_abstracts = [sent_tokenize(abstract) for abstract in all_abstracts]","execution_count":null,"outputs":[]},{"metadata":{"id":"Fa-prcwQJGps","trusted":true},"cell_type":"code","source":"# like 2-gram words, a contiguous sequence of 2 words\n# here I created a list of 2-gram sentences in order to preserve sentence completeness\n\n# Chronic diseases, especially cardiovescular diseases were found to be the most popular underlying conditions that high-risk patients have.\n# We inferred that it might be a risk factor for 2019-ncov.\n\ntwogram_abstracts = []\nfor abstract in tokenized_abstracts:\n    abs_list = []\n    if len(abstract) == 1:\n        abs_list = abstract\n    else:\n        for i in range(len(abstract)-1):\n            abs_list.append(abstract[i]+abstract[i+1])\n        twogram_abstracts.append(abs_list)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ndIK6A75_ada","outputId":"7693ec52-0915-491c-bdb3-dfac11ac079e","trusted":true},"cell_type":"code","source":"#abstract_summary = MakeSummary(twogram_abstracts)\nimport pickle\nwith open(\"/kaggle/input/skipthoughts-results/abstract_summary.pkl\", 'rb') as handle:\n    abstract_summary = pickle.load(handle)","execution_count":null,"outputs":[]},{"metadata":{"id":"cuJTkBtaVf7i","outputId":"d9da74e3-7384-40aa-8426-41d87deb7fd4","trusted":true},"cell_type":"code","source":"twogram_abstracts[999]","execution_count":null,"outputs":[]},{"metadata":{"id":"vIERsEClY8iR","outputId":"7a152982-1d1e-45b1-fce7-dd2896580493","trusted":true},"cell_type":"code","source":"sent_tokenize(abstract_summary[999])","execution_count":null,"outputs":[]},{"metadata":{"id":"ySp_cxjmtASF"},"cell_type":"markdown","source":"# Step 5: Match Sentences to Bullet Points\n\n* Use LDA to help find 20 topics covered in the corpus.\n* Match topics to bullet points under task description.\n* Use topic coverage percentage to sort all the sentences based on relevance.\n* Output the most relevant answers to bullet points.\n* Use word cloud to visualize the results.\n\n","execution_count":null},{"metadata":{"id":"hW20FBIEUeOU","outputId":"12fa3ff1-9f91-4f95-e30e-5cdb184da8b8","trusted":true},"cell_type":"code","source":"# build a large sentence corpus\n# Dump all the sentences in all abstract summaries into the large list\nlarge_list = []\nfrom nltk.tokenize import sent_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfor abstract in abstract_summary:\n  large_list += sent_tokenize(abstract)\nlen(large_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"k0No4ubBV0Oq","outputId":"c6164e2c-a9b4-4ab0-97c9-e6ad49010d45","trusted":true},"cell_type":"code","source":"import re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\nimport operator\nimport string\nimport time\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom collections import defaultdict\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\neng_stopwords = set(stopwords.words(\"english\"))\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"id":"7czpt-CKW4U0","trusted":true},"cell_type":"code","source":"def tokenize(text):\n    '''\n    Convert the text corpus to lower case, remove all punctuations and numbers which lead to\n    a final cleaned corpus with only tokens where all characters in the string are alphabets.\n    '''\n    # convert the text to lower case and replace all new line characters by an empty string\n    lower_text = text.lower().replace('\\n', ' ')\n    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    punct_text = lower_text.translate(table)\n    # use NLTK's word tokenization to tokenize the text \n    # remove numbers and empty tokens, only keep tokens with all characters in the string are alphabets\n    tokens = [word for word in word_tokenize(punct_text) if word.isalpha()]\n    return tokens\n\n    \ndef remove_stopwords(word_list, sw=stopwords.words('english')):\n    \"\"\" \n    Filter out all stop words from the text corpus.\n    \"\"\"\n    # It is important to keep words like no and not. Since the meaning of the text will change oppositely\n    # if they are removed.\n\n    rm_words = ['covid', 'cov', 'sars', 'ncov', 'coronavirus', 'coronaviruses', 'mers', 'corona', 'virus', 'disease', 'diseases', 'viral',\n                'jan', 'january', 'feb', 'february', 'march', 'wuhan', 'china', 'hubei', 'december', 'chinese', 'province', 'article', 'protection',\n                'copyright', 'abstract', 'background', 'conclusion', 'summary']\n    sw += rm_words\n    if 'not' in sw:\n        sw.remove('not')\n    if 'no' in sw:\n        sw.remove('no')\n    \n    cleaned = []\n    for word in word_list:\n        if word not in sw:\n            cleaned.append(word)\n    return cleaned\n\n    \ndef stem_words(word_list):\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in word_list]\n    text = \" \".join(stemmed_words)\n    return stemmed_words, text\n\n    \ndef preprocess(text):\n    \"\"\"\n    Combine all preprocess steps together.\n    Clean each text into tokenized stemmed word list and also return concatenated string\n    \"\"\"\n    tokenized = tokenize(text)\n    stopword_removed = remove_stopwords(tokenized)\n    tokenized_str, cleaned_str = stem_words(stopword_removed)\n    return stopword_removed, tokenized_str, cleaned_str","execution_count":null,"outputs":[]},{"metadata":{"id":"e1mOGTtkXLYQ","trusted":true},"cell_type":"code","source":"# clean sentences\nsentences = large_list\ntokenized_sent = []\nstr_sent = []\nword_dict = {}\nfor sent in sentences:\n  result = preprocess(sent)\n  tokenized = result[0]\n  stemmed = result[1]\n  for i in range(0,len(stemmed)):\n    if stemmed[i] not in word_dict:\n      word_dict[stemmed[i]] = tokenized[i]\n  tokenized_sent.append(stemmed)\n  str_sent.append(result[2])","execution_count":null,"outputs":[]},{"metadata":{"id":"7oWipzsqX58t","trusted":true},"cell_type":"code","source":"new_data = pd.DataFrame(\n    {\"sentence\": large_list,\n     \"tokenized_sent\": tokenized_sent,\n     \"string_sent\": str_sent}\n)\nnew_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Pt545NwcYPeg","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\ntfidf_vectorizer = TfidfVectorizer(min_df=5, norm=\"l2\", sublinear_tf=True, strip_accents='unicode', \n                                                  analyzer='word', token_pattern=r'\\w{1,}', stop_words='english', \n                                                  ngram_range=(1,3))\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(new_data[\"string_sent\"])\nprint(tfidf_matrix.shape)\nterms_title = tfidf_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"id":"Xm0DTtjXxYyN","outputId":"97eb0c55-b459-4e43-ab79-b219a8a34db5","trusted":true},"cell_type":"code","source":"num_clusters = 5\nkm = KMeans(n_clusters = num_clusters, random_state=1)\n\n%time km.fit(tfidf_matrix)\nclusters = km.labels_.tolist()","execution_count":null,"outputs":[]},{"metadata":{"id":"0icyJQRAyutn","outputId":"ffa9494d-afef-4321-958a-c096eb42d802","trusted":true},"cell_type":"code","source":"print(\"Top Vocabularies of each cluster:\")\n\norder_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\nfor i in range(num_clusters):\n  print(\"Cluster %d vocabs\" % i, end = \"\\n\")\n\n  for ind in order_centroids[i, :50]:\n    top_word = \"\"\n    for word in terms_title[ind].split(\" \"):\n      top_word += word_dict[word] + \" \"\n\n    print(\" %s\" % top_word.encode(\"utf-8\", \"ignore\"), end = \"\\n\")\n  print(\"\")\n  print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"id":"mR9vfHNS1H-g","trusted":true},"cell_type":"code","source":"# remove cluster 3,4\nnew_data['cluster_id'] = clusters\ndata_sub = new_data[(new_data['cluster_id'] != 3) & (new_data['cluster_id'] != 4)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use LDA to find target topics:","execution_count":null},{"metadata":{"id":"TYI8cU3mYzy7","trusted":true},"cell_type":"code","source":"from nltk import FreqDist\nimport gensim\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nfrom scipy.stats import entropy\n\ndef train_model_lda(data):\n    \n    num_topics = 20\n    chunksize = 300\n    dictionary = corpora.Dictionary(new_data['tokenized_sent'])\n    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized_sent']]\n\n    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                   alpha=0.005, eta=0.001, chunksize=chunksize, minimum_probability=0.0, passes=2)\n    return dictionary, corpus, lda","execution_count":null,"outputs":[]},{"metadata":{"id":"pBkkY3XTY2ha","trusted":true},"cell_type":"code","source":"np.random.seed(1)\ndictionary, corpus, lda = train_model_lda(data_sub)","execution_count":null,"outputs":[]},{"metadata":{"id":"S6coeuTRZcP7","outputId":"2806ae3a-a1e4-41bf-b507-f9479289557b","trusted":true},"cell_type":"code","source":"for i in range(0,20):\n  print(\"--------Topic: \", i, \"--------\")\n  topic = lda.show_topic(topicid=i, topn=50)\n  print([word_dict[word[0]] for word in topic])\n  print()\n  print()","execution_count":null,"outputs":[]},{"metadata":{"id":"HU0TQIvwSFxl","trusted":true},"cell_type":"code","source":"def assign_topic_importance(text, topic):\n  bow = dictionary.doc2bow(text)\n  doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n  return doc_distribution[topic]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Topics we need:\n\n1. Topic 1 - Pregnancy and Neonates\n2. Topic 2 - Co-morbidities and Pre-existing Diseases\n3. Topic 18 - Gender and Age Difference\n4. Topic 19 - Psychological, Behavioral Factors and Government Mitigation Measures\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [1,2,18,19]:\n  print(\"--------Topic: \", i, \"--------\")\n  topic = lda.show_topic(topicid=i, topn=50)\n  print([word_dict[word[0]] for word in topic])\n  print()\n  print()","execution_count":null,"outputs":[]},{"metadata":{"id":"CpjCTXJ4SFxp","outputId":"52baac14-8f8f-411e-c96e-5d36ad1214b3","trusted":true},"cell_type":"code","source":"topic_importance = []\nfor topic in [1,2,18,19]:\n    temp = []\n    for sent in list(new_data['tokenized_sent']):\n        temp.append(assign_topic_importance(sent, topic))\n    topic_importance.append(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate topic coverage scores for each sentence\nnew_data['pregnancy and neonates'] = topic_importance[0]\nnew_data['comorbidities and pre-existing disease'] = topic_importance[1]\nnew_data['gender and age'] = topic_importance[2]\nnew_data['psychological and mitigation measures'] = topic_importance[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean\nfilter_out = []\nsentences = new_data['sentence'].tolist()\ndel_keywords = ['http', 'copy', 'right', 'copyrights', 'reserved', 'www']\nfor sent in sentences:\n    if any([word in sent for word in del_keywords]):\n        filter_out.append(False)\n    else:\n        filter_out.append(True)","execution_count":null,"outputs":[]},{"metadata":{"id":"UbOpdzpCWgPA","outputId":"b6c5fc5a-e60b-4ef4-bcbe-74f4a70f7c8b","trusted":true},"cell_type":"code","source":"new_data = new_data[filter_out]\nnew_data = new_data.drop_duplicates(subset=\"sentence\")\nprint(new_data.shape)\nnew_data[['pregnancy and neonates', \n          'comorbidities and pre-existing disease', \n          'gender and age', \n          'psychological and mitigation measures']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Step: Answer Questions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What do we know about *Pregnancy and Neonates*?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(new_data['pregnancy and neonates'], 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pregnancy and Neonates\npreg_neo = new_data.sort_values(by=['pregnancy and neonates'], ascending=False)\n# high importance\npreg_neo = preg_neo[preg_neo['pregnancy and neonates']>=0.214]\nkeyword_check = ['pregnan', 'newborn', 'neonat', 'babies', 'baby', 'birth', 'mother', 'delivery',\n                'breastmilk', 'perinatal', 'placental', 'fetal', 'maternal', 'breast', 'uterine']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'unpublished']\npreg_check = []\nfor sent in preg_neo['sentence']:\n    if len(sent.split(' ')) < 30:\n        preg_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        preg_check.append(filtr) \n\npreg_neo = preg_neo[preg_check]\nprint(\"Number of sentences found: \", preg_neo.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):\n    print(preg_neo['sentence'].tolist()[i])\n    print(round(preg_neo['pregnancy and neonates'].tolist()[i],5))\n    print()\n    print()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sw = stopwords.words('english') + ['conclusions', 'conclusion', 'summary', 'data', 'sars', 'cov', 'of', 'and',\n                                   'with', 'covid', 'therefore', 'many', 'data', 'also', 'but', 'however']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n# Generate a word cloud image\ntext =  ' '.join(preg_neo['sentence'].tolist())\n#wordcloud = WordCloud(stopwords = stopwords.words('english')).generate(text)\n\nimport matplotlib.pyplot as plt\n\n# lower max_font_size\nwordcloud = WordCloud(width=800, height=400, background_color='black', stopwords = sw, max_font_size=60, random_state=4).generate(text)\nplt.figure(figsize=(30,30), facecolor = 'k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* pregnant women\n* vertical transmission\n* no evidence\n* similar to\n* limited data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What do we know about *Comorbidities and Pre-existing Disease*?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(new_data['comorbidities and pre-existing disease'], 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comorbidities and Pre-existing Diseases\ncomorbidity = new_data.sort_values(by=['comorbidities and pre-existing disease'], ascending=False)\n# high importance\ncomorbidity = comorbidity[comorbidity['comorbidities and pre-existing disease']>=0.447]\nkeyword_check = ['diabete', 'comorbid', 'cancer', 'cardio', 'pulmonary',\n                'smok', 'condition', 'chronic', 'underlying', 'hypertension', 'pneumonia',\n                 'severe', 'preexisting', 'pre-existing', 'obesity', 'co-infection', 'coinfection',\n                'kidney', 'liver', 'severity']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'unpublished', 'mers', 'middle east', 'investigat', 'explor', '2019',\n                    '2020']\ncomorb_check = []\nfor sent in comorbidity['sentence']:\n    if len(sent.split(' ')) < 30:\n        comorb_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        comorb_check.append(filtr) \n\ncomorbidity = comorbidity[comorb_check]\nprint(\"Number of sentences found: \", comorbidity.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):\n    print(comorbidity['sentence'].tolist()[i])\n    print(round(comorbidity['comorbidities and pre-existing disease'].tolist()[i],5))\n    print()\n    print()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sw = stopwords.words('english') + ['conclusions', 'conclusion', 'summary', 'data', 'sars', 'cov', 'of', 'and',\n                                   'with', 'covid', 'therefore', 'many', 'data', 'also', 'but', 'however', 'or ci',\n                                   'ci and', 'ci to', 'including'\n                                  ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Generate a word cloud image\ntext =  ' '.join(comorbidity['sentence'].tolist())\n\n# lower max_font_size\nwordcloud = WordCloud(width=800, height=400, background_color='black', stopwords = sw, max_font_size=60, random_state=4).generate(text)\nplt.figure(figsize=(30,30), facecolor = 'k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* obesity\n* hypertension\n* diabetes\n* kidney diseases\n* cardiovascular, heart diseases\n* cancer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What do we know about *Age and Gender*?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(new_data['gender and age'], 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender and Age\ngender_age = new_data.sort_values(by=['gender and age'], ascending=False)\n# high importance\ngender_age = gender_age[gender_age['gender and age']>=0.273]\nkeyword_check = ['gender', 'male', 'female', 'woman', 'women', 'man', 'men', 'newborn', 'infant', 'child', \n                 'adult', 'old', 'elder', 'young', 'sex', 'advanced age', 'age', 'years old']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'unpublished', 'mers', 'middle east', 'investigat', 'explor', '2019',\n                    '2020', 'studies']\nagegender_check = []\nfor sent in gender_age['sentence']:\n    if len(sent.split(' ')) < 30:\n        agegender_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        agegender_check.append(filtr) \n\ngender_age = gender_age[agegender_check]\nprint(\"Number of sentences found: \", gender_age.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):\n    print(gender_age['sentence'].tolist()[i])\n    print(round(gender_age['gender and age'].tolist()[i],5))\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What do we know about *Psychological and Mitigation Measures*?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(new_data['psychological and mitigation measures'], 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Psychological and Mitigation Measures\npsy_measures = new_data.sort_values(by=['psychological and mitigation measures'], ascending=False)\n\n# high importance\npsy_measures = psy_measures[psy_measures['psychological and mitigation measures']>=0.255]\nkeyword_check = ['mental', 'psycholog', 'environmental', 'anxiety', 'socioeconomic',\n                'socio-economic', 'econom', 'measure', 'lockdown', 'quarantine', 'isolation',\n                'mitigation', 'distancing', 'emotional', 'regulation', 'order', 'behav']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'capital', 'unpublished', 'mers', 'middle east', 'investigat', \n                    'explor', '2019',\n                    '2020', 'studies']\npsy_check = []\nfor sent in psy_measures['sentence']:\n    if len(sent.split(' ')) < 30:\n        psy_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        psy_check.append(filtr) \n\npsy_measures = psy_measures[psy_check]\nprint(\"Number of sentences found: \", psy_measures.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):    \n    print(psy_measures['sentence'].tolist()[i])\n    print(round(psy_measures['psychological and mitigation measures'].tolist()[i],5))\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sw = stopwords.words('english') + ['conclusions', 'conclusion', 'summary', 'data', 'sars', 'cov', 'of', 'and',\n                                   'with', 'covid', 'therefore', 'many', 'data', 'also', 'but', 'however', \n                                   'including', 'among', 'due to', 'may', 'could'\n                                  ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a word cloud image\ntext =  ' '.join(psy_measures['sentence'].tolist())\n\n# lower max_font_size\nwordcloud = WordCloud(width=800, height=400, background_color='black', stopwords = sw, max_font_size=60, random_state=3).generate(text)\nplt.figure(figsize=(30,30), facecolor = 'k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* mental health\n* social distancing\n* stress\n* lockdown\n* social support\n* pyschological distress\n* depression anxiety","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}