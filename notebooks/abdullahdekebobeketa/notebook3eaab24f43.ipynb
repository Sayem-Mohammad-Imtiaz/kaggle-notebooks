{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\n\n# Deep Learning NLP\n\n**Fake news classifier**: Train a text classification model to detect fake news articles!"},{"metadata":{},"cell_type":"markdown","source":"**Summary**\n\n1. Further fine-tuning efforts using various hyper-parameters optimization techniques might help to get a better result. Also, I didn't apply any text-cleaning, the evaluation result given by the model, as can be seen from the percentage accuracy and loss, is fairly satisfactory.\n\n2. Even, a more simpler model using CBOW or TF-IDF and MLPs might give a satisfactory result. But, I didn't try them out.\n\n3. I couldn't figure out the magic behind the effects of the embedding weights of each word to a category of a given text, thus I couldn't find out the words which have had the highest impact. That is just to be honest, but I'm sure I would have found a solution for it had I worked on it for a few more hours. (I just tried to complete the exrcise in an approximate duration of about 6hrs, as mentioned in the direction of the challange.). Nonetheless, using a TF-IDF, this task would would have been just comparing weights of each word."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframes = []\nfor dirname, _, filenames in os.walk('/kaggle/input/fake-and-real-news-dataset'):\n    for filename in filenames:\n        df = pd.read_csv(os.path.join(dirname, filename))\\\n        .assign(category = 0 if filename.startswith(\"True\") else 1).astype({'category' : 'int32'})\n        # Instead of the above line, I could equivalently use the following 2 lines. I didn't find out the one that performs well.  \n        \n        #df = pd.read_csv(os.path.join(dirname, filename))\n        #df[\"category\"] = np.full((len(df), 1), 0 if filename.find(\"True\") !=-1 else 1, dtype=int)\n        dataframes.append(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(dataframes)):\n    print(\"Length of dataframe {} :\".format(i+1), len(dataframes[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_df = pd.concat(dataframes)\ncombined_df = combined_df.sample(frac=1, random_state=132).reset_index(drop=True)\nprint(len(combined_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combined_df[44890:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef median_of_words_per_texts(texts) -> float:\n    \"\"\" Takes string of texts belonging to a certain category,\n    and returns the median number of words per the given category.\n    \"\"\"\n    words = [len(txt.lower().split()) for txt in texts]\n    return np.median(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_df = combined_df.sample(frac=0.8, random_state=100)\ntest_df = combined_df[~combined_df.index.isin(train_df.index)]\nprocessed_df = pd.DataFrame()\nprocessed_df['texts'] = combined_df[\"title\"].str.lower()  + \" \"*10 + combined_df[\"text\"].str.lower()\n\n# train_df[:6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_of_words_per_texts(processed_df['texts'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts_training = train_df[\"title\"].str.lower() + \" \"*10 + train_df[\"text\"].str.lower()\ntexts_testing = test_df[\"title\"].str.lower() + \" \"*10 + test_df[\"text\"].str.lower()\ncategories_training = train_df[\"category\"]\ncategories_testing = test_df[\"category\"]\n\nvocab_size = 100000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(texts_training)\n\nword_index = tokenizer.word_index\nseq_training = tokenizer.texts_to_sequences(texts_training)\npadded_training_seq = pad_sequences(seq_training, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n\nseq_testing = tokenizer.texts_to_sequences(texts_testing)\npadded_testing_seq = pad_sequences(seq_testing, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, name=\"embedding\"),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-5), metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 10\npadded_training_seq = np.array(padded_training_seq)\ncategories_training = np.array(categories_training)\npadded_testing_seq = np.array(padded_testing_seq)\ncategories_testing = np.array(categories_testing)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory = model.fit(padded_training_seq, categories_training, epochs=num_epochs, validation_data=(padded_testing_seq, categories_testing), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_' + string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"model_fake_real_news.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each word in a top 100,000 vocabulary list, the embedding vector can be give as: \nfor i, layer in  enumerate(model.layers):\n    if(i<1):\n        weights = layer.get_weights()\n        print(\"Layer {}: / Shape of weights: {} X {} \".format(i, len(weights[0]), len(weights[0][0])))\n        print(weights[0][:6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*I couldn't figure out which words are having a greater impact on a category to which a given text belongs*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}