{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom scipy.stats import uniform, truncnorm, randint\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-04T13:57:40.123697Z","iopub.execute_input":"2021-09-04T13:57:40.124151Z","iopub.status.idle":"2021-09-04T13:57:41.209598Z","shell.execute_reply.started":"2021-09-04T13:57:40.124058Z","shell.execute_reply":"2021-09-04T13:57:41.208392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get the Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/churn-modelling/Churn_Modelling.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:41.211165Z","iopub.execute_input":"2021-09-04T13:57:41.211475Z","iopub.status.idle":"2021-09-04T13:57:41.25594Z","shell.execute_reply.started":"2021-09-04T13:57:41.211447Z","shell.execute_reply":"2021-09-04T13:57:41.254609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:41.257787Z","iopub.execute_input":"2021-09-04T13:57:41.258139Z","iopub.status.idle":"2021-09-04T13:57:41.29384Z","shell.execute_reply.started":"2021-09-04T13:57:41.258098Z","shell.execute_reply":"2021-09-04T13:57:41.292725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:41.295502Z","iopub.execute_input":"2021-09-04T13:57:41.295951Z","iopub.status.idle":"2021-09-04T13:57:41.352872Z","shell.execute_reply.started":"2021-09-04T13:57:41.295906Z","shell.execute_reply":"2021-09-04T13:57:41.351819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:41.411617Z","iopub.execute_input":"2021-09-04T13:57:41.412014Z","iopub.status.idle":"2021-09-04T13:57:41.420655Z","shell.execute_reply.started":"2021-09-04T13:57:41.411983Z","shell.execute_reply":"2021-09-04T13:57:41.41939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical features","metadata":{}},{"cell_type":"markdown","source":"* CreditScore\n* Age\n* Tenure\n* Balance\n* NumOfProducts\n* EstimatedSalary","metadata":{}},{"cell_type":"code","source":"df_num = df[['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary']]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:42.219375Z","iopub.execute_input":"2021-09-04T13:57:42.21976Z","iopub.status.idle":"2021-09-04T13:57:42.225843Z","shell.execute_reply.started":"2021-09-04T13:57:42.219725Z","shell.execute_reply":"2021-09-04T13:57:42.22456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30,10))\ngs = fig.add_gridspec(2, df_num.shape[1], hspace=0)\naxs = gs.subplots()\n\nfor idx,col in enumerate(df_num.columns):\n    axs[0, idx].hist(df_num[col])\n    axs[0, idx].title.set_text(col)\n    axs[1, idx].boxplot(df_num[col], vert=False)\nplt.show()    ","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:42.40093Z","iopub.execute_input":"2021-09-04T13:57:42.40149Z","iopub.status.idle":"2021-09-04T13:57:44.082492Z","shell.execute_reply.started":"2021-09-04T13:57:42.401442Z","shell.execute_reply":"2021-09-04T13:57:44.081431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EstimatedSalary feature is uniformly distributed. For now I will keep it like that, but in the future I might do a Box-Muller transformation to the feature, because normally distributed features might help linear models.","metadata":{}},{"cell_type":"code","source":"ax = sns.heatmap(df_num.corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:44.083818Z","iopub.execute_input":"2021-09-04T13:57:44.084112Z","iopub.status.idle":"2021-09-04T13:57:44.648698Z","shell.execute_reply.started":"2021-09-04T13:57:44.084084Z","shell.execute_reply":"2021-09-04T13:57:44.647496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical features","metadata":{}},{"cell_type":"markdown","source":"* Geography\n* Gender\n* HasCrCard\n* IsActiveMember","metadata":{}},{"cell_type":"code","source":"df_cat = df[['Geography','Gender','HasCrCard','IsActiveMember']]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:44.650716Z","iopub.execute_input":"2021-09-04T13:57:44.651075Z","iopub.status.idle":"2021-09-04T13:57:44.657503Z","shell.execute_reply.started":"2021-09-04T13:57:44.651042Z","shell.execute_reply":"2021-09-04T13:57:44.655923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:44.65933Z","iopub.execute_input":"2021-09-04T13:57:44.659659Z","iopub.status.idle":"2021-09-04T13:57:45.184164Z","shell.execute_reply.started":"2021-09-04T13:57:44.659628Z","shell.execute_reply":"2021-09-04T13:57:45.183168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target","metadata":{}},{"cell_type":"markdown","source":"* Exited","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:49:39.585584Z","iopub.execute_input":"2021-07-14T19:49:39.586136Z","iopub.status.idle":"2021-07-14T19:49:39.59218Z","shell.execute_reply.started":"2021-07-14T19:49:39.5861Z","shell.execute_reply":"2021-07-14T19:49:39.591169Z"}}},{"cell_type":"code","source":"sns.countplot(df['Exited']).set_title('Exited')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:45.185349Z","iopub.execute_input":"2021-09-04T13:57:45.185657Z","iopub.status.idle":"2021-09-04T13:57:45.346906Z","shell.execute_reply.started":"2021-09-04T13:57:45.185626Z","shell.execute_reply":"2021-09-04T13:57:45.345773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values = df.Exited.value_counts()\nlabels = ['Not Exited', 'Exited']\n\nfig, ax = plt.subplots(figsize = (4, 3), dpi = 100)\nexplode = (0, 0.09)\n\npatches, texts, autotexts = ax.pie(values, labels = labels, autopct = '%1.2f%%', shadow = True,\n                                   startangle = 90, explode = explode)\n\nplt.setp(texts, color = 'grey')\nplt.setp(autotexts, size = 8, color = 'white')\nautotexts[1].set_color('black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:45.348377Z","iopub.execute_input":"2021-09-04T13:57:45.34874Z","iopub.status.idle":"2021-09-04T13:57:45.512835Z","shell.execute_reply.started":"2021-09-04T13:57:45.348693Z","shell.execute_reply":"2021-09-04T13:57:45.511813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combinations","metadata":{}},{"cell_type":"code","source":"# compare Exit rate across CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary (numericals)\npd.pivot_table(df, index = 'Exited', values = df_num.columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:45.515023Z","iopub.execute_input":"2021-09-04T13:57:45.51548Z","iopub.status.idle":"2021-09-04T13:57:45.563793Z","shell.execute_reply.started":"2021-09-04T13:57:45.515433Z","shell.execute_reply":"2021-09-04T13:57:45.56276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:45.565439Z","iopub.execute_input":"2021-09-04T13:57:45.565816Z","iopub.status.idle":"2021-09-04T13:57:45.572527Z","shell.execute_reply.started":"2021-09-04T13:57:45.56578Z","shell.execute_reply":"2021-09-04T13:57:45.571182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exited - Geography","metadata":{}},{"cell_type":"code","source":"df[['Exited','Geography']].groupby('Geography').agg(['mean','count']).sort_values(by=('Exited','mean'), ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:46.111427Z","iopub.execute_input":"2021-09-04T13:57:46.111828Z","iopub.status.idle":"2021-09-04T13:57:46.13678Z","shell.execute_reply.started":"2021-09-04T13:57:46.111782Z","shell.execute_reply":"2021-09-04T13:57:46.135563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('Geography', hue = 'Exited', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:46.356243Z","iopub.execute_input":"2021-09-04T13:57:46.356617Z","iopub.status.idle":"2021-09-04T13:57:46.588364Z","shell.execute_reply.started":"2021-09-04T13:57:46.356586Z","shell.execute_reply":"2021-09-04T13:57:46.587637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Half the customers are from France, and the other half is divided almost equally between Spain and Germany.\n* Interestingly, the churning rate from German employees is much higher than the other two countries.","metadata":{}},{"cell_type":"markdown","source":"### Exited - Gender","metadata":{}},{"cell_type":"code","source":"df[['Exited','Gender']].groupby('Gender').agg(['mean','count']).sort_values(by=('Exited','mean'), ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:47.051092Z","iopub.execute_input":"2021-09-04T13:57:47.051449Z","iopub.status.idle":"2021-09-04T13:57:47.076342Z","shell.execute_reply.started":"2021-09-04T13:57:47.051418Z","shell.execute_reply":"2021-09-04T13:57:47.074879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('Gender', hue = 'Exited', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:47.254351Z","iopub.execute_input":"2021-09-04T13:57:47.254759Z","iopub.status.idle":"2021-09-04T13:57:47.44979Z","shell.execute_reply.started":"2021-09-04T13:57:47.254722Z","shell.execute_reply":"2021-09-04T13:57:47.448528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The churning rate for females is considerably higher than that for males.","metadata":{}},{"cell_type":"markdown","source":"### Exited - HasCrCard","metadata":{}},{"cell_type":"code","source":"df[['Exited','HasCrCard']].groupby('HasCrCard').agg(['mean','count']).sort_values(by=('Exited','mean'), ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:48.024768Z","iopub.execute_input":"2021-09-04T13:57:48.025136Z","iopub.status.idle":"2021-09-04T13:57:48.047855Z","shell.execute_reply.started":"2021-09-04T13:57:48.025105Z","shell.execute_reply":"2021-09-04T13:57:48.046696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('HasCrCard', hue = 'Exited', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:48.224123Z","iopub.execute_input":"2021-09-04T13:57:48.22449Z","iopub.status.idle":"2021-09-04T13:57:48.409396Z","shell.execute_reply.started":"2021-09-04T13:57:48.224457Z","shell.execute_reply":"2021-09-04T13:57:48.40808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 70% of the employees do have a credit card.\n* The churning rate is the almost exactly the same for the two categories.","metadata":{}},{"cell_type":"markdown","source":"### Exited - IsActiveMember","metadata":{}},{"cell_type":"code","source":"df[['Exited','IsActiveMember']].groupby('IsActiveMember').agg(['mean','count']).sort_values(by=('Exited','mean'), ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:48.884173Z","iopub.execute_input":"2021-09-04T13:57:48.884528Z","iopub.status.idle":"2021-09-04T13:57:48.906389Z","shell.execute_reply.started":"2021-09-04T13:57:48.884495Z","shell.execute_reply":"2021-09-04T13:57:48.905533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('IsActiveMember', hue = 'Exited', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:49.065485Z","iopub.execute_input":"2021-09-04T13:57:49.066047Z","iopub.status.idle":"2021-09-04T13:57:49.273017Z","shell.execute_reply.started":"2021-09-04T13:57:49.065998Z","shell.execute_reply":"2021-09-04T13:57:49.271955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The active / not active members are almost 50-50.\n* The churning rate for active members is considerably higher than that for not active members.","metadata":{}},{"cell_type":"markdown","source":"### Exited - NumOfProducts","metadata":{}},{"cell_type":"code","source":"df[['Exited','NumOfProducts']].groupby('NumOfProducts').agg(['mean','count']).sort_values(by=('Exited','mean'), ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:49.77088Z","iopub.execute_input":"2021-09-04T13:57:49.771227Z","iopub.status.idle":"2021-09-04T13:57:49.791414Z","shell.execute_reply.started":"2021-09-04T13:57:49.771198Z","shell.execute_reply":"2021-09-04T13:57:49.790439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('NumOfProducts', hue = 'Exited', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:49.982607Z","iopub.execute_input":"2021-09-04T13:57:49.983141Z","iopub.status.idle":"2021-09-04T13:57:50.217449Z","shell.execute_reply.started":"2021-09-04T13:57:49.983107Z","shell.execute_reply":"2021-09-04T13:57:50.216694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 96% of the employees have either 1 or 2 products (almost 50-50 between each)\n* However, for 3 or 4 products the MCR is way higher than the GMCR. More interestingly, 100% of the employees who own 4 products have exited.","metadata":{}},{"cell_type":"markdown","source":"### Exited - Tenure","metadata":{}},{"cell_type":"code","source":"df[['Exited','Tenure']].groupby('Tenure').agg(['mean','count']).sort_values(by=('Exited','mean'), ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:50.74066Z","iopub.execute_input":"2021-09-04T13:57:50.741244Z","iopub.status.idle":"2021-09-04T13:57:50.762358Z","shell.execute_reply.started":"2021-09-04T13:57:50.74117Z","shell.execute_reply":"2021-09-04T13:57:50.761366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('Tenure', hue = 'Exited', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:50.966005Z","iopub.execute_input":"2021-09-04T13:57:50.966479Z","iopub.status.idle":"2021-09-04T13:57:51.300143Z","shell.execute_reply.started":"2021-09-04T13:57:50.966442Z","shell.execute_reply":"2021-09-04T13:57:51.299432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Between 1 and 9 years, the feature is almost equally distributed (high variance). Only the extremes (0 and 10 years) have less density.\n* There is not much insight in the MCR of each category, they are all close to the GMCR and there is not a clear correlation between Tenure and the CR.","metadata":{}},{"cell_type":"markdown","source":"### Exited - Balance","metadata":{}},{"cell_type":"code","source":"df[['Exited','Balance']].groupby('Exited').mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:51.72309Z","iopub.execute_input":"2021-09-04T13:57:51.723616Z","iopub.status.idle":"2021-09-04T13:57:51.73814Z","shell.execute_reply.started":"2021-09-04T13:57:51.723581Z","shell.execute_reply":"2021-09-04T13:57:51.736776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(y='Balance', x='Exited', data=df, orient='v',)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:51.940788Z","iopub.execute_input":"2021-09-04T13:57:51.941153Z","iopub.status.idle":"2021-09-04T13:57:52.113723Z","shell.execute_reply.started":"2021-09-04T13:57:51.941122Z","shell.execute_reply":"2021-09-04T13:57:52.112718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are a lot of Employees with Balance = 0, so let's see the MCR for those\ndf[df['Balance'] == 0].mean()['Exited']","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:52.178902Z","iopub.execute_input":"2021-09-04T13:57:52.179291Z","iopub.status.idle":"2021-09-04T13:57:52.198913Z","shell.execute_reply.started":"2021-09-04T13:57:52.179255Z","shell.execute_reply":"2021-09-04T13:57:52.197932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It is not a very big difference but Employees that have left tend to have higher Balance.\n* For Employees with Balance = 0 the MCR is cosniderably lower than the GMCR.","metadata":{}},{"cell_type":"markdown","source":"### Exited - Age","metadata":{}},{"cell_type":"code","source":"# Divide Age into bins by decade\ndf['Age_bin'] = pd.cut(df['Age'], [0, 20, 30, 40, 50, 60, 70, 80, 90, 100])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:52.987141Z","iopub.execute_input":"2021-09-04T13:57:52.987509Z","iopub.status.idle":"2021-09-04T13:57:53.021267Z","shell.execute_reply.started":"2021-09-04T13:57:52.987478Z","shell.execute_reply":"2021-09-04T13:57:53.020102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['Exited','Age_bin']].groupby('Age_bin').agg(['mean','count'])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:53.25632Z","iopub.execute_input":"2021-09-04T13:57:53.256697Z","iopub.status.idle":"2021-09-04T13:57:53.282456Z","shell.execute_reply.started":"2021-09-04T13:57:53.256647Z","shell.execute_reply":"2021-09-04T13:57:53.281222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nsns.countplot('Age_bin', hue = 'Exited', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:53.503396Z","iopub.execute_input":"2021-09-04T13:57:53.503771Z","iopub.status.idle":"2021-09-04T13:57:53.846692Z","shell.execute_reply.started":"2021-09-04T13:57:53.503739Z","shell.execute_reply":"2021-09-04T13:57:53.845534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Exited'].mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:53.848548Z","iopub.execute_input":"2021-09-04T13:57:53.849016Z","iopub.status.idle":"2021-09-04T13:57:53.856233Z","shell.execute_reply.started":"2021-09-04T13:57:53.84897Z","shell.execute_reply":"2021-09-04T13:57:53.855247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['Exited','Age_bin']].groupby('Age_bin').agg(['mean','count'])['Exited','mean']","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:54.052805Z","iopub.execute_input":"2021-09-04T13:57:54.053137Z","iopub.status.idle":"2021-09-04T13:57:54.075002Z","shell.execute_reply.started":"2021-09-04T13:57:54.053109Z","shell.execute_reply":"2021-09-04T13:57:54.073927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exited_agebin = df[['Exited','Age_bin']].groupby('Age_bin').agg(['mean','count'])['Exited','mean']\n\nfig = plt.figure(figsize=(10,5))\nsns.barplot(df_exited_agebin.index,df_exited_agebin.values).set_title('Mean churning rate for Age bins')\nplt.axhline(y=df['Exited'].mean(), color='#ff3300', linestyle='--', linewidth=1, label='Global mean churning rate')\nplt.legend()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:54.257567Z","iopub.execute_input":"2021-09-04T13:57:54.257946Z","iopub.status.idle":"2021-09-04T13:57:54.740373Z","shell.execute_reply.started":"2021-09-04T13:57:54.257916Z","shell.execute_reply":"2021-09-04T13:57:54.739367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"General notes:\n* Age follows a right skewed distribution.\n* 45% of the employees are in the (30, 40] range of age.\n* 87% of the employees are in the (20, 50] range of age.\n\nInsights\n* Global mean churning rate (GMCR): 20.37%\n* (30, 40] mean churning rate: 34% --> CONSIDERABLY HIGHER THAN THE GLOBAL MEAN\n* (40, 50] mean churning rate: 56% --> WAY HIGHER THAN THE GLOBAL MEAN\n* (50, 60] mean churning rate: 31% --> CONSIDERABLY HIGHER THAN THE GLOBAL MEAN\n* All the other bins (very young or very old) have way less mean churning rate than the global mean.\nSummarizing, there are not Age bins with average MCR. There are some with much lower MCR (the extremes) and some with much higher MCR (the center)\n\nI can further divide the age bins into three groups:\n* (0,40]: MCR << GMCR\n* (40,70]: MCR >> GMCR\n* (70,100]: MCR << GMCR","metadata":{}},{"cell_type":"code","source":"# Divide Age into 3 further categories\ndf['Age_bin_3cat'] = pd.cut(df['Age'], [0, 40, 70, 100])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:54.741976Z","iopub.execute_input":"2021-09-04T13:57:54.742436Z","iopub.status.idle":"2021-09-04T13:57:54.77609Z","shell.execute_reply.started":"2021-09-04T13:57:54.742389Z","shell.execute_reply":"2021-09-04T13:57:54.775136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['Exited','Age_bin_3cat']].groupby('Age_bin_3cat').agg(['mean','count'])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:54.947048Z","iopub.execute_input":"2021-09-04T13:57:54.947452Z","iopub.status.idle":"2021-09-04T13:57:54.97041Z","shell.execute_reply.started":"2021-09-04T13:57:54.947418Z","shell.execute_reply":"2021-09-04T13:57:54.969354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exited_agebin3cat = df[['Exited','Age_bin_3cat']].groupby('Age_bin_3cat').agg(['mean','count'])['Exited','mean']\n\nfig, (ax1, ax2) = plt.subplots(2, figsize=(15,12))\nfig.suptitle('Vertically stacked subplots')\nsns.countplot('Age_bin', hue = 'Exited', data = df, ax=ax1)\nsns.barplot(df_exited_agebin3cat.index,df_exited_agebin3cat.values, ax=ax2)\nplt.axhline(y=df['Exited'].mean(), color='#ff3300', linestyle='--', linewidth=1, label='Global mean churning rate')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:55.191028Z","iopub.execute_input":"2021-09-04T13:57:55.191389Z","iopub.status.idle":"2021-09-04T13:57:55.79369Z","shell.execute_reply.started":"2021-09-04T13:57:55.191358Z","shell.execute_reply":"2021-09-04T13:57:55.792602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Raw Age data distribution (right skewed):","metadata":{}},{"cell_type":"code","source":"#histogram and normal probability plot\nsns.distplot(df['Age'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df['Age'], plot=plt)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:55.795095Z","iopub.execute_input":"2021-09-04T13:57:55.795388Z","iopub.status.idle":"2021-09-04T13:57:56.53594Z","shell.execute_reply.started":"2021-09-04T13:57:55.79536Z","shell.execute_reply":"2021-09-04T13:57:56.534925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Log transformed Age data distribution (resembles normal):","metadata":{}},{"cell_type":"code","source":"#histogram and normal probability plot\nsns.distplot(np.log(df['Age']), fit=norm);\nfig = plt.figure()\nres = stats.probplot(np.log(df['Age']), plot=plt)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:56.537533Z","iopub.execute_input":"2021-09-04T13:57:56.53785Z","iopub.status.idle":"2021-09-04T13:57:57.150853Z","shell.execute_reply.started":"2021-09-04T13:57:56.537814Z","shell.execute_reply":"2021-09-04T13:57:57.149802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Box-Cox transformed Age data distribution (resembles normal):","metadata":{}},{"cell_type":"code","source":"#histogram and normal probability plot\nfitted_data, fitted_lambda = stats.boxcox(df['Age'])\nsns.distplot(fitted_data, fit=norm);\nfig = plt.figure()\nres = stats.probplot(fitted_data, plot=plt)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:57.15261Z","iopub.execute_input":"2021-09-04T13:57:57.152947Z","iopub.status.idle":"2021-09-04T13:57:57.750351Z","shell.execute_reply.started":"2021-09-04T13:57:57.152915Z","shell.execute_reply":"2021-09-04T13:57:57.748442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By taking the log or Box-Cox transformation of the 'Age' feature, the positive skewed distribution becomes close to a normal one. \\\n\nI will add a Box-Cox transformed Age column in the Feature Engineering section, as a normally distributed feature it might help linear models.","metadata":{}},{"cell_type":"code","source":"df['Age_BoxCox'] = fitted_data","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:57.753654Z","iopub.execute_input":"2021-09-04T13:57:57.754125Z","iopub.status.idle":"2021-09-04T13:57:57.75991Z","shell.execute_reply.started":"2021-09-04T13:57:57.754079Z","shell.execute_reply":"2021-09-04T13:57:57.758881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exited - CreditScore - EstimatedSalary","metadata":{}},{"cell_type":"code","source":"df[['Exited','CreditScore','EstimatedSalary']].groupby('Exited').mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:57.769661Z","iopub.execute_input":"2021-09-04T13:57:57.770052Z","iopub.status.idle":"2021-09-04T13:57:57.789348Z","shell.execute_reply.started":"2021-09-04T13:57:57.770019Z","shell.execute_reply":"2021-09-04T13:57:57.7883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n\nsns.boxplot(y='CreditScore', x='Exited', data=df, orient='v', ax=axes[0])\nsns.boxplot(y='EstimatedSalary', x='Exited', data=df, orient='v', ax=axes[1])\nsns.pairplot(df, vars = df[['CreditScore','EstimatedSalary']], hue ='Exited')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:57:58.081967Z","iopub.execute_input":"2021-09-04T13:57:58.082327Z","iopub.status.idle":"2021-09-04T13:58:01.744965Z","shell.execute_reply.started":"2021-09-04T13:57:58.082297Z","shell.execute_reply":"2021-09-04T13:58:01.74355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not much difference. The only thing I can see is that people with really low CreditScore (<400) have a much higher CR.\n\nIn order to explore this I will divide the Employees into CreditScore bins, according to this page:\nhttps://www.equifax.com/personal/education/credit/score/what-is-a-credit-score/\n\nI will add an extra category with CreditScore <400 because that is where I see most clear trend.","metadata":{}},{"cell_type":"code","source":"# Divide CreditScore into 6 categories\ndf['CreditScore_bin'] = pd.cut(df['CreditScore'], [0, 400, 579, 669, 739, 799, 850])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:01.746847Z","iopub.execute_input":"2021-09-04T13:58:01.747287Z","iopub.status.idle":"2021-09-04T13:58:01.781546Z","shell.execute_reply.started":"2021-09-04T13:58:01.747243Z","shell.execute_reply":"2021-09-04T13:58:01.78044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['Exited','CreditScore_bin','EstimatedSalary']].groupby('CreditScore_bin').agg({'Exited': 'mean', 'EstimatedSalary': ['mean','count']})","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:01.783259Z","iopub.execute_input":"2021-09-04T13:58:01.783576Z","iopub.status.idle":"2021-09-04T13:58:01.812487Z","shell.execute_reply.started":"2021-09-04T13:58:01.783546Z","shell.execute_reply":"2021-09-04T13:58:01.811437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see something really interesting: ALL the 19 Employees that have a CreditScore<400 have left the company. But what is more strange is that the employees in this group have a much higher Mean EstimatedSalary than all the other categories. This is really counter-intuitive, as one might think that people that make more money are less likely to have lower CreditScore.\n\nThis may be an error in the dataset.","metadata":{}},{"cell_type":"markdown","source":"### Exited - Geography - EstimatedSalary","metadata":{}},{"cell_type":"code","source":"df[['Exited','Geography','EstimatedSalary']].groupby('Geography').agg({'Exited': 'mean', 'EstimatedSalary': ['mean','count']})","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:01.814345Z","iopub.execute_input":"2021-09-04T13:58:01.814692Z","iopub.status.idle":"2021-09-04T13:58:01.840925Z","shell.execute_reply.started":"2021-09-04T13:58:01.814642Z","shell.execute_reply":"2021-09-04T13:58:01.839748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we've seen before, German Employees have higher MCR. However, it seems they have slightly higher EstimatedSalary than employees from France or Spain, which seems counter-intuitive.\n* The differences in EstimatedSalary for each country are very small, so I don't think this is relevant.","metadata":{}},{"cell_type":"markdown","source":"### Exited - Geography - Gender","metadata":{}},{"cell_type":"code","source":"df[['Exited','Geography','Gender']].groupby(['Geography','Gender']).agg(['mean','count'])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:01.842416Z","iopub.execute_input":"2021-09-04T13:58:01.842831Z","iopub.status.idle":"2021-09-04T13:58:01.868532Z","shell.execute_reply.started":"2021-09-04T13:58:01.842789Z","shell.execute_reply":"2021-09-04T13:58:01.867206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = sns.FacetGrid(df, height=5, aspect=1.6)\ngrid.map(sns.pointplot, 'Geography', 'Exited', 'Gender', palette='deep')\ngrid.add_legend()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:01.870072Z","iopub.execute_input":"2021-09-04T13:58:01.870488Z","iopub.status.idle":"2021-09-04T13:58:02.901454Z","shell.execute_reply.started":"2021-09-04T13:58:01.870441Z","shell.execute_reply":"2021-09-04T13:58:02.900376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Females have higher MCR than males in every country.\n* German employees have higher MCR than the other 2 countries for the same Gender.","metadata":{}},{"cell_type":"markdown","source":"### EstimatedSalary - Geography - Gender","metadata":{}},{"cell_type":"code","source":"df[['EstimatedSalary','Geography','Gender']].groupby(['Geography','Gender']).mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:02.90274Z","iopub.execute_input":"2021-09-04T13:58:02.903045Z","iopub.status.idle":"2021-09-04T13:58:02.922131Z","shell.execute_reply.started":"2021-09-04T13:58:02.903017Z","shell.execute_reply":"2021-09-04T13:58:02.920699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Not much information given, values are similar without clear trends.","metadata":{}},{"cell_type":"markdown","source":"### Exited - Numerical features","metadata":{}},{"cell_type":"code","source":"df_num","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:02.926724Z","iopub.execute_input":"2021-09-04T13:58:02.927077Z","iopub.status.idle":"2021-09-04T13:58:02.952206Z","shell.execute_reply.started":"2021-09-04T13:58:02.927045Z","shell.execute_reply":"2021-09-04T13:58:02.950951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:02.953556Z","iopub.execute_input":"2021-09-04T13:58:02.953893Z","iopub.status.idle":"2021-09-04T13:58:04.26587Z","shell.execute_reply.started":"2021-09-04T13:58:02.953863Z","shell.execute_reply":"2021-09-04T13:58:04.264819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, vars = df_num.columns, hue ='Exited')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:04.267396Z","iopub.execute_input":"2021-09-04T13:58:04.268012Z","iopub.status.idle":"2021-09-04T13:58:38.943916Z","shell.execute_reply.started":"2021-09-04T13:58:04.267963Z","shell.execute_reply":"2021-09-04T13:58:38.942655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the numerical category pairplots we can get the following insights:\n* There is very little correlation between the different features, so there is not visible redundance or multicollinearity (with the Pearson matrix only Balance-NumOfProducts show some negative correlation, but it is not so clear in the heatmap.\n* It seems that for high 'NumOfProducts' values, the employee is more likely to leave (Exited=1).\n* It seems like for very low CreditScore values, the employee is more likely to leave (Exited=1). A possible explanation is that maybe someone with poor CreditScore would want to get a job with better payment. However, it doesn't seem that people with lower EstimatedSalary have worse CreditScore (corr: -0.0014).","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:38.945801Z","iopub.execute_input":"2021-09-04T13:58:38.946116Z","iopub.status.idle":"2021-09-04T13:58:38.975733Z","shell.execute_reply.started":"2021-09-04T13:58:38.946088Z","shell.execute_reply":"2021-09-04T13:58:38.974634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, I will get rid of the 'RowNumber', 'CustomerId' & 'Surname' columns as they don't provide useful information.","metadata":{}},{"cell_type":"code","source":"df = df.iloc[:,3:]\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:38.977449Z","iopub.execute_input":"2021-09-04T13:58:38.977845Z","iopub.status.idle":"2021-09-04T13:58:39.006797Z","shell.execute_reply.started":"2021-09-04T13:58:38.977811Z","shell.execute_reply":"2021-09-04T13:58:39.005591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical variables","metadata":{}},{"cell_type":"markdown","source":"* Geography\n* Gender","metadata":{}},{"cell_type":"code","source":"# Convert variables using get_dummies\ndf = pd.get_dummies(df, columns=['Geography','Gender'])\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.008245Z","iopub.execute_input":"2021-09-04T13:58:39.008636Z","iopub.status.idle":"2021-09-04T13:58:39.05249Z","shell.execute_reply.started":"2021-09-04T13:58:39.008592Z","shell.execute_reply":"2021-09-04T13:58:39.0513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical Variables","metadata":{}},{"cell_type":"markdown","source":"#### Age","metadata":{}},{"cell_type":"markdown","source":"Age seems to be an important feature, so I will combine it with other 2 continuous numerical features that don't seem to have a clear relation with the outcome: CreditScore and EstimatedSalary.\n\nThis way, I will create 2 ratios:\n* CreditScore/Age\n* EstimatedSalary/Age\nThis approach follows a Weak heredity implementation, in which one explores the interaction effect between a feature that is correlated with the outcome (Age) and a feature that is not (CreditScore and EstimatedSalary).\n\nMay be this relations will unveil some hidden relationships with the outcome.\n\nThis interactions were not just random, but have interpretability. One could think that both the Credit Score and the Salary of a given individual can have some sort of relationship with his/her Age.","metadata":{}},{"cell_type":"code","source":"df['ratio_CreditScore_Age'] = df['CreditScore']/df['Age']\ndf['ratio_EstimatedSalary_Age'] = df['EstimatedSalary']/df['Age']","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.053965Z","iopub.execute_input":"2021-09-04T13:58:39.054271Z","iopub.status.idle":"2021-09-04T13:58:39.06361Z","shell.execute_reply.started":"2021-09-04T13:58:39.054242Z","shell.execute_reply":"2021-09-04T13:58:39.062583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ordinal Variables","metadata":{}},{"cell_type":"markdown","source":"* Age_bin\n* Age_bin_3cat\n* CreditScore_bin\n\n\n\nI am going to create the following datasets, following different approaches on the ordinal features. This way, I can afterwards train the same models on every one of this datasets and see which one yields the best performance.\n\n**df_1**: Leave 'Age' and 'CreditScore' as continuous variables.\\\n**df_2**: Drop 'Age' and 'CreditScore', keep 'Age_bin' and 'CreditScore_bin'\\\n**df_3**: Drop 'Age' and 'CreditScore', keep 'Age_bin_3cat' and 'CreditScore_bin'\\\n**df_4**: Transform 'Age' with Box-Cox transformation to normalize it.","metadata":{}},{"cell_type":"code","source":"# cols_todrop_1 = ['Age_bin','Age_bin_3cat','CreditScore_bin','Age_BoxCox']\n# cols_todrop_2 = ['Age','CreditScore','Age_bin_3cat','Age_BoxCox']\n# cols_todrop_3 = ['Age','CreditScore','Age_bin','Age_BoxCox']\n# cols_todrop_4 = ['Age','Age_bin','Age_bin_3cat','CreditScore_bin']\n\n# df_1 = df.drop(cols_todrop_1, axis=1)\n# df_2 = df.drop(cols_todrop_2, axis=1)\n# df_3 = df.drop(cols_todrop_3, axis=1)\n# df_4 = df.drop(cols_todrop_4, axis=1)\n\n# df_2['Age_bin'] = df_2['Age_bin'].cat.codes\n# df_2['CreditScore_bin'] = df_2['CreditScore_bin'].cat.codes\n# df_3['Age_bin_3cat'] = df_3['Age_bin_3cat'].cat.codes\n# df_3['CreditScore_bin'] = df_3['CreditScore_bin'].cat.codes","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.065184Z","iopub.execute_input":"2021-09-04T13:58:39.065562Z","iopub.status.idle":"2021-09-04T13:58:39.076901Z","shell.execute_reply.started":"2021-09-04T13:58:39.065531Z","shell.execute_reply":"2021-09-04T13:58:39.075415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n\n# scaler = StandardScaler()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.081444Z","iopub.execute_input":"2021-09-04T13:58:39.081826Z","iopub.status.idle":"2021-09-04T13:58:39.094105Z","shell.execute_reply.started":"2021-09-04T13:58:39.081793Z","shell.execute_reply":"2021-09-04T13:58:39.092857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Shortlist Promising Models","metadata":{}},{"cell_type":"markdown","source":"### Approach","metadata":{}},{"cell_type":"markdown","source":"#### Features\n\nI will test 4 different datasets, according to what I explained in the previous section:\n* df_1\n* df_2\n* df_3\n* df_4\\\nI will train models in each one to test the Feature Engineering techniques I used.","metadata":{}},{"cell_type":"markdown","source":"#### Performance metrics\n\n* Confusion matrix\n* Precision-Recall\n* Sensitivity-Specificity\n* F1-Score\n* ROC-AUC\\\nI will use all of them to get a better sense of what's going on","metadata":{}},{"cell_type":"markdown","source":"#### Resampling\n\n* Under-sampling\n* Over-sampling\n* SMOTE\\\nI will test the 3 techniques and see which one yields the best results","metadata":{}},{"cell_type":"markdown","source":"#### Models\n\n* Naive Bayes\n* Logistic Regression\n* SVM\n* Random Forest\n* XGBoost\\\nI will test every model with each of the 4 prepared datasets and the 3 resampling techniques without hyperparameter tuning.\\\nFinally, I will select the model, dataset and resampling technique with the best performance, and perform hyperparameter tuning with that one.","metadata":{}},{"cell_type":"markdown","source":"### Data preparation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import RocCurveDisplay, roc_curve, auc, recall_score, precision_score, f1_score, accuracy_score, precision_recall_curve, roc_auc_score, classification_report, confusion_matrix\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\nfrom imblearn.pipeline import Pipeline as imbpipeline\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.096543Z","iopub.execute_input":"2021-09-04T13:58:39.096969Z","iopub.status.idle":"2021-09-04T13:58:39.747799Z","shell.execute_reply.started":"2021-09-04T13:58:39.096934Z","shell.execute_reply":"2021-09-04T13:58:39.746758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('Exited', axis=1)\ny = df['Exited']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=11)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.748914Z","iopub.execute_input":"2021-09-04T13:58:39.749201Z","iopub.status.idle":"2021-09-04T13:58:39.77236Z","shell.execute_reply.started":"2021-09-04T13:58:39.749174Z","shell.execute_reply":"2021-09-04T13:58:39.771213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.773621Z","iopub.execute_input":"2021-09-04T13:58:39.773936Z","iopub.status.idle":"2021-09-04T13:58:39.8236Z","shell.execute_reply.started":"2021-09-04T13:58:39.773908Z","shell.execute_reply":"2021-09-04T13:58:39.822291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_todrop_1 = ['Age_bin','Age_bin_3cat','CreditScore_bin','Age_BoxCox']\ncols_todrop_2 = ['Age','CreditScore','Age_bin_3cat','Age_BoxCox']\ncols_todrop_3 = ['Age','CreditScore','Age_bin','Age_BoxCox']\ncols_todrop_4 = ['Age','Age_bin','Age_bin_3cat','CreditScore_bin']\n\nX1_train = X_train.drop(cols_todrop_1, axis=1)\nX1_test = X_test.drop(cols_todrop_1, axis=1)\nX2_train = X_train.drop(cols_todrop_2, axis=1)\nX2_test = X_test.drop(cols_todrop_2, axis=1)\nX3_train = X_train.drop(cols_todrop_3, axis=1)\nX3_test = X_test.drop(cols_todrop_3, axis=1)\nX4_train = X_train.drop(cols_todrop_4, axis=1)\nX4_test = X_test.drop(cols_todrop_4, axis=1)\n\nX2_train['Age_bin'] = X2_train['Age_bin'].cat.codes\nX2_test['Age_bin'] = X2_test['Age_bin'].cat.codes\nX2_train['CreditScore_bin'] = X2_train['CreditScore_bin'].cat.codes\nX2_test['CreditScore_bin'] = X2_test['CreditScore_bin'].cat.codes\nX3_train['Age_bin_3cat'] = X3_train['Age_bin_3cat'].cat.codes\nX3_test['Age_bin_3cat'] = X3_test['Age_bin_3cat'].cat.codes\nX3_train['CreditScore_bin'] = X3_train['CreditScore_bin'].cat.codes\nX3_test['CreditScore_bin'] = X3_test['CreditScore_bin'].cat.codes","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.825356Z","iopub.execute_input":"2021-09-04T13:58:39.825796Z","iopub.status.idle":"2021-09-04T13:58:39.854968Z","shell.execute_reply.started":"2021-09-04T13:58:39.825758Z","shell.execute_reply":"2021-09-04T13:58:39.85374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(X, y, model, string):\n    \n    pipeline_under = imbpipeline(steps = [['under', RandomUnderSampler(random_state=11)],\n                                        ['scaler', StandardScaler()],\n                                        ['classifier', model()]])\n    \n    pipeline_over = imbpipeline(steps = [['over', RandomOverSampler()],\n                                        ['scaler', StandardScaler()],\n                                        ['classifier', model()]])\n    \n    pipeline_smote = imbpipeline(steps = [['smote', SMOTE(random_state=11)],\n                                        ['scaler', StandardScaler()],\n                                        ['classifier', model()]])\n    \n    stratified_kfold = StratifiedKFold(n_splits=5,\n                                       shuffle=True,\n                                       random_state=11)\n    \n    scores_under = cross_val_score(pipeline_under, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=-1)\n    scores_over = cross_val_score(pipeline_over, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=-1)\n    scores_smote = cross_val_score(pipeline_smote, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=-1)\n    \n#     param_grid = {'classifier__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n#     grid_search = GridSearchCV(estimator=pipeline,\n#                                param_grid=param_grid,\n#                                scoring='roc_auc',\n#                                cv=stratified_kfold,\n#                                n_jobs=-1)\n    \n    \n#     grid_search.fit(X_train, y_train)\n#     cv_score = grid_search.best_score_\n#     test_score = grid_search.score(X_test, y_test)\n#     return {'cv_score':cv_score, 'test_score':test_score}\n    print('-----------------------------------------------------')\n    print(string)\n    print('UNDERSAMPLING Mean ROC AUC: %.3f' % scores_under.mean())\n    print('OVERSAMPLING Mean ROC AUC: %.3f' % scores_over.mean())\n    print('SMOTE Mean ROC AUC: %.3f' % scores_smote.mean())\n#     print(scores_under)\n#     print(scores_over)\n#     print(scores_smote)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T22:14:10.71352Z","iopub.execute_input":"2021-09-02T22:14:10.714214Z","iopub.status.idle":"2021-09-02T22:14:10.723937Z","shell.execute_reply.started":"2021-09-02T22:14:10.714165Z","shell.execute_reply":"2021-09-02T22:14:10.722729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Gaussian Naive Bayes:**","metadata":{}},{"cell_type":"code","source":"fit_model(X1_train, y_train, GaussianNB, 'Feature set 1')\nfit_model(X2_train, y_train, GaussianNB, 'Feature set 2')\nfit_model(X3_train, y_train, GaussianNB, 'Feature set 3')\nfit_model(X4_train, y_train, GaussianNB, 'Feature set 4')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T22:14:10.725158Z","iopub.execute_input":"2021-09-02T22:14:10.725557Z","iopub.status.idle":"2021-09-02T22:14:14.468273Z","shell.execute_reply.started":"2021-09-02T22:14:10.725529Z","shell.execute_reply":"2021-09-02T22:14:14.467102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Logistic Regression:**","metadata":{}},{"cell_type":"code","source":"fit_model(X1_train, y_train, LogisticRegression, 'Feature set 1')\nfit_model(X2_train, y_train, LogisticRegression, 'Feature set 2')\nfit_model(X3_train, y_train, LogisticRegression, 'Feature set 3')\nfit_model(X4_train, y_train, LogisticRegression, 'Feature set 4')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T22:14:14.471903Z","iopub.execute_input":"2021-09-02T22:14:14.472217Z","iopub.status.idle":"2021-09-02T22:14:16.646784Z","shell.execute_reply.started":"2021-09-02T22:14:14.472187Z","shell.execute_reply":"2021-09-02T22:14:16.645763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Support Vector Machines:**","metadata":{}},{"cell_type":"code","source":"fit_model(X1_train, y_train, LinearSVC, 'Feature set 1')\nfit_model(X2_train, y_train, LinearSVC, 'Feature set 2')\nfit_model(X3_train, y_train, LinearSVC, 'Feature set 3')\nfit_model(X4_train, y_train, LinearSVC, 'Feature set 4')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T22:14:16.648002Z","iopub.execute_input":"2021-09-02T22:14:16.648283Z","iopub.status.idle":"2021-09-02T22:14:38.189762Z","shell.execute_reply.started":"2021-09-02T22:14:16.648256Z","shell.execute_reply":"2021-09-02T22:14:38.188432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Random Forest:**","metadata":{}},{"cell_type":"code","source":"fit_model(X1_train, y_train, RandomForestClassifier, 'Feature set 1')\nfit_model(X2_train, y_train, RandomForestClassifier, 'Feature set 2')\nfit_model(X3_train, y_train, RandomForestClassifier, 'Feature set 3')\nfit_model(X4_train, y_train, RandomForestClassifier, 'Feature set 4')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T22:14:38.198661Z","iopub.execute_input":"2021-09-02T22:14:38.199025Z","iopub.status.idle":"2021-09-02T22:14:38.203115Z","shell.execute_reply.started":"2021-09-02T22:14:38.198996Z","shell.execute_reply":"2021-09-02T22:14:38.20193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **XGBoost:**","metadata":{}},{"cell_type":"code","source":"fit_model(X1_train, y_train, xgb.XGBClassifier, 'Feature set 1')\nfit_model(X2_train, y_train, xgb.XGBClassifier, 'Feature set 2')\nfit_model(X3_train, y_train, xgb.XGBClassifier, 'Feature set 3')\nfit_model(X4_train, y_train, xgb.XGBClassifier, 'Feature set 4')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T22:14:38.206035Z","iopub.execute_input":"2021-09-02T22:14:38.206502Z","iopub.status.idle":"2021-09-02T22:14:38.21576Z","shell.execute_reply.started":"2021-09-02T22:14:38.206447Z","shell.execute_reply":"2021-09-02T22:14:38.215029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best performer was:\n* Feature set 4\n* Over-sampling\n* Random Forest Model\n\nIt is a little bit strange because Feature set 4 is the one where I used the Log transformation in the Age variable. This should not matter in a tree based method like Random Forest. In addition, XGBoost which is another tree based had also good performance in this dataset.","metadata":{}},{"cell_type":"markdown","source":"## Fine-Tune the System","metadata":{}},{"cell_type":"markdown","source":"### Chosen model: Random Forest","metadata":{}},{"cell_type":"markdown","source":"First of all, I will see the results in the previous section more in depth to see what is going on.","metadata":{}},{"cell_type":"markdown","source":"#### Plot ROC AUC with different Resampling methods","metadata":{}},{"cell_type":"code","source":"# Initialize Model and Pipeline with all the resampling methods\n\nclassifier = RandomForestClassifier()\n\nscaler = StandardScaler()\n\npipeline = [\n    make_pipeline(RandomUnderSampler(random_state=11), scaler, classifier),\n    make_pipeline(RandomOverSampler(random_state=11), scaler, classifier),\n    make_pipeline(SMOTE(random_state=11), scaler, classifier),\n]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.856102Z","iopub.execute_input":"2021-09-04T13:58:39.856505Z","iopub.status.idle":"2021-09-04T13:58:39.862474Z","shell.execute_reply.started":"2021-09-04T13:58:39.856474Z","shell.execute_reply":"2021-09-04T13:58:39.861342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.863646Z","iopub.execute_input":"2021-09-04T13:58:39.863988Z","iopub.status.idle":"2021-09-04T13:58:39.877729Z","shell.execute_reply.started":"2021-09-04T13:58:39.863956Z","shell.execute_reply":"2021-09-04T13:58:39.876423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = X4_train, y_train","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.879294Z","iopub.execute_input":"2021-09-04T13:58:39.879615Z","iopub.status.idle":"2021-09-04T13:58:39.890648Z","shell.execute_reply.started":"2021-09-04T13:58:39.879584Z","shell.execute_reply":"2021-09-04T13:58:39.889339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.reset_index(drop=True, inplace=True)\nX","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.892051Z","iopub.execute_input":"2021-09-04T13:58:39.89239Z","iopub.status.idle":"2021-09-04T13:58:39.930855Z","shell.execute_reply.started":"2021-09-04T13:58:39.892356Z","shell.execute_reply":"2021-09-04T13:58:39.929745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.reset_index(drop=True, inplace=True)\ny","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.932789Z","iopub.execute_input":"2021-09-04T13:58:39.933271Z","iopub.status.idle":"2021-09-04T13:58:39.942452Z","shell.execute_reply.started":"2021-09-04T13:58:39.933217Z","shell.execute_reply":"2021-09-04T13:58:39.941552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_distribution = pd.Series(y).value_counts(normalize=True)\nax = class_distribution.plot.barh()\nax.set_title(\"Class distribution\")\npos_label = class_distribution.idxmin()\nprint(f\"The positive label considered as the minority class is {pos_label}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:39.943843Z","iopub.execute_input":"2021-09-04T13:58:39.94425Z","iopub.status.idle":"2021-09-04T13:58:40.150134Z","shell.execute_reply.started":"2021-09-04T13:58:39.944211Z","shell.execute_reply":"2021-09-04T13:58:40.149021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = []\nfor model in pipeline:\n    # compute the mean fpr/tpr to get the mean ROC curve\n    mean_tpr, mean_fpr = 0.0, np.linspace(0, 1, 100)\n    for train, test in cv.split(X, y):\n        model.fit(X.iloc[train], y.iloc[train])\n        y_proba = model.predict_proba(X.iloc[test])\n\n        pos_label_idx = np.flatnonzero(model.classes_ == pos_label)[0]\n        fpr, tpr, thresholds = roc_curve(\n            y[test], y_proba[:, pos_label_idx], pos_label=pos_label\n        )\n        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n        mean_tpr[0] = 0.0\n\n    mean_tpr /= cv.get_n_splits(X, y)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n\n    # Create a display that we will reuse to make the aggregated plots for\n    # all methods\n    disp.append(\n        RocCurveDisplay(\n            fpr=mean_fpr,\n            tpr=mean_tpr,\n            roc_auc=mean_auc,\n            estimator_name=f\"{model[0].__class__.__name__}\",\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:40.151608Z","iopub.execute_input":"2021-09-04T13:58:40.151961Z","iopub.status.idle":"2021-09-04T13:58:57.759056Z","shell.execute_reply.started":"2021-09-04T13:58:40.151928Z","shell.execute_reply":"2021-09-04T13:58:57.758009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(9, 9))\nfor d in disp:\n    d.plot(ax=ax, linestyle=\"--\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\")\nax.axis(\"square\")\nfig.suptitle(\"Comparison of over-sampling methods with a Random Forest classifier\")\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nsns.despine(offset=10, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:57.760408Z","iopub.execute_input":"2021-09-04T13:58:57.760749Z","iopub.status.idle":"2021-09-04T13:58:57.998294Z","shell.execute_reply.started":"2021-09-04T13:58:57.760708Z","shell.execute_reply":"2021-09-04T13:58:57.997613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We pretty much get the same ROC AUC regardless of the resampling method. I will further explore oversampling, as it was the one with the best results in the previous section.","metadata":{}},{"cell_type":"markdown","source":"#### AUC Performance measurement using 2 different codes","metadata":{}},{"cell_type":"markdown","source":"In order to see if the results are consistent, I will use the same pipeline (Oversampling, Standard Scaler, Random Forest Classifier) with 2 different codes (Option 1 & Option 2) and check the ROC AUC I get from each one.","metadata":{}},{"cell_type":"markdown","source":"* Option 1:","metadata":{}},{"cell_type":"code","source":"classifier = RandomForestClassifier()\n\nscaler = StandardScaler()\n\npipeline_over = [make_pipeline(RandomOverSampler(random_state=11), scaler, classifier)]\n\n# pipeline_over = imbpipeline(steps = [['over', RandomOverSampler(random_state=11)],\n#                                         ['scaler', StandardScaler()],\n#                                         ['classifier', model()]])\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)\n\nX, y = X4_train, y_train","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:57.999291Z","iopub.execute_input":"2021-09-04T13:58:57.999718Z","iopub.status.idle":"2021-09-04T13:58:58.005748Z","shell.execute_reply.started":"2021-09-04T13:58:57.999688Z","shell.execute_reply":"2021-09-04T13:58:58.004502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = []\nfor model in pipeline_over:\n    # compute the mean fpr/tpr to get the mean ROC curve\n    mean_tpr, mean_fpr = 0.0, np.linspace(0, 1, 100)\n    for train, test in cv.split(X, y):\n        model.fit(X.iloc[train], y.iloc[train])\n        y_proba = model.predict_proba(X.iloc[test])\n\n        pos_label_idx = np.flatnonzero(model.classes_ == pos_label)[0]\n        fpr, tpr, thresholds = roc_curve(\n            y[test], y_proba[:, pos_label_idx], pos_label=pos_label\n        )\n        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n        mean_tpr[0] = 0.0\n\n    mean_tpr /= cv.get_n_splits(X, y)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n\n    # Create a display that we will reuse to make the aggregated plots for\n    # all methods\n#     disp.append(\n#         RocCurveDisplay(\n#             fpr=mean_fpr,\n#             tpr=mean_tpr,\n#             roc_auc=mean_auc,\n#             estimator_name=f\"{model[0].__class__.__name__}\",\n#         )\n#     )","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:58:58.006878Z","iopub.execute_input":"2021-09-04T13:58:58.007221Z","iopub.status.idle":"2021-09-04T13:59:05.303082Z","shell.execute_reply.started":"2021-09-04T13:58:58.007184Z","shell.execute_reply":"2021-09-04T13:59:05.302042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_auc","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:59:05.306094Z","iopub.execute_input":"2021-09-04T13:59:05.306408Z","iopub.status.idle":"2021-09-04T13:59:05.312967Z","shell.execute_reply.started":"2021-09-04T13:59:05.306378Z","shell.execute_reply":"2021-09-04T13:59:05.311817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Option 2:","metadata":{}},{"cell_type":"code","source":"sss = StratifiedKFold(n_splits=5, random_state=11, shuffle=False)\n\n# We will oversample during cross validating\noversample_X = X4_train\noversample_y = y_train\noriginal_Xtrain = X4_train\noriginal_ytrain = y_train\n\nfor train_index, test_index in sss.split(oversample_X, oversample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    oversample_Xtrain, oversample_Xtest = oversample_X.iloc[train_index], oversample_X.iloc[test_index]\n    oversample_ytrain, oversample_ytest = oversample_y.iloc[train_index], oversample_y.iloc[test_index]\n    \noversample_Xtrain = oversample_Xtrain.values\noversample_Xtest = oversample_Xtest.values\noversample_ytrain = oversample_ytrain.values\noversample_ytest = oversample_ytest.values \n\noversample_accuracy = []\noversample_precision = []\noversample_recall = []\noversample_f1 = []\noversample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\n# X_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\n# print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n\n# Cross Validating the right way\n\nfor train, test in sss.split(oversample_Xtrain, oversample_ytrain):\n    oversample_pipeline = imbpipeline(steps = [['over', RandomOverSampler(random_state=11)],\n                                        ['scaler', StandardScaler()],\n                                        ['classifier', RandomForestClassifier()]])\n    oversample_model = oversample_pipeline.fit(oversample_Xtrain[train], oversample_ytrain[train])\n    oversample_prediction_proba = oversample_model.predict_proba(oversample_Xtrain[test])\n    oversample_prediction = oversample_model.predict(oversample_Xtrain[test])\n    \n    oversample_accuracy.append(oversample_pipeline.score(original_Xtrain.iloc[test], original_ytrain.iloc[test]))\n    oversample_precision.append(precision_score(original_ytrain.iloc[test], oversample_prediction))\n    oversample_recall.append(recall_score(original_ytrain.iloc[test], oversample_prediction))\n    oversample_f1.append(f1_score(original_ytrain.iloc[test], oversample_prediction))\n    oversample_auc.append(roc_auc_score(original_ytrain.iloc[test], oversample_prediction_proba[:,1]))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:59:05.31501Z","iopub.execute_input":"2021-09-04T13:59:05.315456Z","iopub.status.idle":"2021-09-04T13:59:11.565343Z","shell.execute_reply.started":"2021-09-04T13:59:05.315413Z","shell.execute_reply":"2021-09-04T13:59:11.564295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(oversample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(oversample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(oversample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(oversample_f1)))\nprint(\"ROC AUC: {:.2f}\".format(np.mean(oversample_auc)))\nprint('---' * 45)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:59:11.566744Z","iopub.execute_input":"2021-09-04T13:59:11.567046Z","iopub.status.idle":"2021-09-04T13:59:11.576748Z","shell.execute_reply.started":"2021-09-04T13:59:11.567018Z","shell.execute_reply":"2021-09-04T13:59:11.575447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results of both of the methods are consistent, **the ROC AUC is 0,84**","metadata":{}},{"cell_type":"markdown","source":"Now I will plot the final results.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nX4_train_scaled = scaler.fit_transform(X4_train)\n\nrf = RandomForestClassifier(random_state=11)\n\nrf_pred = cross_val_predict(rf, X4_train_scaled, y_train, cv=5, method=\"predict_proba\")\n\nprint('Random Forest ROC-AUC: ', roc_auc_score(y_train, rf_pred[:,1]))\n\nrf_fpr, rf_tpr, rf_thresold = roc_curve(y_train, rf_pred[:,1])\n\ndef graph_roc_curve(model_fpr, model_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Random Forest Classifier', fontsize=18)\n    plt.plot(model_fpr, model_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, rf_pred[:,1])))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve(rf_fpr, rf_tpr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:59:11.578234Z","iopub.execute_input":"2021-09-04T13:59:11.578579Z","iopub.status.idle":"2021-09-04T13:59:17.062977Z","shell.execute_reply.started":"2021-09-04T13:59:11.578547Z","shell.execute_reply":"2021-09-04T13:59:17.06165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fine tuning using RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"# Initialize steps of the pipeline\noversampling = RandomOverSampler(random_state=11)\nscaler = StandardScaler()\nmodel = RandomForestClassifier(random_state=11)\n\n# Pipeline\npipeline = make_pipeline(oversampling, scaler, model)\n\n# Parameter grid\ngrid = {\n    \"randomforestclassifier__n_estimators\": [10, 25, 50, 100, 250, 500, 750, 1000, 1250, 1500, 1750, 2000],\n    \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"],\n    \"randomforestclassifier__max_depth\": [10, 20, 30, 40, 50, 75, 100, 150, 200, None],\n    \"randomforestclassifier__min_samples_split\": [1, 2, 3, 4, 5, 8, 10, 15, 20],\n    \"randomforestclassifier__min_samples_leaf\": [1, 2, 3, 4, 5, 8, 10, 15, 20],\n    \"randomforestclassifier__max_features\": [\"auto\", None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n    \"randomforestclassifier__bootstrap\": [True, False],\n    \"randomforestclassifier__max_samples\": [None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n}\n\n# Cross Validation\nkf = StratifiedKFold(n_splits=5)\n\n# RandomizedSearchCV using RFC model, RandomOverSampler & StandardScaler. Scoring is ROC-AUC\nrand_rf = RandomizedSearchCV(pipeline, grid, scoring='roc_auc', n_iter=10, n_jobs=-1, cv=kf, random_state=11)\n\n# Fit the model to the data\ntuned_model = rand_rf.fit(X, y)\n\n# Results\nprint(tuned_model.best_params_)\nprint('Best Score: %s' % tuned_model.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:22:13.601947Z","iopub.execute_input":"2021-09-04T14:22:13.60231Z","iopub.status.idle":"2021-09-04T14:25:07.08345Z","shell.execute_reply.started":"2021-09-04T14:22:13.602277Z","shell.execute_reply":"2021-09-04T14:25:07.082337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# According to the previous results, I will further tune the model to reach the maximum possible performance\n\ngrid_2 = {\n    \"randomforestclassifier__n_estimators\": [50, 75, 100, 120, 150, 200],\n    \"randomforestclassifier__criterion\": [\"entropy\"],\n    \"randomforestclassifier__max_depth\": [5, 7, 10, 12, 15],\n    \"randomforestclassifier__min_samples_split\": [6, 7, 8, 9, 10],\n    \"randomforestclassifier__min_samples_leaf\": [6, 7, 8, 9, 10],\n    \"randomforestclassifier__max_features\": [0.2, 0.3, 0.4],\n    \"randomforestclassifier__bootstrap\": [False],\n    \"randomforestclassifier__max_samples\": [None, 0.1, 0.2],\n}\n\nkf = StratifiedKFold(n_splits=5)\n\nrand_rf_2 = RandomizedSearchCV(pipeline, grid_2, scoring='roc_auc', n_iter=10, n_jobs=-1, cv=kf, random_state=11)\n\ntuned_model_2 = rand_rf_2.fit(X, y)\n\nprint(tuned_model_2.best_params_)\nprint('Best Score: %s' % tuned_model_2.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:25:07.085212Z","iopub.execute_input":"2021-09-04T14:25:07.085533Z","iopub.status.idle":"2021-09-04T14:26:01.278368Z","shell.execute_reply.started":"2021-09-04T14:25:07.0855Z","shell.execute_reply":"2021-09-04T14:26:01.277402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Final Training ROC-AUC: 0.858**","metadata":{}},{"cell_type":"markdown","source":"## Results: Test dataset Scores","metadata":{}},{"cell_type":"markdown","source":"Finally, I will use my final model on the Test dataset (which hasn't been used at all yet, and shouldn't be used for training) and see the scores I get.","metadata":{}},{"cell_type":"code","source":"labels = ['No Churn', 'Churn']\ny_pred = tuned_model_2.predict(X4_test)\nprint(classification_report(y_test, y_pred, target_names=labels))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:29:53.414225Z","iopub.execute_input":"2021-09-04T14:29:53.414619Z","iopub.status.idle":"2021-09-04T14:29:53.477135Z","shell.execute_reply.started":"2021-09-04T14:29:53.414577Z","shell.execute_reply":"2021-09-04T14:29:53.476048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#Plot the confusion matrix\nxlabels = ['Predicted: NO', 'Predicted: YES']\nylabels = ['Actual: NO', 'Actual: YES']\nsns.heatmap(data=cm, xticklabels=xlabels, yticklabels=ylabels, annot=True, fmt='g')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:30:17.39285Z","iopub.execute_input":"2021-09-04T14:30:17.393257Z","iopub.status.idle":"2021-09-04T14:30:17.632302Z","shell.execute_reply.started":"2021-09-04T14:30:17.393226Z","shell.execute_reply":"2021-09-04T14:30:17.631288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:30:25.482069Z","iopub.execute_input":"2021-09-04T14:30:25.482483Z","iopub.status.idle":"2021-09-04T14:30:25.49264Z","shell.execute_reply.started":"2021-09-04T14:30:25.482416Z","shell.execute_reply":"2021-09-04T14:30:25.491423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Final Test ROC-AUC: 0.79**","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:59:32.97618Z","iopub.execute_input":"2021-09-04T14:59:32.976603Z","iopub.status.idle":"2021-09-04T14:59:32.98355Z","shell.execute_reply.started":"2021-09-04T14:59:32.976569Z","shell.execute_reply":"2021-09-04T14:59:32.981884Z"}}},{"cell_type":"code","source":"accuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:36:04.120616Z","iopub.execute_input":"2021-09-04T14:36:04.121015Z","iopub.status.idle":"2021-09-04T14:36:04.127452Z","shell.execute_reply.started":"2021-09-04T14:36:04.120986Z","shell.execute_reply":"2021-09-04T14:36:04.126748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance = tuned_model_2.best_estimator_[2].feature_importances_","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:45:47.820259Z","iopub.execute_input":"2021-09-04T14:45:47.820881Z","iopub.status.idle":"2021-09-04T14:45:47.83842Z","shell.execute_reply.started":"2021-09-04T14:45:47.820843Z","shell.execute_reply":"2021-09-04T14:45:47.83732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importance(importance, names, model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:52:10.213586Z","iopub.execute_input":"2021-09-04T14:52:10.214025Z","iopub.status.idle":"2021-09-04T14:52:10.222321Z","shell.execute_reply.started":"2021-09-04T14:52:10.213991Z","shell.execute_reply":"2021-09-04T14:52:10.221261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(importance, X.columns, 'RANDOM FOREST')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:53:01.358993Z","iopub.execute_input":"2021-09-04T14:53:01.35935Z","iopub.status.idle":"2021-09-04T14:53:01.720166Z","shell.execute_reply.started":"2021-09-04T14:53:01.359321Z","shell.execute_reply":"2021-09-04T14:53:01.719251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions & Nest Steps","metadata":{}},{"cell_type":"markdown","source":"Conclusions:\n* The final model is not very accurate in the test data (AUC=0.79). There seems to be overfitting, as the train score was 0.86. This may be due to the resampling technique I used, which is Oversampling.\n* As suspected, the most important features are 'Age' 'NumOfProducts'. 'NumOfProducts' is a strange feature, and might be another reason for the overfitting. 'Age' is really important.\n* 'Age_BoxCox' and 'ratio_CreditScore_Age', two of the three created features are in the Top-3 most important features. Particularly, 'ratio_CreditScore_Age' is an improvement from the plain 'CreditScore' feature. However, 'ratio_EstimatedSalary_Age' which is the other created feature, was not an improvement over 'EstimatedSalary'.\n* Finally, it is worth mentioning that neither Gender nor Nationality were important.\n\nNext Steps:\n* Try not using the least important features. May be even 'NumOfProducts', because although it is very importante I think it is distorsive. This might help with the over-fitting. Also, I could try PCA or PLS for this.\n* Try other reampling techniques. May be Under-sampling would reduce over-fitting. According to the results, it was not far behind Over-sampling in terms of training performance.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}