{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://s23209.pcdn.co/wp-content/uploads/2016/04/5MinuteAvocadoToastIMG_8273edit-600x900.jpg?p=22555)\n# Avocado prices are on the rise over the past decade \nAvocado has become the world's most trendest fruit. This superfood is now a mainstay for foodies everywhere and the millenials love it more than anyone! However, have you noticed your avocado toast seems to be getting more and more expensive? Avocado prices have increased sharply to up to 129%, with an average price of a Hass avocado reaching a price of $2.10 in 2019, almost doubling in a span of one year.\n\nHere, we will perform an exploratory data analysis and create a model to predict future prices."},{"metadata":{},"cell_type":"markdown","source":"## Importing required packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom scipy import stats\nimport plotly.express as px\nfrom sklearn.preprocessing import LabelEncoder\nfrom fbprophet import Prophet\nfrom fbprophet.plot import add_changepoints_to_plot\nfrom fbprophet.diagnostics import cross_validation, performance_metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing dataset into notebook and create an overview of data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def overview():\n    '''\n    Read a comma-separated values (csv) file into DataFrame.\n    Print 5 rows of data\n    Print number of rows and columns\n    Print datatype for each column\n    Print number of NULL/NaN values for each column\n    Print summary data\n    \n    Return:\n    data, rtype: DataFrame\n    '''\n    data = pd.read_csv('../input/avocado-prices/avocado.csv')\n    print(\"The first 5 rows if data are:\\n\", data.head())\n    print(\"\\n\")\n    print(\"The (Row,Column) is:\\n\", data.shape)\n    print(\"\\n\")\n    print(\"Data type of each column:\\n\", data.dtypes)\n    print(\"\\n\")\n    print(\"The number of null values in each column are:\\n\", data.isnull().sum())\n    print(\"\\n\")\n    print(\"Summary of all the test scores:\\n\", data.describe())\n    return data\n\ndf = overview()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There is around 18k rows of data, with 13 variables to analyse. \n- No null values observed.\n- Numerical columns (4046, 4225 and 4770) represent small hass, large hass and extra large hass. We may want to change the column labels to prevent confusion. "},{"metadata":{},"cell_type":"markdown","source":"### Changing column names and dropping redundant columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns={'4046': \"Small_Hass\", '4225': \"Large_Hass\", '4770': \"Extra_Large_Hass\"})\ndf = df.drop(columns=['Unnamed: 0'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis "},{"metadata":{},"cell_type":"markdown","source":"## Overall price distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\n\nsns.distplot(df['AveragePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data = df, x = 'AveragePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the average price fluctuates around 1.00 and 2.00. Looking into our summary we can see that the mean price is around 1.41. \n- We can see a cluster of outliers present after the price of 2.5. We can get rid of the outlier by getting the 99th percentile and keeping the data below it or we can use Z-score to remove any outliers that have scores that ae greater than or less than 3 or -3 respectively. This will be done before we implement our model. "},{"metadata":{},"cell_type":"markdown","source":"## Comparing prices of different types of avocado between the years"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.barplot(x = 'year', y = 'AveragePrice', hue = 'type', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- First, we can see that organic avocado are definitely more expensive than conventional avocado. \n- We can see prices of avocado increasing from 2015 to 2017 before falling back to similar prices in 2016."},{"metadata":{},"cell_type":"markdown","source":"## Avocado prices in different regions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\ng = sns.factorplot('AveragePrice','region', data = df, hue='year', height = 13, aspect = 0.8, palette ='husl', join = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that 2017 is the year where avocado prices were the highest. It was due to a reduced harvest during that year that brought the prices up.\n- Hartford Springfield seems to have the highest prices throughout the year."},{"metadata":{},"cell_type":"markdown","source":"## Measuring correlation \n- Before running the correlation test, we will need to convert the categorical values into numerical values using LabelEncoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\n\n# Implementing LE on type\nle.fit(df.type.drop_duplicates()) \ndf.type = le.transform(df.type)\n\n# Implementing LE on regions\nle.fit(df.region.drop_duplicates()) \ndf.region = le.transform(df.region)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\n\ncorrMatrix = df.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It seems like only type is correlated to average prices. "},{"metadata":{},"cell_type":"markdown","source":"# Predicting future prices of avocado using Prophet\n![](https://miro.medium.com/max/964/0*tVCene42rgUTNv9Q.png)"},{"metadata":{},"cell_type":"markdown","source":"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data.\n\n- We will adjust yearly_seasonality to be True.\n  - Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, we will only need columns that contain the timestamp and the measure that we will like to predict \ncols = ['Date', 'AveragePrice']\ndf1 = df[cols]\ndf1.columns = ['ds', 'y']\n\nm = Prophet()\nm.fit(df1)\n\nfuture = m.make_future_dataframe(periods=365)\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\nfig1 = m.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Plotly to create an interactive dashboard"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.plot import plot_plotly\nimport plotly.offline as py\npy.init_notebook_mode()\n\nfig = plot_plotly(m, forecast)  # This returns a plotly Figure\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performing linear regression\n- Here, we will remove 'total bag', 'total volume' since we have decided to use small_hass, large_hass and extra_large_hass instead. \n    - 'total bag' and 'total volume' are highly correlated to small_hass, large_hass and extra_large_hass. We have decided to drop them since we will be using those. \n    - We will also be removing outliers by getting 99th percentile and keeping the data below it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting rid outliers\nq = df['AveragePrice'].quantile(0.99)\ndf = df[df['AveragePrice']< q]\n\n\ny = df['AveragePrice']\nX = df[['Small_Hass', 'Large_Hass', 'Extra_Large_Hass', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nlr = LinearRegression().fit(X_train,y_train)\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\nprint(lr.score(X_test,y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}