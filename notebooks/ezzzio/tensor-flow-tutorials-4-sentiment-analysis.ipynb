{"cells":[{"metadata":{},"cell_type":"markdown","source":"Since now we have established technques to handle data and preprocess it in a efficient manner , now we shall explore the world of NLP beginning with sentiment analysis on the twitter dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\ncd ../working/\nls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre Processing"},{"metadata":{},"cell_type":"markdown","source":"Since the classes are highly imbalanced so we create a train dataset which will contain only a random sample of equally distributed hateful and non hateful tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"hateful = train[train['label'] == 1]\nnon_hateful = train[train['label'] == 0].sample(n = 2242)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data = hateful.append(non_hateful).sample(2242*2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(final_data)\ndisplay(final_data.describe())\ndisplay(final_data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split(data,train_split = 0.8):\n    train_mask = data.apply(lambda x : (abs(hash(str(x['id']))) % 10000) < train_split * 10000 ,axis = 1)\n    eval_mask = data.apply(lambda x : (abs(hash(str(x['id']))) % 10000) >= train_split * 10000 ,axis = 1)\n    data.loc[train_mask,:].to_csv('train_data.csv')\n    data.loc[eval_mask,:].to_csv('eval_data.csv')\n    \ntrain_test_split(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Data Generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_train_dataset(train_batch_size,epochs):\n    return tf.data.experimental.make_csv_dataset(\n    ['train_data.csv'],\n    train_batch_size,\n    label_name='label',\n    num_epochs=epochs)\n    \ndef generate_eval_dataset(eval_batch_size,epochs):\n    return tf.data.experimental.make_csv_dataset(\n    ['eval_data.csv'],\n    eval_batch_size,\n    label_name='label',\n    num_epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensor flow hub"},{"metadata":{},"cell_type":"markdown","source":"In the world of NLP , we need to convert our words or sentences into embeddings. Embeddings are a numerical representations of the words or sentences. We can traun our own embeddings but these require a lot of textual data and computational resources. Tensor flow provides us with pre trained embeddings which can be downloaded from https://tfhub.dev/. We will mostly use these pre trained embeddings in our code. These embeddings have been trained on a large and varried texxtual corpus and provided as open source resources."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ[\"TFHUB_CACHE_DIR\"] = '../working/'\n\nimport tensorflow_hub as hub\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These lines of code show us how the embedding layers will convert the text into a numerical format."},{"metadata":{"trusted":true},"cell_type":"code","source":"embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\nembeddings = embed([\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"I am a sentence for which I would like to get its embedding\"])\n\nprint(embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generatinng feature columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_feature_column():\n    return [\n       hub.text_embedding_column_v2(\"tweet\",'https://tfhub.dev/google/universal-sentence-encoder/4')\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"code to demonstrate the feature columns and their preprocessing of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"example_batch = next(iter(generate_train_dataset(10,1)))[0]\n\ndef demo(feature_column):\n  feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n  print(feature_layer(example_batch).numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"demo(generate_feature_column())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.DenseFeatures(generate_feature_column()),\n    tf.keras.layers.Dense(16,activation = 'relu'),\n    tf.keras.layers.Dense(1)\n])\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01), loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics=[\"acc\"])\nmodel.fit(generate_train_dataset(10,5) ,epochs=5,verbose = 1,workers=-1,batch_size = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(generate_eval_dataset(10,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_test_dataset(eval_batch_size,epochs):\n    data = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')\n    dataset = tf.data.Dataset.from_tensor_slices(dict(data))\n    return dataset.batch(eval_batch_size).repeat(epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = model.predict(generate_test_dataset(10,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = np.array(list(map(lambda x : 1 if x > 0.8 else 0 ,output)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_temp = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')\ndata_temp['label'] = pd.Series(output)\ndata_temp.to_csv('predictions.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}