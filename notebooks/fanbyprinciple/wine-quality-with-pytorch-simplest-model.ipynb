{"cells":[{"metadata":{},"cell_type":"markdown","source":"Wine quality through hand made simple models."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets start with a linear model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"creating the simplest linear model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef model(t_u, w, b):\n    return w * t_u + b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"initialising w and b"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparams = torch.tensor([1.0, 0.0], requires_grad=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets get the tensors out of training data"},{"metadata":{},"cell_type":"markdown","source":"### getting our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt_u = train.drop('quality', axis=1)\nt_c = train.quality","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"these are our x'es - input"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`t_c` is our target"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### preprocessing"},{"metadata":{},"cell_type":"markdown","source":"`describe` function helps us to know more about our dataset. look at the mean column, we want it to be in the similar range"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that that the columns are not in the same mean range so we need to normalise the data first."},{"metadata":{},"cell_type":"markdown","source":"using mean normalisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un =(t_u-t_u.mean())/t_u.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using min max normalisation****"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un2 = (t_u-t_u.min())/(t_u.max()-t_u.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I personally like min max normalisation.\nNote that we are not normalising `t_c`. (should we?)\n "},{"metadata":{},"cell_type":"markdown","source":"### creating a validation set\n\nwe need to do this before we get into the training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un2 = torch.tensor(t_un2.values)\nt_c = torch.tensor(t_c.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = t_un2.shape[0]\nn_val = int(0.2 * n_samples)\n\nshuffled_indices = torch.randperm(n_samples)\n\ntrain_indices = shuffled_indices[:-n_val]\nval_indices = shuffled_indices[-n_val:]\n\ntrain_indices[:5], val_indices[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u_train = t_un2[train_indices]\nt_c_train = t_c[train_indices]\n\nt_u_val = t_un2[val_indices]\nt_c_val = t_c[val_indices]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### creating tensors\n\nNext step is to convert `t_un2` and `t_c` into tensors of right format to be used as input to the linear model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt_u_train = t_u_train.unsqueeze(1)\nt_c_train = t_c_train.unsqueeze(1)\n\nt_u_val = t_u_val.unsqueeze(1)\nt_c_val = t_c_val.unsqueeze(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets ensure all of the tensors are float now."},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u_train, t_u_val = t_u_train.type(torch.FloatTensor), t_u_val.type(torch.FloatTensor)\nt_c_train, t_c_val = t_c_train.type(torch.FloatTensor), t_c_val.type(torch.FloatTensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u_train.shape, t_u_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c_train.shape, t_c_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"one last thing would be that the input dimension is different from the target dimension. usually pytorch will give a broadcasting warning for this, but lets decide to use the tensor as this."},{"metadata":{},"cell_type":"markdown","source":"So our preprocessing of tensors is complete."},{"metadata":{},"cell_type":"markdown","source":"### Now comes the training loop."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD([params], lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_loop(n_epochs=3000, optimizer= optimizer, params=params,train_t_u= t_u_train, val_t_u=t_u_val, train_t_c=t_c_train, val_t_c=t_c_val, loss_fn=torch.nn.MSELoss(),model=model):\n    for epoch in range(1, n_epochs+1):\n        train_t_p = model(train_t_u, *params)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n        val_t_p = model(val_t_u, *params)\n        val_loss = loss_fn(val_t_p, val_t_c)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        if epoch <= 3 or epoch % 500 == 0:\n            print(\"Epoch: \", epoch, \" Training loss: \", train_loss.item(), \" Val loss \", val_loss.item())\n    return params\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_params = training_loop()\n%time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It does take a lot of time. But lets try using a neural network next."},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_model = torch.nn.Sequential(\n    torch.nn.Linear(11,13),\n    torch.nn.Tanh(),\n    torch.nn.Linear(13,1),\n)\n\nseq_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets look at the parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"[param.shape for param in seq_model.parameters()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check all the sizes of parameters first."},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u_train.shape, t_c_train.shape, t_u_val.shape, t_c_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_loop(n_epochs=3000, optimizer= optimizer, params=params,train_t_u= t_u_train, val_t_u=t_u_val, train_t_c=t_c_train, val_t_c=t_c_val, loss_fn=torch.nn.MSELoss(),model=model):\n    for epoch in range(1, n_epochs+1):\n#         train_t_p = model(train_t_u, *params)\n        train_t_p = model(train_t_u)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n#         val_t_p = model(val_t_u, *params)\n        val_t_p = model(val_t_u)\n        val_loss = loss_fn(val_t_p, val_t_c)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        if epoch <= 3 or epoch % 500 == 0:\n            print(\"Epoch: \", epoch, \" Training loss: \", train_loss.item(), \" Val loss \", val_loss.item())\n#     return params\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.SGD(seq_model.parameters(), lr=1e-3)\n\ntraining_loop(\n    n_epochs=5000,\n    optimizer=optimizer,\n    model = seq_model,\n    loss_fn=torch.nn.MSELoss(),\n)\n\n%time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('output', seq_model(t_u_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('answer', t_c_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets compare the two models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nt_range = torch.arange(20., 90.).unsqueeze(1)\nfig = plt.figure(dpi=600)\n\ntry: \n#     plt.xlabel(\"X\")\n#     plt.ylabel(\"y\")\n#     plt.plot(t_u_train.numpy(), t_c_train.numpy(), 'o')\n    plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n    plt.plot(t_u_train.numpy(), seq_model(0.1 * t_u_train).detach().numpy(), 'kx')\nexcept Exception as e:\n    print(e)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It will be interesting if we finda way to plot all 11 values of x. Lets make some predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(t_u_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u_val[289]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see what linear model gives"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(t_c_val[289], *optimal_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the neural net model"},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_model(t_u_val[289])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c_val[289]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}