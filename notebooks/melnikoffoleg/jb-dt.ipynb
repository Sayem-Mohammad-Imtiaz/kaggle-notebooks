{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-19T17:14:31.795298Z","iopub.execute_input":"2021-09-19T17:14:31.79557Z","iopub.status.idle":"2021-09-19T17:14:31.904382Z","shell.execute_reply.started":"2021-09-19T17:14:31.79549Z","shell.execute_reply":"2021-09-19T17:14:31.90369Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nfrom collections import defaultdict\nfrom collections import Counter\n\n\n\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score\n)\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\n\nfrom transformers import BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:31.905823Z","iopub.execute_input":"2021-09-19T17:14:31.906095Z","iopub.status.idle":"2021-09-19T17:14:45.298697Z","shell.execute_reply.started":"2021-09-19T17:14:31.906052Z","shell.execute_reply":"2021-09-19T17:14:45.297958Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:45.301163Z","iopub.execute_input":"2021-09-19T17:14:45.301433Z","iopub.status.idle":"2021-09-19T17:14:45.30516Z","shell.execute_reply.started":"2021-09-19T17:14:45.301398Z","shell.execute_reply":"2021-09-19T17:14:45.304481Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Скачаем данные","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:45.307655Z","iopub.execute_input":"2021-09-19T17:14:45.308182Z","iopub.status.idle":"2021-09-19T17:14:45.362734Z","shell.execute_reply.started":"2021-09-19T17:14:45.308145Z","shell.execute_reply":"2021-09-19T17:14:45.361923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Удалим лишние столбцы, адекватно назовем колонки и закодируем таргеты.","metadata":{}},{"cell_type":"code","source":"df = df.dropna(how='any', axis=1)\ndf.columns = ['target', 'text']\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df['target'])\n\ndf['target_encoded'] = le.transform(df['target'])\ndel df['target']\ndf.columns = ['text', 'target']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:45.36408Z","iopub.execute_input":"2021-09-19T17:14:45.364336Z","iopub.status.idle":"2021-09-19T17:14:45.396154Z","shell.execute_reply.started":"2021-09-19T17:14:45.364305Z","shell.execute_reply":"2021-09-19T17:14:45.395538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Сделаем первичный анализ данных","metadata":{}},{"cell_type":"markdown","source":"На что хочется обратить внимание в первую очередь:\n- Точно ли все сообщения на английском языке? Я прочитал описание датасета, там есть сообщения написанные жителями Сингапура, а еще часть сообщений взята с какого-то испанского сайта\n- Сколько объектов в датасете\n- Есть ли дисбаланс классов\n- Какие бывают длины сообщений","metadata":{}},{"cell_type":"markdown","source":"Попробуем понять на каком языке сообщения, для простоты посмотрим на множество используемых символов, которые не попали в английский алфавит, цифры и пунктуацию.","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport string\n\nchars = Counter()\nfor i, row in df.iterrows():\n    for char in row['text']:\n        chars[char] += 1\nprint('Символ    Частота')\nfor i in chars:\n    if not re.match(r'[a-z]|[A-Z]|[0-9]|[{} ]'.format(string.punctuation), i):\n        print(i, '        ', chars[i])","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:45.39735Z","iopub.execute_input":"2021-09-19T17:14:45.397583Z","iopub.status.idle":"2021-09-19T17:14:45.921883Z","shell.execute_reply.started":"2021-09-19T17:14:45.397553Z","shell.execute_reply":"2021-09-19T17:14:45.921209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Да, символы не преднадлежащие английскому алфавиту есть, но их очень мало, и почти все отвечают названиям валют, так что будем считать что все тексты на английском.","metadata":{}},{"cell_type":"markdown","source":"Посчитаем сколько в датасете объектов.","metadata":{}},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:45.923223Z","iopub.execute_input":"2021-09-19T17:14:45.923472Z","iopub.status.idle":"2021-09-19T17:14:45.928581Z","shell.execute_reply.started":"2021-09-19T17:14:45.92344Z","shell.execute_reply":"2021-09-19T17:14:45.927913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, имеет ли место дисбаланс классов.","metadata":{}},{"cell_type":"code","source":"target_count = df.groupby('target')['target'].agg('count').values\ntarget_count","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:45.929912Z","iopub.execute_input":"2021-09-19T17:14:45.930164Z","iopub.status.idle":"2021-09-19T17:14:45.942645Z","shell.execute_reply.started":"2021-09-19T17:14:45.930134Z","shell.execute_reply":"2021-09-19T17:14:45.941786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нормальных текстов в 6 раз больше чем спама. Это дисбаланс классов, будем это учитывать при построении модели. Для этого будем использовать стратифицированный train-test split, а в качестве метрики качества дополнительно посчитаем ROC AUC.","metadata":{}},{"cell_type":"markdown","source":"Посмотрим, какие бывают длины сообщений.","metadata":{}},{"cell_type":"code","source":"sms_lengths = df['text'].apply(lambda x: len(x.split(' ')))\nax = sms_lengths.plot.hist(bins=100, alpha=0.5, title='Распределение длин сообщений')\nax.set_xlabel('Длина сообщения')\nax.set_ylabel('Количество сообщений')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:45.944392Z","iopub.execute_input":"2021-09-19T17:14:45.944678Z","iopub.status.idle":"2021-09-19T17:14:46.389527Z","shell.execute_reply.started":"2021-09-19T17:14:45.944625Z","shell.execute_reply":"2021-09-19T17:14:46.388746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Почти все сообщения имеют длину до 50 слов, а самое большое 175. Одна из моделей которую хочется построить -  BERT, для нее можно будет привести все тексты к длине 50, меньшие тексты западить, большие обрезать.","metadata":{}},{"cell_type":"markdown","source":"# Сделаем базовую предобработку текстов","metadata":{}},{"cell_type":"markdown","source":"Немного почистим текст, удалим стоп-слова и сделаем стемминг.","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = nltk.SnowballStemmer('english')\n\ndef preprocess_text(text):\n    \n    text = str(text).lower()\n    text = re.sub('((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', 'url', text)\n    text = re.sub('[.,;-]', '', text)\n    \n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:46.392684Z","iopub.execute_input":"2021-09-19T17:14:46.392887Z","iopub.status.idle":"2021-09-19T17:14:46.403345Z","shell.execute_reply.started":"2021-09-19T17:14:46.392863Z","shell.execute_reply":"2021-09-19T17:14:46.402631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text_clean'] = df['text'].apply(preprocess_text)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:46.404637Z","iopub.execute_input":"2021-09-19T17:14:46.404967Z","iopub.status.idle":"2021-09-19T17:14:47.436746Z","shell.execute_reply.started":"2021-09-19T17:14:46.404929Z","shell.execute_reply":"2021-09-19T17:14:47.435962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Попробуем простые модели","metadata":{}},{"cell_type":"markdown","source":"Я хочу сначала попробовать простые модели, которые не учитывают порядок слов в тексте. То есть сначала надо создать векторное представление отдельного текста, а потом передать его модели. Попробуем два векторых представления текста:\n- TF-IDF\n- Сумма GloVe эмбеддингов слов с весами TF-IDF","metadata":{}},{"cell_type":"markdown","source":"Преобразование корпуса текстов в tf-idf матрицу:","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:29:05.258101Z","iopub.execute_input":"2021-09-19T12:29:05.259148Z","iopub.status.idle":"2021-09-19T12:29:05.267492Z","shell.execute_reply.started":"2021-09-19T12:29:05.259055Z","shell.execute_reply":"2021-09-19T12:29:05.265632Z"}}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef tf_idf(corpus):\n    \n    vectorizer = TfidfVectorizer(analyzer='word',\n                         ngram_range=(1, 2),\n                         min_df=0,\n                         max_df=0.5,\n                         max_features=5000)\n    \n    tfidf_matrix = vectorizer.fit_transform(corpus)\n    \n    return tfidf_matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:47.437937Z","iopub.execute_input":"2021-09-19T17:14:47.438296Z","iopub.status.idle":"2021-09-19T17:14:47.443697Z","shell.execute_reply.started":"2021-09-19T17:14:47.438258Z","shell.execute_reply":"2021-09-19T17:14:47.443039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Скачаем предобученные GloVe эмбедденги.","metadata":{}},{"cell_type":"code","source":"word2vec = dict()\n\nwith open('/kaggle/input/glovedata/glove.6B.200d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        word2vec[word] = vector_dimensions","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:14:47.444826Z","iopub.execute_input":"2021-09-19T17:14:47.445559Z","iopub.status.idle":"2021-09-19T17:15:26.425082Z","shell.execute_reply.started":"2021-09-19T17:14:47.445524Z","shell.execute_reply":"2021-09-19T17:15:26.424312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Преобразование корпуса текстов в матрицу с суммами GloVe векторов текста с весами tf-idf:","metadata":{}},{"cell_type":"code","source":"def doc2vec(corpus):\n    \n    vectorizer = TfidfVectorizer(analyzer='word',\n                         ngram_range=(1, 2),\n                         min_df=0,\n                         max_df=0.5,\n                         max_features=5000)\n    \n    tfidf_matrix = vectorizer.fit_transform(corpus)\n    tfidf_feature_names = vectorizer.get_feature_names()\n    \n    d2v = []\n    mask = [name in word2vec for name in tfidf_feature_names]\n    mask = np.array(mask)\n    tfidf_matrix_2 = tfidf_matrix[:, mask]\n    vs = []\n    for i in tfidf_feature_names:\n        if i in word2vec:\n            vs.append(word2vec[i])\n    vs = np.array(vs)\n    d2v = tfidf_matrix_2.dot(vs)\n    return d2v","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:15:26.427289Z","iopub.execute_input":"2021-09-19T17:15:26.427535Z","iopub.status.idle":"2021-09-19T17:15:26.436784Z","shell.execute_reply.started":"2021-09-19T17:15:26.427502Z","shell.execute_reply":"2021-09-19T17:15:26.436065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вывод confusion_matrix:","metadata":{}},{"cell_type":"code","source":"import seaborn as sn\nfrom sklearn.metrics import confusion_matrix\n\ndef conf_matrix(y_true, y_pred):\n    columns = ['HAM', 'SPAM']\n    confm = confusion_matrix(y_true, y_pred)\n    df_cm = pd.DataFrame(confm, index=columns, columns=columns, dtype=np.int32)\n\n    ax = sn.heatmap(df_cm, cmap='Blues', annot=True, fmt=\"d\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:15:26.438412Z","iopub.execute_input":"2021-09-19T17:15:26.438721Z","iopub.status.idle":"2021-09-19T17:15:26.450499Z","shell.execute_reply.started":"2021-09-19T17:15:26.438685Z","shell.execute_reply":"2021-09-19T17:15:26.449813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Единый пайплайн для простых моделей","metadata":{}},{"cell_type":"markdown","source":"Для всех простых моделей сделаем единый пайплайн, в котором будет сразу все:\n- Выбор способа векторизовать текст: через tf-idf, либо через GloVe с весами tf-idf\n- Отслеживание accuracy на Train и Test\n- Перебор параметров модели на CV решетке\n- Вывод confusion matrix\n- Вывод сэмплов на которых модель ошибается","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport random\nfrom sklearn.metrics import plot_confusion_matrix\n\ndef full_pipeline(model, params, doc2vec_vectorizer=False, print_conf_matrix=False, print_false_classifications=False):\n    \n    if doc2vec_vectorizer:\n        X = doc2vec(df['text_clean'])\n    else:\n        X = tf_idf(df['text_clean'])\n    y = df['target'].to_numpy()\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=239)\n\n    clf = GridSearchCV(model, params)\n    clf.fit(X_train, y_train)\n\n    print(f'Best classifier params: {clf.best_params_}')\n    print(f'TRAIN accuracy: {round(clf.best_score_,3)}, TRAIN ROC AUC: \\\n    {round(roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1]), 3)}')\n    \n    y_pred = clf.predict(X_test)\n    print(f'TEST accuracy: {round(accuracy_score(y_test, y_pred),3)}, TEST ROC AUC: \\\n    {round(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]), 3)}')\n    \n    if print_conf_matrix:\n        conf_matrix(y_test, y_pred)\n    \n    if print_false_classifications:\n        classes = ['ham', 'spam']\n        false_classification0 = []\n        false_classification1 = []\n        y_pred = clf.predict(X)\n        for i in range(len(y)):\n            if y[i] != y_pred[i]:\n                if y[i] == 0:\n                    false_classification0.append({'text': df.iloc[i]['text'], 'y_true': classes[y[i]], \\\n                                                  'y_pred': classes[y_pred[i]]})\n                else:\n                    false_classification1.append({'text': df.iloc[i]['text'], 'y_true': classes[y[i]], \\\n                                                  'y_pred': classes[y_pred[i]]})\n        random.seed(241)\n        miscl0 = random.sample(false_classification0, min(5, len(false_classification0)))\n        miscl1 = random.sample(false_classification1, min(5, len(false_classification1)))\n    \n        print('\\nExamples, when the model missclassify HAM:\\n')\n        for i in miscl0:\n            print(f\"Text: {i['text']}\\nY_true: {i['y_true']}, Y_pred: {i['y_pred']}\\n\")\n    \n        print('\\nExamples, when the model missclassify SPAM:\\n')\n        for i in miscl1:\n            print(f\"Text: {i['text']}\\nY_true: {i['y_true']}, Y_pred: {i['y_pred']}\\n\")\n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:15:26.451854Z","iopub.execute_input":"2021-09-19T17:15:26.452432Z","iopub.status.idle":"2021-09-19T17:15:26.468213Z","shell.execute_reply.started":"2021-09-19T17:15:26.452394Z","shell.execute_reply":"2021-09-19T17:15:26.467216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Обучаем и анализируем простые модели","metadata":{}},{"cell_type":"markdown","source":"Прогоним несколько простых моделей через этот пайплайн, посмотрим, что работает лучше всего.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nmodels = [LogisticRegression(), SVC(), MultinomialNB(), RandomForestClassifier(), KNeighborsClassifier()]\nparams = [{'penalty':['l2'], 'C':[0.1, 1, 10, 20], 'solver': ['liblinear']},\\\n          {'C':[0.5, 1, 2], 'kernel':['linear', 'rbf'], 'probability': [True]},\\\n          {},\\\n          {'max_depth':[5, 7, 12, 20], 'n_estimators':[20, 50, 100]},\\\n          {'n_neighbors':[2, 3, 5, 7, 10, 20]}]\n\nprint()\nfor model, param in zip(models, params):\n    \n    print('Model:', str(model), '\\n')\n    print('Training on tf-idf')\n    full_pipeline(model, param)\n    \n    if str(model) == 'MultinomialNB()':\n        print('\\n')\n        continue\n    print('\\nTraining on GloVe with tf-idf weights')\n    full_pipeline(model, param, doc2vec_vectorizer=True)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:15:26.469287Z","iopub.execute_input":"2021-09-19T17:15:26.470098Z","iopub.status.idle":"2021-09-19T17:20:38.883198Z","shell.execute_reply.started":"2021-09-19T17:15:26.470058Z","shell.execute_reply":"2021-09-19T17:20:38.881729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Мы видим что лучше всего себя показала SVM на tf-idf. Попробуем определить на основе выходов этой модели критерии, по которым она определяет является ли сообщение спамом или нет.\n\n\nПосмотрим для этого на confusion_matrix и сэмплы на которых модель ошибается. Сэмплы возьмем не только из тестовой выборки, но и из трэйна, чтобы примеров ошибок обоих классов было больше.","metadata":{}},{"cell_type":"code","source":"full_pipeline(SVC(), {'C':[2], 'kernel':['linear'], 'probability': [True]}, print_conf_matrix=True, print_false_classifications=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:20:38.884571Z","iopub.execute_input":"2021-09-19T17:20:38.884953Z","iopub.status.idle":"2021-09-19T17:20:48.368847Z","shell.execute_reply.started":"2021-09-19T17:20:38.884913Z","shell.execute_reply":"2021-09-19T17:20:48.368205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Можно сделать 2 простых наблюдения:\n- Модель не очень заточена под детекцию цифр и чисел, хотя они очень свойственны для спама\n- Модель склонна слишком часто относить в спам сообщения где много встречаются ключевые слова типа new, msg, хотя и обычные сообщения порой их содержат\n\nБлагодаря этим наблюдениям мы понимаем как еще можно улучшить модель, при желании.\n\nДвинемся дальше. Мы посмотрели на работу простых моделей, которые никак не учитывают порядок слов в тексте. Теперь попробуем более сложную модель, которая уже учитывает порядок. Я попробую BERT.","metadata":{}},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"markdown","source":"Для начала реализуем перевод сырых текстов в запаженные/обрезанные последовательности id токенов фиксированной длины, которые можно будет подать на вход в BERT","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef bert_encode(data, maximum_length) :\n    input_ids = []\n    attention_masks = []\n\n    for text in data:\n        encoded = tokenizer.encode_plus(\n            text, \n            add_special_tokens=True,\n            max_length=maximum_length,\n            pad_to_max_length=True,\n\n            return_attention_mask=True,\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:20:48.370204Z","iopub.execute_input":"2021-09-19T17:20:48.370454Z","iopub.status.idle":"2021-09-19T17:20:53.036606Z","shell.execute_reply.started":"2021-09-19T17:20:48.370421Z","shell.execute_reply":"2021-09-19T17:20:53.035908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создадим входные данные для обучения модели.","metadata":{}},{"cell_type":"code","source":"X = df['text_clean']\ny = df['target'].to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=239)\n\ntarget_text_len = 70\n\ntrain_input_ids, train_attention_masks = bert_encode(X_train,target_text_len)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:20:53.037793Z","iopub.execute_input":"2021-09-19T17:20:53.038151Z","iopub.status.idle":"2021-09-19T17:20:55.022223Z","shell.execute_reply.started":"2021-09-19T17:20:53.038116Z","shell.execute_reply":"2021-09-19T17:20:55.021464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Прикрутим к BERT-у 2 линейных слоя с дропаутом и сигмоиду на выходе, чтобы модель выдавала нам вероятность 1-ого класса","metadata":{}},{"cell_type":"code","source":"def create_model(bert_model):\n    \n    input_ids = tf.keras.Input(shape=(target_text_len,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(target_text_len,),dtype='int32')\n\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:20:55.023624Z","iopub.execute_input":"2021-09-19T17:20:55.023898Z","iopub.status.idle":"2021-09-19T17:20:55.030933Z","shell.execute_reply.started":"2021-09-19T17:20:55.023865Z","shell.execute_reply":"2021-09-19T17:20:55.02996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Загрузим предобученный BERT","metadata":{}},{"cell_type":"code","source":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:20:55.032434Z","iopub.execute_input":"2021-09-19T17:20:55.032926Z","iopub.status.idle":"2021-09-19T17:21:18.61003Z","shell.execute_reply.started":"2021-09-19T17:20:55.032889Z","shell.execute_reply":"2021-09-19T17:21:18.609306Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создадим и обучим модель","metadata":{}},{"cell_type":"code","source":"model = create_model(bert_model)\n\nhistory = model.fit(\n    [train_input_ids, train_attention_masks],\n    y_train,\n    validation_split=0.2, \n    epochs=4,\n    batch_size=10\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:21:18.611534Z","iopub.execute_input":"2021-09-19T17:21:18.612048Z","iopub.status.idle":"2021-09-19T17:24:50.487515Z","shell.execute_reply.started":"2021-09-19T17:21:18.612009Z","shell.execute_reply":"2021-09-19T17:24:50.48677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Визуализируем процесс обучения модели.","metadata":{}},{"cell_type":"code","source":"def plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)\n\nplot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:25:01.76561Z","iopub.execute_input":"2021-09-19T17:25:01.766171Z","iopub.status.idle":"2021-09-19T17:25:02.189211Z","shell.execute_reply.started":"2021-09-19T17:25:01.766133Z","shell.execute_reply":"2021-09-19T17:25:02.188501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посчитаем ответы на тестовой выборке, посчитаем accuracy, ROC AUC на тестовой выборке.\nВыведем сэмплы на которых модель ошибается (по всему датасету).\nВывыдем confusion_matrix.","metadata":{}},{"cell_type":"code","source":"test_input_ids, test_attention_masks = bert_encode(X_test,target_text_len)\n\ny_pred_proba = model.predict([test_input_ids, test_attention_masks])\ny_pred_proba = y_pred_proba.reshape((-1))\ny_pred_proba\n\ny_pred = np.zeros_like(y_pred_proba)\n\nfor i in range(len(y_pred)):\n    y_pred[i] = 1 if y_pred_proba[i] > 0.5 else 0\n\nprint(f'\\nTEST accuracy: {round(accuracy_score(y_test, y_pred),3)}, TEST ROC AUC: \\\n{round(roc_auc_score(y_test, y_pred_proba), 3)}\\n')\n\nconf_matrix(y_test, y_pred)\n\n# eval on full dataset\ntest_input_ids, test_attention_masks = bert_encode(X,target_text_len)\n\ny_pred_proba = model.predict([test_input_ids, test_attention_masks])\ny_pred_proba = y_pred_proba.reshape((-1))\ny_pred_proba\n\ny_pred = np.zeros_like(y_pred_proba, dtype=np.int32)\n\nfor i in range(len(y_pred)):\n    y_pred[i] = 1 if y_pred_proba[i] > 0.5 else 0\n\n\nclasses = ['ham', 'spam']\nfalse_classification0 = []\nfalse_classification1 = []\nfor i in range(len(y)):\n    if y[i] != y_pred[i]:\n        if y[i] == 0:\n            false_classification0.append({'text': df.iloc[i]['text'], 'y_true': classes[y[i]], \\\n                                          'y_pred': classes[y_pred[i]]})\n        else:\n            try:\n                false_classification1.append({'text': df.iloc[i]['text'], 'y_true': classes[y[i]], \\\n                                              'y_pred': classes[y_pred[i]]})\n            except Exception as e:\n                print(df.iloc[i]['text'], y[i], y_pred[i])\n                raise e\n\nmiscl0 = random.sample(false_classification0, min(5, len(false_classification0)))\nmiscl1 = random.sample(false_classification1, min(5, len(false_classification1)))\n\nprint('\\nExamples, when the model missclassify HAM:\\n')\nfor i in miscl0:\n    print(f\"Text: {i['text']}\\nY_true: {i['y_true']}, Y_pred: {i['y_pred']}\\n\")\n\nprint('\\nExamples, when the model missclassify SPAM:\\n')\nfor i in miscl1:\n    print(f\"Text: {i['text']}\\nY_true: {i['y_true']}, Y_pred: {i['y_pred']}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:53:16.838579Z","iopub.execute_input":"2021-09-19T17:53:16.838849Z","iopub.status.idle":"2021-09-19T17:53:40.127336Z","shell.execute_reply.started":"2021-09-19T17:53:16.83882Z","shell.execute_reply":"2021-09-19T17:53:40.126519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Тут уже видно что примеры на которых модель ошибается не столь тривиальны даже для человека. Например, \"Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?\" - это отнесено к спаму, хотя понятно что это мог написать и твой знакомый. В общем модель ошибается на реально сложных по смыслу кейсах, которые находятся близко к границе разделения на классы. Особого пространтсва для улучшения модели уже нет.","metadata":{}},{"cell_type":"markdown","source":"# Выводы\n\nТак как в сообщениях со спамом часто используются особые слова по типу url-адресов и телефонных номеров, простые модели, которые никак не учитывают порядок слов в тексте, работают довольно хорошо. Но лучший результат конечно же дает тяжелая модель, такая как BERT, которая учитывает порядок слов и лучше определяет контекст в сложных случаях.","metadata":{}}]}