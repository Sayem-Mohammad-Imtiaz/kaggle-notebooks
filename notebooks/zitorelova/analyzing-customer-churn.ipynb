{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analyzing Customer Churn\n\n## Introduction\n\nThe [data](https://www.kaggle.com/shubh0799/churn-modelling) we are using for this analysis consists of customers subscribed to services at a company. Our goal is to explore and solve the problem of predicting **customer churn**. The dataset contains features like Age, Tenure, Salary, and Credit Score; features which could potentially give insight as to why a customer end their subscription or stop buying products from a company.\n<br/><br/>The workflow will cover EDA, where we explore the features of the dataset and try to determine which features are correlated with customer churn. We will then do feature engineering and selection with the intention of creating a predictive model that is able to predict whether a given customer will churn.\n\n### References\n * [numpy API reference](https://numpy.org/doc/stable/reference/index.html)\n * [pandas API reference](https://pandas.pydata.org/docs/reference/index.html#api)\n * [scikit-learn documentation](https://scikit-learn.org/stable/)\n * [xgboost parameter documentation](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster)\n * [original SMOTE paper](https://arxiv.org/abs/1106.1813)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Let's load in the dataset then check the head\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\ndf = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv')\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the summary statistics\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`RowNumber` is just a number that identifies each row. We can drop it and use the dataframe's index instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop RowNumber\ndf.drop('RowNumber', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first check for null values in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values\ndf.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now look at the `CustomerId` column in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check number of unique values for CustomerId\ndf['CustomerId'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can assume from `CustomerId` that every entry in the dataset is a unique individual. Since `CustomerId` is unique, it does not give us any information. We can drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop CustomerId\ndf.drop('CustomerId', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the data once again.\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the data again, we see the column `Surname`. Thinking intutively, any correlation of this column with our target variable would be completely coincidental. These relationships would be spurious since we can never really predict if a customer will churn based on their name. Thus, we will drop this column as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Surname\ndf.drop('Surname', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"Let's create a correlation heatmap to see linear relationships between variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df.corr().round(2), annot=True, ax=ax)\nhighlight_color = 'blue'\nax.add_patch(Rectangle((0, 8), 9, 1, fill=False, edgecolor=highlight_color, lw=3))\nax.add_patch(Rectangle((8, 0), 1, 9, fill=False, edgecolor=highlight_color, lw=3))\nax.set_title('Feature Correlations')\n\nfor axis in [ax.get_xticklabels(), ax.get_yticklabels()]:\n    label = [i for i in axis if i.get_text() == 'Exited']\n    [(l.set_weight('bold'), l.set_size(25), l.set_color(highlight_color)) for l in label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the correlation heatmap, we can see that `Age` has the highest correlation with our target variable `Exited`. This makes sense given that the older you are, the more likely you are to churn as a customer. Let's take a closer look at some of the variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_bar_graph(values, title, xlab, ylab='Proportion of Exited', rotate_x=False):\n    temp_df = pd.DataFrame({'Feature': values, 'Exited': df['Exited']})\n    gb_obj = temp_df.groupby('Feature')['Exited'].mean()\n    plt.bar(gb_obj.index.astype(str), gb_obj.values, width=0.5)\n    plt.title(title, fontsize=15)\n    plt.xlabel(xlab, fontsize=15)\n    plt.ylabel(ylab, fontsize=15)\n    if rotate_x:\n        plt.xticks(rotation=45)\n        \n# Bin Age and plot a bar graph\nbin_age = pd.qcut(df['Age'].values, q=5).astype(str)\nfeature_bar_graph(bin_age, 'Age Analysis', 'Binned Age', rotate_x=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see from the bar graph that `Age` positively correlates with customer churn. We have binned the data and can observe that the bins with higher `Age` have a larger proportion of customers churning."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_bar_graph(df['Tenure'].values, 'Tenure Analysis', 'Tenure')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Tenure` doesn't seem to show any trends when it comes to churn. The proportions are roughly the same for each of the values."},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_cc = pd.qcut(df['CreditScore'].values, q=5).astype(str)\nfeature_bar_graph(bin_cc, 'Credit Score Analysis', 'Binned Credit Score', rotate_x=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see the same thing when we look at `CreditScore`. There is no notable difference between any of the values."},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_salary = pd.qcut(df['EstimatedSalary'].values, q=5).astype(str)\nfeature_bar_graph(bin_salary, 'Salary Analysis', 'Estimated Salary', rotate_x=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at `EstimatedSalary`, the trend continues and there are no differences in the values."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_bar_graph(df['IsActiveMember'].values, 'Active Member Analysis', 'Is Active Member')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at `IsActiveMember` we can see that being inactive is correlated to customer churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_bar_graph(df['NumOfProducts'].values, 'Number of Products Analysis', 'Number of Products')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see a notable trend in `NumOfProducts` as well. It is not linear but when a customer has >2 products, churn seems to increase by a large amount."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will save a copy of our dataframe then one-hot encode it.\ndf_old = df.copy()\ndf = pd.get_dummies(df, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate into features and target variable\nfeatures = df.drop('Exited', axis=1)\ntarget = df['Exited']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training Run 1"},{"metadata":{},"cell_type":"markdown","source":"We will first do a baseline test with all features and no hyperparameter tuning to see which models perform best on our dataset. The only preprocessing we will do is feature scaling particularly to help our non tree-based models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import all libraries that we will need and split the data into training and testing sets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nimport os\nrs = {'random_state': 42}\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, train_size=0.6, **rs)\nX_val, X_test, y_val, y_test, = train_test_split(X_test, y_test, train_size=0.5, **rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use a variety of models and see which one performs the best.<br/>\nThe models include:\n* Logistic Regression\n* Naive Bayes\n* k-nearest neighbors\n* SVM\n* Neural Network\n* Decision Tree\n* Extra Trees\n* Random Forest\n* XGBoost\n\nFor this training run we will use 3-fold cross validation using `cross_val_score`. All tree based models will have a random seed to ensure reproducibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_models(X_train, X_val, X_test, y_train, y_val, y_test):\n    log_reg = LogisticRegression(**rs)\n    nb = BernoulliNB()\n    knn = KNeighborsClassifier()\n    svm = SVC(**rs)\n    mlp = MLPClassifier(max_iter=5000, **rs)\n    dt = DecisionTreeClassifier(**rs)\n    et = ExtraTreesClassifier(**rs)\n    rf = RandomForestClassifier(**rs)\n    xgb = XGBClassifier(**rs, verbosity=0)\n    scorer = make_scorer(f1_score)\n\n    clfs = [('Logistic Regression', log_reg), ('Naive Bayes', nb),\n            ('K-Nearest Neighbors', knn), ('SVM', svm), \n            ('MLP', mlp), ('Decision Tree', dt), ('Extra Trees', et), \n            ('Random Forest', rf), ('XGBoost', xgb)]\n    pipelines = []\n    scores_df = pd.DataFrame(columns=['model', 'val_score', 'test_score'])\n    test_scores = []\n    for clf_name, clf in clfs:\n        pipeline = Pipeline(steps=[\n            ('scaler', StandardScaler()),\n            ('classifier', clf)])\n        pipeline.fit(X_train, y_train)\n        val_score = cross_val_score(pipeline, X_val, y_val, scoring=scorer, cv=3).mean()\n        print(f'{clf_name}\\n{\"-\" * 30}\\nModel Score Validation: {val_score:.4f}')\n        test_score = f1_score(y_test, pipeline.predict(X_test))\n        print(f'Model Score Testing: {test_score:.4f}\\n\\n')\n        pipelines.append(pipeline)\n        scores_df = scores_df.append({'model': clf_name, \n                                      'val_score': val_score, \n                                      'test_score': test_score}, ignore_index=True)\n    return pipelines, scores_df\n\npipelines1, scores1 = train_models(X_train, X_val, X_test, y_train, y_val, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores1.sort_values('test_score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MLP model performs best on the test set in our initial training run. Let's analyze its performance with a classification report and confusion matrix on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create classification report\nfrom sklearn.metrics import classification_report, confusion_matrix\nmodel = pipelines1[4]\nprint(model['classifier'])\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is very good at predicting negative examples as seen from the `f1-score` for class 0. However, it struggles with positive examples. This can be seen from the low recall score that it gets for class 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create confusion matrix\ncfm = confusion_matrix(y_test, preds)\nprint(cfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create confusion matrix with seaborn\ndef create_confusion_matrix(y_true, y_pred):\n    cfm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots(figsize=(7,7))\n    sns.heatmap(cfm, annot=True, annot_kws={\"size\": 15}, ax=ax,\n                cbar=False, square=True, cmap='Blues', fmt='d')\n    sns.set(font_scale=1.5)\n    plt.xlabel('Predicted', fontsize=15)\n    plt.ylabel('Actual', fontsize=15)\n    ax.set_xticklabels(np.unique(y_pred))\n    ax.set_yticklabels(np.unique(y_pred))\n    plt.title('Confusion Matrix\\nChurn Data', fontsize=18)\n    \ncreate_confusion_matrix(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the confusion matrix reinforces our previous analysis of the model struggling to correctly classify positive examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check class distribution\ny_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training Run 2"},{"metadata":{},"cell_type":"markdown","source":"When looking at the class distribution of our training set, we see a significant skew where most of our data consists of negative (0) examples. To overcome this problem, we could either undesample and majority class or oversample the minority. Undersampling will leave us with less training data so we will generate synthetic examples of our minority class. There are many methods to do this but we will be using ADASYN (Adaptive Synthetic) sampling for this purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate synthetic examples of minority class\nfrom imblearn.over_sampling import ADASYN\n\nadasyn = ADASYN(**rs)\nX_train, y_train = adasyn.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the generated training data, we see that our class distribution is now pretty much even. Let's try another round of training and see the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training with synthetic dataset\npipelines2, scores2 = train_models(X_train, X_val, X_test, y_train, y_val, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores2.sort_values('test_score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The XGBoost model performs the best in our second training run. Let's dig deeper and look at the feature importance of the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model classification report\nmodel = pipelines2[-1]\nprint(model['classifier'])\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though our f1_score remains comparable to our first training run, we notice a large increase in sensitivity when adding synthetic samples. This is beneficial to us since we would like to be able to detect when a customer will churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"create_confusion_matrix(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A look at the confusion matrix verifies are findings. Model sensitivity has increased although specificity has taken a small hit. This is an acceptable compromise. Let's look at feature importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = pd.DataFrame({'feature': features.columns, 'importance': model['classifier'].feature_importances_})\nfeat_imp.sort_values('importance', ascending=False, inplace=True)\nfeat_imp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In line with what we saw in our earlier analysis, `IsActiveMember` and `NumOfProducts` play a large role in determining whether an employee will churn or not."},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Let's look at some feature selection methods now. First we will try Recursive Feature Elimination with Logistic Regression. We will test the chosen features with our XGBoost model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# RFE with Logistic Regression\nfrom sklearn.feature_selection import SelectFromModel, RFE\n\nparams = [StandardScaler(), XGBClassifier(**rs, verbosity=0), X_train, X_val, X_test, y_train, y_val, y_test]\n\ndef create_pipeline(feature_selection, scaler, classifier, X_train, X_val, X_test, y_train, y_val, y_test):\n    pipeline = Pipeline(steps=[('feature_selection', feature_selection(LogisticRegression(max_iter=5e3))),\n                        ('scaler', scaler),\n                        ('classifier', classifier)])\n    scorer = make_scorer(f1_score)\n    pipeline.fit(X_train, y_train)\n    chosen_features = X_train.iloc[:, pipeline['feature_selection'].get_support(indices=True)]\n    print(f'Feature Selection Method {feature_selection.__name__} selected {len(chosen_features.columns)} features')\n    print(f'Model Score Validation: {cross_val_score(pipeline, X_val, y_val, scoring=scorer, cv=3).mean():.4f}')\n    print(f'Model Score Testing: {f1_score(y_test, pipeline.predict(X_test)):.4f}')\n    return pipeline, chosen_features\n\nrfe_pipe, rfe_feats = create_pipeline(RFE, *params)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RFE chose 5 features. Our model's score did not improve. Let's try using SelectFromModel."},{"metadata":{"trusted":true},"cell_type":"code","source":"# SelectFromModel with Logistic Regression\nsfm_pipe, sfm_feats = create_pipeline(SelectFromModel, *params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check colums\nsfm_feats.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SelectFromModel chose 1 feature. Our model's score did not improve. What is interesting to note though is we are able to achieve a respectable score with just `Age`."},{"metadata":{},"cell_type":"markdown","source":"We will try a different method of feature selection using our model. By getting the cumulative percentage of feature importance, we can set a percentage cutoff after which we will drop all other features. I will set it to 0.9."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform feature selection using feature importance\nfeat_imp['CumPerc'] = np.cumsum(model['classifier'].feature_importances_)/sum(model['classifier'].feature_importances_)\ncutoff = 0.9\nnew_feats = feat_imp[feat_imp['CumPerc'] < cutoff]\nnew_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new pipeline using new features\nX_train, X_val, X_test = X_train[new_feats['feature']], X_val[new_feats['feature']], X_test[new_feats['feature']]\n\nfi_pipe = Pipeline(steps=[('scaler', StandardScaler()),\n                          ('classifier', XGBClassifier(**rs, verbosity=0))])\nscorer = make_scorer(f1_score)\nfi_pipe.fit(X_train, y_train)\nprint(f'Model Score Validation: {cross_val_score(fi_pipe, X_val, y_val, scoring=scorer, cv=3).mean():.4f}')\nprint(f'Model Score Testing: {f1_score(y_test, fi_pipe.predict(X_test)):.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using feature importance has improved our f1 score by a respectable margin. We will use this feature set moving forward. Let's check the classification report."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, fi_pipe.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sensitivity has increased yet again. Our model is getting better at detecting churn among customers."},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"Let's create a `weight` to try to put more weight on positive classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create class_weight dict and pass this as an argument when creating the classifier\n\nweight = (y_val == 0).sum() / (y_val == 1).sum()\ncw_pipe = Pipeline(steps=[('scaler', StandardScaler()),\n                          ('classifier', XGBClassifier(scale_pos_weight=weight, **rs, verbosity=0))])\ncw_pipe.fit(X_train, y_train)\nprint(f'Model Score Validation: {cross_val_score(cw_pipe, X_val, y_val, scoring=scorer, cv=3).mean():.4f}')\nprint(f'Model Score Testing: {f1_score(y_test, cw_pipe.predict(X_test)):.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weighting the positive class seems to decrease the model's overall f1_score. Let's check the classification report."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, cw_pipe.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When adding the `weight` parameter, we are able to make improvements to sensitivity, but specificity takes a large hit. It is debatable whether we would keep a change like this in our model. Using it would certainly allow us to detect more churn among customers but this would be at the cost of an increase in type 1 errors (false positives). I will opt to leave this out moving forward."},{"metadata":{},"cell_type":"markdown","source":"## Model Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = fi_pipe.predict(X_test)\nprint(classification_report(y_test, test_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create confusion matrix\ncfm = confusion_matrix(y_test, test_preds)\nprint(cfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create confusion matrix with seaborn\ncreate_confusion_matrix(y_test, test_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's create an ROC plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot roc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(y_test, test_preds)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, fi_pipe.predict_proba(X_test)[:,1])\n\nfig = plt.figure()\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.15)\nplt.plot(false_positive_rate, true_positive_rate, label=f'XGBoost (area = {logit_roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], '--', color='grey')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic plot')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Out of Sample Prediction"},{"metadata":{},"cell_type":"markdown","source":"With our final model we are able to address some of the problems regarding sensitivity and arrive at a respectable f1_score. Let's make predictions on out-of-sample data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create out-of-sample data\noos = pd.DataFrame({'RowNumber': [10000, 10001, 10002, 10003, 10004], \n                    'CustomerId': [15849068, 15784210, 15690576, 15984739, 15893045],\n                   'Surname': ['Garcia', 'Miller', 'Rodriguez', 'Lee', 'Hill'],\n                   'CreditScore': [145, 566, 392, 669, 478],\n                   'Geography': ['France', 'Germany', 'Spain', 'France', 'France'],\n                    'Gender': ['Male', 'Female', 'Male', 'Male', 'Female'],\n                   'Age': [46, 32, 25, 66, 47],\n                   'Tenure': [1, 5, 4, 4, 8],\n                   'Balance': [14569.43, 0.00, 129804.44, 1589.04, 0.00],\n                   'NumOfProducts': [3, 3, 1, 3, 2],\n                   'HasCrCard': [1, 1, 1, 1, 1],\n                   'IsActiveMember': [0, 1, 1, 0, 1],\n                   'EstimatedSalary': [164032.87, 56890.44, 98349.51, 57098.64, 122548.65]})\n\noos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's write a function to preprocess the dataset and make sure it matches with our original\ndef preprocess_data(df):\n    df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n    df = pd.get_dummies(df, drop_first=True)\n    df = df[new_feats['feature']]\n    return df\n\noos_processed = preprocess_data(oos)\nassert all(oos_processed.columns == X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict and append to the dataframe\noos['predictions'] = fi_pipe.predict(oos_processed)\noos","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}