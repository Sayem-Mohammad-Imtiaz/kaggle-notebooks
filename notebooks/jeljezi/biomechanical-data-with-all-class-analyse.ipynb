{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nAnalyse the Biomechanical Data with all Model Classifiers!\n\n<font color='blue'>\nContent:\n\n1. [Load and check Data](#1)<br>\n1. [Variable Analyse](#2)\n   * [Numerical Variable and Categorical Variable](#3)\n1. [Train Test Split](#4)    \n1. [Modeling together with Backward Elimination](#5)\n    * [Logistic Regression Model](#6)\n    * [KNN Classification Model](#7)\n    * [Support Vector Machine Model](#8)\n    * [Naive Bayes Classification Model](#9)\n    * [Decision Tree Model](#10)\n    * [Random Forest Model](#11)\n1. [Analyse the best model with accuracy and confusion](#12)\n    * [Analyse accuracy and confusion of Models with Backwards Elimination](#13)\n1. [Hyperparameter Tunning,Cross Validation,GridCross](#14)\n    * [Ensemble Model](#15)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n# Load and Check Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\n\n# for others analyse save we the original data\ndata_new = data.copy()\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data infos\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2'></a>\n# Variable Analyse"},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a>\n## Numerical Variable and Categorical Variable"},{"metadata":{},"cell_type":"markdown","source":"Dtypes: <br>\n\n* <span style=\"color:deepskyblue\"> Float64(2):</span> pelvic_incidence, pelvic_tilt numeric,lumbar_lordosis_angle,sacral_slope,pelvic_radius,degree_spondylolisthesis  <br>\n* <span style=\"color:deepskyblue\"> object(5):</span>class  <br>"},{"metadata":{},"cell_type":"markdown","source":"* <span style=\"color:deepskyblue\">Categorical Variable: </span>class\n* <span style=\"color:deepskyblue\">Numerical Variable: </span>pelvic_incidence,pelvic_tilt,numeric,lumbar_lordosis_angle,sacral_slope,pelvic_radius,degree_spondylolisthesis \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data features scatter plot\nsns.set(style=\"ticks\")\nsns.pairplot(data,hue='class',diag_kind='hist',palette='husl',markers='D');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Staitistical infos of Data\ndata.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature counts of class\ndata['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of class data-value counts\nsns.countplot(data['class']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NaN's feature of Data\ndata.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classification of class (make binary features of 0,1)\ndata['class'] = [1 if i == 'Abnormal'else 0 for i in data['class']]\n\n# labels or dependet features of Data\ny = data[['class']]\n\n# independet features of Data\nx_data = data.drop(['class'],axis = 1)\n\n# and normalization of feature\nx_norm = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Second Method for binary features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modul import for encode\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 'Abnormal' and 'Normal' features\nbinary_values = data_new[['class']]\n\n# model\nohe = OneHotEncoder()\n\nbinary = ohe.fit_transform(binary_values).toarray()\n\nbinary[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlations between features of data\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization data correlation\nsns.heatmap(data.corr(),annot = True,fmt='.2f',linewidths=0.5,linecolor='b')\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* class have with pelvic_radius negativ correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now Correlation analyse with p-Value\n\n# dependet feature\ny = data[['class']]\n\n# independet features of Data\nx_data = data.drop(['class'],axis = 1)\n\nimport statsmodels.api as sm\n\n# model\nanalyse = sm.OLS(y,x_data).fit()\n\nanalyse.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Features who have bigger p-Value is : pelvic_incidence,pelvic_tilt numeric,sacral_slope !"},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a>\n# Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n# Modeling together with Backward Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6'></a>\n## Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modul imort\nfrom sklearn.linear_model import LogisticRegression\n\n# model\nlg = LogisticRegression()\n\n# fit\nlg.fit(x_train,y_train)\n\n# predicts\nlog_predicts = lg.predict(x_test)\n\n# accuracy with Logistic Regression Model\naccuracy_log = lg.score(x_test,y_test)\n\n# confusion metrics of Logistic Regression Model\nfrom sklearn.metrics import confusion_matrix\n\ncm_log = confusion_matrix(y_test,log_predicts)\n\n# correlation the predicts values with x_test of data\nlg_analyse = sm.OLS(log_predicts,x_test).fit()\n\nlg_analyse.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*     **Logistic Regression Model with Backward Elimination !**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.484) and degree_spondylolisthesis(0.884)\n\nx_train_b = x_train.drop(['lumbar_lordosis_angle','degree_spondylolisthesis'],axis = 1)\nx_test_b = x_test.drop(['lumbar_lordosis_angle','degree_spondylolisthesis'],axis = 1)\n\n# model\nlg_b = LogisticRegression()\n\n# fit\nlg_b.fit(x_train_b,y_train)\n\n# predicts\nlog_predicts_b = lg_b.predict(x_test_b)\n\n# accuracy with Logistic Regression Model\naccuracy_log_b = lg_b.score(x_test_b,y_test)\n\n# confusion metrics of Logistic Regression Model\nfrom sklearn.metrics import confusion_matrix\n\ncm_log_b = confusion_matrix(y_test,log_predicts_b)\n\n# correlation the predicts values with x_test of data\nlg_analyse_b = sm.OLS(log_predicts_b,x_test_b).fit()\n\nlg_analyse_b.summary()\n# we have now bigger R-squared ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **C and Penalty with Grid Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":" # Grid Cross Validation\n\nfrom sklearn.model_selection import GridSearchCV\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  # l1 = lasso ve l2 = ridge\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv = 10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",logreg_cv.best_params_)\nprint(\"accuracy: \",logreg_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7'></a>\n## KNN Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modul import\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# find best k\nk_neighbors = np.arange(1,10)\nscore_list = []\nfor i in k_neighbors:\n    \n    knn_i = KNeighborsClassifier(n_neighbors = i)\n    knn_i.fit(x_train,y_train)\n    score_list.append(knn_i.score(x_test,y_test))\n    \nplt.plot(k_neighbors,score_list)\nplt.xlabel('k_number')\nplt.ylabel('score of k');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have the best k = 6\nknn = KNeighborsClassifier(n_neighbors = 6)\n\nknn.fit(x_train,y_train)\n\nknn_predicts = knn.predict(x_test)\n\naccuracy_knn = knn.score(x_test,y_test)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_knn = confusion_matrix(y_test,knn_predicts)\n\n# correlation the predicts values with x_test of data\nknn_analyse = sm.OLS(knn_predicts,x_test).fit()\n\nknn_analyse.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **KNN Model with Backward Elimination** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Backward elimination of features bigger than p-value (0.05)\n# Now elimination of features : lumbar_lordosis_angle(0.088)\n\nx_train_b1 = x_train.drop(['lumbar_lordosis_angle'],axis=1)\nx_test_b1 = x_test.drop(['lumbar_lordosis_angle'],axis=1)\n\n# we have the best k = 6\nknn_b = KNeighborsClassifier(n_neighbors = 6)\n\nknn_b.fit(x_train_b1,y_train)\n\nknn_predicts_b = knn_b.predict(x_test_b1)\n\naccuracy_knn_b = knn_b.score(x_test_b1,y_test)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_knn_b = confusion_matrix(y_test,knn_predicts_b)\n\n# correlation the predicts values with x_test of data\nknn_analyse_b = sm.OLS(knn_predicts_b,x_test_b).fit()\n\nknn_analyse_b.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **K-Value with Grid Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Cross Validation\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn= KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv = 10)  # GridSearchCV\nknn_cv.fit(x_train,y_train)\n\n#%% print hyperparameter KNN \nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned parametreye gore en iyi accuracy (best score): \",knn_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **"},{"metadata":{},"cell_type":"markdown","source":"<a id='8'></a>\n## Support Vector Machine Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modul import\nfrom sklearn.svm import SVC\n\n# model\nsvm = SVC(random_state = 42)\n\n# fit\nsvm.fit(x_train,y_train)\n\n# predicts\nsvm_predicts = svm.predict(x_test)\n\n# accuracy\naccuracy_svm = svm.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_svm = confusion_matrix(y_test,svm_predicts)\n\n# correlation the predicts values with x_test of data\nsvm_analyse = sm.OLS(svm_predicts,x_test).fit()\n\nsvm_analyse.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Support Vector Machine with Bacward Elimination**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.334)\n\nx_train_b2 = x_train.drop(['lumbar_lordosis_angle'],axis=1)\nx_test_b2 = x_test.drop(['lumbar_lordosis_angle'],axis=1)\n\n# modul import\nfrom sklearn.svm import SVC\n\n# model\nsvm_b = SVC(random_state = 42)\n\n# fit\nsvm_b.fit(x_train_b2,y_train)\n\n# predicts\nsvm_predicts_b = svm_b.predict(x_test_b2)\n\n# accuracy\naccuracy_svm_b = svm_b.score(x_test_b2,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_svm_b = confusion_matrix(y_test,svm_predicts_b)\n\n# correlation the predicts values with x_test of data\nsvm_analyse_b = sm.OLS(svm_predicts_b,x_test_b).fit()\n\nsvm_analyse_b.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **K-Fold Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nsvm = SVC(random_state = 42)\n\n# fit\nsvm.fit(x_train,y_train)\n\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = svm, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='9'></a>\n## Naive Bayes Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modul import\nfrom sklearn.naive_bayes import GaussianNB\n\n# model\nnb = GaussianNB()\n\n# fit\nnb.fit(x_train,y_train)\n\n# predicts\nnb_predicts = nb.predict(x_test)\n\n# accuracy\naccuracy_nb = nb.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_nb = confusion_matrix(y_test,nb_predicts)\n\n# correlation the predicts values with x_test of data\nnb_analyse = sm.OLS(nb_predicts,x_test).fit()\n\nnb_analyse.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Naive Bayes Classification Model with Backward Elimination**\n* Now we have not features than p_Value is bigger than 0.05 !"},{"metadata":{},"cell_type":"markdown","source":"* **K-Fold Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nnb = GaussianNB()\n\n# fit\nnb.fit(x_train,y_train)\n\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = nb, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='10'></a>\n## Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# modul import\nfrom sklearn.tree import DecisionTreeClassifier\n\n# model\nd_tree = DecisionTreeClassifier()\n\n# fit\nd_tree.fit(x_train,y_train)\n\n# predicts\nd_tree_predicts = d_tree.predict(x_test)\n\n# accuracy\naccuracy_d_tree = d_tree.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_tree = confusion_matrix(y_test,d_tree_predicts)\n\n# correlation the predicts values with x_test of data\nd_tree_analyse = sm.OLS(d_tree_predicts,x_test).fit()\n\nd_tree_analyse.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Decision Tree Model with Backward Elimination**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.992),pelvic_radius(0.384)\n\nx_train_b3 = x_train.drop(['lumbar_lordosis_angle','pelvic_radius'],axis=1)\nx_test_b3 = x_test.drop(['lumbar_lordosis_angle','pelvic_radius'],axis=1)\n\n# modul import\nfrom sklearn.tree import DecisionTreeClassifier\n\n# model\nd_tree_b = DecisionTreeClassifier()\n\n# fit\nd_tree_b.fit(x_train_b3,y_train)\n\n# predicts\nd_tree_predicts_b = d_tree_b.predict(x_test_b3)\n\n# accuracy\naccuracy_d_tree_b = d_tree_b.score(x_test_b3,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_tree_b = confusion_matrix(y_test,d_tree_predicts_b)\n\n# correlation the predicts values with x_test of data\nd_tree_analyse_b = sm.OLS(d_tree_predicts_b,x_test_b3).fit()\n\nd_tree_analyse_b.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **K-Fold Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nd_tree = DecisionTreeClassifier()\n\n# fit\nd_tree.fit(x_train,y_train)\n\nfrom sklearn.model_selection import cross_val_score\n\n\naccuracies = cross_val_score(estimator = d_tree, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='11'></a>\n## Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modul import\nfrom sklearn.ensemble import RandomForestClassifier\n\n# model\nr_forest = RandomForestClassifier(n_estimators = 100,random_state = 42)\n\n# fit\nr_forest.fit(x_train,y_train)\n\n# predicts\nr_forest_predicts = r_forest.predict(x_test)\n\n# accuracy\naccuracy_r_forest = r_forest.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_forest = confusion_matrix(y_test,r_forest_predicts)\n\n# correlation the predicts values with x_test of data\nr_forest_analyse = sm.OLS(r_forest_predicts,x_test).fit()\n\nr_forest_analyse.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Random Forest with Backward Elimination**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.693)\n\nx_train_b4 = x_train.drop(['lumbar_lordosis_angle'],axis=1)\nx_test_b4 = x_test.drop(['lumbar_lordosis_angle'],axis=1)\n\n# modul import\nfrom sklearn.ensemble import RandomForestClassifier\n\n# model\nr_forest_b = RandomForestClassifier(n_estimators = 100,random_state = 42)\n\n# fit\nr_forest_b.fit(x_train_b4,y_train)\n\n# predicts\nr_forest_predicts_b = r_forest_b.predict(x_test_b4)\n\n# accuracy\naccuracy_r_forest_b = r_forest_b.score(x_test_b4,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_forest_b = confusion_matrix(y_test,r_forest_predicts_b)\n\n# correlation the predicts values with x_test of data\nr_forest_analyse_b = sm.OLS(r_forest_predicts_b,x_test_b).fit()\n\nr_forest_analyse_b.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **N-estimators value with Grid Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = {\"n_estimators\":np.arange(1,50)}\nrandom_f= RandomForestClassifier()\n\nrandom_cv = GridSearchCV(random_f, grid, cv = 10)  # GridSearchCV\nrandom_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameter K: \",random_cv.best_params_)\nprint(\"tuned parametreye gore en iyi accuracy (best score): \",random_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='12'></a>\n# Analyse the best model with accuracy and confusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracys = ({'accuracy_log':accuracy_log,'accuracy_knn':accuracy_knn,\n             'accuracy_svm':accuracy_svm,'accuracy_nb':accuracy_nb,\n             'accuracy_d_tree':accuracy_d_tree,'accuracy_r_forest':accuracy_r_forest})\n\n#confusions = [cm_log,cm_knn,cm_svm,cm_nb,cm_tree,cm_forest]\n\naccuracy = pd.DataFrame.from_dict(accuracys,orient='index')\n\nplt.figure(figsize = (10,6))\n\nplt.plot(accuracy,color='r')\nplt.scatter(accuracy.index,accuracy.values);\n             ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From Graph seems that Best Accuracy have Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusions of Models\nconfusions = {'cm_log':cm_log,'cm_knn':cm_knn,'cm_svm':cm_svm,'cm_nb':cm_nb,'cm_tree':cm_tree,'cm_forest':cm_forest}\n\nfor i,j in confusions.items():\n\n    sns.heatmap(j,annot=True,fmt='.0f',linewidths=0.5,linecolor='r')\n    plt.title('Confusion of {}'.format(i))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion performans of models\n\n# number of normal features is 21\nn_normal = 21\n\n# number of abnormal is 57\nn_abnormal = 57\n\n# True predicts of normal is true/21 and for abnormal true/57\n# performans is equal : true/21 + true/57\n\nconfus_performans = []\n\nfor i ,j  in confusions.items():\n    \n    performans_normal = j[0][0] / n_normal\n    \n    performans_abnormal = j[1][1] / n_abnormal\n    \n    total_performans = performans_normal + performans_abnormal\n    \n    confus_performans.append((i,total_performans))\n    \nconfus_performans\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best cunfusion performans\n\nconf = pd.DataFrame(confus_performans)\n\nconf.set_index(0)\n\nplt.plot(conf[0],conf[1],color = 'r')\nplt.scatter(conf[0],conf[1]);\nplt.title('True predicts of Normal vs Abnormal')\nplt.ylabel('Sum of True predicts');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* Of graph seems that max is by cm_nb - naive bayes models"},{"metadata":{},"cell_type":"markdown","source":"<a id='13'></a>\n## Analyse accuracy and confusion of Models with Backwards Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracys_b = ({'accuracy_log_b':accuracy_log_b,'accuracy_knn_b':accuracy_knn_b,\n             'accuracy_svm_b':accuracy_svm_b,'accuracy_nb':accuracy_nb,\n             'accuracy_d_tree_b':accuracy_d_tree_b,'accuracy_r_forest_b':accuracy_r_forest_b})\n\naccuracy_b = pd.DataFrame.from_dict(accuracys_b,orient='index')\n\nplt.figure(figsize = (16,6))\n\nplt.plot(accuracy_b,color='g')\nplt.scatter(accuracy_b.index,accuracy_b.values);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusions of Models with Backward Elimination\nconfusions_b = {'cm_log_b':cm_log_b,'cm_knn_b':cm_knn_b,'cm_svm_b':cm_svm_b,'cm_nb':cm_nb,'cm_tree_b':cm_tree_b,'cm_forest_b':cm_forest_b}\n\nfor i,j in confusions_b.items():\n\n    sns.heatmap(j,annot=True,fmt='.0f',linewidths=0.5,linecolor='r',cmap=\"Greens\")\n    plt.title('Confusion of {}'.format(i))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion performans of models with Backwards Elimination\n\n# number of normal features is 21\nn_normal_b = 21\n\n# number of abnormal is 57\nn_abnormal_b = 57\n\n# True predicts of normal is true/21 and for abnormal true/57\n# performans is equal : true/21 + true/57\n\nconfus_performans_b = []\n\nfor i ,j  in confusions_b.items():\n    \n    performans_normal_b = j[0][0] / n_normal_b\n    \n    performans_abnormal_b = j[1][1] / n_abnormal_b\n    \n    total_performans_b = performans_normal_b + performans_abnormal_b\n    \n    confus_performans_b.append((i,total_performans_b))\n    \nconfus_performans_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best cunfusion performans with Backward Elimination\n\nconf_b = pd.DataFrame(confus_performans_b)\n\nconf_b.set_index(0)\n\nplt.plot(conf_b[0],conf_b[1],color = 'green')\nplt.scatter(conf_b[0],conf_b[1]);\nplt.title('True predicts of Normal vs Abnormal')\nplt.ylabel('Sum of True predicts');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now compare the confusion performans of models vs models with backward elimination\n\ncompare = list(zip((confus_performans[:6],confus_performans_b)))\n\nprint('The first list is for Models,and second list for Models with Bacward Elimination:\\n')\nfor i in range(len(compare)):\n    print('List {} : {}'.format(i+1,compare[i]))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='14'></a>\n# Hyperparameter Tunning,CrossValidation,GridCross"},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters for cros validation\nrandom_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(probability=True,random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters\ncv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = (GridSearchCV(\n                       #probability=True,\n                       classifier[i],\n                       param_grid=classifier_param[i],\n                       cv = StratifiedKFold(n_splits = 10),\n                       scoring = \"accuracy\", n_jobs = -1,verbose = 1))\n    \n    clf.fit(x_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation\ncv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\",\"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='15'></a>\n## Ensemble Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the bests\nvotingC = VotingClassifier(estimators = [(\"SVM\",best_estimators[1]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3]),\n                                        ('knn',best_estimators[4])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = votingC.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_predict,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}