{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **This Notebook deals with Data cleaning, Feature Engineering, EDA, Feature selection, model selection, model tuning and prediction**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import basic libraries. Other libraries will be added as and when required\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option(\"display.max_columns\", None)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read and inspect the dataset.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/hotel-booking-demand/hotel_bookings.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**On going through this dataset we can do the following feature engineering**\n* Drop arrival_date_year\n* Drop arrival_date_week_number\n* Make a new column for total stays in nights\n* Drop Null Values\n* It is obvious that babies and children are just guests they won't pay or cancel the booking so we can either make a column for total guests or simply drop them all keeping adults in a column named paying_guests\n* identify the Undefined categorical values if they are meaningless then drop them or replace them with some relevant attribute via google search on terminoligies on hotel data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"arrival_date_year\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"arrival_date_week_number\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"stays_in_nights\"] = df[\"stays_in_weekend_nights\"] + df[\"stays_in_weekend_nights\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.children.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.country.fillna(\"Unknown\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.agent.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.company.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"babies\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"paying_guests\"] = df[\"adults\"] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"adults\", \"children\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.meal.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.meal.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.meal.replace(to_replace = dict(Undefined = \"SC\"), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.market_segment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[\"market_segment\"] == \"Undefined\"].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.distribution_channel.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[\"distribution_channel\"] == \"Undefined\"].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing Out-liars**\nadr stands for average daily rate. Its descriptive stats says that it has a minimum value in negative which is possibly an error or at least is unjustified we can drop it and on making distplot it is found that the maximum value is also an outlier so delete it as well. \nForm a new column price multiplying adr with paying guests","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.adr.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[\"adr\"] == -6.38].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[\"adr\"] == 5400].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.adr.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reservation_status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.is_canceled.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is intriguing that reservation_status_values and is_canceled has the same data we can drop either of the columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"reservation_status\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"reservation_status_date\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price\"] = df[\"adr\"] * df[\"paying_guests\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"adr\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Heatmap for Correlation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nsns.heatmap(corr,\n           xticklabels = corr.columns,\n           yticklabels = corr.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One Hot encode the categorical records**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical values to numeric using label encoder\nfrom sklearn import preprocessing\nfrom collections import defaultdict\nd = defaultdict(preprocessing.LabelEncoder)\n\n# Encoding the categorical variable\nfit = df.select_dtypes(include=['object']).fillna('NA').apply(lambda x: d[x.name].fit_transform(x))\n\n#Convert the categorical columns based on encoding\nfor i in list(d.keys()):\n    df[i] = d[i].transform(df[i].fillna('NA'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df[df.columns.difference(['is_canceled'])]\nlabels = df['is_canceled']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n**Weight of Evidence and Information Value**\n(reference from Sundar Balkrishnan's github repository)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport traceback\nimport string\n\nmax_bin = 20\nforce_bin = 3\n\n# define a binning function\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\n\ndef data_vars(df1, target):\n    \n    stack = traceback.extract_stack()\n    filename, lineno, function_name, code = stack[-2]\n    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n    \n    x = df1.dtypes.index\n    count = -1\n    \n    for i in x:\n        if i.upper() not in (final.upper()):\n            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n                conv = mono_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i\n                count = count + 1\n            else:\n                conv = char_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i            \n                count = count + 1\n                \n            if count == 0:\n                iv_df = conv\n            else:\n                iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_iv, IV = data_vars(df[df.columns.difference([\"is_canceled\"])],df.is_canceled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_iv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IV = IV.rename(columns={'VAR_NAME':'index'})\nIV.sort_values(['IV'],ascending=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_vars_list = df.columns.difference(['is_canceled'])\ntransform_prefix = 'new_' # leave this value blank if you need replace the original column values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_vars_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in transform_vars_list:\n    small_df = final_iv[final_iv['VAR_NAME'] == var]\n    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n    replace_cmd = ''\n    replace_cmd1 = ''\n    for i in sorted(transform_dict.items()):\n        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n    replace_cmd = replace_cmd + '0'\n    replace_cmd1 = replace_cmd1 + '0'\n    if replace_cmd != '0':\n        try:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n        except:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier for feature selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\n\nclf.fit(features,labels)\n\npreds = clf.predict(features)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(preds,labels)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nVI = DataFrame(clf.feature_importances_, columns = [\"RF\"], index=features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VI = VI.reset_index()\nVI.sort_values(['RF'],ascending=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Recursive Feature Elimination for feature selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nrfe = RFE(model, n_features_to_select = 20)\nfit = rfe.fit(features, labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nSelected = DataFrame(rfe.support_, columns = [\"RFE\"], index=features.columns)\nSelected = Selected.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Selected[Selected[\"RFE\"] == True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extra Trees Classifier for feature selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(features, labels)\n\nprint(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nFI = DataFrame(model.feature_importances_, columns = [\"Extratrees\"], index=features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FI = FI.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FI.sort_values([\"Extratrees\"], ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Chi2 Test**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nmodel = SelectKBest(score_func=chi2, k=5)\nfit = model.fit(features.abs(), labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\npd.options.display.float_format = '{:.2f}'.format\nchi_sq = DataFrame(fit.scores_, columns = [\"Chi_Square\"], index=features.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_sq = chi_sq.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_sq.sort_values('Chi_Square',ascending=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**L1 for Feature Selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(features, labels)\nmodel = SelectFromModel(lsvc,prefit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nl1 = DataFrame(model.get_support(), columns = [\"L1\"], index=features.columns)\nl1 = l1.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l1[l1['L1'] == True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Combine all**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import reduce\ndfs = [IV, VI, Selected, FI, chi_sq, l1]\nfinal_results = reduce(lambda left,right: pd.merge(left,right,on='index'), dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['IV', 'RF', 'Extratrees', 'Chi_Square']\n\nscore_table = pd.DataFrame({},[])\nscore_table['index'] = final_results['index']\n\nfor i in columns:\n    score_table[i] = final_results['index'].isin(list(final_results.nlargest(5,i)['index'])).astype(int)\n    \nscore_table['RFE'] = final_results['RFE'].astype(int)\nscore_table['L1'] = final_results['L1'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_table['final_score'] = score_table.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_table.sort_values('final_score',ascending=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"final table for importances of various features is above","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.lead_time\ny = df.is_canceled\narea = np.pi*3\n\n# Plot\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.title('Scatter plot lead_time vs cancellation')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.deposit_type\ny = df.is_canceled\narea = np.pi*3\n\n# Plot\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.title('Scatter plot deposit_type vs cancellation')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.country\ny = df.is_canceled\narea = np.pi*3\n\n# Plot\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.title('Scatter plot country vs cancellation')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Multicolinearity check**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_vif(features):\n    vif = pd.DataFrame()\n    vif[\"Features\"] = features.columns\n    vif[\"VIF\"] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]    \n    return(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features[list(score_table[score_table['final_score'] >= 2]['index'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = calculate_vif(features)\nwhile vif['VIF'][vif['VIF'] > 10].any():\n    remove = vif.sort_values('VIF',ascending=0)['Features'][:1]\n    features.drop(remove,axis=1,inplace=True)\n    vif = calculate_vif(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(vif['Features'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Make new dataframe with relevant fetures to end the curse of dimensionality**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_vars = list(vif['Features']) + [\"is_canceled\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df[final_vars].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_color = '#058caa'\nnum_color = '#ed8549'\n\nfinal_iv,_ = data_vars(df1,df1['is_canceled'])\nfinal_iv = final_iv[(final_iv.VAR_NAME != 'is_canceled')]\ngrouped = final_iv.groupby(['VAR_NAME'])\nfor key, group in grouped:\n    ax = group.plot('MIN_VALUE','EVENT_RATE',kind='bar',color=bar_color,linewidth=1.0,edgecolor=['black'])\n    ax.set_title(str(key) + \" vs \" + str('is_canceled'))\n    ax.set_xlabel(key)\n    ax.set_ylabel(str('is_canceled') + \" %\")\n    rects = ax.patches\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x()+rect.get_width()/2., 1.01*height, str(round(height*100,1)) + '%', \n                ha='center', va='bottom', color=num_color, fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df1.iloc[:, :-1]\ny = df1.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nx_train = sc_X.fit_transform(x_train)\nx_test = sc_X.transform(x_test)\n'''sc_y = StandardScaler()\ny_train = sc_y.fit_transform(y_train)''' #since already its categorical dep variable\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression() #classifier is the object of logistic reg class\nclassifier.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \ncm = confusion_matrix(y_test, pred_test)                                           \ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not great distinction of true positives and true negatives","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train,pd.Series(pred_train),rownames=['ACTUAL'],colnames=['PRED'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)\n\nprint(accuracy_train,accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"good score but we can check for better and worse","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB \nclassifier = GaussianNB()\n\nclassifier.fit(x_train,y_train)\n\npred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)\nprint(accuracy_train,accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Terrible score!! drop the idea of naaaivee bayes immediately.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\n\nclassifier.fit(x_train,y_train)\n\npred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)\n\nprint(accuracy_train,accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"wuhooo!! good score..\nlet's check the confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, pred_test, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"pred_test\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, pred_test)                                           \ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Better than Logistic Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tuning of model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(3, 10, num = 1)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = RandomForestClassifier()\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(x_train, y_train)\n\nprint(rf_random.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(**rf_random.best_params_)\n\nclassifier.fit(x_train,y_train)\n\npred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_train, accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final scores look fine. With this we can expect good prediction model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}