{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"overview\"></a>\n# Overview 🧐\n<img src=\"https://i.imgur.com/HVZezzb.jpg\" width=\"600\"><br>\nIn this notebook, we are going to predict whether a breast mass is benign or malignant based on 30 features in the dataset. This prediction can be useful in diagnosing patients with suspected breast cancer.<br>\n<font color=\"RoyalBlue\">このノートブックでは、データセットに含まれる30の特徴量から乳腺腫瘤が良性か悪性かを予測します。この予測は、乳がんの疑いがある患者を診断する際に役立てられるでしょう。</font><br>\n\nI have also run a similar analysis in R ([Breast Cancer🦀EDA & FA / PCA with R (98.2% acc)](https://www.kaggle.com/snowpea8/breast-cancer-eda-fa-pca-with-r-98-2-acc)), \nif you would like to take a look at it.<br>\n<font color=\"RoyalBlue\">同様の分析を R でも実行していますので、そちらも参考にしてください。</font><br>\n\nWe will first discover and visualize the data to gain insights. Then we split the data into a training and a test set and use the training set to train some machine learning models. At the same time, we evaluate the performance of the models with cross-validation. Finally, we will ensemble each model to improve its accuracy.<br>\n<font color=\"RoyalBlue\">まず洞察を得るためにデータを研究、可視化します。それからデータを訓練用とテスト用に分割し、訓練セットを使っていくつかの機械学習モデルを訓練します。同時に、交差検証でモデルの性能を評価します。最後にそれぞれのモデルをアンサンブルし、精度の向上を目指していきます。</font>\n\n# Table of contents 📖\n* [Overview 🧐](#overview)\n* [Setup 💻](#setup)\n* [Load CSV data 📃](#load)\n* [Explore CSV data 📊](#explore)\n* [Data preprocessing 🧹](#preprocessing)\n* [Train models and make predictions 💭](#models)\n    * [LightGBM 🌳](#gbm)\n    * [Extremely randomized trees 🌳](#ert)\n    * [Linear model 📈](#lm)\n* [Simple ensemble 🤝](#ensemble)\n\n<a id=\"setup\"></a>\n# Setup 💻\nAll seed values are fixed at zero.<br>\n<font color=\"RoyalBlue\">シード値は全て0で固定しています。</font><br>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport lightgbm as lgb\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    # tf.random.set_seed(seed)\nseed_everything(0)\n\nsns.set_style(\"whitegrid\")\npalette_ro = [\"#ee2f35\", \"#fa7211\", \"#fbd600\", \"#75c731\", \"#1fb86e\", \"#0488cf\", \"#7b44ab\"]\n\nROOT = \"../input/breast-cancer-wisconsin-data\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n# Load CSV data 📃"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(ROOT + \"/data.csv\")\n\nprint(\"Data shape: \", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset from: [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n\n* `id` - ID number\n* `diagnosis` - Diagnosis (`M`: malignant, `B`: benign)\n<br>　<font color=\"RoyalBlue\">【目的変数】診断（結果）（M : 悪性，B : 良性）</font><br>\n\nThe following features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.<br>\n<font color=\"RoyalBlue\">以下の特徴量は、乳腺腫瘤の穿刺吸引細胞診（FNA）のデジタル画像から計算されたデータです。画像内に存在する細胞核の特徴を説明しています。以下の10の各属性について、それぞれ平均（mean）、標準誤差（se）、最悪値（worst）の3種類、合計30の特徴量が格納されています。</font><br>\n\n* `radius` - mean of distances from center to points on the perimeter\n<br>　<font color=\"RoyalBlue\">半径 - 中心から外周上の点までの距離の平均</font>\n* `texture` - standard deviation of gray-scale values\n<br>　<font color=\"RoyalBlue\">テクスチャ - グレースケール値の標準偏差</font>\n* `perimeter`\n<br>　<font color=\"RoyalBlue\">外周長</font>\n* `area`\n<br>　<font color=\"RoyalBlue\">面積</font>\n* `smoothness` - local variation in radius lengths\n<br>　<font color=\"RoyalBlue\">平滑性 - 半径の長さの局所変動</font>\n* `compactness` - perimeter^2 / area - 1.0\n<br>　<font color=\"RoyalBlue\">コンパクト性 - 外周長^2 / 面積 - 1.0</font>\n* `concavity` - severity of concave portions of the contour\n<br>　<font color=\"RoyalBlue\">凹度 - 輪郭の凹部の程度</font>\n* `concave points` - number of concave portions of the contour\n<br>　<font color=\"RoyalBlue\">凹点数 - 輪郭の凹部の数</font>\n* `symmetry`\n<br>　<font color=\"RoyalBlue\">対称性</font>\n* `fractal dimension` - \"coastline approximation\" - 1\n<br>　<font color=\"RoyalBlue\">フラクタル次元 - 複雑さの程度を表す尺度。複雑であればあるほど値が大きくなる</font>\n\n<a id=\"explore\"></a>\n# Explore CSV data 📊\nAcknowledgements: [Feature Selection and Data Visualization](https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(8, 8))\nsns.countplot(x=\"diagnosis\", ax=ax, data=df, palette=palette_ro[6::-5], alpha=0.9)\n\nax.annotate(len(df[df[\"diagnosis\"]==\"M\"]), xy=(-0.05, len(df[df[\"diagnosis\"]==\"M\"])+5),\n            size=16, color=palette_ro[6])\nax.annotate(len(df[df[\"diagnosis\"]==\"B\"]), xy=(0.95, len(df[df[\"diagnosis\"]==\"B\"])+5),\n            size=16, color=palette_ro[1])\n\nfig.suptitle(\"Distribution of diagnosis\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ncolumns = df.columns.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"])\n\ndata_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 0:10]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Mean values distribution\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 10:20]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Standard error values distribution\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 20:30]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Worst values distribution\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"df_c = df.reindex(columns=[\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\", \"texture_worst\",\n                           \"perimeter_mean\", \"perimeter_se\", \"perimeter_worst\", \"area_mean\", \"area_se\", \"area_worst\",\n                           \"smoothness_mean\", \"smoothness_se\", \"smoothness_worst\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                           \"concavity_mean\", \"concavity_se\", \"concavity_worst\", \"concave points_mean\", \"concave points_se\", \"concave points_worst\",\n                           \"symmetry_mean\", \"symmetry_se\", \"symmetry_worst\", \"fractal_dimension_mean\", \"fractal_dimension_se\", \"fractal_dimension_worst\",\n                           \"diagnosis\"])\ndf_c = df_c.replace({\"M\":1, \"B\":0})\n\nprint(\"Correlation coefficient against diagnosis\")\ndf_c.corr().sort_values(\"diagnosis\", ascending=False)[\"diagnosis\"]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(18, 12))\n\nsns.heatmap(df_c.corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(df_c.corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[30].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since many of the features in this dataset have high correlation coefficients with each other, feature selection is very important.<br>\n<font color=\"RoyalBlue\">このデータセットの特徴量には互いに相関係数の高いものが多いため、特徴量選択が非常に重要になってきます。</font><br>\n\n<a id=\"preprocessing\"></a>\n# Data preprocessing 🧹"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.copy()\ny = X[\"diagnosis\"].replace({\"M\":1, \"B\":0})\nX = X.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"models\"></a>\n# Train models and make predictions 💭\nNow, let's create some models and check the performance measures. The performance measure for classifiers are as follows.<br>\n<font color=\"RoyalBlue\">では、いくつかのモデルを作成し、性能指標を確認していきましょう。分類器の性能指標には以下のようなものがあります。</font>\n\n> Referenced from Hands-On Machine Learning with Scikit-Learn and TensorFlow (Aurelien Geron, 2017).\n* accuracy - the ratio of correct predictions\n<br>　<font color=\"RoyalBlue\">正解率 - 正しい予測の割合</font>\n* confusion matrix - counting the number of times instances of class A are classified as class B\n<br>　<font color=\"RoyalBlue\">混同行列 - クラスＡのインスタンスがクラスＢに分類された回数を数える</font>\n* precision - the accuracy of the positive predictions\n<br>　<font color=\"RoyalBlue\">適合率 - 陽性の予測の正解率（陽性であると予測したうち、当たっていた率）</font>\n* recall (sensitivity, true positive rate: TPR) - the ratio of positive instances that are correctly detected by the classifier\n<br>　<font color=\"RoyalBlue\">再現率（感度、真陽性率）- 分類器が正しく分類した陽性インスタンスの割合（本当に陽性であるケースのうち、陽性だと判定できた率）</font>\n* F1 score - the harmonic mean of precision and recall\n<br>　<font color=\"RoyalBlue\">F1 スコア（F 値） - 適合率と再現率の調和平均（算術平均に比べ、調和平均は低い値にそうでない値よりもずっと大きな重みを置く）</font>\n* AUC - the area under the ROC curve (plotting the true positive rate (another name for recall) against the false positive rate)\n<br>　<font color=\"RoyalBlue\">AUC - ROC 曲線（偽陽性率に対する真陽性率（再現率）をプロットした曲線）の下の面積</font><br>\n\nIn this notebook, we will look at their accuracy, F1 score, and confusion matrix.<br>\n<font color=\"RoyalBlue\">このノートブックでは、正解率、F1 スコア、そして混同行列を見ていきます。</font>\n\n<a id=\"gbm\"></a>\n## LightGBM 🌳\nFirst, let's try a prediction with all the features using LightGBM.<br>\n<font color=\"RoyalBlue\">まずは、LightGBM で全ての特徴量を使った予測を試してみましょう。</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\")\n    clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test, num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default LightGBM\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, narrow down the number of features based on EDA and feature importance. Let's choose the following features.<br>\n<font color=\"RoyalBlue\">次に、EDA や feature importance をもとに特徴量の数を絞ります。下記のような特徴量を選んでいきましょう。</font>\n* High correlation coefficient with the objective variable\n<br>　<font color=\"RoyalBlue\">目的変数との相関係数が高い</font>\n* Less mixing in data distribution for the objective variable\n<br>　<font color=\"RoyalBlue\">目的変数に対するデータ分布において混在が少ない</font>\n* High feature importance\n<br>　<font color=\"RoyalBlue\">feature importance が高い</font>\n* Features are independent of each other (to eliminate multicollinearity)\n<br>　<font color=\"RoyalBlue\">特徴量同士がなるべく独立している（多重共線性を解消するため）</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features1 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                  \"concavity_mean\", \"concavity_se\", \"concavity_worst\", \"concave points_worst\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\"]\nX_1 = X.drop(drop_features1, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_1, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_1], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\",\n                             min_child_samples=10,\n                             reg_alpha=0.1)\n    clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test, num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_gbm = np.mean(y_preds, axis=1)\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized LightGBM\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ert\"></a>\n## Extremely randomized trees 🌳\nWe will also use the Extremely randomized trees.<br>\n<font color=\"RoyalBlue\">Extremely randomized trees も使ってみましょう。</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default Extremely randomized trees\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do the same feature selection as before.<br>\n<font color=\"RoyalBlue\">先程と同じように、特徴量選択を行います。</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features2 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                  \"concavity_mean\",  \"concavity_worst\", \"concave points_mean\", \"concave points_se\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\"]\nX_2 = X.drop(drop_features2, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_2, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_2], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_2, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0,\n                               n_estimators=200,\n                               min_samples_split=5)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_ert = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized Extremely randomized trees\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lm\"></a>\n## Linear model 📈\nIn order to get diverse models, we will also try linear model as a model without decision trees.<br>\n<font color=\"RoyalBlue\">多様性のあるモデルを得るために、決定木を使わないモデルとして、線形モデルも試してみましょう。</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_s = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(X_s, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do feature selection.<br>\n<font color=\"RoyalBlue\">特徴量選択を行います。</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features3 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \n                  \"concavity_se\", \"concavity_worst\", \"concave points_mean\", \"concave points_se\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\", \"fractal_dimension_worst\"]\nX_3 = X_s.drop(drop_features3, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_3, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_3], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_3, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_lm = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ensemble\"></a>\n# Simple ensemble 🤝\nFor better accuracy, ensemble predictions of the three models.<br>\n<font color=\"RoyalBlue\">精度を高めるために、3つのモデルの予測を組み合わせてアンサンブルを行いましょう。</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_em = y_pred_gbm*2 +  y_pred_ert*2 + y_pred_lm\ny_pred_em = (y_pred_em > 3).astype(int)\n\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred_em)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred_em)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred_em)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred_em), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of the ensembled model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred_em), f1_score(y_test, y_pred_em)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We were able to get better accuracy by using the ensemble model. Thanks so much for reading!<br>\n<font color=\"RoyalBlue\">複数のモデルをアンサンブルすることでより良い精度を出すことができました。ここまで読んでくださりどうもありがとうございました！</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}