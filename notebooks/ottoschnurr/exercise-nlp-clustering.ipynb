{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Reference ##\n\n- [Document Clustering with Python](http://brandonrose.org/clustering)\n- [\"Building Machine Learning Systems with Python\"](https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python) by Richert and Coelho\n- [sklearn User Guide: Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n    - [CountVectorizer API](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n    - [TfidfVectorizer API](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\nraw_data = pd.read_csv('../input/imdb-data/IMDB-Movie-Data.csv')\nraw_data.set_index('Rank', inplace=True)\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Titles and Descriptions ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = raw_data[['Title', 'Description']]\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at some of these descriptions."},{"metadata":{"trusted":true},"cell_type":"code","source":"descriptions = data['Description'].tolist()\ndescriptions[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Words and Stems ##\n\nA tokenizer can break a text phrase into individual tokens. In our case, we'll consider the following:\n\n- Ignore punctuation.\n- Ignore words containing numbers.\n- Break hyphenated words into two words."},{"metadata":{"trusted":true},"cell_type":"code","source":"token_pattern = '(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b'\ntoken_expression = re.compile(token_pattern)\ndescription_tokenizer = lambda description: token_expression.findall(description)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The stem of a word is similar to the \"root\" of a word. A stem lets us map similar words to the same stem.\n\nThe first thing we'll build is a map from each word stem to a corresponding word."},{"metadata":{"trusted":true},"cell_type":"code","source":"words = [\n    word \\\n    for description in descriptions \\\n    for word in description_tokenizer(description)\n]\nstemmer = nltk.stem.snowball.SnowballStemmer('english')\nstems = (stemmer.stem(word) for word in words)\n\nword_for_stem = \\\n    pd.DataFrame({ 'stem': stems, 'word': words }) \\\n    .set_index('stem') \\\n    .sort_index()\nword_for_stem = word_for_stem.loc[~word_for_stem.index.duplicated(keep='first')]\nword_for_stem.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preliminary Features ##\n\nNow we'll turn each of the film descriptions into a feature vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer()\ndescription_vectors = vectorizer.fit_transform(descriptions)\nprint(description_vectors.shape)\n\nfeature_names = vectorizer.get_feature_names()\nfeature_names[:6]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try again but only consider words that have all letters. Ignore words with numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(tokenizer=description_tokenizer)\ndescription_vectors = vectorizer.fit_transform(descriptions)\nprint(description_vectors.shape)\n\nfeature_names = vectorizer.get_feature_names()\nfeature_names[:6]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll define some methods for converting the words in an n-gram to/from stems."},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = nltk.stem.snowball.SnowballStemmer('english')\n\ndef stems_for(ngram):\n    stems = (stemmer.stem(word) for word in ngram.split())\n    return ' '.join(stems)\n\ndef words_for(stemmed_phrase):\n    words = (word_for_stem.loc[stem][0] for stem in stemmed_phrase.split())\n    return ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can map similar words to the same stem."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomVectorizer(TfidfVectorizer):\n    def build_analyzer(self):\n        standard_analyzer = super().build_analyzer()\n        stemmer = nltk.stem.snowball.SnowballStemmer('english')\n        return lambda description: (\n            stems_for(ngram) for ngram in standard_analyzer(description)\n        )\n\nvectorizer = CustomVectorizer(tokenizer=description_tokenizer)\ndescription_vectors = vectorizer.fit_transform(descriptions)\nprint(description_vectors.shape)\n\nfeature_names = vectorizer.get_feature_names()\n[words_for(stems) for stems in feature_names[:6]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ignore words that don't carry a lot of meaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CustomVectorizer(tokenizer=description_tokenizer, stop_words='english')\ndescription_vectors = vectorizer.fit_transform(descriptions)\nprint(description_vectors.shape)\n\nfeature_names = vectorizer.get_feature_names()\n[words_for(stems) for stems in feature_names[:6]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition to considering words individually, lets look at pairs and triplets of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CustomVectorizer(\n    tokenizer=description_tokenizer, \n    stop_words='english',\n    ngram_range=(1, 3)\n)\ndescription_vectors = vectorizer.fit_transform(descriptions)\nprint(description_vectors.shape)\n\nfeature_names = vectorizer.get_feature_names()\n[words_for(stems) for stems in feature_names[:6]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selected Features ##\n\nIf an n-gram shows up in almost none of the descriptions, lets ignore it."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CustomVectorizer(\n    tokenizer=description_tokenizer, \n    stop_words='english',\n    ngram_range=(1, 3),\n    min_df=16\n)\ndescription_vectors = vectorizer.fit_transform(descriptions)\nprint(description_vectors.shape)\n\nfeature_names = vectorizer.get_feature_names()\n[words_for(stems) for stems in feature_names[:6]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering ##\n\nNow we'll try to group the descriptions into five clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"clusterer = KMeans(n_clusters=5, random_state=1)\ncluster_indices = pd.Series(\n    clusterer.fit_predict(description_vectors), \n    name='Cluster Index'\n)\n\nclustered_data = data.join(cluster_indices)\nclustered_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the center of the cluster that _\"Guardians of the Galaxy\"_ is a member of."},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_center = clusterer.cluster_centers_[4]\ntop_stem_indices = cluster_center.argsort()[::-1][:8]\ntop_stems = [feature_names[index] for index in top_stem_indices]\nword_for_stem.loc[top_stems]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What other movies are also part of this cluster?"},{"metadata":{"trusted":true},"cell_type":"code","source":"clustered_data.loc[clustered_data['Cluster Index'] == 4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some strange outliers there like _\"La La Land\"_ and _\"Annie\"_.\n\nThe cluster seems to be pretty big: 579 out of the 1000 films. Lets see how many films are in the other clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_indices.value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}