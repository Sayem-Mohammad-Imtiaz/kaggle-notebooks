{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Regression to Predict House Prices using ANN "},{"metadata":{},"cell_type":"markdown","source":"## Introduction and Statement of the Problem"},{"metadata":{},"cell_type":"markdown","source":"- Dataset includes house sale prices for King County in USA. \n- Houses that are sold in the time period: May, 2014 and May, 2015.\n- Data: https://www.kaggle.com/harlfoxem/housesalesprediction\n\n- Interpretation of the Columns:\n    - ida: notation for a house\n    - date: Date house was sold\n    - price: Price is prediction target\n    - bedrooms: Number of Bedrooms/House\n    - bathrooms: Number of bathrooms/House\n    - sqft_living: square footage of the home\n    - sqft_lot: square footage of the lot\n    - floors: Total floors (levels) in house\n    - waterfront: House which has a view to a waterfront\n    - view: Has been viewed\n    - condition: How good the condition is ( Overall )\n    - grade: overall grade given to the housing unit, based on King County grading system\n    - sqft_abovesquare: footage of house apart from basement\n    - sqft_basement: square footage of the basement\n    - yr_built: Built Year\n    - yr_renovated: Year when house was renovated\n    - zipcode: zip\n    - lat: Latitude coordinate\n    - long: Longitude coordinate\n    - sqft_living15: Living room area in 2015(implies -- some renovations) \n    - sqft_lot15: lotSize area in 2015(implies -- some renovations)"},{"metadata":{},"cell_type":"markdown","source":"## Let's Start The Project by Loading the Libraries and the Dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing the Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the Dataset\nhouse_data = pd.read_csv('../input/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see how many rows and columns of data are available in the dataset\nhouse_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset includes 21,613 rows and with 21 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at the top 10 rows of the dataset\nhouse_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at the bottom 10 of the dataset\nhouse_data.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the summary statistics of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the Summary statistics of the data, we can see the following:\n 1. The most expensive house costs 7,700,000.00 dollars and the least expensive costs 75,000.00 dollars\n 2. The oldest house being sold was built on 1900s and the most recently built house was on 2015.\n \n"},{"metadata":{},"cell_type":"markdown","source":"Let's now check if the data have no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is complete. Let's now go to Visualizing the data."},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"    Data Visualization is really important in all Machine Learning work. You have to know what you're working on, know what are the important features in the data, and what are the insights that you can get in the data before training the Machine Learning Model."},{"metadata":{},"cell_type":"markdown","source":"Here's the data again."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the columns available."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the relationship between the area of the house(sqft_living) and the price."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.scatterplot(x = house_data['price'], y = house_data['sqft_living'], color = 'g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also for the relationship between the area of the lot(sqft_lot) and the price."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.scatterplot(x = house_data['price'], y = house_data['sqft_lot'], color = 'g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the bigger the house, the higher the price. Which is logically right.\nWhen it comes to the Area of the Lot, it seems that it is not affecting the price of the house at all."},{"metadata":{},"cell_type":"markdown","source":"Let's take a look now at the distribution of all the data in each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.hist(bins=20, figsize=(20, 20), color = 'g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also see the correlation of each features to each other using a Heat map."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nsns.heatmap(house_data.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see if there's more insight we can extract in the data, we will plot the data using Pair Plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(house_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because of the number of the columns in the dataset, the plots becomes too small to read. We will only consider some of the features that looks more \"important\" than the others."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data_important = house_data[ ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built']   ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data_important","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(house_data_important)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting and Scaling the Dataset"},{"metadata":{},"cell_type":"markdown","source":"Before model training, we have to first split the dataset into the features to train the model with, and the output that we want the model to predict. We will call the features, X, and the output as y. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use only the important features that we set in the Visualization part.\nX = house_data_important.drop(['price'], axis =1)\ny = house_data['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's take a look at X\nX.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#and y\ny.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When working with Neural Networks, one of the most important step to perform before model training is Data Scaling. Neural Networks needs to be fed by values in the same range to obtain much better results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling y\ny = y.values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scaled = scaler.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the Dataset to Training and Testing "},{"metadata":{},"cell_type":"markdown","source":"We will split the dataset into the ones that we will use for training and for the ones that we will use for testing the performance of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will be splitting the data into 75% for training and 25% for testing.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We have\", X_train.shape, \"for training and\", X_test.shape, \"for testing.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building and Training the Model"},{"metadata":{},"cell_type":"markdown","source":"In this project we will use Artificial Neural Networks to predict the house prices based on the features given in the dataset and on new set of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use keras in building our ANN Model. Let's start by importing the needed libraries\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets start building the model.\nmodel = Sequential()\nmodel.add(Dense(32, input_dim = 7, activation = 'relu'))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(1, activation = 'linear'))\nmodel.compile(loss = 'mean_squared_error', optimizer = 'Adam')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will train the model and store all the history of the training in the variable history_epoch\nhistory_epoch = model.fit(X_train, y_train, batch_size = 32, epochs = 50, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating the Model"},{"metadata":{},"cell_type":"markdown","source":"There are many ways to check the performance of a regression model the most widely used ones are the: Root Mean Squared Error (RMSE), Mean Squared Error (MSE), Mean Absolute Error (MAE), R-Squared (R2) and Adjusted R-Squared."},{"metadata":{"trusted":true},"cell_type":"code","source":"#First lets visualize how the loss in the training differs from the validation\nhistory_epoch.history.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_epoch.history['loss'])\nplt.plot(history_epoch.history['val_loss'])\nplt.title('Model Loss During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training and Validation Loss')\nplt.legend(['Training Loss', 'Validation Loss'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the graph above, we can see that the model is not generalizing well like how we need it to be."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's do the prediction for the test data\ny_predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transform back y_predict and y_test into its original values.\norig_y_test = scaler.inverse_transform(y_test)\norig_y_predict = scaler.inverse_transform(y_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(X_test)\nk = X_test.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom math import sqrt\n\nRMSE = float(format(np.sqrt(mean_squared_error(orig_y_test, orig_y_predict)),'.3f'))\nMSE = mean_squared_error(orig_y_test, orig_y_predict)\nMAE = mean_absolute_error(orig_y_test, orig_y_predict)\nr2 = r2_score(orig_y_test, orig_y_predict)\nadj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n\nprint('RMSE =',RMSE, '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot and the metrics, we can see conclude that the model we have built is not good. But it still can be improved. Let's now move on to improving the model."},{"metadata":{},"cell_type":"markdown","source":"## Improving the Model"},{"metadata":{},"cell_type":"markdown","source":"We will now look into some ways to how can we improve the model. We will try adding more features and adjust our model parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will be using more features\nmore_features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors', 'sqft_above', 'sqft_basement', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'yr_built', \n'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n\nX_2 = house_data[more_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_2_scaled = scaler.fit_transform(X_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_2 = house_data['price']\ny_2 = y_2.values.reshape(-1,1)\ny_2_scaled = scaler.fit_transform(y_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset into training and testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_2_scaled, y_2_scaled, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets start building the model again and 1 more layer.\nmodel = Sequential()\nmodel.add(Dense(32, input_dim = 19, activation = 'relu'))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dense(1, activation = 'linear'))\nmodel.compile(loss = 'mean_squared_error', optimizer = 'Adam')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_epoch_2 = model.fit(X_train, y_train, batch_size = 32, epochs = 50, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the losses\nplt.plot(history_epoch_2.history['loss'])\nplt.plot(history_epoch_2.history['val_loss'])\nplt.title('Model Loss During Training')\nplt.ylabel('Training and Validation Loss')\nplt.xlabel('Epoch number')\nplt.legend(['Training Loss', 'Validation Loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's do the prediction for the test data\ny_predictions_2 = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transform back y_predict and y_test into its original values.\norig_y_test = scaler.inverse_transform(y_test)\norig_y_predict_2 = scaler.inverse_transform(y_predictions_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(X_test)\nk = X_test.shape[1]\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom math import sqrt\n\nRMSE = float(format(np.sqrt(mean_squared_error(orig_y_test, orig_y_predict_2)),'.3f'))\nMSE = mean_squared_error(orig_y_test, orig_y_predict_2)\nMAE = mean_absolute_error(orig_y_test, orig_y_predict_2)\nr2 = r2_score(orig_y_test, orig_y_predict_2)\nadj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n\nprint('RMSE =',RMSE, '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding more features to the model really gives the accuracy of the model a big boost. The adjusted R2 is closer to 1.0 which means that the predictions made by the model to the test set is close to the actual values. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}