{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Red wine quality - binary classification: finding a good wine\n\nIn this kernel we will try to find good wines! This is a continuation of https://www.kaggle.com/anarthal/red-wine-quality-linear-regression/, where we tried to predict the quality of red wine based on its physical properties.\n\nWe will try a different approach here: my dear uncle, a curated Basque wine-lover, is coming to have dinner next week with us. I want to impress him but, well, my wine culture is not the best. So let's build a model to help us!\n\nThis dataset contains different wines with their physical properties, together with a numerical quality ranging from 3 to 8, where 8 means an excellent wine and 3 means... not so excellent. I don't think my uncle is very interested in me saying \"hey, try this wine, I bet it's a 5-quality wine!\". I just want to be confident enough to say \"try this *excellent* wine!\". So we will transform the ordered categorical `quality` variable into a binary `good_quality`, and will try to predict this. Let's get started!\n\nFirst of all, some imports..."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_recall_fscore_support, make_scorer, fbeta_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at our data..."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This dataset is really clean, let's double check\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='quality', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. What is good wine?\n\nLooking at the above plot, we see the majority of the wines have quality 5 or 6. Wines with quality 7 or superior are rare. I want to impress my uncle, so let's say a good wine is one with quality 7 or higher (and, well, this is actually what the dataset description recommends as threshold ;)).\n\nLet's build the `high_quality` column and analyze what we have created:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['high_quality'] = (df['quality'] >= 7).astype('int64')\ndf['high_quality'].value_counts() / df['high_quality'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['high_quality'].value_counts().plot.pie(explode=[0, 0.1], figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So just 13% of our wines are worthy. Looks like we have a little bit of imbalance here."},{"metadata":{},"cell_type":"markdown","source":"## 2. What makes good wine good?\n\nLet's now see what are the features with the highest importance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.drop(columns='quality').corr()\nidx = corr['high_quality'].abs().sort_values(ascending=False).index[:5]\nidx_features = idx.drop('high_quality')\nsns.heatmap(corr.loc[idx, idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(1, 4, figsize=(20, 5))\nfor i, var in enumerate(idx_features):\n    sns.boxplot(x='high_quality', y=var, data=df, ax=ax.flatten()[i])\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see the following relationships:\n- The more alcohol, the better the wine. Literature says the best alcohol values around 14%. All our wines are below it, so this makes sense.\n- The less volatile acidity, the better the wine. Volatile acids, like acetic acid, can make the wine taste sour and are considered a cause of wine fault. A negative correlation here also makes sense.\n- The more citric acid, the better the wine. Apparently, citric acid gives wine a more fresh flavour, but I couldn't find anything clear about this in the literature.\n- The more sulphates, the better the wine."},{"metadata":{},"cell_type":"markdown","source":"## 3. Some preprocessing!\n\nWe will take two pre-processing steps for our data:\n- *Scaling*. The range of features is not very distinct, but there are some differences. We are going to try several different classification algorithms, including SVM, which requires features to be normalized. We will use sklearn's StandardScaler, which applies a linear transformation to our features such that their mean is zero and their variance is one. We will apply scaling to all classifiers.\n- *Polynomials*. We will be creating polynomial features for some of the linear classifiers. So, for any pair of features, like $alcohol$ and $sulphates$, we will create $alcohol^2$, $sulphates^2$, $alcohol * sulphates$ and so on.\n\nWe will define these as re-usable functions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale(X_train, X_test):\n    # Note that we only use the training data to fit the scaler, so that\n    # no information from the test set leaks into our model\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    return X_train, X_test\n\ndef polynomials(X_train, X_test, degree=2):\n    pol = PolynomialFeatures(degree)\n    return pol.fit_transform(X_train), pol.transform(X_test)\n\ndef split(X, y):\n    return train_test_split(X, y, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Training and evaluation\n\nThe default scoring for classification is accuracy (correct classification count / total count). However, we are facing a skewed classes classification problem, so accuracy may not the best measure. We also have *precision*, *recall*, and *F1*, which are more suitable for this case. I will be reporting all these scores using sklearn's `precision_recall_fscore_support`.\n\nHowever, it is good to have a single real number evaluation metric in order to compare different models objectively. In this case, what I want is to be confident that my wine will be a good one so I don't let my uncle down. In other words, I am more concerned about false positives than about false negatives. That means that I will be prioritizing precision over recall. I don't want to say \"_no wine is worthy_\", either, so I will be using a weighted version of F1, `fbeta_score`. Beta is a parameter that measures how important is recall over precision, where beta=0 means 'consider only precision' and beta=inf means 'consider only recall'. \n\nLet's define a function to do the training and evaluation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_beta = 0.25\n\ndef _compute_all_scores(model, X, y):\n    return precision_recall_fscore_support(y, model.predict(X), average='binary', beta=f1_beta)[:-1]\n\ndef train(X_train, X_test, y_train, y_test, model):\n    model.fit(X_train, y_train)\n    prec, rec, f1 = _compute_all_scores(model, X_train, y_train)\n    print('TRAIN: prec={:.4f}, recall={:.4f}, f1={:.4f}'.format(prec, rec, f1))\n    prec, rec, f1 = _compute_all_scores(model, X_test, y_test)\n    print('TEST : prec={:.4f}, recall={:.4f}, f1={:.4f}'.format(prec, rec, f1))\n    \nscorer = make_scorer(fbeta_score, beta=f1_beta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These are our features. X_subset are the ones that we identified as the most relevant.\nX = df.drop(columns=['quality', 'high_quality'])\nX_subset = X[idx_features].copy()\ny = df['high_quality']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1. Logistic regression\n\nLet's start with the simplest possible model: a linear one, no polynomial features, no hyperparameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X, y)\nX_train, X_test = scale(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, LogisticRegression(random_state=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad but we can do better than this. As this is a linear model, it may be too simple. It may benefit from including polynomial features. Let's try it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X_subset, y)\nX_train, X_test = scale(X_train, X_test)\nX_train, X_test = polynomials(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, LogisticRegression(random_state=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a little bit better. Let's now try to automatically tune hyperparameters for better performance. Fortunately, logistic regression has this built-in, as a cross-validation grid search:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X, y)\nX_train, X_test = scale(X_train, X_test)\nX_train, X_test = polynomials(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, LogisticRegressionCV(random_state=0, max_iter=500, scoring=scorer))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better in the training set, worse on the test set. It seems like we are overfitting the training set. Let's try simplifying the model a little bit, and just try a subset of the employed features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X_subset, y)\nX_train, X_test = scale(X_train, X_test)\nX_train, X_test = polynomials(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, LogisticRegressionCV(random_state=0, max_iter=500, scoring=scorer))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a little better, but let's try different classifiers and see how they perform."},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Decision trees and random forest\n\nLet's first start with a simple decision tree:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X, y)\nX_train, X_test = scale(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, DecisionTreeClassifier(random_state=0, max_leaf_nodes=30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Promising, it did better than the best tuned logistic regression classifier. Let's try now the omnipresent random forest:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X, y)\nX_train, X_test = scale(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, RandomForestClassifier(random_state=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I can now understand why is so popular. It seems it has completely overfit the training set, but it still performs good on the test set. \n\nLet's try tuning hyperparameters. We will do an exhaustive grid search. Let's set `n_estimators` to values higher than the defaults, as increasing this tends to decrease overfitting; and let's try smaller values for `max_features` and `max_depth`, as biger values for these make trees more complex, thus increasing the chance of overfitting:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X, y)\nX_train, X_test = scale(X_train, X_test)\nmodel = GridSearchCV(RandomForestClassifier(random_state=0), {\n    'n_estimators': [100, 200, 300],\n    'max_features': [1, 2, 3],\n    'max_depth' : [4, 6, 8]\n}, scoring=scorer)\ntrain(X_train, X_test, y_train, y_test, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Support vector machines\n\nLet's try first a SVM with a RBF kernel:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X, y)\nX_train, X_test = scale(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, SVC(random_state=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It outperforms the logistic regression but not the random forest classifier. Let's try now with a linear kernel and adding some polynomial features instead:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split(X, y)\nX_train, X_test = scale(X_train, X_test)\nX_train, X_test = polynomials(X_train, X_test)\ntrain(X_train, X_test, y_train, y_test, SVC(random_state=0, kernel='linear'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusion\n\nAfter evaluating all these different classifiers, the best ones appears to be random forest with the tuned hyperparameters. Hope my uncle enjoys the wine.\n\nThis takes us to the end of this kernel! Thank you for reading it. Any comments or suggestions are more than welcome. If you found it useful, please leave an upvote!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}