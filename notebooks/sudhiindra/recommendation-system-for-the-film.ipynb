{"cells":[{"metadata":{},"cell_type":"markdown","source":" Myslef [Sudhindra V](https://www.linkedin.com/in/vsudhindra/) is creating an ML based Recommendation Engine in collaboration with [Mr. Rocky Jagtiani](https://www.linkedin.com/today/author/rocky-jagtiani-3b390649/)\n> This is a simple Data Science project on Movies Recommendation System which recommends you the movie based on the Review of previous movie.\n\n> Dataset: tmdb_5000_credits.csv,tmdb_5000_movies.csv from kaggle itself\n\n> Tech Stack used: pandas, Scikit-learn,Python\n\n> Recommended links : \n\n> https://datascience.suvenconsultants.com  ( For DS / AI / ML )\n\n> https://monster.suvenconsultants.com  ( For Web development )"},{"metadata":{},"cell_type":"markdown","source":"Recommender systems are among the most popular applications of data science today. They are used to predict the \"rating\" or \"preference\" that a user would give to an item. Almost every major tech company has applied them in some form. Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow.\n\nRecommender systems have also been developed to explore research articles and experts, collaborators, and financial services. "},{"metadata":{},"cell_type":"markdown","source":"Recommender systems can be classified into Two types:\n\n> **Content-based recommenders**: suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person likes a particular item, he or she will also like an item that is similar to it. And to recommend that, it will make use of the user's past item metadata. A good example could be YouTube, where based on your history, it suggests you new videos that you could potentially watch.\n\n> **Collaborative filtering engines**: these systems are widely used, and they try to predict the rating or preference that a user would give an item-based on past ratings and preferences of other users. Collaborative filters do not require item metadata like its content-based counterparts."},{"metadata":{},"cell_type":"markdown","source":"Content Based -> Meta tags\n\n*Collabrative filtering -> consumer or user behaviour\n\nCollabrative -> Cold Start Problem a. U just started your website.\n\nb. U won't have any recommendations / user preferences.\nSoln : Content Based + Collabrative -> Hybrid Model Â¶"},{"metadata":{},"cell_type":"markdown","source":"Here we are going to implement **Content Based Filtering**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import Pandas\nimport pandas as pd\n\n# Loading Data sets\nfull_url='/kaggle/input/tmdb-movie-metadata/tmdb_5000_credits.csv'\n\nfull_url1='/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv'\n\ncredits = pd.read_csv(full_url)\nmovies=pd.read_csv(full_url1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing 1st 5 elements of credits dataset\ncredits.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing 1st 5 elements of movies dataset\nmovies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the shapes of both the datasets\nprint(\"Credits:\",credits.shape)\nprint(\"Movies:\",movies.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column of credits data set\ncredits_renamed=credits.rename(index=str,columns={'movie_id':'id'})\ncredits_renamed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge or Inner Join -> U r finding common between both.\n5000 intersection 4803 => 4803 common elements\n\nEmp \n\nA \t 100\nB \t 101\n\nDept\n\n100\t IT\n102\t SALES\n\nOuter Join : \n1> Left -> o/p of Inner + All those rows of the left table which didn't match. \n\n\nInner : \nA \t 100 \t IT \n\n// No emp works for SALES dept as of now. \n\n// B works for an unknown dept\n\nEmp Left Join Dept :\n\nA \t 100 \t IT \nB \t 101 \t NULL\n\n2> Rgt o/p of Inner + All those rows of the Rgt table which didn't match.\n\nEmp Right Join Dept :\n\nA \t 100 \t IT \nNull 102 \t SALES\n\n3> Full  -> Inner + Left + Rgt "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging both data sets\nmerge=movies.merge(credits_renamed,on='id')\nmerge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list = list(merge)\nmy_list = merge.columns.values.tolist()\nprint(my_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping unnecessary columns \ncleaned=merge.drop(columns=['homepage','title_x','title_y','status','production_countries'])\ncleaned.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list1 = list(cleaned)\nmy_list1 = cleaned.columns.values.tolist()\nprint(my_list1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['overview'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,3),min_df=3,analyzer='word')\n\n#Replace NaN with an empty string\ncleaned['overview'] = cleaned['overview'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(cleaned['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#my_list2 = list(tfidf_matrix)\n#my_list2 = tfidf_matrix.tolist()\n#print(my_list2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cosine_sim.shape)\nprint(cosine_sim[0])\nprint(cosine_sim[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We are going to define a function that takes in a movie title as an input and outputs a list of 10 most similar movies. Firstly, for this we need a reverse mapping of movie titles and DataFrame indices. In other words, weneed a mechanism to identify the index of a movie in our metadata DataFrame given its title**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(cleaned.index, index=cleaned['original_title']).drop_duplicates()\nindices[ :5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now in a good position to define our recommendation function. these are the following steps will follow.\n\n* Get the index of the movie given its title.\n* Get the list of Cosine similarity scores for that particular movie with all movies. convert it into a list of tuples where the first element is its position and the second is the similarity score.\n* Sort the before mentioned list of tuples based on the similarity scores, That is the second element\n* Get the top 10 elements of the list. Ignore the first element as it refers to self (Te movie most similar to a particular movie is the movie itself).\n* Return the Titlescorresponding to the indices of the top elements."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_recommendations(title, sim_matrix):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(sim_matrix[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return cleaned['original_title'].iloc[movie_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the recommendation\nget_recommendations('Avatar',cosine_sim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the recommendation\nget_recommendations('The Dark Knight Rises',cosine_sim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Enhancements"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## have a look at the way data is stored in columns like crew or cast\ncleaned['crew'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parse the stringified features into their corresponding python objects\nfrom ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres']\n\nfor feature in features:\n    cleaned[feature] = cleaned[feature].apply(literal_eval)\n    \n## about literal_eval()    \n## https://stackoverflow.com/questions/15197673/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets see the data stored for the 0th movie.  \ncleaned['crew'].values[0]\n\n## Notice : its an list of dict objects.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['cast'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function to get the director's name\ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## a function that will return the top 3 elements or the entire list, whichever is more. \n## Here the list refers to the cast, keywords, and genres.\n\ndef get_list(x):\n    \n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n    \n    #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[ : 3]\n        return names\n\n    #Return empty list in case of missing/malformed data\n    return []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define new director, cast, genres and keywords features \n## that are in a suitable form.\ncleaned['director'] = cleaned['crew'].apply(get_director)\n\nfeatures = ['cast', 'keywords', 'genres']\n\nfor feature in features:\n    cleaned[feature] = cleaned[feature].apply(get_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the new features of the first 3 films\ncleaned[['original_title', 'cast', 'director', 'keywords', 'genres']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them.\n\nRemoving the spaces between words is an important preprocessing step. It is done so that your vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same. After this processing step, the aforementioned actors will be represented as \"johnnydepp\" and \"johnnygalecki\" and will be distinct to your vectorizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to convert all strings to lower case and strip names of spaces\ndef clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    cleaned[feature] = cleaned[feature].apply(clean_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You are now in a position to create your \"metadata\", which is a string that contains all the metadata that you want to feed to your vectorizer (namely actors, director and keywords).\n\nThe create_metadata function will simply join all the required columns by a space. This is the final preprocessing step, and the output of this function will be fed into the word vector model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_metadata(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new metadata feature\ncleaned['metadata'] = cleaned.apply(create_metadata, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned[['metadata']].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next steps are the same as what you did with above content based recommender.\n\nOne key difference is that you use the CountVectorizer() instead of TF-IDF. This is because you do not want to down-weight the actor/director's presence if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense to down-weight them in this context.\n\nThe major difference between CountVectorizer() and TF-IDF is the inverse document frequency (IDF) component which is present in"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\n\ncount_matrix = count.fit_transform(cleaned['metadata'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset index of your main DataFrame and construct reverse mapping as before\n\n## cleaned = cleaned.reset_index()\nindices = pd.Series(cleaned.index, index = cleaned['original_title'])\nindices[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## You can now reuse your get_recommendations() function \n## by passing in the new cosine_sim2 matrix as your second argument.\n\nget_recommendations('The Dark Knight Rises', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Godfather', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"I would like to humbly and sincerely thank my mentor [Rocky Jagtiani](https://www.linkedin.com/today/author/rocky-jagtiani-3b390649/). He is more of a friend to me then mentor. The Machine Learning course taught by him and various projects we did and are still doing is the best way to learn and skill in Data Science field. See https://datascience.suvenconsultants.com once for more."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}