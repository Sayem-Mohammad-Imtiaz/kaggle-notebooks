{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Modules for data manipulation\nimport numpy as np\nimport pandas as pd\nimport re\n\n# Modules for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Tools for preprocessing input data\nfrom bs4 import BeautifulSoup\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Tools for creating ngrams and vectorizing input data\nfrom gensim.models import Word2Vec, Phrases\n\n# Tools for building a model\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Tools for assessing the quality of model prediction\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom importlib import reload\nimport sys\nfrom imp import reload\n\nif sys.version[0] == '2':\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SMALL_SIZE = 12\nMEDIUM_SIZE = 14\nBIG_SIZE = 16\nLARGE_SIZE = 20\n\nparams = {\n    'figure.figsize': (16, 8),\n    'font.size': SMALL_SIZE,\n    'xtick.labelsize': MEDIUM_SIZE,\n    'ytick.labelsize': MEDIUM_SIZE,\n    'legend.fontsize': BIG_SIZE,\n    'figure.titlesize': LARGE_SIZE,\n    'axes.titlesize': MEDIUM_SIZE,\n    'axes.labelsize': BIG_SIZE\n}\nplt.rcParams.update(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#导入数据\ndf1 = pd.read_csv('/kaggle/input/bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv', delimiter=\"\\t\")\ndf1 = df1.drop(['id'], axis=1)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#导入数据\ndf2 = pd.read_csv('/kaggle/input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\")\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2.drop(['Unnamed: 0','type','file'],axis=1)\ndf2.columns = [\"review\",\"sentiment\"]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2[df2.sentiment != 'unsup']\ndf2['sentiment'] = df2['sentiment'].map({'pos': 1, 'neg': 0})\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df1, df2]).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.hist(df[df.sentiment == 1].sentiment,\n         bins=2, color='green', label='Positive')\nplt.hist(df[df.sentiment == 0].sentiment,\n         bins=2, color='blue', label='Negative')\nplt.title('Classes distribution in the train data', fontsize=MEDIUM_SIZE)\nplt.xticks([])\nplt.xlim(-0.5, 2)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer()\n\n\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\n##\n\ndef lemmatize(tokens: list) -> list:\n    # 1. Lemmatize 词形还原 去掉单词的词缀 比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”\n    tokens = list(map(lemmatizer.lemmatize, tokens))\n    lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), tokens))\n    # 2. Remove stop words 删除停用词\n    meaningful_words = list(filter(lambda x: not x in stop_words, lemmatized_tokens))\n    return meaningful_words\n\n\ndef preprocess(review: str, total: int, show_progress: bool = True) -> list:\n    if show_progress:\n        global counter\n        counter += 1\n        print('Processing... %6i/%6i'% (counter, total), end='\\r')\n    # 1. Clean text\n    review = clean_review(review)\n    # 2. Split into individual words\n    tokens = word_tokenize(review)\n    # 3. Lemmatize\n    lemmas = lemmatize(tokens)\n    # 4. Join the words back into one string separated by space,\n    # and return the result.\n    return lemmas\n##\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Processed_Reviews.apply(lambda x: len(x.split(\" \"))).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nmax_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(df['Processed_Reviews'])\nlist_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])\n\nmaxlen = 130\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\ny = df['sentiment']\n\nembed_size = 128\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 3\nmodel.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv(\"/kaggle/input/testdata/TestData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\ndf_test.head()\ndf_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\ndf_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = df_test[\"sentiment\"]\nlist_sentences_test = df_test[\"review\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\nprediction = model.predict(X_te)\ny_pred = (prediction > 0.5)\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_te)\ndef submit(predictions):\n    df_test['sentiment'] = predictions\n    df_test.to_csv('submission.csv', index=False, columns=['id','sentiment'])\n\nsubmit(y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}