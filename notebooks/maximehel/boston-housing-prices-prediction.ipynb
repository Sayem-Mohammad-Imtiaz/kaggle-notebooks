{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Boston Housing Prices\n\nWelcome everyone! This is my first notebook and I'm so happy to show you this machine\nlearning starting shot. After several theorical and practical courses I thought it was time\nto get involved seriously! For a trying, I'll present you my work on the Boston Housing Prices\npredictions. All comments, suggestions and corrections are welcome, keep progressing is the\ngoal!\n\nI've cut this notebook into 3 major parts:\n* The first one is dedicated to understand our dataset in order to be the best prepared to explain and make the best choices to get the best models possible.\n* Then, I transformed these data, knowing which models I would use and with a better understanding of the features, I was able to do the most appropriate transformations, with sometimes some for a specific model.\n* Finally I trained and compared the different models, denoted the  best one to make future predictions for our goal. \n\n## Initial step : Importing the Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing & Describing the Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of [ Boston MA](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The following describes the dataset columns:\n\n* CRIM - per capita crime rate by town\n* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS - proportion of non-retail business acres per town.\n* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX - nitric oxides concentration (parts per 10 million)\n* RM - average number of rooms per dwelling\n* AGE - proportion of owner-occupied units built prior to 1940\n* DIS - weighted distances to five Boston employment centres\n* RAD - index of accessibility to radial highways\n* TAX - full-value property-tax rate per \\$10,000\n* PTRATIO - pupil-teacher ratio by town\n* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT - % lower status of the population\n* MEDV - Median value of owner-occupied homes in \\$1000's\n\nThe description below already gave us some informations:\n* LSTAT will take values between 0 and 100.\n* CHAS is a binary variable.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"boston = load_boston()\ndataset = pd.DataFrame(boston.data)\ndataset.columns = boston.feature_names\ndataset['MEDV'] = boston.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function```describe()```show us interesting things about our features. The first ones is with the column\n**ZN** and **BN**. These feature seems to be conditional as for **ZN**, until you reach the third quartile, all the values are zero and for **B** the starting 25% values are 0 before increase to around the mean of 356.\n\nWe can also note that for some of our features the distribution of the data seems rather asymmetrical, seeing the difference between the median value and the mean.\n\nLet's confirm it with a quick look over features statistics boxplots and distribution plots.\n\nP.S: Their is no missing data!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in dataset.items():\n    ax = sns.boxplot(y=k, data=dataset, ax=axs[index])\n    ax.set_title(dataset.columns[index] + \" boxplot\")\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in dataset.items():\n    try:\n        plot = sns.distplot(v, ax=axs[index])\n        plot.set_title(dataset.columns[index] + \" dist plot\")\n    except RuntimeError as re:\n        if str(re).startswith(\"Selected KDE bandwidth is 0. Cannot estimate density.\"):\n            plot = sns.distplot(v, kde_kws={'bw': 0.1}, ax=axs[index])\n            plot.set_title(dataset.columns[index] + \" dist plot\")\n        else:\n            raise re\n\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As discussed later on, the different plots confirmed that **ZN**, **B** and **CHAS** are what we identified them to.\n\nNow, these plots show us too that some features like **CRIM**, **B**, **MEDV** etc. got a lot of outlier values. \nIt also confirm what we first said about asymmetric data, the boxplot and distribution show us how much the skewness is important for those features. \n\nAs we will want to train our data through a ordinary least square regression model we won't need to treat our features for skewness. However, normality of the output is an assumption of the OLS regression, so we will try to apply a transformation to our **MEDV** output to make it more gaussian-distributed.  \n\nOne last thing, **MEDV** seems also to be locked at a value of 50, which could impact the learning algorithm best fitting the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature selection & transformations\n\n### 3.1 Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.heatmap(dataset.corr().abs(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(dataset.columns[:-1]):\n    sns.regplot(y=dataset['MEDV'], x=dataset[k], ax=axs[i], color=np.random.rand(3,))\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the correlation matrix, **LSTATS**, **RM**, **PTRATIO**, **INDUS**, **TAX** and **NOX** are the highest correlated features to the output **MEDV**. More generally, none plots seems to show a non-linear correlation pattern. \n\nIn fact, as we still didn't transform our data to take care of the fact that **MEDV** feature is locked at a max value of 50, their could be a change in the correlation values, but trust me and I could show it to you later, it don't affect that much the correlation values and our 6 main features will stay the same. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Transformations\n\nNow let's start the features transformation.\n\nWithout any particular motivation, we choose to try to make predictions with 3 models, OLS (Linear regression) regression, SVR (Support Vector Regression) and Random Forest regression.\nFor each of these models, we need to take care of particular uses conditions:\n* Normality of the output is an OLS assumption, we will need to transform our output to make it the most possible gaussian-distributed.\n* SVR is very sensitive to outliers and different scales between the features, as it is we will need to apply feature scalling and selection to best train this model.\n* Random Forest Regression don't need special treatment looking at the nature of our dataset but will be interesting to see how well he can fit our output.\n\nThe first step will however be to delete values greater or equal to 50 from our output column so our models won't thought it's a special behavior of our case study. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset[~(dataset['MEDV'] >= 50.0)]\n\nX = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.1 Transformations for OLS regression (multiple linear regression)\n\nAs we said later, OLS regression is done making the assumption that the output y is gaussian-distributed. Let's make this assumption a bit more true than it was before. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('old skewness: ', y_train.skew())\npt_y = PowerTransformer(method='yeo-johnson', standardize=False)\ny_train_ols = pt_y.fit_transform(y_train.values.reshape(len(y_train), 1))\ny_test_ols = pt_y.transform(y_test.values.reshape(len(y_test), 1))\nprint('new skewness: ', stats.skew(y_train_ols)[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The skewness measure how much the data distribution is asymmetrical. A value greater than 1 or less than -1 is considered highly skewed. Our output is considered moderately skewed with a value of 0.7.\nUsing a Yeo-Johnson transformation (that I won't try to describe the process here!) we were able to make our **MEDV** output, a far more gaussian-distributed serie. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"3.2.2 Transformations for SVR","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For SVR, the problem is somewhere else. \n\nStill we said it later too, let's make a brief reminder: Support Vector Regression model works in part by exploiting the principle of Kernel functions. These functions allow to determine the membership of an observation to a group by calculating distances between the observation and the centre of the kernel. Thus, this model is very sensitive to outliers and scale difference between your features.\n\nApplying feature selection over the more correlated features and those which have the least outliers combine with a feature scaling we should be able to best fit this model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_svr = X_train.loc[:, ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']]\nX_test_svr = X_test.loc[:, ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train_svr = sc_X.fit_transform(X_train_svr)\nX_test_svr = sc_X.transform(X_test_svr)\ny_train_svr = sc_y.fit_transform(y_train.values.reshape(len(y_train), 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model training & evaluation\n\n\n\n### 4.1 Multiple linear regression (Ordinary Least Squares regression)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train_ols)\n\ny_pred_ols = pt_y.inverse_transform(lin_reg.predict(X_test))\n\nerror_ols = mean_squared_error(pt_y.inverse_transform(y_test_ols), y_pred_ols, squared=False)\nr2_ols = r2_score(pt_y.inverse_transform(y_test_ols), y_pred_ols)\n\nprint('RMSE: ', error_ols)\nprint('R2: ', r2_ols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Support Vector Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svr = SVR(kernel='rbf')\nsvr.fit(X_train_svr, y_train_svr.ravel())\n\ny_pred_svr = sc_y.inverse_transform(svr.predict(X_test_svr))\n\nerror_svr = mean_squared_error(y_test, y_pred_svr, squared=False)\nr2_svr = r2_score(y_test, y_pred_svr)\n\nprint('RMSE: ', error_svr)\nprint('R2: ', r2_svr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Random Forest Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_reg = RandomForestRegressor(n_estimators=20, random_state=0)\ntree_reg.fit(X_train, y_train)\n\ny_pred = tree_reg.predict(X_test)\n\nerror_tree = mean_squared_error(y_test, y_pred, squared=False)\nr2_tree = r2_score(y_test, y_pred)\n\nprint('RMSE: ', error_tree)\nprint('R2: ', r2_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering the different scores and despite all the measures taken, it would seem that the Random Forest Regression is the best model for this case study. \n\nHowever, it was noted that the SVR model is very close behind him.\n\nOverall, each of the models performed well. Given the small amount of data compared to the number of features, the nature of each of them and the objective sought, we felt that we had very good reliability of our models. Improving the accuracy would obviously require more observations in the first place. Then, one could perhaps expect a beneficial effect from a more thorough data cleaning with an outlier treatment. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([[error_ols, error_svr, error_tree], [r2_ols, r2_svr, r2_tree]], index=['RMSE', 'R2'], columns=['OLS reg', 'SVR', 'Random Forest reg'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a last few words, I would make a huge thanks to Shreayan Chaudhary and Prasad Perara for their own notebooks on the subject which were very inspiring for me as a starting point of reflexion!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}