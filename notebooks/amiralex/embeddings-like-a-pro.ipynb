{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\nЕсть несколько вариантов представлений текстов - с помощью OneHotEncoder, CountVectorizer, TfidfVectorizer. Все они работают по общему принципу: мы как-то переводим тексты в вектора, а с векторами мы уже хорошо умеем работать.\n\n\nВсе эти подходы основываются на частоте появления слов в нашем датасете (коллекции документов). Они хорошо работают во многих задачах, но были придуманы в 50ых - 70ых годах прошлого века. Огромным недостатком применения этих методов на чистых необработанных данных является их скорость - на больших данных вы просто не сможете их обучить с плохим аппаратным обеспечением. В этом ноутбуке рассмотрим несколько методов, как оптимизовать обработку таких данных, чтобы минимизировать потерю качества и максимизировать скорочть обработки. А именно рассмотрим:\n* уменьшение размерности с помощью PCA\n* модуль fasttext от Facebook, которая делает эмбеддинги\n\nЧтобы запустить этот ноутбук вам понадобится:\n\n- скачать и распаковать архив crawl-300d-2M.vec.zip с сайта https://fasttext.cc/docs/en/english-vectors.html (либо просто скопировать мой ноутбук - тут уже загружен этот файл)\n\n- установить gensim с помощью `pip install gensim`","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Рассмотрим простой пример - задачу небинарной классификации текстов из ноутбука по NLP:","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = 42)\ntwenty_test = fetch_20newsgroups(subset='test', categories = categories,shuffle = True, random_state = 42)\nX_train = twenty_train.data\ny_train = twenty_train.target\nX_test = twenty_test.data\ny_test = twenty_test.target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В этот раз загрузим весь словарь для улучшения качества наших эмбеддингов. Если он у вас не влезает в память, загрузите часть словаря.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ntfidf = TfidfVectorizer()\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\nlogreg = LogisticRegression(solver = 'liblinear', multi_class = 'ovr', random_state = 1)\n\nlogreg.fit(X_train_tfidf, y_train)\ny_pred = logreg.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))\n\nOutPut:  0.8868175765645806\nprint(X_train_tfidf.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Простое использование `TfidfVectorizer + LogisticRegression` дает нам приличную accuracy 0.896. Но будь у нас больше данных у нас могли бы возникнуть проблемы при обучении из-за размера матрицы `X_train_tfidf` - сейчас он всего `(2257, 35788)`.","metadata":{}},{"cell_type":"markdown","source":"Логичный способ уменьшить размер матрицы - применить  методы понижения размерности, например PCA. В общем случае это может быть непросто, т.к. матрицы получаемые с помощью `TfidfVectorizer` бывают очень большие и почти всегда очень разреженные. Но здесь у нас специально взят очень маленький датасет, так что мы легко можем воспользоваться PCA.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 300, random_state = 1)\nX_train_tfidf_pca = pca.fit_transform(X_train_tfidf.todense())\nX_test_tfidf_pca = pca.transform(X_test_tfidf.todense())\n\nlogreg.fit(X_train_tfidf_pca, y_train)\ny_pred_pca = logreg.predict(X_test_tfidf_pca)\nprint(accuracy_score(y_test, y_pred_pca))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"При понижении размерности с `35788` до `300` качество упало, но не очень сильно.\n\nДавайте теперь рассмотрим подход, основанный на эмбеддингах из fastText.\n\nWord2Vec - это нейросетевая модель, более подробно она и другие похожие модели рассматриваются в курсе по DL. Но результатом работы данной модели является словарь эмбеддингов, позволяющий сопоставлять словам их векторные представления. Чтобы пользоваться такими словарями нам не обязательно прямо сейчас понимать, как именно работает эта модель.\n\n\nЗа время прошедшее с публикации Word2Vec появилось много аналогичных алгоритмов, а также готовые словари с миллионами слов. Они получены из моделей качественно обученных на больших датасетах. Пример таких словарей - fastText от Facebook, он есть в публичном доступе для 157 языков.\n\n\nПоявились также и библиотеки, позволяющие обучить свою модель Word2Vec на своей коллекции документов в пару строк кода. Мы рассмотрим библиотеку gensim.\n\n\nВ большинстве случаев, самостоятельно обученная модель будет уступать готовым, но есть случаи, когда выгодно обучить модель самому - например если вы работаете с задачей в какой-то особенной предметной области и ваши тексты сильно отличаются от стандартных текстов для обучения моделей.\n\nПопробуем считать вектор документа как среднее арифметическое векторов его слов. Слова, которых нет в нашем словаре, будем заменять на средний вектор по всему словарю.\n\nДавайте загрузим словарь эмбеддингов FastText для английского языка (первые 2000000 слов). Код функции `load_vectors` взят с сайта fastText и немного модифицирован.","metadata":{}},{"cell_type":"code","source":"import io\nfrom tqdm import tqdm\nfrom itertools import islice\nimport numpy as np\n\ndef load_vectors(fname, limit):\n  fin = io.open(fname, 'r', encoding = 'utf-8', newline = '\\n', errors = 'ignore')\n  n, d = map(int, fin.readline().split())\n  data = {}\n  for line in tqdm(islice(fin, limit), total = limit):\n    tokens = line.rstrip().split(' ')\n    data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n  return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Загрузим весь словарь для улучшения качества наших эмбеддингов. У меня следующая строчка кода загружалась примерно 4 минуты. Если он у вас не влезает в память, загрузите часть словаря (уменьшите второй параметр).","metadata":{}},{"cell_type":"code","source":"vecs = load_vectors('../input/crawluy/crawl-300d-2M.vec', 2000000)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь в переменной `vecs` у нас обычный питоновский `dict`, в котором ключи - это слова, а значения - вектора в 300-мерном пространстве.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nzero = sum(vecs.values()) / len(vecs)\ndef text2vec(text):\n  words = text.split()\n  return sum(list(map(lambda w: np.array(list(vecs.get(w, zero))), words))) / len(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Преобразуем `X_train` и `X_test` описанным выше способом.\n\n","metadata":{}},{"cell_type":"code","source":"X_train_vec = list(map(lambda text: text2vec(text), X_train))\nX_test_vec = list(map(lambda text: text2vec(text), X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg.fit(X_train_vec, y_train)\ny_pred_vec = logreg.predict(X_test_vec)\nprint(accuracy_score(y_test, y_pred_vec))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как мы видим, качество значительно упало. Это связано с тем, что усреднять слова в документе - не лучший способ получить его векторное представление. Есть много слов, которые встречаются часто и есть почти во всех документах. При таком способе подсчета они будут делать вектора документов слишком похожими друг на друга.\n\nЗначит, нам нужно давать словам разные веса, когда мы вычисляем вектор документа. Большой вес должен быть у слов, которые часто встречаются в этом документе, но редко в других. Но это как раз tf-idf. (Также можно использовать просто idf. TF - частота слова в документе - итак будет учтена, т.к. мы суммируем вектора слов в документе).\n\n\nОсталось понять, как посчитать такую взвешенную сумму векторов. Для этого можно составить матрицу из векторов слов, встречающихся в нашей коллекции (они обязательно должны идти в том же порядке, что и в матрице из `TfidfVectorizer` - но там слова будут столбцами, а тут - строками), после чего перемножить 2 матрицы.\n\n\nПусть\n\n- DOCS - число документов в коллекции\n\n- WORDS - число уникальных слов\n\n- DIM - размерность пространства эмбеддингов из Word2Vec\n\n\nТогда первая матрица будет размера `(DOCS, WORDS)`, вторая - `(WORDS, DIM)`, а их произведение - `(DOCS, DIM)` - это и будет матрица векторных представлений для каждого документа в коллекции.","metadata":{}},{"cell_type":"code","source":"dim = 300\nvocab = np.zeros((len(tfidf.vocabulary_.keys()), dim))\nfor key in tqdm(tfidf.vocabulary_.keys()):\n  vocab[tfidf.vocabulary_[key]] = vecs.get(key, zero)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_weighted = X_train_tfidf.dot(vocab)\nX_test_weighted = X_test_tfidf.dot(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg.fit(X_train_weighted, y_train)\ny_pred_weighted = logreg.predict(X_test_weighted)\nprint(accuracy_score(y_test, y_pred_weighted))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Заключение\nВ итоге мы понизили размерность матрицы из `TfidfVectorizer` не используя PCA или другие методы понижения размерности (которые часто плохо работают в задачах с текстами) и при этом получили качество выше, чем с использованием PCA и совсем немного ниже, чем при использовании исходной матрицы. В вашем соревновании лучше использовать X_train_weighted, а не X_train_vec, последний будет загружаться слишком долго, а мы ведь уже поняли, что первый даст результат лучше.","metadata":{}},{"cell_type":"markdown","source":"## Fun facts\nДавайте в vecs загрузим словарь поменьше","metadata":{}},{"cell_type":"code","source":"vecs = load_vectors('../input/crawluy/crawl-300d-2M.vec', 100000) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь в переменной `vecs` у нас обычный питоновский `dict`, в котором ключи - это слова, а значения - вектора в 300-мерном пространстве.\n\n\nЭто пространство хорошо тем, что в нем похожим по значению словам соответствуют похожие вектора - т.е. близкие друг к другу точки в 300-мерном пространстве.\n\n\nНапишем простую функцию `get_k_nearest_neighbors`, которая будет для данного вектора находить ближайшие к нему вектора в словаре.","metadata":{}},{"cell_type":"code","source":"def get_k_nearest_neighbors(vec, k):\n  return list(zip(*sorted(list(map(lambda key: (np.linalg.norm(vec - vecs[key]), key), vecs.keys())))))[1][:k]\n\nprint(get_k_nearest_neighbors(vecs['Paris'], 20))\nprint(get_k_nearest_neighbors(vecs['brother'], 20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видно из примера это условие выполняется - наиболее близкими к слову `Paris` оказались слова связанные с Францией и названия городов, в основном французских. Наиболее близкими к слову `brother` оказались слова, обозначающие родственников.\n\n\nУ эмбеддингов, получаемых с помощью Word2Vec есть еще одна интересная особенность - если мы можем сказать, что `А относится к B, как С к D`, то вектора слов `A - B` и `C - D` будут довольно похожи. Это значит, что если мы рассмотрим вектор `A - B + D`, то вектор `C` часто будет оказываться среди его ближайших соседей:","metadata":{}},{"cell_type":"code","source":"get_k_nearest_neighbors(vecs['Paris'] - vecs['France'] + vecs['Germany'], 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_k_nearest_neighbors(vecs['brother'] - vecs['man'] + vecs['woman'], 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Самым известным примером таких соотношений векторов является равенство\n\n`king - man + woman ≈ queen`. В данном словаре оно не совсем точно выполняется:","metadata":{}},{"cell_type":"code","source":"get_k_nearest_neighbors(vecs['king'] - vecs['man'] + vecs['woman'], 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}