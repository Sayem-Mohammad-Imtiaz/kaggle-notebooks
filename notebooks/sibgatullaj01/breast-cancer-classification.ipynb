{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breast Cancer Data Analysis and Prediction","metadata":{}},{"cell_type":"code","source":"#we will start with importing the essential libraries for data preprocessing.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:38.722885Z","iopub.execute_input":"2021-06-25T13:05:38.723219Z","iopub.status.idle":"2021-06-25T13:05:38.727718Z","shell.execute_reply.started":"2021-06-25T13:05:38.723143Z","shell.execute_reply":"2021-06-25T13:05:38.726821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns',50)\nsns.set_style('darkgrid')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:38.729325Z","iopub.execute_input":"2021-06-25T13:05:38.72963Z","iopub.status.idle":"2021-06-25T13:05:39.030731Z","shell.execute_reply.started":"2021-06-25T13:05:38.729565Z","shell.execute_reply":"2021-06-25T13:05:39.02984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.032335Z","iopub.execute_input":"2021-06-25T13:05:39.032907Z","iopub.status.idle":"2021-06-25T13:05:39.039088Z","shell.execute_reply.started":"2021-06-25T13:05:39.032862Z","shell.execute_reply":"2021-06-25T13:05:39.038073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Import the dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.040079Z","iopub.execute_input":"2021-06-25T13:05:39.040354Z","iopub.status.idle":"2021-06-25T13:05:39.052083Z","shell.execute_reply.started":"2021-06-25T13:05:39.040322Z","shell.execute_reply":"2021-06-25T13:05:39.051036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.053355Z","iopub.execute_input":"2021-06-25T13:05:39.053792Z","iopub.status.idle":"2021-06-25T13:05:39.077134Z","shell.execute_reply.started":"2021-06-25T13:05:39.05376Z","shell.execute_reply":"2021-06-25T13:05:39.076353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's have a look at the dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.078078Z","iopub.execute_input":"2021-06-25T13:05:39.078509Z","iopub.status.idle":"2021-06-25T13:05:39.081446Z","shell.execute_reply.started":"2021-06-25T13:05:39.078469Z","shell.execute_reply":"2021-06-25T13:05:39.08068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\ndata.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-25T13:05:39.083052Z","iopub.execute_input":"2021-06-25T13:05:39.083471Z","iopub.status.idle":"2021-06-25T13:05:39.129413Z","shell.execute_reply.started":"2021-06-25T13:05:39.083428Z","shell.execute_reply":"2021-06-25T13:05:39.128422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look at target: diagnosis\ndata['diagnosis'].value_counts().plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.132334Z","iopub.execute_input":"2021-06-25T13:05:39.132793Z","iopub.status.idle":"2021-06-25T13:05:39.266885Z","shell.execute_reply.started":"2021-06-25T13:05:39.132759Z","shell.execute_reply":"2021-06-25T13:05:39.266062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## dataset has 33 columns and 569 of total rows. Data has one categorical column 'diagnosis' which infact our target column. Id is the index value column and last column is 'unnamed:32' which appers to have lots of NA values. So next it is important to check the data for missing values.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.268715Z","iopub.execute_input":"2021-06-25T13:05:39.269314Z","iopub.status.idle":"2021-06-25T13:05:39.273082Z","shell.execute_reply.started":"2021-06-25T13:05:39.26925Z","shell.execute_reply":"2021-06-25T13:05:39.272375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.274135Z","iopub.execute_input":"2021-06-25T13:05:39.274551Z","iopub.status.idle":"2021-06-25T13:05:39.290858Z","shell.execute_reply.started":"2021-06-25T13:05:39.274512Z","shell.execute_reply":"2021-06-25T13:05:39.28985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## As we can see only one columns 'Unnamed:32' has missing value. So we will drop this column.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.29254Z","iopub.execute_input":"2021-06-25T13:05:39.292977Z","iopub.status.idle":"2021-06-25T13:05:39.302576Z","shell.execute_reply.started":"2021-06-25T13:05:39.292908Z","shell.execute_reply":"2021-06-25T13:05:39.301567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(columns=['id', 'Unnamed: 32'],axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.303952Z","iopub.execute_input":"2021-06-25T13:05:39.304593Z","iopub.status.idle":"2021-06-25T13:05:39.316723Z","shell.execute_reply.started":"2021-06-25T13:05:39.304545Z","shell.execute_reply":"2021-06-25T13:05:39.315787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let us have a look at trends in dataset.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.318064Z","iopub.execute_input":"2021-06-25T13:05:39.318702Z","iopub.status.idle":"2021-06-25T13:05:39.329694Z","shell.execute_reply.started":"2021-06-25T13:05:39.318659Z","shell.execute_reply":"2021-06-25T13:05:39.328563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-25T13:05:39.331061Z","iopub.execute_input":"2021-06-25T13:05:39.331732Z","iopub.status.idle":"2021-06-25T13:05:39.429924Z","shell.execute_reply.started":"2021-06-25T13:05:39.331687Z","shell.execute_reply":"2021-06-25T13:05:39.429009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can notice that mean and median has difference in every column. In some column this gap is more than the others. This gap indicates that data is skewed in some sense. \n#So let's plot the distribution of column to have better look at data.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.431225Z","iopub.execute_input":"2021-06-25T13:05:39.431746Z","iopub.status.idle":"2021-06-25T13:05:39.435307Z","shell.execute_reply.started":"2021-06-25T13:05:39.431704Z","shell.execute_reply":"2021-06-25T13:05:39.43421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for features in data._get_numeric_data().columns:\n    sns.distplot(data[features])\n    plt.title('Skew : '+str(np.round(data[features].skew(),4)))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:39.436732Z","iopub.execute_input":"2021-06-25T13:05:39.437113Z","iopub.status.idle":"2021-06-25T13:05:46.330463Z","shell.execute_reply.started":"2021-06-25T13:05:39.437072Z","shell.execute_reply":"2021-06-25T13:05:46.329386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Looking at the plots, it is clearly evident that most columns are not normally distributed and appear to be skewed. Some skewness have value more than +5.0. So it is important to deal with the skewnwess of data and bring data if not to normal distrubution, then at least close to it.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:46.33161Z","iopub.execute_input":"2021-06-25T13:05:46.331858Z","iopub.status.idle":"2021-06-25T13:05:46.336092Z","shell.execute_reply.started":"2021-06-25T13:05:46.331834Z","shell.execute_reply":"2021-06-25T13:05:46.335003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Two of many ways to deal with skewness are :Logarithmic transformation and Square root transformation. ","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:46.338793Z","iopub.execute_input":"2021-06-25T13:05:46.339102Z","iopub.status.idle":"2021-06-25T13:05:46.349864Z","shell.execute_reply.started":"2021-06-25T13:05:46.339071Z","shell.execute_reply":"2021-06-25T13:05:46.348908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for features in data._get_numeric_data().columns:\n    print(features, ' : ','skew in Log transformation :', np.round(np.log(data[features]).skew(),4),',', end='')\n    print('\\t','skew in Square root trasnformatioon', ' : ', np.round(np.sqrt(data[features]).skew(),4))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:46.351015Z","iopub.execute_input":"2021-06-25T13:05:46.351327Z","iopub.status.idle":"2021-06-25T13:05:46.423965Z","shell.execute_reply.started":"2021-06-25T13:05:46.351267Z","shell.execute_reply":"2021-06-25T13:05:46.423036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Looking at above output, it is evident that logarithmic transformation has dealt with skewness in a better way than the square root transformation. But logarithmic transforamtion has some output as 'NAN'. This is becuase those columns have zeroes as value in it and log of zero is undefined. On the other hand square root transformation handles these columns well.\n## So We will use Square root transormation in columns that have '0' as values and log for rest.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:46.425153Z","iopub.execute_input":"2021-06-25T13:05:46.425407Z","iopub.status.idle":"2021-06-25T13:05:46.428312Z","shell.execute_reply.started":"2021-06-25T13:05:46.425383Z","shell.execute_reply":"2021-06-25T13:05:46.42753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for features in data._get_numeric_data().columns:\n    plt.figure(figsize=(6,6))\n    if 0 in data[features].unique():\n        data[features]=np.round(np.sqrt(data[features]),4)\n        sns.distplot(data[features])\n        plt.title('Skew : '+str(np.round(data[features].skew(),4)))\n        plt.show()\n    else:\n        data[features]=np.round(np.log(data[features]),4)\n        sns.distplot(data[features])\n        plt.title('Skew : '+str(np.round(data[features].skew(),4)))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:46.429058Z","iopub.execute_input":"2021-06-25T13:05:46.429415Z","iopub.status.idle":"2021-06-25T13:05:53.257531Z","shell.execute_reply.started":"2021-06-25T13:05:46.429376Z","shell.execute_reply":"2021-06-25T13:05:53.256633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## All the features have skewness in the range of +-0.5 which is much less than what we had earlier.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:53.258878Z","iopub.execute_input":"2021-06-25T13:05:53.259122Z","iopub.status.idle":"2021-06-25T13:05:53.262751Z","shell.execute_reply.started":"2021-06-25T13:05:53.259097Z","shell.execute_reply":"2021-06-25T13:05:53.261699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we will look at correlation between independent variables in data.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:53.264248Z","iopub.execute_input":"2021-06-25T13:05:53.264638Z","iopub.status.idle":"2021-06-25T13:05:53.274598Z","shell.execute_reply.started":"2021-06-25T13:05:53.264597Z","shell.execute_reply":"2021-06-25T13:05:53.27364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:53.278616Z","iopub.execute_input":"2021-06-25T13:05:53.279068Z","iopub.status.idle":"2021-06-25T13:05:56.914568Z","shell.execute_reply.started":"2021-06-25T13:05:53.279023Z","shell.execute_reply":"2021-06-25T13:05:56.913749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## WOW, this tells a completely diffent story about data. Many features are strongly correlated and data show high level of multi-collinearity.\n## One way to decrease multi-collinarity is by dropping features that show strong correlation(>0.85). So we prepare a list of columns with strong correlation value and drop them.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:56.915838Z","iopub.execute_input":"2021-06-25T13:05:56.916304Z","iopub.status.idle":"2021-06-25T13:05:56.919672Z","shell.execute_reply.started":"2021-06-25T13:05:56.916258Z","shell.execute_reply":"2021-06-25T13:05:56.91866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_2_Drop=['perimeter_mean','area_mean','concavity_mean','concave points_mean','radius_se','area_se','compactness_se','concavity_se','radius_worst','area_worst','perimeter_worst','compactness_worst','concave points_worst','texture_worst']","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:56.923641Z","iopub.execute_input":"2021-06-25T13:05:56.924033Z","iopub.status.idle":"2021-06-25T13:05:56.933549Z","shell.execute_reply.started":"2021-06-25T13:05:56.923993Z","shell.execute_reply":"2021-06-25T13:05:56.932689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,12))\nsns.heatmap(data.drop(columns=col_2_Drop).corr(),annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:56.935055Z","iopub.execute_input":"2021-06-25T13:05:56.935606Z","iopub.status.idle":"2021-06-25T13:05:58.212683Z","shell.execute_reply.started":"2021-06-25T13:05:56.935566Z","shell.execute_reply":"2021-06-25T13:05:58.21201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Much better now. Though there are fewer features than earlier but data seems promising.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.213676Z","iopub.execute_input":"2021-06-25T13:05:58.214127Z","iopub.status.idle":"2021-06-25T13:05:58.217382Z","shell.execute_reply.started":"2021-06-25T13:05:58.214084Z","shell.execute_reply":"2021-06-25T13:05:58.216362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's set our target variable now and prepare the for next process.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.218684Z","iopub.execute_input":"2021-06-25T13:05:58.218963Z","iopub.status.idle":"2021-06-25T13:05:58.232838Z","shell.execute_reply.started":"2021-06-25T13:05:58.218934Z","shell.execute_reply":"2021-06-25T13:05:58.23148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target=data['diagnosis']\ndataset=data.drop(columns=col_2_Drop+['diagnosis'])","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.234174Z","iopub.execute_input":"2021-06-25T13:05:58.234509Z","iopub.status.idle":"2021-06-25T13:05:58.249725Z","shell.execute_reply.started":"2021-06-25T13:05:58.234475Z","shell.execute_reply":"2021-06-25T13:05:58.248588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.251059Z","iopub.execute_input":"2021-06-25T13:05:58.251343Z","iopub.status.idle":"2021-06-25T13:05:58.279651Z","shell.execute_reply.started":"2021-06-25T13:05:58.251314Z","shell.execute_reply":"2021-06-25T13:05:58.278667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## now that we have dealt with correlation and normality. Let's split the data set into a train and a test set.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.280999Z","iopub.execute_input":"2021-06-25T13:05:58.28131Z","iopub.status.idle":"2021-06-25T13:05:58.291156Z","shell.execute_reply.started":"2021-06-25T13:05:58.281281Z","shell.execute_reply":"2021-06-25T13:05:58.290094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset,target, test_size=.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.292313Z","iopub.execute_input":"2021-06-25T13:05:58.292583Z","iopub.status.idle":"2021-06-25T13:05:58.353287Z","shell.execute_reply.started":"2021-06-25T13:05:58.292555Z","shell.execute_reply":"2021-06-25T13:05:58.352509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now we have a train and test. Lets scale them and bring whole data on same level.\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.354639Z","iopub.execute_input":"2021-06-25T13:05:58.355017Z","iopub.status.idle":"2021-06-25T13:05:58.367264Z","shell.execute_reply.started":"2021-06-25T13:05:58.354978Z","shell.execute_reply":"2021-06-25T13:05:58.366537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Our target is a categorical data with labels M and B. We have to encode these labels as 0 and 1.\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ny_train=le.fit_transform(y_train)\ny_test=le.transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.368794Z","iopub.execute_input":"2021-06-25T13:05:58.369386Z","iopub.status.idle":"2021-06-25T13:05:58.385752Z","shell.execute_reply.started":"2021-06-25T13:05:58.369342Z","shell.execute_reply":"2021-06-25T13:05:58.38475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.386872Z","iopub.execute_input":"2021-06-25T13:05:58.387312Z","iopub.status.idle":"2021-06-25T13:05:58.445543Z","shell.execute_reply.started":"2021-06-25T13:05:58.387249Z","shell.execute_reply":"2021-06-25T13:05:58.444712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name=[]\nmean_validadtion_score=[]\ntraining_score=[]\ntest_accuracy_score=[]\ndef algorithm(models,X_train,y_train,X_test, Y_test):\n    for model in models:\n        model=model()\n        model.fit(X_train,y_train)\n        score=cross_val_score(model,X_train,y_train,cv=5)\n        print('Mean cross-validation Score for',model,' is :',score.mean())\n        y_pred=model.predict(X_test)\n        print('Training score of ',model,' is :',model.score(X_train,y_train))\n        print('Accuracy score of ',model,' is :', accuracy_score(y_test,y_pred))\n        print('\\n','Confusion matrix \\n',confusion_matrix(y_test,y_pred))\n        print('\\n')\n        print('-'*100)\n        print('\\n')\n        model_name.append(model)\n        mean_validadtion_score.append(np.round(score.mean(),4))\n        training_score.append(np.round(model.score(X_train,y_train),4))\n        test_accuracy_score.append(np.round(accuracy_score(y_test,y_pred),4))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.446549Z","iopub.execute_input":"2021-06-25T13:05:58.446944Z","iopub.status.idle":"2021-06-25T13:05:58.453856Z","shell.execute_reply.started":"2021-06-25T13:05:58.446903Z","shell.execute_reply":"2021-06-25T13:05:58.452893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models=[LogisticRegression,SVC,GaussianNB,KNeighborsClassifier,DecisionTreeClassifier,RandomForestClassifier]\nalgorithm(models,X_train,y_train,X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:58.455036Z","iopub.execute_input":"2021-06-25T13:05:58.455403Z","iopub.status.idle":"2021-06-25T13:05:59.813124Z","shell.execute_reply.started":"2021-06-25T13:05:58.455364Z","shell.execute_reply":"2021-06-25T13:05:59.812144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['Models']=[str(x) for x in model_name]\ndf['Mean cross-val-score']=mean_validadtion_score\ndf['Training Score']=training_score\ndf['Accuracy Score']=test_accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:59.814292Z","iopub.execute_input":"2021-06-25T13:05:59.814548Z","iopub.status.idle":"2021-06-25T13:05:59.823609Z","shell.execute_reply.started":"2021-06-25T13:05:59.814523Z","shell.execute_reply":"2021-06-25T13:05:59.822552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-06-25T13:05:59.824815Z","iopub.execute_input":"2021-06-25T13:05:59.825106Z","iopub.status.idle":"2021-06-25T13:05:59.848574Z","shell.execute_reply.started":"2021-06-25T13:05:59.825079Z","shell.execute_reply":"2021-06-25T13:05:59.847468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see the performance of different models in above dataframe.\n### Linear models have consistent score in all three scoring.\n### One can choose SVC or LogisticRegression and tune it with hyperparameter for further procees.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}