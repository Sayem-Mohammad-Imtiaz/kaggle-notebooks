{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import ShuffleSplit\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Load the Boston housing dataset\ndata = pd.read_csv('../input/bostonhoustingmlnd/housing.csv')\nprices = data['MEDV']\nfeatures = data.drop('MEDV', axis = 1)\n    \n# Success\nprint (\"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n###########################################\n\nimport matplotlib.pyplot as pl\nimport numpy as np\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import ShuffleSplit, train_test_split\n\ndef ModelLearning(X, y):\n    \"\"\" Calculates the performance of several models with varying sizes of training data.\n        The learning and testing scores for each model are then plotted. \"\"\"\n\n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Generate the training set sizes increasing by 50\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n\n    # Create the figure window\n    fig = pl.figure(figsize=(10,7))\n\n    # Create three different models based on max_depth\n    for k, depth in enumerate([1,3,6,10]):\n\n        # Create a Decision tree regressor at max_depth = depth\n        regressor = DecisionTreeRegressor(max_depth = depth)\n\n        # Calculate the training and testing scores\n        sizes, train_scores, test_scores = learning_curve(regressor, X, y, \\\n            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n\n        # Find the mean and standard deviation for smoothing\n        train_std = np.std(train_scores, axis = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n\n        # Subplot the learning curve\n        ax = fig.add_subplot(2, 2, k+1)\n        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n        ax.fill_between(sizes, train_mean - train_std, \\\n            train_mean + train_std, alpha = 0.15, color = 'r')\n        ax.fill_between(sizes, test_mean - test_std, \\\n            test_mean + test_std, alpha = 0.15, color = 'g')\n\n        # Labels\n        ax.set_title('max_depth = %s'%(depth))\n        ax.set_xlabel('Number of Training Points')\n        ax.set_ylabel('Score')\n        ax.set_xlim([0, X.shape[0]*0.8])\n        ax.set_ylim([-0.05, 1.05])\n\n    # Visual aesthetics\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize = 16, y = 1.03)\n    fig.tight_layout()\n    fig.show()\n\n\ndef ModelComplexity(X, y):\n    \"\"\" Calculates the performance of the model as model complexity increases.\n        The learning and testing errors rates are then plotted. \"\"\"\n\n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Vary the max_depth parameter from 1 to 10\n    max_depth = np.arange(1,11)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = validation_curve(DecisionTreeRegressor(), X, y, \\\n        param_name = \"max_depth\", param_range = max_depth, cv = cv, scoring = 'r2')\n\n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot the validation curve\n    pl.figure(figsize=(7, 5))\n    pl.title('Decision Tree Regressor Complexity Performance')\n    pl.plot(max_depth, train_mean, 'o-', color = 'r', label = 'Training Score')\n    pl.plot(max_depth, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    pl.fill_between(max_depth, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    pl.fill_between(max_depth, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n\n    # Visual aesthetics\n    pl.legend(loc = 'lower right')\n    pl.xlabel('Maximum Depth')\n    pl.ylabel('Score')\n    pl.ylim([-0.05,1.05])\n    pl.show()\n\n\ndef PredictTrials(X, y, fitter, data):\n    \"\"\" Performs trials of fitting and predicting data. \"\"\"\n\n    # Store the predicted prices\n    prices = []\n\n    for k in range(10):\n        # Split the data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n            test_size = 0.2, random_state = k)\n\n        # Fit the data\n        reg = fitter(X_train, y_train)\n\n        # Make a prediction\n        pred = reg.predict([data[0]])[0]\n        prices.append(pred)\n\n        # Result\n        print(\"Trial {}: ${:,.2f}\".format(k+1, pred))\n\n    # Display price range\n    print(\"\\nRange in prices: ${:,.2f}\".format(max(prices) - min(prices)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Minimum price of the data\nminimum_price = np.min(prices)\n\n# Maximum price of the data\nmaximum_price = np.max(prices)\n\n# Mean price of the data\nmean_price = np.mean(prices)\n\n# Median price of the data\nmedian_price = np.median(prices)\n\n# Standard deviation of prices of the data\nstd_price = np.std(prices)\n\n# Calculated statistics\nprint (\"Statistics for Boston housing dataset:\\n\")\nprint (\"Minimum price: ${:,.2f}\".format(minimum_price))\nprint (\"Maximum price: ${:,.2f}\".format(maximum_price))\nprint (\"Mean price: ${:,.2f}\".format(mean_price))\nprint (\"Median price ${:,.2f}\".format(median_price))\nprint (\"Standard deviation of prices: ${:,.2f}\".format(std_price))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nclr = ['blue', 'green', 'red']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3,figsize=(15,3))\n\nplt.figure(1)\n\nfor i, var in enumerate(['RM', 'LSTAT', 'PTRATIO']):\n    plt.subplot(131 + i)\n    sns.distplot(data[var],  color = clr[i])\n    plt.axvline(data[var].mean(), color=clr[i], linestyle='solid', linewidth=2)\n    plt.axvline(data[var].median(), color=clr[i], linestyle='dashed', linewidth=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3,figsize=(15,3))\n\nplt.figure(1)\n\nfor i, var in enumerate(['RM', 'LSTAT', 'PTRATIO']):\n    plt.subplot(131 + i)\n    if i==0:\n        sns.distplot(data[var],  color = clr[i])\n        plt.axvline(data[var].mean(), color=clr[i], linestyle='solid', linewidth=2)\n        plt.axvline(data[var].median(), color=clr[i], linestyle='dashed', linewidth=2)\n    else:\n        sns.distplot(np.log(data[var]), color = clr[i])\n        plt.axvline(np.log(data[var]).mean(), color=clr[i], linestyle='solid', linewidth=2)\n        plt.axvline(np.log(data[var]).median(), color=clr[i], linestyle='dashed', linewidth=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3,figsize=(15,3))\n\nfor i, var in enumerate(['RM', 'LSTAT', 'PTRATIO']):\n    lm = sns.regplot(data[var], prices, ax = axs[i], color=clr[i])\n    lm.set(ylim=(0, None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3,figsize=(15,3))\n\nfor i, var in enumerate(['RM', 'LSTAT', 'PTRATIO']):\n    lm = sns.regplot(np.log(data[var]), prices, ax = axs[i], color=clr[i])\n    lm.set(ylim=(0, None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr(), square=True,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    \n    score = r2_score(y_true, y_predict)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nprint (\"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = pd.DataFrame([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3]).reset_index()\nsample_df.columns = ['True Value', 'Prediction']\nsns.regplot('True Value', 'Prediction', sample_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(features, prices, \n                                                    test_size=30, random_state=0)\n\n# Success\nprint (\"Training and testing split was successful.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def forward(x, w):\n    out = x.T * w\n    return out\n\ndef backward(x, y, dout):\n    dx = dout * y\n    dy = dout * x.T\n    return dx, dy\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#X_train, X_test, y_train, y_test\nLn = 1.1\nZero2One = forward(X_test, y_test)\n#Zero2One = sigmoid(Zero2One)\nprint(Zero2One)\nOne2Two = forward(Zero2One, Ln)\n#One2Two = sigmoid(One2Two)\nprint(One2Two)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''# back propagation\ndOne2Two = 1\ndZero2One, dLn = backward(X_test, y_test, dOne2Two)\ndX_test, dy_test = backward(X_test, y_test, dZero2One)\n\nprint(dX_test)\nprint()\nprint(dy_test)\nprint()\nprint(dLn)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nX_train=np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test=np.array(y_test)\n\nW1 = W2 = W3 = b = 0.0\n\nn_data = len(X_test)\nepochs = 150\nlearning_rate = 0.01\n\nfor i in tqdm(range(epochs)):\n    gradient_w1 = np.sum((X_test.T[0] * W1 - y_test.T[0] + b) * 2 * X_test.T[0]) / n_data\n    gradient_w2 = np.sum((X_test.T[1] * W2 - y_test.T[1] + b) * 2 * X_test.T[1]) / n_data\n    gradient_w3 = np.sum((X_test.T[2] * W3 - y_test.T[2] + b) * 2 * X_test.T[2]) / n_data\n    gradient_b = np.sum((X_test.T[0] * W1 + X_test.T[1] * W2 + X_test.T[2] * W3 - y_test.T + b) * 2) / n_data\n\n    W1 -= learning_rate * gradient_w1\n    W2 -= learning_rate * gradient_w2\n    W3 -= learning_rate * gradient_w3\n    b -= learning_rate * gradient_b\n\nprint(W1)\nprint(W2)\nprint(W3)\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.dot(np.array([W1, W2, W3]).T) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test=np.array(y_test)\n\nlr = 0.01\n\ndef Hyp(w, b, x):\n    return x.dot(w.T) + b\ndef MSE(w, b, x, y):\n    n = len(x)\n    return 1/n * ((Hyp(w, b, x)-y)**2).sum()\ndef dMSE(w, b, x, y):\n    dS = [x[0], x[1], x[2], 1]\n    Synfunc = 2/len(X_train) * (Hyp(w, b, x) - y)\n    \n    dy = []\n    for i in range(4):\n        dy.append(Synfunc * dS[i])\n    \n    return dy\n\nw = np.array([0.0, 0.0, 0.0])\nb = 0.0\n\nfor _ in range(2000):\n    \n    for i in range(n):\n        grad = dMSE(w, b, X_train[i], y_train[i])\n        w[0] -= lr * grad[0]\n        w[1] -= lr * grad[1]\n        w[2] -= lr * grad[2]\n        b -= lr * grad[3]\n    \n    print(\"%d Epoch, Cost) %6f, Cost_test) %6f\"%(_+1, MSE(w, b, X_train, y_train), MSE(w, b, X_test, y_test)))\n\nprint(w)\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W = (np.linalg.inv((X_train.T).dot(X_train)).dot(X_train.T).dot(y_train))\nprint(W)\nprint(MSE(W, 0, X_train, y_train))\nprint(12423055095.874508 - 8166308337.510345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dMSE(w, b, X_train[i], y_train[i]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}