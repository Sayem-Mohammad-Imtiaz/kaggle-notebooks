{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Brazilian rent price: Data analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Motivation\nThis notebook is going to be focused on solving the problem of predicting house rent in five cities of Brazil.\n\nA house value is simply more than location and square footage. We are going to take advantage of all of the feature variables available to use and use it to analyze and predict house rent prices.\n\nWe are going to break everything into logical steps that allow us to ensure the cleanest, most realistic data for our model to make accurate predictions from.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Understanding the Client and their Problem\n\nA benefit to this study is that we can have two clients at the same time! (Think of being a divorce lawyer for both interested parties) However, in this case, we can have both clients with no conflict of interest!\n\nClient Housebuyer: This client wants to find their next dream home with a reasonable price tag. They have their locations of interest ready. Now, they want to know if the house price matches the house value. With this study, they can understand which features (ex. Number of bathrooms, location, etc.) influence the final price of the house. If all matches, they can ensure that they are getting a fair price.\n\nClient Houseseller: Think of the average house-flipper. This client wants to take advantage of the features that influence a house price the most. They typically want to buy a house at a low price and invest on the features that will give the highest return. For example, buying a house at a good location but small square footage. The client will invest on making rooms at a small cost to get a large return.\nData\n\nOur data comes from a Kaggle competition named “brazilian_houses_to_rent”, which is a dataset houses to rent in diferents cities in Brazil.\nIt contains 10962 training data points and 13 features that might help us predict the selling price of a house.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading Data and Packages","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom scipy.stats.stats import pearsonr\nfrom scipy.stats import norm\nfrom collections import Counter\nfrom sklearn.linear_model import LinearRegression,LassoCV, Ridge, LassoLarsCV,ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\nwarnings.filterwarnings('ignore')\nsns.set(style='white', context='notebook', palette='deep')\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename1 = \"/kaggle/input/brasilian-houses-to-rent/houses_to_rent.csv\"\nfilename2 = \"/kaggle/input/brasilian-houses-to-rent/houses_to_rent_v2.csv\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(filename2)\n\n# take a look at the dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unique cities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cities = df['city'].unique()\ncities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['total (R$)'].describe().apply(lambda x: format(x, 'f'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['total (R$)']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems a very strange price distribuiton. Existe preços muito altos, prém são preços 'fora da curva'. Vamos identifica-los","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"big_price = df[df['total (R$)'] > 50000]\nbig_price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que são apenas 7 casa. Além disso, 255 e 6979 parecem ser o mesmo dado. Vamos retirar esses 7 dados da tabela","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df.drop(big_price.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['total (R$)'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_clean['total (R$)']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_clean['total (R$)'], fit=norm);\n\n(mu, sigma) = norm.fit(df_clean['total (R$)'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('total (R$) distribution')\n\nfig = plt.figure()\nres = stats.probplot(df_clean['total (R$)'], plot=plt)\nplt.show()\n\nprint(\"Skewness: %f\" % df_clean['total (R$)'].skew())\nprint(\"Kurtosis: %f\" % df_clean['total (R$)'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like a normal distribution? Not quite! Looking at the kurtosis score, we can see that there is a very nice peak. However, looking at the skewness score, we can see that the sale prices deviate from the normal distribution. Going to have to fix this later! We want our data to be as \"normal\" as possible.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a figure and axes. Then plot the data\n#fig, ax = plt.subplots()\n#sns.distplot(df['fmr_1'], ax=ax)\n\n# Customize the labels and limits\n#ax.set(xlabel=\"1 Bedroom Fair Market Rent\", xlim=(100,1500), title=\"US Rent\")\n\n# Add vertical lines for the median and mean\n#ax.axvline(x=median, color='m', label='Median', linestyle='--', linewidth=2) #shows the median)\n#ax.axvline(x=mean, color='b', label='Mean', linestyle='-', linewidth=2) #shows the mean\n\n# Show the legend and plot the data\n#ax.legend()\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total = df_clean.isnull().sum().sort_values(ascending=False)\npercent = (df_clean.isnull().sum()/df_clean.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing data! But is it true?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['floor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 2461 a strange value '-'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_floor = df_clean[df_clean['floor'] == '-']\ndf_floor['total (R$)'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_floor['total (R$)']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['area'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_area = df_clean[df_clean['area'] > 1200]\nbig_area","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kind of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.select_dtypes(include=['int64','float64']).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Variables ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Area vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x = 'area',y='total (R$)',data=df_clean,kind='scatter',size='total (R$)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, here we have some problems, there are some strange data. Let's find and drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean_v2 = df_clean.drop(big_area.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x = 'area',y='total (R$)',data=df_clean_v2,kind='scatter',size='total (R$)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You might’ve expected that larger living area should mean a higher price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### City vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'city'\ndata = pd.concat([df_clean_v3['total (R$)'], df_clean_v3[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=var, y=\"total (R$)\", data=df_clean_v3)\nfig.axis(ymin=0, ymax=35000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Claramente, São paulo tem uma média de preço maior que as outras cidade","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Rooms vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'rooms'\ndata = pd.concat([df_clean_v3['total (R$)'], df_clean_v3[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=var, y=\"total (R$)\", data=df_clean_v3)\nfig.axis(ymin=0, ymax=35000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Animal vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x='city', hue='animal', data=df, palette='RdBu')\nplt.xticks([0,1,2,3,4], cities)\nplt.show()\n#gostara de fazer um gráfico barra de cada cidade escrevendo numero de aceitações e nã aceitações","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'animal'\ndata = pd.concat([df_clean_v3['total (R$)'], df_clean_v3[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=var, y=\"total (R$)\", data=df_clean_v3)\nfig.axis(ymin=0, ymax=35000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Furniture vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x='city', hue='furniture', data=df, palette='RdBu')\nplt.xticks([0,1,2,3,4], cities)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'furniture'\ndata = pd.concat([df_clean_v3['total (R$)'], df_clean_v3[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=var, y=\"total (R$)\", data=df_clean_v3)\nfig.axis(ymin=0, ymax=35000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bathrooms vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'bathroom'\ndata = pd.concat([df_clean_v3['total (R$)'], df_clean_v3[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=var, y=\"total (R$)\", data=df_clean_v3)\nfig.axis(ymin=0, ymax=35000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parking spaces vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'parking spaces'\ndata = pd.concat([df_clean_v3['total (R$)'], df_clean_v3[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 8))\nfig = sns.boxplot(x=var, y=\"total (R$)\", data=df_clean_v3)\nfig.axis(ymin=0, ymax=35000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['total (R$)', 'rooms', 'bathroom', 'area']\nsns.pairplot(df_clean_v3[cols], size = 4);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = df_clean_v3.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# São Paulo","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sp = df_clean_v2[df_clean_v2['city'] == 'São Paulo']\ndf_sp.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Belo Horizonte","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bh = df_clean_v2[df_clean_v2['city'] == 'Belo Horizonte']\ndf_bh.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Porto Alegre","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_poa = df_clean_v2[df_clean_v2['city'] == 'Porto Alegre']\ndf_poa.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Campinas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_camp = df_clean_v2[df_clean_v2['city'] == 'Campinas']\ndf_camp.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rio de Janeiro","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rj = df_clean_v2[df_clean_v2['city'] == 'Rio de Janeiro']\ndf_rj.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean_v3[\"total (R$)\"] = np.log1p(df_clean_v3[\"total (R$)\"])\n\nsns.distplot(df_clean_v3[\"total (R$)\"] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_clean_v3[\"total (R$)\"])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(df_clean_v3[\"total (R$)\"], plot=plt)\nplt.show()\n\ny_train = df_clean_v3['total (R$)'].values\n\nprint(\"Skewness: %f\" % df_clean_v3[\"total (R$)\"].skew())\nprint(\"Kurtosis: %f\" % df_clean_v3[\"total (R$)\"].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df_clean_v3['area']\ny = df_clean_v3['total (R$)']\n\nx = (x - x.mean()) / x.std()\nx = np.c_[np.ones(x.shape[0]), x] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(h, y):\n  sq_error = (h - y)**2\n  n = len(y)\n  return 1.0 / (2*n) * sq_error.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import unittest\n\nclass TestLoss(unittest.TestCase):\n\n  def test_zero_h_zero_y(self):\n    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([0])), 0)\n\n  def test_one_h_zero_y(self):\n    self.assertAlmostEqual(loss(h=np.array([1]), y=np.array([0])), 0.5)\n\n  def test_two_h_zero_y(self):\n    self.assertAlmostEqual(loss(h=np.array([2]), y=np.array([0])), 2)\n    \n  def test_zero_h_one_y(self):\n    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([1])), 0.5)\n    \n  def test_zero_h_two_y(self):\n    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([2])), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_tests():\n  unittest.main(argv=[''], verbosity=1, exit=False)\n\nrun_tests()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearRegression:\n  \n  def predict(self, X):\n    return np.dot(X, self._W)\n  \n  def _gradient_descent_step(self, X, targets, lr):\n\n    predictions = self.predict(X)\n    \n    error = predictions - targets\n    gradient = np.dot(X.T,  error) / len(X)\n\n    self._W -= lr * gradient\n      \n  def fit(self, X, y, n_iter=100000, lr=0.01):\n\n    self._W = np.zeros(X.shape[1])\n\n    self._cost_history = []\n    self._w_history = [self._W]\n    for i in range(n_iter):\n      \n        prediction = self.predict(X)\n        cost = loss(prediction, y)\n        \n        self._cost_history.append(cost)\n        \n        self._gradient_descent_step(x, y, lr)\n        \n        self._w_history.append(self._W.copy())\n    return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestLinearRegression(unittest.TestCase):\n\n    def test_find_coefficients(self):\n      clf = LinearRegression()\n      clf.fit(x, y, n_iter=2000, lr=0.01)\n      np.testing.assert_array_almost_equal(clf._W, np.array([180921.19555322,  56294.90199925]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_tests()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LinearRegression()\nclf.fit(x, y, n_iter=2000, lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf._W","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Cost Function J')\nplt.xlabel('No. of iterations')\nplt.ylabel('Cost')\nplt.plot(clf._cost_history)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df_clean_v3[['rooms', 'bathroom', 'area']]\n\nx = (x - x.mean()) / x.std()\nx = np.c_[np.ones(x.shape[0]), x] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LinearRegression()\nclf.fit(x, y, n_iter=2000, lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf._W","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for null values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check correlation\ndf.corr().style.background_gradient(cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see above, apart from the (R$) values the features that make the most impact are rooms, bathrooms and parking spaces.\n\nTo further check the correlation of the features and the features that are most important to determine the Total, let us take the approach of Backward Elimination Algorithm by using the Ordinary Least Square method to find the summary of the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets select some features to explore more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf = df[['area','rooms','bathroom','parking spaces','total (R$)']]\ncdf.head(9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can plot each of these features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"viz = cdf[['rooms','bathroom','parking spaces']]\nviz.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets plot each of these features vs price, to see how linear is their relation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf['totalprice'] = cdf['total (R$)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(cdf.rooms, cdf.totalprice,  color='blue')\nplt.xlabel(\"Number of rooms\")\nplt.ylabel(\"Total Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(cdf.bathroom, cdf.totalprice,  color='blue')\nplt.xlabel(\"Number of Bathrooms\")\nplt.ylabel(\"Total Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(cdf['parking spaces'], cdf.totalprice,  color='blue')\nplt.xlabel(\"Number of Bathrooms\")\nplt.ylabel(\"Total Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating train and test dataset\n\nTrain/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. This will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the data. It is more realistic for real world problems.\n\nThis means that we know the outcome of each data point in this dataset, making it great to test with! And since this data has not been used to train the model, the model has no knowledge of the outcome of these data points. So, in essence, it is truly an out-of-sample testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msk = np.random.rand(len(df)) < 0.8\ntrain = cdf[msk]\ntest = cdf[~msk]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train.rooms, train.totalprice,  color='blue')\nplt.xlabel(\"Rooms\")\nplt.ylabel(\"Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nregr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['rooms']])\ntrain_y = np.asanyarray(train[['totalprice']])\nregr.fit (train_x, train_y)\n# The coefficients\nprint ('Coefficients: ', regr.coef_)\nprint ('Intercept: ',regr.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train.rooms, train.totalprice,  color='blue')\nplt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')\nplt.xlabel(\"Rooms\")\nplt.ylabel(\"Price\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ntest_x = np.asanyarray(test[['rooms']])\ntest_y = np.asanyarray(test[['totalprice']])\ntest_y_ = regr.predict(test_x)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y_ , test_y) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nregr = linear_model.LinearRegression()\nx = np.asanyarray(train[['rooms','bathroom','parking spaces']])\ny = np.asanyarray(train[['totalprice']])\nregr.fit (x, y)\n# The coefficients\nprint ('Coefficients: ', regr.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned before, __Coefficient__ and __Intercept__ , are the parameters of the fit line. \nGiven that it is a multiple linear regression, with 3 parameters, and knowing that the parameters are the intercept and coefficients of hyperplane, sklearn can estimate them from our data. Scikit-learn uses plain Ordinary Least Squares method to solve this problem.\n\n#### Ordinary Least Squares (OLS)\nOLS is a method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function. In other words, it tries to minimizes the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output ($\\hat{y}$) over all samples in the dataset.\n\nOLS can find the best parameters using of the following methods:\n    - Solving the model parameters analytically using closed-form equations\n    - Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton’s Method, etc.)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2 id=\"prediction\">Prediction</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat= regr.predict(test[['rooms','bathroom','parking spaces']])\nx = np.asanyarray(test[['rooms','bathroom','parking spaces']])\ny = np.asanyarray(test[['totalprice']])\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((y_hat - y) ** 2))\n\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(x, y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as smf\n\n# Add a new column with educ squared\ngss['educ2'] = gss['educ']**2\n\n# Run a regression model with educ, educ2, age, and age2\nresults = smf.ols('realinc ~ educ + educ2 + age + age2',data = gss).fit()\n\n# Print the estimated parameters\nprint(results.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run a regression model with educ, educ2, age, and age2\nresults = smf.ols('realinc ~ educ + educ2 + age + age2', data=gss).fit()\n\n# Make the DataFrame\ndf = pd.DataFrame()\ndf['educ'] = np.linspace(0,20)\ndf['age'] = 30\ndf['educ2'] = df['educ']**2\ndf['age2'] = df['age']**2\n\n# Generate and plot the predictions\npred = results.predict(df)\nprint(pred.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot mean income in each age group\nplt.clf()\ngrouped = gss.groupby('educ')\nmean_income_by_educ = grouped['realinc'].mean()\nplt.plot(mean_income_by_educ,'o',alpha=0.5)\n\n# Plot the predictions\npred = results.predict(df)\nplt.plot(df['educ'], pred, label='Age 30')\n\n# Label axes\nplt.xlabel('Education (years)')\nplt.ylabel('Income (1986 $)')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjust to add subgroups based on \"Interested in Pets\"\ng = sns.catplot(x=\"Gender\",\n                y=\"Age\", data=survey_data, \n                kind=\"box\", hue='Interested in Pets')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}