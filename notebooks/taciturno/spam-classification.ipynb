{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# some necessary imports\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, linear_model, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom matplotlib import pyplot as plt\n# using plotly since it's very clear to interpret, though seems complicated to code \nfrom plotly import subplots\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.497095Z","iopub.execute_input":"2021-09-20T08:25:44.497926Z","iopub.status.idle":"2021-09-20T08:25:44.509008Z","shell.execute_reply.started":"2021-09-20T08:25:44.497862Z","shell.execute_reply":"2021-09-20T08:25:44.50799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.511016Z","iopub.execute_input":"2021-09-20T08:25:44.511278Z","iopub.status.idle":"2021-09-20T08:25:44.548354Z","shell.execute_reply.started":"2021-09-20T08:25:44.511249Z","shell.execute_reply":"2021-09-20T08:25:44.547194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Category'].unique() ","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.549811Z","iopub.execute_input":"2021-09-20T08:25:44.550103Z","iopub.status.idle":"2021-09-20T08:25:44.558215Z","shell.execute_reply.started":"2021-09-20T08:25:44.550073Z","shell.execute_reply":"2021-09-20T08:25:44.557161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = [1 if el == 'spam' else 0 for el in train_df['Category']]","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.561039Z","iopub.execute_input":"2021-09-20T08:25:44.561513Z","iopub.status.idle":"2021-09-20T08:25:44.572222Z","shell.execute_reply.started":"2021-09-20T08:25:44.561465Z","shell.execute_reply":"2021-09-20T08:25:44.571175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info() ","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.573453Z","iopub.execute_input":"2021-09-20T08:25:44.573788Z","iopub.status.idle":"2021-09-20T08:25:44.596135Z","shell.execute_reply.started":"2021-09-20T08:25:44.573745Z","shell.execute_reply":"2021-09-20T08:25:44.594846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missed value so we don't need to handle it","metadata":{}},{"cell_type":"code","source":"train_texts, valid_texts, train_y, valid_y = \\\n        train_test_split(train_df['Message'], train_df['Category'], random_state=5, train_size=.75)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.597392Z","iopub.execute_input":"2021-09-20T08:25:44.597751Z","iopub.status.idle":"2021-09-20T08:25:44.615047Z","shell.execute_reply.started":"2021-09-20T08:25:44.597687Z","shell.execute_reply":"2021-09-20T08:25:44.613956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trgt_counts = train_df['Category'].value_counts()\ntrace = go.Bar(\n    x=trgt_counts.index, \n    y = trgt_counts.values,\n    marker=dict(\n        color=trgt_counts.values,\n        colorscale='Picnic',\n        reversescale=True\n    ),\n)\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18),\n    width = 400, \n    height =500,\n)\ndata=[trace]\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig,filename='TargetCount')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.61666Z","iopub.execute_input":"2021-09-20T08:25:44.617245Z","iopub.status.idle":"2021-09-20T08:25:44.676299Z","shell.execute_reply.started":"2021-09-20T08:25:44.61721Z","shell.execute_reply":"2021-09-20T08:25:44.675362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll from the graph we can see that dataset is not balanced","metadata":{}},{"cell_type":"markdown","source":"Now we'll analyse ngrams and co-occurences","metadata":{}},{"cell_type":"code","source":"from wordcloud import STOPWORDS # no need to use nltk here\nfrom collections import defaultdict\n\n\ndef generate_ngrams(text, n_gram = 1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.677841Z","iopub.execute_input":"2021-09-20T08:25:44.678158Z","iopub.status.idle":"2021-09-20T08:25:44.685684Z","shell.execute_reply.started":"2021-09-20T08:25:44.678116Z","shell.execute_reply":"2021-09-20T08:25:44.684637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom function for horizontal bar chart showing n-gram distribution\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y = df['word'].values[::-1],\n        x = df['wordcount'].values[::-1],\n        showlegend=False,\n        orientation='h',\n        marker=dict(color=color),\n    )\n    return trace\n\n\ndef get_bar_chart(df, ngram = 1, color = 'blue'):\n    freq_dict = defaultdict(int)\n    for sent in df:\n        for word in generate_ngrams(sent, ngram):\n            freq_dict[word] += 1\n    fd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\n    fd_sorted.columns = ['word','wordcount']\n    return horizontal_bar_chart(fd_sorted.head(50), color)\n\n\ndef create_two_subplots(trace0, \n                        trace1, \n                        subplot_titles = ['Freq words of ham mes','Freq words of spam mes'],\n                        title = 'Count spam plots', \n                        filename = 'Word_count_plots'\n                       ):\n    fig = subplots.make_subplots(rows=1,cols=2, vertical_spacing=0.01,\n                         subplot_titles = subplot_titles)\n    fig.append_trace(trace0,1,1)\n    fig.append_trace(trace1,1,2)\n    fig['layout'].update(height=1000, width=900,paper_bgcolor='rgb(233,233,233)',title = title)\n    py.iplot(fig, filename)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.687974Z","iopub.execute_input":"2021-09-20T08:25:44.688322Z","iopub.status.idle":"2021-09-20T08:25:44.701803Z","shell.execute_reply.started":"2021-09-20T08:25:44.688278Z","shell.execute_reply":"2021-09-20T08:25:44.700716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we'll look at ngrams distributions:","metadata":{}},{"cell_type":"code","source":"train1_df = train_texts[train_y== 'spam']\ntrain0_df = train_texts[train_y == 'ham']\n\n# get the bar chart for ham messages\ntrace0 = get_bar_chart(train0_df, 1, 'blue')\n\n# get the bar chart for spam messages\ntrace1 = get_bar_chart(train1_df, 1, 'red')\n\n#create two subplots\nsub_tit = ['Freq words of ham mes', 'Freq words of spam mes']\ntit= 'Word_count_plots'\ncreate_two_subplots(trace0, trace1, sub_tit, tit)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.70579Z","iopub.execute_input":"2021-09-20T08:25:44.706088Z","iopub.status.idle":"2021-09-20T08:25:44.840628Z","shell.execute_reply.started":"2021-09-20T08:25:44.706047Z","shell.execute_reply":"2021-09-20T08:25:44.839676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the bar chart for ham messages\ntrace0 = get_bar_chart(train0_df, 2, 'green')\n\n# get the bar chart for spam messages\ntrace1 = get_bar_chart(train1_df, 2, 'yellow')\n\n#create two subplots\nsub_tit = ['Bigram freq of ham mes', 'Bigram freq of spam mes']\ntit= 'Bigram_count_plots'\ncreate_two_subplots(trace0, trace1, sub_tit, tit)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.841718Z","iopub.execute_input":"2021-09-20T08:25:44.841935Z","iopub.status.idle":"2021-09-20T08:25:44.98917Z","shell.execute_reply.started":"2021-09-20T08:25:44.841909Z","shell.execute_reply":"2021-09-20T08:25:44.988219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the bar chart for ham messages\ntrace0 = get_bar_chart(train0_df, 3, 'brown')\n\n# get the bar chart for spam messages\ntrace1 = get_bar_chart(train1_df, 3, 'orange')\n\n#create two subplots\nsub_tit = ['Trigram freq of ham mes', 'Trigram freq of spam mes']\ntit= 'Trigram'\ncreate_two_subplots(trace0, trace1, sub_tit, tit)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:44.990609Z","iopub.execute_input":"2021-09-20T08:25:44.990858Z","iopub.status.idle":"2021-09-20T08:25:45.135808Z","shell.execute_reply.started":"2021-09-20T08:25:44.990826Z","shell.execute_reply":"2021-09-20T08:25:45.134775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So in trigrams distribution we clearly can see specific template for spam messages: \"prizes guaranteed\" etc. While for ham messages we can see friendly speech specific for messengers.","metadata":{}},{"cell_type":"code","source":"train_texts = train_texts.to_frame('message')\nvalid_texts=valid_texts.to_frame('message')\n\ntrain_texts['label'] = train_y\nvalid_texts['label'] = valid_y","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:45.137357Z","iopub.execute_input":"2021-09-20T08:25:45.137654Z","iopub.status.idle":"2021-09-20T08:25:45.145654Z","shell.execute_reply.started":"2021-09-20T08:25:45.137617Z","shell.execute_reply":"2021-09-20T08:25:45.144993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have texts we can do feature engineering and make extra-features for improving our model. \n\nWe'll make features num of words, num of unique words, num of chars, num of stopwords, punctuations, num of punctuations, num of upper words, num title words and mean of words length","metadata":{}},{"cell_type":"code","source":"# creating some extra features for better prediction accuracy\n\ntrain_texts['num_words'] = train_texts['message'].apply(lambda x: len(str(x).split()))\nvalid_texts['num_words'] = valid_texts['message'].apply(lambda x: len(str(x).split()))\n\ntrain_texts['num_unique_words'] = train_texts['message'].apply(lambda x: len(set(str(x).split())))  # for each mess\nvalid_texts['num_unique_words'] = valid_texts['message'].apply(lambda x: len(set(str(x).split())))\n\ntrain_texts['num_chars'] = train_texts['message'].apply(lambda x: len(str(x))) # for each mess\nvalid_texts['num_chars'] = valid_texts['message'].apply(lambda x: len(str(x)))\n\ntrain_texts['num_stopwords'] = train_texts['message'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\nvalid_texts['num_stopwords'] = valid_texts['message'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\ntrain_texts['num_punctuations'] = train_texts['message'].apply(lambda x: len([p for p in str(x) if p in string.punctuation]))\nvalid_texts['num_punctuations'] = valid_texts['message'].apply(lambda x: len([p for p in str(x) if p in string.punctuation]))\n\ntrain_texts['num_words_upper'] = train_texts['message'].apply(lambda x: len([u for u in str(x) if u.isupper()]))\nvalid_texts['num_words_upper'] = valid_texts['message'].apply(lambda x: len([u for u in str(x) if u.isupper()]))\n\ntrain_texts['num_words_title'] = train_texts['message'].apply(lambda x: len([t for t in str(x) if t.istitle()]))\nvalid_texts['num_words_title'] = valid_texts['message'].apply(lambda x: len([t for t in str(x) if t.istitle()]))\n\ntrain_texts['mean_word_len'] = train_texts['message'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\nvalid_texts['mean_word_len'] = valid_texts['message'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:45.146538Z","iopub.execute_input":"2021-09-20T08:25:45.146742Z","iopub.status.idle":"2021-09-20T08:25:45.481868Z","shell.execute_reply.started":"2021-09-20T08:25:45.146717Z","shell.execute_reply":"2021-09-20T08:25:45.481022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the boxplots to make sure we've build some specific features for each class:","metadata":{}},{"cell_type":"code","source":"# Truncate some extreme values for better visuals ##\ntrain_texts['num_words'].loc[train_texts['num_words']>60] = 60\ntrain_texts['num_punctuations'].loc[train_texts['num_punctuations']>10] = 10\ntrain_texts['num_chars'].loc[train_texts['num_chars']>350] = 350\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='label', y='num_words', data=train_texts,ax=axes[0])\naxes[0].set_xlabel('Label', fontsize=12)\naxes[0].set_title('Number of words in each class', fontsize=15)\n\nsns.boxplot(x='label', y='num_chars', data=train_texts,ax=axes[1])\naxes[1].set_xlabel('Label', fontsize=12)\naxes[1].set_title('Number of characters in each class', fontsize=15)\n\nsns.boxplot(x='label', y='num_punctuations', data=train_texts,ax=axes[2])\naxes[2].set_xlabel('Label', fontsize=12)\naxes[2].set_title('Number of punctuations in each class', fontsize=15)\nf.savefig('distributions.pdf', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:45.483137Z","iopub.execute_input":"2021-09-20T08:25:45.483386Z","iopub.status.idle":"2021-09-20T08:25:46.483792Z","shell.execute_reply.started":"2021-09-20T08:25:45.483358Z","shell.execute_reply":"2021-09-20T08:25:46.483134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On boxplots we can see, that distributions for 'ham' and 'spam' messages differ a lot. Thus, it will definitely make our model better (we have unique and specific features for each class)","metadata":{}},{"cell_type":"code","source":"# get the tfidf vectors \ntfidf_vec = TfidfVectorizer(stop_words ='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train_texts['message'].values.tolist() + valid_texts['message'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_texts['message'].values.tolist())\nvalid_tfidf = tfidf_vec.transform(valid_texts['message'].values.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:46.484995Z","iopub.execute_input":"2021-09-20T08:25:46.485914Z","iopub.status.idle":"2021-09-20T08:25:47.204463Z","shell.execute_reply.started":"2021-09-20T08:25:46.485869Z","shell.execute_reply":"2021-09-20T08:25:47.203668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfidf.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:47.205754Z","iopub.execute_input":"2021-09-20T08:25:47.20667Z","iopub.status.idle":"2021-09-20T08:25:47.212662Z","shell.execute_reply.started":"2021-09-20T08:25:47.206604Z","shell.execute_reply":"2021-09-20T08:25:47.21171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\ntrain_matrix = scipy.sparse.hstack([train_tfidf, train_texts.drop(['message', 'label'], axis = 1)]) # concatenate our features to tfidf features\nvalid_matrix = scipy.sparse.hstack([valid_tfidf, valid_texts.drop(['message', 'label'], axis =1)])","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:47.214196Z","iopub.execute_input":"2021-09-20T08:25:47.214509Z","iopub.status.idle":"2021-09-20T08:25:47.236503Z","shell.execute_reply.started":"2021-09-20T08:25:47.214469Z","shell.execute_reply":"2021-09-20T08:25:47.235666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating lists for metrics:","metadata":{}},{"cell_type":"code","source":"f1s = list(); accuracies = list(); rocs = list()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:47.23792Z","iopub.execute_input":"2021-09-20T08:25:47.238142Z","iopub.status.idle":"2021-09-20T08:25:47.243276Z","shell.execute_reply.started":"2021-09-20T08:25:47.238116Z","shell.execute_reply":"2021-09-20T08:25:47.242143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayesian estimator from ```sklearn```","metadata":{}},{"cell_type":"code","source":"train_y = [1 if x == 'spam' else 0 for x in train_y]\nvalid_y = [1 if x == 'spam' else 0 for x in valid_y]","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:47.244633Z","iopub.execute_input":"2021-09-20T08:25:47.244879Z","iopub.status.idle":"2021-09-20T08:25:47.257801Z","shell.execute_reply.started":"2021-09-20T08:25:47.24485Z","shell.execute_reply":"2021-09-20T08:25:47.25693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import naive_bayes\nfrom sklearn.metrics import roc_auc_score as roc\nfrom sklearn.metrics import accuracy_score as accuracy\n\nnb = naive_bayes.GaussianNB()\nnb.fit(train_matrix.toarray(), train_y)\nnp_preds = nb.predict(valid_matrix.toarray())","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:47.259156Z","iopub.execute_input":"2021-09-20T08:25:47.259604Z","iopub.status.idle":"2021-09-20T08:25:53.16469Z","shell.execute_reply.started":"2021-09-20T08:25:47.259516Z","shell.execute_reply":"2021-09-20T08:25:53.163738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1s.append(metrics.f1_score(valid_y, np_preds) )\naccuracies.append(accuracy(valid_y, np_preds))\nrocs.append(roc(valid_y, np_preds))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.166034Z","iopub.execute_input":"2021-09-20T08:25:53.166294Z","iopub.status.idle":"2021-09-20T08:25:53.1791Z","shell.execute_reply.started":"2021-09-20T08:25:53.166263Z","shell.execute_reply":"2021-09-20T08:25:53.178114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F_1 score is {}\".format(f1s[-1]) )\nprint(\"Accuracy is {}\".format(accuracies[-1]) ) \nprint(\"ROC-AUC score is {}\".format(rocs[-1]) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.180633Z","iopub.execute_input":"2021-09-20T08:25:53.181147Z","iopub.status.idle":"2021-09-20T08:25:53.188994Z","shell.execute_reply.started":"2021-09-20T08:25:53.181114Z","shell.execute_reply":"2021-09-20T08:25:53.188362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# kNN classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import Normalizer","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.192464Z","iopub.execute_input":"2021-09-20T08:25:53.192832Z","iopub.status.idle":"2021-09-20T08:25:53.200466Z","shell.execute_reply.started":"2021-09-20T08:25:53.192803Z","shell.execute_reply":"2021-09-20T08:25:53.19975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(train_matrix, train_y)\nknn_preds = knn.predict(valid_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.201414Z","iopub.execute_input":"2021-09-20T08:25:53.201884Z","iopub.status.idle":"2021-09-20T08:25:53.626657Z","shell.execute_reply.started":"2021-09-20T08:25:53.20185Z","shell.execute_reply":"2021-09-20T08:25:53.625846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1s.append(metrics.f1_score(valid_y, knn_preds) )\naccuracies.append(accuracy(valid_y, knn_preds))\nrocs.append(roc(valid_y, knn_preds))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.627805Z","iopub.execute_input":"2021-09-20T08:25:53.628743Z","iopub.status.idle":"2021-09-20T08:25:53.642538Z","shell.execute_reply.started":"2021-09-20T08:25:53.628692Z","shell.execute_reply":"2021-09-20T08:25:53.641604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F_1 score is {}\".format(f1s[-1]) )\nprint(\"Accuracy is {}\".format(accuracies[-1]) )\nprint(\"ROC-AUC score is {}\".format(rocs[-1]) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.644388Z","iopub.execute_input":"2021-09-20T08:25:53.644737Z","iopub.status.idle":"2021-09-20T08:25:53.652074Z","shell.execute_reply.started":"2021-09-20T08:25:53.644695Z","shell.execute_reply":"2021-09-20T08:25:53.651037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression approach\nBegin with only tf-idf features, later we'll do it for extended matrix for quality comparison.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score as accuracy\nfrom seaborn import heatmap","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.653797Z","iopub.execute_input":"2021-09-20T08:25:53.654149Z","iopub.status.idle":"2021-09-20T08:25:53.664901Z","shell.execute_reply.started":"2021-09-20T08:25:53.654109Z","shell.execute_reply":"2021-09-20T08:25:53.663804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"est = LogisticRegression()\nparam_grid = {'C' : np.arange(1, 10, 1), \n              'solver' : ['liblinear', 'newton-cg', 'sag'], \n             'penalty' : ['l1', 'l2']}\ngrid_search = GridSearchCV(est, param_grid, n_jobs = -1, cv=5, verbose = 0)\ngrid_search.fit(train_tfidf, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:25:53.666081Z","iopub.execute_input":"2021-09-20T08:25:53.667142Z","iopub.status.idle":"2021-09-20T08:26:33.208533Z","shell.execute_reply.started":"2021-09-20T08:25:53.667095Z","shell.execute_reply":"2021-09-20T08:26:33.207439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:26:33.210128Z","iopub.execute_input":"2021-09-20T08:26:33.210508Z","iopub.status.idle":"2021-09-20T08:26:33.217614Z","shell.execute_reply.started":"2021-09-20T08:26:33.210462Z","shell.execute_reply":"2021-09-20T08:26:33.216726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit = LogisticRegression(C = 9, penalty = 'l1', solver = 'liblinear')\nlogit.fit(train_tfidf, train_y)\nlogit_preds = logit.predict(valid_tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:26:33.21891Z","iopub.execute_input":"2021-09-20T08:26:33.219193Z","iopub.status.idle":"2021-09-20T08:26:37.188062Z","shell.execute_reply.started":"2021-09-20T08:26:33.219161Z","shell.execute_reply":"2021-09-20T08:26:37.187151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1s.append(metrics.f1_score(valid_y, logit_preds) )\naccuracies.append(accuracy(valid_y, logit_preds))\nrocs.append(roc(valid_y, logit_preds))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:26:37.18945Z","iopub.execute_input":"2021-09-20T08:26:37.18972Z","iopub.status.idle":"2021-09-20T08:26:37.202661Z","shell.execute_reply.started":"2021-09-20T08:26:37.189688Z","shell.execute_reply":"2021-09-20T08:26:37.201799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F_1 score is {}\".format(f1s[-1]) )\nprint(\"Accuracy is {}\".format(accuracies[-1]) )\nprint(\"ROC-AUC score is {}\".format(rocs[-1]) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:26:37.204012Z","iopub.execute_input":"2021-09-20T08:26:37.204295Z","iopub.status.idle":"2021-09-20T08:26:37.211144Z","shell.execute_reply.started":"2021-09-20T08:26:37.204256Z","shell.execute_reply":"2021-09-20T08:26:37.21021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heatmap(confusion_matrix(valid_y, logit_preds) , annot= True, fmt = 'd', cmap=\"YlGnBu\")\nNone","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:26:37.212468Z","iopub.execute_input":"2021-09-20T08:26:37.212721Z","iopub.status.idle":"2021-09-20T08:26:37.462926Z","shell.execute_reply.started":"2021-09-20T08:26:37.212694Z","shell.execute_reply":"2021-09-20T08:26:37.462094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logreg with extended matrix:","metadata":{}},{"cell_type":"code","source":"est = LogisticRegression()\nparam_grid = {'C' : np.arange(1, 10, 1), \n              'solver' : ['liblinear', 'newton-cg', 'sag'], \n             'penalty' : ['l1', 'l2']}\ngrid_search = GridSearchCV(est, param_grid, n_jobs = -1, cv=5, verbose = 0)\ngrid_search.fit(train_matrix, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:26:37.464069Z","iopub.execute_input":"2021-09-20T08:26:37.464299Z","iopub.status.idle":"2021-09-20T08:27:12.92188Z","shell.execute_reply.started":"2021-09-20T08:26:37.464272Z","shell.execute_reply":"2021-09-20T08:27:12.920811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:27:12.923135Z","iopub.execute_input":"2021-09-20T08:27:12.923377Z","iopub.status.idle":"2021-09-20T08:27:12.929393Z","shell.execute_reply.started":"2021-09-20T08:27:12.92335Z","shell.execute_reply":"2021-09-20T08:27:12.928403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_ext = LogisticRegression(C = 9, penalty = 'l1', solver = 'liblinear')\nlogit_ext.fit(train_matrix, train_y)\nlogit_ext_preds = logit_ext.predict(valid_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:27:12.930611Z","iopub.execute_input":"2021-09-20T08:27:12.930856Z","iopub.status.idle":"2021-09-20T08:27:13.596308Z","shell.execute_reply.started":"2021-09-20T08:27:12.930829Z","shell.execute_reply":"2021-09-20T08:27:13.595628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1s.append(metrics.f1_score(valid_y, logit_ext_preds) )\naccuracies.append(accuracy(valid_y, logit_ext_preds))\nrocs.append(roc(valid_y, logit_ext_preds))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:27:13.597381Z","iopub.execute_input":"2021-09-20T08:27:13.598293Z","iopub.status.idle":"2021-09-20T08:27:13.611004Z","shell.execute_reply.started":"2021-09-20T08:27:13.598239Z","shell.execute_reply":"2021-09-20T08:27:13.609964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F_1 score is {}\".format(f1s[-1]) )\nprint(\"Accuracy is {}\".format(accuracies[-1]) ) \nprint(\"ROC-AUC score is {}\".format(rocs[-1]) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:27:13.612438Z","iopub.execute_input":"2021-09-20T08:27:13.612711Z","iopub.status.idle":"2021-09-20T08:27:13.619538Z","shell.execute_reply.started":"2021-09-20T08:27:13.612681Z","shell.execute_reply":"2021-09-20T08:27:13.618712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heatmap(confusion_matrix(valid_y, logit_ext_preds) , annot= True, fmt = 'd', cmap=\"YlGnBu\")\nNone","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:27:13.620715Z","iopub.execute_input":"2021-09-20T08:27:13.621115Z","iopub.status.idle":"2021-09-20T08:27:13.879962Z","shell.execute_reply.started":"2021-09-20T08:27:13.621082Z","shell.execute_reply":"2021-09-20T08:27:13.878998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM approach","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC()\nparams = {'C': np.arange(1, 12, 1), \"degree\" : np.arange(3, 7, 1)}\ngrid = GridSearchCV(svm, params, n_jobs = -1, cv = 5)\ngrid.fit(train_matrix, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:27:13.881235Z","iopub.execute_input":"2021-09-20T08:27:13.881482Z","iopub.status.idle":"2021-09-20T08:28:01.363084Z","shell.execute_reply.started":"2021-09-20T08:27:13.881454Z","shell.execute_reply":"2021-09-20T08:28:01.36229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:28:01.364181Z","iopub.execute_input":"2021-09-20T08:28:01.364396Z","iopub.status.idle":"2021-09-20T08:28:01.372804Z","shell.execute_reply.started":"2021-09-20T08:28:01.36437Z","shell.execute_reply":"2021-09-20T08:28:01.369447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm = SVC(C = 10)\nsvm.fit(train_matrix, train_y)\nsvm_preds = svm.predict(valid_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:28:01.373927Z","iopub.execute_input":"2021-09-20T08:28:01.374176Z","iopub.status.idle":"2021-09-20T08:28:02.203228Z","shell.execute_reply.started":"2021-09-20T08:28:01.37415Z","shell.execute_reply":"2021-09-20T08:28:02.20216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1s.append(metrics.f1_score(valid_y, svm_preds) )\naccuracies.append(accuracy(valid_y, svm_preds))\nrocs.append(roc(valid_y, svm_preds))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:29:06.390732Z","iopub.execute_input":"2021-09-20T08:29:06.391031Z","iopub.status.idle":"2021-09-20T08:29:06.40563Z","shell.execute_reply.started":"2021-09-20T08:29:06.391001Z","shell.execute_reply":"2021-09-20T08:29:06.404589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F_1 score is {}\".format(f1s[-1]) ) \nprint(\"Accuracy is {}\".format(accuracies[-1]) ) \nprint(\"ROC-AUC score is {}\".format(rocs[-1]) )","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:29:07.955231Z","iopub.execute_input":"2021-09-20T08:29:07.955559Z","iopub.status.idle":"2021-09-20T08:29:07.96202Z","shell.execute_reply.started":"2021-09-20T08:29:07.955516Z","shell.execute_reply":"2021-09-20T08:29:07.960956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"methods = ['bayesian', 'knn', 'logit', 'logit-ext', 'svm']\nf1s_s = pd.Series(f1s, index = methods)\naccuracies_s = pd.Series(accuracies, index = methods)\nrocs_s = pd.Series(rocs, index = methods)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:29:11.391457Z","iopub.execute_input":"2021-09-20T08:29:11.391743Z","iopub.status.idle":"2021-09-20T08:29:11.400645Z","shell.execute_reply.started":"2021-09-20T08:29:11.391714Z","shell.execute_reply":"2021-09-20T08:29:11.399627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bar for the F_1 metric\ntrace0 = go.Bar(\n    x=f1s_s.index, y=f1s_s, name=\"F_1\"\n)\n\n# Create a bar for the accuracies\ntrace1 = go.Bar(\n    x=accuracies_s.index,\n    y=accuracies_s,\n    name=\"Accuracy\",\n)\n\ntrace2 = go.Bar(\n    x=rocs_s.index,\n    y=rocs_s,\n    name=\"ROC-AUC\",\n)\n\ndata = [trace0, trace1, trace2]\nlayout = {\"title\": \"Comparison of methods on metrics distribution\"}\n\n# Create a `Figure` and plot it\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, show_link=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:34:41.498729Z","iopub.execute_input":"2021-09-20T08:34:41.499051Z","iopub.status.idle":"2021-09-20T08:34:41.54345Z","shell.execute_reply.started":"2021-09-20T08:34:41.49902Z","shell.execute_reply":"2021-09-20T08:34:41.5424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clearly we see that logit-ext method is best for hacking this case","metadata":{}},{"cell_type":"markdown","source":"Now we'll take ```eli5``` library for showing weights of our best model. Therefore we'll be able to interpret it and see impact of most weightful n-grams for classificating sample as a \"spam\" or \"ham\"","metadata":{}},{"cell_type":"code","source":"import eli5\neli5.show_weights(logit, vec=tfidf_vec, top = 50, feature_filter=lambda x: x != '<BIAS>')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T08:35:29.735252Z","iopub.execute_input":"2021-09-20T08:35:29.735572Z","iopub.status.idle":"2021-09-20T08:35:31.653459Z","shell.execute_reply.started":"2021-09-20T08:35:29.735542Z","shell.execute_reply":"2021-09-20T08:35:31.652469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_weights(logit_ext, vec=tfidf_vec, top=50, feature_filter=lambda x: x != '<BIAS>')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T18:17:14.673299Z","iopub.execute_input":"2021-09-19T18:17:14.673663Z","iopub.status.idle":"2021-09-19T18:17:14.87185Z","shell.execute_reply.started":"2021-09-19T18:17:14.67363Z","shell.execute_reply":"2021-09-19T18:17:14.871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using eli-5 we can see that the biggest weights for spam class is words like \"claim\", \"awarded\", \"reply\", \"service\" - it makes sence since we always get messages, where we were awarded or had service. While for the ham messages most the haviest words describing sms chatting with friends. ","metadata":{}}]}