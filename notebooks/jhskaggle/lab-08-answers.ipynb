{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n[COMP20121 Machine Learning for Data Analytics](https://sites.google.com/site/hejunhomepage/Teaching/machine-learning-for-data-analytics)\n\nAuthor: Jun He "},{"metadata":{},"cell_type":"markdown","source":"## Learning objectives\n* Implement logistic regression for classification \n* Tune parameters in logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import Python libraries \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import metrics\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Activity 1 Implement Logistic Regression on Indians Diabetes Data \n### Load data and understand data\nAdd Pima Indians Diabetes data, which is available at https://www.kaggle.com/uciml/pima-indians-diabetes-database. You can add this data set from `Add Data` button in your Kaggle Kernel with the above URL. \n\n*Question: what is the  data shape? What are columns names? Which is the class  name?*\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"data shape\", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis\n(1) Use a pie-chart to show the percentage of `Outcome` =1 and 0. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outcome'].value_counts().plot(kind = 'pie', title = 'Outcome', autopct='%1.1f%%') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(2)\tUnderstand the relationship between two predictor variables and the target variable.\n\nFor example, scatter plot  \"BloodPressure\", y=\"Glucose\" and \"Outcome\" \n\n*Question: what is your finding 1 from EDA?*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.scatterplot(data=df, x=\"BloodPressure\", y=\"Glucose\", hue=\"Outcome\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(3) Check outliers in the data. For example, check features `BMI`, `Pregnancies` and `BloodPressure`\n\n*Question: is there any outlier in the data? If  yes, how do you handle these outliers?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure() \ndf[\"BMI\"].plot(kind='box', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure() \ndf[\"Pregnancies\"].plot(kind='box', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure() \ndf[\"BloodPressure\"].plot(kind='box', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare data\n(1) Split dataset in predictor and target variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,0:8] #stop is excluded\ny = df.iloc[:,8] \nprint(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(2) Split dataset into training  data and test data\n    * 70% records for training\n    * 30% records for test"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.model_selection as model_selection\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.3,random_state=4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build a logistic regression model\n(1) Create a logistic regression object (classifier)\n\n(2) Train the classifier on training data `fit(X_train,y_train)`\n\n(3) Print parameters used in the classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Create LogisticRegression object\nclf = LogisticRegression(max_iter =2000)\n\n# Train LogisticRegression Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test) \nclf.get_params()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the model\n(1) Predict the label of patients in test data set\n\n(2) Calculate the accuracy of prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the metrics class\nfrom sklearn import metrics\n# Model Accuracy\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(3) Print coefficients of the logistic regression curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = pd.DataFrame(zip(X_train.columns, np.transpose(clf.coef_.tolist()[0])), columns=['features', 'coefficient']) # create a dataframe\ndf3 = df3.append({'features':'intercept','coefficient' : clf.intercept_.tolist()[0]}, ignore_index=True) # append a new row\ndf3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tune parameters in logistic regression\nTune the following parameters in  logistic regression and find out the best parameters\n* `max_iter`: the maximum number of iterations for a solver  to iterate\n* `penalty`:  Used to specify penalization in regularization\n    * ‘l1’, ‘l2’, ‘elasticnet’, ‘none’\n* `solver`: used for fitting the model in logistic regression\n    * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n    * ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n    * ‘liblinear’ and ‘saga’ also handle L1 penalty\n    * ‘saga’ also supports ‘elasticnet’ penalty\n    * ‘liblinear’ does not support setting penalty='none' "},{"metadata":{},"cell_type":"markdown","source":"## Activity 2 Comparison with KNN and decision tree\nYou have learned two classifiers, KNN and decision trees, in previous lectures. In this activity, you will apply KNN and decision trees to the above data.\n\n*Question: which classifier perform the best?*\n"},{"metadata":{},"cell_type":"markdown","source":"## Reflect\nBriefly note what you’ve learnt, found easy and found challenging in your Jupyter notebook. Keep these notes safe and maintain a reflective log for each lab session."},{"metadata":{},"cell_type":"markdown","source":"## Resources/references\n1. Sklearn  logistic regression: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n2. Understanding Logistic Regression in Python: https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}