{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"artists = pd.read_csv('/kaggle/input/best-artworks-of-all-time/artists.csv')\nartists.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort artists by number of paintings\nartists = artists.sort_values(by=['paintings'], ascending=False)\n\nartists_top = artists[artists['paintings'] >= 50].reset_index()\nartists_top = artists_top[['name', 'paintings']]\nartists_top['class_weight'] = max(artists_top.paintings)/artists_top.paintings\nartists_top","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = artists_top['class_weight'].to_dict()\nclass_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_name = \"Albrecht_DuÌˆrer\".replace(\"_\", \" \")\nartists_top.iloc[4, 0] = updated_name\nartists_top","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore images of top artists\nimages_dir = '/kaggle/input/best-artworks-of-all-time/images/images/'\nartists_dirs = os.listdir(images_dir)\nartists_top_name = artists_top['name'].str.replace(' ', '_').values\nprint(artists_top_name)\n\n# See if all directories exist\nfor name in artists_top_name:\n    if os.path.exists(os.path.join(images_dir, name)):\n        print(\"Found -->\", os.path.join(images_dir, name))\n    else:\n        print(\"Did not find -->\", os.path.join(images_dir, name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print few random paintings\nn = 5\nfig, axes = plt.subplots(1, n, figsize=(20,10))\n\nfor i in range(n):\n    random_artist = random.choice(artists_top_name)\n    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n    random_image_file = os.path.join(images_dir, random_artist, random_image)\n    image = plt.imread(random_image_file)\n    axes[i].imshow(image)\n    axes[i].set_title(\"Artist: \" + random_artist.replace('_', ' '))\n    axes[i].axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Augment data\nbatch_size = 64\ntrain_input_shape = (224, 224, 3)\nn_classes = artists_top.shape[0]\n\ntrain_datagen = ImageDataGenerator(validation_split=0.2,\n                                   rescale=1./255.,\n                                   zoom_range=0.7,\n                                  )\n\ntrain_generator = train_datagen.flow_from_directory(directory=images_dir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"training\",\n                                                    shuffle=True,\n                                                    classes=artists_top_name.tolist()\n                                                   )\n\nvalid_generator = train_datagen.flow_from_directory(directory=images_dir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"validation\",\n                                                    shuffle=True,\n                                                    classes=artists_top_name.tolist()\n                                                   )\n\nSTEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\nprint(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_artist = random.choice(artists_top_name)\nrandom_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\nrandom_image_file = os.path.join(images_dir, random_artist, random_image)\n\n# Original image\nimage = plt.imread(random_image_file)\naxes[0].imshow(image)\naxes[0].set_title(\"An original Image of \" + random_artist.replace('_', ' '))\naxes[0].axis('off')\n\n# Transformed image\naug_image = train_datagen.random_transform(image)\naxes[1].imshow(aug_image)\naxes[1].set_title(\"A transformed Image of \" + random_artist.replace('_', ' '))\naxes[1].axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pre-trained model\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n\nfor layer in base_model.layers:\n    layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add layers at the end\nX = base_model.output\nX = Flatten()(X)\n\nX = Dense(512, kernel_initializer='he_uniform')(X)\n#X = Dropout(0.5)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\nX = Dense(16, kernel_initializer='he_uniform')(X)\n#X = Dropout(0.5)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\noutput = Dense(n_classes, activation='softmax')(X)\n\nmodel = Model(inputs=base_model.input, outputs=output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(lr=0.0001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epoch = 20\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n                           mode='auto', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model - all layers\nhistory1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr],\n                              use_multiprocessing=True,\n                              workers=16,\n                              class_weight=class_weights\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freeze core ResNet layers and train again \nfor layer in model.layers[-6:]:\n   layer.trainable = False\n\nfor layer in model.layers:\n    layer.trainable = True\n\noptimizer = Adam(lr=0.0001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])\n\nn_epoch = 25\nhistory2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr, early_stop],\n                              use_multiprocessing=True,\n                              workers=16,\n                              class_weight=class_weights\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = {}\nhistory['loss'] = history1.history['loss'] + history2.history['loss']\nhistory['acc'] = history1.history['accuracy'] + history2.history['accuracy']\nhistory['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\nhistory['val_acc'] = history1.history['val_accuracy'] + history2.history['val_accuracy']\nhistory['lr'] = history1.history['lr'] + history2.history['lr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_training(history):\n    acc = history['acc']\n    val_acc = history['val_acc']\n    loss = history['loss']\n    val_loss = history['val_loss']\n    epochs = range(len(acc))\n\n    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n    \n    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n    axes[0].set_title('Training and Validation Accuracy')\n    axes[0].legend(loc='best')\n\n    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n    axes[1].set_title('Training and Validation Loss')\n    axes[1].legend(loc='best')\n    \n    plt.show()\n    \nplot_training(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction accuracy on train data\nscore = model.evaluate_generator(train_generator)\nprint(\"Prediction accuracy on train data =\", score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction accuracy on CV data\nscore = model.evaluate_generator(valid_generator)\nprint(\"Prediction accuracy on CV data =\", score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report and confusion matrix\nfrom sklearn.metrics import *\nimport seaborn as sns\n\ntick_labels = artists_top_name.tolist()\n\ndef showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n    # Loop on each generator batch and predict\n    y_pred, y_true = [], []\n    for i in range(STEP_SIZE_VALID):\n        (X,y) = next(valid_generator)\n        y_pred.append(model.predict(X))\n        y_true.append(y)\n    \n    # Create a flat list for y_true and y_pred\n    y_pred = [subresult for result in y_pred for subresult in result]\n    y_true = [subresult for result in y_true for subresult in result]\n    \n    # Update Truth vector based on argmax\n    y_true = np.argmax(y_true, axis=1)\n    y_true = np.asarray(y_true).ravel()\n    \n    # Update Prediction vector based on argmax\n    y_pred = np.argmax(y_pred, axis=1)\n    y_pred = np.asarray(y_pred).ravel()\n    \n    # Confusion Matrix\n    fig, ax = plt.subplots(figsize=(10,10))\n    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n    conf_matrix = conf_matrix/np.sum(conf_matrix, axis=1)\n    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False, \n                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n                ax=ax)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=artists_top_name.tolist()))\n\nshowClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\nfrom keras.preprocessing import *\nfrom PIL import Image\n\nn = 5\nfig, axes = plt.subplots(1, n, figsize=(25,10))\n\nfor i in range(n):\n    random_artist = random.choice(artists_top_name)\n    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n    random_image_file = os.path.join(images_dir, random_artist, random_image)\n\n    test_image = Image.open(random_image_file) \n    test_image = test_image.resize(train_input_shape[0:2])\n\n    test_image = np.array(test_image)\n    test_image = test_image / 255.0\n    test_image = np.expand_dims(test_image, axis=0)\n\n    prediction = model.predict(test_image)\n    prediction_probability = np.amax(prediction)\n    prediction_idx = np.argmax(prediction)\n\n    labels = train_generator.class_indices\n    labels = dict((v,k) for k,v in labels.items())\n\n    title = \"Actual artist = {}\\nPredicted artist = {}\\nPrediction probability = {:.2f} %\" \\\n                .format(random_artist.replace('_', ' '), labels[prediction_idx].replace('_', ' '),\n                        prediction_probability*100)\n\n    axes[i].imshow(plt.imread(random_image_file))\n    axes[i].set_title(title)\n    axes[i].axis('off')\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}