{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fetal Health Classification\n* Data Cleaning and Preprocessing\n* Feature Selection\n* Model Selection - Random Forest Classifier - 92%\n* F1_Score, Recall and Precision for each class\n* ROC Curve\n* precision_recall_curve"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom scipy import interp\nfrom itertools import cycle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fetal-health-classification/fetal_health.csv\")\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning and Preprocessing "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df.fetal_health)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_orig = df.fetal_health\nprint(y_orig.unique())\ny = label_binarize(y_orig, classes=[1,2,3])\nn_classes = 3\n# X = df.drop([\"fetal_health\"], axis=1)\nX = df[[\"baseline value\", \"accelerations\", \"fetal_movement\", \"uterine_contractions\", \"light_decelerations\",\n        \"severe_decelerations\", \"prolongued_decelerations\", \"abnormal_short_term_variability\", \"percentage_of_time_with_abnormal_long_term_variability\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting Data - **not stratified**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=500, random_state=42))\ny_score = clf.fit(x_train,y_train)\ny_pred = clf.score(x_test,y_test)\nprint(\"Validation Accuracy\",clf.score(x_test,y_test)*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## F1_Score, Recall and Precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test, clf.predict(x_test), average='macro'))\nprint(f1_score(y_test, clf.predict(x_test), average='micro'))\nprint(f1_score(y_test, clf.predict(x_test), average='weighted'))\n\nprecision, recall, fscore, support = score(y_test, clf.predict(x_test))\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prob = clf.predict_proba(x_test)\nmacro_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n                                  average=\"macro\")\nweighted_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n                                     average=\"weighted\")\nmacro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n                                  average=\"macro\")\nweighted_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n                                     average=\"weighted\")\nprint(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n      \"(weighted by prevalence)\"\n      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\nprint(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n      \"(weighted by prevalence)\"\n      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_prob.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\nlw = 2\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Fetal Health Classification')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Precision recall curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        y_prob[:, i])\n    average_precision[i] = average_precision_score(y_test[:, i], y_prob[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n    y_prob.ravel())\naverage_precision[\"micro\"] = average_precision_score(y_test, y_prob,\n                                                     average=\"micro\")\nprint('Average precision score, micro-averaged over all classes: {0:0.2f}'\n      .format(average_precision[\"micro\"]))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.step(recall['micro'], precision['micro'], where='post')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\n    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n    .format(average_precision[\"micro\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n\nplt.figure(figsize=(7, 8))\nf_scores = np.linspace(0.2, 0.8, num=4)\nlw = 2\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x / (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\nlines.append(l)\nlabels.append('iso-f1 curves')\nl, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\nlines.append(l)\nlabels.append('micro-average Precision-recall (area = {0:0.2f})'\n              ''.format(average_precision[\"micro\"]))\n\nfor i, color in zip(range(n_classes), colors):\n    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n    lines.append(l)\n    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n                  ''.format(i, average_precision[i]))\n\nfig = plt.gcf()\nfig.subplots_adjust(bottom=0.25)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Fetal Health Classification')\nplt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Please Upvote if you like the content"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}