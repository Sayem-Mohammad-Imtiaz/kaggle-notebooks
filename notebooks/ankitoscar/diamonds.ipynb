{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Diamond Price porject\nDiamonds are considered very precious in our society and are also highly valued monetarily. In this project, let's create a model which predicts the price of a diamond based on its features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Problem\nSince we have to predict the price of the diamond, which is a number. Hence this is a regression problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Data\nThe data taken for this project is taken from Kaggle. It contains various features of diamonds and its price. The description of its features are:\n    \n    1. carat weight of the diamond (0.2--5.01)\n    2. cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n    3. color diamond colour, from J (worst) to D (best)\n    4. clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n    5. x length in mm (0--10.74)\n    6. y width in mm (0--58.9)\n    7. z depth in mm (0--31.8)\n    8. depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n    9. table width of top of diamond relative to widest point (43--95)\n    \nThe predicting variable is :\n    \n    price in US dollars (\\$326--\\$18,823)\n    \nLink to the dataset : https://www.kaggle.com/shivam2503/diamonds/download","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Evaluation\nSince it is a regression model, we will evaluate it on metrics like r2 score, mean absolute error and mean squared error.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Analysis\n\nLet's analyse the data by running some visualization on it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing important libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing project csv file\ndf = pd.read_csv(\"../input/diamonds/diamonds.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This column named 'Unnamed: 0' is kind of useless. So it will fine if it is deleted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Unnamed: 0',axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information about the data set\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of samples in this data set, hence it will be fun running visualizations on it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Running visulizations on numerical data\nLet's compare numerical features against the price column. Since there are less features, we can easily cover most of the correlations present in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of carat values in diamonds\ndf[\"carat\"].plot.hist(color = \"lightblue\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w carat and price \nfig,ax = plt.subplots(tight_layout = True,figsize = (10,10))\nax.scatter(df['carat'],df['price'],color = 'salmon')\nax.set(title = \"Relation b/w carat and price of diamond\",\n       xlabel = \"Carat\",\n       ylabel = \"Price(in USD)\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph shows that there is a heavy correlation b/w carat and price, diamonds with large values of carat have high prices.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of depth\ndf['depth'].plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w price and depth \nfig,ax = plt.subplots(tight_layout = True,figsize = (10,10))\nax.scatter(df['depth'],df['price'],color = 'lightgreen')\nax.set(title = \"Relation b/w price and depth of diamond\",\n       xlabel = \"Depth (z/mean(x,y))\",\n       ylabel = \"Price (in USD)\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows that most diamonds have depth ratio in the range of 50-75. There are some outliers but they do not show any significant effect.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of table\ndf['table'].plot.hist(color = 'indigo');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w price and table\nfig,ax = plt.subplots(tight_layout = True,figsize = (10,10))\nax.scatter(df['table'],df['price'],color = 'pink')\nax.set(title = 'Relation b/w table and price',\n       xlabel = 'Table',\n       ylabel = 'Price (in USD)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature also has same properties as in depth feature. The range here is 50-70. Here also the outliers also don't show any significance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w dimensions and price\ndim = ['x','y','z']\ncolors = ['r','b','g']\nfig,ax = plt.subplots(1,3,tight_layout = True,figsize = (15,10))\nfor i in range(3):\n    ax[i].scatter(df[dim[i]],df['price'],color = colors[i])\nax[0].set_xlabel('X')\nax[1].set_xlabel('Y')\nax[2].set_xlabel('Z');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. The X feature has high correlation, although there are some outliers.\n2. The Y and Z feature has way more correlation and it has less outliers. Also, their values is less than that of X feature.\n\nWe will remove these outliers for:\n \n 1. X in range 0-3.\n 2. Y in range >20.\n 3. Z in range >15.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w price and carat w.r.t table\nfig,ax = plt.subplots(tight_layout =True,figsize = (10,10))\nax.scatter(df['carat'],df['price'],c = df['table'],cmap = 'Blues')\nax.set(title = \"Relation b/w price and carat w.r.t table\",\n       xlabel = 'Carat',\n       ylabel = 'Price(in USD)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that table value doesn't have much significance regarding carat and price. Regardless of carat values, the diamonds have similar table value. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w price and carat w.r.t depth\nfig,ax = plt.subplots(tight_layout = True,figsize = (10,10))\nax.scatter(df['carat'],df['price'],c = df['depth'],cmap = 'plasma')\nax.set(title = \"Relation b/w price and carat w.r.t. depth\",\n       xlabel = 'Carat',\n       ylabel = 'Price(in USD)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The depth feature also shows same behaviour as shown by table feature, as most diamonds have same values of depth regardless of carat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation of carat with table and depth\nfig,ax = plt.subplots(1,2,tight_layout = True,figsize = (20,10))\nax[0].scatter(df['carat'],df['table'],color = 'lightblue')\nax[0].set(title = \"Relation b/w table and carat\",\n          xlabel = \"Carat\",\n          ylabel = 'Table')\nax[1].scatter(df['carat'],df['depth'],color = 'purple')\nax[1].set(title = \"Relation b/w depth and carat\",\n              xlabel = \"Carat\",\n              ylabel = \"Depth\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This proves that depth and carat value remains in a specific range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w carat and dimensions w.r.t price\nfig,ax = plt.subplots(1,3,tight_layout = True,figsize = (15,10))\nfor i in range(3):\n    ax[i].scatter(df[dim[i]],df['carat'],c = df['price'],cmap = 'viridis')\n    ax[i].set(xlabel = dim[i],\n              ylabel = 'Carat');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These plots show that :\n    \n    1. All dimension variables have almost linear relation with carat, with varying slope.\n    2. Price starts increasing after 1 carat.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualizations on categorical data\nLet's run some visualization of categorical data present with us to see its impact on price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install seaborn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seaborn is more suitable in plotting categorical data. So let's use it to make some visualizations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar graphs for categorical features\ncategorical_features = ['cut','color','clarity']\nfig,ax = plt.subplots(1,3,tight_layout = True,figsize = (15,10))\nfor i in range(3):\n    df[categorical_features[i]].value_counts().T.plot.bar(color = colors[i],ax = ax[i])\n    ax[i].set_title(categorical_features[i]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Points :\n    \n    1. Most diamonds have ideal cut.\n    2. Most diamonds have G colour.\n    3. Most diamonds have S1 colour.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing price of diamond in different categories\nsns.catplot(x = 'cut',y = 'price',data = df,height = 9,aspect = 11.7/8.27,ax = ax[i]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = 'color',y = 'price',data = df,height = 9,aspect = 11.7/8.27,ax = ax[i]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = 'clarity',y = 'price',data = df,height = 9,aspect = 11.7/8.27,ax = ax[i]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Points to be noted : \n\n    1. In 'cut' category, the categories 'ideal','premium' and 'very good' have diamonds in every price range, while the other categories have very few samples in higher price range.\n    2. In 'color' category, all categories have samples in every price range\n    3. In 'clarity' category, except 'I1', 'WS2','WS1 and 'IF' posses the same property.\n    4. These plots give only a hint at their correlation with price.\n\nLet's compare one category w.r.t. another and see what kind of results we get.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w price and cut w.r.t colour\nsns.catplot(x = 'cut',y = 'price',data = df,height = 11,aspect = 11.7/8.27,hue = 'color');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Points to be noted:\n    \n    1. Colours 'I','G' and 'H' are found in each 'cut'.\n    2. Colour 'E' and 'J' are not prevalent in higher price range.\n    3. Colour 'D' is found in low to mid range.\n    4. All cuts have all colours in them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w price and clarity w.r.t. colour\nsns.catplot(x = 'cut',y = 'price',hue = 'clarity',data = df,height = 11, aspect = 11.7/8.27);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Points to be noted:\n    \n    1. The clarity categories 'S1','S2','VS1' and 'VS2' are present in all cuts.\n    2. The rest are present in low to mid range.\n    3. All cut have all types of clarity in them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w price and clarity w.r.t. colour\nsns.catplot(x = 'clarity',y = 'price', hue = 'color',data = df,height = 11, aspect = 11.7/8.27);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Points :\n    \n    1. In 'clarity' types 'SI1','SI2','VS1' and 'VS2' have colour 'I' dominating in the higher price range.\n    2. The colour 'G' is mid range in the above mentioned types, but is high range in 'WS1' amd 'WS2'.\n    3. The colour 'F' is present in all price ranges and clarity types.\n    4. The colour 'H' is high range in 'SI1', 'SI2' and 'VS2'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting price and carat w.r.t. categories ('IF','I1','VVS1','VVS2')\nfig,ax = plt.subplots(figsize = (10,10),tight_layout = True)\nax.scatter(df.carat[df.clarity == 'IF'],df.price[df.clarity == 'IF'],color = 'salmon')\nax.scatter(df.carat[df.clarity == 'I1'],df.price[df.clarity == 'I1'],color = 'lightblue')\nax.scatter(df.carat[df.clarity == 'VVS1'],df.price[df.clarity == 'VVS1'],color = 'lightgreen')\nax.scatter(df.carat[df.clarity == 'VVS2'],df.price[df.clarity == 'VVS2'],color = 'orange');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting for remaining categories of clarity\nfig,ax = plt.subplots(1,2,figsize = (10,10),tight_layout = True)\ncl1 = ['SI1','SI2','VS1','VS2']\nclr = ['salmon','lightblue','lightgreen','orange']\nfor i in range(4):\n    if i < 2:\n        ax[0].scatter(df.carat[df['clarity']==cl1[i]],df.price[df['clarity']==cl1[i]],color = clr[i])\n        ax[0].set_xticks(range(6))\n    else:\n        ax[1].scatter(df.carat[df['clarity']==cl1[i]],df.price[df['clarity']==cl1[i]],color = clr[i])\n        ax[1].set_xticks(range(6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These plots show that these 4 categories are having high correlation with price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Points : \n    \n    1. The categories 'IF','I1' and 'VVS2' have high impact on price of diamond, as the slope is greater.\n    2. The category 'VVS1' has almost a linear relation with price.\n    3. The categories other than that mentioned in above points have higher range of price.\n    \nHence, the clarity categories in point 1 are indicators of higher price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation of price and carat w.r.t cut types ('Ideal','Premium','Very Good') and ('Good cut','Fair') in separate plots\nfig,ax = plt.subplots(1,2,figsize = (15,12),tight_layout = True)\nct1 = df['cut'].unique()\nclr = ['salmon','lightblue','lightgreen','orange','indigo']\nprint(ct1)\nfor i in range(5):\n    if i < 3:\n        ax[0].scatter(df.carat[df['cut']==ct1[i]],df.price[df['cut']==ct1[i]],color = clr[i])\n        ax[0].set_xticks(range(6))\n    else:\n        ax[1].scatter(df.carat[df['cut']==ct1[i]],df.price[df['cut']==ct1[i]],color = clr[i])\n        ax[1].set_xticks(range(6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly we can see that cuts ('Ideal','Premium','Very Good') have higher price for lower carats. Hence, such diamonds will be more expensive.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation b/w carat and price w.r.t. different colors\nfig,(ax0,ax1) = plt.subplots(2,4,tight_layout = True,figsize = (12,12))\nprint(df.color.unique())\nclr = ['salmon','lightblue','lightgreen','orange','indigo','yellow','pink']\nfor i in range(7):\n    if i < 4:\n        ax0[i].scatter(df.carat[df.color == df.color.unique()[i]],df.price[df.color == df.color.unique()[i]],color = clr[i])\n        ax0[i].set_xticks(range(6))\n    else:\n        ax1[i-4].scatter(df.carat[df.color == df.color.unique()[i]],df.price[df.color == df.color.unique()[i]],color = clr[i])\n        ax1[i-4].set_xticks(range(6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These plots show that for all colours are showing almost same correlation b/w price and carat. Hence we can see that there are same patterns of prices for different colours.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\nThe information obtained about the data after all visualizations are : \n\n    1. Carat has high correlation with price, so does the dimension.\n    2. Depth and table have values in the same range for all carats and prices, hence have relatively low correlation.\n    3. Depth and table are dependent on dimensions as mentioned earlier in description, and they have values in specific ranges, hence they do not impact dimensions.\n    4. The clarity 'VVS2' has lower correlation to price than other clarity types.\n    5. The 'Fair' has lower price than other cuts.\n    6. The price and color correlation is almost the same for all color types.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At last, let's make a correlation matrix to check whether our observations are correct or not(only for numeric data).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drawing correlation matrix\ncorr = df.corr()\ncorr.style.background_gradient(cmap = \"winter\").set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation matrix show : \n    \n    1. The dimensions and carat values have high correlation with price(almost linear).\n    2. Depth and table have low correlation with every other feature.\n    \nHence, the data analysis was quite on point.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. Modelling \nSince we have a lot samples in our dataset, we must use a small yet significant portion of the data to see the training results, as large number of samples take a lot of time to train. For training, let's take 10% of total samples. This much sample will ensure that the model trains on enough data and it will also save our time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking 10% random samples from main dataset for training \nnp.random.seed(42)\ntrain = df.sample(frac = 0.1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some information about the subset\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have 5394 samples, hence it would be easy and time saving for us to train the model. Also, the rows are selected randomly, so the patterns in the original dataset can be easily found here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting into features and label\nX = train.drop('price',axis = 1)\ny = train['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have categorical features, we need to convert them into numeric features. Let's use one hot encoding to do this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding data\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\ncolumn = make_column_transformer((OneHotEncoder(drop = 'if_binary',sparse = False),['cut','color','clarity']),remainder = 'passthrough')\ncolumn.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have made column transformer, let's now split the data into training and test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into training and test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is now split into training and test set. Let's start modelling.\n\nSince it is a regression problem, hence we will start from Ridge regression model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making pipeline function to model using different models and evaluating them\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nimport matplotlib.ticker as ticker\ndef pipeline(model,X_train,y_train,X_test,y_test):\n    pipe = make_pipeline(column,model)\n    pipe.fit(X_train,y_train)\n    # Evaluating metrics on training and test set\n    train_predict = pipe.predict(X_train)\n    test_predict = pipe.predict(X_test)\n    scores = {\n        \"Training R^2 score\" : r2_score(y_train,train_predict),\n        \"Test R^2 score\" : r2_score(y_test,test_predict),\n        \"Training MAE\" : mean_absolute_error(y_train,train_predict),\n        \"Test MAE\" : mean_absolute_error(y_train,train_predict),\n        \"Training MSE\" : mean_squared_error(y_train,train_predict),\n        \"Test MSE\" : mean_squared_error(y_test,test_predict)\n    }\n    print(scores)\n    fig,ax = plt.subplots(1,3,tight_layout = True,figsize = (9,9))\n    ax[0].bar(['Training','Test'],[r2_score(y_train,train_predict),r2_score(y_test,test_predict)],color = 'lightblue')\n    ax[0].set_title('R^2 score')\n    ax[1].bar(['Training','Test'],[mean_absolute_error(y_train,train_predict),mean_absolute_error(y_train,train_predict)],color = 'lightblue')\n    ax[1].set_title('MAE')\n    ax[2].bar(['Training','Test'],[mean_squared_error(y_train,train_predict),mean_squared_error(y_test,test_predict)],color = 'lightblue');\n    ax[2].set_title('MSE');\n    return pipe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge()\npipeline(ridge,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the R^2 score is coming high, the MAE and MSE values are coming very large, which is due to large difference in maximum and minimum values. Let's delete some features and run the model again.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have seen that the 'depth' and 'table' values have very less significance, let's remove them and train the model again. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'depth' and 'table' from main dataset\nX1 =X.drop(['depth','table'],axis = 1)\nX1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training model on modified data\nnp.random.seed(42)\nX1_train,X1_test,y1_train,y1_test = train_test_split(X1,y,test_size = 0.2)\npipeline(ridge,X1_train,y1_train,X1_test,y1_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In genral, training metrics have worsened but test metrics have really improved. Let's see what features we can modify and see what results we can see.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Dropping some potential outliers\nX2 = X.drop(X[(X.x<3) & (X.y>20) & (X.z >15)].index)\nnp.random.seed(42)\nX2_train,X2_test,y2_train,y2_test = train_test_split(X2,y,test_size = 0.2)\npipeline(ridge,X2_train,y2_train,X2_test,y2_test);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model seems to reach its maximum performance. Let's try another model and see the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using LinearRegression model\nfrom sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\npipeline(linear,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LinearRegression() model is performing better on training set and worse in test set, let's train it on the data not having 'depth' and 'table' features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline(linear,X1_train,y1_train,X1_test,y1_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training metrics have very small change, but the test metrics have improved.\n\nLet's train the model on the data which doesn't have some outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline(linear,X2_train,y2_train,X2_test,y2_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's modify some more features and see what results are in front of us.\n\nEarlier we saw that the 'Fair' cut diamonds have lower price to carat relation. Let's remove that feature and see what happens.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train\ntrain1.drop(train1[train1.cut == 'Fair'].index,inplace = True)\nnp.random.seed(42)\nX3 = train1.drop('price',axis = 1)\ny3 = train1['price']\nX3_train,X3_test,y3_train,y3_test = train_test_split(X3,y3,test_size = 0.2)\npipeline(linear,X3_train,y3_train,X3_test,y3_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except MSE on test set, all other metrics have improved significantly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training with Ridge() model\npipeline(ridge,X3_train,y3_train,X3_test,y3_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the Ridge() model, R^2 score on both sets remain somewhat the same, with MAE and MSE on both sets worsening.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's remove 'color' columns, as we saw all colours have same correlation with price and carat\ntrain2 = train1\ntrain2.drop('color',axis = 1)\nX4 = train2.drop('price',axis = 1)\ny4 = train2['price']\nnp.random.seed(42)\nX4_train,X4_test,y4_train,y4_test = train_test_split(X4,y4,test_size = 0.2)\npipeline(ridge,X4_train,y4_train,X4_test,y4_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case also, R^2 score remains the same, but the MAE and MSE are worsening.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on LinearRegression() model\npipeline(linear,X4_train,y4_train,X4_test,y4_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LinaerRegression() model performs a bit better than the Ridge model, but still it is worse than that in previous cases.\n\nLet's remove the clarity 'VVS2' as it was seen to have low correlation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train3 = train2\ntrain3.drop(train3[train3.clarity == 'VVS2'].index,inplace = True)\nX5 = train3.drop('price',axis = 1)\ny5 = train3.price\nnp.random.seed(42)\nX5_train,X5_test,y5_train,y5_test = train_test_split(X5,y5,test_size = 0.2)\npipeline(ridge,X5_train,y5_train,X5_test,y5_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R^2 score remains somewhat the same, MAE has worsened and MSE has improved significantly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using LinearRegression() model\npipeline(linear,X5_train,y5_train,X5_test,y5_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model's R^2 improved a bit, MAE and MSE has improved from previous case.\n\nLet's use a different model and see what results it shows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using RandomForestRegressor() model\nfrom sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor()\npipeline(forest,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metrics have significantly improved from the previous two models. Let's see how it performs on modified data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using RandomForestRegressor() on data not having 'depth' and 'table'\npipeline(forest,X1_train,y1_train,X1_test,y1_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metrics are keeping on improving, although the metrics may seem to show overfitting. Let's train it on more data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on data not having 'Fair' cut \npipeline(forest,X3_train,y3_train,X3_test,y3_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except R^2, other metrics are worsening a bit. Let's try it another set of modified data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using data without color columns\npipeline(forest,X4_train,y4_train,X4_test,y4_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There has been a improvement in the metrics, but it is very small.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using data not having 'VVS2' clarity\npipeline(forest,X5_train,y5_train,X5_test,y5_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metrics have started worsening, hence based on metrics,we should try the RandomForestRegressor() model with the data not having 'depth','table','Fair' cut and 'VVS2' clarity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doing final modfications on data\ntrain4 = train\ntrain4.drop(['depth','table'],axis = 1)\ntrain4.drop(train4[train4.cut == 'Fair'].index,inplace = True)\ntrain4.drop(train4[train4.clarity == 'VVS2'].index,inplace = True)\nX6 = train4.drop('price',axis = 1)\ny6 = train4.price\nnp.random.seed(42)\nX6_train,X6_test,y6_train,y6_test = train_test_split(X6,y6,test_size = 0.2)\npipeline(forest,X6_train,y6_train,X6_test,y6_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After all data munging, the model seems to not improve from the result we have seen after removing 'depth' and 'table' feature. Hence, we should stick with doing only that change in the final dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation final selected model\ntrain.drop(['depth','table'],axis = 1,inplace = True)\nX = train.drop('price',axis  = 1)\ny = train.price\npipe = make_pipeline(column,forest)\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(pipe,X,y).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even in cross validation, the model seems to perfrom a lot better than expected. This may be possible due to:\n    \n    1. Large number of samples to train on.\n    2. Removing features which have very low correlation.\n    \nLet's see the cross validation score for MAE and MSE.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation for MAE and MSE\nprint(abs(cross_val_score(pipe,X,y,scoring = 'neg_mean_absolute_error').mean()))\nprint(abs(cross_val_score(pipe,X,y,scoring = 'neg_mean_squared_error').mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results for MAE and MSE are worse than expected, but still better than what was seen in initial models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Final Modelling\n\nLet's do modelling on now the full data. Here, we will now make three different sets, train, test and valid. Valid set will be used for cross validation, while the other two will be used for the same purpose they were used earlier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making changes in original data\ndf.drop(['depth','table'],axis = 1,inplace = True)\ndf = df.sample(frac = 1)\ntrain_set = df[:round(0.6*len(df))]\nvalid_set = df[round(0.6*len(df)):round(0.8*len(df))]\ntest_set = df[round(0.8*len(df)):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shapes of different sets\ntrain_set.shape,valid_set.shape,test_set.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have made the final split, let's start doing the final modelling process.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_set.drop('price',axis = 1)\ny_train = train_set.price\nmodel = make_pipeline(column,forest)\nmodel.fit(X_train,y_train)\nmodel.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is seeming fit almost perfectly on the training data. Let's cross validate it on all 3 metrics.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation\nX_valid = valid_set.drop('price',axis = 1)\ny_valid = valid_set.price\ncross_val_score(model,X_valid,y_valid,cv = 5).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross validation score on valid set is also very good. Let's finally calculate the metrics on the test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating metrics on test data\nX_test = train_set.drop('price',axis = 1)\ny_test = train_set.price\ntest_predict = model.predict(X_test)\nscores = {\n    \"R^2 score\" : r2_score(y_test,test_predict),\n    \"Mean Absolute Error\" : mean_absolute_error(y_test,test_predict),\n    \"Mean Squared Error\" : mean_squared_error(y_test,test_predict)\n}\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These metrics show that the model is fit very nicely, although the r2 score looks like it has overfitted. Let's now export the model and end the project.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exporting the model\nimport joblib\njoblib.dump(model,\"final_model.joblib\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}