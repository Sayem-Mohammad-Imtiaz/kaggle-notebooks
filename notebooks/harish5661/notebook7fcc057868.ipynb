{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"995eb03a-da7b-a47d-2174-a8dcb9f633b7"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nimport os\nimport codecs\nfrom sklearn import feature_extraction\nimport mpld3\nglobal word\n\nimport random\n#random.seed( 3 )\nword=list()\nnp.random.seed(3)\n\nvocab_frame=list()\n\nx=pd.read_excel(\"C:/Users/harishs/Desktop/Stamper/Press Release.xlsx\")\n\n#x=x[:1000]\nprint(len(x))\n\nstopwords = nltk.corpus.stopwords.words('english')\n#print(stopwords[:10])\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\n\ndef tokenize_and_stem(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems\n\n\ndef tokenize_only(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    return filtered_tokens\n\ntotalvocab_stemmed = []\ntotalvocab_tokenized = []\nfor i in x['NewsFeed']:\n    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n    \n    allwords_tokenized = tokenize_only(i)\n    totalvocab_tokenized.extend(allwords_tokenized)\nvocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\nprint('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n#print(vocab_frame.head())\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200,\n                                 min_df=0.2, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(x[\"NewsFeed\"]) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)\n\n\nterms = tfidf_vectorizer.get_feature_names()\n\nfrom sklearn.metrics.pairwise import cosine_similarity\ndist = 1 - cosine_similarity(tfidf_matrix)\n\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 5\n\nkm = KMeans(n_clusters=num_clusters)\n\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()\n\n#from sklearn.externals import joblib\n\n#uncomment the below to save your model \n#since I've already run my model I am loading from the pickle\n\n#joblib.dump(km,  'doc_cluster.pkl')\n\n#km = joblib.load('C:/Users/harishs/Desktop/document_cluster-master/doc_cluster.pkl')\n#clusters = km.labels_.tolist()\n\narts = { 'title': x['Title'],'Articles': x['NewsFeed'], 'cluster': clusters, 'Index' : x['Article Number']}\n\n\nprint(arts['Articles'])\n\nframe=pd.DataFrame(arts)\n#\n#frame = pd.DataFrame(arts, index = [clusters] , columns = ['title','cluster','Articles','Index'])\nframe.to_csv(\"C:/Users/harishs/Desktop/kmeans.csv\")\n\n#print(frame[\"Articles\"])\nprint(len(frame[\"Articles\"]))\n\nprint(frame['cluster'].value_counts())\n\n#grouped = frame['Title'].groupby(frame['cluster'])\n\n#print(cluster)\n\n#from __future__ import print_function\n\nprint(\"Top terms per cluster:\")\nprint()\n#sort cluster centers by proximity to centroid\nprint(\"Top terms per cluster:\")\nprint()\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    print(\"Cluster %d words:\" % i, end='')\n    \n    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n    print() #add whitespace\n    print() #add whitespace\n    \n    print(\"Cluster %d titles:\" % i, end='')\n    for title in frame['title']:\n        print(' %s,' % title)\n    print() #add whitespace\n    print() #add whitespace\n    \nprint()\nprint()\n\n\nimport os  # for os.path.basename\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn.manifold import MDS\n\nMDS()\n\n# convert two components as we're plotting points in a two-dimensional plane\n# \"precomputed\" because we provide a distance matrix\n# we will also specify `random_state` so the plot is reproducible.\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n\npos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n\nxs, ys = pos[:, 0], pos[:, 1]\nprint()\nprint()\n\ncluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n\ncluster_names = {0: 'Business', \n                 1: 'R & D', \n                 2: 'Share Market', \n                 3: 'Product', \n                 4: 'Heterogenous'}\n                 \n#print(len(frame[\"title\"]))\n#print(len(frame[\"cluster\"]))\n\ndf = pd.DataFrame(dict(x=xs, y=ys, label=frame[\"cluster\"], title=frame['Index'])) \nprint(df)\n\n#group by cluster\ngroups = df.groupby('label')\n\n\n# set up plot\nfig, ax = plt.subplots(figsize=(30, 19)) # set size\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n\n##iterate through groups to layer the plot\n##note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n            label=cluster_names[name], color=cluster_colors[name], \n            mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(\\\n        axis= 'x',          # changes apply to the x-axis\n        which='both',      # both major and minor ticks are affected\n        bottom='off',      # ticks along the bottom edge are off\n        top='off',         # ticks along the top edge are off\n        labelbottom='off')\n    ax.tick_params(\\\n        axis= 'y',         # changes apply to the y-axis\n        which='both',      # both major and minor ticks are affected\n        left='off',      # ticks along the bottom edge are off\n        top='off',         # ticks along the top edge are off\n        labelleft='off')\n    \nax.legend(numpoints=1)  #show legend with only 1 point\n#\n##add label in x,y position with the label as the film title\nfor i in range(len(df)):\n    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n\n    \n    \nplt.show() #show the plot\n\n#class TopToolbar(mpld3.plugins.PluginBase):\n#    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n#\n#    JAVASCRIPT = \"\"\"\n#    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n#    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n#    TopToolbar.prototype.constructor = TopToolbar;\n#    function TopToolbar(fig, props){\n#        mpld3.Plugin.call(this, fig, props);\n#    };\n#\n#    TopToolbar.prototype.draw = function(){\n#      // the toolbar svg doesn't exist\n#      // yet, so first draw it\n#      this.fig.toolbar.draw();\n#\n#      // then change the y position to be\n#      // at the top of the figure\n#      this.fig.toolbar.toolbar.attr(\"x\", 150);\n#      this.fig.toolbar.toolbar.attr(\"y\", 400);\n#\n#      // then remove the draw function,\n#      // so that it is not called again\n#      this.fig.toolbar.draw = function() {}\n#    }\n#    \"\"\"\n#    def __init__(self):\n#        self.dict_ = {\"type\": \"toptoolbar\"}\n#df = pd.DataFrame(dict(x=xs, y=ys, label=frame[\"cluster\"], title=frame[\"title\"])) \n#\n##group by cluster\n#groups = df.groupby('label')\n#\n##define custom css to format the font and to remove the axis labeling\n#css = \"\"\"\n#text.mpld3-text, div.mpld3-tooltip {\n#  font-family:Arial, Helvetica, sans-serif;\n#}\n#\n#g.mpld3-xaxis, g.mpld3-yaxis {\n#display: none; }\n#\n#svg.mpld3-figure {\n#margin-left: -200px;}\n#\"\"\"\n#\n## Plot \n#fig, ax = plt.subplots(figsize=(14,6)) #set plot size\n#ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n#\n##iterate through groups to layer the plot\n##note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n#for name, group in groups:\n#    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, \n#                     label=cluster_names[name], mec='none', \n#                     color=cluster_colors[name])\n#    ax.set_aspect('auto')\n#    labels = [i for i in group.title]\n#    \n#    #set tooltip using points, labels and the already defined 'css'\n#    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n#                                       voffset=10, hoffset=10, css=css)\n#    #connect tooltip to fig\n#    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n#    \n#    #set tick marks as blank\n#    ax.axes.get_xaxis().set_ticks([])\n#    ax.axes.get_yaxis().set_ticks([])\n#    \n#    #set axis as blank\n#    ax.axes.get_xaxis().set_visible(False)\n#    ax.axes.get_yaxis().set_visible(False)\n#\n#    \n#ax.legend(numpoints=1) #show legend with only one dot\n#\n#mpld3.display() #show the plot\n\n#uncomment the below to export to html\n#html = mpld3.fig_to_html(fig)\n#\n#print(html)\n\n#############Hierarchy Clustering##########\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\n\nlinkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n#print(linkage_matrix)\nimport sys\n#sys.setrecursionlimit(100000000)\n#\n#fig, ax = plt.subplots(figsize=(150, 200)) # set size\n#ax = dendrogram(linkage_matrix, orientation=\"right\", labels=frame[\"title\"]);\n###\n#plt.tick_params(\\\n#    axis= 'x',          # changes apply to the x-axis\n#    which='both',      # both major and minor ticks are affected\n#    bottom='off',      # ticks along the bottom edge are off\n#    top='off',         # ticks along the top edge are off\n#    labelbottom='off')\n##\n#plt.tight_layout() #show plot with tight layout\n##\n##uncomment below to save figure\n#plt.savefig('C:/Users/harishs/Desktop/ward_clusters.png', dpi=200)\n\n\n\n###########LDA##################################\n\nimport string\ndef strip_proppers(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n    \nfrom nltk.tag import pos_tag\n\ndef strip_proppers_POS(text):\n    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n    return non_propernouns\n    \nfrom gensim import corpora, models, similarities \n\n#remove proper names\n#titles=x['Title']\npreprocess = [strip_proppers(doc) for doc in frame['Articles']]\n\n#tokenize\ntokenized_text = [tokenize_and_stem(text) for text in preprocess]\n\n#remove stop words\ntexts = [[word for word in text if word not in stopwords] for text in tokenized_text]\n\ndictionary = corpora.Dictionary(texts)\n\n#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\ndictionary.filter_extremes(no_below=1, no_above=0.8)\n\n#convert the dictionary to a bag of words corpus for reference\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n#print(corpus)\n\nlda = models.ldamulticore.LdaMulticore(corpus, num_topics=5, \n                            id2word=dictionary, workers=3, \n                            chunksize=100,\n                            passes=1,iterations = 300)\n\n#lda.show_topics()\n#print(lda.get_document_topics(frame['Articles']))\n                            \nprint(lda[corpus])\n                            \ntopics_matrix = lda.show_topics(formatted=False, num_words=1000)\n#print(topics_matrix)\ntopics_matrix=pd.DataFrame(topics_matrix)\ntopics_matrix.to_csv(\"C:/Users/harishs/Desktop/lda.csv\")\ntopics_matrix = np.array(topics_matrix)\n\n\n\ntopic_words = topics_matrix[:,1]\n#Firstt=get_topic_terms(0, topn=10)\n#print()\nfor i in range(0,5):\n    for j in range(0,100):\n        word.append([i,topic_words[i][j]])\nword=pd.DataFrame(word)\n#for i in frame['Articles']:\n#    doc_topic = lda.get_document_topics(i)\n#    print(doc_topic)\n\n#for i in range(10):\n#    print(\"{} (top topic: {})\".format(doc_topic[i]))\n#print(lda.update(x['Title'][1]))\n\nword.to_csv(\"C:/Users/harishs/Desktop/dic.csv\")\n   \n\n\n\n##############DBSCAN##################\n#\n#from sklearn.cluster import DBSCAN\n#from sklearn import metrics\n#\n#db = DBSCAN(eps=0.03, min_samples=3).fit(tfidf_matrix)\n#\n#\n#core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n#core_samples_mask[db.core_sample_indices_] = True\n#labels = db.labels_\n#labels=pd.DataFrame(labels)\n#labels.to_csv(\"C:/Users/harishs/Desktop/lda.csv\")\n\n# Number of clusters in labels, ignoring noise if present.\n#n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\n#print('Estimated number of clusters: %d' % n_clusters_)\n\n#print(\"Silhouette Coefficient:\",metrics.silhouette_score(tfidf_matrix, labels))\n\n\n\n\n\n\n\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30e329ee-8618-26df-194b-36a13e427e98"},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"18450fac-8e4a-53ba-0001-e520ce70681a"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64f00c84-9b9d-4bcd-6cdd-6f6589e8b802"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport random\nrandom.seed(90)\nfrom sklearn.neural_network import MLPClassifier\nprint('Random',random.random())\nimport matplotlib as plt\nfrom sklearn.preprocessing import StandardScaler  \n#from sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom numpy import corrcoef, sum, log, arange\nfrom numpy.random import rand\nfrom pylab import pcolor, show, colorbar, xticks, yticks\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nimport sklearn.svm as svm\nfrom sklearn.cross_validation import cross_val_score, train_test_split\nimport numpy as np\nfrom sklearn import preprocessing,cross_validation,svm,neighbors\nimport pandas as pd\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f6f6c17-106c-7b1e-aea3-cd7f835cc500"},"outputs":[],"source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ndata = pd.read_csv('../input/UCI_Credit_Card.csv',skiprows=1)\n\ndf = data.copy()\ntarget = 'default payment next month'\nprint(df.columns)\n\ndef describe_factor(x):\n    ret = dict()\n    for lvl in x.unique():\n        if pd.isnull(lvl):\n            ret[\"NaN\"] = x.isnull().sum()\n        else:\n           ret[lvl] = np.sum(x==lvl)\n    return ret\n\nprint('Sex')\nprint(describe_factor(df['SEX']))\n# {1: 11888, 2: 18112}\n\nprint('Education is ordinnal Keep it, but set, others to NA')\nprint(describe_factor(df[\"EDUCATION\"]))\n# {0: 14, 1: 10585, 2: 14030, 3: 4917, 4: 123, 5: 280, 6: 51}\n\n\ndf[\"EDUCATION\"] = df[\"EDUCATION\"].map({0: np.NaN, 1:1, 2:2, 3:3, 4:np.NaN, \n    5: np.NaN, 6: np.NaN})\nprint(describe_factor(df[\"EDUCATION\"]))\n# {1.0: 10585, 2.0: 14030, 3.0: 4917, 'NaN': 468}\n\nprint('MARRIAGE 0,3=>NA')\nprint(describe_factor(df[\"MARRIAGE\"]))\n# {0: 54, 1: 13659, 2: 15964, 3: 323}\n\ndf.MARRIAGE = df.MARRIAGE.map({0:np.NaN, 1:1, 2:0, 3:np.NaN})\nprint(describe_factor(df.MARRIAGE))\n# {0.0: 15964, 1.0: 13659, 'NaN': 377}\n\nprint(\"Others are quantitative and presents\")\n\nprint(df.describe())\n\n\nprint(df.isnull().sum())\n\n\ndf.ix[df[\"EDUCATION\"].isnull(), \"EDUCATION\"] = df[\"EDUCATION\"].mode()\ndf.ix[df[\"MARRIAGE\"].isnull(), \"MARRIAGE\"] = df[\"MARRIAGE\"].mode()\nprint(df.isnull().sum().sum())\n\n\ndescribe_factor(df[target])\n{0: 23364, 1: 6636}\n\n\npredictors = df.columns.drop(['ID', target])\nX = np.asarray(df[predictors])\ny = np.asarray(df[target])\n\n#data=X[:300,:].transpose()\n#R = corrcoef(data)\n#pcolor(R)\n#colorbar()\n#yticks(arange(0,21),range(0,22))\n#xticks(arange(0,21),range(0,22))\n#show()\n#X = X\n#Y =y\n#model = LogisticRegression()\n#rfe = RFE(model, 1)\n#fit = rfe.fit(X, Y)\n#print(\"Num Features:\",fit.n_features_)\n#print(\"Selected Features:\",fit.support_)\n#print(\"Feature Ranking: \",fit.ranking_)\n#X=pd.DataFrame(X)\n\n\n#X1=X[[1,2,3,4,5,6,7,8,9,10,12,18]]\n\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=0, stratify=y)\n\nX_train=preprocessing.robust_scale(X_train, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n#\nX_test=preprocessing.robust_scale(X_test, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n\n\nclf=svm.LinearSVC(C=1.0,random_state=0)\nclf.fit(X_train,y_train)\n\n\naccuracy=clf.score(X_test,y_test)\nprint(\"SVM using Linear SVC accuracy\",accuracy)\n\nclf=DecisionTreeClassifier(random_state=0)\nclf.fit(X_train,y_train)\n\naccuracy=clf.score(X_test,y_test)\nprint(\"Decision Tree accuracy\",accuracy)\n\nclf= RandomForestClassifier(n_estimators=300, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=True, n_jobs=1, random_state=1, verbose=0, warm_start=False, class_weight=None)\nclf.fit(X_train,y_train)\n\naccuracy=clf.score(X_test,y_test)\nprint(\"Random Forest\",accuracy)\n\n\nclf = KNeighborsClassifier(n_neighbors=7)\nclf.fit(X_train,y_train)\n\naccuracy=clf.score(X_test,y_test)\nprint('KNN Accuracy',accuracy)\n\nclf=MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n       beta_1=0.9, beta_2=0.999, early_stopping=False,\n       epsilon=1e-08, hidden_layer_sizes=(5,2), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n       solver='adam', tol=0.001, validation_fraction=0.1, verbose=False,\n       warm_start=False)\n\n\nclf.fit(X_train,y_train)\n##\naccuracy=clf.score(X_test,y_test)\nprint(\"ANN\",accuracy)\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}