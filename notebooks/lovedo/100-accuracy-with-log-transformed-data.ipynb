{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4ab09104-aa3a-afb7-7ef8-97fbf20b3884"},"source":"## Read in libraries and import functions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d32cfb59-f124-a144-c917-f06f4ed4e0b0"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8fc581f-578a-d2f8-16fd-861d2e254e2d"},"outputs":[],"source":"# Read and clean dataset\ndf =  pd.read_csv('../input/data.csv', header=0)\nprint(df.columns)\n\ndf = df[df.notnull()].copy()\ndf = df.drop(\"Unnamed: 32\",1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db77b559-c3a1-7180-127c-bdbbc3055a85"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\n#Split dataset into Train and Test\ndf_train, df_test = train_test_split(df, test_size = 0.3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bcde9512-9136-2361-0eba-65455726cd4b"},"source":"### Convert \"Diagnosis\" from string values 'B'/'M' to binary (0/1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"138d2e3d-fa5a-7660-bd12-add2865fa41a"},"outputs":[],"source":"#Benign = 0 and Malignant = 1\ndf_train[\"diagnosis\"] = df_train[\"diagnosis\"].apply(lambda diagnosis: 0 if diagnosis == \"B\" else 1)\ndf_test[\"diagnosis\"] = df_test[\"diagnosis\"].apply(lambda diagnosis: 0 if diagnosis == \"B\" else 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b22fcd36-d68a-0fa7-ea17-0f4793a44e59"},"source":"## Dataset Characteristics"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f401183f-8feb-039c-2698-3e164e0e4ce7"},"outputs":[],"source":"#Display header data\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04cc037d-37e6-8532-8020-179641af38d0"},"outputs":[],"source":"#Describe key stats\nprint(df_train.describe())\n\n#Confirm whether there is missing data\nprint(df_train.shape, df_train.isnull().sum())  \n#print(df.isnull().sum())"},{"cell_type":"markdown","metadata":{"_cell_guid":"4ed203d1-6f71-500d-4b49-4a2a5e5f9538"},"source":"## Separate columns into smaller dataframes to perform visualisations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99053a10-fbbc-58d7-5f96-179cacb09b7b"},"outputs":[],"source":"#Break up columns into groups, according to their suffix designation (_mean, _se,\n# and __worst) to perform visualisation plots off. Join the 'ID' and 'Diagnosis' back on\ndf_id_diag=df_train.loc[:,[\"id\",\"diagnosis\"]]\ndf_diag=df_train.loc[:,[\"diagnosis\"]]\n\n#For a merge + slice:\ndf_cut1=df_train.ix[:,2:11]\ndf_cut2=df_train.ix[:,12:21]\ndf_cut3=df_train.ix[:,22:]\n\n#print(df_id_diag.columns)\nprint(df_cut1.columns)\nprint(df_cut2.columns)\nprint(df_cut3.columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c846a52f-6e24-ce11-db63-b48c9462b17d"},"source":"## Visualise distribution of data via histograms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78798354-fb76-bc65-32a8-b7f271a447d6"},"outputs":[],"source":"#Plot histograms of CUT1 variables\ndf_cut1.hist(bins=10, figsize=(10, 10))\n\n#Any individual histograms, use this:\n#df_cut['radius_worst'].hist(bins=100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c705d71a-790e-51e1-c2f3-04bb57ead1ed"},"outputs":[],"source":"#Plot histograms of CUT2 variables\ndf_cut2.hist(bins=10, figsize=(10, 10))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc7c92ef-e699-e212-2866-26ab91c41ff9"},"outputs":[],"source":"#Plot histograms of CUT3 variables\ndf_cut3.hist(bins=10, figsize=(10, 10))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8be0f55-eefb-5694-0049-318b49c36f71"},"source":"### Box plot _MEAN variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0c0d5d9-0b2c-eb63-6ce5-7406063e93fc"},"outputs":[],"source":"#Plot _mean predictor variables (i.e. df_cut1)\nplt.rcParams['figure.figsize']=(15,11)\nsns.boxplot(df_cut1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8e629de-4bc6-a604-174e-b89776da93a7"},"source":"Initial assumption was that as with many biological phenomena, tumours may be displaying logarithmic distribution. Data will be log transformed and re-displayed in box plots"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fc3164c-ffa6-2202-fa78-44e87d919ec0","collapsed":true},"outputs":[],"source":"#Log transform data in dataframe\nlog_columns = ['radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean',\n       'concave points_mean','symmetry_mean']\ndf_cut1_log_trans=df_cut1.loc[:,log_columns]\ndf_cut1_log_trans[log_columns] = df_cut1_log_trans[log_columns].apply(np.log10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f282fbbb-9454-095e-6b36-bcbb289fd2f5"},"outputs":[],"source":"#Reproduce boxplot for _mean variables\nplt.rcParams['figure.figsize']=(15,11)\nsns.boxplot(df_cut1_log_trans)"},{"cell_type":"markdown","metadata":{"_cell_guid":"93c593b4-c3f3-3207-06d7-1b43a1d619df"},"source":"### Box plot _SE variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5d7b31f-aa21-7cc3-f206-eb2ef0d38465"},"outputs":[],"source":"#Plot _se predictor variables (i.e. df_cut2)\nplt.rcParams['figure.figsize']=(10,10)\nsns.boxplot(df_cut2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4062af2-f9bf-03c4-8fa6-9f20133a32ed","collapsed":true},"outputs":[],"source":"#Log transform data in dataframe\nlog_columns = ['radius_se', 'texture_se', 'perimeter_se', 'area_se','smoothness_se', 'compactness_se', 'concavity_se',\n       'concave points_se', 'symmetry_se']\ndf_cut2_log_trans=df_cut2.loc[:,log_columns]\ndf_cut2_log_trans[log_columns] = df_cut2_log_trans[log_columns].apply(np.log10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41bc387d-dc57-eed8-63d3-738cfd634cfb"},"outputs":[],"source":"#Reproduce boxplot for _SE variables\nplt.rcParams['figure.figsize']=(15,11)\nsns.boxplot(df_cut1_log_trans)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6b5c7a77-ad60-22c8-677e-c40534b0a9b2"},"source":"### Box plot _WORST variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5028558-01a2-d3e8-f950-36f06e163fc0"},"outputs":[],"source":"#Plot _WORST predictor variables (i.e. df_cut3)\nplt.rcParams['figure.figsize']=(10,10)\nsns.boxplot(df_cut3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f23d71f5-1110-fdad-48de-1bdc5f70359c","collapsed":true},"outputs":[],"source":"#Log transform data in dataframe\nlog_columns = ['radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n       'smoothness_worst', 'compactness_worst', 'concavity_worst',\n       'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\ndf_cut3_log_trans=df_cut3.loc[:,log_columns]\ndf_cut3_log_trans[log_columns] = df_cut3_log_trans[log_columns].apply(np.log10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fd6777a-971b-f0a1-33bb-fb766ea9bb53"},"outputs":[],"source":"#Reproduce boxplot for _SE variables\nplt.rcParams['figure.figsize']=(15,11)\nsns.boxplot(df_cut3_log_trans)"},{"cell_type":"markdown","metadata":{"_cell_guid":"32592e00-da6a-e6d1-312d-393adc0c84b0"},"source":"After log transformations, data seems relatively well normally distributed. Now visualisation analysis will move to identifying whether categorisation is feasible and degree of colinearity using pairplot distributions. It should be noted that distribution of data overlaps heavily between smoothness, compactness, concavity, concave points, symmetry, and fractal dimension for all meta-parameters (mean, se, and worst)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2ac37d69-005f-76ca-dbf7-9bb7d3697722"},"source":"## Visualisation: Pairplots"},{"cell_type":"markdown","metadata":{"_cell_guid":"035ebe19-b427-5f81-ed89-07971f34d06e"},"source":"Due to the number of variables, each dataframe cut will be assessed individually. This is not ideal, as it would be preferable to visualise each predictor plotted against one another. However, given that each dataframe cut (mean, se, worst) has a respective version of each of the parameters (radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal_dimension), it is assumed that collinear relationships will carry across cuts"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2900dce0-392b-ecd5-497b-cf93c29a5986"},"outputs":[],"source":"#Merge back on Diagnosis (to allow discrimination) for the dataframe containing _MEAN predictor variables before plotting pairplot\ndf_diag_cut1=df_diag.join(df_train.ix[:,2:11])\n\n# Quick plot of the data using seaborn\nsns.pairplot(df_diag_cut1, hue = \"diagnosis\")\nsns.plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b65eda7-2da7-53a2-9b91-c517951a6b79"},"outputs":[],"source":"#Repeat for dataframe with _SE before plotting pairplot\ndf_diag_cut2=df_diag.join(df_train.ix[:,12:21])\n\n# Quick plot of the data using seaborn\nsns.pairplot(df_diag_cut2, hue = \"diagnosis\")\nsns.plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61c174fb-e5ab-2620-bc9c-d5e79df53c67"},"outputs":[],"source":"#Merge back on ID and Diagnosis before plotting pairplot\ndf_diag_cut3=df_diag.join(df_train.ix[:,22:])\n\n# Quick plot of the data using seaborn\nsns.pairplot(df_diag_cut3, hue = \"diagnosis\")\nsns.plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a56c8574-5b3f-4820-48db-951bea9625fc"},"source":"# Begin model building"},{"cell_type":"markdown","metadata":{"_cell_guid":"887e6267-765a-be2f-0cfe-5623421d6290"},"source":"### Take slices of the data to create the training (X) and target (Y) arrays"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29beacf5-23b7-285e-a1fe-55cabbf5a934"},"outputs":[],"source":"#Specify Target (i.e. class labels in classification): \nY_train = df_train.ix[:,1]\n\n#Do the same for the Training sample space\nX_train = df_train.ix[:,2:]\n\n#Y_train.head()\n#X_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7cc4231c-bf0d-7ea5-b5d4-385da9859b22"},"outputs":[],"source":"#Specify target from the TEST dataset\nY_test = df_test.ix[:,1]\n\n#Cut to match training df\nX_test = df_test.ix[:,2:]\n\n\n#Create a dataset of the test data's 'true' results to enable a comparison \n#with model output if required for Confusion Matrices\ndf_true_diagnosis = df_test[[\"diagnosis\"]]"},{"cell_type":"markdown","metadata":{"_cell_guid":"8da63dcf-59e5-6099-a545-25c52c4361a4"},"source":"### Plot the decision surfaces of various classification models to get an idea of how they fit to data"},{"cell_type":"markdown","metadata":{"_cell_guid":"446566d1-38e4-91a2-4095-6ecc6ffabcb7"},"source":"#### Each variable pair will use Concave Points Mean (index:7) plotted against progressively ranked important features Area_Worst (23), Concave Points_Worst (27), Perimeter_Worst (22) and Radius_Worst (20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a83f5e4-bd0a-a418-19fe-9b4988212b55"},"outputs":[],"source":"#http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py\nprint(__doc__)\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import clone\n\n#Don't scale data since we want to plot the support vectors\nn_estimators = 150\n\n# Models to plot\nmodels = [LogisticRegression(C=1),\n          ExtraTreesClassifier(n_estimators=150),\n          SVC(kernel='linear', C=1),\n          SVC(kernel='poly', degree=2, C=1000, probability = True),\n         GradientBoostingClassifier(n_estimators=150),\n         AdaBoostClassifier(n_estimators=150)]\n\n#Optional: RandomForestClassifier(n_estimators=n_estimators),\n\n# title for the plots\ntitles = ['Logit Reg','Extra Trees',\n          'SVC (linear kernel)','SVC (poly kernel)',\n         'GradBoost','Adaboost']\n\nfor pair in ([7, 23], [7, 27], [7,22]): #[7,20]\n    for i, clf in enumerate(models):\n        #Log transform pairs\n        #X = X_train.ix[:,pair]\n        #X.ix[pair] = X.ix[pair].apply(np.log10)\n        \n        #Normalise\n        X = X_train.ix[:, pair]\n        X = ((X - X.mean())/X.std())\n                \n        #Pass dataframes to variables\n        # = X_train\n        y = Y_train\n\n        h = .01 #h = step size in the mesh, want the decision surface to be very fine\n        \n        # create a mesh to plot in\n        x_min, x_max = X.ix[:, 0].min() - 0.05, X.ix[:, 0].max() + 0.05\n        y_min, y_max = X.ix[:, 1].min() - 0.05, X.ix[:, 1].max() + 0.05\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        plt.subplot(2, 3, i+1)\n        #plt.subplot(2, 2, i+1)\n        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n        \n        # Train\n        clf.fit(X, y)\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        \n        #Get scores\n        score = clf.score(X, y)\n                \n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n\n        # Also plot the training points\n        plt.scatter(X.ix[:, 0], X.ix[:, 1], c=y, cmap=plt.cm.coolwarm)\n        plt.xlabel(X.columns.values[0])\n        plt.ylabel(X.columns.values[1])\n        plt.xlim(xx.min(), xx.max())\n        plt.ylim(yy.min(), yy.max())\n        plt.xticks(())\n        plt.yticks(())\n        plt.title(titles[i] + \" Score: {:0.5f}\".format(score))\n    \n    print(\"{} vs {}\".format(X.columns.values[0],X.columns.values[1]))\n    plt.show()\n    print(\"_____________________________________________________________________\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"645ba900-1ecb-b8a8-8751-de3f26524e29"},"source":"## Parameter estimation using grid search with cross-validation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4df0cf2e-cbbf-91a7-c198-23a7e42d2c18"},"outputs":[],"source":"#http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html\nfrom __future__ import print_function\nfrom sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold\nfrom sklearn.metrics import classification_report,roc_curve, auc\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,AdaBoostClassifier\nprint(__doc__)\n\n#Scoring options for GridsearchCV\n#scores = ['precision_macro', 'recall_macro','f1_macro','accuracy','roc_auc']\nscores = ['accuracy']\n\ndef run_model(model_type,model_short_name,tuning_parameters):\n    for score in scores:\n        print(\"# Tuning hyper-parameters for %s on scoring method: %s\" % (model_short_name,score))\n        print()\n\n        clf = GridSearchCV(model_type, tuning_parameters, cv=10,\n                           scoring='%s' % score)\n        clf.fit(X_train, Y_train)\n             \n        print(\"Best parameters set found on development set:\")\n        print(clf.best_params_)\n        print()\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf.cv_results_['mean_test_score']\n        stds = clf.cv_results_['std_test_score']\n        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n            print(\"%0.5f (+/-%0.03f) for %r\"\n                  % (mean, std * 2, params))\n        print()\n\n        print(\"Detailed classification report:\")\n        print()\n        print(\"The model is trained on the full development set.\")\n        print(\"The scores are computed on the full evaluation set.\")\n        print()\n        y_true, y_pred = Y_test, clf.predict(X_test)\n        print(classification_report(y_true, y_pred))\n        print()\n        \n    return \n\n#Run models and tuning parameters to optimise ROC_AUC\n\n#1: Logistic Regression\nrun_model(LogisticRegression(),'Logit',[{'C': [1, 10, 100, 1000]}])\n\n#2: Random Forest\nrun_model(RandomForestClassifier(n_jobs=-1),'RF',[{'n_estimators':[10,50,100,150,200,250]}])\n\n#3: Extra Trees Classifier\nrun_model(ExtraTreesClassifier(n_jobs=-1),'ET',[{'n_estimators':[10,50,100,150,200,250]}])\n\n#4: SVM Classifier\nrun_model(SVC(C=1),'SVM',[{'kernel': ['linear'],'C': [1, 10, 100, 1000]}])\n\n#5: Gradient Boosting Classifier\n# Optional paramaters: learning_rate=1.0,max_depth=1,random_state=0\nrun_model(GradientBoostingClassifier(),'GradientBoost',[{'n_estimators':[10,50,100,150,200,250]}])\n\n#6: Adaboosting Classifier\n# Optional paramaters: learning_rate=1.0,random_state=0\nrun_model(AdaBoostClassifier(),'AdaBoost',[{'n_estimators':[10,50,100,150,200,250]}])"},{"cell_type":"markdown","metadata":{"_cell_guid":"e10191ca-39b4-b9f6-321d-fde695047348"},"source":"## Rerun GridsearchCV using log transformed data"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c2c7efb-6c44-95e5-8973-9d9c93354234"},"source":"As observed above in the exploratory data analysis, log transformed data provided far greater clarity into the data and a lot more explanatory power. Transform data and see whether it improves accuracy at all"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd093ca1-145e-1c43-cbda-dda7e012c308"},"outputs":[],"source":"#Define new Training sample space with features consistently defined in the top 10 of RF and ET models\n#Cut off ID and DIAGNOSIS columns\ndf_id_diag_train=df_train.loc[:,[\"id\",\"diagnosis\"]]\ndf_id_diag_test=df_test.loc[:,[\"id\",\"diagnosis\"]]\n\n#Log transform\nX_train_log = df_train.ix[:,2:].apply(np.log10)\nX_test_log = df_test.ix[:,2:].apply(np.log10)\n\n#Merge back on ID and DIAGNOSIS columns\nX_train_log=df_id_diag_train.join(X_train_log)\nX_test_log=df_id_diag_test.join(X_test_log)\n\n#Compare before and after log transforms\n#df_train.describe()\nX_train_log.describe()\n\n#-inf results in all columns with 'concavity', will just drop\nX_train_log = X_train_log.drop(['concavity_mean','concave points_mean','concavity_se','concave points_se','concavity_worst','concave points_worst'], 1)\nX_test_log = X_test_log.drop(['concavity_mean','concave points_mean','concavity_se','concave points_se','concavity_worst','concave points_worst'], 1)\n\nX_train_log.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8cbc34a7-47f1-f8a2-6392-45104c9d6f35"},"outputs":[],"source":"from __future__ import print_function\nfrom sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold\nfrom sklearn.metrics import classification_report,roc_curve, auc\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,AdaBoostClassifier\nprint(__doc__)\n\n#Re-run GRIDSEARCH with log transformed data\nscores = ['accuracy']\n\ndef run_model(model_type,model_short_name,tuning_parameters):\n    for score in scores:\n        print(\"# Tuning hyper-parameters for %s on scoring method: %s\" % (model_short_name,score))\n        print()\n\n        clf = GridSearchCV(model_type, tuning_parameters, cv=10,\n                           scoring='%s' % score)\n        clf.fit(X_train_log, Y_train)\n             \n        print(\"Best parameters set found on development set:\")\n        print(clf.best_params_)\n        print()\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf.cv_results_['mean_test_score']\n        stds = clf.cv_results_['std_test_score']\n        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n            print(\"%0.5f (+/-%0.03f) for %r\"\n                  % (mean, std * 2, params))\n        print()\n\n        print(\"Detailed classification report:\")\n        print()\n        print(\"The model is trained on the full development set.\")\n        print(\"The scores are computed on the full evaluation set.\")\n        print()\n        y_true, y_pred = Y_test, clf.predict(X_test_log)\n        print(classification_report(y_true, y_pred))\n        print()\n        \n    return \n\n#Run models and tuning parameters to optimise ROC_AUC\n\n#1: Logistic Regression\nrun_model(LogisticRegression(),'Logit',[{'C': [1, 10, 100, 1000]}])\n\n#2: Random Forest\nrun_model(RandomForestClassifier(n_jobs=-1),'RF',[{'n_estimators':[10,50,100,150,200,250]}])\n\n#3: Extra Trees Classifier\nrun_model(ExtraTreesClassifier(n_jobs=-1),'ET',[{'n_estimators':[10,50,100,150,200,250]}])\n\n#4: SVM Classifier (taking too long)\n#run_model(SVC(C=1),'SVM',[{'kernel': ['linear'],'C': [1, 10, 100, 1000]}])\n\n#5: Gradient Boosting Classifier\n# Optional paramaters: learning_rate=1.0,max_depth=1,random_state=0\nrun_model(GradientBoostingClassifier(),'GradientBoost',[{'n_estimators':[10,50,100,150,200,250]}])\n\n#6: Adaboosting Classifier\n# Optional paramaters: learning_rate=1.0,random_state=0\nrun_model(AdaBoostClassifier(),'AdaBoost',[{'n_estimators':[10,50,100,150,200,250]}])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a3bd9c4-a20d-5e85-f2d1-ff6b346b2250","collapsed":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"6fa9fc96-7639-ee04-b2e9-7df523df5e41"},"source":"100% accuracy on Extra Trees Classifier, Gradient Boost and Adaboost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab3cedde-58f4-f882-ff21-b680c06d5671"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}