{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings  \nwarnings.filterwarnings(\"ignore\")   # ignore warnings\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c529e718781dabb01f77a06b24cd2a9154c89a7"},"cell_type":"markdown","source":"**CLASSIFICATION ALGORITHMS**"},{"metadata":{"_uuid":"c48b5c7afd3698f29e8a9d684a31c137a93d7bb2"},"cell_type":"markdown","source":"**In this kernel, 6 different classification algorithms and an evaluation method have been explained for machine learning. Scikit - learn library has been used.  \nAt the end of the kernel, ROC curve which is the evaluation method, has been explained.**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Classification algoritms are generally used to predict for categoric data sets."},{"metadata":{"_uuid":"79c035b54d5c2217c4585b0f7cb3cdf12290e379"},"cell_type":"markdown","source":"**EXPLORATORY DATA ANALYSIS (EDA)**"},{"metadata":{"_uuid":"c04a4ff9e2512b2a11dfc132b021591040f5f820"},"cell_type":"markdown","source":"In this part, we try to understand the features of data."},{"metadata":{"trusted":true,"_uuid":"26fef6c98d5d6a32af36c7555268be113f8ba087"},"cell_type":"code","source":"# Load the data from csv file\ndata = pd.read_csv('../input/StudentsPerformance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa264dc9aafec8cc2c215cc29e73a78a3eb99524"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33da6727b62e34d22b625c6ca3a55b9dfd2fd455"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e23b920dcd6fbee0840d63aa99030c9dfcddfb4d"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31ee795ea11d968cb6bb78257831aa1f8e04353c"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e3b2881236ee6adc6523fafd99a1b21eec21abb"},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be213950f2732223bca00d913c95b25c960ac7c6"},"cell_type":"code","source":"# Correlation\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51ced9d3fd7132de9292b2520980b135e66ea466"},"cell_type":"markdown","source":"**Classification Algorithms**"},{"metadata":{"_uuid":"d7c9e8bf7997a5da44d71914549145d9d4cd74fd"},"cell_type":"markdown","source":"**1. Logistic Regression**"},{"metadata":{"_uuid":"0994ceba6774352e43279b218649a278a2c8ae3b"},"cell_type":"markdown","source":"* Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. \n* In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). "},{"metadata":{"trusted":true,"_uuid":"2202cc8497e8337209a11e7a5eefb398bd4402ed"},"cell_type":"code","source":"# Independent variables\nx = data.iloc[:, -3:]\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81712a45c7dd76f94c7b783cbf5b05692e53761f"},"cell_type":"code","source":"# Dependent variable\ny = data.iloc[:, 0:1]\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e601c68da067d9b8168bc625c6815c31a5a5ca00"},"cell_type":"code","source":"# Dividing into the data as train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3d5745dba4f7927ac810f432ea71f66a0170a43"},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"820fd613afeff26ad8dccfab66bbb087781cd3fe"},"cell_type":"code","source":"x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fccc6261dff655735bafe13a4be711b06ed9769"},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"761737146dd955848f7ae0a75f84dc86d7e05f7a"},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8475f2827db9e1a960bd594a865420f45d10ee96"},"cell_type":"code","source":"# Scaling of data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)  # training and transforming from x_train\nX_test = sc.transform(x_test)    # only transforming from x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd085ae05fef4d54f470e653042c5cc7d67addfb"},"cell_type":"code","source":"# Creation of model\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(random_state=0)\nlog_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"440a837f35e24b9a90c157bfd69de6cccd6b1918"},"cell_type":"code","source":"# Prediction\ny_pred = log_reg.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b696d45cc8a355ad93e259b742a16cf06962e6b"},"cell_type":"markdown","source":"**Confusion Matrix**"},{"metadata":{"_uuid":"0de65087949da6cd75e09d26ecc38926ee7320d6"},"cell_type":"markdown","source":"A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. \n\ntp = true positive, fp = false positive, fn = false negative, tn = true negative\n\n* tp = Prediction is positive and actual is positive.\n\n* fp = Prediction is positive and actual is negative. (Also known as a \"Type I error.\")\n\n* fn = Prediction is negative and actual is positive. (Also known as a \"Type II error.\")\n\n* tn = Prediction is negative and actual is negative.\n\n\nValues of diagonal of matrix always give us successful classification.\n\n* accuracy : the percentage of correct classification for model\n* sensitivity = True positive rate = tp / (tp + fn)\n* specificity = tn / (tn + fp)\n* precision = tp / (tp+fp)\n* recall = tp / (tp+fn)\n* fall-out = False positive rate = fp / (tn + fp)\n* error rate = 1 - acccuracy"},{"metadata":{"trusted":true,"_uuid":"cfa01558fbf0c7aeb9f1320e27d30b24c48686bb"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('*********************')\nprint('*********************')\nprint(294/330) # Accuracy\n# As it is seen below, the model has been predicted 294 values correctly from 330 values. ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e31641c4edc9da4660d6999424f0dcd69124f3e"},"cell_type":"markdown","source":"**2. K-NN (K Nearest Neighborhood)**"},{"metadata":{"trusted":true,"_uuid":"a83e2e92f60d3417b17864edca3ec05b9b58f742"},"cell_type":"markdown","source":"* K-NN is one of the strongest classification algorithms. \n* It is also basic, fast and widely used. \n* In KNN, K is the number of nearest neighbors. "},{"metadata":{"trusted":true,"_uuid":"6ecaa933df3d8786f36c8f616b08ae6529e02149"},"cell_type":"code","source":"# Creation of model\n# K = 5 (Default value of the algorithm)\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, metric='minkowski')\nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e7ad4f50ab8d355be4832c18d1dce65aac37c96"},"cell_type":"code","source":"# Prediction \ny_pred2 = knn.predict(X_test)\n\n# Confusion matrix\ncm2 = confusion_matrix(y_test, y_pred2)\nprint(cm2)\nprint('*********************')\nprint('*********************')\nprint(288/330) # Accuracy\n# As it is seen below, the model has been predicted 288 values correctly from 330 values. ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c385758705def7bc03ca30443654cdc4b0bed6a"},"cell_type":"markdown","source":"**3. Support Vector Machine (SVM)**"},{"metadata":{"_uuid":"2fa159aa50e0e25baa31405266e9f321e57e9b25"},"cell_type":"markdown","source":"In this algorithm, the aim is to obtain linear, polynomial, gaussian or exponential function that divides into the data points with maximum margin.\n\nScaling, in other words standardization is very important for this method."},{"metadata":{"trusted":true,"_uuid":"845c9614f4ee37311f646406732818f4dbcfaf48"},"cell_type":"code","source":"# Creation of model\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear') \nsvc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dd64edbede6b7846ac12b097fc83d688474eaf6"},"cell_type":"code","source":"# Prediction\ny_pred3 = svc.predict(X_test)\ny_pred3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"637736e8ec694431311128410a76317823854624"},"cell_type":"code","source":"# Confusion Matrix\ncm3 = confusion_matrix(y_test, y_pred3)\nprint(cm3)\nprint('*********************')\nprint('*********************')\nprint(291/330) # Accuracy\n# As it is seen below, the model has been predicted 291 values correctly from 330 values. ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a91dded0dbf5445121efee2ca793574fa3a9a7c"},"cell_type":"markdown","source":"**Kernel Trick**"},{"metadata":{"_uuid":"086d4c87c9326703f355945963d6aca207ef818a"},"cell_type":"markdown","source":"Kernel functions such as poly, rbf are preferred especially to classify non-linear datasets."},{"metadata":{"trusted":true,"_uuid":"f749807896e302c7c274e5144ea2bc1e839d6ea8"},"cell_type":"code","source":"# Here, Kernel has been chosen as \"rbf\".\nsvc2 = SVC(kernel='rbf') \nsvc2.fit(X_train, y_train)\n\ny_pred_3 = svc2.predict(X_test)\ny_pred_3\n\ncm_3 = confusion_matrix(y_test, y_pred_3)\nprint(cm_3)\nprint('*********************')\nprint('*********************')\nprint(292/330) # Accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6b1211b00298d808916dd64621b7164592cd633"},"cell_type":"markdown","source":"**4. Naive Bayes**"},{"metadata":{"_uuid":"b3d905a2b6708160d67a8dba0da752b2197199d1"},"cell_type":"markdown","source":"* This method can work with unbalanced data sets.  \n* It works on Bayes theorem of probability to predict the class of unknown data set. \n* Naive Bayes model is easy to build and particularly useful for very large data sets.\n* There are three types of Naive Bayes model under scikit learn library: Gaussian, Multinomial, Bernoulli."},{"metadata":{"trusted":true,"_uuid":"9c90cda4d59a2c7d50b8f56724c61158f4e57805"},"cell_type":"code","source":"# Creation of model\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bc6342beebdc16ed7e70087f76f1743f1b551d6"},"cell_type":"code","source":"# Prediction\ny_pred4 = gnb.predict(X_test)\ny_pred4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"499e8656b6238118ddad575862310f43431ff136"},"cell_type":"code","source":"# Confusion matrix\ncm4 = confusion_matrix(y_test, y_pred4)\nprint(cm4)\nprint('*********************')\nprint('*********************')\nprint(229/330) # Accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0493f651e164369c50eaff5238793490464925cf"},"cell_type":"code","source":"# Creation of model\n# In Multinomial naive bayes, input x_train must be non-negative.\nfrom sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eefbf433c936c926f3dbefd16dae869b34f846c9"},"cell_type":"code","source":"# Prediction\ny_pred_4 = mnb.predict(x_test)\ny_pred_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"502d5deed9d0c48b2a0a0707045f394ff42b4e92"},"cell_type":"code","source":"# Confusion matrix\ncm_4 = confusion_matrix(y_test, y_pred_4)\nprint(cm_4)\nprint('*********************')\nprint('*********************')\nprint(294/330) # Accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"139a9c091399d4096ab93237786f2f103c2ba27e"},"cell_type":"markdown","source":"**5. Decision Tree**"},{"metadata":{"_uuid":"b2a42ddfc37d3c9dd7c02e5138cdb03026973396"},"cell_type":"markdown","source":"* Decision tree builds classification model in the form of a tree structure. \n* It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. \n* The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification. \n* Decision trees can handle both categorical and numerical data. \n* The core algorithm for building decision trees called ID3.\n* ID3 uses Entropy and Information Gain to construct a decision tree. \n* An independent variable that provides maximum information gain, is chosen as root node and the other nodes are chosen with using same method."},{"metadata":{"trusted":true,"_uuid":"bffd6b17a493261f2aed0a657d85335ddb49f174"},"cell_type":"code","source":"# Creation of model\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion='entropy')\ndtc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16cb5fb7e144461ba7ecd41f9e25557303bc65fb"},"cell_type":"code","source":"# Prediction\ny_pred5 = dtc.predict(X_test)\ny_pred5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bff5d4c09d7d205cd56311fb8da9518d30e922e"},"cell_type":"code","source":"# Confusion matrix\ncm5 = confusion_matrix(y_test, y_pred5)\nprint(cm5)\nprint('*********************')\nprint('*********************')\nprint(271/330) # Accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77ba25e60655f5f5eacf3d90612b52d7a21ad5f1"},"cell_type":"markdown","source":"**6. Random Forest**"},{"metadata":{"_uuid":"610b59a416b6ec0347d8dea328f52a7880b0051e"},"cell_type":"markdown","source":"* Random Forests grows many classification trees.\n* To classify a new object from an input vector, put the input vector down each of the trees in the forest. \n* Each tree gives a classification, and we say the tree \"votes\" for that class. \n* The forest chooses the classification having the most votes (over all the trees in the forest). This is known as majority vote."},{"metadata":{"trusted":true,"_uuid":"c37ef122d40a59718858431974d2c5b99ba2a3e6"},"cell_type":"code","source":"# Creation of model\n# Criterion' s default value is \"gini\".\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=10, criterion='entropy')\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0161d1d6348bcc335214897ccbef90211e7cb1b"},"cell_type":"code","source":"# Prediction\ny_pred6 = rfc.predict(X_test)\ny_pred6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0ebc9d0125439e728defa00e5917df585675931"},"cell_type":"code","source":"# Confusion matrix\ncm6 = confusion_matrix(y_test, y_pred6)\nprint(cm6)\nprint('*********************')\nprint('*********************')\nprint(278/330) # Accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6731dd435799296d7806ec3ca13b0e45125cd67f"},"cell_type":"markdown","source":"**ROC CURVE (Receiver Operating Characteristic)**"},{"metadata":{"trusted":true,"_uuid":"e486c82e5896ab6a69436986e4dc49ab470c65bf"},"cell_type":"markdown","source":"* The ROC curve is a fundamental tool for diagnostic test evaluation.\n* In a ROC curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points of a parameter.\n* Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. \n* The area under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups (diseased/normal).\n* The closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test.\n\n* fpr = False Positive Rate\n* tpr = True Positive Rate"},{"metadata":{"trusted":true,"_uuid":"f3cc7b2dd08cb624caab5a0eace1fdc0231cb571"},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d970040b11f0bc2783535f774b0a869cf2b91269"},"cell_type":"code","source":"# ROC Curve with Random Forest Classification\ny_proba_6 = rfc.predict_proba(X_test)\ny_proba_6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8672003934e585ff204caf7b319f3ac6f764334f"},"cell_type":"code","source":"fpr, tpr, thold = metrics.roc_curve(y_test, y_proba_6[:,1], pos_label='male')\nprint(y_test)\nprint(y_proba_6[:,1])\nprint('fpr')\nprint(fpr)\nprint('tpr')\nprint(tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b81576f1efd62bd32b59c254e28246e8df8a11e"},"cell_type":"code","source":"# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"738490372f3c321003e36347a638959e46df68cc"},"cell_type":"markdown","source":"**CONCLUSION**"},{"metadata":{"_uuid":"3be928ca4543325bc0129ed3e35ab5feba64df0f"},"cell_type":"markdown","source":"\nMy other kernels are here: \n\nhttps://www.kaggle.com/armagansarikey/machine-learning-1-data-preprocessing\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-2-prediction-algorithms\n\nIf you have any question or suggest, I will be happy to hear it."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}