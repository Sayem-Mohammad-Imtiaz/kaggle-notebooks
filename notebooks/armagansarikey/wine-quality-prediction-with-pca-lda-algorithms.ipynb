{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings  \nwarnings.filterwarnings(\"ignore\")   # ignore warnings\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**In this kernel, the aim is to predict to quality of wine correctly by using features of wine. Data is reduced from 11 columns to 2 columns with using PCA and LDA algorithms and the results of algorithms are evaluated.**"},{"metadata":{"trusted":true,"_uuid":"a6903f26240c33a7ed537fcaabbca0cd0125806c"},"cell_type":"code","source":"wine = pd.read_csv('../input/winequality-red.csv')\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0a51bd65b9099c78156bf157176c250079bd406"},"cell_type":"code","source":"wine.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4b6bc37b5c0cd9eb40c64f7010bad8325110e15"},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true,"_uuid":"c7dfbd5fc4a2b4443eb42f87d36b3b787e4018cc"},"cell_type":"code","source":"X = wine.iloc[:,0:11].values\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd91006e69c1fb3ad6b3d7af9bfa0a5d68c41dde"},"cell_type":"code","source":"y = wine.iloc[:,11].values\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b7c9ace42db81d922f75e7163a4890515ef514"},"cell_type":"code","source":"# Creation of train and test sets from the data.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"806c6098319ce0f146e6c0959b7cf11bbf503c54"},"cell_type":"code","source":"# Scaling with Standardization\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ea1f575b21c50d7458f7f41911abc170ede3672"},"cell_type":"markdown","source":"# **PCA (Principal Component Analysis)**"},{"metadata":{"_uuid":"638a0306a6319859316aa2a88eeaec3ce3c24303"},"cell_type":"markdown","source":"* PCA is one of the most broadly used of unsupervised algorithms. \n* PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more. \n* Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data.\n* In PCA technique, we find the component axes that maximize the variance of our data."},{"metadata":{"trusted":true,"_uuid":"cae5438220c2d76aa46bcdeffa2a25151c31cb62"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components= 2)  # we will reduce the data set from 11 columns to 2 columns.\nX_train2 = pca.fit_transform(X_train) # fit means train, fit_transform means train and apply to a data set.\nX_test2 = pca.transform(X_test)  # Only transformation\n\n# X_train2 is a 2 dimensional data set.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3db0b01cd3b7e57a7c0fd1d066dc1c2ab31107ec"},"cell_type":"markdown","source":"## Machine Learning Part"},{"metadata":{"_uuid":"851edf1e4d4b54684e495e5d51f8336e700ef2f3"},"cell_type":"markdown","source":"In this part, Logistic regression (LR) has been used as a classification algorithm."},{"metadata":{"trusted":true,"_uuid":"6fa5511fe05ada63df14d04d23cc822da621a224"},"cell_type":"code","source":"# LR before PCA transformation\nfrom sklearn.linear_model import LogisticRegression\n\n# random_state = 0 because the model will be used two times and we want to have same structure.\n# Thus, same LR algorithm structure will run.\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c238eabdb1e6c3c352e0b959bd4cc358ab21d641"},"cell_type":"code","source":"# LR after PCA transformation\n\nclassifier2 = LogisticRegression(random_state=0)\nclassifier2.fit(X_train2, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d40ead1586fab0d7c6e4fd20b4bafa8f90103743"},"cell_type":"code","source":"# Predictions\n\n# Prediction from the data that is not applied PCA.\ny_pred = classifier.predict(X_test) \n\n# Prediction from the data that is applied PCA.\ny_pred2 = classifier2.predict(X_test2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea8949538ebec717b305a8e135f17100b6e10220"},"cell_type":"code","source":"# Evaluation\n\nfrom sklearn.metrics import confusion_matrix\n\n# actual / result without PCA\nprint('actual / without PCA')\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('          ')\n\n# actual / result with PCA\nprint('actual / with PCA')\ncm2 = confusion_matrix(y_test, y_pred2)\nprint(cm2)\nprint('          ')\n\n# after PCA / before PCA\nprint('without PCA / with PCA')\ncm3 = confusion_matrix(y_pred, y_pred2)\nprint(cm3)\n\n# acuracy is 0.63 without PCA.\n# accuracy is 0.56 with PCA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad175b13074f0a99ecdcc1367c84ff003dbe55fe"},"cell_type":"markdown","source":"# LDA (Linear Discriminant Analysis)"},{"metadata":{"trusted":true,"_uuid":"2ed6be400d24903201a03e094001938e41229b19"},"cell_type":"markdown","source":"* Linear Discriminant Analysis (LDA) is the most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. \n* The goal is to project a dataset onto a lower-dimensional space with good class-separability in order to avoid overfitting (“curse of dimensionality”) and also reduce computational costs.\n* The general LDA approach is very similar to a Principal Component Analysis.\n* In contrast to PCA, LDA is “supervised” and computes the directions (“linear discriminants”) that will represent the axes that maximize the separation between multiple classes."},{"metadata":{"trusted":true,"_uuid":"30d47d71a0690a5277ba4e73e8ea5fce6f08ac2e"},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA(n_components = 2)\nX_train_lda = lda.fit_transform(X_train, y_train)\nX_test_lda = lda.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b8d09faef7c7baba46bcfef8bf29d67817830da"},"cell_type":"code","source":"# LR after LDA transformation\n\nclassifier_lda = LogisticRegression(random_state=0)\nclassifier_lda.fit(X_train_lda, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fba1e1df03668fa5d7983b76aedce8046b21322"},"cell_type":"code","source":"# Predictions of LDA data\n\ny_pred_lda = classifier_lda.predict(X_test_lda)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ce68bf86805bd2e226d5ae6652880156db90c87"},"cell_type":"code","source":"# Evaluation with Confusion matrix\n\n# original / After LDA \nprint('Original & LDA')\ncm4 = confusion_matrix(y_pred, y_pred_lda)\nprint(cm4)\nprint('          ')\n\n# actual / result without LDA\nprint('actual / without LDA')\ncm5 = confusion_matrix(y_test, y_pred)\nprint(cm5)\nprint('          ')\n\n# actual / result with LDA\nprint('actual / with LDA')\ncm6 = confusion_matrix(y_test, y_pred_lda)\nprint(cm6)\n\n# acuracy is 0.63 without LDA.\n# acuracy is 0.62 with LDA.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5966d44b4cf59412d2f1d0e5a3b9e2881e615159"},"cell_type":"markdown","source":"## CONCLUSION"},{"metadata":{"_uuid":"727cabbc2a72e78c62aa89a440764f2fdb657ee0"},"cell_type":"markdown","source":"After dimensional reduction, there is so little decrease in the accuracy. According to the results, both PCA and LDA are very successful algorithms for this study. \n\nMy other kernels are here:\n\nhttps://www.kaggle.com/armagansarikey/data-science-project\n\nhttps://www.kaggle.com/armagansarikey/data-visualization-project\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-1-data-preprocessing\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-2-prediction-algorithms\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-3-classification-algorithms\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-4-clustering-algorithms\n\nhttps://www.kaggle.com/armagansarikey/natural-language-processing-nlp-project\n\nIf you have any question or suggest, I will be happy to hear it.\n\nIf you like it, please upvote :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}