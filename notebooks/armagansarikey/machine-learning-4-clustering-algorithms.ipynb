{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings  \nwarnings.filterwarnings(\"ignore\")   # ignore warnings\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb7eb179ac09c276cacbff475829932ffe025ca8"},"cell_type":"markdown","source":"**In this kernel, 2 different clustering algorithms have been explained for machine learning.**"},{"metadata":{"_uuid":"de68911516802bf3f4de2616c8d56ceb3e55cdf3"},"cell_type":"markdown","source":"**CLUSTERING ALGORITHMS**"},{"metadata":{"_uuid":"3dec55c3e1b862b1d878150561f47c36c0158a1c"},"cell_type":"markdown","source":"* Clustering is the technique of dividing the data points into a number of groups such that data points in the same groups are more similar than the others.\n* Clustering is an unsupervised learning method. "},{"metadata":{"_uuid":"c1e23e0b0f5bee6ba202b42b0cc7578f716669e4"},"cell_type":"markdown","source":"**1. K - Means**"},{"metadata":{"trusted":true,"_uuid":"244711557c66887e99e931c02e03942fffc78792"},"cell_type":"markdown","source":"* K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). \n* The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. \n* The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. \n* Data points are clustered based on feature similarity."},{"metadata":{"trusted":true,"_uuid":"ae59204527636978f5e6735359a598582fd882be"},"cell_type":"code","source":"data = pd.read_csv('../input/Mall_Customers.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a196729f2df65adb66142dc9f5ddd208d9956f9a"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3fcb9e9840be3b3e685d7559d9dcff4d701539b"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"799c95d60b3f8085815ee993b5ffb7feebf6b8d4"},"cell_type":"code","source":"X = data.iloc[:, 3:].values\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f454ebfbbeeed1aedbfb028993aa96201123d0da"},"cell_type":"code","source":"# Loading Library\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd3a1f7f6888df0ac08f8122e4138bac3c93b897"},"cell_type":"markdown","source":"**WCSS (Within-Cluster Sums of Squares)**\n* Let’s take there are 3 clusters. That means, we have 3 center points (C1, C2, C3). Each data point falls into the zone of either C1 or C2 or C3. \n* First we calculate the sum of squares of the distance of each data point in cluster 1 from their center point C1. \n* This is cluster 1 sum of squares.\n[dist(C1, c1p1) ]² + [dist(C1, c1p2)]² + [dist(C1, c1p3)]². \n* Similarly we do the same for C2 & C3. \n* We add the sum of all 3 clusters sum of squares to get **WCSS**.\n* WCSS always decreases with the increase in the number of clusters."},{"metadata":{"trusted":true,"_uuid":"d41c8b04514583b7646f89e8e058a0bb55e93af1"},"cell_type":"code","source":"# To decide variable K, we use WCSS.\nresult = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters = i, init='k-means++', random_state = 123)\n    kmeans.fit(X)\n    result.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a24fcce22107765c185b6c7dc9f70a8ee7119c91"},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62f9206545e31ba047d5a1fef06e1e1215b61187"},"cell_type":"code","source":"plt.plot(range(1,11), result)\nplt.xlabel('the number of clusters')\nplt.ylabel('result')\nplt.title('The Elbow Method')\nplt.show()\n# From below figure, we can choose K as 3 because there is a decreasing of acceleration at that point. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61bc52a638d5b8f5aea31d8d91df870fc4d9a3c9"},"cell_type":"code","source":"# Applying K-Means Algorithm\n# Creation of model\nkmeans = KMeans(n_clusters = 3, init='k-means++')\nkmeans.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"341ac0c5a6fe63440fdb64c9232874e62d8af500"},"cell_type":"code","source":"# We learn center point of each cluster with below function.\n# For example, first column is Annual Income (k$) whose center point is 44.15447154 for first cluster.\nkmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72f8d0883d5fb4b0c020067dcf335ec864068821"},"cell_type":"code","source":"# Prediction\ny_kmeans = kmeans.fit_predict(X)\ny_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdc388feccc3ff3dd9379dc8fe447faef88ca651"},"cell_type":"code","source":"# Visualising the clusters\nplt.scatter(X[y_kmeans == 0,0], X[y_kmeans == 0,1], s=100, c='red')\nplt.scatter(X[y_kmeans == 1,0], X[y_kmeans == 1,1], s=100, c='blue')\nplt.scatter(X[y_kmeans == 2,0], X[y_kmeans == 2,1], s=100, c='green')\nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=300, c='yellow')\nplt.title('K-Means Clustering')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a860403389a721ba9ff6d5907da95731a62fd436"},"cell_type":"markdown","source":"**2. Hierarchical Clustering**"},{"metadata":{"_uuid":"17217bc1227535e99dfce5e641dbaf401dda5099"},"cell_type":"markdown","source":"* Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. \n* This hierarchy of clusters is represented as a dendrogram. \n* Hierarchical clustering can be performed with distance matrix.\n\n**Agglomerative :** bottom up approach, each observation starts in its own cluster, and clusters are successively merged together.\n\n** Divisive :** top down approach."},{"metadata":{"trusted":true,"_uuid":"f5707070b925514a3815af357980d583731879a5"},"cell_type":"code","source":"# Creation of model & prediction\nfrom sklearn.cluster import AgglomerativeClustering\nac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\nY_predict = ac.fit_predict(X)\nY_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c35ba3e480f15cc1ff46f2a84f1026db739cb788"},"cell_type":"code","source":"# Visualising the clusters\nplt.scatter(X[Y_predict==0,0], X[Y_predict==0,1], s=100, c='red')\nplt.scatter(X[Y_predict==1,0], X[Y_predict==1,1], s=100, c='blue')\nplt.scatter(X[Y_predict==2,0], X[Y_predict==2,1], s=100, c='green')\nplt.title('Hierarchical Clustering')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35bf5936adcf112ece39275f5facb00b921a4287"},"cell_type":"markdown","source":"**Dendrogram**"},{"metadata":{"trusted":true,"_uuid":"d8dec5c03b3b48343ccb4ff3cf06a1bd58692b0f"},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e40e80bd1d644736fc3141b95d51f41e2166820"},"cell_type":"code","source":"dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f2016f7d0636fa2f2754da80f919e3179042b4a"},"cell_type":"markdown","source":"**CONCLUSION**"},{"metadata":{"_uuid":"b682061939f028de29bef1efb322b256da8eed25"},"cell_type":"markdown","source":"My other kernels are here:\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-1-data-preprocessing\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-2-prediction-algorithms\n\nhttps://www.kaggle.com/armagansarikey/machine-learning-3-classification-algorithms\n\nIf you have any question or suggest, I will be happy to hear it."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}