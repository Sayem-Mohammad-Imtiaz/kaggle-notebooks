{"cells":[{"metadata":{},"cell_type":"markdown","source":"Xiao Song 宋骁\n\n\nreferences:\n\nhttps://www.kaggle.com/rikdifos/eda-vintage-analysis  \n\nhttps://www.kaggle.com/rikdifos/credit-card-approval-prediction-using-ml/\n\nhttps://www.kaggle.com/jiweiliu/lgb-2-leaves-augment\n\n[王汉生：再论正负样本分布不均衡问题](https://mp.weixin.qq.com/s/y-IEltRsmNdbZGyO2hBrwA)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom scikitplot.metrics import plot_roc_curve\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, balanced_accuracy_score, roc_curve, auc, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\napplication_record = pd.read_csv('../input/credit-card-approval-prediction/application_record.csv') \ncredit_record = pd.read_csv('../input/credit-card-approval-prediction/credit_record.csv')  \ndata = pd.read_pickle('../input/credit-card-approval-prediction-using-ml/data.pkl')\ndata['ID'] =  data['ID'].astype('int64')\ndata['Gender'] =  data['Gender'].astype('int64')\ndata['Car'] =  data['Car'].astype('int64')\ndata['Reality'] =  data['Reality'].astype('int64')\ndata['FLAG_MOBIL'] =  data['FLAG_MOBIL'].astype('int64')\nprint('data has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.facecolor'] = 'white'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# The label is highly unbalanced. Positive class occupies 422/422+24712 = 0.0168.\ndata['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_dt(dt,col_list):\n    '''get test set'''\n    y = dt['target']\n    y = y.astype('int')\n    x = dt[col_list]\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state = 1024)\n    print(pd.Series(y_test).value_counts(normalize=True))\n    return x_train, x_test, y_train, y_test ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = ['Gender','Reality','ChldNo_1', 'ChldNo_2More','wkphone',\n              'gp_Age_high', 'gp_Age_highest', 'gp_Age_low',\n       'gp_Age_lowest','gp_worktm_high', 'gp_worktm_highest',\n       'gp_worktm_low', 'gp_worktm_medium','occyp_hightecwk', \n              'occyp_officewk','famsizegp_1', 'famsizegp_3more',\n       'houtp_Co-op apartment', 'houtp_Municipal apartment',\n       'houtp_Office apartment', 'houtp_Rented apartment',\n       'houtp_With parents','edutp_Higher education',\n       'edutp_Incomplete higher', 'edutp_Lower secondary','famtp_Civil marriage',\n       'famtp_Separated','famtp_Single / not married','famtp_Widow']\nx_train, x_test, y_train, y_test = split_dt(data, col_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, I won't use any of data augment method and calculate accuracy accore. However, this accuracy is meaningless. Cause when classifiers guess biggest class, the accuracy will be overestimated. As a result, I use balanced_accuracy_score\n\ndef classifier_metrics(x_train, y_train, x_test):\n    '''print classification evaluation metrics'''\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    model = XGBClassifier(max_depth=12,\n                          n_estimators=250,\n                          min_child_weight=8, \n                          subsample=0.8, \n                          colsample_bytree=0.8,\n                          learning_rate=0.02,    \n                          seed=42)\n\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    y_pred_proba0 = model.predict_proba(x_test)\n    y_pred_proba = y_pred_proba0[:, 1] \n    print('test set accuracy score is {:.5}'.format(accuracy_score(y_test, y_pred)))\n    print('test set balanced accuracy score is {:.5}'.format(balanced_accuracy_score(y_test, y_pred)))\n    print('test set roc_auc_score is {:.5}'.format(roc_auc_score(y_test, y_pred_proba)))\n    print('test set log_loss is {:.5}'.format(log_loss(y_test, y_pred_proba)))\n    print('test set precision_score is {:.5}'.format(precision_score(y_test, y_pred)))\n    print('test set recall_score is {:.5}'.format(recall_score(y_test, y_pred)))\n    print('test set f1_score is {:.5}'.format(f1_score(y_test, y_pred)))\n    plot_roc_curve(y_test, y_pred_proba0)\n    plt.show()\n    \nclassifier_metrics(x_train, y_train, x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy score = 0.98, don't be confused with the high score, that just because 98.3% of label is 1. Then we use `balanced_accuracy_score`, it shows that if samples are balanced, the real accuracy is 0.5, which equals random guessing. Next we try to use `SMOTE` to oversample our data. \n\nOn the other h"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_bal, y_bal = SMOTE().fit_sample(x_train, y_train)\npd.Series(y_bal).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_metrics(x_bal, y_bal, x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now experiment using Jiwe Liu’s data augment method to see if the auc roc curve has improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augment(x, y, t, col):\n    '''data augment using random shuffle'''\n    mask = y>0\n    x1 = x[mask].copy() # positive samples\n    y1 = y[mask].copy() # positive samples\n    for i in range(t): # augment t times\n        for j in col:\n            np.random.shuffle(x1.loc[:,j].values)\n        x = x.append(x1)\n        y = y.append(y1)\n    print(pd.Series(y).value_counts())\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_aug, y_aug = data_augment(x_train, y_train, 40, x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_metrics(x_aug, y_aug, x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the effect is not as good as that of smooth, which may be because we have augmented the data encoded by onehot. If it is augmented and then encoded by onehot, the situation may be different."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_eng(data, record):\n    '''do some fe'''\n    begin_month = pd.DataFrame(record.groupby([\"ID\"])[\"MONTHS_BALANCE\"].agg(min))\n    begin_month = begin_month.rename(columns={'MONTHS_BALANCE':'begin_month'}) \n    new_data = pd.merge(data, begin_month,how=\"left\",on=\"ID\") #merge to record data\n    \n    record['target'] = None\n    record['target'][record['STATUS'] =='2'] = 'Yes' \n    record['target'][record['STATUS'] =='3'] = 'Yes' \n    record['target'][record['STATUS'] =='4'] = 'Yes' \n    record['target'][record['STATUS'] =='5'] = 'Yes' \n    \n    target_cnt = record.groupby('ID').count()\n    target_cnt['target'][target_cnt['target'] > 0] = 1\n    target_cnt['target'][target_cnt['target'] == 0] = 0\n    target_cnt = target_cnt[['target']]\n    \n    new_data['age'] = -(new_data['DAYS_BIRTH']) // 365\n    new_data['work_years'] = -(new_data['DAYS_EMPLOYED']) // 365\t\n    new_data = pd.merge(new_data, target_cnt, how = 'inner', on = 'ID')\n    new_data.rename(columns={'CODE_GENDER':'gender','FLAG_OWN_CAR':'car','FLAG_OWN_REALTY':'reality',\n                         'CNT_CHILDREN':'child_no','AMT_INCOME_TOTAL':'income',\n                         'NAME_EDUCATION_TYPE':'edu_type','NAME_FAMILY_STATUS':'famliy_type',\n                         'NAME_HOUSING_TYPE':'house_type','FLAG_EMAIL':'email',\n                         'NAME_INCOME_TYPE':'income_type','FLAG_WORK_PHONE':'work_phone',\n                         'FLAG_PHONE':'phone','CNT_FAM_MEMBERS':'famliy_size',\n                        'OCCUPATION_TYPE':'occupation'\n                        },inplace=True)\n    \n    \n    new_data.loc[new_data['income_type'] == 'Pensioner','income_type'] = 'State servant'\n    new_data.loc[new_data['income_type'] == 'Student','income_type'] = 'State servant'\n    new_data.loc[(new_data['occupation']=='Cleaning staff') | (new_data['occupation']=='Cooking staff') | (new_data['occupation']=='Drivers') | (new_data['occupation']=='Laborers') | (new_data['occupation']=='Low-skill Laborers') | (new_data['occupation']=='Security staff') | (new_data['occupation']=='Waiters/barmen staff'),'occupation']='Laborwk'\n    new_data.loc[(new_data['occupation']=='Accountants') | (new_data['occupation']=='Core staff') | (new_data['occupation']=='HR staff') | (new_data['occupation']=='Medicine staff') | (new_data['occupation']=='Private service staff') | (new_data['occupation']=='Realty agents') | (new_data['occupation']=='Sales staff') | (new_data['occupation']=='Secretaries'),'occupation']='officewk'\n    new_data.loc[(new_data['occupation']=='Managers') | (new_data['occupation']=='High skill tech staff') | (new_data['occupation']=='IT staff'),'occupation']='hightecwk'\n    new_data.loc[new_data['edu_type']=='Academic degree','edu_type'] = 'Higher education'\n    \n    new_data.drop_duplicates(keep='first', inplace=True)\n    new_data.drop(['begin_month', 'DAYS_BIRTH','DAYS_EMPLOYED','FLAG_MOBIL'], axis=1,inplace=True)\n    new_data.loc[new_data['work_years'] < 0,'work_years'] = np.nan\n    new_data['work_years'].fillna(round(new_data['work_years'].mean()), inplace=True)\n    print('data has {} rows and {} columns'.format(new_data.shape[0], new_data.shape[1]))\n    return new_data\n\ndt = feature_eng(application_record, credit_record)\ndt.to_pickle('data_no_onehot.pkl')\ndt.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = ['gender', 'car', 'reality', 'child_no', 'income', 'income_type',\n           'edu_type', 'famliy_type', 'house_type', 'work_phone', 'phone', 'email',\n           'occupation', 'famliy_size', 'age', 'work_years']\n\nx_train, x_test, y_train, y_test = split_dt(dt, col_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def OneHotEncoder_transfrom(data, col):\n    '''try encoding'''\n    enc = ce.OneHotEncoder(cols = col).fit(data)\n    data2 = enc.transform(data)\n    return data2\n\ncat_cols = ['gender', 'car', 'reality', 'income_type', 'edu_type', 'famliy_type', 'house_type', 'work_phone', 'phone', 'email','occupation']\nx_org = OneHotEncoder_transfrom(x_train, cat_cols)\nx_test_org = OneHotEncoder_transfrom(x_test, cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_metrics(x_org, y_train, x_test_org)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_aug, y_aug = data_augment(x_train, y_train, 40, x_train.columns)\nx_aug = OneHotEncoder_transfrom(x_aug, cat_cols)\nx_aug_test = OneHotEncoder_transfrom(x_test, cat_cols)\nclassifier_metrics(x_aug, y_aug, x_aug_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc_curve(y,prob):\n    fpr,tpr,threshold = roc_curve(y,prob) ###计算真正率和假正率\n    roc_auc = auc(fpr,tpr) ###计算auc的值\n \n    plt.figure(figsize=(6, 4.5))\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc) ###假正率为横坐标，真正率为纵坐标做曲线\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(x_train, y_train,x_test, y_test, n_folds, cat):\n    '''train a lightgbm model cv\n    '''\n    features = list(x_train.columns)\n\n    params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 16,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 20,\n    'learning_rate': 0.1,\n    'verbose': 0,\n    'nthread': 4,\n    }\n\n    y_pred_proba = np.zeros(len(x_test))\n    \n    for fold, (trn_idx, val_idx) in enumerate(KFold(n_folds).split(x_train, y_train)):\n        evals_result = {} \n        print('Training fold {}'.format(fold + 1))\n        train_set = lgb.Dataset(x_train.iloc[trn_idx][features], \n                                y_train.iloc[trn_idx], \n                                categorical_feature = cat)\n        \n        val_set = lgb.Dataset(x_train.iloc[val_idx][features], \n                              y_train.iloc[val_idx], \n                              categorical_feature = cat)\n\n        model = lgb.train(params, train_set, \n                          num_boost_round = 300, \n                          early_stopping_rounds = 30, \n                          valid_sets = [train_set, val_set], \n                          evals_result = evals_result,\n                          verbose_eval = 50)\n\n        y_pred_proba += model.predict(x_test[features], \n                                    num_iteration = model.best_iteration)  / n_folds \n        y_pred = np.where(y_pred_proba>0.413, 1, 0)\n\n        print('-' * 50)\n        print('\\n')\n    #print(y_pred)\n    print('test set accuracy score is {:.5}'.format(accuracy_score(y_test, y_pred)))\n    print('test set balanced accuracy score is {:.5}'.format(balanced_accuracy_score(y_test, y_pred)))\n    print('test set roc_auc_score is {:.5}'.format(roc_auc_score(y_test, y_pred_proba)))\n    print('test set log_loss is {:.5}'.format(log_loss(y_test, y_pred_proba)))\n    print('test set precision_score is {:.5}'.format(precision_score(y_test, y_pred)))\n    print('test set recall_score is {:.5}'.format(recall_score(y_test, y_pred)))\n    print('test set f1_score is {:.5}'.format(f1_score(y_test, y_pred)))\n    auc_curve(y_test, y_pred_proba)\n    plt.show()\n    return y_pred\n \n_ = run_lgb(x_aug, y_aug, x_aug_test, y_test, 5, [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_aug, y_aug = data_augment(x_train, y_train, 40, x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['gender', 'car', 'reality', 'income_type', 'edu_type', 'famliy_type', 'house_type', 'work_phone', 'phone', 'email','occupation']\n    \ndef OrdinalEncoder_transfrom(data, col):\n    '''try encoding'''\n    enc = ce.OrdinalEncoder(cols = col).fit(data)\n    data2 = enc.transform(data)\n    return data2\n\nx_aug = OrdinalEncoder_transfrom(x_aug, cat_cols)\nx_test = OrdinalEncoder_transfrom(x_test, cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = run_lgb(x_aug, y_aug, x_test, y_test, 5, cat_cols)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}