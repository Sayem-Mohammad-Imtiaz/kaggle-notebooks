{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=5>SMS Text classification</font>"},{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1\">Data Import</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-some-functions\" data-toc-modified-id=\"Define-some-functions-1.1\">Define some functions</a></span></li></ul></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2\">Preprocessing</a></span></li><li><span><a href=\"#Feature-Extraction\" data-toc-modified-id=\"Feature-Extraction-3\">Feature Extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-Count\" data-toc-modified-id=\"Word-Count-3.1\">Word Count</a></span></li><li><span><a href=\"#Tf-Idf\" data-toc-modified-id=\"Tf-Idf-3.2\">Tf-Idf</a></span></li><li><span><a href=\"#N-gram\" data-toc-modified-id=\"N-gram-3.3\">N-gram</a></span></li></ul></li><li><span><a href=\"#Text-Classification\" data-toc-modified-id=\"Text-Classification-4\">Text Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-4.1\"><a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\" target=\"_blank\">Naive Bayes</a></a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-4.2\">SVM</a></span></li><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-4.3\">LogisticRegression</a></span></li><li><span><a href=\"#GBDT\" data-toc-modified-id=\"GBDT-4.4\">GBDT</a></span></li></ul></li><li><span><a href=\"#word2vec\" data-toc-modified-id=\"word2vec-5\">word2vec</a></span></li><li><span><a href=\"#Bert-TensorFlow\" data-toc-modified-id=\"Bert-TensorFlow-6\">Bert-TensorFlow</a></span></li></ul></div>"},{"metadata":{"trusted":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport itertools\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models import Word2Vec\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nsns.set_style('white') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Import"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.read_csv('../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv')\n#stopword_list = [k.strip() for k in open(\"E:/MaLearning/souhu/stopwords.txt\", encoding='utf8').readlines() if k.strip() != '']\nstopword_list = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define some functions"},{"metadata":{"trusted":false},"cell_type":"code","source":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data[\"Category\"] = data[\"Category\"].map({'ham': 0,'spam':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Feature Extraction\n\nThere is several ways to extract features from text data, including word count method and tf-idf encoding. Now I will do both of them and compare their effect of predicting."},{"metadata":{},"cell_type":"markdown","source":"## Word Count"},{"metadata":{"trusted":false},"cell_type":"code","source":"description_list = []\nfor article in data[\"Message\"]:\n    article = re.sub(\"[^a-zA-Z]\",\" \",article)\n    article = article.lower()   # low case letter\n    article = word_tokenize(article)\n    lemma = WordNetLemmatizer()\n    article = [ lemma.lemmatize(word) for word in article]\n    article = \" \".join(article)\n    description_list.append(article) #we hide all word one section\n    \n    \ndef text_replace(text):\n    '''some text cleaning method'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://i.loli.net/2019/11/18/kdH1gfSlezstUwL.png\">\n    <br>\n    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n    display: inline-block;\n    color: #999;\n    padding: 2px;\">Word Count Vectorizer</div>\n</center>"},{"metadata":{"trusted":false},"cell_type":"code","source":"count_vectorizer = CountVectorizer(max_features = 100, stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\ntokens = count_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(type(sparce_matrix))\nsparce_matrix = pd.DataFrame(sparce_matrix, columns=tokens)\nsparce_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tf-Idf\n\n Term Frequency-Inverse Document Frequency"},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features = 100)\ntfidfmatrix = vectorizer.fit_transform(description_list)\ncname = vectorizer.get_feature_names()\ntfidfmatrix = pd.DataFrame(tfidfmatrix.toarray(),columns=cname)\ntfidfmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tfidfmatrix.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## N-gram "},{"metadata":{"trusted":false},"cell_type":"code","source":"count_vectorizer = CountVectorizer(max_features = 100, stop_words = \"english\",ngram_range=(2, 2),)\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\ntokens = count_vectorizer.get_feature_names()\ngram2 = pd.DataFrame(sparce_matrix, columns=tokens)\ngram2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Classification"},{"metadata":{},"cell_type":"markdown","source":"## [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering)\n\nNaive Bayes gives us a baseline accuracy of predicting."},{"metadata":{"trusted":false},"cell_type":"code","source":"\ny = data.iloc[:,0].values   \nx = sparce_matrix\ntfidfx = tfidfmatrix\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 2019)\ntf_x_train, tf_x_test, tf_y_train, tf_y_test = train_test_split(tfidfmatrix ,y,\n                                                                test_size = 0.3,\n                                                                random_state = 2019)\n\ngm_x_train, gm_x_test, gm_y_train, gm_y_test = train_test_split(gram2 ,y,\n                                                                test_size = 0.3,\n                                                                random_state = 2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',nb.score(x_test,y_test))\nnb.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',nb.score(tf_x_test,tf_y_test))\nnb.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',nb.score(gm_x_test,gm_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',nb.score(x_test,y_test))\nnb.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',nb.score(tf_x_test,tf_y_test))\nnb.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',nb.score(gm_x_test,gm_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nb = BernoulliNB()\nnb.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',nb.score(x_test,y_test))\nnb.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',nb.score(tf_x_test,tf_y_test))\nnb.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',nb.score(gm_x_test,gm_y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nsvmmodel = svm.SVC(kernel='linear', C = 1)\nsvmmodel.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',svmmodel.score(x_test,y_test))\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',svmmodel.score(tf_x_test,tf_y_test))\nsvmmodel.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',svmmodel.score(gm_x_test,gm_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svmmodel = svm.SVC(kernel='linear', C = 1)\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',svmmodel.score(tf_x_test,tf_y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LogisticRegression"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nlogit = LogisticRegression(random_state=0, solver='lbfgs')\nlogit.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',logit.score(x_test,y_test))\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',logit.score(tf_x_test,tf_y_test))\nsvmmodel.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',logit.score(gm_x_test,gm_y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GBDT"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nclf = GradientBoostingClassifier(n_estimators=50)\nclf.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',clf.score(x_test,y_test))\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',clf.score(tf_x_test,tf_y_test))\nsvmmodel.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',clf.score(gm_x_test,gm_y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# word2vec"},{"metadata":{"trusted":false},"cell_type":"code","source":"description_list = []\nfor article in data[\"Message\"]:\n    article = re.sub(\"[^a-zA-Z]\",\" \",article)\n    article = article.lower() \n    cutWords = [k for k in word_tokenize(article) if k not in stopword_list]\n    cutWords = [ lemma.lemmatize(word) for word in cutWords]\n    description_list.append(cutWords)\n#description_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def getVector_v2(cutWords, word2vec_model):\n    vector_list = [word2vec_model[k] for k in cutWords if k in word2vec_model]\n    vector_df = pd.DataFrame(vector_list)\n    cutWord_vector = vector_df.mean(axis=0).values\n    return cutWord_vector\n\nword2vec_model = Word2Vec(description_list, size=100, iter=10, min_count=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vector_list = []\nfor c in description_list:\n    vec = getVector_v2(c, word2vec_model)\n    vector_list.append(vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = pd.DataFrame(vector_list)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y = data[\"Category\"]\nY = pd.DataFrame(Y)\nY.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = X.fillna(X.mean())\nY = Y.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.3)\nlogistic_model = LogisticRegression()\nlogistic_model.fit(train_X, train_y)\ny_predict = logistic_model.predict(test_X)\n\nprint('CountVectorizer Accuracy Score',accuracy_score(y_test, y_predict))\npd.DataFrame(confusion_matrix(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf = GradientBoostingClassifier(n_estimators=50)\ngbdt = clf.fit(train_X, train_y)\ny_predict = gbdt.predict(test_X)\nprint('CountVectorizer Accuracy Score',accuracy_score(y_test, y_predict))\npd.DataFrame(confusion_matrix(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bert-TensorFlow"},{"metadata":{},"cell_type":"markdown","source":"See this notebook: <https://www.kaggle.com/rikdifos/bert-test>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}