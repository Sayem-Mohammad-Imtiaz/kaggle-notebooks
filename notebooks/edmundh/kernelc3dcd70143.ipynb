{"cells":[{"metadata":{},"cell_type":"markdown","source":"Added a few extra packages (cartopy, seaborn etc) to default given by notebook starting blurb below (np, pd, os). The referenced docker images say what packages are available - e.g. the [image gets cartopy via conda](https://github.com/Kaggle/docker-python/blob/master/Dockerfile#L45), and [pandas via pip](https://github.com/Kaggle/docker-python/blob/master/Dockerfile#L55)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Goal\nThis notebook is trying out a simple linear model to predict real-estate prices, based on [the dataset provided in this challenge](https://www.kaggle.com/quantbruce/real-estate-price-prediction)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Read in the data\nSet \"No\" as the index, to avoid seeing it in analysis!","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/real-estate-price-prediction/Real estate.csv\", index_col=\"No\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial exploration of data\n## Dataset basic properties\nWhat does dataset contain, and how big is it?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations in dataset\nFrom above, we see dataset has 414 entries. I'm assuming from column names that for purposes of this challenge, [XN are the explanatory variables, and Y is the explained variable](https://en.wikipedia.org/wiki/Dependent_and_independent_variables)\n* So we've got 6 potential explanatory variables, and 1 explained variable.\n\nLet's have a quick look at the correlations between the different variables.\n\nSeaborn's handy [pairplot function](https://seaborn.pydata.org/generated/seaborn.pairplot.html) helps us look at:\n- the distributions of each explanatory variable XN - any evidence of things we'd need to be careful of (non-flat distributions etc)\n- the basic single variable correlations present between XN and Y\n- any [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) between the XN we should be aware of (and hence be careful, as may cause issues!)\n\nIrritatingly, pairplot doesn't give the count # on the distributions - insights using that based on the handy visualisation of the .csv in [the original Kaggle dataset](https://www.kaggle.com/quantbruce/real-estate-price-prediction)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hpp = sns.pairplot(df, corner=True)  # corner: get rid of duplicate \"noise\" from upper triangle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlations analysis\n#### Individual variables and their distributions\nGroup's guess at properties - looks like [others have run into lack of metadata issue too](https://www.kaggle.com/quantbruce/real-estate-price-prediction/discussion/128822) - and remarks on distribution\n* X1 transaction dates [float, decimal year]:\n  * monthly data by looks of it\n  * ~1 year's worth, mid-2012 to mid-2013\n  * reasonably flat, bit biased to end of dataset\n    * usually ~30/bin, though ~doubles (60-70) in final two bins (mid-2013)\n* X2 house age [float, assume years old]:\n  * bimodal distribution dominated by broad (10$\\pm$10) cluster of recent houses\n  * smaller, thinner cluster of older properties at ~35$\\pm$7???\n* X3 distance to the nearest MRT station [float, assume meters]\n  * Steven H spotted in Taipei, so guessing distance to nearest [Taipei metro station](https://en.wikipedia.org/wiki/Taipei_Metro)\n  * ~exponential distribution\n  * dominated by 1st bin (~60% of properties are very near a station aka within 670 m???)\n  * Widely spread after, so rest (other 40%) far lower, distribution generally falling off ~monotonically\n  * Up to ~4 clusters - see X2:X3\n* Number of convenience stores [int, assume within a certain distance of property]\n  * Fairly flat, up to 10 (very convenient!)\n* Latitude [float, assume degrees]\n  * Looks like a city - unimodal distribution, asymmetric\n    * Steven H spotted in Taipei!\n* Longitude [float, assume degrees in -180:180 convention]\n  * As per above, interesting hole (see below)\n* House price of unit area [float, assume $TWD/m^2$]\n  * Think this is our intended explained variable\n    * Helpfully, looks like it's already been transformed to a nice \"normalised\" variable, removing need to look at nasty things like int-y \"number of rooms\" which would be poor proxy for this\n  * ~normal distribution, centered on ~40, up to ~75\n    * One outlier at ~120, which we may want to exclude...\n\nNB we might be missing other important explanatory variables - are there good schools nearby, is property damaged, etc. \n* But frankly who cares for now - anything like this will contribute to our residuals!\n* And some things like this we may be able to infer - do residuals show hotspots geographically, hinting at local factors not captured\n\n#### Individual explanatory variables and Y\nLook at final row for this\n* X1: Possible slight peak in 3rd quarter of dataset (economy? supply/demand?) - i.e. not simple linear\n* X2: Trough at ~25 (poor construction practices then?) - again not simple linear\n* X3: ~noisy exponential falloff (premium for convenient MRT nearby?) - again not simple linear\n* X4: noisy ~linear increase (premium for convenience of convenience stores?) - yay, finally\n* X5, X6: complex, but generally more expensive near center. Makes sense to look at in 2D\n\n#### Collinearity:\n* X1: nothing obvious\n* X2:\n  * X2:X3 - some. Slight tendency for mid-age properties (~20 years old) to have larger upper bound (furthest from MRT)\n* X3:\n  * X3:X4 - some. All places with lots of convenience stores are close to MRT\n  * X3:X5&X6 - strong, esp longitude. Distance to MRT clearly geographically dependent ([MRT map](https://en.wikipedia.org/wiki/Taipei_Metro#/media/File:Taipei_Metro_geographical_map.svg))\n* X4:\n  * X4:X5&X6 - strong. More convenience stores near centre.\n* X5:\n  * X5:X6 - strong. Sideways map! Let's have a closer look shortly.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Correlation matrix\nLook at correlation values (positive, red = correlated; negative, blue = anticorrelated) for another way of looking at:\n* correlations between each XN and Y (lowest row)\n* collinearity between XN (other entries)\n\nNote here 0 = X1, 1 = X2, ... 6 = Y\n\nIgnore diagonal (perfect correlation with itself) and upper triangle (duplicated info)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hms = plt.matshow(df.corr(), cmap=\"RdBu_r\", vmin=-1, vmax=1)\nhcb = plt.colorbar(hms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Mapping\nLet's look at that geographical distribution a little more, in a \"normal\" orientation\n\nWhen we do, we can see distribution strange. There's a big \"hole\" to SW of main cluster. From lat/lon values & comparing with Google maps, I think cluster is in [Xindian district](https://en.wikipedia.org/wiki/Xindian_District), and that we're looking at roughly [this bounding box](https://www.google.com/maps/place/Xindian+District,+New+Taipei+City,+Taiwan/@24.9665234,121.4967858,8124m/data=!3m1!1e3!4m5!3m4!1s0x346803de337f4fe1:0xe29baf27fbf0968f!8m2!3d24.978282!4d121.5394822), so hole possibly due to terrain (Maria A's point)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hjp = sns.jointplot(x=\"X6 longitude\", y=\"X5 latitude\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Proper mapping-based exploration of data\nLet's have a look at our various variables in a lat-lon sense - i.e. on a \"proper\" map, to see what insights we can glean","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"varibs = [\n    \"Y house price of unit area\",\n    \"X3 distance to the nearest MRT station\",\n    \"X4 number of convenience stores\",\n    \"X2 house age\",\n    \"X1 transaction date\",\n]\n\nfig, axes = plt.subplots(ncols=2, nrows=3, sharex=True, sharey=True, figsize=[10,8],\n                         subplot_kw={'projection': ccrs.PlateCarree()})\n\n# Set background color for fig and subplots to sth to help see data\n# over full range without changing away from default viridis cmap\nback_col = \"darkgrey\"\nfig.patch.set_facecolor(back_col)\n\n# Name lats and lons for convenience\nlons = df[\"X6 longitude\"]\nlats = df[\"X5 latitude\"]\n\n# Get the (tight) bounds\nx0, x1 = lons.min(), lons.max()\ny0, y1 = lats.min(), lats.max()\nm = 0.01  # Add a margin - set_xmargin / plt.margins seem to be ignored :(\n\nvaxes = axes.flatten()[:-1]  # 5 variables, but 6 axes - make our zip simpler by lopping off!\naxes[-1,-1].set_visible(False)  # Hide unused final axis https://stackoverflow.com/a/10035974\nfor varib, vax in zip(varibs, vaxes):\n    plt.sca(vax)\n    vax.background_patch.set_facecolor(back_col)  # https://github.com/SciTools/cartopy/issues/880\n    vax.set_extent([x0-m, x1+m, y0-m, y1+m], ccrs.PlateCarree())\n    \n    hs = plt.scatter(lons, lats, c=df[varib], transform=ccrs.PlateCarree(), s=1)\n    plt.colorbar(hs)\n    \n    plt.title(varib)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Analysis of map\nInteresting! \n* Y: house prices generally max out in Xindian\n* X3: fairly radial increase in \"distance to nearest MRT\" away from Xindian centre\n  * Makes ~sense - looks like [Xindian MRT stop is a terminus](https://upload.wikimedia.org/wikipedia/commons/0/0a/Taipei_Metro_geographical_map.svg) - so would get a radial dependence away from that final node\n* X4: number of convenience stores *within* Xindian does ~mimic a banana-like feature in there\n  * Unclear whether this is causative (house prices depend on # of convenience stores) or correlative (you get more convenience stores where people have disposable income; latter also correlates with house prices)\n  * Distinction probably doesn't matter for modelling purposes - either way, given feature seems to show up, prob worth including this as explanatory variable!\n* X2: oldest houses (~40 y/o) are in Xindian. Then medium age (~30-10 y/o) everywhere. Newest <10 y/o generally near Xindian\n* X1: nothing obvious in this view\n\nDoes look like there's quite a bit of duplicated info here (X3:X6), so worth being parsimonious about what we throw at the regression. \n\nReally need a verification chain to work out what is worth including. And thinking carefully about how to correct for \"dumb\"/meaningless skill inflation obtained by simply adding another variable - aka overfitting.\n\nProb not worth transforming any of the variables - only contender is poss X3, but that would be just throwing data away?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Will stop here for now - need to get hands dirty with actual modelling next!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}