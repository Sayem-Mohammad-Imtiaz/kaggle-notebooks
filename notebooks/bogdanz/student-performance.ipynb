{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook gives an analysis of Student Performance in exams dataset. The structure is as follows:\n1) EDA\n2) Outliers detection\n3) Model fitting, validation, test and comparison - PCA and one-hot encoding\n4) Model fitting, validation, test and comparison - categorical to ordinal"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/students-performance-in-exams/StudentsPerformance.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****1) EDA****"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in data.columns:\n    print(col)\n    print(data[col].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plan to predict the scores for a subset of data (test set), while training and validating on the rest of the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata, test = train_test_split(data, test_size = 0.1, random_state =3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of students by each category"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain_col = []\nfor col in data.columns:\n    if type(data[col][0]) == str:\n        train_col.append(col)\n\nfor i in range(len(train_col)):\n    plt.figure()\n    data[train_col[i]].value_counts().plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scores distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ncol_num = ['math score', 'reading score', 'writing score']\ncols = ['blue', 'green', 'orange']\n\nfor i in range(3):\n    plt.figure()\n    sns.distplot(data[col_num[i]], color = cols[i])\n    x0, x1 = plt.xlim()\n    y0, y1 = plt.ylim()\n    plt.text(x=x0 + 2, y=y1 - 0.002, s=\"Skewness: \" + str(data[col_num[i]].skew()), color = 'xkcd:poo brown')\n    plt.text(x=x0 + 2, y=y1 - 0.004, s=\"Kurtosis: \" + str(data[col_num[i]].kurt()), color = 'xkcd:dried blood')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 10))\ncorr = sns.heatmap(data.corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, writing score correlates more with reading score than maths score. Now, we need to one-hot encode categorical variables and further explore correlations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot = pd.get_dummies(data[train_col])\ndata = data.join(one_hot)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_test = pd.get_dummies(test[train_col])\ntest = test.join(one_hot_test)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ncorr2 = sns.heatmap(data.corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further oxploration of proportions of students by combination of categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 8))\n\nplt.subplot(131)\nax1 = plt.subplot(1,3,1)\npd.crosstab(data['gender'], data['race/ethnicity']).plot(kind='bar', ax=ax1)\nax2 = plt.subplot(1,3,2)\npd.crosstab(data['lunch'], data['race/ethnicity']).plot(kind='bar', ax=ax2)\nax3 = plt.subplot(1,3,3)\npd.crosstab(data['test preparation course'], data['race/ethnicity']).plot(kind='bar', ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar plots for parental level of education."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 8))\n\nplt.subplot(131)\nax1 = plt.subplot(1,3,1)\npd.crosstab(data['gender'], data['parental level of education']).plot(kind='bar', ax=ax1)\nax2 = plt.subplot(1,3,2)\npd.crosstab(data['lunch'], data['parental level of education']).plot(kind='bar', ax=ax2)\nax3 = plt.subplot(1,3,3)\npd.crosstab(data['test preparation course'], data['parental level of education']).plot(kind='bar', ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that the distribution of race/ethnicity and parental education acress male/female, lunch, test prep is quite even. Now, we look at charts for race vs education:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15, 9)\npd.crosstab(data['race/ethnicity'], data['parental level of education']).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we look at the numbers and proportions of genders, lunch and test prep in different racial groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 8))\n\nplt.subplot(131)\nax1 = plt.subplot(1,3,1)\npd.crosstab(data['race/ethnicity'], data['gender']).plot(kind='bar', ax=ax1)\nax2 = plt.subplot(1,3,2)\npd.crosstab(data['race/ethnicity'], data['lunch']).plot(kind='bar', ax=ax2)\nax3 = plt.subplot(1,3,3)\npd.crosstab(data['race/ethnicity'], data['test preparation course']).plot(kind='bar', ax=ax3)\n\nplt.figure(figsize = (18, 8))\n\nplt.subplot(131)\nax1 = plt.subplot(1,3,1)\nx = pd.crosstab(data['race/ethnicity'], data['gender'])\nx.div(x.sum(1).astype(float), axis = 0).plot(kind='bar', ax=ax1)\nax2 = plt.subplot(1,3,2)\nx = pd.crosstab(data['race/ethnicity'], data['lunch'])\nx.div(x.sum(1).astype(float), axis = 0).plot(kind='bar', ax=ax2)\nax3 = plt.subplot(1,3,3)\nx = pd.crosstab(data['race/ethnicity'], data['test preparation course'])\nx.div(x.sum(1).astype(float), axis = 0).plot(kind='bar', ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar for parental level of education:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 8))\n\nplt.subplot(131)\nax1 = plt.subplot(1,3,1)\npd.crosstab(data['parental level of education'], data['gender']).plot(kind='bar', ax=ax1)\nax2 = plt.subplot(1,3,2)\npd.crosstab(data['parental level of education'], data['lunch']).plot(kind='bar', ax=ax2)\nax3 = plt.subplot(1,3,3)\npd.crosstab(data['parental level of education'], data['test preparation course']).plot(kind='bar', ax=ax3)\n\nplt.figure(figsize = (18, 8))\n\nplt.subplot(131)\nax1 = plt.subplot(1,3,1)\nx = pd.crosstab(data['parental level of education'], data['gender'])\nx.div(x.sum(1).astype(float), axis = 0).plot(kind='bar', ax=ax1)\nax2 = plt.subplot(1,3,2)\nx = pd.crosstab(data['parental level of education'], data['lunch'])\nx.div(x.sum(1).astype(float), axis = 0).plot(kind='bar', ax=ax2)\nax3 = plt.subplot(1,3,3)\nx = pd.crosstab(data['parental level of education'], data['test preparation course'])\nx.div(x.sum(1).astype(float), axis = 0).plot(kind='bar', ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some descriptive statistics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = data[['math score','parental level of education']].iloc[:len(data)].groupby('parental level of education')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['math score', 'gender']].iloc[:len(data)].groupby('gender')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['math score', 'lunch']].iloc[:len(data)].groupby('lunch')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['math score', 'test preparation course']].iloc[:len(data)].groupby('test preparation course')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['math score', 'race/ethnicity']].iloc[:len(data)].groupby('race/ethnicity')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = data[['reading score','parental level of education']].iloc[:len(data)].groupby('parental level of education')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['reading score', 'gender']].iloc[:len(data)].groupby('gender')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['reading score', 'lunch']].iloc[:len(data)].groupby('lunch')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['reading score', 'test preparation course']].iloc[:len(data)].groupby('test preparation course')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['reading score', 'race/ethnicity']].iloc[:len(data)].groupby('race/ethnicity')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = data[['writing score','parental level of education']].iloc[:len(data)].groupby('parental level of education')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['writing score', 'gender']].iloc[:len(data)].groupby('gender')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['writing score', 'lunch']].iloc[:len(data)].groupby('lunch')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['writing score', 'test preparation course']].iloc[:len(data)].groupby('test preparation course')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))\ngrouped = data[['writing score', 'race/ethnicity']].iloc[:len(data)].groupby('race/ethnicity')\nprint(grouped.agg(['min', 'max', 'mean', 'median', 'std']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are clearly some trends here, e.g. exam scores increase with parental education level and across ethnic groups. Moreover, ethnic groups/parental level of education groups that have higher scores also generally have better level of test preparation/lunch, as seen in diagrams above. This means that we can treat parental education and ethnicity as ordinal variables as well. We first, however, treat it as categorical variables and only after that, as ordinal variables in order to compare the performance. "},{"metadata":{},"cell_type":"markdown","source":"**2) Outlier detection**"},{"metadata":{},"cell_type":"markdown","source":"There are many outliers, e.g. in many cases there are people scoring 100 even from the background with lower mean/median. We can plot a few distributions of scores withing some groups to see how many outliers are there etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 15))\nplt.subplot(325)\n\neth = data['race/ethnicity'].unique()\n\nfor k in range(1, len(eth)+1):\n    ax = plt.subplot(3,2,k)\n    data_tmp = data[data['race/ethnicity'] == eth[k-1]]\n    sns.distplot(data_tmp['math score'])\n    x0, x1 = plt.xlim()\n    y0, y1 = plt.ylim()\n    plt.text(x=x0 + 2, y=y1 - 0.002, s=\"Skewness: \" + str(data_tmp['math score'].skew()), color = 'xkcd:poo brown')\n    plt.text(x=x0 + 2, y=y1 - 0.004, s=\"Kurtosis: \" + str(data_tmp['math score'].kurt()), color = 'xkcd:dried blood')\n    plt.title(eth[k-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 15))\nplt.subplot(325)\n\neth = data['parental level of education'].unique()\n\nfor k in range(1, len(eth)+1):\n    ax = plt.subplot(3,2,k)\n    data_tmp = data[data['parental level of education'] == eth[k-1]]\n    sns.distplot(data_tmp['math score'])\n    x0, x1 = plt.xlim()\n    y0, y1 = plt.ylim()\n    plt.text(x=x0 + 2, y=y1 - 0.002, s=\"Skewness: \" + str(data_tmp['math score'].skew()), color = 'xkcd:poo brown')\n    plt.text(x=x0 + 2, y=y1 - 0.004, s=\"Kurtosis: \" + str(data_tmp['math score'].kurt()), color = 'xkcd:dried blood')\n    plt.title(eth[k-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at similar plots for reading and writing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 15))\nplt.subplot(325)\n\neth = data['race/ethnicity'].unique()\n\nfor k in range(1, len(eth)+1):\n    ax = plt.subplot(3,2,k)\n    data_tmp = data[data['race/ethnicity'] == eth[k-1]]\n    sns.distplot(data_tmp['reading score'])\n    x0, x1 = plt.xlim()\n    y0, y1 = plt.ylim()\n    plt.text(x=x0 + 2, y=y1 - 0.002, s=\"Skewness: \" + str(data_tmp['reading score'].skew()), color = 'xkcd:poo brown')\n    plt.text(x=x0 + 2, y=y1 - 0.004, s=\"Kurtosis: \" + str(data_tmp['reading score'].kurt()), color = 'xkcd:dried blood')\n    plt.title(eth[k-1])\n    \nplt.figure(figsize = (18, 15))\nplt.subplot(325)\n\neth = data['parental level of education'].unique()\n\nfor k in range(1, len(eth)+1):\n    ax = plt.subplot(3,2,k)\n    data_tmp = data[data['parental level of education'] == eth[k-1]]\n    sns.distplot(data_tmp['reading score'])\n    x0, x1 = plt.xlim()\n    y0, y1 = plt.ylim()\n    plt.text(x=x0 + 2, y=y1 - 0.002, s=\"Skewness: \" + str(data_tmp['reading score'].skew()), color = 'xkcd:poo brown')\n    plt.text(x=x0 + 2, y=y1 - 0.004, s=\"Kurtosis: \" + str(data_tmp['reading score'].kurt()), color = 'xkcd:dried blood')\n    plt.title(eth[k-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18, 15))\nplt.subplot(325)\n\neth = data['race/ethnicity'].unique()\n\nfor k in range(1, len(eth)+1):\n    ax = plt.subplot(3,2,k)\n    data_tmp = data[data['race/ethnicity'] == eth[k-1]]\n    sns.distplot(data_tmp['writing score'])\n    x0, x1 = plt.xlim()\n    y0, y1 = plt.ylim()\n    plt.text(x=x0 + 2, y=y1 - 0.002, s=\"Skewness: \" + str(data_tmp['writing score'].skew()), color = 'xkcd:poo brown')\n    plt.text(x=x0 + 2, y=y1 - 0.004, s=\"Kurtosis: \" + str(data_tmp['writing score'].kurt()), color = 'xkcd:dried blood')\n    plt.title(eth[k-1])\n    \nplt.figure(figsize = (18, 15))\nplt.subplot(325)\n\neth = data['parental level of education'].unique()\n\nfor k in range(1, len(eth)+1):\n    ax = plt.subplot(3,2,k)\n    data_tmp = data[data['parental level of education'] == eth[k-1]]\n    sns.distplot(data_tmp['writing score'])\n    x0, x1 = plt.xlim()\n    y0, y1 = plt.ylim()\n    plt.text(x=x0 + 2, y=y1 - 0.002, s=\"Skewness: \" + str(data_tmp['writing score'].skew()), color = 'xkcd:poo brown')\n    plt.text(x=x0 + 2, y=y1 - 0.004, s=\"Kurtosis: \" + str(data_tmp['writing score'].kurt()), color = 'xkcd:dried blood')\n    plt.title(eth[k-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that some outliers are present in training data, since some scores are clearly deviating from the group as a whole. Those might be different for different scores. Now, we define criteria to remove certain datapoints from training/validation - we are going to remove math scores < 20, reading scores for Group E < 40, writing score for Group E <30, "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_for_math(X):\n    return (X[X['math score'] > 20]['math score'], X[X['math score'] > 20].drop(['math score', 'reading score', 'writing score'], axis = 1))\n\ndef data_for_reading(X):\n    return (X[(X['race/ethnicity_group E'] == 0) | ((X['race/ethnicity_group E'] == 1) & (X['reading score'] > 40))]['reading score'], X[(X['race/ethnicity_group E'] == 0) | ((X['race/ethnicity_group E'] == 1) & (X['reading score'] > 40))].drop(['math score', 'reading score', 'writing score'], axis = 1))\n\ndef data_for_writing(X):\n    return (X[(X['race/ethnicity_group E'] == 0) | ((X['race/ethnicity_group E'] == 1) & (X['writing score'] > 30))]['writing score'], X[(X['race/ethnicity_group E'] == 0) | ((X['race/ethnicity_group E'] == 1) & (X['writing score'] > 30))].drop(['math score', 'reading score', 'writing score'], axis = 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In fact, the functions above allow defining any rules for removing outliers. "},{"metadata":{},"cell_type":"markdown","source":"**3) Model fitting, validation, test and comparison - PCA and one-hot encoding**"},{"metadata":{},"cell_type":"markdown","source":"We now fit the simple linear regression on all components. Note that we drop some of the one-hot encoded variables since they have correlation of -1 with the others (e.g. gender_male with gender_female) and hence contain no new information."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'gender_female', 'lunch_standard', 'test preparation course_completed'], axis = 1)\n\ncol_num = ['math score', 'reading score', 'writing score']\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression(normalize = True)\n#print(cross_val_score(lr, X, data[col_num[i]], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(lr, data_for_math(X)[1], data_for_math(X)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(lr, data_for_reading(X)[1], data_for_reading(X)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(lr, data_for_writing(X)[1], data_for_writing(X)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\n#-179.42219401041666\n#-170.67588758680554\n#-159.81538024902343 - scores if we do not remove variables with correlation -1 - as we can see, they cause larger errors.\n\n#-178.22145182291666\n#-169.28279527452256\n#-158.72642145368786 - scores for lr if we do not remove outliers\n\n#-171.16810390479515\n#-167.92170391667636\n#-157.68080284227577 - scores if we remove outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing test data\nX_test = test.drop(['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score', 'gender_female', 'lunch_standard', 'test preparation course_completed'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now try finding which components are more important for linear regression, by looking at correlations (absolute values)"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_num = ['math score', 'reading score', 'writing score']\ncols = ['blue', 'green', 'orange']\ncol_train = []\nfor col in X.columns:\n    if type(X[col][0]) != str and (col not in col_num):\n        col_train.append(col)\n        \n\nfor i in range(3):\n    d = {}\n    for col in col_train:\n        d[col] = abs(data[col_num[i]].corr(X[col]))\n        \n    plt.figure()\n    d_s = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n    plt.bar(list(d_s.keys()), list(d_s.values()), color = cols[i])\n    plt.xticks(rotation=90)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is obvious that lunch, gender and test preparation have the highest correlation with scores, which makes common sense as well (e.g. those who did not take test prep course are more likely to fail). However, there are other features as well. One might just find a few linear combinations of them using PCA (i.e. decorrelate the correlated features in parental education/ethnicity) and fit the obtained model. However, it is not going to work when we do the cross-validation."},{"metadata":{},"cell_type":"markdown","source":"The reason is that we take the unfair advantage of the whole dataset, on which the correlations are likely more accurate than only on the training set in cross-validation. That would lead to an underestimated error - not the right way of model selection. What one should do is to create a separate PCA for each of the K-folds in cross-validation. The function below gives the cross-validation error estimate when a model is fit on principal components."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\n\ndef error(n_pca, score, model):\n    kf = KFold(n_splits = 5) # 5-fold cross-validation\n    #y = data['math score'] #first just do for math score\n    \n    if score == 'math score':\n        (y_tmp, X_tmp) = data_for_math(X)\n    elif score == 'reading score':\n        (y_tmp, X_tmp) = data_for_reading(X)\n    else:\n        (y_tmp, X_tmp) = data_for_writing(X)\n\n    error = 0\n\n    for train_index, test_index in kf.split(X_tmp):\n        X_train, X_test = X_tmp.iloc[list(train_index), :], X_tmp.iloc[list(test_index), :]\n        y_train, y_test = y_tmp.iloc[list(train_index)], y_tmp.iloc[list(test_index)]\n        pca = PCA(n_components = n_pca, whiten = True)\n        train_red = pca.fit_transform(X_train)\n        test_red = pca.transform(X_test)\n    \n        model.fit(train_red, y_train)\n        y_pred = model.predict(test_red)\n        err = metrics.mean_squared_error(y_pred, y_test)\n        error += err\n    \n\n    return error/5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at how errors change with number of principal components:"},{"metadata":{"trusted":true},"cell_type":"code","source":"errs = [error(i, 'math score', LinearRegression(normalize = True)) for i in range(1, 15)]\nplt.plot(range(1, 15), errs)\nprint(min(errs), errs.index(min(errs)))\nerrs = [error(i, 'reading score', LinearRegression(normalize = True)) for i in range(1, 15)]\nplt.plot(range(1, 15), errs)\nprint(min(errs), errs.index(min(errs)))\nerrs = [error(i, 'writing score', LinearRegression(normalize = True)) for i in range(1, 15)]\nplt.plot(range(1, 15), errs)\nprint(min(errs), errs.index(min(errs)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, PCA can give a slightly better validation error than the benchmark, with best result for 12 principal components in general. At the end, we will compare the test scores of our models. But before that, we should also try tree-based models and support vector machines.  "},{"metadata":{},"cell_type":"markdown","source":"We try random forest, using both PCA and initial dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state = 3)\nprint(cross_val_score(rf, data_for_math(X)[1], data_for_math(X)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(rf, data_for_reading(X)[1], data_for_reading(X)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(rf, data_for_writing(X)[1], data_for_writing(X)[0], cv=5, scoring = 'neg_mean_squared_error').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scores are clearly bad - we therefore would use parameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\npipeline_rf = Pipeline(\n                    [ \n                     ('rf', RandomForestRegressor(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['rf__min_samples_split'] = [38, 40, 42] \nparameters['rf__max_depth'] = [2, 4, 6, 8, None] \nparameters['rf__n_estimators'] = [10, 25, 50, 100] \n\nCV = GridSearchCV(pipeline_rf, parameters, scoring = 'neg_mean_squared_error', n_jobs= 4, cv = 5)\nCV.fit(data_for_math(X)[1], data_for_math(X)[0])   \n\nprint('Best score and parameter combination = ')\n\nprint(CV.best_score_)    \nprint(CV.best_params_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is much better, but still worse than linear regression. For the other scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"CV = GridSearchCV(pipeline_rf, parameters, scoring = 'neg_mean_squared_error', n_jobs= 4, cv = 5)\nCV.fit(data_for_reading(X)[1], data_for_reading(X)[0])   \n\nprint('Best score and parameter combination = ')\n\nprint(CV.best_score_)    \nprint(CV.best_params_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV = GridSearchCV(pipeline_rf, parameters, scoring = 'neg_mean_squared_error', n_jobs= 4, cv = 5)\nCV.fit(data_for_writing(X)[1], data_for_writing(X)[0])   \n\nprint('Best score and parameter combination = ')\n\nprint(CV.best_score_)    \nprint(CV.best_params_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation error on random forest is clearly higher. What we can do, is to try and fit it on PCA components. "},{"metadata":{},"cell_type":"markdown","source":"Finding optimal parameter for fitting random forest onto PCA components."},{"metadata":{"trusted":true},"cell_type":"code","source":"err_min = 10000\nparams = [0,0,0,0]\ndepth = [4,5,6]\nsamples = [20, 30, 40]\nn_est = [40, 50, 60]\nn_pca = [i for i in range(6, 12)]\n\n\n# Finding the optimal parameters\n#for d in depth:\n#    for s in samples:\n#        for n in n_est:\n#            for n_p in n_pca:\n#                model = RandomForestRegressor(max_depth = d, min_samples_split = s, n_estimators = n)\n#                if error(n_p, 'math score', model) < err_min:\n#                    err_min = error(n_p, 'math score', model)\n#                    params = [n_p, s, d, n]\n#                    \n#    print(\"tried depth \" + str(d))\n                    \n#print(\"Min error: \" + str(err_min))\n#print(\"parameters: \" + str(params))\n\n#Min error: 185.30627310373836\n#parameters: [11, 40, 4, 60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, PCA does not work that well with random forest - no point to try with predicting other scores. That might be explained by the fact that having 0 and 1 (as in the original dataset), makes it much easier (and more stable) for RF to decide on the values of the output. We would expect tree-based models to perform worse in general than linear regression. One last thing that we could try is Support Vector Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\n\npipeline_svr = Pipeline(\n                    [ \n                     ('svr', SVR())\n                     \n])\n\nparameters = {}\nparameters['svr__kernel'] = ['rbf', 'poly', 'sigmoid', 'linear'] \nparameters['svr__C'] = [0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n\nCV = GridSearchCV(pipeline_svr, parameters, scoring = 'neg_mean_squared_error', n_jobs= 4, cv = 5)\nCV.fit(data_for_math(X)[1], data_for_math(X)[0])   \n\nprint('Best score and parameter combination = ')\n\nprint(CV.best_score_)    \nprint(CV.best_params_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the SV Regressor with linear kernel does a better job than random forest. Predicting other scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"CV = GridSearchCV(pipeline_svr, parameters, scoring = 'neg_mean_squared_error', n_jobs= 4, cv = 5)\nCV.fit(data_for_reading(X)[1], data_for_reading(X)[0])   \n\nprint('Best score and parameter combination = ')\n\nprint(CV.best_score_)    \nprint(CV.best_params_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV = GridSearchCV(pipeline_svr, parameters, scoring = 'neg_mean_squared_error', n_jobs= 4, cv = 5)\nCV.fit(data_for_writing(X)[1], data_for_writing(X)[0])   \n\nprint('Best score and parameter combination = ')\n\nprint(CV.best_score_)    \nprint(CV.best_params_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, in terms of performance, SVR > RF. We can, of course, try to fit SVR on PCA components as well. We would expect better performance than RF on principal components."},{"metadata":{"trusted":true},"cell_type":"code","source":"err_min = 10000\nparams = [0,0]\nkernels = ['rbf', 'poly', 'sigmoid', 'linear'] \nCs = [0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\nn_pca = [i for i in range(6, 15)]\n\nfor k in kernels:\n    for c in Cs:\n        for n in n_pca:\n            model = SVR(kernel = k, C = c)\n            if error(n, 'math score', model) < err_min:\n                err_min = error(n, 'math score', model)\n                params = [k, c, n]\n                \n    print(\"tried \" + str(k))\n                    \nprint(\"Min error: \" + str(err_min))\nprint(\"parameters: \" + str(params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting! That gives us an even better result than linear regression. However, we can notice that the regularization parameter is relatively large, meaning that the regularization is not large. That would lead us to thought that on math scores, SVR might overfit. Trying for other scores to see if our hypothesis is correct:"},{"metadata":{"trusted":true},"cell_type":"code","source":"err_min = 10000\nparams = [0,0]\n\nfor k in kernels:\n    for c in Cs:\n        for n in n_pca:\n            model = SVR(kernel = k, C = c)\n            if error(n, 'reading score', model) < err_min:\n                err_min = error(n, 'reading score', model)\n                params = [k, c, n]\n                \n    print(\"tried \" + str(k))\n                    \nprint(\"Min error: \" + str(err_min))\nprint(\"parameters: \" + str(params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"err_min = 10000\nparams = [0,0]\n\nfor k in kernels:\n    for c in Cs:\n        for n in n_pca:\n            model = SVR(kernel = k, C = c)\n            if error(n, 'writing score', model) < err_min:\n                err_min = error(n, 'writing score', model)\n                params = [k, c, n]\n                \n    print(\"tried \" + str(k))\n                    \nprint(\"Min error: \" + str(err_min))\nprint(\"parameters: \" + str(params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we look at test scores predictions for different models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear regression\")\n\nlr = LinearRegression(normalize = True)\n\nlr.fit(data_for_math(X)[1], data_for_math(X)[0])\ny_pred = lr.predict(X_test)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['math score'])))\nplt.plot(test['math score'], y_pred, 'o')\nplt.xlabel(\"Actual score\")\nplt.ylabel(\"Predicted score\")\nplt.ylim([0, 100])\n\nlr.fit(data_for_reading(X)[1], data_for_reading(X)[0])\ny_pred = lr.predict(X_test)\nprint(\"MSE for reading score \" + str(metrics.mean_squared_error(y_pred, test['reading score'])))\nplt.plot(test['reading score'], y_pred, 'o')\n\nlr.fit(data_for_writing(X)[1], data_for_writing(X)[0])\ny_pred = lr.predict(X_test)\nprint(\"MSE for writing score \" + str(metrics.mean_squared_error(y_pred, test['writing score'])))\nplt.plot(test['writing score'], y_pred, 'o')\n\nplt.plot(test['math score'], test['math score'], 'r')\n\nplt.legend(('math', 'reading', 'writing'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear regression on principal components\")\n\nlr = LinearRegression(normalize = True)\npca = PCA(n_components = 12, whiten = True)\n\ny_tmp, X_tmp = data_for_math(X)\nX_tr = pca.fit_transform(X_tmp)\ntest_set = pca.transform(X_test)\nlr.fit(X_tr, y_tmp)\ny_pred = lr.predict(test_set)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['math score'])))\nplt.plot(test['math score'], y_pred, 'o')\nplt.xlabel(\"Actual score\")\nplt.ylabel(\"Predicted score\")\nplt.ylim([0, 100])\n\ny_tmp, X_tmp = data_for_reading(X)\nX_tr = pca.fit_transform(X_tmp)\ntest_set = pca.transform(X_test)\nlr.fit(X_tr, y_tmp)\ny_pred = lr.predict(test_set)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['reading score'])))\nplt.plot(test['math score'], y_pred, 'o')\n\ny_tmp, X_tmp = data_for_writing(X)\nX_tr = pca.fit_transform(X_tmp)\ntest_set = pca.transform(X_test)\nlr.fit(X_tr, y_tmp)\ny_pred = lr.predict(test_set)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['writing score'])))\nplt.plot(test['math score'], y_pred, 'o')\n\nplt.plot(test['math score'], test['math score'], 'r')\n\nplt.legend(('math', 'reading', 'writing'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVR\")\n\nsvr = SVR(kernel = 'linear', C = 0.5)\n\nsvr.fit(data_for_math(X)[1], data_for_math(X)[0])\ny_pred = svr.predict(X_test)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['math score'])))\nplt.plot(test['math score'], y_pred, 'o')\nplt.xlabel(\"Actual score\")\nplt.ylabel(\"Predicted score\")\nplt.ylim([0, 100])\n\nsvr.fit(data_for_reading(X)[1], data_for_reading(X)[0])\ny_pred = svr.predict(X_test)\nprint(\"MSE for reading score \" + str(metrics.mean_squared_error(y_pred, test['reading score'])))\nplt.plot(test['reading score'], y_pred, 'o')\n\nsvr.fit(data_for_writing(X)[1], data_for_writing(X)[0])\ny_pred = svr.predict(X_test)\nprint(\"MSE for writing score \" + str(metrics.mean_squared_error(y_pred, test['writing score'])))\nplt.plot(test['writing score'], y_pred, 'o')\n\nplt.plot(test['math score'], test['math score'], 'r')\n\nplt.legend(('math', 'reading', 'writing'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVR on principal components\")\n\nsvr = SVR(kernel = 'sigmoid', C = 2.0)\npca = PCA(n_components = 12, whiten = True)\n\ny_tmp, X_tmp = data_for_math(X)\nX_tr = pca.fit_transform(X_tmp)\ntest_set = pca.transform(X_test)\nsvr.fit(X_tr, y_tmp)\ny_pred = svr.predict(test_set)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['math score'])))\nplt.plot(test['math score'], y_pred, 'o')\nplt.xlabel(\"Actual score\")\nplt.ylabel(\"Predicted score\")\nplt.ylim([0, 100])\n\ny_tmp, X_tmp = data_for_reading(X)\nX_tr = pca.fit_transform(X_tmp)\ntest_set = pca.transform(X_test)\nsvr.fit(X_tr, y_tmp)\ny_pred = svr.predict(test_set)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['reading score'])))\nplt.plot(test['math score'], y_pred, 'o')\n\ny_tmp, X_tmp = data_for_writing(X)\nX_tr = pca.fit_transform(X_tmp)\ntest_set = pca.transform(X_test)\nsvr.fit(X_tr, y_tmp)\ny_pred = svr.predict(test_set)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['writing score'])))\nplt.plot(test['math score'], y_pred, 'o')\n\nplt.plot(test['math score'], test['math score'], 'r')\n\nplt.legend(('math', 'reading', 'writing'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is clear that sigmoid kernel on principal components overfits, as we expected. Overall, SVR works best for predicting reading and writing, whereas LR on principal components is the best for predicting math score. Now, we can try to see how changing the categorical ethnicity and parental education data to numerical (ordinal) will improve the predictions."},{"metadata":{},"cell_type":"markdown","source":"**4) Model fitting, validation, test and comparison - categorical to ordinal**"},{"metadata":{"trusted":true},"cell_type":"code","source":"r_e = {'group A':0, 'group B':1, 'group C':2, 'group D':3, 'group E':4}\np_e = {'high school':0, 'some high school':1, 'some college':2, 'associate\\'s degree':3, 'bachelor\\'s degree':4, 'master\\'s degree':5}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a new dataframe for model fitting on ordinal data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ord = pd.DataFrame()\n\ndata_ord['r_e'] = data['race/ethnicity'].apply(lambda row: r_e[row])\ndata_ord['p_e'] = data['parental level of education'].apply(lambda row: p_e[row])\ndata_ord[['gender_male', 'lunch_free/reduced', 'test preparation course_none', 'math score', 'reading score', 'writing score']] = data[['gender_male', 'lunch_free/reduced', 'test preparation course_none', 'math score', 'reading score', 'writing score']]\ndata_ord\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Redefining the functions for dropping outliers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_for_math_ord(X):\n    return (X[X['math score'] > 20]['math score'], X[X['math score'] > 20].drop(['math score', 'reading score', 'writing score'], axis = 1))\n\ndef data_for_reading_ord(X):\n    return (X[(X['r_e'] != 4) | ((X['r_e'] == 4) & (X['reading score'] > 40))]['reading score'], X[(X['r_e'] != 4) | ((X['r_e'] == 4) & (X['reading score'] > 40))].drop(['math score', 'reading score', 'writing score'], axis = 1))\n\ndef data_for_writing_ord(X):\n    return (X[(X['r_e'] != 4) | ((X['r_e'] == 4) & (X['writing score'] > 30))]['writing score'], X[(X['r_e'] != 4) | ((X['r_e'] == 4) & (X['writing score'] > 30))].drop(['math score', 'reading score', 'writing score'], axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression(normalize = True)\nprint(cross_val_score(lr, data_for_math_ord(data_ord)[1], data_for_math_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(lr, data_for_reading_ord(data_ord)[1], data_for_reading_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(lr, data_for_writing_ord(data_ord)[1], data_for_writing_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, that is slightly better than before. Looking at the correlations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ncorr_ord = sns.heatmap(data_ord.corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that the predictors have almost zero correlation between them. That means we do not need to apply PCA - we can look at scores for different models straight away. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(random_state = 3, max_depth = 5, min_samples_split = 30, n_estimators = 50)\nprint(cross_val_score(rf, data_for_math_ord(data_ord)[1], data_for_math_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(rf, data_for_reading_ord(data_ord)[1], data_for_reading_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(rf, data_for_writing_ord(data_ord)[1], data_for_writing_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr = SVR(kernel = 'linear', C = 0.5)\nprint(cross_val_score(svr, data_for_math_ord(data_ord)[1], data_for_math_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(svr, data_for_reading_ord(data_ord)[1], data_for_reading_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())\nprint(cross_val_score(svr, data_for_writing_ord(data_ord)[1], data_for_writing_ord(data_ord)[0], cv=5, scoring = 'neg_mean_squared_error').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying the score of linear regression on test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ord = pd.DataFrame()\n\ntest_ord['r_e'] = test['race/ethnicity'].apply(lambda row: r_e[row])\ntest_ord['p_e'] = test['parental level of education'].apply(lambda row: p_e[row])\ntest_ord[['gender_male', 'lunch_free/reduced', 'test preparation course_none']] = test[['gender_male', 'lunch_free/reduced', 'test preparation course_none']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear regression\")\n\nlr = LinearRegression(normalize = True)\n\nlr.fit(data_for_math_ord(data_ord)[1], data_for_math_ord(data_ord)[0])\ny_pred = lr.predict(test_ord)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['math score'])))\nplt.plot(test['math score'], y_pred, 'o')\nplt.xlabel(\"Actual score\")\nplt.ylabel(\"Predicted score\")\nplt.ylim([0, 100])\n\nlr.fit(data_for_reading_ord(data_ord)[1], data_for_reading_ord(data_ord)[0])\ny_pred = lr.predict(test_ord)\nprint(\"MSE for reading score \" + str(metrics.mean_squared_error(y_pred, test['reading score'])))\nplt.plot(test['reading score'], y_pred, 'o')\n\nlr.fit(data_for_writing_ord(data_ord)[1], data_for_writing_ord(data_ord)[0])\ny_pred = lr.predict(test_ord)\nprint(\"MSE for writing score \" + str(metrics.mean_squared_error(y_pred, test['writing score'])))\nplt.plot(test['writing score'], y_pred, 'o')\n\nplt.plot(test['math score'], test['math score'], 'r')\n\nplt.legend(('math', 'reading', 'writing'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVR\")\n\nsvr = SVR(kernel = 'linear', C = 0.5)\n\nsvr.fit(data_for_math_ord(data_ord)[1], data_for_math_ord(data_ord)[0])\ny_pred = svr.predict(test_ord)\nprint(\"MSE for math score \" + str(metrics.mean_squared_error(y_pred, test['math score'])))\nplt.plot(test['math score'], y_pred, 'o')\nplt.xlabel(\"Actual score\")\nplt.ylabel(\"Predicted score\")\nplt.ylim([0, 100])\n\nsvr.fit(data_for_reading_ord(data_ord)[1], data_for_reading_ord(data_ord)[0])\ny_pred = svr.predict(test_ord)\nprint(\"MSE for reading score \" + str(metrics.mean_squared_error(y_pred, test['reading score'])))\nplt.plot(test['reading score'], y_pred, 'o')\n\nsvr.fit(data_for_writing_ord(data_ord)[1], data_for_writing_ord(data_ord)[0])\ny_pred = svr.predict(test_ord)\nprint(\"MSE for writing score \" + str(metrics.mean_squared_error(y_pred, test['writing score'])))\nplt.plot(test['writing score'], y_pred, 'o')\n\nplt.plot(test['math score'], test['math score'], 'r')\n\nplt.legend(('math', 'reading', 'writing'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that transforming data to ordinal, after noticing the trends from data statistics/visualizations, gives an improvement without unnecessary complications, and linear regression works better than SVR and RF. "},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nIn this notebook, a comprehensive EDA of the dataset was performed, allowing to decide on the outliers and potential new features, such as ordinal variables used in the last section. With one-hot encoding, importance of PCA and model validation when PCA was used was shown. Comparison of various tuned ML models has shown that Linear Regression and SVR perform better than Random Forest on this dataset, and that using ordinal variables allows more accurate predictions without unnecessary complications. PS would greatly appreciate if you could upvote"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}