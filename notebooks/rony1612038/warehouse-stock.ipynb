{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-13T10:40:38.996113Z","iopub.execute_input":"2021-08-13T10:40:38.99651Z","iopub.status.idle":"2021-08-13T10:40:39.019114Z","shell.execute_reply.started":"2021-08-13T10:40:38.996426Z","shell.execute_reply":"2021-08-13T10:40:39.01818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport seaborn as sns; sns.set_theme()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import norm, skew \nfrom scipy import stats #qqplot\nimport statsmodels.api as sm \nfrom matplotlib import rcParams\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX \nimport tensorflow as tf\nprint(tf.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:42:15.826385Z","iopub.execute_input":"2021-08-13T10:42:15.826739Z","iopub.status.idle":"2021-08-13T10:42:15.840109Z","shell.execute_reply.started":"2021-08-13T10:42:15.826707Z","shell.execute_reply":"2021-08-13T10:42:15.838766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading the datafarme**","metadata":{}},{"cell_type":"code","source":"df  = pd.read_csv('../input/productdemandforecasting/Historical Product Demand.csv', parse_dates=['Date'])","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:41.102627Z","iopub.execute_input":"2021-08-13T10:40:41.102944Z","iopub.status.idle":"2021-08-13T10:40:42.622063Z","shell.execute_reply.started":"2021-08-13T10:40:41.10291Z","shell.execute_reply":"2021-08-13T10:40:42.621031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:42.62411Z","iopub.execute_input":"2021-08-13T10:40:42.624429Z","iopub.status.idle":"2021-08-13T10:40:42.646392Z","shell.execute_reply.started":"2021-08-13T10:40:42.624399Z","shell.execute_reply":"2021-08-13T10:40:42.645311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finding the NAN values**\n1. Find the percentage of total NAN value","metadata":{}},{"cell_type":"code","source":"print (df.isna().sum())\nprint((100 * df['Date'].isna().sum()) / len(df))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:42.64821Z","iopub.execute_input":"2021-08-13T10:40:42.648566Z","iopub.status.idle":"2021-08-13T10:40:43.115971Z","shell.execute_reply.started":"2021-08-13T10:40:42.648531Z","shell.execute_reply":"2021-08-13T10:40:43.115054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since only 1% of data is NAN therefore we will drop all NAN values","metadata":{}},{"cell_type":"code","source":"df = df.dropna(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:43.117214Z","iopub.execute_input":"2021-08-13T10:40:43.117673Z","iopub.status.idle":"2021-08-13T10:40:43.756747Z","shell.execute_reply.started":"2021-08-13T10:40:43.117635Z","shell.execute_reply":"2021-08-13T10:40:43.755813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Visualizign whole data using heatmap**\n1. make sure there is no NAN value ","metadata":{}},{"cell_type":"code","source":"df.isnull()\nsns.heatmap(df.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:43.761242Z","iopub.execute_input":"2021-08-13T10:40:43.763228Z","iopub.status.idle":"2021-08-13T10:40:50.635016Z","shell.execute_reply.started":"2021-08-13T10:40:43.763188Z","shell.execute_reply":"2021-08-13T10:40:50.634203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing some unnecessary () Found in Order_Demand Column**","metadata":{}},{"cell_type":"code","source":"df.sort_values('Date')[10:20] \n\ndf['Order_Demand'] = df['Order_Demand'].str.replace('(',\"\")\ndf['Order_Demand'] = df['Order_Demand'].str.replace(')',\"\")\n\n#Chaning the dataType into int64\ndf['Order_Demand'] = df['Order_Demand'].astype('int64')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:50.636929Z","iopub.execute_input":"2021-08-13T10:40:50.637277Z","iopub.status.idle":"2021-08-13T10:40:52.050654Z","shell.execute_reply.started":"2021-08-13T10:40:50.63724Z","shell.execute_reply":"2021-08-13T10:40:52.04979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing the max order shipped by each warehouse**","metadata":{}},{"cell_type":"code","source":"x = df.groupby('Warehouse').sum().sort_values('Order_Demand', ascending = False)\nequiv = {3363200396:\"Whse_J\", 1038024700:\"Whse_S\", 585071404:\"Whse_C\",147877431:\"Whse_A\"}\nx[\"Order\"] = x[\"Order_Demand\"].map(equiv)\nx.plot.bar(x='Order', y='Order_Demand', rot=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:52.052457Z","iopub.execute_input":"2021-08-13T10:40:52.052804Z","iopub.status.idle":"2021-08-13T10:40:52.395877Z","shell.execute_reply.started":"2021-08-13T10:40:52.052767Z","shell.execute_reply":"2021-08-13T10:40:52.394952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing all distinct product along with their quantity**","metadata":{}},{"cell_type":"code","source":"print (\"Total Distinct Proudct Category\",len(df['Product_Category'].value_counts()))\nrcParams['figure.figsize'] = 50,10\nsb.countplot(df['Product_Category'].sort_values(ascending = True))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:52.39723Z","iopub.execute_input":"2021-08-13T10:40:52.397698Z","iopub.status.idle":"2021-08-13T10:40:55.926929Z","shell.execute_reply.started":"2021-08-13T10:40:52.397659Z","shell.execute_reply":"2021-08-13T10:40:55.925109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing All Orders By Warehouses**\n","metadata":{}},{"cell_type":"code","source":"rcParams['figure.figsize'] = 16,4\nf, axes = plt.subplots(1, 2)\nfig3 = sb.boxplot( df['Warehouse'],df['Order_Demand'], ax = axes[0])\nfig4 = sb.boxplot( df['Warehouse'], np.log1p(df['Order_Demand']),ax = axes[1])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:55.928162Z","iopub.execute_input":"2021-08-13T10:40:55.9285Z","iopub.status.idle":"2021-08-13T10:40:58.162813Z","shell.execute_reply.started":"2021-08-13T10:40:55.928464Z","shell.execute_reply":"2021-08-13T10:40:58.161888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing All Orders by Product Category**\n","metadata":{}},{"cell_type":"code","source":"rcParams['figure.figsize'] = 50,12\ndf_temp = df.sample(n=20000).reset_index()\nfig5 = sb.boxplot( df_temp['Product_Category'].sort_values(),np.log1p(df_temp['Order_Demand']))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:40:58.164011Z","iopub.execute_input":"2021-08-13T10:40:58.164327Z","iopub.status.idle":"2021-08-13T10:40:59.067447Z","shell.execute_reply.started":"2021-08-13T10:40:58.164298Z","shell.execute_reply":"2021-08-13T10:40:59.066461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparing the Dataset**","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv('../input/productdemandforecasting/Historical Product Demand.csv')\n\n\ndf.dropna(axis=0, inplace=True) #remove all rows with na's.\ndf.reset_index(drop=True)\n\n#Target Feature - Order_Demand\n#Removing () from the target feature.\ndf['Order_Demand'] = df['Order_Demand'].str.replace('(',\"\")\ndf['Order_Demand'] = df['Order_Demand'].str.replace(')',\"\")\n\n#Next step is to change the data type.\ndf['Order_Demand'] = df['Order_Demand'].astype('int64')\n\ndf.drop('Product_Code', inplace=True, axis=1)\ndf.drop('Warehouse', inplace=True, axis=1)\ndf.drop('Product_Category', inplace=True, axis=1)\n\ndf = df.groupby('Date')['Order_Demand'].sum().reset_index()\n\n\n#Index the date\ndf = df.set_index('Date')\ndf.index #Lets check the index\n\ndf.to_csv('./final.csv')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:44:16.431382Z","iopub.execute_input":"2021-08-13T10:44:16.431706Z","iopub.status.idle":"2021-08-13T10:44:19.264355Z","shell.execute_reply.started":"2021-08-13T10:44:16.431674Z","shell.execute_reply":"2021-08-13T10:44:19.26338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Making Prediction**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('./final.csv')\n\ndf1=df.reset_index()['Order_Demand']\n\ndf1","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:46:50.079455Z","iopub.execute_input":"2021-08-13T10:46:50.07978Z","iopub.status.idle":"2021-08-13T10:46:50.092401Z","shell.execute_reply.started":"2021-08-13T10:46:50.079747Z","shell.execute_reply":"2021-08-13T10:46:50.091374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:47:12.754324Z","iopub.execute_input":"2021-08-13T10:47:12.754648Z","iopub.status.idle":"2021-08-13T10:47:12.992362Z","shell.execute_reply.started":"2021-08-13T10:47:12.754619Z","shell.execute_reply":"2021-08-13T10:47:12.991444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler(feature_range=(0,1))\ndf1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n\nprint(df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:47:44.837328Z","iopub.execute_input":"2021-08-13T10:47:44.837647Z","iopub.status.idle":"2021-08-13T10:47:44.844223Z","shell.execute_reply.started":"2021-08-13T10:47:44.837616Z","shell.execute_reply":"2021-08-13T10:47:44.843278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##splitting dataset into train and test split\ntraining_size=int(len(df1)*0.8)\ntest_size=len(df1)-training_size\ntrain_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]\n\ntraining_size,test_size","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:48:17.414308Z","iopub.execute_input":"2021-08-13T10:48:17.414664Z","iopub.status.idle":"2021-08-13T10:48:17.422044Z","shell.execute_reply.started":"2021-08-13T10:48:17.414631Z","shell.execute_reply":"2021-08-13T10:48:17.420877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy\n# convert an array of values into a dataset matrix\ndef create_dataset(dataset, time_step=1):\n\tdataX, dataY = [], []\n\tfor i in range(len(dataset)-time_step-1):\n\t\ta = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n\t\tdataX.append(a)\n\t\tdataY.append(dataset[i + time_step, 0])\n\treturn numpy.array(dataX), numpy.array(dataY)\n\n# reshape into X=t,t+1,t+2,t+3 and Y=t+4\ntime_step = 100\nX_train, y_train = create_dataset(train_data, time_step)\nX_test, ytest = create_dataset(test_data, time_step)\n\nprint(X_train.shape), print(y_train.shape)\n\nprint(X_test.shape), print(ytest.shape)\n\n# reshape input to be [samples, time steps, features] which is required for LSTM\nX_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\nX_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:49:00.04952Z","iopub.execute_input":"2021-08-13T10:49:00.049849Z","iopub.status.idle":"2021-08-13T10:49:00.064205Z","shell.execute_reply.started":"2021-08-13T10:49:00.049816Z","shell.execute_reply":"2021-08-13T10:49:00.063185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Create the Stacked LSTM model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\n\nmodel=Sequential()\nmodel.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\nmodel.add(LSTM(50,return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error',optimizer='adam')\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:49:19.963525Z","iopub.execute_input":"2021-08-13T10:49:19.963877Z","iopub.status.idle":"2021-08-13T10:49:20.586979Z","shell.execute_reply.started":"2021-08-13T10:49:19.963842Z","shell.execute_reply":"2021-08-13T10:49:20.586081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=50,batch_size=64,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:49:49.591305Z","iopub.execute_input":"2021-08-13T10:49:49.591638Z","iopub.status.idle":"2021-08-13T10:50:09.621474Z","shell.execute_reply.started":"2021-08-13T10:49:49.591607Z","shell.execute_reply":"2021-08-13T10:50:09.620683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### prediction and performance metrics\ntrain_predict=model.predict(X_train)\ntest_predict=model.predict(X_test)\n##Transformback to original form\ntrain_predict=scaler.inverse_transform(train_predict)\ntest_predict=scaler.inverse_transform(test_predict)\n### Calculate RMSE performance metrics\nimport math\nfrom sklearn.metrics import mean_squared_error\nmath.sqrt(mean_squared_error(y_train,train_predict))\n\n### Test Data RMSE\nmath.sqrt(mean_squared_error(ytest,test_predict))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:51:07.171204Z","iopub.execute_input":"2021-08-13T10:51:07.171538Z","iopub.status.idle":"2021-08-13T10:51:07.473263Z","shell.execute_reply.started":"2021-08-13T10:51:07.171507Z","shell.execute_reply":"2021-08-13T10:51:07.472399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Plotting \n# shift train predictions for plotting\nlook_back=100\ntrainPredictPlot = numpy.empty_like(df1)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(df1)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(df1))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T10:51:26.984465Z","iopub.execute_input":"2021-08-13T10:51:26.984887Z","iopub.status.idle":"2021-08-13T10:51:27.217404Z","shell.execute_reply.started":"2021-08-13T10:51:26.984847Z","shell.execute_reply":"2021-08-13T10:51:27.21663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}