{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1) Read data and get quick overview about dataset using pandas"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv('/kaggle/input/indian-startup-funding/startup_funding.csv',parse_dates=['Date dd/mm/yyyy'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### find total nan values in each column\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : there are many NaNs values in different columns so we can to preprocessed or clean the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) Data Preprocessing and Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"### 2.1) preprocessing on Date column"},{"metadata":{},"cell_type":"markdown","source":"Observations : i want to convert Date dd/mm/yyyy column into datetime but some how i am not able to do that so i have to find all\ninvalid dates so that we can correct them"},{"metadata":{"trusted":true},"cell_type":"code","source":"invalid_dates = []\n## write re to match date\ndate_pattern = '\\d{2}/\\d{2}/\\d{4}'\nfor i in range(len(df)):\n    d = df.loc[i,'Date dd/mm/yyyy']\n    res = re.findall(date_pattern,d)\n    if len(res) <=0:\n        ## append with index\n        invalid_dates.append([i,d])\nelse:\n    [print(x) for x in invalid_dates]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see all invalid dates and there correspond index value was printed"},{"metadata":{},"cell_type":"markdown","source":"there are few invalid dates which contains . and // so let's replace it with /"},{"metadata":{"trusted":true},"cell_type":"code","source":"def findInvalidDates(df):\n\n    invalid_dates = []\n    for i in range(len(df)):\n        d = df.loc[i,'Date dd/mm/yyyy']\n        res = re.findall(date_pattern,d)\n        if len(res) <=0:\n            \n            if(d.find(\"//\")!=-1):\n                d = d.replace(\"//\",'/')\n            elif (d.find(\".\")!=-1):\n                d = d.replace(\".\",'/')\n            else:\n                invalid_dates.append([i,d])\n\n            df.loc[i,'Date dd/mm/yyyy'] = d\n            \n    return f\"remaining invalid dates : {invalid_dates}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"findInvalidDates(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : only 3 dates are invalid so we have to correct them manually otherwise it will take much time to write program for them"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[192,'Date dd/mm/yyyy'] = \"05/07/2018\"\ndf.loc[2571,'Date dd/mm/yyyy'] = \"01/07/2015\"\ndf.loc[2606,'Date dd/mm/yyyy'] = \"01/07/2015\"\n\nfindInvalidDates(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : Now there is no invalid date in Date column , so we can convert it into datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date dd/mm/yyyy'] = pd.to_datetime(df['Date dd/mm/yyyy'])\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2) preprocessing on StartupName column"},{"metadata":{"trusted":true},"cell_type":"code","source":"## this is program to find invalid stratup names\n\n# re to match urls/website links\nlink_pattern = \"(http)|(www.\\w+)\"\ninvalid_names = []\nfor i in range(len(df)):\n    d = df.loc[i,'Startup Name']\n    res = re.findall(link_pattern,d)\n    \n    if len(res) >0:\n        invalid_names.append([i,d])\ninvalid_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we can see that there is only one entry where startupname was wrong   , so correct it manually"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[3,'Startup Name'] = \"wealthbucket\"\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3) To fill nan values in columns we have to draw some of visuals so we can identify some data patterns."},{"metadata":{"trusted":true},"cell_type":"code","source":"## this function return dataframe which show relation between 2 categories in term of counts\ndef showNaNValuesForEachItemInCategory(col):\n    g=df.groupby(col)\n\n    return -g.count().sub(g.size(),0)\nshowNaNValuesForEachItemInCategory('InvestmentnType')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## most of nan values are for Private Equity , seed funding\n\ndf[df['InvestmentnType']==\"Private Equity\"][\"Industry Vertical\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Industry Vertical']=='Consumer Internet'][\"Industry Vertical\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### most of the private equality and seed funding investmenttype have industry vertical is consumer internet\ndf['Industry Vertical'] = df['Industry Vertical'].fillna(\"Consumer Internet\")\ndf.isnull().sum()\ndf['Industry Vertical'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['City  Location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : there are some invalid city names in city location column so first make that correct"},{"metadata":{"trusted":true},"cell_type":"code","source":"invalid_location = []\nfor i in range(len(df)):\n    \n    l = str(df.loc[i,'City  Location'])\n    res = re.findall('[^a-zA-Z\\s]+',l)\n    \n    if (len(res) > 0):\n        \n        invalid_location.append([i,l])\nelse:\n    [print(x) for x in invalid_location]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeSpecialChars(s):\n    \"\"\" this function remove special characters from string\"\"\"\n    s = re.sub('[^a-zA-Z\\s]','',s)\n    return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## just test function \nremoveSpecialChars(\"del///2/*hi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"invalid_location = []\nfor i in range(len(df)):\n    \n    l = str(df.loc[i,'City  Location'])\n    res = re.findall('[^a-zA-Z\\s]+',l)\n    \n    if (len(res) > 0):\n        if(l.find('/')!=-1):\n            s = l.split(\"/\")[0]\n            s = removeSpecialChars(s)\n            df.loc[i,'City  Location'] = s\n        else:\n            invalid_location.append([i,l])\nelse:\n    [print(x) for x in invalid_location]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### many of the city names are corrected , but few of them are still not corrected "},{"metadata":{"trusted":true},"cell_type":"code","source":"## correct all city names which contains xc2 as substring\n\nfor i in range(len(df)):\n    l = str(df.loc[i,'City  Location'])\n    if l.find(\"xc2\")!=-1:\n        ## if string contains substring xc2 then remove go for further spliting \n        df.loc[i,'City  Location'] = l.split(\"0\")[-1]\n    \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['City  Location'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now there is no invalid values in city location column ,so we can peform further observation"},{"metadata":{"trusted":true},"cell_type":"code","source":"showNaNValuesForEachItemInCategory(\"City  Location\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Industry Vertical\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## use stastical test to check city location is dependent on any other column or not \n\n\ndef checkDependencyBetweenCategories(cat1,cat2):\n    data = pd.crosstab(index=df[cat1],columns = df[cat2])\n    stat, p, dof, expected = chi2_contingency(data) \n\n    # interpret p-value \n    alpha = 0.05\n    print(\"p value is \" + str(p)) \n    if p <= alpha: \n        print(cat1 ,' is Dependent  on ',cat2,\" (reject H0)\") \n    else: \n        print(cat1 , \" & \",cat2,'bath are Independent (H0 holds true)') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkDependencyBetweenCategories(\"City  Location\",\"Industry Vertical\")\ncheckDependencyBetweenCategories(\"City  Location\",\"InvestmentnType\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : City names are depend on both Industry vertical and investType"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's try to identify some patterns\ndf[pd.isnull(df['City  Location'])]['Industry Vertical'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Industry Vertical']==\"Consumer Internet\"]['City  Location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : most of the enteries for Banglore and mumbai in consumer internet type"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMaxCountValueBetweenTwoCats(cat1,cat2):\n    b = pd.crosstab(df[cat1],df[cat2])\n    a = b.values\n    (i,j) = np.unravel_index(a.argmax(),a.shape)\n    print(f\"For index : {b.index[i]} and column : {b.columns[j]} max value is : {a.max()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getMaxCountValueBetweenTwoCats(\"City  Location\",\"Industry Vertical\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getMaxCountValueBetweenTwoCats(\"City  Location\",\"InvestmentnType\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getMaxCountValueBetweenTwoCats(\"Industry Vertical\",\"InvestmentnType\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['InvestmentnType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## fill nan values in City location column based on values of industry vertical , InvestmentnType\n\nfor i in range(len(df)):\n    d = df.loc[i,\"City  Location\"]\n    \n    if(pd.isnull(d)):\n        if(df.loc[i,\"Industry Vertical\"]==\"Consumer Internet\"):\n            df.loc[i,\"City  Location\"] = \"Bangalore\"\n    #     if(df.loc[i,\"InvestmentnType\"] == \"PrivateEquity\"):\n    #         df.loc[i,\"City  Location\"] = \"Bangalore\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(df)):\n    d = df.loc[i,\"City  Location\"]\n    \n    \n    if(pd.isnull(df.loc[i,\"City  Location\"])):\n        df.loc[i,\"City  Location\"] = \"Mumbai\"\n    \n    \nelse:\n    df['City  Location'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## bengaluru and bangalore both are same .....\ndf.loc[df['City  Location']==\"Bengaluru\",\"City  Location\"] = \"Bangalore\"\ndf['City  Location'].value_counts()\n## there are too many cities let's draw wordcloud for it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\n## its return a image we can plot that image using plt.imshow() method\n## we don't want to consider stopwords so we remove those stopwords ( ex : is,a,an, the, he , she .....etc)\nwordcloud = WordCloud(width=3000,height=2000,background_color=\"black\",stopwords=STOPWORDS).generate(\" \".join(df['City  Location']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 20))\nplt.axis('off')\nplt.imshow(wordcloud) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## fill nan values in SubVertical\ndf['SubVertical'] = df['SubVertical'].fillna(\"others\")\ncounter = 1\n\n## fill nan values in Investors Name\nfor i in range(len(df)):\n    iname = df.loc[i,'Investors Name']\n    if pd.isnull(iname):\n        df.loc[i,'Investors Name'] = f\"Name {counter}\"\n        counter+=1\n        \n\ndf.isnull().sum()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove nan values in InvestmentType\ndf = df.dropna(subset=['InvestmentnType'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## reset index\ndf = df.reset_index(drop=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4) preprocessing on InvestmentnType"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_angel_funding = ['Seed/ Angel Funding','Seed / Angel Fundin','Seed/Angel Funding','Angel / Seed Funding','Seed / Angle Funding']\nseed_funding = ['Seed Funding','Seed\\\\nFunding','Seed','Seed funding','Seed Funding Round']\ndebt_funding = ['Debt Funding','Debt-Funding','Debt']\ndf['InvestmentnType'].value_counts()\ndf.loc[df['InvestmentnType'].isin(seed_funding),\"InvestmentnType\"] = \"Seed Funding\"\ndf.loc[df['InvestmentnType'].isin(seed_angel_funding),\"InvestmentnType\"] = \"Seed / Angel Funding\"\ndf.loc[df['InvestmentnType'].isin(debt_funding),\"InvestmentnType\"] = \"Debt Funding\"\ndf['InvestmentnType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5) preprocessing on Amount in USD column"},{"metadata":{"trusted":true},"cell_type":"code","source":"## first convert it into float\ninvalid_vals = []\n\nfor j in range(len(df)):\n    p = df.loc[j,\"Amount in USD\"]\n    if not pd.isnull(p):\n        df.loc[j,\"Amount in USD\"] = p.replace(\",\",\"\")\n        res = re.findall(\"[0-9]+\",p)\n        if(len(res)<=0):\n            invalid_vals.append([j,p])\n        res1 = re.findall(\"[\\W]+\",p)\n        if(len(res1)>0):\n            invalid_vals.append([j,p])\n        \n    \ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"invalid_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[20,\"Amount in USD\"] = np.nan\ndf.loc[33,\"Amount in USD\"] = np.nan\ndf.loc[57,\"Amount in USD\"] = np.nan\ndf.loc[86,\"Amount in USD\"] = np.nan\ndf.loc[88,\"Amount in USD\"] = np.nan\ndf.loc[109,\"Amount in USD\"] = np.nan\ndf.loc[136,\"Amount in USD\"] = np.nan\ndf.loc[80,\"Amount in USD\"] = 15109500\ndf.loc[106,\"Amount in USD\"] = 14342000\n\nfor i in range(len(df)):\n    p = str(df.loc[i,\"Amount in USD\"])\n    if (p.find(\"xa0\")!=-1):\n        if(p.find(\"N/A\")!=-1):\n            df.loc[i,\"Amount in USD\"] = np.nan\n            print(\"N/A settal\")\n        else:\n            df.loc[i,\"Amount in USD\"] = p.split(\"xa0\")[-1]\n            print(\"Amount settaled\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Amount in USD'] = df['Amount in USD'].astype(\"float\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's fill nan values in amount column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Amount in USD'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## for now we fill nan values with median value \ndf['Amount in USD'].mean() , df['Amount in USD'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Amount in USD'] = df['Amount in USD'].fillna(df['Amount in USD'].median())\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## remarks columns is not much useful for data analysis and modeling so drop it\ndf = df.drop(\"Remarks\",axis=1)\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's start with data visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"def countplotForCategory(col1):\n    t = df[col1].value_counts()[:10].index\n    sns.countplot(y=col1,order=t,data=df)\n    plt.title(f\"Countplot for {col1}\")\n    plt.show()\n\ncountplotForCategory(col1 = \"Industry Vertical\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplotForCategory(\"City  Location\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplotForCategory(\"Investors Name\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## above we can see that there are Undisclosed Investors and Undisclosed investors  meaning of both category is same just spelling is different let's make it one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disclosed_invet_names = ['Undisclosed Investors','Undisclosed Investor','Undisclosed investor','undisclosed investors','undisclosed investor','Undisclosed investors','Undisclosed','Undisclosed']\n\ndf.loc[df['Investors Name'].isin(disclosed_invet_names) ,\"Investors Name\"] = \"Undisclosed\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplotForCategory(\"Investors Name\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplotForCategory(\"InvestmentnType\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplotForCategory(\"InvestmentnType\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hope you learn something from this , Thanks for watching\n## Please give a upvote if you have learnt something :) Happy Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}