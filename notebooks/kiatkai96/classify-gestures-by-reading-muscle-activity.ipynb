{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data0 = pd.read_csv(\"/kaggle/input/emg-4/0.csv\", header=None) # For class 0: rock\ndata1 = pd.read_csv(\"/kaggle/input/emg-4/1.csv\", header=None) # For class 1: scissors\ndata2 = pd.read_csv(\"/kaggle/input/emg-4/2.csv\", header=None) # For class 2: paper\ndata3 = pd.read_csv(\"/kaggle/input/emg-4/3.csv\", header=None) # For class 3: ok\n\n# 8 consecutive readings of all 8 sensors which is why 64 columns plus last column is the class = 65 columns\n\n# Now, we will combine all the dataset into 1 big dataset\ndata = pd.concat([data0,data1,data2,data3], axis=0)\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into X and Y\nY = data.iloc[:,-1]\nX = data.drop(data.columns[-1], axis=1)\n\n# Now, train test split\nX_train, Xtest, Y_train, Ytest = train_test_split(X, Y, train_size=0.8, random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using different models for classification","metadata":{}},{"cell_type":"markdown","source":"### 1) Linear Discriminant Analysis","metadata":{}},{"cell_type":"code","source":"lda = LinearDiscriminantAnalysis()\ny_pred = lda.fit(X_train, Y_train).predict(Xtest)\nf1_lda = f1_score(Ytest, y_pred, average='micro')\nprint(\"F1 Score for Linear Discriminant Analysis Classifier is\", f1_lda)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2) Quadratic Discriminant Analysis","metadata":{}},{"cell_type":"code","source":"qda = QuadraticDiscriminantAnalysis()\ny_pred = qda.fit(X_train, Y_train).predict(Xtest)\nf1_qda = f1_score(Ytest, y_pred, average='micro')\nprint(\"F1 Score for Quadratic Discriminant Analysis Classifier is\", f1_qda)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3) Naive Bayes","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB()\ny_pred = gnb.fit(X_train, Y_train).predict(Xtest)\nf1_nb = f1_score(Ytest, y_pred, average='micro')\nprint(\"F1 Score for Naive Bayes Classifier is\", f1_nb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4) Random Forest","metadata":{}},{"cell_type":"code","source":"rfc=RandomForestClassifier(random_state=100)\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8,9,10],\n    'criterion' :['gini', 'entropy']\n}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CV_rfc.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best parameters after tuning:\nrf = RandomForestClassifier(random_state=100, n_estimators=500, criterion='gini', max_depth=10, max_features='log2')\ny_pred = rf.fit(X_train, Y_train).predict(Xtest)\nf1_rf = f1_score(Ytest, y_pred, average='micro')\nprint(\"F1 Score for Random Forest Classifier is\", f1_rf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"svc=SVC(random_state=100)\n\nparam_grid = { \n    'C': [0.01, 0.1, 1],\n    'kernel': ['linear','rbf'],\n}\n\nCV_svc = GridSearchCV(estimator=svc, param_grid=param_grid, cv= 5)\nCV_svc.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CV_svc.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best parameters after tuning:\nrf = SVC(random_state=100, C=1, kernel=\"rbf\")\ny_pred = rf.fit(X_train, Y_train).predict(Xtest)\nf1_svc = f1_score(Ytest, y_pred, average='micro')\nprint(\"F1 Score for Support Vector Classifier is\", f1_svc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating all models","metadata":{}},{"cell_type":"code","source":"# Dataframe to contain model results\nmodel_results = pd.DataFrame(columns=[\"Models\",\"F1 Score\"])\n\n# LDA\nmodel_results = model_results.append(pd.DataFrame({\"Models\":\"Linear Discriminant Analysis\", \n                                  \"F1 Score\":f1_lda}, index = [0]), ignore_index = False)\n# QDA\nmodel_results = model_results.append(pd.DataFrame({\"Models\":\"Quadratic Discriminant Analysis\", \n                                  \"F1 Score\":f1_qda}, index = [1]), ignore_index = False)\n# Naive Bayes\nmodel_results = model_results.append(pd.DataFrame({\"Models\":\"Naive Bayes\", \n                                  \"F1 Score\":f1_nb}, index = [2]), ignore_index = False)\n# Random Forest\nmodel_results = model_results.append(pd.DataFrame({\"Models\":\"Random Forest\", \n                                  \"F1 Score\":f1_rf}, index = [3]), ignore_index = False)\n# Support Vector Classifier\nmodel_results = model_results.append(pd.DataFrame({\"Models\":\"Support Vector Classifier\", \n                                  \"F1 Score\":f1_svc}, index = [4]), ignore_index = False)\n\nmodel_results.sort_values(by=\"F1 Score\", ascending = False)","metadata":{},"execution_count":null,"outputs":[]}]}