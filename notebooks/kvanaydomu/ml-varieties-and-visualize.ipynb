{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduce\n\n## I will mention that orthopedic trouble and it's elements.Actually,I examine this data why implementing machine learning methods above there.While visualize and explore data ,at the same time,I'm going to show you exhaustive methods and I will predict class of troubles.\n\n<font color=\"blue\">\nContent :\n    \n1. [Load,Check and Describe Data](#1)\n    \n2. [Visulize](#2)\n    * [Correlation to slope that columns to class of patients which abnormal and normal](#3)\n    * [Rates class of patients](#4)\n    * [class -- pelvic_incidence](#5)\n    * [class -- pelvic_tilt numeric](#6)\n    * [class -- lumbar_lordosis_angle](#7)\n    * [class -- pelvic_incidence -- pelvic_tilt numeric](#8)\n    * [class -- sacral_slope](#9)\n    * [class -- pelvic_radius](#10)\n    * [class -- degree_spondylolisthesis](#11)\n    * [class -- pelvic_radius -- pelvic_incidence](#12)\n    * [class -- pelvic_tilt numeric -- lumbar_lordosis_angle](#13)\n    * [class -- sacral_slope -- pelvic_incidence](#14)\n    * [class -- pelvic_radius -- sacral_slope](#15)\n    * [class -- lumbar_lordosis_angle -- pelvic_radius](#16)\n    * [class -- pelvic_tilt numeric -- degree_spondylolisthesis](#17)\n    * [class -- lumbar_lordosis_angle numeric -- degree_spondylolisthesis](#18)\n    * [class -- pelvic_tilt numeric -- degree_spondylolisthesis](#19)\n    * [Except for class that in columns,show relatipnship between each other](#20)\n3. [Machine Learning](#21)\n\n    * [Supervised Learning](#ek1)\n        * [K-NEAREST NEIGHBORS (KNN)](#22)\n        * [Regression](#23)\n            * [Linear Regression](#24)\n            * [Polynomal Regression](#25)\n            * [Desicion Tree](#26)\n            * [Random Forest](#27)\n        * [Logistic Regression](#28)\n            * [Computation Graph](#29)\n            * [Changes Data](#30)\n            * [Initializing parameters](#31)\n            * [Forward Propagation](#32)\n            * [Optimization Algorithm with Gradually Descent](#33)\n            * [Update](#34)\n            * [Prediction](#35)\n            * [Logistic Regression by sklearn](#36)\n        * [Support Vector Machine(SVM) Algorithm](#37)\n        * [Naive Bayes Classification](#38)\n    * [Unsupervised Learning](#39)\n        * [KMEANS](#40)\n        * [EVALUATE OF CLUSTERING](#41)\n        * [STANDARDIZATION](#42)\n        * [HIERARCHY](#43)\n        * [T - Distributed Stochastic Neighbor Embedding (T - SNE)](#44)\n        * [PRINCIPLE COMPONENT ANALYSIS (PCA)](#45)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n## Load,Check and Describe Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you want to carried out to effective product,your data's values should be closer.So I will examine the values by describe function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n## Visualize","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now,let's start the visualize to more learn this event","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This code groups show that slope to being abnormal or normal.Features this code that exhaustive evulate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" <a id=\"3\"></a>\n ### Correlation to slope that columns to class of patients which abnormal and normal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"colorList = [\"red\" if i==\"Abnormal\" else \"green\" for i in data[\"class\"]]\npd.plotting.scatter_matrix(data.loc[:,data.columns!=\"class\"],\n                          c=colorList,\n                           figsize=(15,15),\n                           diagonal=\"hist\",\n                           alpha=0.5,\n                           s=200,\n                           marker=\"*\",\n                           edgecolor=\"black\"\n                          )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n## Rates class of patients","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.factorplot(x=\"class\",size=6,data=data,kind=\"count\")\ng.add_legend()\ng.set_ylabels(\"Rates class of trouble\")\nplt.show()\nprint(data[\"class\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n## class -- pelvic_incidence","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will research that relationship between class of patient and pelvic_incidence angle.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,col=\"class\",height=4)\ng.map(plt.hist,\"pelvic_incidence\",bins=25)\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n## class -- pelvic_tilt numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,col=\"class\",height=4)\ng.map(plt.hist,\"pelvic_tilt numeric\",bins=25)\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see two instances,anbormal patient has lots of increase angle or It will more and more increasing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n## class -- lumbar_lordosis_angle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=4)\ng.map(plt.hist,\"lumbar_lordosis_angle\",bins=25)\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=4)\ng.map(sns.pointplot,\"lumbar_lordosis_angle\",bins=25)\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n## class -- pelvic_incidence -- pelvic_tilt numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=5)\ng.map(sns.pointplot,\"pelvic_incidence\",\"pelvic_tilt numeric\")\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A=data[data[\"class\"]==\"Abnormal\"]\nN=data[data[\"class\"]==\"Normal\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"pelvic_incidence\"],A[\"pelvic_tilt numeric\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"pelvic_incidence\"],N[\"pelvic_tilt numeric\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"pelvic_tilt numeric\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n## class -- sacral_slope","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=5)\ng.map(sns.pointplot,\"sacral_slope\")\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=5)\ng.map(plt.hist,\"sacral_slope\",bins=25)\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n## class -- pelvic_radius","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=5)\ng.add_legend()\ng.map(plt.hist,\"pelvic_radius\",bins=5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=5)\ng.add_legend()\ng.map(sns.pointplot,\"pelvic_radius\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n## class -- degree_spondylolisthesis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=6)\ng.map(plt.hist,\"degree_spondylolisthesis\",bins=35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(data,row=\"class\",height=6)\ng.map(sns.pointplot,\"degree_spondylolisthesis\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n### class -- pelvic_radius -- pelvic_incidence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"pelvic_radius\"],A[\"pelvic_incidence\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"pelvic_radius\"],N[\"pelvic_incidence\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"pelvic_incidence\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n### class -- pelvic_tilt numeric -- lumbar_lordosis_angle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"pelvic_tilt numeric\"],A[\"lumbar_lordosis_angle\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"pelvic_tilt numeric\"],N[\"lumbar_lordosis_angle\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"pelvic_tilt numeric\")\nplt.ylabel(\"lumbar_lordosis_angle\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n### class -- sacral_slope -- pelvic_incidence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"sacral_slope\"],A[\"pelvic_incidence\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"sacral_slope\"],N[\"pelvic_incidence\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"sacral_slope\")\nplt.ylabel(\"pelvic_incidence\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a>\n### class -- pelvic_radius -- sacral_slope","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"pelvic_radius\"],A[\"sacral_slope\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"pelvic_radius\"],N[\"sacral_slope\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"sacral_slope\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a>\n### class -- lumbar_lordosis_angle -- pelvic_radius","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"lumbar_lordosis_angle\"],A[\"pelvic_radius\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"lumbar_lordosis_angle\"],N[\"pelvic_radius\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"pelvic_radius\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a>\n### class -- pelvic_tilt numeric -- degree_spondylolisthesis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"pelvic_tilt numeric\"],A[\"degree_spondylolisthesis\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"pelvic_tilt numeric\"],N[\"degree_spondylolisthesis\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"pelvic_tilt numeric\")\nplt.ylabel(\"degree_spondylolisthesis\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a>\n### class -- lumbar_lordosis_angle numeric -- degree_spondylolisthesis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"lumbar_lordosis_angle\"],A[\"degree_spondylolisthesis\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"lumbar_lordosis_angle\"],N[\"degree_spondylolisthesis\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"degree_spondylolisthesis\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"19\"></a>\n### class -- pelvic_tilt numeric -- degree_spondylolisthesis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"pelvic_tilt numeric\"],A[\"degree_spondylolisthesis\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"pelvic_tilt numeric\"],N[\"degree_spondylolisthesis\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"pelvic_tilt numeric\t\")\nplt.ylabel(\"degree_spondylolisthesis\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will observe relationship between columns to more learn","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"20\"></a>\n## Except for class that in columns,show relationship between each other","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.loc[:,data.columns!=\"class\"].corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see above,sacrol_slope and pelvic_incidence more closer than others","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"21\"></a>\n# Machine Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As you know,machine learning has very wide fields of artifical intelligent,and you can do effectively things.However,I 'm new this branch,and I'm going to implement methods that I know on data.I will show KNN algorithms,Regressions,LinearRegression ,as well as fit,predict methods.While look at my scripts,you will have basic ML info","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ek1\"></a>\n## Supervised Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"22\"></a>\n### K-NEAREST NEIGHBORS (KNN)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* This is classification methods\n* Firstly,we will train data,namely using fit()\n* fit() : train data,fits data\n* predict() : predict data\n* x : features, y : target variables(normal,abnormal)\n* n_neighbours = 3 ,usually equal 3 at the begin\n* Later , we will try to find most reasonable value for n_neighours\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below,algorithms will predict that class of patients according to current data.But it will not completevly true ,because I will data (x) and I will predict data(x).This is not a real predict methods.However good for begin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=3)\nx,y=data.loc[:,data.columns!=\"class\"],data.loc[:,\"class\"]\nknn.fit(x,y)\nprediction=knn.predict(x)\nprint(\"Prediction : {}\".format(prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now,we split the data and reexamine to behavior why diverges the reality.Above,we find predict target variables , however we used same data(x),and predict same data.This is not much hard.So we will try to split data how train and test then,find data that include more reality","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"R^2 score is control and rates of accordance.Takes value between 0-1.If value will how much approach to 1 ,then ,this is good for algorithm.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Also fit methods implement between xTrain and yTrain,r^2 score calculate between xTest and yTest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If we will want to show healthy results, we should split our data as test and train.So we will have train and test struct and we can implement fit and predict methods true","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.3,random_state=1)\nknn=KNeighborsClassifier(n_neighbors=3)\nx,y=data.loc[:,data.columns!=\"class\"],data.loc[:,\"class\"]\nknn.fit(xTrain,yTrain)\nprediction=knn.predict(xTest)\nprint(\"R^2 score is (k=3) = {}\".format(knn.score(xTest,yTest)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above,I said that usually n_neighbors equal to 3,however actually not true totally.According to data,most reasonable n_neighbors numbers can change.Here,I will explain and show how we can find most reasonable n_neighbors numbers to our algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ny_pred=knn.predict(x_test)\ny_true=y_test\ncm1=confusion_matrix(y_pred,y_true)\n\n\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cm1,annot=True,color=\"red\",fmt=\"0.5f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Table of erros about predict\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbours = np.arange(1,25)\ntrainAccuracy=[]\ntestAccuracy=[]\n\nfor i,k in enumerate(neighbours):\n    # k between 1-25\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # train(fit) data\n    knn.fit(xTrain,yTrain)\n    # append accuracy values to relevant places\n    trainAccuracy.append(knn.score(xTrain,yTrain))\n    testAccuracy.append(knn.score(xTest,yTest))\n\nplt.figure(figsize=(15,8))\nplt.plot(neighbours, testAccuracy, label = 'Testing Accuracy')\nplt.plot(neighbours, trainAccuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title(\"Values and Accuracy\")\nplt.xticks(neighbours)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see,here most reasonable numbers is 18","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"23\"></a>\n## Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* This section includes logistic and linear algorithms ,also I will mention desicion tree and random forest algorithms\n* I will use sacral_slope and pelvic_incidence inside of columns,I will think pelvic_incidence and target is sacral_slope of abnormal\n* I will use scatter plot to more understand and predict future\n* Data shape must be (x,1) if it not so you can use reshape(-1,1) to convert reasonable shape","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I created data1 list inside of abnormal patients.Later,I occured x and y.Here important point that convert to reasonable struct for methods.(reshape(-1,1)).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data1=data[data[\"class\"]==\"Abnormal\"]\nx=data1.loc[:,\"pelvic_incidence\"].values.reshape(-1,1)\ny=np.array(data1.loc[:,\"sacral_slope\"]).reshape(-1,1)\n# I showed two way to find x and y,you can use what you want\nplt.figure(figsize=(15,8))\nplt.scatter(x=x,y=y)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.title(\"This scatter belong to abnormal patients\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"24\"></a>\n### Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* In this code,I will use fit method to train data.\n* I will create yHead ,yHead represented predicts of x values in plot.This is helpful way to predict future for patients statue.\n* R^2 score means that minimum error squre,namely show how diverges or convergence to target and take square.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinearReg=LinearRegression()\nlinearReg.fit(x,y)\n\nyHead = linearReg.predict(x)\n\nplt.figure(figsize=(15,8))\nplt.scatter(x=x,y=y)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.title(\"This scatter belong to abnormal patient\")\nplt.plot(x,yHead,color=\"red\")\nplt.show()\nprint(\"R^2 score is {}\".format(linearReg.score(x,y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"25\"></a>\n### Polynomal Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As you know linear regression has formula that (b0+b1*x),and multiple lineer regression has formula that (b0+b1*x1+b2*x2+++bn*xn) too.However polynomal regression has formula that (b0+b1*x1+b2*x2+++bn*xn).Usually lineer reg. and multiple lineer regression's plots more linear than polynomal reg.But polynomal lineer reg has much slope so curve is more than others algorithms.It demonstrates that polynomal reg. add reality to plot and data predicting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nplt.figure(figsize=(15,8))\nplt.scatter(x,y)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\n\n# prediction section\nlr=LinearRegression()\nlr.fit(x,y)\n# visualize\nyHead = lr.predict(x)\nplt.plot(x,yHead,color=\"green\",label=\"linear\")\nplt.legend()\nplt.show()\n# visualize 2\n\npl = PolynomialFeatures(degree=2)\nxpl=pl.fit_transform(x)  # use as both implement fith(train) data and save as variable\nlr2 = LinearRegression()\nlr2.fit(xpl,y)\n#  \nplt.figure(figsize=(15,8))\nyHead2 = lr2.predict(xpl)\nplt.scatter(x,y)\nplt.plot(x,yHead2,color=\"red\",label=\"polynomal\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"26\"></a>\n### Desicion Tree","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thinks of desicion tree in basicly include logic of split.Namely,with conditions, occur desicion tree to direct to result.Also splits detect according to information entropy.For instance,suppose that an graphic include x and y,conditions split to fields maybe between x1 = 3 , x2=17 then features(x1,x2) detect y.Actually,this framework advance yes and no.Purpose that make zero(0) to mean squre error.How linear regression have some equation,Desicion Tree have desicion tree framework directly how yes-no too","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ntreeReg = DecisionTreeRegressor()\ntreeReg.fit(x,y)\nyHead = treeReg.predict(x)\n# visualize\nplt.figure(figsize=(15,8))\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x,yHead,color=\"red\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.title(\"This scatter belong to abnormal patient\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we will keep wide to space of x(x_) , we are going to get effectively results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_=np.arange(min(x),max(x),0.01).reshape(-1,1)\nyHead=treeReg.predict(x_)\n\nplt.figure(figsize=(15,8))\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x_,yHead,color=\"red\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.title(\"This scatter belong to abnormal patient\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"27\"></a>\n### Random Forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This algorithm members of ensemble learning.This ensemeble's algorithm include lots of algorithm inside of it's.So this algortihm robust and common.Assume that we have data,according to random forest,firstly we will select samples counts of n and this samples will occur sub data.Then sub data will divide inside it's as tree1,tree2,tree3,....treen,and each tree create it's value to send joint field.In this field(area) ,will take average of whole values and this is result of algorithm","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Random forest uses some branches which,reccommendation systems(movie,body part classification),stock price precition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Methods of RandomForestRegressor take 2 elements :\n* n_estimators = counts of tree,if this elements how much more,good for algorithm.\n* random_state = maybe select types of random elements.I said above,data divide to sub data how random operation.This elements shape of selecting randomly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf=RandomForestRegressor(n_estimators=100,random_state=42)\nrf.fit(x,y)\nx_=np.arange(min(x),max(x),0.01).reshape(-1,1)\nyHead=rf.predict(x_)\n# visualize\nplt.figure(figsize=(15,8))\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x_,yHead,color=\"green\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.title(\"This scatter belong to abnormal patient\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"28\"></a>\n### Logistic Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* I will explain logistic regression exhaustively.You can think that it works on binary classification.(0 and 1 outputs).\n* Also,logistic regression is basic of Deep learning and Machine learning.In other words,it is simple neural network\n* In order to more understand ,I mention computation graph because logistic regression expression with computation graph.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"29\"></a>\n#### Computation Graph","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* It is simple way to think about mathematical expression\n* It's like a mathematical visualize\n* For instance\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://www.deepideas.net/wp-content/uploads/2017/08/addition.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will start to expression logistic regression with it's steps","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* You assume that dataFrame,and this dataframe include whole values except class of data(binary(normal,abnormal)).\n* Also you have parameters as weight and bias.Wieight is coefficent of each values,bias is intercept.\n* And you must multiply weight with values and plus with bias.At the end of the process you have z .\n* And you must implement sigmoid function to z why occur probabilities pattern of reasonable.\n* End of the process too,you will have yHead,and you will compare to 0.5.\n* If value that get this loop,bigger than 0.5 you can say it is 1,else it is 0.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before to start write code and expression , I must some changes on data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"30\"></a>\n### Changes Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"class\"]=[1 if i==\"Normal\" else 0 for i in data[\"class\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In colums of class in data,1 represeneted by normal person,0 is abnormal person","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data[\"class\"].values\n#x_data=data.loc[:,data.columns!=\"class\"].values\nx_data=data.drop([\"class\"],axis=1)\n#x_data\nx= (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\nx  # normalze","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\nprint(\"\\nLater...\\n\")\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"31\"></a>\n#### Initializing parameters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We must associative some values to start logistic regression loops.And usually associative 0.01 to weight,and 0.0 bias.\n* we dont associative 0.0 to wieght at the begin because,then dont carried out learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"32\"></a>\n#### Forward Propagation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The all steps from values to cost is called forward propagation.Cost function is summotion of losts.(We will see as soon)\n* z=(w.T,x)+b => we know that w is weight,x is values of data,b is bias and .T is transpose.\n* Then I put z in sigmoid function to normalize data.\n* Then I will calculate loss function according to decision.\n* Cost function is summation of all loss(error)\n* Let's write sigmoid function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigmoid(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I writed,I occured yhead how calculate with it's equation.Now,I will explain loss function.You know that weight and bias assemble with certain process and occur z.Then implement sigmoid methods to z ,We will occur yHead.If yHead became 0.7,it's bigger than 0.5 so our prediction probably is 1 or yHead became 0.3,then we will predict probably 0.This event loss function is 0 because we will predict truely,however we will predict wrongly,then loss function is going to come big.Below,take place mathematical expression of loss function","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://image.ibb.co/eC0JCK/duzeltme.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's implement Forward Propagation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z) # probabilistic 0-1\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    return cost ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"33\"></a>\n#### Optimization Algorithm with Gradually Descent","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Up to now,we know that cost and error.\n* Therefore,we need to decrease cost function.If our prediction is high,likely our prediction is wrong.\n* Everyting starts with initialize values,maybe cost comes high at the begin why random associative.But we can change how update weight and bias values.\n* In other words, our model needs to learn the parameters weights and bias that minimize cost function.\n* Last,descent process implement by special derivative equation with minus from previously value.\n* It is equation that update weight and bias.\n![](https://image.ibb.co/hYTTJH/8.jpg)\n* Its called backward propagation process that updates to optimizate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* derivative_weight=1/m(x*(yHead-y)^T\n* derivative_bias = 1/m(sum(yHead-y)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Up to now,we learned that;\n* associative inital value\n* make loop logistic regression between values to cost function (forward propagation)\n* and start backwards process to decrease cost function  (backward propagation)\n\nLater steps likely update function that weight and bias.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"34\"></a>\n### Update","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Let's update parameters(w,b)\n* In update function,I will call forward_backward_propagation in for loop to create cost and gradients.\n* And I will add each cost to list.\n* Later,I will update parameters(weight,bias) thanks to derivative.\n* I will save each 10 indexes to convert plot.\n* I will create dictionary of parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"35\"></a>\n### Prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now,we come section of prediction.While prediction,using x_test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction=np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            y_prediction[0,i]=0\n        else:\n            y_prediction[0,i]=1\n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I make prediction,and I will put everything here.\n* Firstly I will detect dimension according to shape of x_train\n* I will occur weight and bias thanks to initialize_weights_and_bias method\n* I will call update method to became easily.(direct solving).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    dimension=x_train.shape[0]\n    w,b=initialize_weights_and_bias(dimension)\n    parameters,gradients,cost_list=update(w,b,x_train,y_train,learning_rate,num_iterations)\n    y_prediction_train=predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    print(\"train accuracy : {}\".format(100-np.mean(np.abs(y_prediction_train-y_train))*100))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=0.01,num_iterations=150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"36\"></a>\n### Logistic Regression by sklearn","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* You can implement logistic regression by sklearn easily.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=42,max_iter=150)\n\nprint(\"Train accuracy = {}\".format(logreg.fit(x_train.T,y_train.T).score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy value is different from our result why sklearn use spesific way","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"37\"></a>\n## Support Vector Machine(SVM) Algorithm\n* Algorithm purpose that expand margin between different binaries.(best desicion boundry)\n* Algorith can draw lines to seperate as boundry(maximum margin classifier)\n* Support vector means,lines when margin become maximum.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data[\"class\"].values\nx_data=data.drop([\"class\"],axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nx=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We must be converted old pattern to data[\"class\"]\n* Otherwise doesnt work svm algorithm.\n* Algorithm doesnt want to numbers layout","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\nprint(\"Output which accuracy of svm algorithm = {}\".format(svm.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"38\"></a>\n## Naive Bayes Classification\n\n* This algorithm depended by complicated probabilites and process.\n* This algorithm use way which computate some probabilities to evulate and reach results.\n* This algorithm has spesific equation itselfs.\n* We mention Naive Bayes Classification works,this algorithm has several approach to data.This is reasonable for us.For instance,you need to know variety of cancer aint it.And algorithm supply this consider already so it calculates almost probabilites","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"A=data[data[\"class\"]==\"Abnormal\"]\nN=data[data[\"class\"]==\"Normal\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(A[\"pelvic_tilt numeric\"],A[\"sacral_slope\"],color=\"red\",label=\"Abnormal\")\nplt.scatter(N[\"pelvic_tilt numeric\"],N[\"sacral_slope\"],color=\"green\",label=\"Normal\")\nplt.xlabel(\"pelvic_tilt\")\nplt.ylabel(\"sacral_slope\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"print accuracy of naive bayes algorithm: \",nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"39\"></a>\n## Unsupervised Learning\n\n- It uses data like supervised learning learning but different them is that data has not any label.In other words you dont know what which one normal or abnormal\n\n- In order to work on unsupervised learning , I will use 2 columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"40\"></a>\n## KMEANS\n\n- I will implement first member of unsupervised learning that kmeans\n- The basic idea of kmeans depend b clustering , that algotihms works iteratively and assign each data point that are provided.\n- KMeans(n_clusters = 2): n_clusters = 2 means that create 2 cluster","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()\nplt.scatter(data[\"pelvic_radius\"],data[\"degree_spondylolisthesis\"])\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"degree_spondylolisthesis\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data.loc[:,[\"pelvic_radius\",\"degree_spondylolisthesis\"]]\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2) # it will occur 2 clusters\nkmeans.fit(data2)\nlabels = kmeans.predict(data2)\nlabels # normal or abnormal\nplt.scatter(data.pelvic_radius,data.degree_spondylolisthesis,c=labels)\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"degree_spondylolisthesis\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"41\"></a>\n## EVALUATE OF CLUSTERING\n- I will evualate algorithms predicts by cross tabulation table.We will see how much true it's clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({ \"labels\" : labels , \"class\" : data[\"class\"]})\n\nct = pd.crosstab(df[\"labels\"],df[\"class\"])\nprint(ct)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now, you may have questions in your mind that how we find most suitable n_clusters value ? \n\n- This answers is depends like Knn algorithms.It same as hyper parameter tuning.We will plot cluster values , and choose elbow shape","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clusterList = np.empty(10)\n\nfor i in range(1,10):\n    kmeans=KMeans(n_clusters=i)\n    kmeans.fit(data2)\n    clusterList[i] = kmeans.inertia_\n\nplt.plot(range(0,10),clusterList)    \nplt.xlabel('Number of cluster')\nplt.ylabel('Distances')\nplt.show()\n# it seems that 3 is most suitable value for kmeans's cluster value.Reason why elbow shape start to occur in 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"42\"></a>\n## STANDARDIZATION\n- Standardizaton is important for both supervised and unsupervised learning\n- We can use pipeline like supervised learning.\n\n    - A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data3 = data.drop('class',axis = 1)\ndata3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nscaler = StandardScaler()\nkmeans = KMeans(n_clusters=2)\npipe = make_pipeline(scaler,kmeans)\npipe.fit(data3)\nlabels = pipe.predict(data3)\n\ndf = pd.DataFrame({'labels':labels,\"class\":data['class']})\n\nct = pd.crosstab(df['labels'],df['class'])\n\nprint(ct)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"43\"></a>\n##  HIERARCHY\n\n* It is different methods of clustering.\n* It expressions by vertical lines.\n* Methods is single closest points of clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage,dendrogram\n\nmerg = linkage(data3.iloc[100:120,:],method=\"single\")\ndendrogram(merg,leaf_rotation=90,leaf_font_size=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"44\"></a>\n## T - Distributed Stochastic Neighbor Embedding (T - SNE)\n\n- It is one of the dimention reduction technique\n- It works according to distances between data points and cluster them.\n- In below  term called \"fit_transform\" means that both predict and assign variable with transfered\n- Also , I will use different colors to more understandable clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ncolor_list = [\"red\" if i == \"Abnormal\" else \"green\" for i in data.loc[:,\"class\"]]\n\nmodel = TSNE(learning_rate=100)\ntransform = model.fit_transform(data2)\ntransform\n\nx= transform[:,0]\ny= transform[:,1]\n\nplt.scatter(x,y,c=color_list)\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"degree_spondylolisthesis\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"45\"></a>\n## PRINCIPLE COMPONENT ANALYSIS (PCA)\n\n- This is one of the dimension reduction technique too.\n- You assume that you have data and it has 4 different types inside.If you want to explain relationship between data points you must use 4 dimension graphics or scatter matrix which members 100 plots.\n- They are silly.\n- In here , you should use PCA.\n- PCA , find relationship between whole elemets of data and convert them plots to expression 2 dimesions graphics\n- And create clusters which are high correlated each other.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nmodel = PCA()\nmodel.fit(data3)\ntransform = model.transform(data3)\ntransform\nprint('Principle components: ',model.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(range(model.n_components_),model.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In above , you see relationship n_components and variance(number of PCA feature with significant variance).At the same time it equals algorithm's dimensions.\n\n* In order to choose intrinsic dimension try all of them and find best accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\npca.fit(data3)\n\ntransform = pca.transform(data3)\n#transform\n\nx= transform[:,0]\ny= transform[:,1]\n\n\nplt.scatter(x,y,c=color_list)\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}