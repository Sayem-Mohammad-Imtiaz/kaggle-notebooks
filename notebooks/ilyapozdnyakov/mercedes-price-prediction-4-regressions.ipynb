{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mercedes price prediction","metadata":{}},{"cell_type":"markdown","source":"<img src =\"https://images.hgmsites.net/hug/mercedes-benz-historical-logos_100711609_h.jpg\" width=\"200\" height=\"200\">","metadata":{}},{"cell_type":"markdown","source":"**Dataset used:** https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes\n\n**Goal:** predict the car price depending on the features of the car (e.g. mileage, engine type, transmission etc.) - both categorical and continuous.\n\n**ML task type:** regression.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# machine learning\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:10.490544Z","iopub.execute_input":"2021-05-21T05:45:10.491161Z","iopub.status.idle":"2021-05-21T05:45:11.611261Z","shell.execute_reply.started":"2021-05-21T05:45:10.491052Z","shell.execute_reply":"2021-05-21T05:45:11.610255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/used-car-dataset-ford-and-mercedes/merc.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:11.613746Z","iopub.execute_input":"2021-05-21T05:45:11.614179Z","iopub.status.idle":"2021-05-21T05:45:11.659863Z","shell.execute_reply.started":"2021-05-21T05:45:11.614134Z","shell.execute_reply":"2021-05-21T05:45:11.658834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:11.660895Z","iopub.execute_input":"2021-05-21T05:45:11.661178Z","iopub.status.idle":"2021-05-21T05:45:11.778948Z","shell.execute_reply.started":"2021-05-21T05:45:11.661151Z","shell.execute_reply":"2021-05-21T05:45:11.777828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:11.78023Z","iopub.execute_input":"2021-05-21T05:45:11.780629Z","iopub.status.idle":"2021-05-21T05:45:21.501788Z","shell.execute_reply.started":"2021-05-21T05:45:11.780587Z","shell.execute_reply":"2021-05-21T05:45:21.500887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's check the price distribution among the models represented. Hypothesis: price is very correlated with the model.**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10,5))\nsns.countplot(y = 'model', data = df, order = df['model'].value_counts().index)\nplt.ylabel('Car model')\nplt.title('Model distribution')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:21.505216Z","iopub.execute_input":"2021-05-21T05:45:21.505527Z","iopub.status.idle":"2021-05-21T05:45:21.858906Z","shell.execute_reply.started":"2021-05-21T05:45:21.505488Z","shell.execute_reply":"2021-05-21T05:45:21.8581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**One-hot encoding of categorical features:**\n\n**We do this to avoid the misinterpretation of feature correlations by the ML algorithm.**","metadata":{}},{"cell_type":"code","source":"ohe = pd.get_dummies(df)\nohe.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:21.861396Z","iopub.execute_input":"2021-05-21T05:45:21.861686Z","iopub.status.idle":"2021-05-21T05:45:21.903735Z","shell.execute_reply.started":"2021-05-21T05:45:21.861657Z","shell.execute_reply":"2021-05-21T05:45:21.902973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = ohe.drop(['price'], axis=1)\ny = ohe['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:21.904749Z","iopub.execute_input":"2021-05-21T05:45:21.905136Z","iopub.status.idle":"2021-05-21T05:45:21.91679Z","shell.execute_reply.started":"2021-05-21T05:45:21.905108Z","shell.execute_reply":"2021-05-21T05:45:21.915921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([('scaler', StandardScaler()), ('LinReg', LinearRegression())])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:21.918219Z","iopub.execute_input":"2021-05-21T05:45:21.918536Z","iopub.status.idle":"2021-05-21T05:45:21.924685Z","shell.execute_reply.started":"2021-05-21T05:45:21.918505Z","shell.execute_reply":"2021-05-21T05:45:21.923613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:21.926051Z","iopub.execute_input":"2021-05-21T05:45:21.926382Z","iopub.status.idle":"2021-05-21T05:45:21.998236Z","shell.execute_reply.started":"2021-05-21T05:45:21.926353Z","shell.execute_reply":"2021-05-21T05:45:21.997191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.score(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:21.99977Z","iopub.execute_input":"2021-05-21T05:45:22.000396Z","iopub.status.idle":"2021-05-21T05:45:22.016343Z","shell.execute_reply.started":"2021-05-21T05:45:22.000349Z","shell.execute_reply":"2021-05-21T05:45:22.015261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:22.017889Z","iopub.execute_input":"2021-05-21T05:45:22.018663Z","iopub.status.idle":"2021-05-21T05:45:22.032516Z","shell.execute_reply.started":"2021-05-21T05:45:22.018616Z","shell.execute_reply":"2021-05-21T05:45:22.030966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_lin = r2_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:22.039037Z","iopub.execute_input":"2021-05-21T05:45:22.044364Z","iopub.status.idle":"2021-05-21T05:45:22.053691Z","shell.execute_reply.started":"2021-05-21T05:45:22.044293Z","shell.execute_reply":"2021-05-21T05:45:22.052377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's plot learning curves (depending on the number of samples in the set). The RMSE should tend to converge closer to the max limit of samples.**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef plot_learning_curves (model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_pred = model.predict(X_train[:m])\n        y_val_pred = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train_pred, y_train[:m]))\n        val_errors.append(mean_squared_error(y_val_pred, y_val))\n        plt.plot(np.sqrt(train_errors), 'r--', linewidth=2, label='train')\n        plt.plot(np.sqrt(val_errors), 'b--', linewidth=2, label='val')\n        plt.ylabel('RMSE')\n        plt.xlabel('Number of samples')\n\nlinReg = LinearRegression()\nplot_learning_curves(linReg, X[:200], y[:200])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:22.060649Z","iopub.execute_input":"2021-05-21T05:45:22.063475Z","iopub.status.idle":"2021-05-21T05:45:23.848677Z","shell.execute_reply.started":"2021-05-21T05:45:22.063413Z","shell.execute_reply":"2021-05-21T05:45:23.847989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So, learning curves tell us that the RMSE stabilizes as long as the number of samples grow in volume.**","metadata":{}},{"cell_type":"markdown","source":"**Let's try a polynomial regression (maybe there are some hidden non-linear correlation between the features).**\n\n**+ CrossValScore + feature importances**","metadata":{}},{"cell_type":"markdown","source":"*This part of code is commented cause I ran out of memory in the kernel :(*","metadata":{}},{"cell_type":"code","source":"#from sklearn.preprocessing import PolynomialFeatures\n#poly_features = PolynomialFeatures(degree=2)\n#X_poly_train = poly_features.fit_transform(X_train)\n#X_poly_test = poly_features.fit_transform(X_test)\n\n#pipe_poly = Pipeline([('poly', poly_features), ('linreg', LinearRegression())])\n#pipe_poly.fit(X_poly_train, y_train)\n#y_pred_poly = pipe_poly.predict(X_poly_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:23.849836Z","iopub.execute_input":"2021-05-21T05:45:23.850316Z","iopub.status.idle":"2021-05-21T05:45:23.85421Z","shell.execute_reply.started":"2021-05-21T05:45:23.850285Z","shell.execute_reply":"2021-05-21T05:45:23.852894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pipe_poly.score(X_poly_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:23.855399Z","iopub.execute_input":"2021-05-21T05:45:23.855687Z","iopub.status.idle":"2021-05-21T05:45:23.865592Z","shell.execute_reply.started":"2021-05-21T05:45:23.855659Z","shell.execute_reply":"2021-05-21T05:45:23.864727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#r2_poly = pipe_poly.score(X_poly_test, y_pred_poly)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:23.867205Z","iopub.execute_input":"2021-05-21T05:45:23.8675Z","iopub.status.idle":"2021-05-21T05:45:23.8768Z","shell.execute_reply.started":"2021-05-21T05:45:23.867467Z","shell.execute_reply":"2021-05-21T05:45:23.875778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#r2_score(y_test, y_pred_poly)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:23.877983Z","iopub.execute_input":"2021-05-21T05:45:23.878289Z","iopub.status.idle":"2021-05-21T05:45:23.887575Z","shell.execute_reply.started":"2021-05-21T05:45:23.878261Z","shell.execute_reply":"2021-05-21T05:45:23.886871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.model_selection import cross_val_score\n\n\n#print(cross_val_score(pipe_poly, X, y, cv=3))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:23.888708Z","iopub.execute_input":"2021-05-21T05:45:23.889289Z","iopub.status.idle":"2021-05-21T05:45:23.898291Z","shell.execute_reply.started":"2021-05-21T05:45:23.889247Z","shell.execute_reply":"2021-05-21T05:45:23.897364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Apparently, a more complex model with polynomial features is too much for this model. Let's try Ridge regression.**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\npipe_ridge = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge(alpha=100, solver='sag', max_iter=2000))])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:23.899605Z","iopub.execute_input":"2021-05-21T05:45:23.899896Z","iopub.status.idle":"2021-05-21T05:45:23.908435Z","shell.execute_reply.started":"2021-05-21T05:45:23.899869Z","shell.execute_reply":"2021-05-21T05:45:23.907603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_ridge.fit(X_train, y_train)\npipe_ridge.score(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:23.909974Z","iopub.execute_input":"2021-05-21T05:45:23.910491Z","iopub.status.idle":"2021-05-21T05:45:24.017995Z","shell.execute_reply.started":"2021-05-21T05:45:23.91045Z","shell.execute_reply":"2021-05-21T05:45:24.017074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_ridge = pipe_ridge.predict(X_test)\nr2_ridge = r2_score(y_test, y_pred_ridge)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:24.019382Z","iopub.execute_input":"2021-05-21T05:45:24.019821Z","iopub.status.idle":"2021-05-21T05:45:24.029437Z","shell.execute_reply.started":"2021-05-21T05:45:24.019772Z","shell.execute_reply":"2021-05-21T05:45:24.028353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nprint(cross_val_score(pipe_ridge, X, y, cv=3))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:24.030758Z","iopub.execute_input":"2021-05-21T05:45:24.031072Z","iopub.status.idle":"2021-05-21T05:45:24.330644Z","shell.execute_reply.started":"2021-05-21T05:45:24.031039Z","shell.execute_reply":"2021-05-21T05:45:24.329559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**And finally, Lasso regression.**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\npipe_lasso = Pipeline([('scaler', StandardScaler()), ('ridge', Lasso(alpha=0.1, max_iter=100000, warm_start=True))])\npipe_lasso.fit(X_train, y_train)\npipe_lasso.score(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:24.3323Z","iopub.execute_input":"2021-05-21T05:45:24.332607Z","iopub.status.idle":"2021-05-21T05:45:30.160307Z","shell.execute_reply.started":"2021-05-21T05:45:24.332576Z","shell.execute_reply":"2021-05-21T05:45:30.159203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_lasso = pipe_lasso.predict(X_test)\nr2_lasso = r2_score(y_test, y_pred_lasso)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:30.161799Z","iopub.execute_input":"2021-05-21T05:45:30.162386Z","iopub.status.idle":"2021-05-21T05:45:30.174325Z","shell.execute_reply.started":"2021-05-21T05:45:30.162341Z","shell.execute_reply":"2021-05-21T05:45:30.173147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\npipe_en = Pipeline([('scaler', StandardScaler()), \n                    ('ridge', ElasticNet(alpha=0.01, l1_ratio=0.5))])\npipe_en.fit(X_train, y_train)\npipe_en.score(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:30.175817Z","iopub.execute_input":"2021-05-21T05:45:30.176404Z","iopub.status.idle":"2021-05-21T05:45:30.740019Z","shell.execute_reply.started":"2021-05-21T05:45:30.176361Z","shell.execute_reply":"2021-05-21T05:45:30.738993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_en = pipe_en.predict(X_test)\nr2_en = r2_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:30.741468Z","iopub.execute_input":"2021-05-21T05:45:30.742057Z","iopub.status.idle":"2021-05-21T05:45:30.756658Z","shell.execute_reply.started":"2021-05-21T05:45:30.741998Z","shell.execute_reply":"2021-05-21T05:45:30.755566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_scores = sorted([r2_lin, r2_ridge, r2_lasso, r2_en])\nnames = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:30.757943Z","iopub.execute_input":"2021-05-21T05:45:30.758346Z","iopub.status.idle":"2021-05-21T05:45:30.772253Z","shell.execute_reply.started":"2021-05-21T05:45:30.758305Z","shell.execute_reply":"2021-05-21T05:45:30.771299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\ng = px.bar(x=names, y=r2_scores, log_y=True)\ng.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:45:30.77357Z","iopub.execute_input":"2021-05-21T05:45:30.774063Z","iopub.status.idle":"2021-05-21T05:45:32.756688Z","shell.execute_reply.started":"2021-05-21T05:45:30.774Z","shell.execute_reply":"2021-05-21T05:45:32.755471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So, even though the R2-score among these four models vary insignificantly, the best model to predict Mercedes-car prices is ElasticNet.**","metadata":{}},{"cell_type":"markdown","source":"**To sum up,**\n\nMercedes car price correlates with the features in the dataset. ElasticNet regression proves to perform best out of the ones presented in the kernel.","metadata":{}}]}