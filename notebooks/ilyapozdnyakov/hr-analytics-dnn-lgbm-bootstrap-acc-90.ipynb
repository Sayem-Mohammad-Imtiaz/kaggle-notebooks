{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HR Analytics notebook","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://www.digitalvidya.com/wp-content/uploads/2019/05/HR-Analytics.jpg\" width=500 height=200>","metadata":{}},{"cell_type":"markdown","source":"* **Task type:** classification\n* **Models used:** DNN, LGBM\n* **Other methods used:** shap","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\n\ntest = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.dropna()\ntest = test.dropna()\n\n# This is the simplest approach, however you can replace N/A values with mean/median, Nth percentile, to avoid data distortion.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA","metadata":{}},{"cell_type":"code","source":"# Here I will use a very powerful library which provides almost all the necessary EDA features out-of-the-box.\n\nimport pandas_profiling as pp\npp.ProfileReport(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So, the dataset appears to be quite small, which is quite good for the purpose of exercise, though lack of data can result in ending up with a poor performing model.**","metadata":{}},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Another thing, the target has a long-tail distribution which means that the dataset is quite imbalanced.\n80% of target is '0', while 20% is '1'. Therefore, we need to evaluate a model based not only on accuracy score, but also precision & recall (confusion matrix).**","metadata":{}},{"cell_type":"markdown","source":"# 3. Feature preparation","metadata":{}},{"cell_type":"markdown","source":"**Let's look at how the number of Data Scientists who change the job varies across features.**","metadata":{}},{"cell_type":"code","source":"print(pd.pivot_table(train, values='target',\n                    columns=['relevent_experience'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['education_level'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['enrolled_university'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['gender'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['major_discipline'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['company_type'], aggfunc=np.sum).T.sort_values('target', ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A lot of background-related differences in these features. We will need to encode them manually to improve the model.**\n\n**The number of people who change the job vary significantly and inconsistenly!**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# I do this manually to explicitly tell the model that a better education & experience serves well as a trustworthy input.\n\n# However, later we wil see the feature importanes report in SHAP and notice interesting results.\nexperience_dict = {'Has relevent experience' : 1,\n             'No relevent experience': 0}\n\neducation_dict = {'Graduate' : 2,\n             'Masters' : 1,\n             'Phd' : 0}\n\nenrollment_dict = {'no_enrollment' : 2,\n             'Full time course' : 1,\n             'Part time course' : 0}\n\ngender_dict = {'Male' : 2,\n             'Female' : 1,\n             'Other' : 0}\n\ndiscipline_dict = {'STEM' : 5,\n             'Humanities' : 4,\n             'Business Degree' : 3,\n             'Other' : 2,\n             'No Major' : 1,\n             'Arts' : 0 }\n\ncompany_dict = {'Pvt Ltd' : 5,\n             'Funded Startup' : 4,\n             'Public Sector' : 3,\n             'Early Stage Startup' : 2,\n             'NGO' : 1,\n             'Other' : 0 }\n\n\n# Train encoding\nle = LabelEncoder()\ntrain['gender'] = train['gender'].map(gender_dict)\ntrain['relevent_experience'] = train['relevent_experience'].map(experience_dict)\ntrain['education_level'] = train['education_level'].map(education_dict)\ntrain['enrolled_university'] = train['enrolled_university'].map(enrollment_dict)\ntrain['major_discipline'] = train['major_discipline'].map(discipline_dict)\ntrain['experience'] = le.fit_transform(train['experience'].astype(str))\ntrain['company_size'] = le.fit_transform(train['company_size'].astype(str))\ntrain['company_type'] = train['company_type'].map(company_dict)\ntrain['last_new_job'] = le.fit_transform(train['last_new_job'].astype(str))\n#train['city'] = le.fit_transform(train['city'].astype(str))\n\ntrain = pd.get_dummies(train, columns=['city']) # I do one-hot encoding here, since a higher value of the encoded feature is not related to the 'importance' of a feature.\n\n# Test encoding\ntest['gender'] = le.fit_transform(test['gender'].astype(str))\ntest['relevent_experience'] = test['relevent_experience'].map(experience_dict)\ntest['education_level'] = test['education_level'].map(education_dict)\ntest['enrolled_university'] = test['enrolled_university'].map(enrollment_dict)\ntest['major_discipline'] = test['major_discipline'].map(discipline_dict)\ntest['experience'] = le.fit_transform(test['experience'].astype(str))\ntest['company_size'] = le.fit_transform(test['company_size'].astype(str))\ntest['company_type'] = test['company_type'].map(company_dict)\ntest['last_new_job'] = le.fit_transform(test['last_new_job'].astype(str))\n#test['city'] = le.fit_transform(test['city'].astype(str))\n\n\ntest = pd.get_dummies(test, columns=['city'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = train.drop('enrollee_id', axis=1)\n#test = test.drop('enrollee_id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['city_development_index'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop('target', axis=1)\ny = train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n\n# Further in this notebook we will use 'val' for validation dataset, since we have all the corresponding data and columns unlike in the 'test' dataset.\n# Test dataset does not contain the target and thus we will not be able to measure the performance of the model.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_test = test\n#y_test = test['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['city_development_index'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model building\n## 4.1. Deep Neural Network","metadata":{}},{"cell_type":"markdown","source":"**We will use a very basic neural network here with 5 layers.**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, ThresholdedReLU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**First, let's create a normalization layer for input data.**","metadata":{}},{"cell_type":"code","source":"norm = tf.keras.layers.LayerNormalization(\n    epsilon=0.001,\n    center=True,\n    scale=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here you can add whatever metrics you are interested in.**","metadata":{}},{"cell_type":"code","source":"metrics = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.AUC(name='auc'),\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(norm)\nmodel.add(ThresholdedReLU(theta=10)) # Theta is a threshold which determines the output result of a particular neuron.\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(100, activation='softmax'))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n#model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=10) # A higher N of epochs doesn't improve the performance since the dataset is small.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Not the stellar performance, but anyway. Let's try Gradient Boosting.**","metadata":{}},{"cell_type":"markdown","source":"**To interpret the results of the Neural network, you can use:**\n\n1) feature permutation;\n\n2) SHAP library (see further);\n\n3) LIME library.","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Gradient Boosting","metadata":{}},{"cell_type":"code","source":"#conda install -c conda-forge lightgbm \nfrom lightgbm import LGBMClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMClassifier(objective='binary', num_leaves=10, learning_rate=0.05, \n                      max_depth=1, n_estimators=50, boosting_type='goss') # You can play with hyperparameters, pay special attention to num_leaves, max_depth and n_estimators.\nlgbm.fit(X_train, y_train)\ny_pred = lgbm.predict(X_val)\n\n#cross_val_score(lgbm, X_test, y_test, cv=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(lgbm.feature_importances_, X_train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix, recall_score, precision_score, accuracy_score\n\n#confusion_matrix(y_test, y_pred)\nprint('Accuracy: %f, \\nRecall: %f \\nPrecision: %f'\n      % (accuracy_score(y_val, y_pred), recall_score(y_val, y_pred), precision_score(y_val, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Conclusion","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Feature importances","metadata":{}},{"cell_type":"code","source":"import shap\n\nX_importance = X_train\n\nexplainer = shap.TreeExplainer(lgbm)\nshap_values = explainer.shap_values(X_importance)\nshap.summary_plot(shap_values, X_importance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Further steps","metadata":{}},{"cell_type":"markdown","source":"**Apparently, the most important feature in the task is city development index. Which is not quite good, because it predominates over other features.**","metadata":{}},{"cell_type":"markdown","source":"**As the performance of both DNN and LGBM model is not perfect, the further steps to complete might be as follows:**\n\n1. Bootstrapping the dataset to make it more balanced. (see **Step 7**)\n\n2. Feature insertion & feature engineering based on the most important features.\n\n3. Play with neural networks & try to use recurrent networks. Or add different layers.\n\n4. Use other conventional ML models and/or Boosting (e.g. CAT boost).\n\n","metadata":{}},{"cell_type":"markdown","source":"# 7. Bootstrapping","metadata":{}},{"cell_type":"code","source":"train[train['target'] == 0]['city_development_index'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['target'] == 1]['city_development_index'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cdi = pd.DataFrame(train['city_development_index'].value_counts())\ncdi.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We need to get samples of the DataFrame with target=1 and pick several slices of the dataset with underrepresented 'city_development_index' features (with indices between 0.4 and 0.7).**\n\n**Why? Because cities with lower indies have more Data Scientists who change the job. And they are poorly represented in the original dataset.**","metadata":{}},{"cell_type":"code","source":"def change(x):\n    x = np.random.randint(400, 800)/1000\n    return x\n\ndef change2(x):\n    x = np.random.randint(0, 21)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Slice 1\n\ninsert1 = train[train['city_development_index'] == 0.897].sample(frac=1)\ninsert1['experience'] = insert1['experience'].apply(lambda x: change2(x))\n#insert1['city_development_index'] = insert1['city_development_index'].apply(lambda x: change(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Slice 2\n\ninsert2 = train[train['city_development_index'] == 0.926].sample(frac=1)\ninsert2['experience'] = insert2['experience'].apply(lambda x: change2(x))\n#insert2['city_development_index'] = insert2['city_development_index'].apply(lambda x: change(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs = [train, insert1, insert2]\ntrain_new = pd.concat(dfs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_new = train_new['target']\nX_train_new = train_new.drop(['target'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm2 = LGBMClassifier()\nlgbm2.fit(X_train_new, y_train_new)\ny_pred2 = lgbm2.predict(X_val)\n\nprint('Accuracy: %f, \\nRecall: %f \\nPrecision: %f'\n      % (accuracy_score(y_val, y_pred2), recall_score(y_val, y_pred2), precision_score(y_val, y_pred2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we observe a spike in both accuracy and precision. Thus, bootstrapping has proven its efficiencty in this particular dataset. Even though we have used only 2 samples, we can take it further and improve the model performance.**","metadata":{}}]}