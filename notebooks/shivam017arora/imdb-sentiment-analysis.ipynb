{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hi!\n\nThis notebook will give a beginner guide to NLP and perform a Sentiment Analysis on the IMDB movie reviews dataset using a reccurent neural network. \n\n# What is NLP?\n\nThere are different levels of tasks in NLP, from speech processing to semantic interpretation and discourse processing. The goal of NLP is to be able to design algorithms to allow computers to \"understand\" natural language in order to perform some task.\n\n# What is Sentiment Analysis?\n\nSentiment Classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they are talking about. It is one of the most important building blocks in NLP and used in many applications. ","metadata":{}},{"cell_type":"markdown","source":"Importing Dependencies","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom keras.initializers import Constant\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing Data","metadata":{}},{"cell_type":"code","source":"imdb = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\nimdb.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of target variable","metadata":{}},{"cell_type":"code","source":"imdb.sentiment.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How to represent words?\n\nThe first and arguably most important common denominator across all NLP tasks is how we represent words as input to any of our models. Much of the earlier NLP work that we will not cover treats words as atomic symbols. To perform well on most NLP tasks we first need\nto have some notion of similarity and difference between words.\n\nOne common solution is using WordNet. This is something which uses NLTK which is the swiss army knife for NLP meaning it is not terribly good for anything but has a lot of basic functions. WordNet makes very fine distinctions of a word like a thesaurus containing lists of synonym sets and hypernyms ('is a' relationship). But there are a few problems such as it is built with human labour, can't compute accurate word similarities and is subjective. \n\nWell, how about one-hot-vectors? There are few things which are bad here such as language has a lot of words and there is no notion of similarity. \n\nhow about representing a word using its context?\n\nWhen a word is used in a text, its context is the set of words that appear nearby, right?\nWhen you get a sense of the idea of 'oh no that's the wrong word to use there' you understand the meaning of the word right? This is the idea of Distributional Semantics to understand the meaning of a word.\n\nSo that leads us to representing words in a better way using Word Embeddings. \n\n# What are Word Embeddings?\n\nWe will build a dense vector for each word, chosen so that it's similar to vectors of words that appear in the same context.\n\nThere are 2 types of algorithms used for word embedding:\n\nThe first set are count-based and rely on matrix factorization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indicating a suboptimal vector space structure. The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW models), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.\n\nhow about we combine them but how do we do that?\n\nwe could use ratios of co-occurrence probabilities to encode meaning components which is called \n# GLOVE Embedding. \n\nThe training objective of Glove is to learn word vectors such that their dot product equals the logarithm of the words probability of co-occurence. ","metadata":{}},{"cell_type":"markdown","source":"Factoring sentences into words","metadata":{}},{"cell_type":"code","source":"corpus = []\nfor text in imdb['review']:\n    words = [word.lower() for word in word_tokenize(text)] \n    corpus.append(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(corpus)\nprint(num_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting data to train(80%) and test(20%)","metadata":{}},{"cell_type":"code","source":"train_size = int(imdb.shape[0] * 0.8)\nX_train = imdb.review[:train_size]\ny_train = imdb.sentiment[:train_size]\n\nX_test = imdb.review[train_size:]\ny_test = imdb.sentiment[train_size:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenizing the words and padding for equal input dimensions","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=128, truncating='post', padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[0], len(X_train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=128, truncating='post', padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[0], len(X_test[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\nprint(\"Number of unique words: {}\".format(len(word_index)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Glove Embedding","metadata":{}},{"cell_type":"markdown","source":"Task1: Make dictionary of all words in corpus in pre-trained glove embeddings","metadata":{}},{"cell_type":"code","source":"embedding = {}\nwith open(\"/kaggle/input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt\") as file:\n    for line in file:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding[word] = vectors\nfile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Task2: Make matrix of all words in imdb-dataset with vectors from embedding dictionary","metadata":{}},{"cell_type":"code","source":"embedding_matrix = np.zeros((num_words, 100))\nfor i, word in tokenizer.index_word.items():\n    if i < (num_words+1):\n        vector = embedding.get(word)\n        if vector is not None:\n            embedding_matrix[i] = vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n\nText is a sequence of words, and sequential data has some problems when it comes to modelling:\n\n##### Problem #1: Can't model long-term dependencies\n\nFor eg, consider this sentence \"France is where I grew up, I can speak really good _____\" and in order to predict the blank word we need information from the distant past to accurately predict the correct word. \n\n##### Problem #2: Counts don't preserve order\n\nFor eg, consider these sentences \"The food is bad, not good at all\" and \"The food is good, not bad at all\" hence this means that order is neccessary to not lose sequential information. \n\n##### Problem #3: Parameters don't share information\n\nFor eg, consider these sentences \"I took the cat out this morning\" and \"This morning, I took the cat out\" when encoded using a count algorithm would mean that things we learn about the sequence won't transfer if they appear elsewhere in the sequence. \n\n##### Problem #4: Variable-length input\n\nHow to approach this?\n\n### Recurrent Neural Networks","metadata":{}},{"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Creating a Base Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(input_dim=num_words, output_dim=100, \n                    embeddings_initializer=Constant(embedding_matrix), \n                    input_length=128, trainable=False))\nmodel.add(LSTM(100, dropout=0.1))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=5, batch_size=2048, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['loss'], 'b', label='Training Loss', color='red')\nplt.plot(epochs, history.history['val_loss'], 'b', label='Validation Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy', color='red')\nplt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Deeper n Deeper","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim=num_words, output_dim=100, \n                    embeddings_initializer=Constant(embedding_matrix), \n                    input_length=128, trainable=False))\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=10, batch_size=1024, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['loss'], 'b', label='Training Loss', color='red')\nplt.plot(epochs, history.history['val_loss'], 'b', label='Validation Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy', color='red')\nplt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_sentence = ['This movie was not good at all. It had some good parts like the acting was pretty good but the story was not impressing at all.']\nvalidation_sentence_tokened = tokenizer.texts_to_sequences(validation_sentence)\nvalidation_sentence_padded = pad_sequences(validation_sentence_tokened, maxlen=128, \n                                    truncating='post', padding='post')\nprint(validation_sentence[0])\nprint(\"Probability of Positive: {}\".format(model.predict(validation_sentence_padded)[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though the sentence had words like 'good' and 'impressing' in it but the overall review was negative and this model predicted correctly (almost perfectly) with only 1.9% chance of being positive. ","metadata":{}},{"cell_type":"code","source":"validation_sentence = ['It had some bad parts like the storyline although the actors performed really well and that is why overall I enjoyed it.']\nvalidation_sentence_tokened = tokenizer.texts_to_sequences(validation_sentence)\nvalidation_sentence_padded = pad_sequences(validation_sentence_tokened, maxlen=128, \n                                    truncating='post', padding='post')\nprint(validation_sentence[0])\nprint(\"Probability of Positive: {}\".format(model.predict(validation_sentence_padded)[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a neutral review and this model predicted correctly with only 50.5% chance of being positive meaning it was neutral. ","metadata":{}},{"cell_type":"code","source":"validation_sentence = ['I can watch this movie forever just because of the beauty in its cinematography.']\nvalidation_sentence_tokened = tokenizer.texts_to_sequences(validation_sentence)\nvalidation_sentence_padded = pad_sequences(validation_sentence_tokened, maxlen=128, \n                                    truncating='post', padding='post')\nprint(validation_sentence[0])\nprint(\"Probability of Positive: {}\".format(model.predict(validation_sentence_padded)[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a positive review with utmost love for the movie and this model predicted correctly with 90.9% chance of being positive. ","metadata":{}},{"cell_type":"markdown","source":"Scope of Improvements: \n\nAs you can see, the model is not generalised and starts overfitting after 9 iterations. Here is an example: ","metadata":{}},{"cell_type":"code","source":"validation_sentence = ['What can I say? It was so astonishing that I dont have any words for it.']\nvalidation_sentence_tokened = tokenizer.texts_to_sequences(validation_sentence)\nvalidation_sentence_padded = pad_sequences(validation_sentence_tokened, maxlen=128, \n                                    truncating='post', padding='post')\nprint(validation_sentence[0])\nprint(\"Probability of Positive: {}\".format(model.predict(validation_sentence_padded)[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nMIT: Introduction to Deep Learning: 6.S191\n\nStanford: Natural Language Processing with Deep Learning: CS224N\n\n##### I highly recommend going through these amazing courses available for free.","metadata":{}},{"cell_type":"markdown","source":"##### Please upvote if you like my work and comment your feedback!","metadata":{}}]}