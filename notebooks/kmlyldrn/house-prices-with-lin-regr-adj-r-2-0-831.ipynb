{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PRESENTATION PLAN\n\n### 1. INTRODUCTION\n\nThis dataset contains house sale prices for King County, which includes Seattle. It includes houses sold between May 2014 and May 2015. There are 21613 observations with 19 features, price and id columns.\n\n### 2. ANALYZE PREPARATION\n\nWe are going to make some changes and clean the data.\n\n### 3. DATA EXPLARATORY\n\nWe are going to dive into data set to understand relations among features add new features to the data set.\n\n>####\t3.1. OUTLIERS\n>####\t3.2. DATA CLEANING AND TRANSFORMATION\n>####\t3.3. DISTRIBUTIONS\n>####\t3.4. MAPPING HOUSES\n>####\t3.5. CORRELATIONS\n\n### 4. MODELING\n\nWe are going to set some models and test them whether they are good or bad.\n\n>####\t4.1. MODELS\n>####\t4.2. TRAINING AND PREDICTING\n>####\t4.3. RESULTS\n>####\t4.4. VISUALIZATION OF RESULTS\n    \n### 5. MODEL VERIFICATION\n\nWe have Gauss-Markov Assumptions (Conditions) to check problems if any. When the model passes the tests, we can trust our model.\n\n>####\t5.1. LINEARITY\n>####\t5.2. RANDOM\n>####\t5.3. NON-COLLINEARITY\n>####\t5.4. EXOGENEITY\n>####\t5.5. HOMOSCEDASTICITY\n    \n### 6. CONCLUSION"},{"metadata":{},"cell_type":"markdown","source":"#### ---------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"### 1. INTRODUCTION"},{"metadata":{},"cell_type":"markdown","source":"- Definitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as sts\nimport statsmodels.api as sm\nimport math\nimport warnings\n\nfrom matplotlib.mlab import PCA as mlabPCA\nfrom statsmodels.tools.eval_measures import mse, rmse\nfrom statsmodels.tsa.stattools import acf\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import normalize, scale, StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_absolute_error\n\nfrom scipy.stats.mstats import winsorize\nfrom scipy.stats import zscore, jarque_bera, normaltest, bartlett, levene\nfrom sqlalchemy import create_engine\n\nwarnings.filterwarnings('ignore')\n\nimport folium","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. ANALYZE PREPARATION"},{"metadata":{},"cell_type":"markdown","source":"- Let's load the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"hs=pd.read_csv(\"../input/kc_house_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Having a glance on the data."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"hs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we see, there are no null values in our dataset. All observations are full."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"hs.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hs.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We should make some changes in order to make easy to understand the data; e.g. parsing the date and converting units."},{"metadata":{"trusted":true},"cell_type":"code","source":"house=hs\nhouse[\"year_sold\"]=np.int64([i[0:4] for i in hs[\"date\"]])\nhouse[\"month_sold\"]=np.int64([i[4:6] for i in hs[\"date\"]])\nhouse[\"day_sold\"]=np.int64([i[6:8] for i in hs[\"date\"]])\n\nsq_conv=10.7639\nsqft=[\"sqft_living\", \"sqft_lot\", \"sqft_above\", \"sqft_basement\", \"sqft_living15\", \"sqft_lot15\"]\nsqmt=[\"sqmt\"+i[4:] for i in sqft]\nfor i in range(0, len(sqft)):\n    house[sqmt[i]]=[j/sq_conv for j in hs[sqft[i]]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house.drop(sqft, axis=1, inplace=True)\nhouse.drop(\"date\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. DATA EXPLATORY"},{"metadata":{},"cell_type":"markdown","source":"#### 3.1. OUTLIERS"},{"metadata":{},"cell_type":"markdown","source":"- We should check presence of outliers and decide what to do with them. We can use boxplot for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplots_adjust(hspace=1, wspace=0.2)\nfor i in range(1,len(house.columns)):\n    plt.subplot(5,5,i)\n    sns.boxplot(house.iloc[:,i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Thoughts:\n> 1. Outliers of price can't be understood from boxplot, because there are some very big houses in area size. Winsorizing may cause faults,\n 2. There is only one outlier for 'bedroom' parameter,\n 3. 'bedroom' and 'bathroom' has 0 (zero) values which are not to be considered valid,\n 4. 'bathroom' and 'floors' parameters' data type is float64 including halves and quarters,\n 5. 'yr_renovated' parameter has 0 (zero) values for not being renovated,\n 6. Parameters relating to area sizes and price are spread in a wide range. \n\n- What to do:\n> 1. For price parameter's outliers, we may choose to soften,\n 2. 'bedrooms' outlier and zero bedroom/bathroom values should be cleaned,\n 3. 'bathrooms' parameter type must be converted to integer with rounding lower. But, garret can be thought as a half floor.\n 4. If house is not renovated, renovation year must be built year,\n 5. If we think that some observations can be wrong, those must be cleaned. But, I don't think so. All observations are possible. So, they can be converted to a less effective form such as calculating logaritm or square root."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2. DATA CLEANING AND TRANSFORMATION"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"house=house.drop(house[house[\"bedrooms\"]>15].index, axis=0)\nhouse=house.drop(house[house[\"bedrooms\"]==0].index, axis=0)\nhouse=house.drop(house[house[\"bathrooms\"]==0].index, axis=0)\nhouse=house.reset_index(drop=True)\n\nhouse[\"bathrooms\"]=[math.trunc(i) for i in house[\"bathrooms\"]]\nhouse[\"yr_renovated\"]=house[\"yr_renovated\"].replace(0, house[\"yr_built\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can create a new parameter: age of the building at trade date. Some houses were brand new and some hadn't been finished then."},{"metadata":{"trusted":true},"cell_type":"code","source":"house[\"age_sold\"]=house[\"year_sold\"]-house[\"yr_built\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's make some parameters distributing in a wide range less effective."},{"metadata":{"trusted":true},"cell_type":"code","source":"prmt_to_soften=[\"sqmt_living\", \"sqmt_lot\", \"sqmt_above\", \"sqmt_basement\", \"sqmt_living15\", \"sqmt_lot15\"]\nfor i in prmt_to_soften:\n    house[i]+=1\n    house[i+\"_log\"]=np.log(house[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.3. DISTRIBUTIONS"},{"metadata":{},"cell_type":"markdown","source":"- Here is distributions of parameters."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 35))\nplt.tight_layout()\n\nfor i in range(1, len(house.columns)-1):\n    plt.subplot(8,4,i)\n    plt.hist(house[house.columns[i]])\n    plt.title(house.columns[i])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now, we can examine all parameters' relationship with price. Also, we may compare results with logaritmic prices."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 50))\nplt.tight_layout()\n\nfor i in range(2, len(house.columns)-1):\n    plt.subplot(16,4,2*i-3)\n    plt.scatter(house[house.columns[i]], house[\"price\"])\n    plt.title(house.columns[i]+\" (price)\")\n    \n    plt.subplot(16,4,2*i-2)\n    plt.scatter(house[house.columns[i]], np.log(house[\"price\"]))\n    plt.title(house.columns[i]+\" (price is logaritmic)\")\n\nplt.subplots_adjust(hspace=0.3, wspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we see;\n\n>- Up to 6 bedrooms, price increases; and then decreases,\n- There is a slight relation between price and grade,\n- Prices are getting higher as latitude increases and longtitude decreases,\n- Squaremeters of living area is strongly related with price,\n- Squaremeters of above ground level has relation with the target."},{"metadata":{},"cell_type":"markdown","source":"#### 3.4. MAPPING HOUSES"},{"metadata":{},"cell_type":"markdown","source":"- Let's see locations of houses on a map and check whether it says us something."},{"metadata":{"trusted":true},"cell_type":"code","source":"rt=MinMaxScaler()\nratio=rt.fit_transform(house[[\"price\"]])\n\nm = folium.Map(location=[47.55, -122.21], tiles=\"Mapbox Bright\", zoom_start=10)\n \nfor i in range(0,len(house[\"long\"])):\n    if ratio[i]<0.2:\n       folium.Circle(\n           location=[house[\"lat\"][i], house[\"long\"][i]],\n           radius=1,\n           fill=True,\n           color='yellow',\n           fill_color='yellow'\n       ).add_to(m)\n    \nfor i in range(0,len(house[\"long\"])):    \n    if ratio[i]>=0.2 and ratio[i]<0.4:\n       folium.Circle(\n           location=[house[\"lat\"][i], house[\"long\"][i]],\n           radius=3,\n           fill=True,\n           color='blue',\n           fill_color='blue'\n       ).add_to(m)\n    \nfor i in range(0,len(house[\"long\"])):    \n    if ratio[i]>=0.4 and ratio[i]<0.6:\n       folium.Circle(\n           location=[house[\"lat\"][i], house[\"long\"][i]],\n           radius=20,\n           fill=True,\n           color='red',\n           fill_color='red'\n       ).add_to(m)\n    \nfor i in range(0,len(house[\"long\"])):\n    if ratio[i]>=0.6 and ratio[i]<0.8:\n       folium.Circle(\n           location=[house[\"lat\"][i], house[\"long\"][i]],\n           radius=20,\n           fill=True,\n           color='cyan',\n           fill_color='cyan'\n       ).add_to(m)\n    \nfor i in range(0,len(house[\"long\"])):\n    if ratio[i]>=0.8:\n       folium.Circle(\n           location=[house[\"lat\"][i], house[\"long\"][i]],\n           radius=20,\n           fill=True,\n           color='black',\n           fill_color='black'\n       ).add_to(m)\n\nm.save('mymap.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It seems houses around the lake and streams costs pretty higher. Latitude can be used to add that feature to the model, but unfortunately longtitude doesn't seem to be correlated with the target."},{"metadata":{},"cell_type":"markdown","source":"#### 3.5. CORRELATIONS"},{"metadata":{},"cell_type":"markdown","source":"- Belove is the correlation matrix. We should check the correlations with our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"house[\"price_log\"]=np.log(house[\"price\"])\nhouse_corr=house.corr()\n\nplt.figure(figsize=(20,15))\nsns.heatmap(house_corr, vmin=-1, vmax=1, cmap=\"bwr\", annot=True, linewidth=0.1)\nplt.title(\"Parameter Correlation Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- This figure is difficult to interpret. If we reduce size, we can easily see relations. Selecting values above 0.2 may be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns=[\"price\", \"bedrooms\", \"bathrooms\",\"floors\", \"waterfront\", \"view\", \"grade\", \"lat\", \n                  \"sqmt_living\", \"sqmt_above\", \"sqmt_basement\", \"sqmt_living15\", \"price_log\"]\n\nreduced=house_corr.loc[selected_columns, selected_columns]\n\nplt.figure(figsize=(12,6))\nsns.heatmap(reduced, vmin=-1, vmax=1, cmap=\"bwr\", annot=True, linewidth=0.1)\nplt.title(\"Reduced Parameter Correlation Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We explored our data set until now. From now on, we are going to set and test several models. Primarily, we should split the data set into 2 pieces for training and testing. Parameters look like a little bit more related with logaritmic price. So, we can use that in our models."},{"metadata":{},"cell_type":"markdown","source":"### 4. MODELING"},{"metadata":{},"cell_type":"markdown","source":"#### 4.1. MODELS"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = house[\"price_log\"]\nX = house[[\"bedrooms\", \"bathrooms\",\"floors\", \"waterfront\", \"view\", \"grade\", \"lat\", \n                  \"sqmt_living\", \"sqmt_above\", \"sqmt_basement\", \"sqmt_living15\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 465)\n\nprint(\"Observation Quantity in Training Set : {}\".format(X_train.shape[0]))\nprint(\"Observation Quantity in Test Set     : {}\".format(X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Here are our models and definitions:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Having less correlation with eachother (also with target) having unique features\nmodel_1=house[[\"bedrooms\", \"floors\", \"waterfront\", \"view\", \"lat\", \"sqmt_basement\"]]\n\n# Having less correlation with eachother including 1 high target-correlated parameter\nmodel_2=house[[\"bedrooms\", \"floors\", \"waterfront\", \"view\", \"lat\", \"sqmt_basement\", \"sqmt_living\"]]\n\n# Having less correlation with eachother including 1 (different) high target-correlated parameter\nmodel_3=house[[\"bedrooms\", \"floors\", \"waterfront\", \"view\", \"lat\", \"sqmt_basement\", \"grade\"]]\n\n# Highest target-correlated parameters (May contain heteroskedasticity)\nmodel_4=house[[\"bathrooms\", \"grade\", \"sqmt_living\", \"sqmt_above\", \"sqmt_living15\"]]\n\n# Balanced (half is less correlated having unique features, and other half is high target-correlated)\nmodel_5=house[[\"waterfront\", \"view\", \"lat\", \"grade\", \"sqmt_living\", \"bathrooms\"]]\n\n# 7 components of PCA result\nhouse_stdized = StandardScaler().fit_transform(house)\npca = PCA(n_components=7)\nh_std=pca.fit_transform(house_stdized)\n\npca_result=pd.DataFrame()\nfor i in range(7):\n    pca_result[i]=h_std.T[i]\n\nmodel_6=pca_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2. TRAINING AND PREDICTING"},{"metadata":{},"cell_type":"markdown","source":"- Now, let's train our models and check which is the best."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# MODEL 1\nm_1_train, m_1_test, y_train, y_test = train_test_split(model_1, Y, test_size = 0.25, random_state = 465)\nm_1_train = sm.add_constant(m_1_train)\nresult_1 = sm.OLS(y_train, m_1_train).fit()\nprint(result_1.summary())\n\n# MODEL 2\nm_2_train, m_2_test, y_train, y_test = train_test_split(model_2, Y, test_size = 0.25, random_state = 465)\nm_2_train = sm.add_constant(m_2_train)\nresult_2 = sm.OLS(y_train, m_2_train).fit()\nprint(result_2.summary())\n\n# MODEL 3\nm_3_train, m_3_test, y_train, y_test = train_test_split(model_3, Y, test_size = 0.25, random_state = 465)\nm_3_train = sm.add_constant(m_3_train)\nresult_3 = sm.OLS(y_train, m_3_train).fit()\nprint(result_3.summary())\n\n# MODEL 4\nm_4_train, m_4_test, y_train, y_test = train_test_split(model_4, Y, test_size = 0.25, random_state = 465)\nm_4_train = sm.add_constant(m_4_train)\nresult_4 = sm.OLS(y_train, m_4_train).fit()\nprint(result_4.summary())\n\n# MODEL 5\nm_5_train, m_5_test, y_train, y_test = train_test_split(model_5, Y, test_size = 0.25, random_state = 465)\nm_5_train = sm.add_constant(m_5_train)\nresult_5 = sm.OLS(y_train, m_5_train).fit()\nprint(result_5.summary())\n\n# MODEL 6\nm_6_train, m_6_test, y_train, y_test = train_test_split(model_6, Y, test_size = 0.25, random_state = 465)\nm_6_train = sm.add_constant(m_6_train)\nresult_6 = sm.OLS(y_train, m_6_train).fit()\nprint(result_6.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Model_6 using PCA gives the best results having the ratio of 83.1% according to adjusted R-square test. Also, it has the best results according to AIC and BIC tests."},{"metadata":{},"cell_type":"markdown","source":"#### 4.3. RESULTS"},{"metadata":{},"cell_type":"markdown","source":"- What about test results? Which model predicts target parameter the best with unseen observations? Let's test them."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Model 1 predicts\nm_1_test=sm.add_constant(m_1_test)\nresult_1_predicted=result_1.predict(m_1_test)\ntrain_1_predicted=result_1.predict(m_1_train)\n\n# Model 2 predicts\nm_2_test=sm.add_constant(m_2_test)\nresult_2_predicted=result_2.predict(m_2_test)\ntrain_2_predicted=result_2.predict(m_2_train)\n\n# Model 3 predicts\nm_3_test=sm.add_constant(m_3_test)\nresult_3_predicted=result_3.predict(m_3_test)\ntrain_3_predicted=result_3.predict(m_3_train)\n\n# Model 4 predicts\nm_4_test=sm.add_constant(m_4_test)\nresult_4_predicted=result_4.predict(m_4_test)\ntrain_4_predicted=result_4.predict(m_4_train)\n\n# Model 5 predicts\nm_5_test=sm.add_constant(m_5_test)\nresult_5_predicted=result_5.predict(m_5_test)\ntrain_5_predicted=result_5.predict(m_5_train)\n\n# Model 6 predicts\nm_6_test=sm.add_constant(m_6_test)\nresult_6_predicted=result_6.predict(m_6_test)\ntrain_6_predicted=result_6.predict(m_6_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"---------- MODEL 1 ----------\")\nprint(\"Mean Absolute Error (MAE)             : {}\".format(mean_absolute_error(y_test, result_1_predicted)))\nprint(\"Mean Squared Error (MSE)              : {}\".format(mse(y_test, result_1_predicted)))\nprint(\"Root Mean Square Error (RMSE)         : {}\".format(rmse(y_test, result_1_predicted)))\nprint(\"Mean Absolute Percentage Error (MAPE) : {}\".format(np.mean(np.abs((y_test - result_1_predicted) / y_test)) * 100))\nprint(\"\\n\")\n\nprint(\"---------- MODEL 2 ----------\")\nprint(\"Mean Absolute Error (MAE)             : {}\".format(mean_absolute_error(y_test, result_2_predicted)))\nprint(\"Mean Squared Error (MSE)              : {}\".format(mse(y_test, result_2_predicted)))\nprint(\"Root Mean Square Error (RMSE)         : {}\".format(rmse(y_test, result_2_predicted)))\nprint(\"Mean Absolute Percentage Error (MAPE) : {}\".format(np.mean(np.abs((y_test - result_2_predicted) / y_test)) * 100))\nprint(\"\\n\")\n\nprint(\"---------- MODEL 3 ----------\")\nprint(\"Mean Absolute Error (MAE)             : {}\".format(mean_absolute_error(y_test, result_3_predicted)))\nprint(\"Mean Squared Error (MSE)              : {}\".format(mse(y_test, result_3_predicted)))\nprint(\"Root Mean Square Error (RMSE)         : {}\".format(rmse(y_test, result_3_predicted)))\nprint(\"Mean Absolute Percentage Error (MAPE) : {}\".format(np.mean(np.abs((y_test - result_3_predicted) / y_test)) * 100))\nprint(\"\\n\")\n\nprint(\"---------- MODEL 4 ----------\")\nprint(\"Mean Absolute Error (MAE)             : {}\".format(mean_absolute_error(y_test, result_4_predicted)))\nprint(\"Mean Squared Error (MSE)              : {}\".format(mse(y_test, result_4_predicted)))\nprint(\"Root Mean Square Error (RMSE)         : {}\".format(rmse(y_test, result_4_predicted)))\nprint(\"Mean Absolute Percentage Error (MAPE) : {}\".format(np.mean(np.abs((y_test - result_4_predicted) / y_test)) * 100))\nprint(\"\\n\")\n\nprint(\"---------- MODEL 5 ----------\")\nprint(\"Mean Absolute Error (MAE)             : {}\".format(mean_absolute_error(y_test, result_5_predicted)))\nprint(\"Mean Squared Error (MSE)              : {}\".format(mse(y_test, result_5_predicted)))\nprint(\"Root Mean Square Error (RMSE)         : {}\".format(rmse(y_test, result_5_predicted)))\nprint(\"Mean Absolute Percentage Error (MAPE) : {}\".format(np.mean(np.abs((y_test - result_5_predicted) / y_test)) * 100))\nprint(\"\\n\")\n\nprint(\"---------- MODEL 6 ----------\")\nprint(\"Mean Absolute Error (MAE)             : {}\".format(mean_absolute_error(y_test, result_6_predicted)))\nprint(\"Mean Squared Error (MSE)              : {}\".format(mse(y_test, result_6_predicted)))\nprint(\"Root Mean Square Error (RMSE)         : {}\".format(rmse(y_test, result_6_predicted)))\nprint(\"Mean Absolute Percentage Error (MAPE) : {}\".format(np.mean(np.abs((y_test - result_6_predicted) / y_test)) * 100))\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All tests tell us that the best model is the sixth one having the lowest values."},{"metadata":{},"cell_type":"markdown","source":"#### 4.4. VISUALIZATION OF RESULTS"},{"metadata":{},"cell_type":"markdown","source":"- Visualization of predictions."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"header_font = {'family':'arial', 'color':'darkred', 'weight':'bold', 'size':15}\naxis_font = {'family':'arial', 'color':'darkblue', 'weight':'bold', 'size':12}\n\nplt.figure(figsize=(15,8))\n\nplt.subplot(2,3,1)\nplt.scatter(y_train, train_1_predicted, color=\"brown\", label=\"Training Set\")\nplt.scatter(y_test, result_1_predicted, color=\"blue\", label=\"Test Set\", alpha=0.2)\nplt.plot(y_test, y_test, color=\"red\", label=\"Price (Logaritmic)\")\nplt.xlabel(\"True Prices\", fontdict=axis_font)\nplt.ylabel(\"Predicted Prices\", fontdict=axis_font)\nplt.title(\"True and Predicted Prices (Model 1)\", fontdict=header_font)\nplt.legend(loc = \"upper left\")\n\nplt.subplot(2,3,2)\nplt.scatter(y_train, train_2_predicted, color=\"brown\", label=\"Training Set\")\nplt.scatter(y_test, result_2_predicted, color=\"blue\", label=\"Test Set\", alpha=0.2)\nplt.plot(y_test, y_test, color=\"red\", label=\"Price (Logaritmic)\")\nplt.xlabel(\"True Prices\", fontdict=axis_font)\nplt.ylabel(\"Predicted Prices\", fontdict=axis_font)\nplt.title(\"True and Predicted Prices (Model 2)\", fontdict=header_font)\nplt.legend(loc = \"upper left\")\n\nplt.subplot(2,3,3)\nplt.scatter(y_train, train_3_predicted, color=\"brown\", label=\"Training Set\")\nplt.scatter(y_test, result_3_predicted, color=\"blue\", label=\"Test Set\", alpha=0.2)\nplt.plot(y_test, y_test, color=\"red\", label=\"Price (Logaritmic)\")\nplt.xlabel(\"True Prices\", fontdict=axis_font)\nplt.ylabel(\"Predicted Prices\", fontdict=axis_font)\nplt.title(\"True and Predicted Prices (Model 3)\", fontdict=header_font)\nplt.legend(loc = \"upper left\")\n\nplt.subplot(2,3,4)\nplt.scatter(y_train, train_4_predicted, color=\"brown\", label=\"Training Set\")\nplt.scatter(y_test, result_4_predicted, color=\"blue\", label=\"Test Set\", alpha=0.2)\nplt.plot(y_test, y_test, color=\"red\", label=\"Price (Logaritmic)\")\nplt.xlabel(\"True Prices\", fontdict=axis_font)\nplt.ylabel(\"Predicted Prices\", fontdict=axis_font)\nplt.title(\"True and Predicted Prices (Model 4)\", fontdict=header_font)\nplt.legend(loc = \"upper left\")\n\nplt.subplot(2,3,5)\nplt.scatter(y_train, train_5_predicted, color=\"brown\", label=\"Training Set\")\nplt.scatter(y_test, result_5_predicted, color=\"blue\", label=\"Test Set\", alpha=0.2)\nplt.plot(y_test, y_test, color=\"red\", label=\"Price (Logaritmic)\")\nplt.xlabel(\"True Prices\", fontdict=axis_font)\nplt.ylabel(\"Predicted Prices\", fontdict=axis_font)\nplt.title(\"True and Predicted Prices (Model 5)\", fontdict=header_font)\nplt.legend(loc = \"upper left\")\n\nplt.subplot(2,3,6)\nplt.scatter(y_train, train_6_predicted, color=\"brown\", label=\"Training Set\")\nplt.scatter(y_test, result_6_predicted, color=\"blue\", label=\"Test Set\", alpha=0.2)\nplt.plot(y_test, y_test, color=\"red\", label=\"Price (Logaritmic)\")\nplt.xlabel(\"True Prices\", fontdict=axis_font)\nplt.ylabel(\"Predicted Prices\", fontdict=axis_font)\nplt.title(\"True and Predicted Prices (Model 6)\", fontdict=header_font)\nplt.legend(loc = \"upper left\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As seen on graphics, all models  are approximately parallel with logaritmic prices. But, model 6 has the best result. In addition; because of our target parameter is logaritmic, we should not forget to apply anti-logaritmic transformation to the predicted results."},{"metadata":{},"cell_type":"markdown","source":"### 5. MODEL VERIFICATION"},{"metadata":{},"cell_type":"markdown","source":"We are going to verify our model according to Gauss-Markov Assumptions (Conditions). Below is the assumptions to use:\n>1. Linearity: The parameters we are estimating using the OLS method must be themselves linear.\n2. Random: Our data must have been randomly sampled from the population. Errors must be distrubuted normally.\n3. Non-Collinearity: The regressors being calculated aren’t perfectly correlated with each other.\n4. Exogeneity: The regressors aren’t correlated with the error term.\n5. Homoscedasticity: No matter what the values of our regressors might be, the error of the variance is constant.\n\nNow, let's verify."},{"metadata":{},"cell_type":"markdown","source":"#### 5.1. LINEARITY"},{"metadata":{},"cell_type":"markdown","source":"- In principle, this assumption is not about estimation, but about how we determine our model. Therefore, as long as we use models that take linearity assumption into account, there is no need to worry about this. Our co-efficients can be seen above."},{"metadata":{},"cell_type":"markdown","source":"#### 5.2. RANDOM"},{"metadata":{},"cell_type":"markdown","source":"- Our data set is taken from real life in a time period and from a certain place that we are estimating the prices.\n- train_test_split method that we made use of chooses the test sample randomly for us.\n- P value of Jarque-Bera test for model 6 (as can be seen above) is 1.16e-260 (converges to zero). This means, errors are distributed randomly."},{"metadata":{"trusted":true},"cell_type":"code","source":"errors=math.e**result_6_predicted-math.e**y_test\nacf_data = acf(errors)\n\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.hist(errors/math.e**y_test, bins=100)\nplt.title(\"Distribution of Errors\")\n\nplt.subplot(1,2,2)\nplt.plot(acf_data[1:])\nplt.title(\"Autocorrelation Grapic\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Error distribution generally fits to normal distribution with slight skewness. As we look at the auto correlation graphic, all correlations are in 0.05 confidence interval. "},{"metadata":{},"cell_type":"markdown","source":"#### 5.3. NON-COLLINEARITY"},{"metadata":{},"cell_type":"markdown","source":"To determine multicollinearity, we can look at the correlation matrix of properties. multicollinearity can be eliminated by PCA or by disposing of some associated properties. Therefore our model 6 is result of PCA, this condition is OK."},{"metadata":{},"cell_type":"markdown","source":"#### 5.4. EXOGENEITY"},{"metadata":{},"cell_type":"markdown","source":"If we check Durbin-Watson Test result from the model summary, it is seen as 2.009. This result tells us that error terms are not correlated with any regressors. If it were +1 or -1, there would be strong correlation and the model wouldn't be able to be used."},{"metadata":{},"cell_type":"markdown","source":"#### 5.5. HOMOSCEDASTICITY"},{"metadata":{},"cell_type":"markdown","source":"- We are going to use Bartlett and Levene tests for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"bart_stats = bartlett(math.e**result_6_predicted, errors)\nlev_stats = levene(math.e**result_6_predicted, errors)\n\nprint(\"Bartlett test value : {0:3g} and p value : {1:.21f}\".format(bart_stats[0], bart_stats[1]))\nprint(\"Levene test value   : {0:3g} and p value : {1:.21f}\".format(lev_stats[0], lev_stats[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As p values are equal to zero, our model is homocedasticitic."},{"metadata":{},"cell_type":"markdown","source":"### 6. CONCLUSION"},{"metadata":{},"cell_type":"markdown","source":"1. Price taken logaritm gave better results than raw price parameter.\n\n2. Our best model's adj.R^2 value is 83.1%,\n\n3. According to all 5 Gauss-Markov Assumptions, our model is successful."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}