{"cells":[{"metadata":{},"cell_type":"markdown","source":"Problem statement\n\nDetect whether a text message is spam or ham.\n\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam."},{"metadata":{},"cell_type":"markdown","source":"Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load datasets\ndf = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding=\"ISO-8859-1\")\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 2', 'Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyse target"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_count = df.groupby('v1').v1.count()\ntarget_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_target = (target_count / len(df)) * 100\npercent_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('v1').v1.count().plot.bar(ylim=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Map v1"},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {'ham':1 ,'spam':0}\ndf.v1 = df.v1.map(dic)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess raw text and get ready for machine learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create new column\ndf['processedtext'] = df['v2']\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nstemmer = PorterStemmer()\nwords = stopwords.words(\"english\")\n\ndf['processedtext'] = df['processedtext'].apply(lambda x: \" \".join([stemmer.stem(i) \nfor i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make all words lower case\ndf['processedtext'] = df['processedtext'].str.lower()\n\n# remove special characters, numbers, punctuations\ndf['processedtext'] = df['processedtext'].str.replace(\"[^a-zA-Z#]\", \" \")\n\n#remove words less than 3 characters\ndf['processedtext'] = df['processedtext'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_words = ' '.join([text for text in df['processedtext']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(spam_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define X and y variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define X and y\ny = df['v1']\nX = df['processedtext']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert text to word frequency vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\ndf_tfIdf = vectorizer_tfidf.fit_transform(X.values.astype('U'))\nprint(vectorizer_tfidf.get_feature_names()[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split X for training and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(df_tfIdf, y, test_size=0.10, random_state=1, shuffle=True)\nX_train.shape, X_val.shape, y_train.shape,y_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\n\nmodel = PassiveAggressiveClassifier(max_iter=1000, random_state=1,tol=1e-3).fit(X_train, y_train)\nprint(model.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict on validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_val)\nprint(model.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_val,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val = pd.DataFrame({'Actual': y_val, 'Predicted':y_pred})\ndf_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd_val = TruncatedSVD(n_components=2, random_state=1)\nprincipalComponents_val = svd_val.fit_transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\nplt.scatter(principalComponents_val[:, 0], principalComponents_val[:,1], c = y_pred == y_val - 1, alpha = .8, s = 50)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}