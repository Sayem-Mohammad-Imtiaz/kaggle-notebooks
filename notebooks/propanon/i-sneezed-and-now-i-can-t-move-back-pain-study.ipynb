{"cells":[{"metadata":{"_cell_guid":"6c7f5e24-787a-49f1-8c27-2ab81c8b8d06","_uuid":"96713ece09fa88e01a333b1d39b527ab5319293f"},"cell_type":"markdown","source":"# <center>\"I sneezed and now I can't move.\"</center>\n## <center>A study of lower back pain using machine learning</center> \n\n\n![](https://images.duckduckgo.com/iu/?u=http%3A%2F%2Fcentralwestrehab.com.au%2Fimages%2Fuploads%2FChronic%2520Pain%2FBackpainCartoon.png&f=1)"},{"metadata":{"_cell_guid":"efac2651-9c53-4a3c-89ee-6216b7a28393","_uuid":"c2d17e1f952309396bc02e5a804ada6de5d583cf"},"cell_type":"markdown","source":"With lower back pain it's either you have it or you know someone who has it. Our increasingly sedentary lifestyle has led to some apalling figures when it comes to the subject of back pains.  \nAccording to [thegoodbody.com](www.thegoodbody.com), 8 in 10 Americans will experience back pain in their lifetime. They affect men almost as much as they affect women, reduce the ability to work, to focus, and are just unbearable. No need to convince anyone that lower back pain is a b\\*\\*\\*\\*.  \n\nIn this short study we'll try to get some leads on identifying the strongest factors causing lower back pain and we'll create models and representation to classify back pain as well as possible. To do so, we'll follow this **clickable** plan."},{"metadata":{"_cell_guid":"0f6a9c24-192d-4df6-9443-fad540a10ce9","_uuid":"dfb57e4fa7e4374c0279a5b3e31c6f9f563e517f"},"cell_type":"markdown","source":"\n* **[Introduction](#Introduction)**\n    1. [About the Data](#About-the-Data)\n    2. [Packages used](#Packages-used)\n    3. [Goals](#Goals)\n\n* ** [Exploratory Data Analysis](#Exploratory-Data-Analysis)**\n    1. [Class Distribution](#Class-Distribution)\n    2. [Correlations](#Correlations)\n    3. [Shallow tree intuition](#Shallow-tree-intuition)\n    4. [Univariate exploration of relevant factors](#Univariate-exploration-of-relevant-factors)\n        1. [Degree spondylolisthesis](#Degree-spondylolisthesis)\n        2. [Sacral slope, Pelvic incidence, and Pelvic tilt](#Sacral-slope,-Pelvic-incidence,-and-Pelvic-tilt)\n        3. [Pelvic radius](#Pelvic-radius)\n        4. [Lumbar lordosis angle](#Lumbar-lordosis-angle)\n* **[Feature Engineering](#Feature-Engineering)**\n    1. [PCA](#PCA)\n    2. [Polynomial Features](#Polynomial-Features)\n    3. [Boxing Features](#Boxing-Features)\n    4. [Effect on a simple model](#Effect-on-a-simple-model)\n\n* **[Modelling](#Modelling)**\n    1. [Choosing interpretability](#Choosing-interpretability)\n    2. [Decision Tree](#Decision-Tree)\n    3. [Logistic Regression](#Logistic-Regression)\n* **[Visualisations](#)**\n    1. [PCA on the new data](#PCA-on-the-new-data)\n    2. [TSNE](#TSNE)\n    3. [Spectral Embedding](#Spectral-Embedding)\n    4. [MDS](#MDS)"},{"metadata":{"_cell_guid":"c37532e3-f30f-4096-b184-18d61a7ef3f3","_uuid":"f4b88b806189efa393dfd1839ef43ddf95de15e3"},"cell_type":"markdown","source":"## Introduction\n### About the Data\nThe data set that we are going to study compiles 309 observations of patients with and without lower back pains. It contains 12 different predictors with **pretty complicated** names but we'll try to explain the most relevant of these predictors as we progress through the study. It also contains 1 target feature called \"Attribute\" that will get the value \"Abnormal\" when lower back pain is present and \"Normal\" when everything is ? Normal, you guessed it.  \nThe description of the data set explain a few of the reasons why lower back pain can occur : \n* The large nerve roots in the low back that go to the legs may be irritated\n* The smaller nerves that supply the low back may be irritated\n* The large paired lower back muscles (erector spinae) may be strained\n* The bones, ligaments or joints may be damaged\n* An intervertebral disc may be degenerating  \n\nWe'll see in the data set that we get mostly morphological/positional values, nothing **directly** telling us that one of the foremetionned things is happening but positional indicators that can definitely put us on the way. This data set does **not** contain any demographic information ( to my great regret ) about the subject observed. It would've been interesting as some of the values we're going to study typically change between men, women, and ethnicities.\n\n### Packages used\nMight as well put everything in one line and be done with it, here's a list of what we will use here"},{"metadata":{"collapsed":true,"_cell_guid":"d91df51c-3908-4b5b-bb54-b8ad97a4e2bb","_uuid":"1c962aab468b4ba12910ed23bbacb8e1001f5ff2","trusted":true},"cell_type":"code","source":"# The classics\nimport numpy as np\nimport pandas as pd\n\n# Visualisation tools\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nfrom matplotlib import style\n\nimport plotly\nfrom plotly.offline import iplot, init_notebook_mode\nfrom plotly.graph_objs import Scatter3d, Layout, Figure\n\nimport graphviz \n\n# Machine learning unavoidables\nfrom sklearn.decomposition import KernelPCA, PCA\nfrom sklearn.preprocessing import normalize,MinMaxScaler,PolynomialFeatures\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import SelectKBest, chi2\n","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"bd95e908-83d7-4d25-8dd0-9bd8955e5cbc","_uuid":"7d909bf5a72d3b47edf721c4dfd5136070f531c0"},"cell_type":"markdown","source":"### Goals\nThrough this notebook, I want to be able to identify the key factors causing lower back pain as well and I want to be able to create understandable classifiers to further human understanding of the problem while keeping a good quality of classification.  \nI will try to explain my thought process and my decision as thouroughly as possible so that everyone can learn something from this notebook, would it be in the subject of data science or just about back pain.\n\n## Exploratory Data Analysis"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Defining the name of each column as it was given in the dataset\ncol_list = ['Pelvic_incidence',\n               'Pelvic_tilt',\n               'Lumbar_lordosis_angle',\n               'Sacral_slope',\n               'Pelvic_radius',\n               'Degree_spondylolisthesis',\n               'Pelvic_slope',\n               'Direct_tilt',\n               'Thoracic_slope',\n               'Cervical_tilt',\n               'Sacrum_angle',\n               'Scoliosis_slope',\n               'Attribute',\n               'To_drop']\n\n# Loading the data\ndata = pd.read_csv(\"../input/Dataset_spine.csv\", names=col_list, header=1)\n\n# The last column contained meta-data about the other columns and is irrelevant in our study\ndata.drop('To_drop', axis=1, inplace=True)\n\n\ndata.head()","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"d9c7c59a-8209-4b78-a354-4ac87631679f","_uuid":"48d52820db159380d708a9742b6bd673f50d6d93","trusted":true},"cell_type":"code","source":"# Checking for the integrity of the data is good practice\ndata.info()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"eda86766-90f5-4cbb-bb85-0bff3944af76","_uuid":"092419cd0087c92e3eccc4cfe0bf2f832a2d33e1"},"cell_type":"markdown","source":"### Class Distribution"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"56dd1429-cfe0-443f-9df7-8e833c70ca5e","_uuid":"cbe03a047ffeb459755cfe7d240d081bcde2ff08","trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\ng=sns.factorplot(x='Attribute', hue='Attribute', data= data, kind='count',size=5,aspect=.8)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"d459a512-7fa8-4f5a-8a1e-f42422c55312","_uuid":"29ad99625ca81ce2fed3392174db4f6ae01753e4"},"cell_type":"markdown","source":"From a simple look, we could tell that the proportions are 2/3 Abnormal and 1/3 Normal which is actually quite unusual for medical data. Usually the Abnormal cases are extremely rare and this creates very skewed data sets. We don't have any information on how the dataset was gathered and created so unfortunately we can't really know why these numbers seem so unusual.  \n\nWe'll get to the true count of each class later."},{"metadata":{"collapsed":true,"_cell_guid":"8625863d-81d0-416a-ad6b-611493723e0c","_uuid":"663c6d95749f39c4e57881ed6e2cde9c21ab9b32","trusted":true},"cell_type":"code","source":"# Replacing our attribute with binary values : \ndata['Attribute'] = data['Attribute'].map({'Abnormal': 1, 'Normal': 0})","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"e4a4314c-7cd2-4dca-8ff5-faa42e16c2c0","_uuid":"e1655c2749a3b9c8e6a46a3995146e1359c2d652"},"cell_type":"markdown","source":"### Correlations\nI find it always interesting to plot a correlation map when possible. They are very simple and easily readable and can provide a tremendous amount of information in just a look."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\nd = data\ncorr = d.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\ng=sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"abb650f3-57cd-4804-9968-ff2a74c278e4","_uuid":"1da881c172c867db5ceafec302a3f31b03ce010d"},"cell_type":"markdown","source":"The first thing we want to look at is how our features are correlated with our target. As they are represented on the last line of this heatmap they are quite easy to see.\nThis way we identify that : \n* Pelvic_incidence\n* Pelvic_tilt\n* Lumbar_lordosis_angle\n* Sacral_slope\n* Pelvic_radius\n* Degree_spondylolisthesis\n\nAre all strongly correlated with our target values, with Degree_spondylolisthesis being the strongest one and Pelvic_radius being a negative correlation.\n\nSecondly, we might want to look at other correlations, maybe some features are completely correlated and redundant, thus, not that usefull in a classification problem...  \nThe strongest correlations that I see between predictors is the one between Sacral_slope, Pelvic_Tilt, and Pelvic_incidence, we will definitely look into that later.\n            \nHold down to you hats because it gets **fancier**, the package `seaborn` comes with a great tool to visualize more accurately the correlation inside of a dataset.\n\n### More correlations !"},{"metadata":{"_cell_guid":"44ababc7-16fb-4677-becb-0cd72b4747a8","_uuid":"d8af5feb4db4f55449d72f83b59477a889462642","trusted":true},"cell_type":"code","source":"sns.pairplot(data, hue=\"Attribute\")\nplt.show()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"d83c39af-7e27-4d97-9bc7-6274d2974b02","_uuid":"7a2881590fb09bc97faf427aeb6a52b936afd88c"},"cell_type":"markdown","source":"It may seem a bit much at first sight, but taking each cell independently this graph is actually very understandable and holds even more information than the correlation map we plotted before.  \nAs I was about to plot this I was somewhat hoping for a **holy grail** of classification, a natural feature that would separate almost perfectly the two classes of attributes, but alas it wasn't here... But it doesn't mean that this graph was for nothing.  \nIt shows us _more or less_ some kind of linear relations between the first 6 columns of the data set.  \nIt also shows us the distribution of these 6 first columns and the separation that appears there between Normal and Abnormal albeit not perfect but always useful.\n\n## Shallow tree intuition\nGranted, we got to look at pretty much everything above and we got a pretty good hunch of which parameters were going to be important and which were not.   \nBut **looking** is not **knowing**. Thus, to get a deeper, more explainable intuition, we will create a graph for a shalllow decision tree. Because it is shallow the computation time and memory needed will be very small ( even though for a 309\\*12 matrix this isn't our strongest concern), and it will only retain the most relevant factors.  \nOn a side note, I think this is **great** practice to do this to familiarise yourself with a dataset in a simple way."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"c6726582-04a8-4f40-bde2-6d9d9e8d6f36","_uuid":"78dbc4c3b2f63638cada5940b999f1d2228a4edd","trusted":true},"cell_type":"code","source":"# Creating the arrays\nX = data.iloc[:,:-1]\ny = data.iloc[:,-1]\n\n# Creating the shallow decision tree\nclf = DecisionTreeClassifier(max_depth=3)\n\n# Fitting the decision tree to the data set\nclf = clf.fit(X, y)\n\n# Plotting the results\ndot = tree.export_graphviz(clf,out_file=None,\n                         feature_names=col_list[:-2],  \n                         class_names=['Normal','Abnormal'],  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot) \ngraph.render(\"back problems\")\ngraph","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"299206ad-5dd0-4063-894c-c39f992b41f5","_uuid":"ddd3e45a38d90880154a33e748ef1a3c205dd0ad"},"cell_type":"markdown","source":"A true **beauty** that's for sure, but what do we get out of it ?\n* Degree_spondylolisthesis is definitely a **key** factor in the understanding of lower back pain\n* Sacral_slope is our second best split and will provide great information as well\n* Cervical_tilt and Pelvic_radius seems interesting as well\n\nI started this study with little domain knowledge, but I believe that cervical tilt refers to the upper vertabraes of the spine. We are talking about lower back pain but it is also possible that the cervical tilt would be a result of other displacements lower in the spine."},{"metadata":{"_cell_guid":"40a7e6c9-12f0-4a23-a146-ee76d361af8f","_uuid":"e0cff5d3cfff39cf02e30829c2153e7e5fef66bf"},"cell_type":"markdown","source":"## Univariate exploration of relevant factors\n### Degree spondylolisthesis\nSpondylolisthesis is derived from the Greek words “spondy”, meaning vertebra, and “listhesis”, meaning movement. Spondylolisthesis is an abnormal condition in which there is instability in the spinal column, as one vertebral body is shifting forward over the next vertebrae.   \n![tt](https://www.spine.org/portals/0/img/KnowYourBack/Conditions/LumbarSpondy1.png)Pardon my French, more simply, the degree of spondylolisthesis measures ( more or less ) how much a vertebra has slipped from its original position.  \n\nLet's see how it is distributed among the classes."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"73cb1b51-1081-42e6-aa9b-f4ee7f7bc4a9","_uuid":"009541e5ad6b659a0b986be9235d241578024e73","trusted":true},"cell_type":"code","source":"# A simple reusable function to plot the distribution of one feature with different colours for each class\ndef hist_graph(column):\n    a4_dims = (11.7, 8.27)\n    fig, ax = plt.subplots(figsize=a4_dims)\n    sns.distplot(data[data['Attribute']==1][column], color='r')\n    sns.distplot(data[data['Attribute']==0][column],ax=ax, color='b') \n\nhist_graph('Degree_spondylolisthesis')","execution_count":9,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"eba4861c-cac9-4583-94d2-962f1687557e","_uuid":"e368990a00a246390f134f2cd6fb537af6b382fc","trusted":true},"cell_type":"code","source":"# A simple and reusable function to show the numbers in a dataframe\ndef compare_df(column):\n    norm = data[data['Attribute']==0][[column]].describe()\n    abnorm = data[data['Attribute']==1][[column]].describe()\n\n    df = pd.DataFrame(data = norm)\n    df['Normal'] = df[column]\n    df.drop(column, axis=1, inplace=True)\n    df['Abnormal']= abnorm\n    return df\n\ncompare_df('Degree_spondylolisthesis')","execution_count":10,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8c2ac39a-b70e-4828-adda-5f72b2a997df","_uuid":"cd9fec145a887b3cbd76a04e77e6d8f27f5472ce"},"cell_type":"markdown","source":"For the Normal class : \n\n* The population is centered around 1.15.\n* Small standard deviation showing a **very** centered classs (just see the histogram).\n* The minimum value for the Normal class is lower than the minimum value for the Abnormal class showing that the degree spondylolisthesis doesn't have the same influence on back pains for a positive or for a negative displacement.\n* The maximum value seems a bit high but since 75% of the Normal population is under ~5, these cases could be either outliers, or show that other factors can maybe compensate for the degree spondylolisthesis.\n\nFor the Abnormal class : \n\n* Both the mean and median are above the Normal's class maximum at respectively ~38 and ~32.\n* High standard deviation showing a wide spread of the values and of the resulting issues.\n* Still goes down in the negative values, then again showing little influence of this parameter once below zero.\n* We'll take a closer look at that guy with the max value of 418..."},{"metadata":{"_cell_guid":"c9f130e4-02a9-4ff0-823f-1d68db5f3d80","_uuid":"531aab9e0d5fc61d6c5bfc08891c5c39e24b782c"},"cell_type":"markdown","source":"### Sacral slope, Pelvic incidence, and Pelvic tilt\n\nThe sacral slope is the angle between a horizontal plane  and the upper surface of the sacrum. The sacrum is the last bone in our spine where the 5 vertebraes have fused together to create one bone. At the end of the sacrum, we can find the coccyx, remnant bone of what used to be a tail for human beings.\nThe sacral slope is closely related to the pelvic incidence and to the pelvic tilt by definition : \n$$ \\text{Pelvic Incidence} = \\text{Pelvic Tilt} + \\text{Sacral Slope} $$\n![sacral slope](https://musculoskeletalkey.com/wp-content/uploads/2016/07/C19-FF1-4.gif)\n\nWe don't have any information on the ethnicity of the patients in this data set, but it is good to notice that different values are found among different ethnicities for the sacral slope, this will help explain for variations of values that would seem abnormal otherwise. This is not only true for ethnicities but also for genders... A shame that we don't have any demographic data really.\n\n### Sacral slope distribution"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"e92cfa13-f098-4490-b544-aded60fd0e7e","_uuid":"909f3ab58d1bc42c0a3e319890dabf44f998e7eb","trusted":true},"cell_type":"code","source":"hist_graph('Sacral_slope')","execution_count":11,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"fefb7c55-76f6-476f-9e4f-ac518e4d536f","_uuid":"1f1dd78004238a77cd1dbb31091d4a0719af8461","trusted":true},"cell_type":"code","source":"compare_df('Sacral_slope')","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"c8cef8cc-0c33-429c-a115-14f459bb7be7","_uuid":"cb13829ecf438e6c0a628bfc238ac23aa67c5715"},"cell_type":"markdown","source":"Not much stands out from the observation of the sacral slope alone. The Abnormal class usually has a slightly higher sacral slope than the normal class but it stays within one standard deviation of the Normal class.  \nOne thing we can notice is that then again, the Abnormal class is more widely spread than the Normal class and that we have among the Abnormal class one ( or several ) outliers around 120.\n\n### Pelvic incidence distribution"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"daa59ff2-37e1-4fba-ac39-f7c522188a02","_uuid":"f440fced7f6cedd76a4f45aabaecbf5265c3124d","trusted":true},"cell_type":"code","source":"hist_graph('Pelvic_incidence')","execution_count":13,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"6eb40231-3138-4fbe-aab5-a055c91d2562","_uuid":"00112dde27c9dbbeaa1664d4d0acefcdcd617f31","trusted":true},"cell_type":"code","source":"compare_df('Pelvic_incidence')","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"e6a10ed8-a7ed-48a0-a483-024b1a2232f1","_uuid":"196305290fbb5f6e701eba21b1e6c82ec6775831"},"cell_type":"markdown","source":"Same thing as before, Normal class is more centered and usually lower than the Abnormal class which is again more widely spread.  \nAs we saw in the pairplot drawn earlier, there is almost never in this data set one single factor that will allow us to determine the normality or abnormality of a patient, but combinations of these parameters through feature engineering may help, even a human to tell at first glance the status of a patient.\n\n### Pelvic tilt distribution"},{"metadata":{"scrolled":true,"_kg_hide-input":true,"_cell_guid":"347bb926-8751-47e4-8715-a3bae6375392","_uuid":"6b9407a32bcdb5b142ea03b386aa7ffdf4e68d46","trusted":true},"cell_type":"code","source":"hist_graph('Pelvic_tilt')","execution_count":15,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"2cd04f75-eaf2-438b-a019-14bc569194a0","_uuid":"b175e17196ade96c578ceea3a289416851dec997","trusted":true},"cell_type":"code","source":"compare_df('Pelvic_tilt')","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"dcc1890c-49ad-4991-b672-af67ec381726","_uuid":"8ff75609f4ea4d788d917db60ec94a2b69e48f3b"},"cell_type":"markdown","source":"A slight shift to the right for the Abnormal class that is again more widespread than the Normal one.\n* While the Normal class seems to follow a Gaussian Distribution, the Abnormal class seems to follow more of a Poisson distribution with its longer tail on the right.\n* Every case with a Pelvic tilt about 29.89 will be classified as Abnormal.\n* Negative values show little impact by themselves."},{"metadata":{"_cell_guid":"1fe59766-780e-493d-b8d1-3dda75af2a79","_uuid":"a466171a6163f29c0342e2f7f860886e8aa5f9d1"},"cell_type":"markdown","source":"### Pelvic radius\nThe pelvic radius is the angle formed between a vertical plane and the hip-axis/posterior superior corner of S1 ( first element of the sacrum ) line.  \n\n![](https://synapse.koreamed.org/ArticleImage/0043JKOA/jkoa-51-9-g003-l.jpg)\n\n<center>Pelvic radius is noted PR-S1 in this image</center>\n\n\n"},{"metadata":{"_cell_guid":"3dce46d0-dc4c-41a7-b8e6-532ba54e4ef3","_uuid":"499d8217c5f526deae82ef6cd9137dce371b4e0d"},"cell_type":"markdown","source":"Let's take a look at the distribution of the pelvic radius amon the different classes."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"82ca10f2-99c8-4aef-bd30-85c628660c1e","_uuid":"0c20d734c0f40a722df18ea60cc9efb3e2c672b2","trusted":true},"cell_type":"code","source":"hist_graph('Pelvic_radius')","execution_count":17,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"c5569063-5933-45e6-b23a-1bbb7511eb0b","_uuid":"630b26135777dbfc1480f35e5862e6657b3916e6","trusted":true},"cell_type":"code","source":"compare_df('Pelvic_radius')","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"63b56f07-0b8f-4a1c-8618-d10eef9075c0","_uuid":"b903b6c59fc4e79ee2fe124c3007ea2f964a2545"},"cell_type":"markdown","source":"This is the first time that we are seeing the Abnormal class shifted to the left compared to the Normal class.\n* Both have almost normal distributions, as per usual.\n* The Abnormal class is still more widespread than the Normal class.\n* The Normal class distribution is entirely contained into the Abnormal class distribution, giving us a lower and upper bound for the Normal class between ~100.5 and ~147.9."},{"metadata":{"collapsed":true,"_cell_guid":"e2040b9c-2340-4b1e-93ae-f17a45af9b19","_uuid":"0e3dfd50f382a39fd7ba59237c78936ae29a393f"},"cell_type":"markdown","source":"### Lumbar lordosis angle\nThe lumbar lordosis angle is the angle formed by the intersection of two planes.\n* The first plane is the one created following the upper surface of the L1 vertebra.\n* The second plane is created by following the upper surface of the S1 vertebra.\n\n![](http://sittingsafely.com/wp-content/uploads/2013/07/Lumbar-lordosis.png)\n"},{"metadata":{"_cell_guid":"48d4d478-aaee-40b4-b04f-1f6ccf9a05d3","_uuid":"b5b5613e7931daca50e987d54f2ae7d2baf731a5"},"cell_type":"markdown","source":"Let's see its distribution."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"80df90ba-fd66-4054-9fa4-2a16a102e17c","_uuid":"90ed71edc7d3b6911ef9bbc86b54baf439802428","trusted":true},"cell_type":"code","source":"hist_graph('Lumbar_lordosis_angle')","execution_count":19,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"d7d38844-e60d-4e78-9e6c-445f69af84b8","_uuid":"c3aae2f0f6577972d68d616ca8c28510b10d8b97","trusted":true},"cell_type":"code","source":"compare_df('Lumbar_lordosis_angle')","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"86d0a175-4a1d-45ca-870b-de7a8a9d70c5","_uuid":"ee7edea999f80187423b53f24617b6685d77a7dc"},"cell_type":"markdown","source":"We recognize here what we've been seeing previously in the most of the other parameters that we have studied.\n* Higher values for the Abnormal class ( right-shift ).\n* Higher spread for the Abnormal class.\n* Normal class almost contained by the Abnormal class, giving us here just an upper bound where all patients with above  ~90 wil be classified as Abnormal."},{"metadata":{"_cell_guid":"f90ae9b4-7651-4f04-bc54-4d99bd2487a0","_uuid":"b970cdfc15b15dac253c980fbbc8146d7b1eb47d"},"cell_type":"markdown","source":"Looking at the pairplot and at the correlation graph, we have drawn the most relevant distributions so far. The others parameters present very similar distribution for both Normal classes and Abnormal classes and their univariate study would not give us much insight.\n\nBoth our correlation plot and the shallow decision tree we used gave us an idea of what parameters were the most relevant to determine the Attribute of a patient. Let's try to create a new variable from the previous ones that will have even more weight for the predictions.\n\n## Feature Engineering\n\nFeature engineering is the process of creating new features from data to achieve better modelisation of the problem. It can be done before or after fitting the dataset to a simple model to see how each feature would impact on the quality of the model, but in the case of binary classification, we will see clearly in the histogram distribution if a new feature helps to separate the classes even more.\nThere are **lots** of ways to generate new features : \n* From domain knowledge\n* From observation of the data\n* From dimensionality reduction techniques such as PCA\n* Just creating combinations of our current parameters\n\nNow, I have no domain knowledge, and let's assume that we haven't really looked at the data yet for the fun of it, we'll try to create new features through PCA, LDA and polynomial combinations of our parameters\n\n### PCA\n\nIn simple words, Principal Component Analysis or PCA tries to create linear combinations of the features in our data set to represent its variance as well as possible.  \nFor us, it can be a tool to try to create a feature that will separate the target as well as possible. That is, if PCA manages to represent the data set accurately."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"01d29a1b-7f94-412d-8d5d-bcc1b08aa998","_uuid":"3a4e3772b7f7741569f496130b339febc4d0f614","trusted":true},"cell_type":"code","source":"# Feature scaling\nsc = MinMaxScaler()\nX_std = sc.fit_transform(X)\n\n# Creating the PCA\npca = PCA(n_components=3)\n\n# Fitting the PCA to the data set\npca.fit(X_std)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"9e4a3318-a609-4ae9-b6ff-3dce7e5cabcf","_uuid":"c920a91051cfde5fe416ccec7a8e5a5f612ebdbd"},"cell_type":"markdown","source":"A good way to verify the quality of the PCA is by checking its explained_variance. This will show how much of the variance of the dataset the PCA manages to express for each of its component, thankfully our PCA object has a method `.explained_variance_ratio_`that does exactly this."},{"metadata":{"_cell_guid":"570937d8-1e95-4b49-bf64-fa02d64d2636","_uuid":"2cf7006ca78376240b1cdb548726859408d0aa59","trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":22,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_cell_guid":"444bbac4-562e-4ee8-9281-bab9a28cbb40","_uuid":"0c6901aaa46b88b6ab1c6744ee98b97242756ed4"},"cell_type":"markdown","source":"So this is **pretty terrible** to be honest.  \nEven after adding the three components together we don't even get to 50% of explained variance for the data set...  \nIt doesn't mean that we should throw PCA away for this problem, other implementations of the PCA with the kernel trick ( kPCA in sklearn ) are also available, but I want to keep this notebook somewhat short so you'll have to look up this one by yourself."},{"metadata":{"_cell_guid":"7b1915b7-3723-4002-b173-22d486b6f766","_uuid":"e2d5f5daaad5f589b3832db533b771737b6e383f"},"cell_type":"markdown","source":"### Polynomial Features\nCreating new features from polynomial combinations and then filtering through them is also way of doing feature engineering. That being said, using `PolynomialFeatures`can be computationally expensive and slow. The reason behind this is that it is going to create all the combinations possible between all the selected features, including ( if you wish so ) power features from the previous ones ( understand squared, cubed, etc.) .\n\nHere I specified that I wanted a maximum degree of 2 and I have 12 parameters.\n\nI can expect:  \n* ${12 \\choose 2}=66$ New interacting features,  \n* $12*1=12$ Squared features, and  \n* $12$ of our old features.\n\nThat gives us a total of 90 features, let's check it."},{"metadata":{"_cell_guid":"5e35ae4f-d05e-4fd1-9a50-210e93916b40","_uuid":"9598f1ed9c2b8bd98024fb96bafc27237617ebb6","trusted":true},"cell_type":"code","source":"PF = PolynomialFeatures(degree=2, include_bias=False)\nX_std_pf= PF.fit_transform(X_std)\nnew_feats = PF.get_feature_names()\nX_std_pf.shape","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"6582d6bb-8312-4402-a53e-2c81c1273292","_uuid":"0ad7acdbac54cbcff3b8e58507d2658f2d7a0825"},"cell_type":"markdown","source":"Everything checks out with what I thought and that's great, **but** more features doesn't always mean a more accurate model. Some of them will introduce noise, some of them will make us overfit, some of them are just straight out bad.  \n\nNow there are a lot of different methods to evaluate the quality of a given feature in regards to its target, and a lot of them happen actually **after** fitting an algorithm to the new data set and seeing an improvement or a loss in the quality of the algorithm ( here we will measure the quality of a classifier via the roc-auc value).\n\nFor the moment I will focus on trying to evaluate the quality of the new features **_a priori_** following the $\\chi^2$ evaluation of these features and by keeping only the 10 best results."},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"b9182cd9-05c1-44dd-960e-3d6fbb7bfb97","_uuid":"53262d24e8aae3c0019fb5c2a50a8f9a8266589f","trusted":true},"cell_type":"code","source":"Kbest =  SelectKBest(chi2, k=10)\nX_std_pf_chi10 = Kbest.fit_transform(X_std_pf, y)\nselected = Kbest.get_support()","execution_count":24,"outputs":[]},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"515c7165-66b5-4c9a-ad00-5be4e5267ea4","_uuid":"7a4bf66da25871d491c7ab829e5095b8ac6c5d53","trusted":true},"cell_type":"code","source":"features=[]\nfor feat, sel in zip(new_feats, selected) : \n    if sel == True :\n        features.append(feat)\n\nfeat_col=[]\nfor i in features :\n    split = i.split()\n    if len(split)==1 :\n        pow = split[0].split('^')\n        if len(pow) == 1:\n            nb =int(''.join([j for j in pow[0] if j.isdigit()]))\n            col=data.columns[nb]\n            feat_col.append(col)\n        else :\n            nb =int(''.join([j for j in pow[0] if j.isdigit()]))\n            col=data.columns[nb]+'^'+pow[1]\n            feat_col.append(col)\n    else:\n        clean =''.join([j for j in i if j.isdigit()])\n        col=data.columns[int(clean[0])]+'*'+data.columns[int(clean[1])]\n        feat_col.append(col)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"4bef202c-c14d-4ebe-a511-930ebf6d9ad1","_uuid":"4e66973a8b9e48d9264c71d80e870b5973ba6505"},"cell_type":"markdown","source":"The feature selected by the $\\chi^2$ are as follow : "},{"metadata":{"_cell_guid":"38566358-81ca-4653-b34b-c291c58219c6","_uuid":"0fcd75bcd443c5462af2747f0bfc0f09cecc1b10","trusted":true},"cell_type":"code","source":"feat_col","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"95f97953-bc2e-4c41-8827-f6612325be11","_uuid":"4b826f5cfabad23a9b3f4ce3e9f0cffe4ab0c6dc"},"cell_type":"markdown","source":"### Boxing Features\nIt is another technique that can be used to create new features. To do this we use the things we learned through our previous EDA to create new categorical features telling us if a value is above, below, or in between certain values.\n\nFor example, when we were looking at the distribution of the Sacral_slope amongst the classes, we realised that there was no Normal patient with a Sacral_slope value above 67.  \nThus, we can create a category 'ss>67' that would take 1 as a value if it is True and 0 if it is False.\n\nLet's try to implement a few of these boxing features."},{"metadata":{"collapsed":true,"_cell_guid":"da1666ee-3f10-4138-99c2-407c5ef854a3","_uuid":"dd2d4a5680a9a84e7c51f27d670c33bef96e285b","trusted":true},"cell_type":"code","source":"box_deg = (data.Degree_spondylolisthesis > data[data['Attribute']==0].Degree_spondylolisthesis.max()).map({False: 0, True: 1})\nbox_ss  = ((data.Sacral_slope > data[data['Attribute']==0].Sacral_slope.max()) & (data.Sacral_slope > data[data['Attribute']==0].Sacral_slope.min())).map({False: 0, True: 1})\nbox_pi  = (data.Pelvic_incidence > data[data['Attribute']==0].Pelvic_incidence.max()).map({False: 0, True: 1})\nbox_pt  = ((data.Pelvic_tilt > data[data['Attribute']==0].Pelvic_tilt.max()) & (data.Pelvic_tilt > data[data['Attribute']==0].Pelvic_tilt.min())).map({False: 0, True: 1})\nbox_pr  = ((data.Pelvic_radius > data[data['Attribute']==0].Pelvic_radius.max()) & (data.Pelvic_radius > data[data['Attribute']==0].Pelvic_radius.min())).map({False: 0, True: 1})\nbox_lla = ((data.Lumbar_lordosis_angle > data[data['Attribute']==0].Lumbar_lordosis_angle.max()) & (data.Lumbar_lordosis_angle > data[data['Attribute']==0].Lumbar_lordosis_angle.min())).map({False: 0, True: 1})\nX_box = np.array([box_deg,box_ss,box_pi,box_pt,box_pr,box_lla]).reshape(309,6)","execution_count":27,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c6b00c51-7c29-4bed-af43-cfb5a5aa93a2","_uuid":"4505fceb412df83abb10f4ad0828db886cd97fc9","trusted":true},"cell_type":"code","source":"# Adding the boxing features to the other ones\nX_std_box = np.hstack([X_std,X_box])\nX_std_pf_chi10_box = np.hstack([X_std_pf_chi10,X_box])","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"8a5281b3-6c3e-42c6-812a-27d256a032a8","_uuid":"f76dc95d7433940a4eed65f2c0ec3ee0902dd744"},"cell_type":"markdown","source":"### Effect on a simple model\nNow that we have created these new features and these new sets of predictors, we would like to see their effect on the AUC of a simple classifier, since we used a Decision Tree earlier, we'll keep using it for simplicity reasons.  \nBecause the scores can have a high variance due to the randomization of some factors in the construction of the algorithm, the training data, and the testing data, we will cross-validate all the results we have to stabilize this variance and get insights from these results."},{"metadata":{"scrolled":true,"_cell_guid":"b0c8f87b-ed58-494a-9d2b-7a214f1d5876","_uuid":"dd5fb7d32dbc3e4699cf9b04e21207ef5e462cb7","trusted":true},"cell_type":"code","source":"# Creating a list through which we'll iterate the Decision Tree\nX_list=[X_std, X_std_pf, X_std_pf_chi10,X_std_box, X_std_pf_chi10_box]\nresults = []\n\nclf = DecisionTreeClassifier(random_state=42)\n\n# Getting cross-validated scores for each of the new data sets\nfor X_set in X_list :\n    clf.fit(X_set,y)\n    y_pred = clf.predict(X_set)\n    rez = cross_val_score(clf, X_set, y, scoring='roc_auc', cv=100 )\n    results.append(rez.mean())\nresults","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"c55ab312-d112-4914-8198-cdb2f5c7ea45","_uuid":"3b1b6f2e5768371cd4624426d2c895beb86d20a3"},"cell_type":"markdown","source":"Let's take a look at the results above and try to understand them a little bit.\n* The standard data set allows for a CV-AUC of 70.75%\n* After adding 88 polynomial features it goes up ~2.25%\n* After adding the boxing features it goes up ~2.5%\n\n* Filtering the polynomial features with the standard data set make it go up by ~4.5%\n* Adding the boxing features make the latter go up ~2.25%\n\nFirst of all, even though this is cross validated and verified and all, after running it multiple times I was still getting a variance higher than I'd like on my results, so this is to take with a **grain of salt**. One thing that systematically came back though was the fact that either the filtered polynomial features or the filtered polynomial features + boxing would get the highest scores.\n\nAs we expected, the creation of polynomial features **increased** the base score, but not from much... How so ? It was certainly introducing a lot of **noise** and useless extra feature, this explanation is backed by the facte that once filtered through the $\\chi2$ the score goes up **drastically**.\n\nI am quite satisfied with the effect of the boxing features on the AUC score. Compared to the computational power required to first create all the polynomial features and then filter them, creating the boxing features was a **walk in the park** and their effect was definetely non negligeable on the outcome.\n\nNow I want to keep this dataset simple without too many features, and even if the score is slightly better once the boxing features are added, I will use the filtered polynomial features without the boxing features for the modelling part of this notebook.\n\nHere are the **winning** predictors with the targe in a DataFrame : "},{"metadata":{"_kg_hide-input":true,"_cell_guid":"a7000d48-de1d-410c-84bb-cc2f24765812","_uuid":"f1a15d83b5afba65b29ed9653336c0296697312b","trusted":true},"cell_type":"code","source":"df_new = pd.DataFrame(X_std_pf_chi10, columns=feat_col)\ndf_new['Attribute'] = data['Attribute']\ndf_new.head()","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"f18f9299-6b76-4358-ae59-5c7deb85d144","_uuid":"7d906b59a82ad0994e0e2d491681c139368a0bb8"},"cell_type":"markdown","source":"## Modelling\n### Choosing Interpretability\n\nSome machine learning models can help people understand a problem more fully or in a different manner through the way they are constructed. These models are **interpretable** .  \nTheir decision process can be easily showed and understood to a human. Interpretable models can provide great insight, this is exactly what we did in the first part when building a shallow decision tree to then examine its decision process. Thanks to the shallow decision tree, we knew immediatly what parameters were the most relevant to our problem.  \nOne drawback is that most of the time these models are inherently more simple than others \"black-box\" models, and their results usually can't compare to more complex algorithms that lose in interpretability.  \nIn this part, we will use a **Decision Tree** and a **Logistic Regression** algorithm, tune them to maximize AUC,  and interpret their architecture or coefficients.\n\n### Decision tree\n\nThe last time we used a Decision Tree with the newly engineered features we had an AUC ~80% . Now we want to get a better result and tune the algorithm to maximize the AUC. To do so, we will use the **GridSearchCV** from sklearn. It allows the construction of a parameter grid that will be applied and tested on the metrics we have specified ( in our case AUC ). Now, one has to be careful with GridSearch as it will train as many models as there are combinations of parameters in the parameter grid. That means it can take some time and can be computationally expensive. Since the data set is pretty small here, there is no problem with testing a few parameters at the same time. Once the GridSearch has finished testing everything, we look for the best parameters and ta-da, we tuned our algorithm."},{"metadata":{"_cell_guid":"f57649c5-e45e-4bcd-b10d-3aee0a82d2f9","_uuid":"6e1e8d693e2df81be50f32da68532e70448ce3d9","trusted":true},"cell_type":"code","source":"param_grid = {'max_depth': np.arange(1, 10),\n             'min_samples_leaf' : np.arange(1, 10),\n             'max_features' : ['auto','sqrt','log2',None],\n             'random_state' : [37,]}\n\ntrees = GridSearchCV(DecisionTreeClassifier(), param_grid, scoring='roc_auc')\ntrees.fit(X_std_pf_chi10, y)\n\nprint(\"The best parameters are : \", trees.best_params_,' giving an AUC : ', trees.best_score_)","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"e74e265d-da8a-4117-b4b9-fc5d7ec47c61","_uuid":"e0241cf335639d212f0db99547dc7560457cebd0"},"cell_type":"markdown","source":"Since our untuned Decision Tree, we got a **significant** increase for the AUC.  \nAs we've seen in an earlier part of this notebook, one of the **greatest** things with Decision Tree is that we can actually plot their architecture on a simple graph and interpret the results directly. This is exactly what we are going to do here :"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"a8808ddd-85a1-4626-89f2-d9495819ec44","_uuid":"06f17d0ec4e2dc609a8a60ec6f2a5a102cfd6a38","trusted":true},"cell_type":"code","source":"clf_tree = trees.best_estimator_\n\ndot = tree.export_graphviz(clf_tree,out_file=None,\n                         feature_names=df_new.columns[:-1],  \n                         class_names=['Normal','Abnormal'], \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot) \ngraph.render(\"back problems2\")\ngraph","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"c855004b-d772-48fc-bb4d-19b3dff313de","_uuid":"6ad8d432977e04c038be6668cbe713d3a20b78b6"},"cell_type":"markdown","source":"From the top to the bottom we go from the most important parameters to the least important ones ( in regards to the Decision Tree algorithm ).\n\nAs such : \n* Degree_spondylolisthesis * Pelvic_incidence is the most influencial factor towards having back problems ( we've seen this result previously )\n* Degree_spondylolisthesis is the second one\n\nIf you paid attention to all the parameters we were using, you probably realised that some parameters were missing from this tree.  \nThe Decision Tree as implemented in sklearn also comes with a method : `.feature_importances`. The name is pretty self-explanatory, let's dig into this."},{"metadata":{"_cell_guid":"6d2ae0da-1a3a-4399-8f80-8c66302bde5a","_uuid":"03a4147e5e631872f0f9735992f960b6819e6061","trusted":true},"cell_type":"code","source":"clf_tree.feature_importances_","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"6e67db22-2184-4168-8e32-3f7010ce3068","_uuid":"a73115878dfbe9af9a0ad8911c76726e0f6a018b"},"cell_type":"markdown","source":"We see that five values are set to 0, they correspond to the columns : \n* Pelvic_incidence * Pelvic_tilt\n* Pelvic_tilt^2\n* Pelvic_tilt * Degree_spondylolisthesis\n* Lumbar_lordosis_angle^2\n* And Lumbar_lordosis_angle * Degree_spondylolisthesis\n\nThis isn't a mistake, it is simply due to the shallowness of the tree. Remember, the best parameters set a `'max_depth': 3`. With a deeper tree, these features would get a non-zero value for their importance, but probably still pretty close to zero. The fact that while doing GridSearch we got the best results for a shallower tree may show that the inclusion of the current zero features would have lead to overfitting the data set and thus reduce the AUC."},{"metadata":{"_cell_guid":"1d17f79f-4c8a-4d08-90bf-2d15597c1cd6","_uuid":"3fe2b92b8112a383912bcae212b440d3aeb3a697"},"cell_type":"markdown","source":"### Logistic Regression\nWe're also going to use GridSearchCV to optimize the AUC of the Logistic Regression, but the difference with the Decision Tree is that this time we will interpret the coefficients given to each feature by the algorithm instead of interpreting the architecture of it."},{"metadata":{"_cell_guid":"285c36f5-5555-40f9-a366-6d49830da235","_uuid":"f4539ca28a57debb56e2b6dc7f4eeec8bc008912","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"\nparam_grid = {'penalty': ['l1','l2'],\n             'tol'     : [1e-5, 1e-4, 1e-3, 1e-2],\n             'C'        : [1.0, 10.0, 25.0, 50.0, 100.0, 200.0, 500.0, 1000.0] ,\n             'solver'    : ['liblinear',  'saga'],\n             'random_state' : [37,],\n             'max_iter' : [700,]}\n\nlogit = GridSearchCV(LogisticRegression(), param_grid, scoring='roc_auc',verbose=0)\nlogit.fit(X_std_pf_chi10, y)","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"5135b2eb-a530-4a8a-a579-d1693599e673","_uuid":"4161c93e1f33e98eda2574a1292f0e2c64621aab","trusted":true},"cell_type":"code","source":"print(\"The best parameters are : \", logit.best_params_,' giving an AUC : ', logit.best_score_)","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"5c470585-de0d-4ff4-9681-fcf18cc4e774","_uuid":"2ae510d3aaa7e4b82959af097988922f727cfd02"},"cell_type":"markdown","source":"For the tuned Logistic Regression, we get an AUC of ~87.3%, not bad.  \nLet's see how we got to this result and check out the coefficient of each feature."},{"metadata":{"_cell_guid":"278dc6fa-257d-4b07-a49d-525684597db4","_uuid":"2f951ea4bdbc0c542ae211530dd63da114175fdb","trusted":true},"cell_type":"code","source":"clf = logit.best_estimator_\nclf.coef_","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"a225e422-b291-4e19-9c6c-c499cc320944","_uuid":"68050c679f8f2888ab823631cd01746cb404685e"},"cell_type":"markdown","source":"The array above represents the coefficients given to each of the parameters inside of the logistic regression. Their absolute value give an idea of the influence of each parameter while their sign tells us in which way they will influence the outcome.  \n\nAs such : \n* Degree_spondylolisthesis * Pelvic_incidence is the most influencial factor towards having back problems ( we've seen this result previously )\n* Degree_spondylolisthesis  is the second one\n* Pelvic_incidence * Pelvic_tilt_angle is the most influencial factor towards **not** having back problems closely followed by\n* Pelvic_tilt * Degree_spondylolisthesis .\n\nThis information from the Decision Tree and from the Logistic Regression could help a practitioner decide the severity of one case or help define more specifically what the problem is and how to fix it for example.\n\nIt is interesting to notice that the two classifiers don't necessarily \"agree\" on the importance of each feature. This is due to the different way they compute and classify each case and hence fit to the data set. That being said, combining the interpretation of these different models can help refine even more our understanding of the problem on a human base."},{"metadata":{"collapsed":true,"_cell_guid":"e076895f-8500-4b79-9b94-8ce6edb789c3","_uuid":"ec31d5270ecbe5f6781398a294cd4a770adef961"},"cell_type":"markdown","source":"## Visualisations\nI have beeng playing with plotly for a while now and I really find that the interactivity of the plots adds so much to the reading and understanding experience of this very plot. It allows also to single out and identify outlier points in many cases for extended study later.  \nWe'll see some outlier points in the next visualisations in which a medical practitioner could be interested.  \nIf this patient is classified as normal, why is he so far from the normal cluster of points ?  \nIn one of my previous studies, after using an auto-encoder to perform dimmensionality reduction, I was able to redefine and create new classifications for a disease.  \nAs such, I believe that the study of different visualisations of the data sets through dimensionality reduction can also carry a great values\n\n### PCA on the new data"},{"metadata":{"_cell_guid":"737fb808-acc9-43d0-8848-a9681ee43ac1","_uuid":"6bb89daca965d7dc9cebdb294d5857225ad2d555","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"init_notebook_mode(connected=True)\n\nX = df_new.iloc[:,:-1]\ny = df_new.iloc[:,-1]\n\npca = PCA(n_components = 3)\nX_PCA = pca.fit_transform(X)\n    \nxs = X_PCA[:,0]\nys = X_PCA[:,1]\nzs = X_PCA[:,2]\n\n# Recreating the df with the new coordinates\ndf = pd.DataFrame(dict(x=xs, y=ys, z=zs, Attribute=y)) \nl = []\nnames = ['Normal','Abnormal']\n\nfor i in [0,1]:    \n    trace= Scatter3d(\n        x= df[df['Attribute']==i]['x'],\n        y= df[df['Attribute']==i]['y'],\n        z= df[df['Attribute']==i]['z'],\n        mode= 'markers',\n        marker= dict(size= 5,\n                    line= dict(width=1),\n                    color= i,\n                    colorscale='Jet',\n                    opacity= 0.8\n                   ),#name= y[i],\n        name = names[i],\n        text= df[df['Attribute']==i].index,# The hover text goes here...\n        hoverinfo = 'text+name'\n    )\n\n    l.append(trace)\n\nlayout= Layout(\n    title= '3D Representation of the patients characteristics using PCA',\n    hovermode= 'closest',\n    showlegend= True)\n\nfig= Figure(data=l, layout=layout)\nplotly.offline.iplot(fig)\nprint('The PCA explains the variance of the data by {:3f}%'.format(100*pca.explained_variance_ratio_.sum()))","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"47a25664bc83b67bc29db13e1c824c063b0a3c87"},"cell_type":"markdown","source":"This time, PCA has performed very well, we can see that by the high explained variance ratio that was computed with only 3 components.  \nNow what can we see from this ?\n* The normal class is somewhat clustered together with a few outlier points\n* Some points in the abnormal class are mixed within what seems to be the normal cluster, that shows us overall similarity to the normal points with probably just one factor making them abnormal ( certainly the Degree Spondylolistehsis )\n* The abnormal class is very spread out, we can assume that the points the fursthext from all the other poiunts are extreme cases with extreme values.\n\n### TSNE"},{"metadata":{"_cell_guid":"f8415ebc-d959-4bb7-ae9a-1cf6e068dbfd","_uuid":"eda26afba85b0054946f538af633f3154643b2b4","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=3,learning_rate=115.0)\nX_tsne = tsne.fit_transform(X, y)\n\n\nxs = X_tsne[:,0]\nys = X_tsne[:,1]\nzs = X_tsne[:,2]\n\n# Recreating the df with the new coordinates\ndf = pd.DataFrame(dict(x=xs, y=ys, z=zs, Attribute=y)) \nl = []\nnames = ['Normal','Abnormal']\n\nfor i in [0,1]:    \n    trace= Scatter3d(\n        x= df[df['Attribute']==i]['x'],\n        y= df[df['Attribute']==i]['y'],\n        z= df[df['Attribute']==i]['z'],\n        mode= 'markers',\n        marker= dict(size= 5,\n                    line= dict(width=1),\n                    color= i,\n                    colorscale='Jet',\n                    opacity= 0.8\n                   ),#name= y[i],\n        name = names[i],\n        text= df[df['Attribute']==i].index,# The hover text goes here...\n        hoverinfo = 'text+name'\n    )\n\n    l.append(trace)\n\nlayout= Layout(\n    title= '3D Representation of the patients characteristics using TSNE',\n    hovermode= 'closest',\n    showlegend= True)\n\nfig= Figure(data=l, layout=layout)\nplotly.offline.iplot(fig)\n","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"a5c3317334e7363677f8fb36e450d9dc911df2cc"},"cell_type":"markdown","source":"I always liked the results of a t-SNE dimensionality reduction...  \nCompared to the results we've had for the PCA, the separation between clusters makes itself clearer. Yet we still have an important mix of classes in what should be the normal cluster. It is interesting to note that the outlier points in this representation are not the same as the outlier points we've had using the PCA. Different techniques different results, but it would still be interesting to compare these outlier points wioth the rest of the data set.\n### Spectral Embedding"},{"metadata":{"_cell_guid":"7e02b840-2aea-4751-9af7-40c1b96668ce","_uuid":"170acd95bcd71cee89cbf1e10277cfd18e68e3c6","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.manifold import SpectralEmbedding\n\nSE = SpectralEmbedding(n_components=3)\nX_SE = SE.fit_transform(X, y)\n\n\nxs = X_SE[:,0]\nys = X_SE[:,1]\nzs = X_SE[:,2]\n\n# Recreating the df with the new coordinates\ndf = pd.DataFrame(dict(x=xs, y=ys, z=zs, Attribute=y)) \nl = []\nnames = ['Normal','Abnormal']\n\nfor i in [0,1]:    \n    trace= Scatter3d(\n        x= df[df['Attribute']==i]['x'],\n        y= df[df['Attribute']==i]['y'],\n        z= df[df['Attribute']==i]['z'],\n        mode= 'markers',\n        marker= dict(size= 5,\n                    line= dict(width=1),\n                    color= i,\n                    colorscale='Jet',\n                    opacity= 0.8\n                   ),#name= y[i],\n        name = names[i],\n        text= df[df['Attribute']==i].index,# The hover text goes here...\n        hoverinfo = 'text+name'\n    )\n\n    l.append(trace)\n\nlayout= Layout(\n    title= '3D Representation of the patients characteristics using Spectral Embedding',\n    hovermode= 'closest',\n    showlegend= True)\n\nfig= Figure(data=l, layout=layout)\nplotly.offline.iplot(fig)\n","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"8e8e3341408f8aedba3339cf452c8f4746eadbce"},"cell_type":"markdown","source":"Now that's a funny shape...  \nThis is the clearest classes separation we've seen here. The \"nose\" that comes out seems very class pure with only 3 points classified as normal in a cluster of **only** abnormal points. In this case, studying these points included in the abnormal cluster could also provide insight on this _special_ cases. But there is much more that we can do with this !  \nSee the triangular shape of the \"base\" ? We could study how the different characteristic of a patient change depending on which tip of the triangle is the closes to him, giving yet another visual representation of possible sub-classes in the back pains.  \nWe still see that the normal points anre mixed in with the abnormal points, that reinforces my hypothesis that some abnormal points were classified as such just becasue of one single parameter that was too high, something to look into...\n\n## Conclusion\n\nI hope you enjoyed reading this notebook as much as I enjoyed writing it, feel free to get inspired from it and use the code in your own notebooks, I'd love to see what you make of it.  \nIf you learnt something or just had a good reading experience, don't forget to upvote and check out my other kernels !"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}