{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# classify mushrooms as either poisonous or non-poisonous\n# using data from https://www.kaggle.com/uciml/mushroom-classification","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# import essentials\nimport numpy as np\nimport pandas as pd\n\n# set randomness for reproducability\nseed = 42\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get our data\ndata = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")\n# give it a quick overview\nprint(data.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# use the documentation to clean our data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data types of our data\nprint('\\ndata types of our dataset')\nprint(str(data.dtypes) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# unfortunately our variables are all classified as\n# objects with string names, but we need most of them as categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# grab our column names to iterate over\ncolumns = data.keys()\n# changing almost all to categorical variables\n# will make a few exceptions\ncolumn_exceptions = ['class','bruises']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# convert our target variable \"class\" to False for\n# non-poisonous and True for poisonous\n# do the same for bruises\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"newData = pd.DataFrame()\nnewData['class'] = (data['class'] == 'p')\nnewData['class'] = newData['class'].astype(int)\nnewData['bruises'] = data['bruises'] == 't'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# change all non-exception columns to categorical vars\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['class','bruises'])\ndummy_data = pd.get_dummies(data,drop_first=True, dtype='int')\nnewData = pd.concat([newData['class'],newData['bruises'],dummy_data],axis=1)\ndata = newData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check dtypes again\nprint('\\nCleaned data types')\nprint(data.dtypes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Success!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# next we split into train / test data for evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_independent = data.iloc[:, 1:]  # X\ndata_dependent = data.iloc[:, 0] # y\nX_train, X_test, y_train, y_test = train_test_split(\n    data_independent,data_dependent.values,shuffle=True,\n    random_state=seed, test_size=.2, stratify=data_dependent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import scoring metrics for evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# define our custom scoring methods","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# our cross validation strategy\ndef log_loss(model, X=X_train, y=y_train, _scoring='neg_log_loss') :\n    _kfold = KFold(n_splits=5)\n    _score = -cross_val_score(model, X, y, cv=_kfold, scoring=_scoring)\n    return _score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and the ratio that we predicted correctly\ndef final_accuracy(model,_X_train=X_train,_y_train=y_train,\n                   _X_test=X_test,_y_test=y_test) :\n    model.fit(_X_train,_y_train)\n    _y_hat = model.predict(_X_test)\n    _final_score = np.sum(_y_hat == _y_test) / len(_y_test)\n    return _final_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now make linear estimations and evaluate if we need\n# a more complex model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# define models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\nLoading and scoring models...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(random_state=seed)\nsvc = SVC(probability=True,random_state=seed,kernel='rbf')\nforest = RandomForestClassifier(random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using log loss, what score do we get on our test set?\nscore = log_loss(logit)\nprint('\\nLogit score: {:.4f} ({:.4f})'\n      .format(score.mean(),score.std()))\nscore = log_loss(svc)\nprint('\\nSVC score: {:.4f} ({:.4f})'\n      .format(score.mean(),score.std()))\nscore = log_loss(forest)\nprint('\\nRandom Forest score: {:.4f} ({:.4f})'\n      .format(score.mean(),score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# extremely accurate scores on all. What's our prediction rate?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\naccuracy = final_accuracy(logit)\nprint('Logit final accuracy: {:.4f}'.format(accuracy))\naccuracy = final_accuracy(svc)\nprint('\\nSVC final accuracy: {:.4f}'.format(accuracy))\naccuracy = final_accuracy(forest)\nprint('\\nRandom Forest final accuracy: {:.4f}'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 100% accuracy on several methods! Wow, machines are great mycologists!\n# let's analyze the data to get some intuitions on our results\n# let's use our logostic regression since it got everything right\n# while being quite fast","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logit.fit(X_train,y_train)\npredictions = logit.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# now perform PCA to view how our machine \n# seperates the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create our PCA and fit it to the data\npca = PCA(n_components=2, random_state=seed)\npca.fit(X_test)\nX_test_pca = pca.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create our PCA dataframes\npca_df = pd.DataFrame(data=X_test_pca, columns=['PCA 1', 'PCA 2'])\ny_test_series = pd.DataFrame(y_test, columns=['target'])\nfinal_df = pd.concat([pca_df, y_test_series],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import our plotting methods\nimport matplotlib.pyplot as plt\n\n# create our figure\nfig :plt.Figure = plt.figure(figsize=(8,8))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('PCA 1')\nax.set_ylabel('PCA 2')\nax.set_title('PCA Graph', fontsize=20)\ntargets = [0, 1]\ncolors = ['b','r']\n\nfor target, color in zip(targets, colors) :\n    kept_indicies = predictions == target\n    ax.scatter(final_df.loc[kept_indicies, 'PCA 1'],\n               final_df.loc[kept_indicies, 'PCA 2'],\n               c = color,\n               s= 50)\nax.legend(['Non-Poisonous','Poisonous'])\nax.grid()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}