{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"248211ad-2329-2bfa-ae06-fa049f6585d1"},"source":"Multi linear regression and some feature selection on world happiness data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c87619b-1e60-8798-ab29-f10cb2f0e9e3"},"outputs":[],"source":"#importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#importing the dataset\ndataset = pd.read_csv('../input/2016.csv')\n\n#Splitting up the dataset\nx = dataset.iloc[:, 6:].values #take all the data from col 6 onwards\ny = dataset.iloc[:, 2].values #Happiness rank\nGDP = dataset.iloc[:, 6].values \nFamily = dataset.iloc[:, 7].values \nHealth = dataset.iloc[:, 8].values\nFreedom = dataset.iloc[:, 9].values\nCorruption = dataset.iloc[:, 10].values\nGenerosity = dataset.iloc[:, 11].values\nDystopia = dataset.iloc[:, 12].values\n\n#splitting data to train/test sets\nfrom sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"624cbd82-a973-eddd-f9ce-3af1f355fa99"},"outputs":[],"source":"#Fitting mult linear regre to training set!!\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train, y_train)\n\n#Y_pred for multi-linear\ny_pred = regressor.predict(x_test)\ny_pred"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1097df23-512a-e306-b913-2ee41ae7d5ad"},"outputs":[],"source":"y_test"},{"cell_type":"markdown","metadata":{"_cell_guid":"90e80e90-e255-cc35-6c6b-f60abfca1448"},"source":"As you can see y_pred versus y_test is quite accurate in its predictions. Now lets look at the correlations between particular features of x one by one."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c782dfa-b7d2-8f8a-7676-9e0baff3bf21"},"outputs":[],"source":"plt.scatter(GDP, y)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8122e40-d1e5-6935-f714-52119dc85942"},"outputs":[],"source":"plt.scatter(Family, y)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d8d823f-a16f-2cdf-99d6-dbb2b47e5ea7"},"outputs":[],"source":"plt.scatter(Health, y)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86f0c3da-8627-223e-8dee-b07105d04e56"},"outputs":[],"source":"plt.scatter(Freedom, y)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d4d94c4-6ac5-c2e9-d3af-c15ccad8b96a"},"outputs":[],"source":"plt.scatter(Corruption, y)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c87ee988-d2bb-8a1d-d679-e337c2840c9f"},"outputs":[],"source":"plt.scatter(Generosity, y)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66a2e52c-a044-9133-4c89-4cad3f1b173e"},"outputs":[],"source":"plt.scatter(Dystopia, y)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"dbbb87da-c47a-e8b2-cc9a-1a28b9c3313b"},"source":"The above graphs show that while most features of X correlate with happiness rank in some way, some correlate much more significantly that others. GDP and Health seem to have a strong correlation with the smallest amount of variance whilst Freedom, and Family have a strong correlation but with very high variance. Corruption shows something different to those features stated above. It seems to have a non-linear trend, some sort of exponential curve. This means that a linear regression doesn't fit this data well and there would be a large sum of squares error. Interestingly generosity seems to have no trend, whether your happiness is low or high there is nonetheless a large variance in generosity. Below we will try some methods for fea"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a622e1af-cfe9-cdc5-b91c-e2c5230bbc3e"},"outputs":[],"source":"#Firstly we will try a step wise regression method know as backward elimination\nimport statsmodels.formula.api as sm\n\n#We need a constant x0 = 1 for this to work\nx = np.append(arr = np.ones((157, 1)).astype(int), values = x, axis = 1)\n\n#backward elim we will set P = 0.05 as our threshold for elimation\nx_opt = x[:, [0, 1, 2, 3, 4, 5, 6, 7]]   \nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"429d2611-9a98-557b-4e19-b548b3415428"},"source":"In stepwise regression we first think of a P value to use as a threshold for elimination of features. For this example we will use p = 0.05 as our threshold. In the above statistical summary you can see that x7 (Dystopia) has a high p value of 0.758. We eliminate the highest p value above our threshold with each step."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7a1df2b-4c81-4ea0-d1cc-17fea1fcd5cc"},"outputs":[],"source":"x_opt = x[:, [0, 1, 2, 3, 4, 5, 6]]  #x7 has been remove from our optimal x\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bae67d38-5657-8aa3-57ba-2d07cda726ad"},"source":"In our 2nd step we can see x6 (Generosity) is 0.089, just above our threshold. We will eliminate it. It should be said that sometimes when the p-value is this close to your threshold it may negatively impact your model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d33be77d-0eb5-d81a-a40c-052893e7e12b"},"outputs":[],"source":"x_opt = x[:, [0, 1, 2, 3, 4, 5]]   #x6 has now been removed\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c38959ba-070c-e577-39c3-b48e4d2d660d"},"source":"Now nothing is above the threshold, these are the features we will keep."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da59a6ee-ce12-a821-7a47-43dd6257b0e5"},"outputs":[],"source":"#Testing on a new y_pred\n#Splitting into train/test set\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x_opt, y, test_size = 0.2, random_state = 20)\n\n#Fitting mult linear regre to training set!!\nfrom sklearn.linear_model import LinearRegression\nregressor2 = LinearRegression()\nregressor2.fit(x_train2, y_train2)\n\ny_pred2 = regressor2.predict(x_test2)\ny_pred2\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbd3f58f-e283-b3de-8c0a-0dbcf550f341"},"outputs":[],"source":"y_test2"},{"cell_type":"markdown","metadata":{"_cell_guid":"21aac854-328f-91e7-0c39-d2d9f8c511d3"},"source":"Now y_pred2 is actually performing worse than y_pred with all the features of x. This is because of essentially a loss of information that would have actually been valuable to the model. There is always a trade off in data science, keeping all of your information versus computational intensity."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2ec6895-fc24-baf6-e9a2-c779c5110cfa"},"outputs":[],"source":"#Now we will try another method: Univariate feature selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nx_new = SelectKBest(chi2, k=5).fit_transform(x, y)\nx.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b431b9ed-1ef3-f01c-fd58-7a65125fb522"},"outputs":[],"source":"x_new.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"2d36b737-6a39-d084-a28a-5a39e5989098"},"source":"The above code reduces the original x down from 9 features to the 5 most impactful ones. Below we will test this on y_pred"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2dfe3580-b1cc-f0ea-c634-a69775df6be8"},"outputs":[],"source":"#train / test split\nx_train3, x_test3, y_train3, y_test3 = train_test_split(x_new, y, test_size = 0.2, random_state = 20)\n\n#Multilinear regression\nfrom sklearn.linear_model import LinearRegression\nregressor3 = LinearRegression()\nregressor3.fit(x_train3, y_train3)\n\n#y_pred\ny_pred3 = regressor3.predict(x_test3)\ny_pred3"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0be4607-60c6-87f5-0178-7be1e3ad356d"},"outputs":[],"source":"y_test"},{"cell_type":"markdown","metadata":{"_cell_guid":"d7900d29-4cad-60f7-afea-9451ae0d8e68"},"source":"Univariate feature selection seems to perform a lot better than backward elimination in this case."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}