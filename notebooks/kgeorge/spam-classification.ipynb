{"cells":[{"metadata":{"_execution_state":"idle","_uuid":"56432db8f8eb839103e4aaaf811ffe229291aba5","_cell_guid":"00e2e6c6-2ad9-403e-a02f-418112cabf3f"},"cell_type":"markdown","source":"This notebook uses a naive bayes classifier and a SVM to classify messages as spam or not spam."},{"metadata":{"_execution_state":"idle","_uuid":"c8e374793f0787dd74998d7fc2ab990b5509103a","_cell_guid":"33f64a51-55d3-4087-a441-b513dd69e6f7","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndata = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')\ndata.loc[(data['v1'] == 'ham'), 'v1'] = 0\ndata.loc[(data['v1'] == 'spam'), 'v1'] = 1\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"c8dd88cf95b4a2e5faa4f394d9b519a068912bbb","_cell_guid":"0b5ff30e-4a5a-4774-997b-8da21dd33b60"},"cell_type":"markdown","source":"As the preview of the data above shows there are three useless columns, these should be removed. I will also rename the remaining columns as \"v1\" and \"v2\" are not descriptive."},{"metadata":{"_execution_state":"idle","_uuid":"58ead51dda478f70994ace0041073083ad5b9c70","_cell_guid":"b2bd472f-6ba0-4f0b-886f-efe08dcebd2c","trusted":true},"cell_type":"code","source":"data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"class\", \"v2\":\"text\"})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"a325e6283fd9e725f9c371cab82ecdef00f166b6","_cell_guid":"defe68d6-bef7-42d1-b615-cfac034dc01b","trusted":true},"cell_type":"code","source":"data['length'] = data['text'].apply(len)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"e3b423ca2c7237e2fb6b3059cdcd4972de300d43","_cell_guid":"e883c2dc-cdfa-47aa-8a83-a41a4acdb71f"},"cell_type":"markdown","source":"In order to apply a model, the necessary preprocessing must be completed. For text classification, usual preprocessing includes removing stop words (words that don't provide useful meaning, i.e. \"and\" \"or\"). Also the characters are converted to a single case (the below function converts to lower case). The function below then stems each word (this means that it replaces a word with the root of that word, for example \"tasted\" or \"tasting\" would become \"taste\").\n\n(Below preprocessing function from Evgeny Volkov)"},{"metadata":{"_execution_state":"idle","_uuid":"c4447358bc18d88759c4a173f33146fedd14b07e","_cell_guid":"8867b48a-338c-45f8-8a65-75f749da22e3","trusted":true},"cell_type":"code","source":"def pre_process(text):\n    \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    words = \"\"\n    for i in text:\n            stemmer = SnowballStemmer(\"english\")\n            words += (stemmer.stem(i))+\" \"\n    return words","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"cd8f8dc418b9a109e9a64324525b2ae4c63aa608"},"cell_type":"markdown","source":"The below code copies the text data column, so any processing is not compelted on the original data. And then uses a TFIDF vectoriser to provide useful numerical values related to the data. TFIDF (term frequency - inverse document frequency) is a statistical method to tell how important a word is to a particular document by increasing the numerical value for an occurrence in the specific document but decreasing relative to number of occurrences in the entire corpus. \n\nAfter this, a function available in the sklearn library is used to randomly assign training and test data to train and test the machine learning models. "},{"metadata":{"_execution_state":"idle","_uuid":"b9740ac75e5ff94922df3a4e55b78a33aa079699","_cell_guid":"b26dbce3-4f2c-4cd9-b51d-43d3e38a5db9","trusted":true},"cell_type":"code","source":"textFeatures = data['text'].copy()\ntextFeatures = textFeatures.apply(pre_process)\nvectorizer = TfidfVectorizer(\"english\")\nfeatures = vectorizer.fit_transform(textFeatures)\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, data['class'], test_size=0.3, random_state=111)\nlabels_train_binary = np.array(labels_train, dtype=np.uint8)\nlabels_test_binary = np.array(labels_test, dtype=np.uint8)\n","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"913473e9e180e592a343e31fbd8e9900ff3cf750"},"cell_type":"markdown","source":""},{"metadata":{"_execution_state":"idle","_uuid":"cc5ee1263e179d6e3e75f27fe437383744935a44","_cell_guid":"468a801a-05fb-4d42-ae94-c8bd9520efeb","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel='sigmoid', gamma=1.0)\nsvc.fit(features_train, labels_train_binary)\nprediction = svc.predict(features_test)\nf1_score(labels_test_binary, prediction)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"12fb5cd7aecd9524138e51d37138086e7ca364cc"},"cell_type":"markdown","source":"The following code trains and test a Multinomial Naive Bayes Model using sklearn. It can be seen that is provided a slightly more accurate result than the SVM model so should therefore be used. There are many other models that may be more suitable for this dataset, however both of these model produce sufficient results. "},{"metadata":{"_execution_state":"idle","_uuid":"f86e606391062d1cbcb1bc18924c8b2309329524","_cell_guid":"666aa0bf-3ee1-4665-8b6a-f6365547fa63","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB(alpha=0.2)\nmnb.fit(features_train, labels_train_binary)\nprediction = mnb.predict(features_test)\nf1_score(labels_test_binary, prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7ca5ded8800634698a481f2b8147f6238679624"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}