{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark=SparkSession.builder.appName('eda').getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=spark.read.csv('../input/Features data set.csv', header='TRUE')\nsales=spark.read.csv('../input/sales data-set.csv',header='TRUE')\nstores=spark.read.csv('../input/stores data-set.csv',header='TRUE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales.filter(sales.Dept)\n#sales.filter(sales.Dept=='77').show()\n#df00.filter(df00.Store=='1').show(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=sales.join(features, ['Store','Date','IsHoliday'], 'left_outer')\ndf.show(2)\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import *\n#df.withColumn('year', year('Date')).show(5)                    #we got null values in year if we take year directly from date, so we need to convert date into std format first\n\nd=to_date(df['Date'], 'dd/MM/yy').cast('date')\ndf=df.withColumn('Date', d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df=df.filter(df['Weekly_Sales']>'0')                                               #420212\n#df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.select(df['Date']).distinct().orderBy('Date', ascending=True).show(2)  \ndf.select(df['Date']).distinct().orderBy('Date', ascending=False).show(2)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df00=df.filter(df.Date=='2010-02-05')\n#df00.filter(df00.Store=='1').show(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df01=df.filter(df.Date=='2010-02-19')\n#df01.filter(df01.Store=='25').show(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.select(['Store','Dept']).distinct()                             #3323, we cant remove now itself the neg weekly sales, we need to remove before FF only\\\n                                                                                           #after data gets countinuous\n\ndf1.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import *\nbase = date(2010,2,5)\nnew_date_list = []\nfor x in range(0, 1000, 7):\n    date_list = [base + timedelta(days=x)]\n    new_date_list.append(date_list)\n\nfrom pyspark.sql.types import *\ndf12=spark.createDataFrame( new_date_list)\n#dfd=spark.createDataFrame('new_date_list', ['Date'])\n\n\ndf12=df12.withColumnRenamed('_1', 'Date')\ndf12=df12.select(df12['Date']).distinct().orderBy('Date', ascending=True)              #max=2012-10-26   #min=2010-02-05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df12.select(df12['Date']).distinct().orderBy('Date', ascending=True).show(2)  \ndf12.select(df12['Date']).distinct().orderBy('Date', ascending=False).show(2)  \ndf12.count()\n#df12.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df12.crossJoin(df1.select('Store','Dept'))\ndf2.count()    \n#df2.orderBy('Store', 'Dept','Date').show(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df2.join(df, ['Store', 'Dept', 'Date'], 'left_outer')\ndfm=df2.join(df, ['Date', 'Store', 'Dept'], 'left_outer')\ndfm.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dfm.select([count(when(isnull(mshc),'mshc')).alias(mshc) for mshc in dfm.columns]).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from pyspark.sql.types import *\ndfm=dfm.withColumn('Weekly_Sales', dfm['Weekly_Sales'].cast('float'))\ndfm=dfm.withColumn('Store', dfm['Store'].cast(IntegerType()))\ndfm=dfm.withColumn('Dept', dfm['Dept'].cast(IntegerType()))\ndfm=dfm.withColumn('Temperature', dfm['Temperature'].cast('float'))\ndfm=dfm.withColumn('Fuel_Price', dfm['Fuel_Price'].cast('float'))\ndfm=dfm.withColumn('MarkDown1', dfm['MarkDown1'].cast('float'))\ndfm=dfm.withColumn('MarkDown2', dfm['MarkDown2'].cast('float'))\ndfm=dfm.withColumn('MarkDown3', dfm['MarkDown3'].cast('float'))\ndfm=dfm.withColumn('MarkDown4', dfm['MarkDown4'].cast('float'))\ndfm=dfm.withColumn('MarkDown5', dfm['MarkDown5'].cast('float'))\ndfm=dfm.withColumn('CPI', dfm['CPI'].cast('float'))\ndfm=dfm.withColumn('Unemployment', dfm['Unemployment'].cast('float'))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dfm.filter(dfm.Weekly_Sales.isNull()).orderBy('Date','Store','Dept').show(5)\n#df.filter(df.height.isNull()).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import *\nfrom pyspark.sql.functions import *\nimport sys\n\n# define the window\nwindow = Window.partitionBy('Store').orderBy('Date').rowsBetween(0, sys.maxsize)\n\n# define the forward-filled column\nfilled_column = last(dfm['Weekly_Sales'], ignorenulls=True).over(window)\n\n# do the fill\ndfm= dfm.withColumn('Weekly_Sales', filled_column)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window = Window.partitionBy('Store').orderBy('Date').rowsBetween(-sys.maxsize,0)\nfilled_column = last(dfm['Temperature'], ignorenulls=True).over(window)\ndfm= dfm.withColumn('Temperature', filled_column)\n\nwindow = Window.partitionBy('Store').orderBy('Date').rowsBetween(-sys.maxsize,0)\nfilled_column = last(dfm['Fuel_Price'], ignorenulls=True).over(window)\ndfm= dfm.withColumn('Fuel_Price', filled_column)\n\n\nwindow = Window.partitionBy('Store').orderBy('Date').rowsBetween(-sys.maxsize,0)\nfilled_column = last(dfm['IsHoliday'], ignorenulls=True).over(window)\ndfm= dfm.withColumn('IsHoliday', filled_column)\n\nwindow = Window.partitionBy('Store').orderBy('Date').rowsBetween(-sys.maxsize,0)\nfilled_column = last(dfm['CPI'], ignorenulls=True).over(window)\ndfm= dfm.withColumn('CPI', filled_column)\n\nwindow = Window.partitionBy('Store').orderBy('Date').rowsBetween(-sys.maxsize,0)\nfilled_column = last(dfm['Unemployment'], ignorenulls=True).over(window)\ndfm= dfm.withColumn('Unemployment', filled_column)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfm.select([count(when(isnull(mshc),'mshc')).alias(mshc) for mshc in dfm.columns]).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfm.columns","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"['Date',\n 'Store',\n 'Dept',\n 'IsHoliday',\n 'Weekly_Sales',\n 'Temperature',\n 'Fuel_Price',\n 'MarkDown1',\n 'MarkDown2',\n 'MarkDown3',\n 'MarkDown4',\n 'MarkDown5',\n 'CPI',\n 'Unemployment']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfm.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dfm.to_pandas()\n#pandas_df = some_df.toPandas()\npdf = dfm.toPandas()","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\n\npdf.to_csv('submission.csv')\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')\n","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<a href=submission.csv>Download CSV file</a>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}