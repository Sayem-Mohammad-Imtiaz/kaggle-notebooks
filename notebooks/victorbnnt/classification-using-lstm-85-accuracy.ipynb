{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"display: block; height: 500px; overflow:hidden;position: relative\">\n     <img src=\"https://imgur.com/6I1AHP5.jpg\" style=\"position: absolute;top: 0px;\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"# 1.  Imports","metadata":{}},{"cell_type":"code","source":"# generics\nimport pandas as pd\nimport numpy as np\nimport random\n\n# visu\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# texts\nimport re\nimport unicodedata\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical, plot_model\n\n# Model\nfrom tensorflow.keras import layers, Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# NLTK\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n# sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:11.87102Z","iopub.execute_input":"2021-08-23T20:19:11.87133Z","iopub.status.idle":"2021-08-23T20:19:11.881016Z","shell.execute_reply.started":"2021-08-23T20:19:11.871302Z","shell.execute_reply":"2021-08-23T20:19:11.879894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.  Loading data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", encoding=\"latin_1\")\ndf_test = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\", encoding=\"latin_1\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:12.998185Z","iopub.execute_input":"2021-08-23T20:19:12.998813Z","iopub.status.idle":"2021-08-23T20:19:13.245292Z","shell.execute_reply.started":"2021-08-23T20:19:12.99877Z","shell.execute_reply":"2021-08-23T20:19:13.244242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.  Data overview","metadata":{}},{"cell_type":"code","source":"df_train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:14.333698Z","iopub.execute_input":"2021-08-23T20:19:14.33411Z","iopub.status.idle":"2021-08-23T20:19:14.347591Z","shell.execute_reply.started":"2021-08-23T20:19:14.33408Z","shell.execute_reply":"2021-08-23T20:19:14.346616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_tweet_length = 0\ntweet_length = []\n#\nfor tweet in df_train[\"OriginalTweet\"]:\n    tweet_length.append(len(tweet))\n    if len(tweet) > max_tweet_length:\n        max_tweet_length = len(tweet)\nprint(\"Longest tweet: \" + str(max_tweet_length) + \" characters\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:14.768675Z","iopub.execute_input":"2021-08-23T20:19:14.769018Z","iopub.status.idle":"2021-08-23T20:19:14.805185Z","shell.execute_reply.started":"2021-08-23T20:19:14.768985Z","shell.execute_reply":"2021-08-23T20:19:14.80432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {'axes.labelsize': 20,\n              'axes.titlesize': 30}\n#\nplt.rcParams.update(parameters)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18.5, 6)\nsns.histplot(tweet_length, palette='Blues', stat='density', bins=50, ax=ax1);\nsns.kdeplot(tweet_length, color='red', ax=ax1)\nax1.set_xlabel('Character count per tweet');\ndf_train[\"Sentiment\"].reset_index().groupby(\"Sentiment\").count().rename(columns={\"index\": \"Count\"}).sort_values(by= \n       \"Count\").plot(kind=\"barh\", legend=False, \n        ax=ax2).grid(axis='x')\nax1.tick_params(axis='x', labelsize=16)\nax1.tick_params(axis='y', labelsize=16)\nax1.set_ylabel(\"\")\nax1.set_title(\"Tweet length distribution\", color =\"#292421\")\nax2.tick_params(axis='x', labelsize=16)\nax2.tick_params(axis='y', labelsize=16)\nax2.set_ylabel(\"\")\nax2.set_title(\"Tweet sentiment count\", color =\"#292421\")\nfig.tight_layout(pad=2.0)\nplt.rcParams.update(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:15.117162Z","iopub.execute_input":"2021-08-23T20:19:15.11748Z","iopub.status.idle":"2021-08-23T20:19:15.936641Z","shell.execute_reply.started":"2021-08-23T20:19:15.117444Z","shell.execute_reply":"2021-08-23T20:19:15.935851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.  Turning 5 categories into 3 categories\nHere we convert <b>extremely positive</b> tweets into <b style=\"color: green\">positive</b> and <b>extremely negative</b> tweets into <b style=\"color: red\">negative</b>.","metadata":{}},{"cell_type":"code","source":"def set_3_classes(x):\n  if x==\"Extremely Negative\":\n    return \"Negative\"\n  elif x==\"Extremely Positive\":\n    return \"Positive\"\n  else:\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:15.938586Z","iopub.execute_input":"2021-08-23T20:19:15.938833Z","iopub.status.idle":"2021-08-23T20:19:15.945676Z","shell.execute_reply.started":"2021-08-23T20:19:15.938806Z","shell.execute_reply":"2021-08-23T20:19:15.944671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"Sentiment\"] = df_train[\"Sentiment\"].apply(set_3_classes)\ndf_test[\"Sentiment\"] = df_test[\"Sentiment\"].apply(set_3_classes)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:15.984254Z","iopub.execute_input":"2021-08-23T20:19:15.984505Z","iopub.status.idle":"2021-08-23T20:19:16.002997Z","shell.execute_reply.started":"2021-08-23T20:19:15.984479Z","shell.execute_reply":"2021-08-23T20:19:16.002055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.suptitle(\"Count\", fontsize=12)\ndf_train[\"Sentiment\"].reset_index().groupby(\"Sentiment\").count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:16.261678Z","iopub.execute_input":"2021-08-23T20:19:16.262124Z","iopub.status.idle":"2021-08-23T20:19:16.399215Z","shell.execute_reply.started":"2021-08-23T20:19:16.262091Z","shell.execute_reply":"2021-08-23T20:19:16.398221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Cleaning tweets","metadata":{}},{"cell_type":"code","source":"df_train[\"CleanTweet\"] = df_train[\"OriginalTweet\"]\ndf_train.sample(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:16.827058Z","iopub.execute_input":"2021-08-23T20:19:16.827402Z","iopub.status.idle":"2021-08-23T20:19:16.852324Z","shell.execute_reply.started":"2021-08-23T20:19:16.827374Z","shell.execute_reply":"2021-08-23T20:19:16.85148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing end-of-line, tabulation and carriage return. Turning into lower case:</b>","metadata":{}},{"cell_type":"code","source":"def clean_eol_tabs(df, label):\n    \"\"\" text lowercase\n        removes \\n\n        removes \\t\n        removes \\r \"\"\"\n    df[label] = df[label].str.lower()\n    df[label] = df[label].apply(lambda x: x.replace(\"\\n\", \" \"))\n    df[label] = df[label].apply(lambda x: x.replace(\"\\r\", \" \"))\n    df[label] = df[label].apply(lambda x: x.replace(\"\\t\", \" \"))\n    return df\n#\ndf_train = clean_eol_tabs(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:17.478674Z","iopub.execute_input":"2021-08-23T20:19:17.479026Z","iopub.status.idle":"2021-08-23T20:19:17.570531Z","shell.execute_reply.started":"2021-08-23T20:19:17.478991Z","shell.execute_reply":"2021-08-23T20:19:17.569646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing e-mails:</b>","metadata":{}},{"cell_type":"code","source":"def remove_emails(df, label):\n    \"\"\" This function removes email adresses\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\", \" \", x))\n    return df\n#\ndf_train = remove_emails(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:18.018991Z","iopub.execute_input":"2021-08-23T20:19:18.019392Z","iopub.status.idle":"2021-08-23T20:19:22.758405Z","shell.execute_reply.started":"2021-08-23T20:19:18.019361Z","shell.execute_reply":"2021-08-23T20:19:22.757554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing mentions:</b>","metadata":{}},{"cell_type":"code","source":"def remove_mentions(df, label):\n    \"\"\" This function removes mentions (Twitter - starting with @) from texts\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", x))\n    return df\n#\ndf_train = remove_mentions(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:22.759914Z","iopub.execute_input":"2021-08-23T20:19:22.760423Z","iopub.status.idle":"2021-08-23T20:19:22.833741Z","shell.execute_reply.started":"2021-08-23T20:19:22.760385Z","shell.execute_reply":"2021-08-23T20:19:22.832975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing hyperlinks:</b>","metadata":{}},{"cell_type":"code","source":"def remove_hyperlinks(df, label):\n    \"\"\" This function removes hyperlinks from texts\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"http\\S+\", \" \", x))\n    return df\n#\ndf_train = remove_hyperlinks(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:22.835571Z","iopub.execute_input":"2021-08-23T20:19:22.835932Z","iopub.status.idle":"2021-08-23T20:19:22.917271Z","shell.execute_reply.started":"2021-08-23T20:19:22.835895Z","shell.execute_reply":"2021-08-23T20:19:22.916506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing hashtags:</b>","metadata":{}},{"cell_type":"code","source":"def remove_hashtags(df, label):\n    \"\"\" This function removes hashtags\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"#\\w+\", \" \", x))\n    return df\n#\ndf_train = remove_hashtags(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:22.918768Z","iopub.execute_input":"2021-08-23T20:19:22.91913Z","iopub.status.idle":"2021-08-23T20:19:23.024101Z","shell.execute_reply.started":"2021-08-23T20:19:22.919093Z","shell.execute_reply":"2021-08-23T20:19:23.023264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing html tags:</b>","metadata":{}},{"cell_type":"code","source":"def remove_html_tags(df, label):\n    \"\"\" This function removes html tags from texts\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"<.*?>\", \" \", x))\n    return df\n#\ndf_train = remove_html_tags(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:23.025514Z","iopub.execute_input":"2021-08-23T20:19:23.025858Z","iopub.status.idle":"2021-08-23T20:19:23.092686Z","shell.execute_reply.started":"2021-08-23T20:19:23.025821Z","shell.execute_reply":"2021-08-23T20:19:23.091862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing numbers:</b>","metadata":{}},{"cell_type":"code","source":"def remove_numbers(df, label):\n    \"\"\" This function removes numbers from a text\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n    return df\n#\ndf_train = remove_numbers(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:23.093976Z","iopub.execute_input":"2021-08-23T20:19:23.094474Z","iopub.status.idle":"2021-08-23T20:19:23.372123Z","shell.execute_reply.started":"2021-08-23T20:19:23.094428Z","shell.execute_reply":"2021-08-23T20:19:23.371262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Encode unknown characters:</b>","metadata":{}},{"cell_type":"code","source":"def encode_unknown(df, label):\n    \"\"\" This function encodes special caracters \"\"\"\n    df[label] = df[label].apply(lambda x: unicodedata.normalize(\"NFD\", x).encode('ascii', 'ignore').decode(\"utf-8\"))\n    return df\n#\ndf_train = encode_unknown(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:23.373372Z","iopub.execute_input":"2021-08-23T20:19:23.373871Z","iopub.status.idle":"2021-08-23T20:19:23.44521Z","shell.execute_reply.started":"2021-08-23T20:19:23.373831Z","shell.execute_reply":"2021-08-23T20:19:23.444382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing punctuations and special characters:</b><br>\n*Note this function will remove punctuation AND accented characters. Thus it is not necessary usable on languages that have accented characters. But for english it is ok.*","metadata":{}},{"cell_type":"code","source":"def clean_punctuation_no_accent(df, label):\n    \"\"\" This function removes punctuation and accented characters from texts in a dataframe \n        To be appplied to languages that have no accents, ex: english \n    \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n    return df\n#\ndf_train = clean_punctuation_no_accent(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:23.447324Z","iopub.execute_input":"2021-08-23T20:19:23.44769Z","iopub.status.idle":"2021-08-23T20:19:23.757571Z","shell.execute_reply.started":"2021-08-23T20:19:23.447652Z","shell.execute_reply":"2021-08-23T20:19:23.756687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing stop words. Here, the list is from nltk stopwords library:</b>","metadata":{}},{"cell_type":"code","source":"def remove_stop_words(text, stopwords=set(stopwords.words('english'))):\n    \"\"\" This function removes stop words from a text\n        inputs:\n         - stopword list\n         - text \"\"\"\n\n    # prepare new text\n    text_splitted = text.split(\" \")\n    text_new = list()\n    \n    # stop words updated\n    #stopwords = stopwords.union({\"amp\", \"grocery store\", \"covid\", \"supermarket\", \"people\", \"grocery\", \"store\", \"price\", \"time\", \"consumer\"})\n    \n    # loop\n    for word in text_splitted:\n        if word not in stopwords:\n            text_new.append(word)\n    return \" \".join(text_new)\n\ndef clean_stopwords(df, label):\n    \"\"\" This function removes stopwords \"\"\"\n    df[label] = df[label].apply(lambda x: remove_stop_words(x))\n    return df\n#\ndf_train = clean_stopwords(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:23.759097Z","iopub.execute_input":"2021-08-23T20:19:23.759453Z","iopub.status.idle":"2021-08-23T20:19:24.190029Z","shell.execute_reply.started":"2021-08-23T20:19:23.759403Z","shell.execute_reply":"2021-08-23T20:19:24.188985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Removing one and two letters words, removing unnecessary spaces, droping empty lines:</b>","metadata":{}},{"cell_type":"code","source":"def more_cleaning(df, label):\n    \"\"\" This function\n     1) removes remaining one-letter words and two letters words\n     2) replaces multiple spaces by one single space\n     3) drop empty lines \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', \" \", x))\n    df[label] = df[label].apply(lambda x: re.sub(r\"[ \\t]{2,}\", \" \", x))\n    df[label] = df[label].apply(lambda x: x if len(x) != 1 else '')\n    df[label] = df[label].apply(lambda x: np.nan if x == '' else x)\n    df = df.dropna(subset=[label], axis=0).reset_index(drop=True).copy()\n    return df\n#\ndf_train = more_cleaning(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:24.194721Z","iopub.execute_input":"2021-08-23T20:19:24.196842Z","iopub.status.idle":"2021-08-23T20:19:24.95777Z","shell.execute_reply.started":"2021-08-23T20:19:24.1968Z","shell.execute_reply":"2021-08-23T20:19:24.956905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Lemmatizing words:</b><br>\n*Note: Here the lemmatizer works only for its default parameter which is <b>nouns</b>. That is to say, it will only find the closest root for nouns and will not work on verbs or adjectives ect ... I tried with lemmatization of everything but the accuracy was lower*","metadata":{}},{"cell_type":"code","source":"def lemmatize_one_text(text):\n    \"\"\" This function lemmatizes words in text (it changes word to most close root word)\n        inputs:\n         - lemmatizer\n         - text \"\"\"\n\n    # initialize lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    \n    # tags\n    lem_tags = ['a', 'r', 'n', 'v']\n\n    # prepare new text\n    text_splitted = text.split(\" \")\n    text_new = list()\n\n    # change bool\n    changed = ''\n    \n    # loop\n    for word in text_splitted:\n        text_new.append(lemmatizer.lemmatize(word))\n        #changed = ''\n        #for tag in lem_tags:\n        #    if lemmatizer.lemmatize(word, tag) != word:\n        #        changed = tag\n        #if changed == '':\n        #    text_new.append(word)\n        #else:\n        #    text_new.append(lemmatizer.lemmatize(word, changed))\n\n    return \" \".join(text_new)\n\ndef lemmatize(df, label):\n    \"\"\" This function lemmatizes texts \"\"\"\n    df[label] = df[label].apply(lambda x: lemmatize_one_text(x))\n    return df\n#\ndf_train = lemmatize(df_train, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:24.959507Z","iopub.execute_input":"2021-08-23T20:19:24.959838Z","iopub.status.idle":"2021-08-23T20:19:28.062572Z","shell.execute_reply.started":"2021-08-23T20:19:24.959799Z","shell.execute_reply":"2021-08-23T20:19:28.06172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:28.06404Z","iopub.execute_input":"2021-08-23T20:19:28.06436Z","iopub.status.idle":"2021-08-23T20:19:28.080728Z","shell.execute_reply.started":"2021-08-23T20:19:28.064325Z","shell.execute_reply":"2021-08-23T20:19:28.079849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's apply all of these cleaning on test data set as well:","metadata":{}},{"cell_type":"code","source":"df_test[\"CleanTweet\"] = df_test[\"OriginalTweet\"]\ndf_test = clean_eol_tabs(df_test, \"CleanTweet\")\ndf_test = remove_emails(df_test, \"CleanTweet\")\ndf_test = remove_mentions(df_test, \"CleanTweet\")\ndf_test = remove_hyperlinks(df_test, \"CleanTweet\")\ndf_test = remove_hashtags(df_test, \"CleanTweet\")\ndf_test = remove_html_tags(df_test, \"CleanTweet\")\ndf_test = remove_numbers(df_test, \"CleanTweet\")\ndf_test = encode_unknown(df_test, \"CleanTweet\")\ndf_test = clean_punctuation_no_accent(df_test, \"CleanTweet\")\ndf_test = clean_stopwords(df_test, \"CleanTweet\")\ndf_test = more_cleaning(df_test, \"CleanTweet\")\ndf_test = lemmatize(df_test, \"CleanTweet\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:28.08222Z","iopub.execute_input":"2021-08-23T20:19:28.082582Z","iopub.status.idle":"2021-08-23T20:19:29.06256Z","shell.execute_reply.started":"2021-08-23T20:19:28.082545Z","shell.execute_reply":"2021-08-23T20:19:29.061623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.sample(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:29.06449Z","iopub.execute_input":"2021-08-23T20:19:29.064878Z","iopub.status.idle":"2021-08-23T20:19:29.079519Z","shell.execute_reply.started":"2021-08-23T20:19:29.064838Z","shell.execute_reply":"2021-08-23T20:19:29.07848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's have a look on before/after cleaning on several tweets:","metadata":{}},{"cell_type":"code","source":"tweet_num = random.randint(0, df_train.shape[0])\nprint(\"############################# Original Tweet #############################\")\nprint(df_train.iloc[tweet_num].at[\"OriginalTweet\"])\nprint(\"\\n\")\nprint(\"############################# Clean Tweet ################################\")\nprint(df_train.iloc[tweet_num].at[\"CleanTweet\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:31.10082Z","iopub.execute_input":"2021-08-23T20:19:31.10117Z","iopub.status.idle":"2021-08-23T20:19:31.108672Z","shell.execute_reply.started":"2021-08-23T20:19:31.101139Z","shell.execute_reply":"2021-08-23T20:19:31.107544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_num = random.randint(0, df_train.shape[0])\nprint(\"############################# Original Tweet #############################\")\nprint(df_train.iloc[tweet_num].at[\"OriginalTweet\"])\nprint(\"\\n\")\nprint(\"############################# Clean Tweet ################################\")\nprint(df_train.iloc[tweet_num].at[\"CleanTweet\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:32.428221Z","iopub.execute_input":"2021-08-23T20:19:32.428649Z","iopub.status.idle":"2021-08-23T20:19:32.436449Z","shell.execute_reply.started":"2021-08-23T20:19:32.428611Z","shell.execute_reply":"2021-08-23T20:19:32.435687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks good","metadata":{}},{"cell_type":"markdown","source":"# 6.  Looking at data","metadata":{}},{"cell_type":"markdown","source":"<b>Sentiment repartition:</b>","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18.5, 5)\nfig.suptitle('Sentiment repartition among tweets in train and test sets')\ndf_train[\"Sentiment\"].value_counts().plot(kind=\"bar\", ax=ax1);\ndf_test[\"Sentiment\"].value_counts().plot(kind=\"bar\", ax=ax2);","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:37.66886Z","iopub.execute_input":"2021-08-23T20:19:37.669224Z","iopub.status.idle":"2021-08-23T20:19:37.939222Z","shell.execute_reply.started":"2021-08-23T20:19:37.669192Z","shell.execute_reply":"2021-08-23T20:19:37.93843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Word cloud in each sentiment categories:</b>","metadata":{}},{"cell_type":"code","source":"all_words_positive = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Positive\"][\"CleanTweet\"]])\nall_words_neutral = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Neutral\"][\"CleanTweet\"]])\nall_words_negative = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Negative\"][\"CleanTweet\"]])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:38.557884Z","iopub.execute_input":"2021-08-23T20:19:38.558255Z","iopub.status.idle":"2021-08-23T20:19:38.60089Z","shell.execute_reply.started":"2021-08-23T20:19:38.558223Z","shell.execute_reply":"2021-08-23T20:19:38.60003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud_positive = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Greens\").generate(all_words_positive)\nwordcloud_neutral = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"YlOrBr\").generate(all_words_neutral)\nwordcloud_negative = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Reds\").generate(all_words_negative)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:38.951045Z","iopub.execute_input":"2021-08-23T20:19:38.951368Z","iopub.status.idle":"2021-08-23T20:19:45.979234Z","shell.execute_reply.started":"2021-08-23T20:19:38.951338Z","shell.execute_reply":"2021-08-23T20:19:45.977508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {'axes.labelsize': 12,\n              'axes.titlesize': 10}\n#\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\nfig.set_size_inches(18.5, 7)\nax1.imshow(wordcloud_positive, interpolation='bilinear')\nax1.axis(\"off\")\nax1.set_title(\"WordCloud of positive tweets\", fontsize=12)\nax2.imshow(wordcloud_neutral, interpolation='bilinear')\nax2.axis(\"off\")\nax2.set_title(\"WordCloud of neutral tweets\", fontsize=12)\nax3.imshow(wordcloud_negative, interpolation='bilinear')\nax3.axis(\"off\")\nax3.set_title(\"WordCloud of negative tweets\", fontsize=12)\nplt.rcParams.update(parameters)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:45.980759Z","iopub.execute_input":"2021-08-23T20:19:45.981105Z","iopub.status.idle":"2021-08-23T20:19:46.746815Z","shell.execute_reply.started":"2021-08-23T20:19:45.981068Z","shell.execute_reply":"2021-08-23T20:19:46.745838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Sentiment encoding","metadata":{}},{"cell_type":"code","source":"df_train_encoded = df_train.copy()\ndf_test_encoded = df_test.copy()\n#\nprint(\"train set shape: \" + str(df_train_encoded.shape))\nprint(\"test set shape: \" + str(df_test_encoded.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:50.074677Z","iopub.execute_input":"2021-08-23T20:19:50.075026Z","iopub.status.idle":"2021-08-23T20:19:50.084295Z","shell.execute_reply.started":"2021-08-23T20:19:50.07499Z","shell.execute_reply":"2021-08-23T20:19:50.082853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_sentiment = {\"Neutral\":0, \"Positive\":1,\"Negative\":2}\ndf_train_encoded['Sentiment'] = df_train_encoded['Sentiment'].map(map_sentiment)\ndf_test_encoded['Sentiment']  = df_test_encoded['Sentiment'].map(map_sentiment)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:50.5643Z","iopub.execute_input":"2021-08-23T20:19:50.56463Z","iopub.status.idle":"2021-08-23T20:19:50.580315Z","shell.execute_reply.started":"2021-08-23T20:19:50.564601Z","shell.execute_reply":"2021-08-23T20:19:50.579414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Feature and target preparation","metadata":{}},{"cell_type":"code","source":"y_train = df_train['Sentiment'].copy()\ny_test = df_test['Sentiment'].copy()\n#\ny_train_encoded = to_categorical(df_train_encoded['Sentiment'], 3)\ny_test_encoded = to_categorical(df_test_encoded['Sentiment'], 3)\n#\ny_train_mapped = df_train_encoded['Sentiment'].copy()\ny_test_mapped = df_test_encoded['Sentiment'].copy()\n#\nX_train = df_train_encoded[['CleanTweet']].copy()\nX_test = df_test_encoded[['CleanTweet']].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:53.325994Z","iopub.execute_input":"2021-08-23T20:19:53.326324Z","iopub.status.idle":"2021-08-23T20:19:53.339735Z","shell.execute_reply.started":"2021-08-23T20:19:53.326295Z","shell.execute_reply":"2021-08-23T20:19:53.33865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Tokenization, sequences, padding","metadata":{}},{"cell_type":"markdown","source":"<b>The result of tokenizer is a dictionnary with:</b><br>\n* key = word<br>\n* value = unique number","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train[\"CleanTweet\"])\nvocab_length = len(tokenizer.word_index) + 1\nvocab_length","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:54.794763Z","iopub.execute_input":"2021-08-23T20:19:54.795129Z","iopub.status.idle":"2021-08-23T20:19:55.571263Z","shell.execute_reply.started":"2021-08-23T20:19:54.795095Z","shell.execute_reply":"2021-08-23T20:19:55.570283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The <b>texts_to_sequences</b> function first transforms a text into list of words. Then, thanks to the dictionnary previously created by the tokenizer (see above), transforms list of list of words into list of list of numbers","metadata":{}},{"cell_type":"code","source":"X_train = tokenizer.texts_to_sequences(X_train[\"CleanTweet\"])\nX_test = tokenizer.texts_to_sequences(X_test[\"CleanTweet\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:55.572862Z","iopub.execute_input":"2021-08-23T20:19:55.573226Z","iopub.status.idle":"2021-08-23T20:19:56.223847Z","shell.execute_reply.started":"2021-08-23T20:19:55.57319Z","shell.execute_reply":"2021-08-23T20:19:56.222995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each tweets has differents length. Thus the result of the <b>texts_to_sequences</b> function will be a list of list of numbers of different length: ","metadata":{}},{"cell_type":"code","source":"print(\"First tweet encoded:\")\nprint(X_train[0])\nprint(\"\\nSecond tweet encoded:\")\nprint(X_train[1])\nprint(\"\\nThird tweet encoded:\")\nprint(X_train[2])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:56.278476Z","iopub.execute_input":"2021-08-23T20:19:56.278783Z","iopub.status.idle":"2021-08-23T20:19:56.285341Z","shell.execute_reply.started":"2021-08-23T20:19:56.278753Z","shell.execute_reply":"2021-08-23T20:19:56.284382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To feed the deep learning model, we need all these lists to be the same length. Thus we need to apply padding. In other words, we are going to add several zeros (0) at the end of the shortest tweets so that at the end, all of our lists have the same length. <br><br>\nFirst let's get the maximum number of words in one tweet:","metadata":{}},{"cell_type":"code","source":"max_word_count = 0\nword_count = []\n#\nfor encoded_tweet in X_train:\n    word_count.append(len(encoded_tweet))\n    if len(encoded_tweet) > max_word_count:\n        max_word_count = len(encoded_tweet)\nprint(\"Maximum number of word in one tweet: \" + str(max_word_count) + \" words\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:57.180456Z","iopub.execute_input":"2021-08-23T20:19:57.180873Z","iopub.status.idle":"2021-08-23T20:19:57.22491Z","shell.execute_reply.started":"2021-08-23T20:19:57.180835Z","shell.execute_reply":"2021-08-23T20:19:57.223978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {'axes.labelsize': 20,\n              'axes.titlesize': 30}\n#\nplt.rcParams.update(parameters)\nfig, ax1 = plt.subplots(1, 1)\nfig.set_size_inches(18.5, 8)\nsns.histplot(word_count, palette='Blues', stat='density', bins=30, ax=ax1);\nsns.kdeplot(word_count, color='red', ax=ax1)\nax1.set_xlabel('Word count per tweet');\nax1.tick_params(axis='x', labelsize=16)\nax1.tick_params(axis='y', labelsize=16)\nax1.set_ylabel(\"\")\nax1.set_title(\"Tweet length distribution\", color =\"#292421\")\nfig.tight_layout(pad=2.0)\nplt.rcParams.update(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:57.514994Z","iopub.execute_input":"2021-08-23T20:19:57.515315Z","iopub.status.idle":"2021-08-23T20:19:58.668059Z","shell.execute_reply.started":"2021-08-23T20:19:57.515287Z","shell.execute_reply":"2021-08-23T20:19:58.667247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the longest tweet we have is composed of 37 words. We are going to pad the sequences with a maximum length of 37.","metadata":{}},{"cell_type":"code","source":"X_train = pad_sequences(X_train, maxlen=max_word_count, padding='post')\nX_test = pad_sequences(X_test, maxlen=max_word_count, padding='post')\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:58.669563Z","iopub.execute_input":"2021-08-23T20:19:58.669892Z","iopub.status.idle":"2021-08-23T20:19:59.021856Z","shell.execute_reply.started":"2021-08-23T20:19:58.669859Z","shell.execute_reply":"2021-08-23T20:19:59.020851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the encoded 3 tweets after padding:","metadata":{}},{"cell_type":"code","source":"print(\"First tweet encoded:\", \"Size = \", len(X_train[0]))\nprint(X_train[0])\nprint(\"\\nSecond tweet encoded:\", \"Size = \", len(X_train[1]))\nprint(X_train[1])\nprint(\"\\nThird tweet encoded:\", \"Size = \", len(X_train[2]))\nprint(X_train[2])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:19:59.023745Z","iopub.execute_input":"2021-08-23T20:19:59.024107Z","iopub.status.idle":"2021-08-23T20:19:59.03212Z","shell.execute_reply.started":"2021-08-23T20:19:59.024069Z","shell.execute_reply":"2021-08-23T20:19:59.031168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now every encoded tweet has the same length, the data is ready for the model.","metadata":{}},{"cell_type":"markdown","source":"# 10. Model","metadata":{}},{"cell_type":"markdown","source":"<b>The model is composed of:</b>\n1. <b>An embedding layer with parameters</b>\n    * input dim = vocabulary size\n    * output dim = 32\n    * input length = size of padded sequences\n    * mask_zero = True to ignore 0 (from padding)\n2. <b>An LSTM (Long Short Term Memory) Layer with parameter</b>\n    * units = 100 (don't ask me why, the resulting accuracy is almost the same regardless this value)\n3. <b>Three Dense layers</b>\n4. <b>An output dense layer with parameters</b>\n    * units = 3 (output dim)\n    * activation = softmax (for multiclassification problem)\n\n<b>Compilation with parameters:</b>\n1. loss = categorical_crossentropy (for multiclassification problem)\n2. optimizer = adam\n3. metrics = accuracy","metadata":{}},{"cell_type":"code","source":"model_LSTM = Sequential()\nmodel_LSTM.add(layers.Embedding(vocab_length, output_dim=32, input_length=max_word_count, mask_zero=True))\nmodel_LSTM.add(layers.LSTM(100))\nmodel_LSTM.add(layers.Dense(64, activation=\"relu\"))\nmodel_LSTM.add(layers.Dense(32, activation=\"relu\"))\nmodel_LSTM.add(layers.Dense(16, activation=\"relu\"))\nmodel_LSTM.add(layers.Dense(3, activation='softmax'))\nmodel_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_LSTM.summary())","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:20:00.811824Z","iopub.execute_input":"2021-08-23T20:20:00.8123Z","iopub.status.idle":"2021-08-23T20:20:01.52134Z","shell.execute_reply.started":"2021-08-23T20:20:00.812264Z","shell.execute_reply":"2021-08-23T20:20:01.520348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I set an early stopping after 10 epochs and set the parameter <i><b>restore_best_weights</b></i> to <b style=\"color:green\">True</b> so that the weights of best score on monitored metric - here <b>val_accuracy</b> (accuracy on test set) - are restored when training stops. This way the model has the best accuracy possible on unseen data.","metadata":{}},{"cell_type":"code","source":"es = EarlyStopping(patience=10, monitor='val_accuracy', restore_best_weights=True)\nhistory = model_LSTM.fit(X_train,\n                         y_train_encoded,\n                         validation_data=(X_test, y_test_encoded),\n                         epochs=30,\n                         batch_size=16,\n                         verbose=1,\n                         callbacks=[es]\n                        )","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:20:03.414046Z","iopub.execute_input":"2021-08-23T20:20:03.414379Z","iopub.status.idle":"2021-08-23T20:28:25.636475Z","shell.execute_reply.started":"2021-08-23T20:20:03.41435Z","shell.execute_reply":"2021-08-23T20:28:25.635661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that after epoch 2, the accuracy on test set - val_accuracy - doesn't increase any more while accuracy on train set continues to increase untill almost 100%! The model is overfitting from epoch 2 and is not able to generalize well on unseen data from there.","metadata":{}},{"cell_type":"markdown","source":"# 11. Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"<b>Prediction on test set:</b>","metadata":{}},{"cell_type":"code","source":"predicted = model_LSTM.predict(X_test)\ny_pred = predicted.argmax(axis=-1)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:28:30.653926Z","iopub.execute_input":"2021-08-23T20:28:30.654299Z","iopub.status.idle":"2021-08-23T20:28:31.965938Z","shell.execute_reply.started":"2021-08-23T20:28:30.654268Z","shell.execute_reply":"2021-08-23T20:28:31.964972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Calculation of accuracy and Area Under (ROC) Curve - AUC - scores:</b>","metadata":{}},{"cell_type":"code","source":"acc_score = accuracy_score(y_test_mapped, y_pred)\nauc_score = roc_auc_score(y_test_mapped, predicted, multi_class=\"ovr\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:28:34.634902Z","iopub.execute_input":"2021-08-23T20:28:34.635275Z","iopub.status.idle":"2021-08-23T20:28:34.647483Z","shell.execute_reply.started":"2021-08-23T20:28:34.635244Z","shell.execute_reply":"2021-08-23T20:28:34.64665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Classification report:</b>","metadata":{}},{"cell_type":"code","source":"report = classification_report(y_test_mapped, y_pred, target_names=list(y_test.unique()), output_dict=True)\naccuracy_col = ([\"\"]*3) + [round(acc_score, 2)]\nroc_auc_col = ([\"\"]*3) + [round(auc_score, 2)]\naccuracy_col = pd.Series(accuracy_col, index=list(report[\"Neutral\"].keys()))\nroc_auc_col = pd.Series(roc_auc_col, index=list(report[\"Neutral\"].keys()))\ndf_report = pd.DataFrame(report)[[\"Neutral\", \"Positive\", \"Negative\", \"macro avg\", \"weighted avg\"]].apply(lambda x: round(x, 2))\ndf_report[\"accuracy\"] = accuracy_col\ndf_report[\"roc_auc\"] = roc_auc_col\ndf_report","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:28:37.393054Z","iopub.execute_input":"2021-08-23T20:28:37.393387Z","iopub.status.idle":"2021-08-23T20:28:37.427823Z","shell.execute_reply.started":"2021-08-23T20:28:37.393357Z","shell.execute_reply":"2021-08-23T20:28:37.426854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Confusion Matrix:</b>","metadata":{}},{"cell_type":"code","source":"## Plot confusion matrix\ncm = confusion_matrix(y_test_mapped, y_pred)\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 8)\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\nax.set(xticklabels=list(y_test.unique()), yticklabels=list(y_test.unique()), title=\"Confusion matrix\")\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.set_ylabel(\"True\", color=\"royalblue\", fontsize=35, fontweight=700)\nax.set_xlabel(\"Prediction\", color=\"royalblue\", fontsize=35, fontweight=700)\nplt.yticks(rotation=0);","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:28:42.598248Z","iopub.execute_input":"2021-08-23T20:28:42.598568Z","iopub.status.idle":"2021-08-23T20:28:42.804475Z","shell.execute_reply.started":"2021-08-23T20:28:42.598538Z","shell.execute_reply":"2021-08-23T20:28:42.803694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>ROC and precision-recall curves</b>","metadata":{}},{"cell_type":"code","source":"y_test_array = pd.get_dummies(y_test_mapped, drop_first=False).values\nclasses = y_train.unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:28:47.299002Z","iopub.execute_input":"2021-08-23T20:28:47.29934Z","iopub.status.idle":"2021-08-23T20:28:47.309976Z","shell.execute_reply.started":"2021-08-23T20:28:47.299308Z","shell.execute_reply":"2021-08-23T20:28:47.309122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(18.5, 5)\n## Plot roc\nfor i in range(len(classes)):\n    fpr, tpr, thresholds = roc_curve(y_test_array[:,i], predicted[:,i])\n    ax[0].plot(fpr, tpr, lw=3, \n              label='{0} (area (AUC) = {1:0.2f})'.format(classes[i], metrics.auc(fpr, tpr))\n               )\nax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\nax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n          xlabel='False Positive Rate', \n          ylabel=\"True Positive Rate (Recall)\", \n          title=\"Receiver operating characteristic\")\nax[0].legend(loc=\"lower right\")\nax[0].grid(True)\n\n## Plot precision-recall curve\nfor i in range(len(classes)):\n    precision, recall, thresholds = metrics.precision_recall_curve(\n                 y_test_array[:,i], predicted[:,i])\n    ax[1].plot(recall, precision, lw=3, \n               label='{0} (area ={1:0.2f})'.format(classes[i], metrics.auc(recall, precision))\n              )\nax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n          ylabel=\"Precision\", title=\"Precision-Recall curve\")\nax[1].legend(loc=\"best\")\nax[1].grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:28:50.198082Z","iopub.execute_input":"2021-08-23T20:28:50.198398Z","iopub.status.idle":"2021-08-23T20:28:50.517573Z","shell.execute_reply.started":"2021-08-23T20:28:50.198368Z","shell.execute_reply":"2021-08-23T20:28:50.516586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size:15pt; color:#104E8B; font-weight:700; width:80%; display:block; margin:auto; text-align:center\">Thanks for reading, I hope you enjoyed it. If there is anything wrong or if you have any suggestions for improvement, please feel free to comment, I'll be glad to get feedback to improve.</div>","metadata":{}}]}