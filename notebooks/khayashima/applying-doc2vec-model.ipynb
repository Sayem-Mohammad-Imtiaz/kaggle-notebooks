{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task: What do we know about non-pharmaceutical interventions?"},{"metadata":{},"cell_type":"markdown","source":"## Install/Load Packages"},{"metadata":{},"cell_type":"markdown","source":"The first block of code is (almost) directly from kaggle"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# There are too many paths and printing them takes\n# up too much space so I don't do this normally\nif 1==0: \n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            pass\n            #print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we installl scispacy, a repo of commands to deal with scientific documents. *Note that internet access needs to be switched on for this to work!*"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Progress bar\nimport tqdm\n\n# Word2Vec\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec,TaggedDocument\n\nfrom nltk.tokenize import word_tokenize \nfrom scipy.spatial.distance import cdist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Now that all our libraries are loaded we need data. We explore the full text in the files using the output generated from the following notebook:\nhttps://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv\")\nclean_comm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv\")\nclean_noncomm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv\")\nclean_pmc = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv\")\n\nall_data = pd.concat([biorxiv_clean, clean_comm_use, clean_noncomm_use, clean_pmc]).reset_index(drop=True)\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del biorxiv_clean,clean_comm_use,clean_noncomm_use,clean_pmc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Rows in Table: %i\" % len(all_data))\nprint(\"Number of Titles: %i \" % all_data['title'].count())\nprint(\"Number of Abstracts: %i \" % all_data['abstract'].count())\nprint(\"Number of Texts: %i \" % all_data['text'].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Doc2Vec Model\nIn this notebook, I applied [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n\nThe algorithm is introduced [here](https://arxiv.org/pdf/1405.4053v2.pdf)\n### first start with the title\nTo get our word2vec model we first get all the text from each document into a list. We do this using our `title` column from `all_data`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace empty text with empty strings\nall_title = all_data.title.str.replace('\\n\\n', ' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_na = all_title.notna()\nnon_na_title = all_title[non_na]\nnon_na_paper_ids = all_data[non_na]['paper_id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So `all_title` is now a list of documents. However, instead of the text in each document being a string, we need the texts to be formatted as a list of words that make up the document.  In other words, We are making a list of documents and then in the list of documents is lists of words in all_text_list. Just so you know the cool computational lingo, each words is also known as a token. "},{"metadata":{"trusted":true},"cell_type":"code","source":"non_na_title_list = list(\n    map(\n        lambda x: word_tokenize(x), non_na_title.values\n    ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can perform doc2vec. Doc2vec converts words and document itself to vectors using a neural network.  Using the context of each word and document id, it predicts a vector that represents how it is used contextually. In other words, the vectors created by doc2vec are highly dependent on the texts it is trained on. These vectors for words may be very different if trained on scientific journals verses twitter data."},{"metadata":{"trusted":true},"cell_type":"code","source":"documents = [TaggedDocument(doc, [non_na_paper_ids.values[i]]) for i, doc in enumerate(non_na_title_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Doc2Vec(documents,vector_size = 300,window=2, min_count=1, workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Document Vectors"},{"metadata":{},"cell_type":"markdown","source":"Ok so now `model` is like a dictionary that converts title into vectors. For each document, we convert title into vector.  We set this averaged vector to the document's paper id in the dictionary `document_dict`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"document_dict = {}\nfor idx, text_df in tqdm.tqdm(all_data[non_na][[\"paper_id\", \"title\"]].iterrows()):\n    text = text_df['title'].replace(\"\\n\\n\", ' ')\n    document_dict[text_df['paper_id']] = model.docvecs[text_df['paper_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(document_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(document_dict.values())[0].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert this dictionary to a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_embeddings_df = pd.DataFrame.from_dict(document_dict, orient=\"index\")\ndocument_embeddings_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Document Cosine Similarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will use costine similarity to measure how related documents are to the  a specific word. For example of how cosine similarity works see this [image](https://datascience-enthusiast.com/figures/cosine_sim.pn), Higher numbers indicate documents that are closer to the word vector of interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cos_sim(text):\n\n    mean_word_vec = pd.np.stack(word_vector_list, axis=1).mean(axis=1)\n    \n    # compute similarity\n    doc_sim = (\n        1-cdist(\n            document_embeddings_df.values,\n            mean_word_vec,\n            'cosine'\n        )\n    )\n    # convert result to a date frame\n    document_sim_df = (\n        pd.DataFrame(doc_sim, columns=[\"cos_sim\"])\n        .assign(document_id=list(document_embeddings_df.index))\n    )\n    # sort from most similar to least\n    document_sim_df = document_sim_df.sort_values(\"cos_sim\", ascending=False)\n    \n    # perform left-join to get information about the documents\n    doc_sim_meta_df = document_sim_df.merge(all_data,\n                      how='left',\n                     left_on='document_id',\n                     right_on='paper_id')\n    return(doc_sim_meta_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_sim = (\n    1 - cdist(\n        document_embeddings_df.values,\n        [model.wv['airborne']],\n        'cosine'\n    )\n)\ndoc_sim.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"document_sim_df = (\n    pd.DataFrame(doc_sim, columns=[\"cos_sim\"])\n    .assign(document_id=list(document_embeddings_df.index))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"document_sim_df.sort_values(by='cos_sim',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cos_sim(text,model):\n    # compute similarity\n    doc_sim = (\n        1-cdist(\n            document_embeddings_df.values,\n            [model.wv[text]],\n            'cosine'\n        )\n    )\n    # convert result to a date frame\n    document_sim_df = (\n        pd.DataFrame(doc_sim, columns=[\"cos_sim\"])\n        .assign(document_id=list(document_embeddings_df.index))\n    )\n    # sort from most similar to least\n    document_sim_df = document_sim_df.sort_values(\"cos_sim\", ascending=False)\n    \n    # perform left-join to get information about the documents\n    doc_sim_meta_df = document_sim_df.merge(all_data,\n                      how='left',\n                     left_on='document_id',\n                     right_on='paper_id')\n    return doc_sim_meta_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def majority_voting(text,model):\n    # choose top 100\n    doc_sim_meta_dfs = [cos_sim(word,model).iloc[:100] for word in text.split()]\n    return pd.merge(*doc_sim_meta_dfs,how = 'inner',on = 'document_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do the same with abstract\nLet's apply the same stuff on abstract to see how it goes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace empty text with empty strings\nall_abstract = all_data.abstract.str.replace('\\n\\n', ' ')\nptn = r'\\[[0-9]{1,2}\\]'\nall_abstract = all_abstract.str.replace(ptn,'').str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_na = all_abstract.notna()\nnon_na_abstract = all_abstract[non_na]\nnon_na_abstract_paper_ids = all_data[non_na]['paper_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_na_abstract_list = list(\n    map(\n        lambda x: word_tokenize(x), non_na_abstract.values\n    ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abst_documents = [TaggedDocument(doc, [non_na_abstract_paper_ids.values[i]]) for i, doc in enumerate(non_na_abstract_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abst_model = Doc2Vec(abst_documents,vector_size = 300,window=2, min_count=1, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"document_dict_abst = {}\nfor idx, text_df in tqdm.tqdm(all_data[non_na][[\"paper_id\", \"abstract\"]].iterrows()):\n    document_dict_abst[text_df['paper_id']] = abst_model.docvecs[text_df['paper_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abst_document_embeddings_df = pd.DataFrame.from_dict(document_dict_abst, orient=\"index\")\nabst_document_embeddings_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abst_doc_sim = (\n    1 - cdist(\n        abst_document_embeddings_df.values,\n        [abst_model.wv['airborne']],\n        'cosine'\n    )\n)\nabst_doc_sim.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abst_document_sim_df = (\n    pd.DataFrame(abst_doc_sim, columns=[\"cos_sim\"])\n    .assign(document_id=list(abst_document_embeddings_df.index))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abst_document_sim_df.sort_values('cos_sim',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"majority_voting('non-pharmaceutical interventions',abst_model).loc[:100,'title_x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cos_sim('airborne',abst_model).loc[:100,'title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace empty text with empty strings\nall_text = all_data.text.str.replace('\\n\\n', ' ')\nptn = r'\\[[0-9]{1,2}\\]'\nall_text = all_text.str.replace(ptn,'').str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_na_text_list = list(\n    map(\n        lambda x: word_tokenize(x), all_text.values\n    ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del all_text\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_documents = [TaggedDocument(doc, [all_data.loc[i,'paper_id']]) for i, doc in enumerate(non_na_abstract_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_model = Doc2Vec(text_documents,vector_size = 300,window=2, min_count=1, workers=4)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}