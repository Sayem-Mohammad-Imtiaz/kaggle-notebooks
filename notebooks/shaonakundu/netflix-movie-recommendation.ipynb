{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nimport os\nimport random\nimport matplotlib\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\n\nfrom sklearn.decomposition import TruncatedSVD\n#from sklearn.metrics.pariwise import cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = datetime.now()\nif not os.path.isfile('data.csv'):\n    #read all txt file and store them in one big file\n    data = open('data.csv', mode='w')\n    \n    row = list()\n    files = ['../input/netflix-prize-data/combined_data_1.txt', '../input/netflix-prize-data/combined_data_2.txt',\n            '../input/netflix-prize-data/combined_data_3.txt', '../input/netflix-prize-data/combined_data_4.txt']\n    for file in files:\n        print('reading ratings from {}...'.format(file))\n        with open(file) as f:\n            for line in f:\n                del row[:]\n                line = line.strip()\n                if line.endswith(':'):\n                    #all are rating\n                    movid_id = line.replace(':', '')\n                else:\n                    row = [x for x in line.split(',')]\n                    row.insert(0, movid_id)\n                    data.write(','.join(row))\n                    data.write('\\n')\n        print('Done.\\n')\n    data.close()\nprint('time taken:', datetime.now() - start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('creating the dataframe from data.csv file..')\ndf = pd.read_csv('data.csv', sep=',', names=['movie','user','rating','date'])\n\ndf.date = pd.to_datetime(df.date)\nprint('Done.\\n')\n\n#arranging the rating according to time\nprint('sorting the dataframe by date..')\ndf.sort_values(by='date', inplace=True)\nprint('sorting done.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()['rating']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking NaN values**","metadata":{}},{"cell_type":"code","source":"print('number of NaN values in our dataset:', sum(df.isnull().any()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check and Remove Duplicate**","metadata":{}},{"cell_type":"code","source":"dup = df.duplicated(['movie','user','rating'])\ndups = sum(dup) #considering by column\nprint('there are {} duplicate rating entries in the data.....'.format(dups))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total Data')\nprint(\"-\"*60)\nprint('\\nTotal number of rating:', df.shape[0])\nprint('Total number of users:', len(np.unique(df.user)))\nprint('total number of movie:', len(np.unique(df.movie)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split the dataset**","metadata":{}},{"cell_type":"code","source":"if not os.path.isfile('train.csv'):\n    #create a dataframe and store it\n    df.iloc[:int(df.shape[0]*0.80)].to_csv(\"train.csv\", index=False)\nif not os.path.isfile('test.csv'):\n    #create a dataframe and store it\n    df.iloc[int(df.shape[0]*0.80)].to_csv(\"test.csv\", index=False)\n\ntrain_df = pd.read_csv('train.csv', parse_dates=['date'])\ntest_df = pd.read_csv('test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(columns = 'date')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of rating:',train_df.shape[0])\nprint('Total number of users:', len(np.unique(train_df.user)))\nprint('Total number of movies:', len(np.unique(train_df.movie)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of rating:',test_df.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA on Train Data**","metadata":{}},{"cell_type":"code","source":"def human(num, units='M'):\n    units = units.lower()\n    num = float(num)\n    if units == 'k':\n        return str(num/10**3) + \"K\"\n    elif units == 'm':\n        return str(num/10**6) + \"M\"\n    elif units == 'b':\n        return str(num/10**9) + \"B\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nplt.title('Rating Distribution over train data', fontsize=10)\nsns.countplot(train_df.rating, palette=\"Set2\")\nax.set_yticklabels([human(item,'M') for item in ax.get_yticks()])\nax.set_ylabel('No. of Ratings (Million)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above distribution we see that most people give a rating of 4 and few people gave a rating of 1.","metadata":{}},{"cell_type":"code","source":"no_of_rated_movie_per_user = train_df.groupby(by='user')['rating'].count().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_rated_movie_per_user.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating sparse matrix from data frame","metadata":{}},{"cell_type":"code","source":"start = datetime.now()\nif os.path.isfile('train_sparse_matrix.npz'):\n    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\nelse:\n    train_sparse_matrix = sparse.csr_matrix((train_df.rating.values, (train_df.user.values, train_df.movie.values)),)\n    print('It is shape is:(user, movie):', train_sparse_matrix.shape)\n    \nprint(datetime.now() - start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sparsity of Train Sparse Matrix","metadata":{}},{"cell_type":"code","source":"us, mv = train_sparse_matrix.shape\nelem = train_sparse_matrix.count_nonzero()\n\nprint(elem)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('sparsity of train matrix:{}%'.format((1-(elem/us*mv)))*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Find Average of all movie ratings, average rating per user, average rating per movie**","metadata":{}},{"cell_type":"code","source":"def get_average_ratings(sparse_matrix, of_users):\n    #avg rating from user\n    ax = 1 if of_users else 0\n    \n    #'.A1' is for converting column_matrix to 1-D numpy array\n    sum_of_ratings = sparse_matrix.sum(axis=ax).A1\n    \n    #boolean matrix of ratings (user read or not)\n    is_rated = sparse_matrix!=0\n    \n    #no.of ratings that each user\n    no_of_ratings = is_rated.sum(axis=ax).A1\n    \n    u,m = sparse_matrix.shape\n    \n    #create a dictionary of users and their avg \n    average_ratings = {i : sum_of_ratings[i]/no_of_ratings[i] for i in range(u if of_users else m) if no_of_ratings[i]!=0}\n    \n    return average_ratings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Global average of all movie ratings**","metadata":{}},{"cell_type":"code","source":"train_averages = dict()\n\n#get global average \ntrain_global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\ntrain_averages['global'] = train_global_average\ntrain_averages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Avg Rating per user","metadata":{}},{"cell_type":"code","source":"train_averages['user'] = get_average_ratings(train_sparse_matrix, of_users=True)\nprint('\\nAverage rating of user 10 :',train_averages['user'][10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Avg Rating per movie","metadata":{}},{"cell_type":"code","source":"train_averages['movie'] = get_average_ratings(train_sparse_matrix, of_users=False)\nprint('\\n Average rating of movie 15:', train_averages['movie'][15])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_user_similarity(sparse_matrix, compute_for_few=False, top = 100, verbose=False, verb_for_n_rows = 20,\n                            draw_time_taken=True):\n    no_of_users, _ = sparse_matrix.shape\n    # get the indices of  non zero rows(users) from our sparse matrix\n    row_ind, col_ind = sparse_matrix.nonzero()\n    row_ind = sorted(set(row_ind)) # we don't have to\n    time_taken = list() #  time taken for finding similar users for an user..\n    \n    # we create rows, cols, and data lists.., which can be used to create sparse matrices\n    rows, cols, data = list(), list(), list()\n    if verbose: print(\"Computing top\",top,\"similarities for each user..\")\n    \n    start = datetime.now()\n    temp = 0\n    \n    for row in row_ind[:top] if compute_for_few else row_ind:\n        temp = temp+1\n        prev = datetime.now()\n        \n        # get the similarity row for this user with all other users\n        sim = cosine_similarity(sparse_matrix.getrow(row), sparse_matrix).ravel()\n        # We will get only the top ''top'' most similar users and ignore rest of them..\n        top_sim_ind = sim.argsort()[-top:]\n        top_sim_val = sim[top_sim_ind]\n                # add them to our rows, cols and data\n        rows.extend([row]*top)\n        cols.extend(top_sim_ind)\n        data.extend(top_sim_val)\n        time_taken.append(datetime.now().timestamp() - prev.timestamp())\n        if verbose:\n            if temp%verb_for_n_rows == 0:\n                print(\"computing done for {} users [  time elapsed : {}  ]\"\n                      .format(temp, datetime.now()-start))\n            \n        \n    # lets create sparse matrix out of these and return it\n    if verbose: print('Creating Sparse matrix from the computed similarities')\n    #return rows, cols, data\n    \n    if draw_time_taken:\n        plt.plot(time_taken, label = 'time taken for each user')\n        plt.plot(np.cumsum(time_taken), label='Total time')\n        plt.legend(loc='best')\n        plt.xlabel('User')\n        plt.ylabel('Time (seconds)')\n        plt.show()\n        \n    return sparse.csr_matrix((data, (rows, cols)), shape=(no_of_users, no_of_users)), time_taken       ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = datetime.now()\nu_u_sim_sparse, _ = compute_user_similarity(train_sparse_matrix, compute_for_few=True, top = 100,\n                                                     verbose=True)\nprint(\"-\"*100)\nprint(\"Time taken :\",datetime.now()-start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Computing Movie-Movie similarity Matrix**","metadata":{}},{"cell_type":"code","source":"start = datetime.now()\nif not os.path.isfile('m_m_sparse.npz'):\n    print('It seems dont have a file. computing movie_movie smimilarity...')\n    start = datetime.now()\n    m_m_sim_sparse = cosine_similarity(X=train_sparse_matrix.T, dense_output = False)\n    \n    #store this sparse matrix in disk \n    #print('saving it to disk without the need of re-computing it again')\n    #sparse.save_npz(\"m_m_sim_sparse.npz\", m_m_sim_sparse)\nelse:\n    print('it is there.')\n    m_m_sim_sparse = sparse.load_npz(\"m_m_sim_sparse\")\n    \nprint(\"it is a \", m_m_sim_sparse.shape, \"dimensional matrix\")\n\nprint(datetime.now() - start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We take only those top similar movie ratings and store them in a separate dictionary.","metadata":{}},{"cell_type":"code","source":"movie_ids = np.unique(m_m_sim_sparse.nonzero()[1])\n\nstart  = datetime.now()\nsimilar_movies = dict()\nfor movie in movie_ids:\n    sim_movies = m_m_sim_sparse[movie].toarray().ravel().argsort()[::-1][1:]\n    similar_movies[movie] = sim_movies[:100]\nprint(datetime.now() - start)\n\n#testing similar movies for movie_15\nsimilar_movies[15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding Most Similar Movie","metadata":{}},{"cell_type":"code","source":"movie_titles = pd.read_csv(\"../input/netflix-prize-data/movie_titles.csv\", sep=',', header = None,\n                           names=['movie_id', 'year_of_release', 'title'], verbose=True,\n                      index_col = 'movie_id', encoding = \"ISO-8859-1\")\n\nmovie_titles.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mv_id = 36\n\nprint(\"\\nMovie ----->\",movie_titles.loc[mv_id].values[1])\n\nprint(\"\\nIt has {} Ratings from users.\".format(train_sparse_matrix[:,mv_id].getnnz()))\n\nprint(\"\\nWe have {} movies which are similarto this  and we will get only top most..\".format(m_m_sim_sparse[:,mv_id].getnnz()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarities = m_m_sim_sparse[mv_id].toarray().ravel()\n\nsimilar_indices = similarities.argsort()[::-1][1:]\n\nsimilarities[similar_indices]\n\nsim_indices = similarities.argsort()[::-1][1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie_titles.loc[sim_indices[:10]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now using ML models(SVD model using surprise package)**\n* reading the text files and combining them together in a single dataset \"Data\"","metadata":{}},{"cell_type":"code","source":"# write a function to read files to a dictionary\ndef read_file(file_path, nrows = 1000):\n    datadict = {\"User\":[],\"Movie\":[],\"Rating\":[]}; # dictionary holder, no values\n    file = open(file_path,\"r\"); # open file for reading\n    count =1;\n    for line in file:\n        if count>nrows:\n            break;\n        if \":\" in line:\n            movie_id = line[:-2];\n            movie_id = int(movie_id);\n        else:\n            user_id,rating,date = line.split(\",\");\n            datadict[\"User\"].append(user_id) ;\n            datadict[\"Movie\"].append(movie_id);\n            datadict[\"Rating\"].append(rating);\n            # exclude date because we do not need it for prediction\n        count +=1;\n    file.close(); #close file after reading\n    return pd.DataFrame(datadict); ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRow = 50000;\nfilepath = \"/kaggle/input/netflix-prize-data/combined_data_1.txt\"\ndf1 = read_file(filepath, nrows = nRow)\nfilepath = \"/kaggle/input/netflix-prize-data/combined_data_2.txt\"\ndf2 = read_file(filepath, nrows = nRow)\nfilepath = \"/kaggle/input/netflix-prize-data/combined_data_3.txt\"\ndf3 = read_file(filepath, nrows = nRow)\nfilepath = \"/kaggle/input/netflix-prize-data/combined_data_4.txt\"\ndf4 = read_file(filepath, nrows = nRow)\n# merge data to make a user-product data\nData = df1.append(df2)\nData=Data.append(df3)\nData.append(df4)\nData.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Using surprise package to use SVD technique","metadata":{}},{"cell_type":"code","source":"#import the libraries from python surprise package\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection.validation import cross_validate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create objects of Reader and SVD classes\nreader = Reader()\nsvd = SVD()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform cross validation to check accuracy using evaluation metrics ","metadata":{}},{"cell_type":"code","source":"# prepare the trainig data set in the order of \"item, user, rating\"\ntrain_data = Dataset.load_from_df(Data[['User', 'Movie', 'Rating']], reader)\n# validate the svd model with cross_validate\ncross_validate(svd,train_data,measures = [\"RMSE\",\"MAE\"],cv = 5,verbose = True)\n\n# build model on the entire data set\ntrain_set = train_data.build_full_trainset()\n\n# fit model on train set\nsvd.fit(train_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie_titles = pd.read_csv(\"../input/netflix-prize-data/movie_titles.csv\", sep=',', header = None,\n                           names=['movie_id', 'year_of_release', 'title'], verbose=True, encoding = \"ISO-8859-1\")\n\nmovie_titles.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding the top similar movies by estimating their ranks(top values)","metadata":{}},{"cell_type":"code","source":"# use the model for recommendation for a userID, say userID = Data.iloc[0,0]\nuserID = 823519;#Data.iloc[0,0]; # example of a customer\nmovie_titles[\"EstimateRank\"] = movie_titles[\"movie_id\"].apply(lambda x: svd.predict(userID,x).est);\nmovie_titles=movie_titles.sort_values(by=[\"EstimateRank\"], ascending = False)\nmovie_titles.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}