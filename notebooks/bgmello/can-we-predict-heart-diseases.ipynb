{"cells":[{"metadata":{"_uuid":"1e2ae1f9bcd71c04b75a270753d5f44c86b14604"},"cell_type":"markdown","source":"# Heart Disease UCI"},{"metadata":{"_uuid":"642c181af77331af35c807f550b5407cc3a376f5"},"cell_type":"markdown","source":"This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\nCan we find a corrleation between the features and the presence of heart dieases?"},{"metadata":{"_uuid":"7f64bbb3e58f899b994580e0db0d53d7dffb4e81"},"cell_type":"markdown","source":"## Features descriptions\n\nAttribute Information: \n- 1. age: age\n- 2. sex: sex\n- 3. cp: chest pain type (4 values) \n- 4. trespbps: resting blood pressure \n- 5. chol: serum cholestoral in mg/dl \n- 6. fbs: fasting blood sugar > 120 mg/dl\n- 7. restecg: resting electrocardiographic results (values 0,1,2)\n- 8. thalach: maximum heart rate achieved \n- 9. exang: exercise induced angina \n- 10. oldpeak: ST depression induced by exercise relative to rest \n- 11. slope: the slope of the peak exercise ST segment \n- 12. ca: number of major vessels (0-3) colored by flourosopy \n- 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"},{"metadata":{"_uuid":"4a9c53127ef643f31aa86b3db5f4df5e5f548c1d"},"cell_type":"markdown","source":"## Imports and configurations"},{"metadata":{"trusted":true,"_uuid":"5fc86f6ef0c30cdeb38b85065f2c6ee3ee1f9397"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_curve, confusion_matrix\nfrom mlxtend.classifier import StackingClassifier\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings('ignore')\ndefaultcolor = '#66ccff'\npd.options.display.float_format = '{:.2f}'.format\nrc={'savefig.dpi': 75, 'figure.autolayout': False, 'figure.figsize': [12, 8], 'axes.labelsize': 18,\\\n   'axes.titlesize': 18, 'font.size': 18, 'lines.linewidth': 2.0, 'lines.markersize': 8, 'legend.fontsize': 16,\\\n   'xtick.labelsize': 16, 'ytick.labelsize': 16}\nsns.set(style='ticks',rc=rc)\nsns.set_palette('husl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a50e4329191a15ef35b3c71eae4ef506f8d052f2"},"cell_type":"markdown","source":"## Overall look at the data trying to find any quick insight"},{"metadata":{"trusted":true,"_uuid":"33306f23e9d8a58162c5210d609b3b90c61d1205"},"cell_type":"code","source":"df = pd.read_csv('../input/heart.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7bef07c7af81c4025ff9fb8d6b88282f7eaf51f8"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61b86bae252f09dae16c8b6925927b9d5be7af35"},"cell_type":"markdown","source":"Let's try to get quick insights about the data"},{"metadata":{"trusted":false,"_uuid":"5f2b54cefaf29df30e216c7fa607e401c75090b9"},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.countplot(df.target, ax=ax)\nfor i,p in enumerate(ax.patches):\n    ax.annotate('{:.2f}%'.format(df['target'].value_counts().apply(lambda x: 100*x/df['target'].value_counts().sum())[i]), (p.get_x()+0.32, p.get_height()+1)).set_fontsize(15)\nax.set_ylabel(\"\")\nax.set_xlabel(\"\")\nax.set_title(\"Target distribution\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baabebfb8de90ee35c684a434ea71c7d05126718"},"cell_type":"markdown","source":"We have a farely well distrubited dataset so we won't have to worry to much with the model \"remebering\" the train targets."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"14fd9ecc563c7d685488aebc5b87e56885218469"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=[15,15])\ndf.hist(ax=ax, bins=30, color='b');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"138f42d50ab139a836ac9881bcc145fd6c5ee2b7"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=[20,15])\nsns.heatmap(df.corr(), ax=ax, cmap='Blues', annot=True);\nax.set_title(\"Pearson correlation coefficients\", size=20);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"876de4102f96487972df4a1d0a013da8155880d9"},"cell_type":"markdown","source":"## Exploratory data analysis"},{"metadata":{"_uuid":"329b73db61524d4efb825de2544e22c42e84979e"},"cell_type":"markdown","source":"### Age distribution"},{"metadata":{"trusted":false,"_uuid":"c07c71de7755d033fd9f79e240316b788dbbffec"},"cell_type":"code","source":"fig, ax = plt.subplots()\ndf.groupby(['age', 'target']).size().reset_index().pivot(index='age', columns='target', values=0).fillna(0).plot.bar(stacked=True, ax=ax)\nax.set_title(\"Distribution of the target according to the age\")\nax.set_xlabel(\"\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b6c644b3c6767623421fa66985a0d298ae4c223"},"cell_type":"markdown","source":"First thing that can be noticed is that for ages between 40 and 50 the proportion of target=1 is pretty high comparing to pepole with ages ranging from 57 to 67. "},{"metadata":{"trusted":false,"_uuid":"7a1934af1089b789fa5970ac9b4b1e4c1f89af16"},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.scatterplot(x='age', y='thalach', data=df[df.target==1], color='b', ax=ax)\nsns.scatterplot(x='age', y='thalach', data=df[df.target==0], color='r', ax=ax)\nax.legend(['1', '0']);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c2fd20c7d368fa3eef20d8999fa0fefc65ce388"},"cell_type":"markdown","source":"We can see we have a good separtion here between 1 and 0, which is good for our model."},{"metadata":{"_uuid":"12440fbcfd253610420c9d4a15ba28741fb6efeb"},"cell_type":"markdown","source":"### Target and cp"},{"metadata":{"trusted":false,"_uuid":"c4985471a0ea8b6909fd7840298f587eacd1a67c"},"cell_type":"code","source":"sns.heatmap(df.groupby(['exang', 'cp']).size().reset_index().pivot(columns='exang', index='cp', values=0), cmap='Blues', fmt='g', annot=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8162275f95e073f86becba3c064df403d7bde9a"},"cell_type":"markdown","source":"### Slope and thalach correlation"},{"metadata":{"trusted":false,"_uuid":"277a6d298b858bb0944d1bd545dd3456e81c5850"},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.boxplot(x='slope', y='thalach', data=df, ax=ax);\nax.set_title(\"Thalach distribution by slope values\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90d433b1073ee198b771c903c1a846ebe4dba49e"},"cell_type":"markdown","source":"Looks like we don't have a big difference between 0 and 1 but when slope=2 the thalach distribution get's narrower and higher."},{"metadata":{"_uuid":"b58e216d560693d67ac788c1617a57c61d067c51"},"cell_type":"markdown","source":"### Thalach and cp"},{"metadata":{"trusted":false,"_uuid":"5b9d7754882729de6a3b2c31dfcb7b6831774f03"},"cell_type":"code","source":"sns.violinplot(x='cp', y='thalach', data=df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cd36488c448f8ff5e89e2d8ddf6204849c1fed8"},"cell_type":"markdown","source":"### Slope and oldpeak"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"a7d76954428f087135245e344ddd5f167d8b44d9"},"cell_type":"code","source":"sns.boxplot(x='slope', y='oldpeak', data=df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64d75f0512a79f674f6f47abe4fa8e4593d61ec4"},"cell_type":"markdown","source":"We can see a clear correlation between these two features, why is that?\n\nAfter searching around in the internet we can found out that both of them are metrics to evaluate the ST segment and having an low oldpeak means you probably will have a high slope.\n\nThere is a lot of bilbiography on the internet about those features. "},{"metadata":{"_uuid":"d6ab24aa106ed29c2b9a750b6fb78cfc92b5cec9"},"cell_type":"markdown","source":"Here is a figure representing the ST segment."},{"metadata":{"_uuid":"57ac8da54e4d070cb522cc9443505699aa6bd2c7"},"cell_type":"markdown","source":"<img src='https://www.teachingmedicine.com/media/lessons/images/Screen%20Shot%202014-06-01%20at%204_12_09%20PM.png'></img>\n\n<p>Extracted from <a href='https://www.teachingmedicine.com/Lesson.aspx?l_id=139'>teachingmedicine</a></p>"},{"metadata":{"_uuid":"81c70d938ae3396db65ae87b0cf445a1e3081748"},"cell_type":"markdown","source":"## Baseline models"},{"metadata":{"_uuid":"46524a54edddceb12472b861dc4133f713aa79d0"},"cell_type":"markdown","source":"#### Let's first split into training and testing and create a dictionary with the most common models"},{"metadata":{"trusted":false,"_uuid":"f9a3e8eaddba6fd7fcf30d56b148ccdc8e044710"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df.target, test_size=0.2, random_state=56)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9913df87b019a4a265d2510e11eeb2e881d1e645"},"cell_type":"code","source":"models = {\n    'CART': DecisionTreeClassifier(),\n    'SVC': SVC(probability=True),\n    'XGB': XGBClassifier(n_jobs=-1),\n    'GNB': GaussianNB(),\n    'LDA': LinearDiscriminantAnalysis(),\n    'LR': LogisticRegression(),\n    'KNN': KNeighborsClassifier()\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"306036ef123871c990d3443a1ff7d4618f7fbcd2"},"cell_type":"markdown","source":"#### Some useful functions"},{"metadata":{"trusted":false,"_uuid":"77c05b3ac76ab0f55bd971dea64b791037cd9e67"},"cell_type":"code","source":"def cv_report(models, X, y):\n    results = []\n    for name in models.keys():\n        model = models[name]\n        scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n        print(\"Accuracy: %.3f (+/- %.3f) [%s]\" %(scores.mean(), scores.std(), name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1dc3e0f1c176cae3948a56094db187c31fc157fb"},"cell_type":"code","source":"cv_report(models, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2e7d2838809f99e1e7217708039fd52bec963a3"},"cell_type":"markdown","source":"As we can see the baseline models already had a good performance, let's try to improve them."},{"metadata":{"_uuid":"e0d2e8bfd2e5634cf1bb6cd986037e5e8052af0c"},"cell_type":"markdown","source":"#### Hyperparameters tunneling using gridsearch"},{"metadata":{"_uuid":"d702ec742cc80806e9b4e274d47cae485166c8c4"},"cell_type":"markdown","source":"##### XGBoost"},{"metadata":{"trusted":false,"_uuid":"98f4bd4429b72ab4b7af1b86f253364d537f8ac7"},"cell_type":"code","source":"xgb_params = {\n    'max_depth': [2,3,4],\n    'n_estimators': [50, 100, 400, 1000],\n    'learning_rate': [0.1, 0.01, 0.05]\n}\n\nxg_grid = GridSearchCV(models['XGB'], xgb_params, cv=5)\nmodels['XGB_Grid'] = xg_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea0296c110f8d604f4d7656b960a6ca6981d6311"},"cell_type":"code","source":"cv_report(models, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6af6000407d90a0ea0eae253aeca9ca89108168f"},"cell_type":"markdown","source":"##### Logistic Regression"},{"metadata":{"trusted":false,"_uuid":"e4b66b844ebbfecde13bca979d9e4f9756d0553f"},"cell_type":"code","source":"lr_params = [{\n                'penalty': ['l2'],\n                'C': (0.1, 0.5, 1.0, 1.5, 2.0),\n                'solver': ['newton-cg', 'lbfgs', 'sag'],\n                'max_iter': [50, 100, 200, 500]\n            },\n            {\n                'penalty': ['l1', 'l2'],\n                'C': (0.1, 0.5, 1.0, 1.5, 2.0),\n                'solver': ['liblinear', 'saga']\n            }\n]\n\nlr_grid = GridSearchCV(models['LR'], lr_params, cv=5)\nmodels['LR_Grid'] = lr_grid","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"faa0cc4a084e9ca3459292a2d040b134fd4a323a"},"cell_type":"code","source":"cv_report(models, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47346b6eedbe356321fcd16382c148a4f6981926"},"cell_type":"markdown","source":"Let's use the tunneled LR for predicting in the test set"},{"metadata":{"trusted":false,"_uuid":"fd3186e19a12103db79795054a231670b91c20ce"},"cell_type":"code","source":"models['LR_Grid'].fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aa24031ffe643844d96bb75cdc0900ea93389762"},"cell_type":"code","source":"predictions = models['LR_Grid'].predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8326c77eeb9f41e302892afa0436f2e549045fa2"},"cell_type":"code","source":"print(\"Accuracy of the model: {:.2f}%\".format(100*accuracy_score(predictions, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c3df8fa3f303133e08900fbfa0808178741a37c9"},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.set_title(\"Confusion Matrix\")\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap='Blues');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}