{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Building a Model to Predict Company Bankruptcy\nUsing the bankruptcy data from the Taiwan Economic Journal for the years 1999â€“2009, I'll create a model to predict whether or not a company will go bankrupt.\n\n### Outline:\n1. Import libraries and data\n2. Check data for categorical data and null values\n3. Check if classes are even (ie. how many records do we have for each class?)\n4. Build and compare algorithms with and without the SMOTE sampling technique"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\n\n# Import dataset\nfile_path = '/kaggle/input/company-bankruptcy-prediction/data.csv'\ndf = pd.read_csv(file_path)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check datatypes of dataframe columns\ndf.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features are numeric. Since there are 96 columns, I'll do a quick check to see if there are any null values in the entire dataset and then drill down by column if needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Null Values in Dataset: ',df.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are no null values in the dataset, I can move on to check if the two classes (bankrupt, and not bankrupt) are even:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigate count of classes\ndf['Bankrupt?'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classes are not even. This is something I'll look to handle as I decide on algorithms and sampling techniques to use.\n\nI'll define a function to output the metrics I want to see when comparing models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ModelPerformanceMetrics(Y_test,Y_pred):\n    cf = confusion_matrix(Y_test,Y_pred)\n    precision = precision_score(Y_test,Y_pred)\n    recall = recall_score(Y_test,Y_pred)\n    accuracy = accuracy_score(Y_test,Y_pred)\n    fscore = f1_score(Y_test,Y_pred)\n    print(cf)\n    print('Precision: {} \\nRecall: {} \\nAccuracy: {} \\nFScore: {}' \\\n          .format(round(precision,2),round(recall,2),round(accuracy,2),round(fscore,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear that Logistic Regression does not perform well on this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a logistic regression model\nX = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This also illustrates why it is necessary to consider a variety of metrics when evaluating a model. Next, I'll try oversampling by increasing the less prevalent class (bankrupt companies) to be less than or equal to the more prevalent class (solvent companies):"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = pd.concat([X_train,Y_train], axis = 1)\ndf_bankrupt = training_set.loc[df['Bankrupt?'] == 1]\ndf_solvent = training_set.loc[df['Bankrupt?'] == 0]\nmultiplier = len(df_solvent)//len(df_bankrupt)\ndf_bankrupt_boosted = pd.concat([df_bankrupt]*multiplier, ignore_index = True)\ndf_oversampled = pd.concat([df_bankrupt_boosted,df_solvent], ignore_index = True)\n\nX_train = df_oversampled.drop(['Bankrupt?'], axis = 1)\nY_train = df_oversampled['Bankrupt?']\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That was better than logistic regression, but still not a useful model. I'll try undersampling, by removing a large number of records from the more prevalent class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bankrupt = df.loc[df['Bankrupt?'] == 1]\ndf_solvent = df.loc[df['Bankrupt?'] == 0].iloc[0:220,:]\ndf_undersampled = pd.concat([df_bankrupt,df_solvent], ignore_index = True)\n\nX = df_undersampled.drop(['Bankrupt?'], axis = 1)\nY = df_undersampled['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Undersampling improved performance, but still not enough to be useful. I'll see if random forest can handle the imbalanced classes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\nClassifier = RandomForestClassifier(n_estimators = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest isn't useful either.\n\nI'll try the XGBoost algorithm:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\nClassifier = XGBClassifier(use_label_encoder=False)\nClassifier = Classifier.fit(X_train, Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost didn't perform very well here. I'll try the SMOTE sampling technique and Logistic Regression next:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nsm = SMOTE(sampling_strategy = 'auto', k_neighbors = 5, random_state = 0)\nX_smote, Y_smote = sm.fit_resample(X, Y)\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, test_size = 0.2, random_state = 0)\n\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SMOTE and Logistic Regression performed better than the undersampling technique, which was the best option so far. I'll combine SMOTE and XGBoost to see if that helps:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nsm = SMOTE(sampling_strategy = 'auto', k_neighbors = 5, random_state = 0)\nX_smote, Y_smote = sm.fit_resample(X, Y)\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, test_size = 0.2, random_state = 0)\n\nClassifier = XGBClassifier(use_label_encoder=False)\nClassifier = Classifier.fit(X_train, Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SMOTE technique used with the XGBoost algorithm performed exceptionally well, with precision of 98%, recall close to 100%, and an F-Score of 99%."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}