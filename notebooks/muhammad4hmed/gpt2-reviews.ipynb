{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\nimport torch.nn.functional as F\nimport os\nimport csv","metadata":{"id":"BvAEHzZ5c3_1","execution":{"iopub.status.busy":"2021-07-27T08:20:05.715808Z","iopub.execute_input":"2021-07-27T08:20:05.716207Z","iopub.status.idle":"2021-07-27T08:20:11.911181Z","shell.execute_reply.started":"2021-07-27T08:20:05.716114Z","shell.execute_reply":"2021-07-27T08:20:11.910315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GPT2 with Fine Tuning","metadata":{"id":"Yd7iKbfRPNXs"}},{"cell_type":"markdown","source":"### Prepare data","metadata":{"id":"ugqA8sTyorE7"}},{"cell_type":"code","source":"df = pd.read_csv('../input/amazon-cell-phones-reviews/20191226-reviews.csv')","metadata":{"id":"VA0fsB0kkO3P","execution":{"iopub.status.busy":"2021-07-27T08:20:11.913459Z","iopub.execute_input":"2021-07-27T08:20:11.913708Z","iopub.status.idle":"2021-07-27T08:20:17.635603Z","shell.execute_reply.started":"2021-07-27T08:20:11.913683Z","shell.execute_reply":"2021-07-27T08:20:17.634723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['words'] = df['body'].apply(lambda x: len(str(x).split(' ')))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:20:17.637279Z","iopub.execute_input":"2021-07-27T08:20:17.637599Z","iopub.status.idle":"2021-07-27T08:20:21.078948Z","shell.execute_reply.started":"2021-07-27T08:20:17.637564Z","shell.execute_reply":"2021-07-27T08:20:21.078035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[~(df['words']<2)].reset_index(drop = True)\ndf = df[~(df['words']>1023)].reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:20:21.083802Z","iopub.execute_input":"2021-07-27T08:20:21.086224Z","iopub.status.idle":"2021-07-27T08:20:21.329797Z","shell.execute_reply.started":"2021-07-27T08:20:21.086176Z","shell.execute_reply":"2021-07-27T08:20:21.32896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"id":"Pt0dHgftPqu1","outputId":"65424d2d-32ef-47db-9809-095d4916df31","execution":{"iopub.status.busy":"2021-07-26T16:48:41.852036Z","iopub.execute_input":"2021-07-26T16:48:41.85245Z","iopub.status.idle":"2021-07-26T16:48:41.861536Z","shell.execute_reply.started":"2021-07-26T16:48:41.852419Z","shell.execute_reply":"2021-07-26T16:48:41.860133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a very small test set to compare generated text with the reality\ntest_set = df.sample(n = 500)\ndf = df.loc[~df.index.isin(test_set.index)]\n\n#Reset the indexes\ntest_set = test_set.reset_index()\ndf = df.reset_index()","metadata":{"id":"JAoKj2AaNJ8y","execution":{"iopub.status.busy":"2021-07-26T15:37:20.342564Z","iopub.execute_input":"2021-07-26T15:37:20.343088Z","iopub.status.idle":"2021-07-26T15:37:20.464964Z","shell.execute_reply.started":"2021-07-26T15:37:20.343045Z","shell.execute_reply":"2021-07-26T15:37:20.463822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For the test set only, keep last 20 words in a new column, then remove them from original column\ntest_set['Reviews'] = test_set['body'].str.split().str[3:].apply(' '.join)\ntest_set['Tags'] = test_set['body'].str.split().str[:3].apply(' '.join)","metadata":{"id":"hJoTyB_v4x0U","execution":{"iopub.status.busy":"2021-07-26T15:37:20.46661Z","iopub.execute_input":"2021-07-26T15:37:20.467094Z","iopub.status.idle":"2021-07-26T15:37:20.494704Z","shell.execute_reply.started":"2021-07-26T15:37:20.467052Z","shell.execute_reply":"2021-07-26T15:37:20.493697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.head(50)","metadata":{"id":"r0KSKkcMOuRh","outputId":"c6d2c7ad-8d9a-40b5-95be-f02c611430ad","execution":{"iopub.status.busy":"2021-07-26T15:37:20.495897Z","iopub.execute_input":"2021-07-26T15:37:20.49632Z","iopub.status.idle":"2021-07-26T15:37:20.532213Z","shell.execute_reply.started":"2021-07-26T15:37:20.496278Z","shell.execute_reply":"2021-07-26T15:37:20.530921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare the dataset","metadata":{"id":"gqNpDGm_JnFk"}},{"cell_type":"code","source":"class Reviews(Dataset):\n    \n    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.reviews = []\n\n        for row in df['body']:\n          self.reviews.append(torch.tensor(\n                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n            ))\n                \n        if truncate:\n            self.reviews = self.reviews[:20000]\n        self.reviews_count = len(self.reviews)\n        \n    def __len__(self):\n        return self.reviews_count\n\n    def __getitem__(self, item):\n        return self.reviews[item]","metadata":{"id":"V71yg83t6Tlt","execution":{"iopub.status.busy":"2021-07-26T15:37:20.536637Z","iopub.execute_input":"2021-07-26T15:37:20.537096Z","iopub.status.idle":"2021-07-26T15:37:20.70534Z","shell.execute_reply.started":"2021-07-26T15:37:20.537053Z","shell.execute_reply":"2021-07-26T15:37:20.703835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = Reviews(df['body'], truncate=True, gpt2_type=\"gpt2\")","metadata":{"id":"wauU2WYi92dp","execution":{"iopub.status.busy":"2021-07-26T15:37:20.708019Z","iopub.execute_input":"2021-07-26T15:37:20.708787Z","iopub.status.idle":"2021-07-26T16:10:43.790404Z","shell.execute_reply.started":"2021-07-26T15:37:20.708717Z","shell.execute_reply":"2021-07-26T16:10:43.78938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare training","metadata":{"id":"MwIRpL_5MnIY"}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')","metadata":{"id":"3fPgwmaNFTib","execution":{"iopub.status.busy":"2021-07-26T16:10:43.79232Z","iopub.execute_input":"2021-07-26T16:10:43.792828Z","iopub.status.idle":"2021-07-26T16:11:11.3311Z","shell.execute_reply.started":"2021-07-26T16:10:43.792751Z","shell.execute_reply":"2021-07-26T16:11:11.329834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accumulated batch size (since GPT2 is so big)\ndef pack_tensor(new_tensor, packed_tensor, max_seq_len):\n    if packed_tensor is None:\n        return new_tensor, True, None\n    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n        return packed_tensor, False, new_tensor\n    else:\n        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n        return packed_tensor, True, None","metadata":{"id":"Maf_wuBuJl2n","execution":{"iopub.status.busy":"2021-07-26T16:11:11.336029Z","iopub.execute_input":"2021-07-26T16:11:11.337955Z","iopub.status.idle":"2021-07-26T16:11:11.346015Z","shell.execute_reply.started":"2021-07-26T16:11:11.337898Z","shell.execute_reply":"2021-07-26T16:11:11.344684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    dataset, model, tokenizer,\n    batch_size=16, epochs=10, lr=2e-5,\n    max_seq_len=400, warmup_steps=200,\n    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n    test_mode=False,save_model_on_epoch=True,\n):\n\n    acc_steps = 100\n    device=torch.device(\"cuda\")\n    model = model.cuda()\n    model.train()\n\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n    )\n\n    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n    loss=0\n    accumulating_batch_count = 0\n    input_tensor = None\n\n    for epoch in range(epochs):\n\n        print(f\"Training epoch {epoch}\")\n        print(loss)\n        for idx, entry in tqdm(enumerate(train_dataloader)):\n            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n\n            if carry_on and idx != len(train_dataloader) - 1:\n                continue\n\n            input_tensor = input_tensor.to(device)\n            outputs = model(input_tensor, labels=input_tensor)\n            loss = outputs[0]\n            loss.backward()\n\n            if (accumulating_batch_count % batch_size) == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                model.zero_grad()\n\n            accumulating_batch_count += 1\n            input_tensor = None\n        if save_model_on_epoch:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n            )\n    return model","metadata":{"id":"65ZWYy8EJl0D","execution":{"iopub.status.busy":"2021-07-26T16:11:11.34924Z","iopub.execute_input":"2021-07-26T16:11:11.350564Z","iopub.status.idle":"2021-07-26T16:11:11.385637Z","shell.execute_reply.started":"2021-07-26T16:11:11.350496Z","shell.execute_reply":"2021-07-26T16:11:11.383817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Actual Training","metadata":{"id":"LIXXMDBONZtR"}},{"cell_type":"code","source":"#Train the model on the specific data we have\nmodel = train(dataset, model, tokenizer)","metadata":{"id":"qY7dh37IvscH","outputId":"521d618e-e69b-4e65-a1b5-eeef06ca4134","execution":{"iopub.status.busy":"2021-07-26T16:11:11.388727Z","iopub.execute_input":"2021-07-26T16:11:11.392885Z","iopub.status.idle":"2021-07-26T16:25:27.445365Z","shell.execute_reply.started":"2021-07-26T16:11:11.392833Z","shell.execute_reply":"2021-07-26T16:25:27.44364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save the model to a pkl or something so it can be reused later on\ntorch.save(model, 'model.pt')","metadata":{"id":"gvk9JcukKKq1","execution":{"iopub.status.busy":"2021-07-26T16:25:27.44736Z","iopub.execute_input":"2021-07-26T16:25:27.447824Z","iopub.status.idle":"2021-07-26T16:25:28.584235Z","shell.execute_reply.started":"2021-07-26T16:25:27.447764Z","shell.execute_reply":"2021-07-26T16:25:28.583137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text generation","metadata":{"id":"9BlNPVfXPNQf"}},{"cell_type":"code","source":"#Load the model to use it\nmodel = torch.load('./model.pt')","metadata":{"id":"GI3VOzbVUN7v","execution":{"iopub.status.busy":"2021-07-26T16:25:28.58568Z","iopub.execute_input":"2021-07-26T16:25:28.586118Z","iopub.status.idle":"2021-07-26T16:25:28.872336Z","shell.execute_reply.started":"2021-07-26T16:25:28.586079Z","shell.execute_reply":"2021-07-26T16:25:28.871294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(\n    model,\n    tokenizer,\n    prompt,\n    entry_count=10,\n    entry_length=30, #maximum number of words\n    top_p=0.8,\n    temperature=1.,\n):\n\n    model.eval()\n\n    generated_num = 0\n    generated_list = []\n\n    filter_value = -float(\"Inf\")\n\n    with torch.no_grad():\n\n        for entry_idx in trange(entry_count):\n\n            entry_finished = False\n\n            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n\n            for i in range(entry_length):\n                outputs = model(generated, labels=generated)\n                loss, logits = outputs[:2]\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                    ..., :-1\n                ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n\n                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n                generated = torch.cat((generated, next_token), dim=1)\n\n                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n                    entry_finished = True\n\n                if entry_finished:\n\n                    generated_num = generated_num + 1\n\n                    output_list = list(generated.squeeze().numpy())\n                    output_text = tokenizer.decode(output_list)\n                    generated_list.append(output_text)\n                    break\n            \n            if not entry_finished:\n              output_list = list(generated.squeeze().numpy())\n              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n              generated_list.append(output_text)\n                \n    return generated_list","metadata":{"id":"MQUN1Da2JluS","execution":{"iopub.status.busy":"2021-07-26T16:25:28.87418Z","iopub.execute_input":"2021-07-26T16:25:28.874613Z","iopub.status.idle":"2021-07-26T16:25:28.889302Z","shell.execute_reply.started":"2021-07-26T16:25:28.874568Z","shell.execute_reply":"2021-07-26T16:25:28.887783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to generate multiple sentences. Test data should be a dataframe\ndef text_generation(test_data):\n  generated_lyrics = []\n  for i in range(len(test_data)):\n    try:\n        x = generate(model.to('cpu'), tokenizer, test_data['body'][i], entry_count=1)\n        generated_lyrics.append(x)\n    except:\n        generated_lyrics.append('None')\n  return generated_lyrics","metadata":{"id":"5usWIXOKKxij","execution":{"iopub.status.busy":"2021-07-26T16:25:28.89121Z","iopub.execute_input":"2021-07-26T16:25:28.891853Z","iopub.status.idle":"2021-07-26T16:25:28.906179Z","shell.execute_reply.started":"2021-07-26T16:25:28.891808Z","shell.execute_reply":"2021-07-26T16:25:28.905061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_lyrics = text_generation(test_set)","metadata":{"id":"OR4XumaWMx9d","execution":{"iopub.status.busy":"2021-07-26T16:25:28.909101Z","iopub.execute_input":"2021-07-26T16:25:28.910102Z","iopub.status.idle":"2021-07-26T16:46:16.628238Z","shell.execute_reply.started":"2021-07-26T16:25:28.910055Z","shell.execute_reply":"2021-07-26T16:46:16.625146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loop to keep only generated text and add it as a new column in the dataframe\nmy_generations=[]\n\nfor i in range(len(generated_lyrics)):\n  a = test_set['body'][i].split()[-3:] #Get the matching string we want (30 words)\n  b = ' '.join(a)\n  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n  my_generations.append(c.split(b)[-1])\n\ntest_set['Generated_Reviews'] = my_generations","metadata":{"id":"LJY4jjNGGHVk","execution":{"iopub.status.busy":"2021-07-26T16:46:16.629708Z","iopub.status.idle":"2021-07-26T16:46:16.630884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finish the sentences when there is a point, remove after that\nfinal=[]\n\nfor i in range(len(test_set)):\n  to_remove = test_set['Generated_Reviews'][i].split('.')[-1]\n  final.append(test_set['Generated_Reviews'][i].replace(to_remove,''))\n\ntest_set['Generated_Reviews'] = final\ntest_set.head()","metadata":{"id":"HH53eDl6tnyf","outputId":"4cc53049-fdb3-41cf-b73f-2ccfe37e7ff7","execution":{"iopub.status.busy":"2021-07-26T16:46:16.632687Z","iopub.status.idle":"2021-07-26T16:46:16.633498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set['Generated_Reviews'][7]","metadata":{"id":"_nd84NvRqoqU","outputId":"d27bf97b-bf43-42a2-ecdb-76ad3a21f34c","execution":{"iopub.status.busy":"2021-07-26T16:46:16.634977Z","iopub.status.idle":"2021-07-26T16:46:16.635971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set['Reviews'][7]","metadata":{"id":"8_SSPBLkq54G","outputId":"e122c32c-b6e5-4331-fe9d-f57eb5e23ec1","execution":{"iopub.status.busy":"2021-07-26T16:46:16.637541Z","iopub.status.idle":"2021-07-26T16:46:16.638441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyze performance","metadata":{"id":"obMubE_dPJnV"}},{"cell_type":"code","source":"# #Using BLEU score to compare the real sentences with the generated ones\n# import statistics\n# from nltk.translate.bleu_score import sentence_bleu\n\n# scores=[]\n\n# for i in range(len(test_set)):\n#   reference = test_set['Reviews'][i]\n#   candidate = test_set['Generated_Reviews'][i]\n#   scores.append(sentence_bleu(reference, candidate))\n\n# statistics.mean(scores)","metadata":{"id":"MEfAjgyyFnMl","outputId":"469cc208-2eae-443c-f4bb-9a54dd43153d","execution":{"iopub.status.busy":"2021-07-26T16:46:16.640013Z","iopub.status.idle":"2021-07-26T16:46:16.640807Z"},"trusted":true},"execution_count":null,"outputs":[]}]}