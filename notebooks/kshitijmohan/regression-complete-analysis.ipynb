{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Beginners Guide to Regression Analysis and Plot Interpretations**\n![](https://expertsystem.com/wp-content/uploads/2017/03/machine-learning-definition.jpeg)"},{"metadata":{},"cell_type":"markdown","source":"## Table Of Contents:\n* **Understanding Regression**\n* **How Does Regression Work?**\n* **Types of Algorithms**\n* **Testing of Algorithms**"},{"metadata":{},"cell_type":"markdown","source":"## Algorithms that we will consider:-\n1. Simple Linear Regression\n2. Multiple Linear Regression\n3. Polynomial Regression\n4. Support Vestor Regression\n5. Decision Tree Regression\n6. Random Forest Regression"},{"metadata":{},"cell_type":"markdown","source":"## Lets Start with Understanding what is Regression?\n**Regression is a technique used to predict value of one variable(Dependent Variable) on the basis of other variables(Independent Variables). It is parametric in nature because it makes certain assumptions based on the data set. If the data set follows those assumptions, regression gives incredible results. Otherwise, it struggles to provide convincing accuracy.**"},{"metadata":{},"cell_type":"markdown","source":"## How Does Regression Work?\n**Regression is a part of supervised learning which basically means that we train our models on the basis of given training data and our model tries to relate between the dependent and the independent variable. It does this using various functions that maps the independent variables to the dependent variables. When the model is completely trained and the error is minumised then we are able to make predictions on testing data as well.**"},{"metadata":{},"cell_type":"markdown","source":"## We can apply machine learning model by following six steps:-\n* Indentifying Problem\n* Analysing Data\n* Preparing Data\n* Evaluating Algorithm\n* Improving Results\n* Presenting Results"},{"metadata":{},"cell_type":"markdown","source":"## Simple Linear Regression\n**It is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \ny = a*X + b where:**\n* y – Dependent Variable\n* X – Independent variable\n* b – intercept\n* a – Slope"},{"metadata":{},"cell_type":"markdown","source":"## Preparing Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Testing_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values) # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Model\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.scatter(X_test, y_test, color = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multiple Linear Regression\n**It is also a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \ny = b + a1*X1 + a2*X2 + a3*X3 + a4*X4 + ... where:**\n* y – Dependent Variable\n* X1, X2, X3, X4 – Independent variable\n* b – intercept\n* a1, a2, a3 – Slopes "},{"metadata":{},"cell_type":"markdown","source":"## Preparing Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/insurance/insurance.csv')\nprint(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:, :-1] # Independent Variable\ny = dataset.iloc[:, -1] # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have to apply encoding in the dataset as there are words present.\n# for 'sex' and 'smoker' column we will apply Label Encoding as there are only 2 catagories\n# for 'region' we will apply OneHot Encoding as there are more than 2 catagories\n\n# Label Encoding:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX.iloc[:, 1] = le.fit_transform(X.iloc[:, 1])\nX.iloc[:, 4] = le.fit_transform(X.iloc[:, 4])\n\n# OneHot Encoding:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [5])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Model\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Polynomial Regression\n**It is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \ny = b + a1*X + a1*X^2 + a1*X^3 + a1*X^4 where:**\n* y – Dependent Variable\n* X1, X2, X3, X4 – Independent variable\n* b – intercept\n* a1, a2, a3 – Coefficients of independent variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/polynomialregressioncsv/polynomial-regression.csv')\nX = dataset.iloc[:, :-1] # Independent Variable\ny = dataset.iloc[:, -1] # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trianing the Model\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 5)\nX_poly = poly_reg.fit_transform(X)\nlin_reg_2 = LinearRegression()\nlin_reg_2.fit(X_poly, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.scatter(X, y, color = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Regression\n**SVR gives us the flexibility to define how much error is acceptable in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data. It uses following constraints-\n|y - aX| <= e, Where:**\n* e - maximum error\n![](https://miro.medium.com/max/1212/1*bSZn9bK43MaA5vVDamRQ2A.png)\nThe points outside the margin are the Support Vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values) # Dependent Variable\ny_train = y_train.reshape(len(y_train),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Testing_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values) # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values) # Dependent Variable\ny_test = y_test.reshape(len(y_test),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scalling X and y\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\ny_train = sc_y.fit_transform(y_train)\ny_test = sc_y.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Model\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(sc_X.inverse_transform(X_test), sc_y.inverse_transform(y_test), color = 'red', \n           marker = 'o', s = 35, alpha = 0.5, label = 'Test data')\nplt.plot(sc_X.inverse_transform(X_test), sc_y.inverse_transform(regressor.predict(X_test)), \n           color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Regression\n**Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Testing_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values) # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_grid = np.arange(min(X_test), max(X_test), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X_test, y_test, color = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regression\n**Random Forest combines multiple trees to predict the class of the dataset, it is possible that some decision trees may predict the correct output, while others may not. But together, all the trees predict the correct output.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Testing_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values) # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_grid = np.arange(min(X_test), max(X_test), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X_test, y_test, c = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Position level')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you very much for your attention to my work. I wish you good datasets for research!"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.pinimg.com/originals/4f/92/fe/4f92fe4ee07e79bc3495e41bb5ae1bd3.gif)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}