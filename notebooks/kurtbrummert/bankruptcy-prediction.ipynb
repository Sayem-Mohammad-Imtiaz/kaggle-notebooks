{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is an example of using random forest method to predict bankruptcy for a company based on the Kaggle bankruptcy-prediction dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# variables\npath_str = \"../input/company-bankruptcy-prediction/data.csv\"\n# For random forest, a number of trees must be selected.\n# The higher number, the more thorough the calculation, but it takes longer to run.\nnumber_of_trees = 200\n# Target column for random forest prediction\ntarget_column_name = 'Bankrupt?'\n# Usually, decision trees can be large.  Setting this variable to 3 or 4 makes the result tree easier to see and interpret.\ntree_depth = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Load data\n# create dataframe from data\n\ndf = pd.read_csv(path_str)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check length-rows and width-columns of data\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use numpy to convert to arrays.\n# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, \n# along with a large collection of high-level mathematical functions to operate on these arrays.\nimport numpy as np\n\n# Assign target variable to separate array\ntarget = np.array(df[target_column_name])\n\n# Remove target column from features\nfeatures = df.drop(target_column_name, axis = 1)\n\n# Saving feature names for later use\nfeature_list = list(features.columns)\n\n# convert features dataframe to array\nfeatures = np.array(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Using Skicit-learn to split data into training and testing sets.\n#  Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.\n#  It features various classification, #  regression and clustering algorithms including support vector machines, random forests, \n#  gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets.  test_size is n% of the rows. The other % will train the model.\ntrain_features, test_features, train_target, test_target = train_test_split(features, target, test_size = 0.25, random_state = 42)\n\n# Check to see that training features and labels have the same rows, and testing features and labels have the same rows\nprint('Training Features Shape:', train_features.shape)\nprint('Training target Shape:', train_target.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing target Shape:', test_target.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model.  n_estimators is the number of decision trees you want to use\nrf = RandomForestRegressor(n_estimators = number_of_trees, random_state = 42)\n\n# Train the model on training data\nrf.fit(train_features, train_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n# pydot may need to be installed. \ntry:\n    import pydot\nexcept ImportError as e:\n    !pip install pydot\n    import pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Limit depth of tree to n levels\nrf_small = RandomForestRegressor(n_estimators=10, max_depth = tree_depth)\nrf_small.fit(train_features, train_target)\n# Extract the small tree\ntree_small = rf_small.estimators_[5]\n# Save the tree as a png image\nexport_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\ngraph.write_png('small_tree.png')\n# show png file\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n* From the above list of importances (listed in descending order), as well as the sample tree from the random forest, we can see that growth rate and income-to-equity are the top factors.  Whether the growth-rate factor would apply to large established companies that grow more slowly, we do not have enough information to determine.\n* As a cross-check on the model, a quick sort does indeed show a high concentration of bankruptcies where net value growth rate is lowest.\n* A view of the raw data shows a correlation between net-income/equity and bankruptcy cases.  This combination of the two top might lead us to conclude that low growth rate and low income are red flags for small firms.\n* A second tier of factors: interest-bearing debt interest rate and borrowing dependency, likely indicate things typically associated with bankruptcy risk, such as high-risk/high-interest debt, and being highly leveraged or dependent on borrowing to stay afloat."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}