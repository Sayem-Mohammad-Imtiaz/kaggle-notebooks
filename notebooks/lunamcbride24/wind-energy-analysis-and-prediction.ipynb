{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wind Energy Analysis and Predictions (Using ARIMA)"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt #Plotting\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (10,10) #Make the plots bigger by default\nplt.rcParams[\"lines.linewidth\"] = 2 #Setting the default line width\nplt.style.use(\"ggplot\") #Define the style of the plot\n\nfrom statsmodels.tsa.stattools import adfuller #Check if data is stationary\nfrom statsmodels.graphics.tsaplots import plot_acf #Compute lag for ARIMA\nfrom statsmodels.graphics.tsaplots import plot_pacf #Compute partial lag for ARIMA\nfrom statsmodels.tsa.arima.model import ARIMA #Predictions and Forecasting\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"companies = [\"50Hertz\", \"Amprion\", \"TenneTTSO\", \"TransnetBW\"] #Get the company names into a list\npath = \"../input/wind-power-generation/\" #Get the path to the company CSVs\ndataframeHolder = [] #Create a list to hold all of the dataframes\n\n#For each company, collect the CSVs\nfor comp in companies:\n    compPath = path + comp + \".csv\" #Build the path to the CSV\n    compDF = pd.read_csv(compPath) #Read the CSV for the company\n    dataframeHolder.append(compDF) #Put the company CSV into the dataframe holder\n    \ndataframeHolder[0].head() #Show the head of the first dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Melt and Combine the Dataframes"},{"metadata":{},"cell_type":"markdown","source":"## Melt the Dataframes"},{"metadata":{},"cell_type":"markdown","source":"Reference to the melt technique in this notebook: https://www.kaggle.com/ggopinathan/german-wind-power-generation-eda"},{"metadata":{"trusted":true},"cell_type":"code","source":"#MeltComp: melts the wide time columns into a single datetime row\n#Input: The company dataframe, the company name\n#Output: The melted dataframe\ndef meltComp(compDF, comp):\n    df = pd.melt(compDF, id_vars = [\"Date\"], var_name = \"Time\", value_name = comp) #Melt the dataframe by date and time\n    df[\"dateTime\"] = df[\"Date\"] + \" \" +df[\"Time\"] #Create a combined datetime column\n    df[\"dateTime\"] = pd.to_datetime(df[\"dateTime\"]) #Turn the datetime column into a datetime type\n\n    df = df.drop(columns = {\"Date\", \"Time\"}) #Drop the date and time columns (they are now redundant)\n    df = df.set_index(\"dateTime\") #Set the index to the new dateTime column\n    df = df.sort_index() #Sort them based on index so that the dates are grouped together\n    df.reset_index(drop = False, inplace = True) #Reset the index\n    return df #Return the new dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meltDF = [] #Create another holder for melted dataframes\ncompIterator = 0 #Create an iterator for the company names\n\n#For each company dataframe, melt it down. We want the times to be rows, not columns.\nfor compDF in dataframeHolder:\n    meltDF.append(meltComp(compDF, companies[compIterator])) #Melt the company dataframe and add it to the melted dataframe holder\n    compIterator += 1 #Increase the company name iterator\n\nmeltDF[0].head() #Show one of the melted dataframes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combine the Dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"mainDF = meltDF[0] #Get the first DF as the one to merge on\nmeltLength = len(meltDF) #Get the number of melted dataframe\n\n#For each company in meltDF, merge the dataframe with the main one (start at 1 to ignore the original main one)\nfor company in range(1, meltLength):\n    mainDF = mainDF.merge(meltDF[company], on = \"dateTime\") #Merge the company dataframes with the main one\n\nmainDF = mainDF.set_index(\"dateTime\") #Set the index to dateTime\nmainDF = mainDF.asfreq(freq = \"15T\", fill_value = 0) #Set the frequency to every 15 minutes\nmainDF.head() #Take a peek at the new dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mainDF.index) #Get the number of rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mainDF.isnull().any()) #Check for any null values\nprint(mainDF.describe()) #Describe the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values. The values also range from 0, meaning there are no -1's that are replacing null values or anything of the sort."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Quick Visualization/Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10,6) #Fix the graph sizes\ncolumns = mainDF.columns #Get the columns to show\n\n#For each company, plot the wind data\nfor col in columns:\n    plt.figure() #Construct the figure so it gives each a separate graph\n    mainDF[col].plot(title = col + \" Wind Data\") #Plot the company wind data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mainDF.loc[mainDF[\"TransnetBW\"] > 200]) #Show the range of the TransnetBW anomoly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mainDF.loc[mainDF.index == \"2019-03-15 00:15:00\"] #Show a date where it was empty, and thus filled with 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset not only has some missing data (represented by the flat lines at 0, as I filled them in previously), but also a pretty significant anomoly for TransnetBW on February 12th, 2019.\n\nOverall, 50Hertz and TenneTTSO appear to register in windier areas, with Amprion in a less windy area and TransnetBW being in an area with very low wind (with exception to that anomoly registered on February 12th, 2019). The wind patterns are relatively flat with very little seasonality (I will still test to see if adfuller agrees, though). The only issue I can see is that anomoly, but I will leave it be for now. \n\nAnother note is that there is over 60000 rows due to the sampling every 15 minutes. I tried working with this how it is, but I could not even run one ARIMA model without it taking up all of the memory. For this case, I will try making this a daily mean, reducing the datapoints from over 60000 to around 700."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Resample to Daily Wind Measures"},{"metadata":{},"cell_type":"markdown","source":"Documentation on Resampling: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"mainDF = mainDF.resample(\"D\").mean() #Change the data to get the daily wind energy means\nmainDF.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mainDF.index) #Get the number of rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That should work much better."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# ADFuller Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For each column, perform the ADFuller test\nfor col in columns:\n    print(\"ADFuller Test; Significance: 0.05. Company: {}\".format(col)) #Print the significance level\n    adf = adfuller(mainDF[col]) #Call adfuller to test\n    print(\"ADF test static for {} is {}\".format(col, adf[1])) #Print the adfuller results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ADFuller agrees that these are already flat."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Get the P and Q lags"},{"metadata":{},"cell_type":"markdown","source":"## AR (P)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (20,6) #Fix the graph size\n\n#For each company, graph the data and get a p value\nfor comp in companies:\n    fig,axes = plt.subplots(1,2) #Create a subplot for the Partial ACF\n    a = axes[0].plot(mainDF[comp]) #Plot one company's data\n    a = axes[0].set_title(comp) #Ensure the data is named\n    b = plot_pacf(mainDF[comp], ax = axes[1], method = \"ols\") #Plot the partial ACF","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5 seems to be a fair p value for all graphs."},{"metadata":{},"cell_type":"markdown","source":"## MA (Q)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For each company, graph the data and get a q value\nfor comp in companies:\n    fig,axes = plt.subplots(1,2) #Create a subplot for the ACF\n    a = axes[0].plot(mainDF[comp]) #Plot one company's data\n    a = axes[0].set_title(comp) #Ensure the data is named\n    b = plot_acf(mainDF[comp], ax = axes[1]) #Plot the ACF","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of these really fit all too well. I will go with 8, since it is the lowest with them in the red interval."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Train the Model"},{"metadata":{},"cell_type":"markdown","source":"Note: this could all be in one loop to lower processing need. I just wanted them to be separate."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [] #Create a list for the models\n\n#For loop to run an ARIMA model for each company\nfor col in columns:\n    model = ARIMA(mainDF[col], order = (5, 1, 8)) #Build the ARIMA model\n    fitModel = model.fit() #Fit the ARIMA model\n    models.append(fitModel) #Put the fit model into the models list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10,6) #Fix the graph size\nmodelIterator = 0 #An iterator to line up the ARIMA models\n\n#For loop to plot ARIMA predictions with the originals\nfor col in columns:\n    plt.figure() #Reset onto a new graph\n    predict = models[modelIterator].predict() #Predict the model outcome\n    predict.plot(title = \"Predict {}\".format(col)) #Plot the predictions\n    mainDF[col].plot() #Plot the original\n    plt.show() #Show the ARIMA plot\n    \n    modelIterator += 1 #Add one to the iterator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These seem to make fair predictions based on what they are given. The large gaps in the data are filled with interesting small predictions instead of predicting 0. This is also likely why it goes negative sometimes. I would like to see how it decides to predict the future based on this."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10, 6) #Fix the graph size\n\nmodelIterator = 0 #An iterator to line up the ARIMA models\n\n#For loop to plot ARIMA predictions with the originals\nfor col in columns:\n    plt.figure() #Reset onto a new graph\n    predict = models[modelIterator].simulate(200, anchor = \"end\") #Predict the future outcomes\n    predict.plot(title = \"Forecast {}\".format(col)) #Plot the predictions\n    mainDF[col].plot() #Plot the original\n    plt.show() #Show the ARIMA plot\n    \n    modelIterator += 1 #Add one to the iterator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems the empty 0 data made the predictions trend lower, even going pretty far negative at times. I think I will try this again with the ARIMA's drop null functionality instead."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Do the Model with NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"mainNA = mainDF.copy() #Make a copy to have with NANs\nmainNA.head() #Take a peek at this dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For each column, make all 0's into NAN\nfor col in columns:\n    mainNA[col] = mainNA[col].apply(lambda x: np.nan if x == 0 else x) #Change 0 to NAN\n    \nmainNA.loc[mainNA.index == \"2019-01-13\"] #Show a date where it was empty, and thus filled with NAN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelsNA = [] #Create a list for the models\n\n#For loop to run an ARIMA model for each company\nfor col in columns:\n    model = ARIMA(mainNA[col], order = (5, 1, 8), missing = \"drop\") #Build the ARIMA model\n    fitModel = model.fit() #Fit the ARIMA model\n    modelsNA.append(fitModel) #Put the fit model into the models list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10,6) #Fix the graph size\nmodelIterator = 0 #An iterator to line up the ARIMA models\n\n#For loop to plot ARIMA predictions with the originals\nfor col in columns:\n    plt.figure() #Reset onto a new graph\n    predictNA = modelsNA[modelIterator].predict() #Predict the model outcome\n    predictNA.plot(title = \"Predict {} no NA\".format(col)) #Plot the predictions\n    mainNA[col].plot() #Plot the original\n    plt.show() #Show the ARIMA plot\n    \n    modelIterator += 1 #Add one to the iterator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10, 6) #Fix the graph size\n\nmodelIterator = 0 #An iterator to line up the ARIMA models\n\n#For loop to plot ARIMA predictions with the originals\nfor col in columns:\n    plt.figure() #Reset onto a new graph\n    predictNA = modelsNA[modelIterator].simulate(200, anchor = \"end\") #Predict the future outcomes\n    predictNA.plot(title = \"Forecast {} no NA\".format(col)) #Plot the predictions\n    mainNA[col].plot() #Plot the original\n    plt.show() #Show the ARIMA plot\n    \n    modelIterator += 1 #Add one to the iterator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The negative values are not as bad with dropped nulls. The model fills the blanks with more middle values when predicting the given data. The forecasts are then pulled up like I predicted, telling me the 0's were pulling the forecasts down. It really shows how much this dataset is missing, which could also be making the forecasts have negative points. I am happy with these forecasts despite this."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}