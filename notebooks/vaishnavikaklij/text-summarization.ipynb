{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Link of Original Code - https://github.com/llSourcell/How_to_make_a_text_summarizer/blob/master/vocabulary-embedding.ipynb**"},{"metadata":{},"cell_type":"markdown","source":"# Installing packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jsonlines\n#!pip install cPickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing and opening .JSONL dataset"},{"metadata":{},"cell_type":"markdown","source":"Dataset link : https://www.kaggle.com/hsankesara/medium-articles \n\nOn kaggle I got this dataset in .csv format then:\n\n1. I converted .csv to .json\n2. Convert .json to .jsonl(.jl)\n3. Convert .jl to .pkl"},{"metadata":{},"cell_type":"markdown","source":"1. I converted the file from  .csv to .json  \n\nHere is gdrive link to download the .jsonfile : https://drive.google.com/file/d/1Qpq18as3I7qR-tgB2q93FBcMCpd1MRed/view?usp=sharing"},{"metadata":{},"cell_type":"markdown","source":"2. Converting JSON file to JSONL"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport jsonlines\n\nwith open(\"../input/aarticle/articles.json\", 'r') as f:\n    json_data = json.load(f)\n    \nwith open(\"../working/articles.jl\", 'w') as outfile:\n    for entry in json_data:\n        json.dump(entry, outfile)\n        outfile.write('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../working/articles.jl') as myfile:\n    firstNLines=myfile.readlines()[0:1]\n    print(firstNLines)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating pickle file"},{"metadata":{},"cell_type":"markdown","source":"1.Use the 'content / description / text' as the 'desc' and the 'title' as the 'head'.\n2.Once you have the data ready save it in a python pickle file as a tuple: (heads, descs, keywords) were heads is a list of all the head strings, descs is a list of all the article strings in the same order and length as heads. I ignore the keywrods information so you can place None.\n\nCode Link - https://github.com/KevinDanikowski/nlp-tensorflow-practice/blob/master/text_summarizer/create_pickle_file.py "},{"metadata":{"trusted":true},"cell_type":"code","source":"import _pickle as pickle\nimport jsonlines\n\nheads = []\ndescs = []\nkeywords = []\n\nwith jsonlines.open('../working/articles.jl','r') as reader:\n    i = 0\n    for obj in reader:\n        if i < 1000:\n            i += 1\n            head = obj[\"title\"]\n            desc = obj[\"text\"]\n            heads.append(head)\n            descs.append(desc)\n            keywords.append(None)\n        else:\n            break\n        \n# print(heads, descs, keywords)\nwith open('../working/tokens.pkl', 'wb') as f:\n    pickle.dump((heads,descs,keywords),f)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Embedding"},{"metadata":{},"cell_type":"markdown","source":"#Generate intial word embedding for headlines and description\n\n#The embedding is limited to a fixed vocabulary size (vocab_size) but a vocabulary of all the words that appeared in the data is built."},{"metadata":{"trusted":true},"cell_type":"code","source":"FN = 'vocabulary-embedding'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed=42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 40000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower = False # dont lower case the text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read tokenized headlines and descriptions"},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle as pickle\nFN0 = 'tokens' # this is the name of the data file which I assume you already have\nwith open('../working/%s.pkl'%FN0, 'rb') as fp:\n    heads, descs, keywords = pickle.load(fp) # keywords are not used in this project","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if lower:\n    heads = [h.lower() for h in heads]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if lower:\n    descs = [h.lower() for h in descs]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Headings tuple"},{"metadata":{},"cell_type":"markdown","source":"# Articles description"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nheads[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"descs[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(desc)\n#type(head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(keywords[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(heads),len(set(heads))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(desc),len(set(desc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Vocabulary :"},{"metadata":{},"cell_type":"markdown","source":"# Tokenize text, return vocab in order of usage\n\nLink - https://github.com/KevinDanikowski/nlp-tensorflow-practice/blob/master/text_summarizer/text_summarizer.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ndef get_vocab(combinedText):\n    #  to get vocab and count in another way, \n    words = combinedText.split()\n    vocab = [word for word, word_count in Counter(words).most_common()]\n    return vocab,words\n\nvocab,words = get_vocab(heads[i]+descs[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (vocab[:50])\nprint ('...', len(vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(words[w] for w in vocab);\nplt.gca().set_xscale(\"log\", nonposx='clip')\nplt.gca().set_yscale(\"log\", nonposy='clip')\nplt.title('word distribution in headlines and discription')\nplt.xlabel('rank')\nplt.ylabel('total appearances');\nwarnings.warn(self.msg_depr % (key, alt_key))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Index words"},{"metadata":{"trusted":true},"cell_type":"code","source":"empty = 0 # RNN mask of no data\neos = 1  # end of sentence\nstart_idx = eos+1 # first real word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\ndef get_idx(vocab):\n    word2idx = dict((word, idx+start_idx) for idx,word in enumerate(vocab))\n    word2idx['<empty>'] = empty\n    word2idx['<eos>'] = eos\n    idx2word = dict((idx,word) for word,idx in word2idx.items())\n\n    return vocab, word2idx, idx2word\nvocab,word2idx, idx2word = get_idx(vocab)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Word Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = get_idx(vocab)\nprint(len(vocab))\nprint(len(word2idx))\nprint(len(idx2word))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Read Glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  GLOVE code"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install -q tqdm flair\nimport numpy as np\nfrom tqdm import tqdm\nglove_name= ! wget \"http://nlp.stanford.edu/data/glove.6B.zip\" -O glove.6B.zip && unzip glove.6B.zip\n\nword2vec = {}\nwith open('glove.6B.300d.txt','r', encoding='utf-8',errors='ignore') as f:\n  for line in tqdm(f, total =400000):\n    #glove_n_symbols = line.decode().split(b\":\")[1]\n    glove_n_symbols = line.split()\n    word = glove_n_symbols[0]\n    #coefs = np.fromstring(glove_n_symbols, \"f\", sep=\" \")\n    #coefs = np.asarray(glove_n_symbols[1:], dtype='float32')\n    coefs = np.asarray([float(val) for val in glove_n_symbols[1:]])\n    word2vec[word] = coefs\n\nprint('Found %s word vectors.' % len(word2vec))\n#http://nlp.stanford.edu/data/glove.42B.300d.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(word)\nprint(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(coefs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(coefs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(glove_n_symbols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#glove_n_symbols = get_ipython().getoutput(u'wc -l {glove_name}')\n#glove_n_symbols = int(glove_n_symbols[0].split()[0])\n#%debug\nglove_n_symbols = int(float(glove_n_symbols[0].split()[0]))\nglove_n_symbols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%debug\nimport numpy as np\nglove_index_dict = {}\nglove_embedding_weights = np.empty((glove_n_symbols, embedding_dim),axis=0)\nglobale_scale=.1\nprint(glove_embedding_weights)\n\nwith open(glove_name, 'r',encoding='utf-8',errors='ignore') as fp:\n    i = 0\n    for l in fp:\n        l = l.strip().split()\n        w = l[0]\n        glove_index_dict[w] = i\n        glove_embedding_weights[i,:] = map(float,l[1:])\n        i += 1\nglove_embedding_weights *= globale_scale   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}