{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dimond Rate Prediction\n![](https://news.mit.edu/sites/default/files/styles/news_article__image_gallery/public/images/202010/MIT-Metallic-Diamond-01-Press_0.jpg?itok=386hZmMI)","metadata":{}},{"cell_type":"markdown","source":"# Read Dataset","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/diamonds/diamonds.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Details\n1. price price in US dollars (\\$326--\\$18,823)\n\n2. carat weight of the diamond (0.2--5.01)\n\n3. cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n\n4. color diamond colour, from J (worst) to D (best)\n\n5. clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n\n6. x length in mm (0--10.74)\n\n7. y width in mm (0--58.9)\n\n8. z depth in mm (0--31.8)\n\n9. depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n\n10. table width of top of diamond relative to widest point (43--95)","metadata":{}},{"cell_type":"markdown","source":"# Data Types","metadata":{}},{"cell_type":"code","source":"#column name Unnamed:0 is not a valid columns. so, we will drop the column name Unnamed:0\ndf.drop('Unnamed: 0', axis=1, inplace=True)\ndf=df[df['x']!=0]\ndf=df[df['y']!=0]\ndf=df[df['z']!=0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Target variable is \"price\"** so let us check the relationship with price with other variables.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolors=['#003f5c','#2f4b7c','#665191','#a05195','#d45087','#f95d6a','#ff7c43','#ffa600']\nsns.set(palette=colors, font='San', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})\nsns.despine(left=False, right=False)\nsns.palplot(colors)\nplt.title(\"Theme for EDA\", family='Sherif', size=15, weight=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_cols = df.select_dtypes(exclude='object').columns.to_list()\n#print(int_cols)\nint_cols.remove('price')\nj=0\nfig=plt.figure(figsize=(15,10), constrained_layout =True)\nplt.suptitle(\"Regression of the Numeric variables\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.regplot(data=df, x=i, y='price', color=colors[1], line_kws={'color':'#ffa600'})\n    ax.set_title(\"Price and {} comparision analysis\".format(i), family='Sherif')\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    \n    j=j+1\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Above chart shows the linear relationship with the Target variable, however, there are outliers**","metadata":{}},{"cell_type":"code","source":"# let us find the distribution of integer variables\nint_cols = df.select_dtypes(exclude='object').columns.to_list()\nj=0\nfig=plt.figure(figsize=(15,10), constrained_layout =True)\nplt.suptitle(\"Distribution of the Numeric variables\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.kdeplot(data=df, x=i, color=colors[0], fill=True, edgecolor=colors[-1], alpha=1)\n    ax.set_title(\"Distribution of Numeric variables - {}\".format(i), family='Sherif')\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    \n    j=j+1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j=0\nfig=plt.figure(figsize=(15,10))\nplt.suptitle(\"Box plot for Numeric variables\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.boxplot(data=df, x=i,color=colors[0])\n    ax.set_title(\"Box plot for {}\".format(i))\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    j=j+1\nax=plt.subplot(331+j)\nax.text(x=0,y=0.5, s='Obviously there are outliers in the data, we need to examine the outliers to verify if it is extreme value or data error')\nfor s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation with Price column\nfig=plt.figure(figsize=(15,8))\nsns.heatmap(df.corr(), linewidths=3, annot=True)\nplt.title(\"Correlation matrics\", family='Sherif', size=20, weight='bold')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obviously there are outliers in the data, we need to examine the outliers to verify if it is extreme value or data error","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=(15,10), constrained_layout=True)\n# let us find the target variable relationship with Categorical variables\nplt.suptitle(\"Categorical feature comparison with Price\", family='Sherif', size=20, weight='bold')\ncat_cols = df.select_dtypes(include='object').columns.to_list()\nax=fig.subplot_mosaic(\"\"\"\n                        AAB\n                        AAC\n                        AAD\n                        \"\"\")\nsns.kdeplot(df['price'], fill=True, edgecolor=colors[-1], linewidth=2, color=colors[0], ax=ax['A'], alpha=0.8)\nax['A'].text(x=2000,y=0.00025, s=\"Target Feature Price is not normally distributed\", family='San', fontweight='bold')\nax['A'].text(x=2000,y=0.00023, s=\"Comparing Price with Categorical feature we can see the Median is more or less same\",family='San', fontweight='bold')\nsns.boxplot(data=df, x=cat_cols[0],y='price', ax=ax['B'])\nsns.boxplot(data=df, x=cat_cols[1],y='price', ax=ax['C'])\nsns.boxplot(data=df, x=cat_cols[2],y='price', ax=ax['D'])\nfor i in 'ABCD':\n    for s in ['left','right','top','bottom']:\n        ax[i].spines[s].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Price feature is not normally distribured","metadata":{}},{"cell_type":"code","source":"cat_cols=df.select_dtypes(include='object').columns.to_list()\n\nfig=plt.figure(figsize=(15,5))\nplt.suptitle(\"Distribution of Categorical variable\",family='Sherif', size=20, weight='bold')\nax1=plt.subplot(131)\nsns.countplot(data=df, x=cat_cols[0], ax=ax1, linewidth=2, edgecolor=colors[-1])\nfor s in ['left','right','top','bottom']:\n        ax1.spines[s].set_visible(False)\nax2=plt.subplot(132, sharey=ax1)\nsns.countplot(data=df, x=cat_cols[1], ax=ax2,linewidth=2, edgecolor=colors[-1])\nfor s in ['left','right','top','bottom']:\n        ax2.spines[s].set_visible(False)\nax3=plt.subplot(133, sharey=ax1)\nsns.countplot(data=df, x=cat_cols[2], ax=ax3,linewidth=2, edgecolor=colors[-1])\nfor s in ['left','right','top','bottom']:\n        ax3.spines[s].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical Analysis","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as stats\nfrom statsmodels.stats.anova import anova_lm\nfrom   statsmodels.formula.api import ols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hypothesis Testing\n**comparing Price value with Categorical feature and check if the mean has significant difference**\n1. H0 = there is no significant difference \n2. H1 = there are significant difference","metadata":{}},{"cell_type":"code","source":"formula='price ~ C(clarity)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formula='price ~ C(color)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formula='price ~ C(cut)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formula='price ~ C(cut)+C(color)+C(clarity)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion on Hypothesis testing\nPrice value has significant ***(CI=95%)*** impact on the Cut, Clarity & Color of the Dimond","metadata":{}},{"cell_type":"markdown","source":"# Outlier handling","metadata":{}},{"cell_type":"code","source":"import scipy.stats as st\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR=Q3-Q1\ndf_clean=df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_cols=df_clean.select_dtypes(exclude='object').columns.to_list()\nj=0\nfig=plt.figure(figsize=(15,10))\nplt.suptitle(\"Box plot for Numeric variables after Outlier removal\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.boxplot(data=df_clean, x=i,color=colors[0])\n    ax.set_title(\"Box plot for {}\".format(i))\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    j=j+1\nax=plt.subplot(331+j)\nax.text(x=0,y=0.5, s='Outliers are handled with IQR method')\nfor s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One hot encoding for Categorical variables","metadata":{}},{"cell_type":"code","source":"df1=pd.get_dummies(df_clean, columns=cat_cols, drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train test split","metadata":{}},{"cell_type":"code","source":"X=df1.drop('price', axis=1)\ny=df1['price']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standardization","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n#creating Polynomial features as there is some degree of variation in the linear relationship\nscaler = PolynomialFeatures(degree=2, interaction_only=True)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npred=model.predict(X_test)\nprint()\nprint()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(15,8))\nresidual = y_test - pred\nplt.suptitle(\"Comparing y_test and Predicted value\", family='Sherif', size=20, weight='bold')\nax=fig.subplot_mosaic(\"\"\"AA\n                          BB\n                          CC\"\"\")\nsns.scatterplot(y_test, residual, ax=ax['A'])\nax['A'].axhline(y=0, ls='--', c=colors[-1], linewidth=3)\nsns.kdeplot(residual, ax=ax['B'], fill=True, color=colors[0], edgecolor=colors[-1], linewidth=2)\n\nfrom sklearn.metrics import mean_squared_error\nax['C'].text(x=0.2,y=0.2,s=\"Root squared mean error: {}\".format(np.round(mean_squared_error(y_test, pred, squared=False),2)), ha='left',family='cursive' ,weight='bold', size=15, style='italic')\nax['C'].text(x=0.2,y=0.4,s=\"Accuracy of model with Train data: {}\".format(np.round(model.score(X_train, y_train),2)), ha='left',family='cursive' ,weight='bold', size=15, style='italic')\nax['C'].text(x=0.2,y=0.6,s=\"Accuracy of model with Test data: {}\".format(np.round(model.score(X_test, y_test),2)), ha='left',family='cursive' ,weight='bold', size=15, style='italic')\nax['C'].text(x=0.2,y=0.8,s=\"Result:\", ha='left',family='cursive' ,weight='bold', size=15, style='italic')\n\nax['C'].axis('off')\n\nfor i in 'ABC':\n    for s in ['left','right','top','bottom']:\n        ax[i].spines[s].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nModel can predict the Price of the diamond with 98% accuracy. and with the Root Square mean error of 462.62\n\n**Please review and provide your inputs <br>\nBest wishes**","metadata":{}}]}