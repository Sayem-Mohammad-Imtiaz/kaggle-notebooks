{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport os\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, GRU, Bidirectional, Dense, MaxPooling1D, Convolution1D, Dropout, Activation, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom tensorflow.keras import regularizers\nfrom gensim.models import Word2Vec, Phrases\nimport nltk\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\n#nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.metrics import accuracy_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() #All non NULL values in both columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sentiment.value_counts() #Sentiments need to be converted to 1 for positive, 0 for -ve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[0][0] #Data needs to be cleaned as it contains some html tags(<br /><br />), uppercase letters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(text):\n  text = re.sub(r'<br />', ' ', text) #Removes Html tag\n  text = re.sub(r'[^\\ a-zA-Z0-9]+', '', text)  #Removes non alphanumeric\n  text = re.sub(r'^\\s*|\\s\\s*', ' ', text).strip() #Removes extra whitespace, tabs\n  stop_words = set(stopwords.words('english')) \n  lemmatizer = WordNetLemmatizer()\n  text = text.lower().split() #Converts text to lowercase\n  cleaned_text = list()\n  for word in text:        \n    if word in stop_words:    #Removes Stopwords, i.e words that don't convey any meaningful context/sentiments\n      continue    \n    word = lemmatizer.lemmatize(word, pos = 'v')    #Lemmatize words, pos = verbs, i.e playing, played becomes play\n    cleaned_text.append(word)\n  text = ' '.join(cleaned_text)\n  return text\n\ndf['cleaned_review'] = df['review'].apply(lambda x: clean_data(x))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_sentiment_to_int(text):  #Convert sentiment positive to 1, negative to 0\n  if(text.lower() == 'positive'):\n    text = 1\n  else:\n    text = 0\n  return text\n\ndf['sentiment'] = df['sentiment'].apply(lambda x: convert_sentiment_to_int(x))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = [len(x) for x in [df['cleaned_review'].iloc[i].split() for i in range(50000)]]\nnp.mean(result) #Mean no of words in each cleaned review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = [text for text in list(df['cleaned_review'].iloc[:25000])] #Preparation of X,Y\nX_test = [text for text in list(df['cleaned_review'].iloc[25000:])]\nY_train = [text for text in list(df['sentiment'].iloc[:25000])]\nY_test = [text for text in list(df['sentiment'].iloc[25000:])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(np.unique(np.hstack(X_train)))) #No of unique words in cleaned review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [text for text in list(df['cleaned_review'])] \nmax_vocab = 10000  #Max features\nmax_sent_length = 150  #Max word length of every review\ntokenizer = Tokenizer(num_words = max_vocab)\ntokenizer.fit_on_texts(X)\nX_train_tokenized = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen = max_sent_length) #Tokenization, i.e converting words to int\nX_test_tokenized = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen = max_sent_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_scheduler(epoch, lr):      #For tuning the learning rate\n    if epoch > 0:\n        lr = 0.0001\n        return lr\n    return lr\nmodel = Sequential()  #Sequential layers\nmodel.add(Embedding(max_vocab, 150, input_length = max_sent_length)) #Embedding layer\nmodel.add(Bidirectional(LSTM(60, return_sequences = True, dropout = 0.2))) #BiLSTM\nmodel.add(Convolution1D(32, 3, padding = 'valid', activation = 'relu'))  # 1D Conv\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dropout(0.6))     #High droput to reduce overfitting\nmodel.add(Dense(40, activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation =  'sigmoid'))\noptimizer = keras.optimizers.Adam(lr=0.01)\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])  #Adam gave better results than SGD\nprint(model.summary())\nbatch_size = 64\nepochs = 10\ncallbacks = [keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\nhist = model.fit(X_train_tokenized, np.array(Y_train), batch_size = batch_size, epochs = epochs, verbose = 1,  validation_data = (X_test_tokenized, np.array(Y_test)),  callbacks = callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test_tokenized, np.array(Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict_classes(X_train_tokenized)    #Predicted output\ny_test_pred = model.predict_classes(X_test_tokenized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(Y_train,y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\nepochs = range(1,11)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()      #The below graph shows that the model is a bit overfit :(","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = hist.history['accuracy']\nloss_val = hist.history['val_accuracy']\nepochs = range(1,11)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}