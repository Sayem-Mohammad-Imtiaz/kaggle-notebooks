{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End-to-end Vehicle Sales Price Recommendation Project\n\n[Front-end](https://recommend-vehicle-price.herokuapp.com/)\n\n[GitHub repo](https://github.com/MichaelBryantDS/vehicle-price-rec)","metadata":{}},{"cell_type":"markdown","source":"**Import libraries and data**","metadata":{}},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import data\ncar_data = pd.read_csv('../input/vehicle-dataset-from-cardekho/car data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"**Defining variables and cleaning data**","metadata":{}},{"cell_type":"code","source":"#look at formatting of entries\ncar_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at null count and dtype\ncar_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#numerical features\nnumerical = [\n    'Year',\n    'Present_Price',\n    'Kms_Driven',\n    'Selling_Price'\n]\n\n#categorical features\ncategorical = [\n    'Car_Name',\n    'Fuel_Type',\n    'Seller_Type',\n    'Transmission',\n    'Owner'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data distribution and outliers**\n","metadata":{}},{"cell_type":"code","source":"#look at distribution of data\ncar_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at outliers in selling price as a percentage\npercentage=(len(car_data.Selling_Price[np.abs(stats.zscore(car_data.Selling_Price)) >= 3])/len(car_data))*100\nprint('Percentage of Selling_Price outliers >= 3 std from the mean: {}%'.format(percentage))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at number of outliers greater than or equal to 3 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at number of outliers greater than or equal to 4 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at number of outliers greater than or equal to 5 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at number of outliers greater than or equal to 6 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#selling price outliers visualized\nsns.boxplot(x=car_data['Selling_Price'])\nplt.xlabel('Selling_Price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#present price outlier visualized\nsns.boxplot(x=car_data['Present_Price'])\nplt.xlabel('Present_Price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#kms driven outlier visualized\nsns.boxplot(x=car_data['Kms_Driven'])\nplt.xlabel('Kms_Driven')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data cleaning**","metadata":{}},{"cell_type":"code","source":"#capitalize all car names\nfor name in car_data['Car_Name']:\n    car_data = car_data.replace(name,name.title())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look for anything that needs to be fixed\nvalues,counts=np.unique(car_data['Car_Name'],return_counts=True)\nunique_cars_counts = pd.DataFrame({'car names':values, 'counts':counts})\nvalues","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reassign categorical names to numbers\ncar_data = car_data.replace('Petrol',0)\ncar_data = car_data.replace('Diesel',1)\ncar_data = car_data.replace('CNG',2)\n\ncar_data = car_data.replace('Dealer',0)\ncar_data = car_data.replace('Individual',1)\n\ncar_data = car_data.replace('Manual',0)\ncar_data = car_data.replace('Automatic',1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replace car names with numbers\nfor i in unique_cars_counts['car names']:\n    idx = pd.Index(unique_cars_counts['car names'])\n    car_data = car_data.replace(i,idx.get_loc(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assign categorical variables to int dtype\ncar_data[categorical].astype('int64')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data distributions**","metadata":{}},{"cell_type":"code","source":"#look at numerical data distribution\nfor i in car_data[numerical].columns:\n    plt.hist(car_data[numerical][i], edgecolor='black')\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of cars')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at categorical data distribution\nfor i in car_data[categorical].columns:\n    plt.hist(car_data[categorical][i], edgecolor='black')\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of cars')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finding correlations with a heat map and visualizations**","metadata":{}},{"cell_type":"code","source":"#heat map to find extreme positive and negative correlations in numerical data\nplt.figure(figsize=(16, 6))\nsns.heatmap(car_data[numerical].corr(), annot=True)\nplt.title('Correlation Heatmap for Numerical Variables', fontdict={'fontsize':12}, pad=12);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at how target is distributed among variables\nsns.pairplot(car_data)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lmplot comparing year and kms driven (-0.52 corr)\nsns.lmplot(x='Year', y='Kms_Driven',data=car_data)\n\n#settings to display all markers\nxticks, xticklabels = plt.xticks()\nxmin = 2002\nxmax = 2019\nplt.xlim(xmin, xmax)\nplt.xticks(xticks)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#violin plot comparing selling price and seller type\nsns.violinplot(y='Selling_Price',\n              x='Seller_Type', data = car_data)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stripplot comparing selling price and fuel type\nsns.stripplot(y=car_data['Selling_Price'],\n              x=car_data['Fuel_Type'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lmplot comparing selling price and present price (0.88 corr)\nsns.lmplot(x='Selling_Price', y='Present_Price',data=car_data)\n\n#settings to display all markers\nxmin = -2\nxmax = 37\nplt.xlim(xmin, xmax)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vionlinplot comparing present price and seller type\nsns.violinplot(y='Present_Price',\n              x='Seller_Type', data = car_data)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Applying linear model to better understand feature relationship with selling price**","metadata":{}},{"cell_type":"code","source":"#change dtype of categorical features to object\ncar_data[categorical]=car_data[categorical].astype('object')\n\n#copy of variables and target\nX = car_data.copy().drop('Selling_Price', axis=1)\ny = car_data.pop('Selling_Price')\n\n#remove Selling_Price from numerical variables\nnumerical.remove('Selling_Price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create dummy variables for categorical variables\ncar_data_dum = pd.get_dummies(X, drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generate OLS Regression Results\nimport statsmodels.api as sm\n\nX_sm = sm.add_constant(car_data_dum)\nmodel = sm.OLS(y,X_sm)\nmodel.fit().summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mutual information**","metadata":{}},{"cell_type":"code","source":"X_mi = X.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label encoding for categorical variables\nfor colname in X_mi.select_dtypes(\"object\"):\n    X_mi[colname], _ = X_mi[colname].factorize()\n\n#all discrete features have int dtypes\ndiscrete_features = X_mi.dtypes == int","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#some continuous variables also have int dtypes\ndiscrete_features[['Year','Kms_Driven']] = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use regression since the target variable is continuous\nfrom sklearn.feature_selection import mutual_info_regression\n\n#define a function to produce mutual information scores\ndef make_mi_scores(X_mi, y, discrete_features):\n    mi_scores = mutual_info_regression(X_mi, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_mi.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n#compute mutual information scores\nmi_scores = make_mi_scores(X_mi, y, discrete_features)\nmi_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define a function to plot mutual information scores\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n#plot the scores\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot selling_price against car_name\nfig, ax = plt.subplots(figsize=(12,4))\nsns.scatterplot(x=X_mi.Car_Name, y=y, ax=ax)\n\n# ax.text(15,33,\"{}\".format(values[15]))\n# ax.text(24,35,\"{}\".format(values[24]))\n\n#add names and arrows to highest values\nax.annotate(\"{}\".format(values[15]), xy=(15,33), xytext=(30,15), arrowprops=dict(facecolor='black',shrink=0.05))\nax.annotate(\"{}\".format(values[24]), xy=(24,35), xytext=(39,30), arrowprops=dict(facecolor='black',shrink=0.05))\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML Modeling","metadata":{}},{"cell_type":"markdown","source":"**Peparing data for ML**","metadata":{}},{"cell_type":"code","source":"#import ML preprocessing packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#one hot encoder for categorical variables\nencoder=OneHotEncoder(handle_unknown='error', drop='first')\nX = pd.concat([X[numerical],pd.get_dummies(X[categorical], drop_first=True)],axis=1)\nfeature_names = X.columns\n\n# train/test split with stratify making sure classes are evenlly represented across splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1)\n\n#numerical pipeline\nscaler=MinMaxScaler()\n\n#apply scaler to numerical data\nX_train[numerical] = scaler.fit_transform(X_train[numerical])\nX_test[numerical] = scaler.transform(X_test[numerical])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Untuned model prerformance**","metadata":{}},{"cell_type":"code","source":"#import ML packages\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LinearRegression mean cross-validation\nlm = LinearRegression()\nlm.fit(X_train, y_train)\ncv = cross_val_score(lm,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('LinearRegression')\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lasso mean cross-validation\nlm_l = Lasso(random_state = 1)\ncv = cross_val_score(lm_l,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('Lasso')\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ridge mean cross-validation\nrid = Ridge(random_state = 1)\ncv = cross_val_score(rid,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('Ridge')\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ElasticNet mean cross-validation\nenr = ElasticNet(random_state = 1)\ncv = cross_val_score(enr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('ElasticNet')\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RandomForestRegressor mean cross-validation\nrf = RandomForestRegressor(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('RandomForestRegressor')\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GradientBoostingRegressor mean cross-validation\ngbr = GradientBoostingRegressor(random_state = 1)\ncv = cross_val_score(gbr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('GradientBoostingRegressor')\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVR mean cross-validation\nsvr = SVR()\ncv = cross_val_score(svr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('SVR')\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tuning model performance**","metadata":{}},{"cell_type":"code","source":"#ml algorithm tuner\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \n\n#performance reporting function\ndef clf_performance(regressor, model_name):\n    print(model_name)\n    print('Best Score: {} +/- {}'.format(str(regressor.best_score_),str(regressor.cv_results_['std_test_score'][regressor.best_index_])))\n    print('Best Parameters: ' + str(regressor.best_params_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LinearRegression GridSearchCV\nlm = LinearRegression()\nparam_grid = {\n                'fit_intercept':[True,False],\n                'normalize':[True,False],\n                'copy_X':[True, False]\n}\nclf_lm = GridSearchCV(lm, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_lm = clf_lm.fit(X_train,y_train)\nclf_performance(best_clf_lm,'LinearRegressor')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#determine optimal lasso alpha value\nalpha = []\nerror = []\n\nfor i in range(1,100):\n    alpha.append(i/5000)\n    lm_l = Lasso(random_state = 1,alpha=(i/5000))\n    error.append(np.mean(cross_val_score(lm_l,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)))\n    \nplt.plot(alpha,error)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print optimal alpha value\nerr = tuple(zip(alpha,error))\ndf_err = pd.DataFrame(err, columns=['alpha','error'])\ndf_err[df_err.error == max(df_err.error)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lasso GridSearchCV\nlm_l = Lasso(random_state = 1)\nparam_grid = {\n                'alpha':[0.0038],\n                'fit_intercept':[True,False],\n                'normalize':[True, False],\n                'copy_X':[True, False]\n}\nclf_lm_l = GridSearchCV(lm_l, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_lm_l = clf_lm_l.fit(X_train,y_train)\nclf_performance(best_clf_lm_l,'Lasso')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ridge GridSearchCV\nrid = Ridge(random_state = 1)\nparam_grid = {\n                'fit_intercept':[True,False],\n                'normalize':[True, False],\n                'copy_X':[True, False],\n                'solver': ['auto','svd','cholesky','lsqr','sparse_cg','sag','saga']\n}\nclf_rid = GridSearchCV(rid, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_rid = clf_rid.fit(X_train,y_train)\nclf_performance(best_clf_rid,'Ridge')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#determine optimal elasticnet alpha value\nalpha = []\nerror = []\n\nfor i in range(1,100):\n    alpha.append(i/10000)\n    enr = ElasticNet(random_state = 1,alpha=(i/10000))\n    error.append(np.mean(cross_val_score(enr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)))\n    \nplt.plot(alpha,error)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print optimal alpha value\nerr = tuple(zip(alpha,error))\ndf_err = pd.DataFrame(err, columns=['alpha','error'])\ndf_err[df_err.error == max(df_err.error)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ElasticNet GridSearchCV\nenr = ElasticNet(random_state = 1)\nparam_grid = {\n                'alpha':[0.0018],\n                'fit_intercept':[True,False],\n                'normalize':[True, False],\n                'copy_X':[True, False],\n}\nclf_enr = GridSearchCV(enr, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_enr = clf_enr.fit(X_train,y_train)\nclf_performance(best_clf_enr,'ElasticNet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RanddomForestRegressor GridSearchCV\nrf = RandomForestRegressor(random_state = 1)\nparam_grid = {\n                'n_estimators': [385] , \n                'bootstrap': [True],\n                'max_depth': [9],\n                'max_features': ['auto'],\n                'min_samples_leaf': [1,],\n                'min_samples_split': [2]\n              }\nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train,y_train)\nclf_performance(best_clf_rf,'RandomForestRegressor')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#determine optimal gbr alpha value\nalpha = []\nerror = []\n\nfor i in range(1,10):\n    alpha.append(i/10000)\n    gbr = GradientBoostingRegressor(random_state = 1,alpha=(i/10000))\n    error.append(np.mean(cross_val_score(gbr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)))\n    \nplt.plot(alpha,error)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print optimal alpha value\nerr = tuple(zip(alpha,error))\ndf_err = pd.DataFrame(err, columns=['alpha','error'])\ndf_err[df_err.error == max(df_err.error)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GradientBoostingRegressor GridSearchCV\ngbr = GradientBoostingRegressor(random_state = 1)\nparam_grid = {\n                'n_estimators': [20], \n                'max_depth': [7],\n                'max_features': ['auto'],\n                'learning_rate': [0.2],\n#                 'alpha': [0.0001],\n                'min_samples_leaf': [3],\n                'min_samples_split': [2]\n              }\nclf_gbr = GridSearchCV(gbr, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_gbr = clf_gbr.fit(X_train,y_train)\nclf_performance(best_clf_gbr,'GradientBoostingRegressor')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVR GridSearchCV\nsvr = SVR()\nparam_grid = {\n                'kernel' : ['poly'],\n                'C' : [24],\n                'coef0' : [0.9],\n                'gamma' : ['scale','auto']\n}\nclf_svr = GridSearchCV(svr, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_svr = clf_svr.fit(X_train,y_train)\nclf_performance(best_clf_svr,'SVR')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**StackingRegressor**","metadata":{}},{"cell_type":"code","source":"#import ensemble packages and numpy functions\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor, BaggingRegressor, AdaBoostRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Baseline","metadata":{}},{"cell_type":"code","source":"#StackingRegressor mean cross-validation\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('lm', LinearRegression()))\n    level0.append(('lm_l', Lasso(random_state = 1)))\n    level0.append(('rid', Ridge(random_state = 1)))\n    level0.append(('enr', ElasticNet(random_state = 1)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1)))\n    level0.append(('svr', SVR()))\n    # define meta learner model\n    level1 = LinearRegression()\n    # define the stacking ensemble\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return model\n\ndef get_models():\n    models = dict()\n    models['lm'] = LinearRegression()\n    models['lm_l'] = Lasso(random_state = 1)\n    models['rid'] = Ridge(random_state = 1)\n    models['enr'] = ElasticNet(random_state = 1)\n    models['rf'] = RandomForestRegressor(random_state = 1)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1)\n    models['svr'] = SVR()\n    models['stacking'] = get_stacking()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +/- %.3f' % (name, mean(scores), std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"#StackingRegressor mean cross-validation\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    #level0.append(('lm', LinearRegression(copy_X= True, fit_intercept= True, normalize= True)))\n    level0.append(('lm_l', Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)))\n    level0.append(('rid', Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')))\n    #level0.append(('enr', ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)))\n    level0.append(('svr', SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')))\n    # define meta learner model\n    level1 = LinearRegression()\n    # define the stacking ensemble\n    stacking_model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return stacking_model\n\ndef get_models():\n    models = dict()\n    #models['lm'] = LinearRegression(copy_X= True, fit_intercept= True, normalize= True)\n    models['lm_l'] = Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)\n    models['rid'] = Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')\n    #models['enr'] = ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)\n    models['rf'] = RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)\n    models['svr'] = SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')\n    models['stacking'] = get_stacking()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +/- %.3f' % (name, mean(scores), std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VotingRegressor**","metadata":{}},{"cell_type":"markdown","source":"Baseline","metadata":{}},{"cell_type":"code","source":"#VotingRegressor mean cross-validation\ndef get_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('lm', LinearRegression()))\n    level0.append(('lm_l', Lasso(random_state = 1)))\n    level0.append(('rid', Ridge(random_state = 1)))\n    level0.append(('enr', ElasticNet(random_state = 1)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1)))\n    level0.append(('svr', SVR()))\n    # define the stacking ensemble\n    voting_model = VotingRegressor(estimators=level0)\n    return voting_model\n\ndef get_models():\n    models = dict()\n    models['lm'] = LinearRegression()\n    models['lm_l'] = Lasso(random_state = 1)\n    models['rid'] = Ridge(random_state = 1)\n    models['enr'] = ElasticNet(random_state = 1)\n    models['rf'] = RandomForestRegressor(random_state = 1)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1)\n    models['svr'] = SVR()\n    models['voting'] = get_voting()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +/- %.3f' % (name, mean(scores), std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"#VotingRegressor mean cross-validation\ndef get_voting():\n    # define the base models\n    level0 = list()\n    #level0.append(('lm', LinearRegression(copy_X= True, fit_intercept= True, normalize= True)))\n    #level0.append(('lm_l', Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)))\n    #level0.append(('rid', Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')))\n    #level0.append(('enr', ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)))\n    level0.append(('svr', SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')))\n    # define the stacking ensemble\n    voting_model = VotingRegressor(estimators=level0)\n    return voting_model\n\ndef get_models():\n    models = dict()\n    #models['lm'] = LinearRegression(copy_X= True, fit_intercept= True, normalize= True)\n    #models['lm_l'] = Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)\n    #models['rid'] = Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')\n    #models['enr'] = ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)\n    models['rf'] = RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)\n    models['svr'] = SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')\n    models['voting'] = get_voting()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +/- %.3f' % (name, mean(scores), std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BaggingRegressor: Bagging and Pasting**","metadata":{}},{"cell_type":"markdown","source":"Bagging baseline","metadata":{}},{"cell_type":"code","source":"#BaggingRegressor mean cross-validation\nbagging_model = BaggingRegressor(\n                                     bootstrap=True,\n                                     random_state=1,\n                                     n_jobs=-1\n                                     )\n\nbagging_model.fit(X_train, y_train)\n\ncv = cross_val_score(bagging_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter tuning bagging","metadata":{}},{"cell_type":"code","source":"#BaggingRegressor mean cross-validation\nbagging_model = BaggingRegressor(\n#                                     base_estimator=RandomForestRegressor(),\n                                     bootstrap=True,\n                                     random_state=1,\n                                     n_estimators=20,\n                                     n_jobs=-1\n                                     )\n\nbagging_model.fit(X_train, y_train)\n\ncv = cross_val_score(bagging_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pasting baseline","metadata":{}},{"cell_type":"code","source":"#BaggingRegressor (pasting) mean cross-validation\npasting_model = BaggingRegressor(\n                                     bootstrap=False,\n                                     random_state=1,\n                                     n_jobs=-1\n                                     )\n\npasting_model.fit(X_train, y_train)\n\ncv = cross_val_score(pasting_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter tuning pasting","metadata":{}},{"cell_type":"code","source":"#BaggingRegressor (pasting) mean cross-validation\npasting_model = BaggingRegressor(\n                                     base_estimator=RandomForestRegressor(),\n                                     bootstrap=False,\n                                     random_state=1,\n                                     n_estimators=40,\n                                     n_jobs=-1\n                                     )\n\npasting_model.fit(X_train, y_train)\n\ncv = cross_val_score(pasting_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**AdaBoostRegressor**","metadata":{}},{"cell_type":"markdown","source":"Baseline","metadata":{}},{"cell_type":"code","source":"#AdaBoostRegressor mean cross-validation\nadaboost_model = AdaBoostRegressor(\n                                       random_state=1)\n\nadaboost_model.fit(X_train , y_train)\n\ncv = cross_val_score(adaboost_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"#AdaBoostRegressor mean cross-validation\nadaboost_model = AdaBoostRegressor(\n                                       base_estimator=RandomForestRegressor(),\n                                       learning_rate=0.01,\n                                       random_state=1)\n\nadaboost_model.fit(X_train , y_train)\n\ncv = cross_val_score(adaboost_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+/-', std(cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MSE, RMSE, MAE, and R-squared values for best models using test set**","metadata":{}},{"cell_type":"code","source":"#import metrics packages\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#VotingRegressor metrics\nvoting_model = get_voting()\nvoting_model.fit(X_train,y_train)\ntpred_voting=voting_model.predict(X_test)\nprint('VotingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_voting)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_voting))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_voting)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_voting)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#StackingRegressor metrics\nstacking_model = get_stacking()\nstacking_model.fit(X_train,y_train)\ntpred_stack=stacking_model.predict(X_test)\nprint('StackingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_stack)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_stack))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_stack)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_stack)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BaggingRegressor (pasting) metrics\npasting_model.fit(X_train,y_train)\ntpred_pasting=pasting_model.predict(X_test)\nprint('BaggingRegressor (Pasting)')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_pasting)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_pasting))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_pasting)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_pasting)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RandomForestRegressor metrics\nrf = RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)\nrf.fit(X_train,y_train)\ntpred_rf=rf.predict(X_test)\nprint('RandomForestRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_rf)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_rf))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_rf)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_rf)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BaggingRegressor metrics\nbagging_model.fit(X_train,y_train)\ntpred_bagging=bagging_model.predict(X_test)\nprint('BaggingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_bagging)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_bagging))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_bagging)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_bagging)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AdaBoostRegressor metrics\nadaboost_model.fit(X_train,y_train)\ntpred_adaboost=adaboost_model.predict(X_test)\nprint('AdaBoostRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_adaboost)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_adaboost))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_adaboost)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_adaboost)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GradientBoostingRegressor metrics\ngbr = GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)\ngbr.fit(X_train,y_train)\ntpred_gbr=gbr.predict(X_test)\nprint('GradientBoostingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_gbr)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_gbr))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_gbr)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_gbr)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVR metrics\nsvr = SVR(C=24, coef0= 0.9, gamma='scale', kernel='poly')\nsvr.fit(X_train,y_train)\ntpred_svr=svr.predict(X_test)\nprint('SVR')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_svr)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_svr))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_svr)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_svr)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{}},{"cell_type":"code","source":"#import packages for explaining feature importance\nfrom pdpbox import pdp, get_dataset, info_plots\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preparing data for shap\nX_shap = pd.DataFrame(X_train)\nX_shap.columns = feature_names\n\npred_data = pd.DataFrame(X_test)\npred_data.columns = feature_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create object that can calculate shap values\nexplainer = shap.Explainer(svr.predict, X_shap)\nshap_values = explainer(pred_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#summary_plot using SVR\nshap.initjs()\nshap.summary_plot(shap_values, pred_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#car that has the most impact on SVR model: car_name_37\nvalues[37]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#permutation importance from Voting Regressor\nperm = PermutationImportance(voting_model).fit(pred_data, y_test)\neli5.show_weights(perm, feature_names = list(feature_names), top=len(feature_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cars that has the most impact on Voting Regressor model: car_name_27\nprint(values[27])\nprint(values[28])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions","metadata":{}},{"cell_type":"markdown","source":"**Best model**\n- SVR\n- MSE: 0.61430\n- RMSE: 783.77 USD\n- MAE: 502.69 USD\n- R-squared: 0.97032\n\n**Most important features**\n- Present_Price\n- Year\n- Seller_Type\n","metadata":{}},{"cell_type":"markdown","source":"# Productionization\n\nI created a [front-end](https://recommend-vehicle-price.herokuapp.com/) using this model using Flask and Heroku to recommend vehicle sales prices.\n\nSee the [GitHub repo](https://github.com/MichaelBryantDS/vehicle-price-rec) for more information.","metadata":{}}]}