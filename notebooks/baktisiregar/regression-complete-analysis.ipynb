{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Beginners Guide to Regression Analysis and Visualize Interpretations**\n![](https://lh3.googleusercontent.com/proxy/G1A8T4niNrin0vIFFJUTpq3e0ZjfHnhnwSgdYwNJGTDax7PgALUObiFeV8tXBI95_v_pHdNup7BiRdxERfMnolvGLuOHlED2g3O1M8jZoMyhgxgYqeOg0ib6wtQVJy86GMI-RFYL2eLdYDtPIkFxe35x5feREsFXX9zHw_OzRem5lm-573yrKK1oOuPd76vY1JA)"},{"metadata":{},"cell_type":"markdown","source":"## Table Of Contents:\n* **Understanding Regression**\n* **How Does Regression Work?**\n* **Types of Algorithms**\n* **Testing of Algorithms**"},{"metadata":{},"cell_type":"markdown","source":"## Algorithms that we will consider:-\n1. Simple Linear Regression\n2. Multiple Linear Regression\n3. Polynomial Regression\n4. Support Vestor Regression\n5. Decision Tree Regression\n6. Random Forest Regression"},{"metadata":{},"cell_type":"markdown","source":"## What is Regression?\n\nRegression is a technique used to predict value of one variable(Dependent Variable) on the basis of other variables(Independent Variables). It is parametric in nature because it makes certain assumptions based on the data set. If the data set follows those assumptions, regression gives incredible results. Otherwise, it struggles to provide convincing accuracy."},{"metadata":{},"cell_type":"markdown","source":"## How Does Regression Work?\n\nRegression is a part of supervised learning which basically means that we train our models on the basis of given training data and our model tries to relate between the dependent and the independent variable. It does this using various functions that maps the independent variables to the dependent variables. When the model is completely trained and the error is minumised then we are able to make predictions on testing data as well."},{"metadata":{},"cell_type":"markdown","source":"## The 7 Steps of Machine Learning\n\n### 1 Data Collection\n   * The quantity & quality of your data dictate how accurate our model is\n   * The outcome of this step is generally a representation of data which we will use for training\n   * Using pre-collected data, by way of datasets from Kaggle, UCI, etc., still fits into this step\n \n### 2 Data Preparation\n   * Wrangle data and prepare it for training\n   * Clean that which may require it (remove duplicates, correct errors, deal with missing values, normalization, data type conversions, etc.)\n   * Randomize data, which erases the effects of the particular order in which we collected and/or otherwise prepared our data\n   * Visualize data to help detect relevant relationships between variables or class imbalances (bias alert!), or perform other exploratory analysis\n   * Split into training and evaluation sets (Good train/eval split? 80/20, 70/30, or similar, depending on domain, data availability, dataset particulars, etc.)\n \n### 3 Choose a Model\n   * Different algorithms are for different tasks; choose the right one\n \n### 4 Train the Model\n   * The goal of training is to answer a question or make a prediction correctly as often as possible\n   * Linear regression example: algorithm would need to learn values for m (or W) and b (x is input, y is output)\n   * Each iteration of process is a training step\n \n### 5 Evaluate the Model\n   * Uses some metric or combination of metrics to \"measure\" objective performance of model\n   * Test the model against previously unseen data\n   * This unseen data is meant to be somewhat representative of model performance in the real world, but still helps tune the model (as opposed to test data, which does not)\n \n### 6 Parameter Tuning\n   * This step refers to hyperparameter tuning, which is an \"artform\" as opposed to a science\n   * Tune model parameters for improved performance\n   * Simple model hyperparameters may include: number of training steps, learning rate, initialization values and distribution, etc.\n \n### 7 Make Predictions\n   * Using further (test set) data which have, until this point, been withheld from the model (and for which class labels are known), are used to test the model; a better approximation of how the model will perform in the real world"},{"metadata":{},"cell_type":"markdown","source":"## Simple Linear Regression\n\nIt is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \n\n$$y = \\beta + \\alpha*X $$ \n\nwhere:\n\n* $y$ – Dependent Variable\n* $X$ – Independent variable\n* $\\beta$ – intercept\n* $\\alpha$ – Slope"},{"metadata":{},"cell_type":"markdown","source":"## Data Collection & Data Preparation:\n\nIn this section, you may have to collect data sets from different sources, from the internet or your personal repository. You also need to do data cleaning if necessary, then separate training data and testing data. Here, I already have a separate data set, so no separation of data is required."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport numpy as np                  # working with array \nimport pandas as pd                 # import data set\nimport matplotlib.pyplot as plt     # for visualization\n          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Training Data Set\nTraining_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values)   # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Testing Data Set\nTesting_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values)   # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values)     # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Model\n\nIn this section, the selection of the best regression model is carried out using the `LinearRegression` library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.scatter(X_test, y_test, color = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multiple Linear Regression\n\nIt is also a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \n\n$$Y = \\beta + \\alpha_1*X_1 + \\alpha_2*X_2 + \\cdots + \\alpha_n*X_n$$ \n\nwhere:\n* $Y:$ Dependent Variable\n* $X_1, X_2, X_3, X_4:$ Independent variable\n* $\\beta:$ Intercept\n* $\\alpha_1, \\alpha_2, \\cdots, \\alpha_n:$ Slopes "},{"metadata":{},"cell_type":"markdown","source":"## Preparing Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/insurance/insurance.csv')\nprint(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:, :-1] # Independent Variable\ny = dataset.iloc[:, -1]  # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have to apply encoding in the dataset as there are words present.\n# for 'sex' and 'smoker' column we will apply Label Encoding as there are only 2 catagories\n# for 'region' we will apply OneHot Encoding as there are more than 2 catagories\n\n# Label Encoding:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX.iloc[:, 1] = le.fit_transform(X.iloc[:, 1])\nX.iloc[:, 4] = le.fit_transform(X.iloc[:, 4])\n\n# OneHot Encoding:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [5])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Model\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Polynomial Regression\n\nIt is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \n\n$$y = \\beta + \\alpha_1*X + \\alpha_2*X^2 + \\cdots+ \\alpha_n*X^n$$\n\nwhere:\n* $y:$ Dependent Variable\n* $X, X^2, \\cdots, X^n:$ Independent variable\n* $\\beta:$ Intercept\n* $\\alpha_1, \\alpha_2, \\alpha_n:$ Coefficients of independent variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/polynomialregressioncsv/polynomial-regression.csv')\nX = dataset.iloc[:, :-1] # Independent Variable\ny = dataset.iloc[:, -1] # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trianing the Model\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 5)\nX_poly = poly_reg.fit_transform(X)\nlin_reg_2 = LinearRegression()\nlin_reg_2.fit(X_poly, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.scatter(X, y, color = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Regression\n\nSVR gives us the flexibility to define how much error is acceptable in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data. It uses following constraints:\n\n$$|y - aX| <= e$$\n\nwhere:\n* e - maximum error\n![](https://miro.medium.com/max/1212/1*bSZn9bK43MaA5vVDamRQ2A.png)\nThe points outside the margin are the Support Vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values) # Dependent Variable\ny_train = y_train.reshape(len(y_train),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Testing_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values) # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values) # Dependent Variable\ny_test = y_test.reshape(len(y_test),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scalling X and y\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\ny_train = sc_y.fit_transform(y_train)\ny_test = sc_y.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Model\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(sc_X.inverse_transform(X_test), sc_y.inverse_transform(y_test), color = 'red', \n           marker = 'o', s = 35, alpha = 0.5, label = 'Test data')\nplt.plot(sc_X.inverse_transform(X_test), sc_y.inverse_transform(regressor.predict(X_test)), \n           color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Regression\n\nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Testing_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values) # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values)   # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_grid = np.arange(min(X_test), max(X_test), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X_test, y_test, color = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Inputs')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regression\n\nRandom Forest combines multiple trees to predict the class of the dataset, it is possible that some decision trees may predict the correct output, while others may not. But together, all the trees predict the correct output."},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_Dataset = pd.read_csv(\"../input/random-linear-regression/train.csv\")\nTraining_Dataset = Training_Dataset.dropna()\nX_train = np.array(Training_Dataset.iloc[:, :-1].values) # Independent Variable\ny_train = np.array(Training_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Testing_Dataset = pd.read_csv(\"../input/random-linear-regression/test.csv\")\nTesting_Dataset = Testing_Dataset.dropna()\nX_test = np.array(Testing_Dataset.iloc[:, :-1].values) # Independent Variable\ny_test = np.array(Testing_Dataset.iloc[:, 1].values) # Dependent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = regressor.score(X_test, y_test)\nprint('Accuracy = '+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_grid = np.arange(min(X_test), max(X_test), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X_test, y_test, c = 'red', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Test data')\nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue', label='Model Plot')\nplt.title('Predicted Values vs Inputs')\nplt.xlabel('Position level')\nplt.ylabel('Predicted Values')\nplt.legend(loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you very much for your attention. I wish you will find good datasets for your mini-research!"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.pinimg.com/originals/40/12/1a/40121a3616ecf2439a5b04d733b6f437.gif)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}