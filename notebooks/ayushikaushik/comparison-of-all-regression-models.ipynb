{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting the prices of Avacados\n\n### About the data-\n> The dataset represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n\nSome relevant columns in the dataset:\n\n* Date - The date of the observation\n* AveragePrice - the average price of a single avocado\n* type - conventional or organic\n* year - the year\n* Region - the city or region of the observation\n* Total Volume - Total number of avocados sold\n* 4046 - Total number of avocados with PLU 4046 sold\n* 4225 - Total number of avocados with PLU 4225 sold\n* 4770 - Total number of avocados with PLU 4770 sold"},{"metadata":{"trusted":true},"cell_type":"code","source":"#display image using python\nfrom IPython.display import Image\nurl = 'https://img.etimg.com/thumb/msid-71806721,width-650,imgsize-807917,,resizemode-4,quality-100/avocados.jpg'\nImage(url,height=300,width=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n#importing the dataset\ndata = pd.read_csv('../input/avocado-prices/avocado.csv',index_col=0)\n# Check the data\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 categorical features and luckily no missing value. Let's explore the data further."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['AveragePrice']);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.countplot(x='year',data=data,hue='type');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are almost equal numbers of conventional and organic avacados. Though, there is very less observations in the year 2018."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.year.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=\"type\", x=\"AveragePrice\", data=data);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Organic avocados are more expensive. This is obvious, because their cultivation is more expensive and we all love natural products and are willing to pay a higher price for them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.year=data.year.apply(str)\nsns.boxenplot(x=\"year\", y=\"AveragePrice\", data=data);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avacados were slightly more expensive in the year 2017.(as there was shortage due to some reasons)"},{"metadata":{},"cell_type":"markdown","source":"### Dealing with categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['type']= data['type'].map({'conventional':0,'organic':1})\n\n# Extracting month from date column.\ndata.Date = data.Date.apply(pd.to_datetime)\ndata['Month']=data['Date'].apply(lambda x:x.month)\ndata.drop('Date',axis=1,inplace=True)\ndata.Month = data.Month.map({1:'JAN',2:'FEB',3:'MARCH',4:'APRIL',5:'MAY',6:'JUNE',7:'JULY',8:'AUG',9:'SEPT',10:'OCT',11:'NOV',12:'DEC'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,5))\nsns.countplot(data['Month'])\nplt.title('Monthwise Distribution of Sales',fontdict={'fontsize':25});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It implies that sales of avacado see a rise in January, Febuary and March.\n\n## Preparing data for ML models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummy variables\ndummies = pd.get_dummies(data[['year','region','Month']],drop_first=True)\ndf_dummies = pd.concat([data[['Total Volume', '4046', '4225', '4770', 'Total Bags',\n       'Small Bags', 'Large Bags', 'XLarge Bags', 'type']],dummies],axis=1)\ntarget = data['AveragePrice']\n\n# Splitting data into training and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_dummies,target,test_size=0.30)\n\n# Standardizing the data\ncols_to_std = ['Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags','Large Bags', 'XLarge Bags']\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X_train[cols_to_std])\nX_train[cols_to_std] = scaler.transform(X_train[cols_to_std])\nX_test[cols_to_std] = scaler.transform(X_test[cols_to_std])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing ML models from scikit-learn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to save time all models can be applied once using for loop\nregressors = {\n    'Linear Regression' : LinearRegression(),\n    'Decision Tree' : DecisionTreeRegressor(),\n    'Random Forest' : RandomForestRegressor(),\n    'Support Vector Machines' : SVR(gamma=1),\n    'K-nearest Neighbors' : KNeighborsRegressor(n_neighbors=1),\n    'XGBoost' : XGBRegressor()\n}\nresults=pd.DataFrame(columns=['MAE','MSE','R2-score'])\nfor method,func in regressors.items():\n    model = func.fit(X_train,y_train)\n    pred = model.predict(X_test)\n    results.loc[method]= [np.round(mean_absolute_error(y_test,pred),3),\n                          np.round(mean_squared_error(y_test,pred),3),\n                          np.round(r2_score(y_test,pred),3)\n                         ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting train set into training and validation sets.\nX_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.20)\n\n#importing tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n#creating model\nmodel = Sequential()\nmodel.add(Dense(76,activation='relu',kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n    bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)))\nmodel.add(Dense(200,activation='relu',kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n    bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(200,activation='relu',kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n    bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(200,activation='relu',kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n    bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='Adam', loss='mean_squared_error')\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model.fit(x=X_train.values,y=y_train.values,\n          validation_data=(X_val.values,y_val.values),\n          batch_size=100,epochs=150,callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dnn_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results table"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results.loc['Deep Neural Network']=[mean_absolute_error(y_test,dnn_pred).round(3),mean_squared_error(y_test,dnn_pred).round(3),\n                                    r2_score(y_test,dnn_pred).round(3)]\nresults","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f\"10% of mean of target variable is {np.round(0.1 * data.AveragePrice.mean(),3)}\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at methods performing best as they have R2-score close to 1."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"results.sort_values('R2-score',ascending=False).style.background_gradient(cmap='Greens',subset=['R2-score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion:\n\n* Except linear regression model, all other models have mean absolute error less than 10% of mean of target variable.\n* For this dataset, XGBoost and Random Forest algorithms have shown best results."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}