{"cells":[{"metadata":{},"cell_type":"markdown","source":"# K-Means Clustering and K Nearest Neighbors Project\n\nJump to:\n* [K Means clustering model](#kmeans)\n* [K Nearest Neighbors model](#kneighbor)"},{"metadata":{},"cell_type":"markdown","source":"## About the dataset\n\n<img src=\"https://previews.123rf.com/images/aomeditor/aomeditor1903/aomeditor190300021/122254680-illustrator-of-body-parts-of-penguin.jpg\" height='400px' width='400px'>"},{"metadata":{},"cell_type":"markdown","source":"### <b>Columns in the dataset</b>\n<ul>\n    <li><b>Species: </b>penguin species (Chinstrap, Ad√©lie, or Gentoo)</li>\n    <li><b>Island: </b>island name (Dream, Torgersen, or Biscoe) in the Palmer Archipelago (Antarctica)</li>\n    <li><b>culmen_length_mm: </b>culmen length (mm)</li>\n    <li><b>culmen_depth_mm: </b>culmen depth (mm)</li>\n    <li><b>flipper_length_mm: </b>flipper length (mm)</li>\n    <li><b>body_mass_g: </b>body mass (g)</li>\n    <li><b>Sex: </b>penguin sex</li>\n</ul>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# import basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = pd.read_csv('../input/palmer-archipelago-antarctica-penguin-data/penguins_size.csv')\n# Check the data\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['species'],palette='spring');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is not balanced. As data is not big enough so I will not balance it.\n\nIn case of unbalanced data we can either up-sample minority class or down-sample majority class. To see an example see my following notebooks:\n* [up-sampling-minority-class](https://www.kaggle.com/ayushikaushik/up-sampling-to-tackle-unbalanced-dataset)\n* [down-sampling-majority-class](https://www.kaggle.com/ayushikaushik/down-sampling-majority-class-6-classification-algo)\n\nTo know cons of imbalanced data and more ways to handle it, read [this](https://elitedatascience.com/imbalanced-classes) article."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data,hue='species');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see clusters are easily separable in the cases:\n1. culmen_length_mm  vs  culmen_depth_mm ;\n2. culmen_length_mm  vs  flipper_length_mm ;\n3. culmen_length_mm  vs  body_mass_g."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Let's explore distribution of our data.\")\nfig,axes=plt.subplots(4,1,figsize=(5,20))\nsns.boxplot(x=data.species,y=data.flipper_length_mm,ax=axes[0],palette='summer')\naxes[0].set_title(\"Flipper length distribution\",fontsize=20,color='Red')\nsns.boxplot(x=data.species,y=data.culmen_length_mm,ax=axes[1],palette='rocket')\naxes[1].set_title(\"Culmen length distribution\",fontsize=20,color='Red')\nsns.boxplot(x=data.species,y=data.culmen_depth_mm,ax=axes[2],palette='twilight')\naxes[2].set_title(\"Culmen depth distribution\",fontsize=20,color='Red')\nsns.boxplot(x=data.species,y=data.body_mass_g,ax=axes[3],palette='Set2')\naxes[3].set_title(\"Body mass distribution\",fontsize=20,color='Red')\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Mean body mass index distribution\")\ndata.groupby(['species','sex']).mean()['body_mass_g'].round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with missing values\n\nLet's check out the percentage of missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"100*data.isnull().sum()/len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percentage of missing data is very less. Let's impute it with median in numerical features and mode in categorical feature.\nHere, I have used .fillna method from pandas library.\n\nMissing values can also be filled using pre-defined functions like SimpleImputer from sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sex'].fillna(data['sex'].mode()[0],inplace=True)\ncol_to_be_imputed = ['culmen_length_mm', 'culmen_depth_mm','flipper_length_mm', 'body_mass_g']\nfor item in col_to_be_imputed:\n    data[item].fillna(data[item].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.species.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.island.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oops! Where did this '.' entry came from?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['sex']=='.']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[336,'sex'] = 'FEMALE'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variable can also be encoded using sklearn.preprocessing.LabelEncoder\ndata['species']=data['species'].map({'Adelie':0,'Gentoo':1,'Chinstrap':2})\n\n# creating dummy variables for categorical features\ndummies = pd.get_dummies(data[['island','sex']],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardizing feature variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we do not standardize dummy variables \ndf_to_be_scaled = data.drop(['island','sex'],axis=1)\ntarget = df_to_be_scaled.species\ndf_feat= df_to_be_scaled.drop('species',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_feat)\ndf_scaled = scaler.transform(df_feat)\ndf_scaled = pd.DataFrame(df_scaled,columns=df_feat.columns[:4])\ndf_preprocessed = pd.concat([df_scaled,dummies,target],axis=1)\ndf_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='kmeans'>K-Means Clustering</a>\n\nIt is very important to note, we actually have the labels for this data set, but we will NOT use them for the K-Means clustering algorithm, since that is an **unsupervised learning algorithm**. K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid. The main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.\n\nK Means Clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Unsupervised learning means that there is no outcome to be predicted, and the algorithm just tries to find patterns in the data. In k means clustering, we have to specify the ***number of clusters k*** we want the data to be grouped into."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\nkmeans = KMeans(3,init='k-means++')\nkmeans.fit(df_preprocessed.drop('species',axis=1))\nprint(confusion_matrix(df_preprocessed.species,kmeans.labels_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Running above cell again and again gives different results each time. While making this notebook, best accuracy wass 95%. After commiting, it can show anything. I don't know how to handle this."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(df_preprocessed.species,kmeans.labels_))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"f\"Accuracy is {np.round(100*accuracy_score(df_preprocessed.species,kmeans.labels_),2)}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss=[]\nfor i in range(1,10):\n    kmeans = KMeans(i)\n    kmeans.fit(df_preprocessed.drop('species',axis=1))\n    pred_i = kmeans.labels_\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,10),wcss)\nplt.ylim([0,1800])\nplt.title('The Elbow Method',{'fontsize':20})\nplt.xlabel('Number of clusters')\nplt.ylabel('Within-cluster Sum of Squares');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from graph that as k goes from 1 to 3 there is a steep decline in wcss. As k increases further decrease in k becomes linear. So, k=3 is a good choice."},{"metadata":{},"cell_type":"markdown","source":"## <a id ='kneighbor'>K-Nearest Neighbours Classification</a>\n\nIt is a **supervised learning algorithm** which can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry.\n\nWith the given data, KNN can classify new, unlabelled data by analysis of the ***k number of the nearest data points***."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# We need to split data for supervised learning models.\nX_train, X_test, y_train, y_test = train_test_split(df_preprocessed.drop('species',axis=1),target,test_size=0.50)\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\npreds_knn = knn.predict(X_test)\nprint(confusion_matrix(y_test,preds_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(confusion_matrix(y_test,preds_knn),annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24});","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,preds_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,preds_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(knn.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(knn.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Figuring out best value for k**"},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate=[]\nfor i in range(1,10):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i!=y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1,10),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph, we can notice that best vaalue for k is 6.\n\n**Best Model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(X_train,y_train)\npreds_knn = knn.predict(X_test)\nprint(confusion_matrix(y_test,preds_knn))\nprint(classification_report(y_test,preds_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we achieved 99% accuracy."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}