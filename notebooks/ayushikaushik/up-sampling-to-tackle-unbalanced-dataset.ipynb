{"cells":[{"metadata":{},"cell_type":"markdown","source":"**About the dataset:**\nInput variables (based on physicochemical tests):\n* 1 - fixed acidity\n* 2 - volatile acidity\n* 3 - citric acid\n* 4 - residual sugar\n* 5 - chlorides\n* 6 - free sulfur dioxide\n* 7 - total sulfur dioxide\n* 8 - density\n* 9 - pH\n* 10 - sulphates\n* 11 - alcohol\n\nOutput variable (based on sensory data):\n* 12 - quality (score between 0 and 10)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\n# Check the data\nraw_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 1599 observations in the dataset. Luckily, no missing values!\nLet's have a look at the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(raw_data.quality);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Though feature quality can take values from 0 to 10, here we have only 6 possible values,i.e.,(3,4,5,6,7,8). Let us partition it into 'good' and 'bad' range. Values less than and equal to 5 will corespond to bad quality wine and vice versa.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()['quality'].sort_values()[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = raw_data.copy()\nplt.figure(figsize=(12,12))\nsns.heatmap(data.corr(),annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe quality is highly correlated with volatile acidity and alcohol features.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def quality_trans(x):\n    if x<6:\n        return 0\n    else:\n        return 1\ndata.quality = data.quality.map(quality_trans)\nsns.countplot(data.quality);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make this data balanced let's upsample the minority class using sklearn library resample.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample,shuffle\ndf_majority = data[data['quality']==1]\ndf_minority = data[data['quality']==0]\ndf_minority_upsampled = resample(df_minority,replace=True,n_samples=855,random_state = 123)\nbalanced_df = pd.concat([df_minority_upsampled,df_majority])\nbalanced_df = shuffle(balanced_df)\nbalanced_df.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"balanced_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing mean values of all the features we can see there is difference in their magnitude. So, we will standardize our data to get all the features on same scale.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Also, mean and max value of feature residual sugar have a huge gap implying resence of outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(balanced_df['residual sugar']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(balanced_df[balanced_df['residual sugar']>4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(balanced_df['volatile acidity']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As for now I am not dealing with outliers in features because data is not much big.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardization\nfrom sklearn.preprocessing import StandardScaler\nX = balanced_df.drop('quality',axis=1)\ny = balanced_df.quality\nscaled_X = pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(scaled_X,y,test_size=0.3,shuffle=True,random_state=42)\nx_train.shape,x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score,accuracy_score\n\nclassifiers = {\n    'Logistic Regression' : LogisticRegression(),\n    'Decision Tree' : DecisionTreeClassifier(),\n    'Random Forest' : RandomForestClassifier(),\n    'Support Vector Machines' : SVC(),\n    'K-nearest Neighbors' : KNeighborsClassifier(),\n    'XGBoost' : XGBClassifier()\n}\nresults=pd.DataFrame(columns=['Accuracy in %','F1-score'])\nfor method,func in classifiers.items():\n    func.fit(x_train,y_train)\n    pred = func.predict(x_test)\n    results.loc[method]= [100*np.round(accuracy_score(y_test,pred),decimals=4),\n                         round(f1_score(y_test,pred),2)]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets try to do some evaluation for random forest model using cross validation.\nfrom sklearn.model_selection import cross_val_score\nrfc_eval = cross_val_score(estimator = RandomForestClassifier(), X = x_train, y = y_train, cv = 10)\nrfc_eval.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest model seems promising. :)\n\n***Please upvote!!!***","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}