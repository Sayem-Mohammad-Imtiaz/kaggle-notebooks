{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create DataFrames\ndf_train = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", encoding=\"latin1\")\ndf_test = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\", encoding=\"latin1\")\n\n# Shuffle DataFrames\ndf_train = df_train.sample(frac=1)\ndf_test = df_test.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the length of the tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ndef token_counter(text, tokenizer):\n    return len(tokenizer.encode(text))\n\ntok_len = df_train[\"OriginalTweet\"].apply(lambda x : token_counter(x, tokenizer))\n\nmax(list(tok_len))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The longest tweet contains 184 tokens, we don't have to use padding up to the 512th token, we will stop at 200 to reduce the size of the tensors handled."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.model_max_length = 200","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The training dataframe contains {} Tweets\".format(len(df_train)))\nprint(\"The test dataframe contains {} Tweets\".format(len(df_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Sentiment.value_counts().loc[[\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"]].plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Dataset is well balanced between categories."},{"metadata":{},"cell_type":"markdown","source":"## Processing the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef remove_links(text):\n    to_remove = ['\\r','\\n',',',';',':','.']\n    \n    out = re.sub(r'http\\S+', '', text)\n    \n    for token in to_remove:\n        out = out.replace(token, '')\n    \n    return re.sub(' +', ' ', out.lower()) #Remove duplicate spaces\n\ndef tokenize(text, tokenizer):\n    return tokenizer.encode(text, padding='max_length')\n\nname_to_idx = {\n    \"Extremely Negative\" : 0,\n    \"Negative\" : 1,\n    \"Neutral\" : 2,\n    \"Positive\" : 3,\n    \"Extremely Positive\" : 4\n}\n\ndef process_tgt(value):\n    return name_to_idx[value]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = list(df_train[\"OriginalTweet\"].apply(remove_links).apply(lambda x : tokenize(x, tokenizer)))\ntrain_labels = list(df_train[\"Sentiment\"].apply(process_tgt))\n\ntest_text = list(df_test[\"OriginalTweet\"].apply(remove_links).apply(lambda x : tokenize(x, tokenizer)))\ntest_labels = list(df_test[\"Sentiment\"].apply(process_tgt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CreateDataset(Dataset):\n    \n    def __init__(self, data, labels):\n        super().__init__()\n        self.data = data\n        self.labels = labels\n        \n        \n    def __getitem__(self, idx):\n        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx])\n    \n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = CreateDataset(train_text, train_labels)\ntest_dataset = CreateDataset(test_text, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dataset),len(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size = 32)\ntest_loader = DataLoader(test_dataset, batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMModel(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(input_size = embedding_dim, \n                            hidden_size = hidden_size, \n                            num_layers = num_layers,\n                            dropout = dropout,\n                            batch_first = True,\n                            bidirectional = True)\n        self.linear = nn.Linear(512*200, 5)\n        \n    def forward(self, inputs):\n        emb = self.embedding(inputs)\n        lstm_out, _ = self.lstm(emb)\n        \n        output = self.linear(lstm_out.reshape(lstm_out.size()[0], -1))\n        \n        return output\n    \nmodel = LSTMModel(tokenizer.vocab_size, 256, 256, 4, 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nclass Trainer():\n    \n    def __init__(self, model, train_loader, valid_loader):\n        \n        self.model = model\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n\n    def train_epoch(self, f_loss, optimizer, device):\n\n        # We enter train mode. This is useless for the linear model\n        # but is important for layers such as dropout, batchnorm, ...\n        self.model.train()\n\n        correct = 0\n        tot_loss = 0\n        N = 41157 # Dataset length\n\n        # iterator = tqdm(enumerate(self.train_loader))\n        iterator = enumerate(self.train_loader)\n\n        for i, (inputs, targets) in iterator:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Compute the forward pass through the network up to the loss\n            outputs = self.model(inputs)\n\n            loss = f_loss(outputs, targets)\n\n            loss_value = loss.item()\n\n            # Backward and optimize\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            tot_loss += inputs.shape[0] * loss_value\n\n            predicted_targets = outputs.argmax(dim=1)\n            correct += (predicted_targets == targets).sum().item()\n\n            #iterator.set_description(\"loss : {: .3f} | accuracy : {: .3f}\".format(tot_loss/(inputs.shape[0]*(i+1)), correct/(inputs.shape[0]*(i+1))))\n\n        return tot_loss/N, correct/N\n\n    def valid_epoch(self, f_loss, device):\n        # We enter train mode. This is useless for the linear model\n        # but is important for layers such as dropout, batchnorm, ...\n        self.model.eval()\n\n        correct = 0\n        tot_loss = 0\n        N = 3798 # Dataset length\n\n        # iterator = tqdm(enumerate(self.valid_loader))\n        iterator = enumerate(self.valid_loader)\n\n        with torch.no_grad():\n            for i, (inputs, targets) in iterator:\n                inputs, targets = inputs.to(device), targets.to(device)\n\n                # Compute the forward pass through the network up to the loss\n                outputs = self.model(inputs)\n\n                loss = f_loss(outputs, targets)\n\n                tot_loss += inputs.shape[0] * loss.item()\n\n                predicted_targets = outputs.argmax(dim=1)\n                correct += (predicted_targets == targets).sum().item()\n\n                # iterator.set_description(\"loss : {: .3f} | accuracy : {: .3f}\".format(tot_loss/(inputs.shape[0]*(i+1)), correct/(inputs.shape[0]*(i+1))))\n\n        return tot_loss/N, correct/N\n\n    def training(self, f_loss, optimizer, device, epochs = 10):\n\n        train_loss = []\n        train_acc = []\n        valid_loss = []\n        valid_acc = []\n\n        for i in range(epochs):\n            print(\"EPOCH {}/{}\".format(i + 1, epochs))\n            train_results = self.train_epoch(f_loss, optimizer, device)\n            print(\"Training loss : {: .3f} | Training accuracy : {: .3f}\".format(*train_results))\n            valid_results = self.valid_epoch(f_loss, device)\n            print(\"Validation loss : {: .3f} | Validation accuracy : {: .3f}\\n\".format(*valid_results))\n\n            train_loss.append(train_results[0])\n            train_acc.append(train_results[1])\n            valid_loss.append(valid_results[0])\n            valid_acc.append(valid_results[1])\n\n        return train_loss, train_acc, valid_loss, valid_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda')\nmodel = model.cuda()\n\nf_loss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = Trainer(model, train_loader, test_loader)\n\ntrain_loss, train_acc, valid_loss, valid_acc = trainer.training(f_loss, optimizer, device, epochs = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_loss, label = \"train set\")\nplt.plot(valid_loss, label = \"test set\")\nplt.legend()\nplt.title(\"Loss of the model during training\")\nplt.show()\n\nplt.plot(train_acc, label = \"train set\")\nplt.plot(valid_acc, label = \"test set\")\nplt.legend()\nplt.title(\"Accuracy of the model during training\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}