{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction \n\nThe purpose of this notebook is perform *sentiment analysis*, which is one of the most common use cases in NLP. The data consists of tweets were 6 major US Airlines are mentioned. Various information can be found in the data, such as:\n* Tweet\n* User\n* Airlines mentioned\n* Tweet's sentiment\n\nThere are more informations, but for the purpose of this project they aren't relevant. This notebook consists of 4 major steps:\n1. Import and Text preprocessing\n2. Text to Numbers Conversion\n3. Model Training\n4. Model Validation\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nimport re\n\n# text preprocessing\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, TweetTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\n# deep learning model\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom keras.layers import Embedding, Input, Dense, LSTM, GlobalMaxPooling1D, GRU, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\n\n# regularizers\nfrom keras.regularizers import L1L2, l2\n# Wordcloud\nfrom wordcloud import WordCloud\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Import","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/twitter-airline-sentiment/Tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Introduction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"text\", \"airline_sentiment\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    \"removes stopwords\"\n    # customize stopwords\n    my_stopwords = stopwords.words(\"english\")\n    my_stopwords.remove(\"not\")\n    # remove stopwords\n    filtered_words = [word for word in text if word not in my_stopwords]\n    return filtered_words\n\ndef remove_punctuation(text):\n    \"removes punctutation\"\n    my_punct = string.punctuation\n    my_punct += \"“”’\" # add unusual apostrophes\n    no_punct = [w for w in text if w not in my_punct]\n    return no_punct\n\ndef remove_numbers(text):\n    \"removes strings containing only digits\"\n    reduced = re.sub(r'\\b[0-9]+\\b\\s*', '', text)\n    return reduced\n\ndef remove_signs(text, sign):\n    \"removes a particular sign\"\n    try:\n        reduced = [w for w in text if sign not in w]\n        return reduced\n    except Exception as e:\n        print(e)\n        return text\n    \ndef remove_links(text):\n    \"removes links\"\n    reduced = re.sub(r'http\\S+', '', text)\n    return reduced\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Text cleaning pipeline\nFor tokenization we use `TweetTokenizer`. Generally `word_tokenize()` is a default method used by majority. However it splits english contractions (e.g. \"didn't\" -> [\"did\", \"n't\"]), which isn't a wanted behaviour. `TweetTokenizer` handles contractions just as expected - without splitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(inp_text):\n    \"\"\"\n        This function is a pipeline for text preprocessing\n        It consists of following steps:\n            - converting text to lowercase\n            - removing words containing only digits\n            - removing links\n            - removing stopwords\n            - removing punctuation\n            - removing mentions and hashtags\n    \"\"\"\n    #lowercase\n    text = inp_text.lower()\n    # remove only number words\n    text = remove_numbers(text)\n    # remove links \n    text = remove_links(text)\n    # divide input sentence into words\n    tknzr = TweetTokenizer()\n    text = tknzr.tokenize(text)\n    # remove stopwords\n    text = remove_stopwords(text)\n    # remove punctuation\n    text = remove_punctuation(text)\n    # remove mentions\n    text = remove_signs(text, \"@\")\n    # remove hashtags\n    text = remove_signs(text, \"#\")\n    # join a list of words into a sentence\n    filtered_sentence = (\" \").join(text)\n    return filtered_sentence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting tweets with text preprocessing pipeline and writing it to `df[\"filtered_tweets\"]`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"filtered_tweets\"] = df[\"text\"].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud\n\nLet's see which words are most common for each type of tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neg = \" \".join(tweet for tweet in df[df[\"airline_sentiment\"]==\"negative\"][\"filtered_tweets\"])\nneu = \" \".join(tweet for tweet in df[df[\"airline_sentiment\"]==\"neutral\"][\"filtered_tweets\"])\npos = \" \".join(tweet for tweet in df[df[\"airline_sentiment\"]==\"positive\"][\"filtered_tweets\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 9))\nneg_cloud = WordCloud(max_words=50).generate(neg)\nplt.imshow(neg_cloud, interpolation='bilinear')\nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 9))\npos_cloud = WordCloud(max_words=50).generate(pos)\nplt.imshow(pos_cloud, interpolation='bilinear')\nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 9))\nneu_cloud = WordCloud(max_words=50).generate(neu)\nplt.imshow(neu_cloud, interpolation='bilinear')\nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting sentiments into numerical values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_labels(sentiment):\n    if sentiment == \"negative\":\n        return 0\n    if sentiment == \"neutral\":\n        return 1\n    if sentiment == \"positive\":\n        return 2\n    \ndf[\"label\"] = df[\"airline_sentiment\"].apply(get_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the distribution of sentiments of all tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"airline_sentiment\"].hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels distribution\ndf[\"label\"].value_counts() / df[\"label\"].value_counts().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data\n\nLabels are slightly imbalanced with over 62% tweets being negative. This is why I am gonna use `stratify=df[\"label\"]` parameter to make sure the distribution of labels in train and test data is almost identical.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_txt, test_txt, train_labels, test_labels = train_test_split(\n    df[\"filtered_tweets\"], df[\"label\"], \n    test_size=0.2, stratify=df[\"label\"], random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.value_counts() / train_labels.value_counts().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting text into numbers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Tokenizing\nIn other words we split sentences into vectors of words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tknz = Tokenizer()\ntknz.fit_on_texts(train_txt)\ntrain_sentences = tknz.texts_to_sequences(train_txt)\ntest_sentences = tknz.texts_to_sequences(test_txt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unique words count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tknz.word_counts)\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add padding\n\nWe extend each vector to the length of sentence containing the most words. Padding adds 0s at the beginning by default","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# add padding to the train set\ntrain_pad = pad_sequences(train_sentences)\n\n# max length of words in a single sentence\nmax_len = train_pad.shape[1]\n\n# add padding to the test set using max len\ntest_pad = pad_sequences(test_sentences, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pad.shape, test_pad.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding labels\n\nThis step is necessary since the output layer in our model expects a 2D array.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = to_categorical(train_labels)\ny_test = to_categorical(test_labels)\ny_train[:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Training a model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's begin with a complete basic `LSTM`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of the Embeddings vector\nD = 15\n# size of the hidden vector\nM = 10\n# vocab size\nV = vocab_size\n# sequence length\nT = max_len\n\nbatch_size = 32\n\ni = Input(shape=(T,))\nx = Embedding(V+1, D)(i)\nx = LSTM(M)(x)\nx = Dense(3, activation='softmax')(x)\n\nmbasic = Model(i, x)\n# model.summary()\n\nmbasic.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nbasic = mbasic.fit(train_pad, y_train, batch_size=batch_size, epochs=10, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see, that we have to deal with overfitting. Training loss decreases, but validation loss increases after few first epochs\n\nLet's increase the # of hidden units - M","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of the Embeddings vector\nD = 15\n# size of the hidden vector\nM = 64\n# vocab size\nV = vocab_size\n# sequence length\nT = max_len\n\nbatch_size = 32\n\ni = Input(shape=(T,))\nx = Embedding(V+1, D)(i)\nx = LSTM(M)(x)\nx = Dense(3, activation='softmax')(x)\n\nmbig = Model(i, x)\n# model.summary()\n\nmbig.compile(optimizer=Adam(lr=0.0003), loss='categorical_crossentropy', metrics=['accuracy'])\n\nbig = mbig.fit(train_pad, y_train, batch_size=batch_size, epochs=10, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No improvement at all...\n\nLet's use `GlobalMaxPooling1D`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of the Embeddings vector\nD = 10\n# size of the hidden vector\nM = 10\n# vocab size\nV = vocab_size\n# sequence length\nT = max_len\n\nbatch_size = 32\n\ni = Input(shape=(T,))\nx = Embedding(V+1, D)(i)\nx = LSTM(M, return_sequences=True)(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(3, activation='softmax')(x)\n\npool = Model(i, x)\n# model.summary()\n\npool.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nlstm_pool = pool.fit(train_pad, y_train, batch_size=batch_size, epochs=10, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nope...\n\nMaybe `recurrent_dropout` will help?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of the Embeddings vector\nD = 15\n# size of the hidden vector\nM = 10\n# vocab size\nV = vocab_size\n# sequence length\nT = max_len\n\nbatch_size = 32\n\ni = Input(shape=(T,))\nx = Embedding(V+1, D)(i)\nx = LSTM(M, recurrent_dropout=0.2)(x)\nx = Dense(3, activation='softmax')(x)\n\nrec_drop = Model(i, x)\n# model.summary()\n\nrec_drop.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nrd = rec_drop.fit(train_pad, y_train, batch_size=batch_size, epochs=10, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far nothing helps... \n\nUsing *masking* allows us to ignore padding values - all 0s we added before.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nfrom keras.layers import Lambda\n# size of the Embeddings vector\nD = 15\n# size of the hidden vector\nM = 10\n# vocab size\nV = vocab_size\n# sequence length\nT = max_len\n\nbatch_size = 32\n\ni = Input(shape=(T,))\nmask = Lambda(lambda inputs: K.not_equal(inputs, 0))(i)\nx = Embedding(V+1, D)(i)\nx = LSTM(M, return_sequences=True)(x, mask=mask)\nx = LSTM(M)(x, mask=mask)\n# x = GlobalMaxPooling1D()(x)\nx = Dense(3, activation='softmax')(x)\n\nmmask = Model(i, x)\n# model.summary()\n\nmmask.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nmask = mmask.fit(train_pad, y_train, batch_size=batch_size, epochs=10, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nothing really helps...\n\nLet's try some other, non-standard models, since we have nothing to lose. `GRU` is a simplified, less powerful version of LSTM. But can it perform similarly?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of the Embeddings vector\nD = 15\n# size of the hidden vector\nM = 10\n# vocab size\nV = vocab_size\n# sequence length\nT = max_len\n\nbatch_size = 64\n\ni = Input(shape=(T,))\nx = Embedding(V+1, D)(i)\nx = GRU(M, return_sequences=True)(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(3, activation='softmax')(x)\n\nmgru = Model(i, x)\n# model.summary()\n\nmgru.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\ngru = mgru.fit(train_pad, y_train, epochs=10, batch_size=batch_size, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GRU didn't perform worse than his older and stronger brother\n\nSince we are already experimenting, what about `CNN`s?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import BatchNormalization, Conv1D, Flatten\n# size of the Embeddings vector\nD = 15\n# size of the hidden vector\nM = 10\n# vocab size\nV = vocab_size\n# sequence length\nT = max_len\n\nbatch_size = 64\n\ni = Input(shape=(T,))\nx = Embedding(V+1, D)(i)\nx = Conv1D(32, kernel_size=5)(x)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\n# x = Conv1D(32, kernel_size=3)(x)\n# x = Dropout(0.2)(x)\n# x = BatchNormalization()(x)\nx = Flatten()(x)\nx = Dense(3, activation='softmax')(x)\n\nmcnn = Model(i, x)\n# model.summary()\n\nmcnn.compile(optimizer=Adam(lr=0.0003), loss='categorical_crossentropy', metrics=['accuracy'])\n\ncnn = mcnn.fit(train_pad, y_train, batch_size=batch_size, epochs=10, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(cnn.history['val_loss'], label=\"cnn val loss\")\nplt.plot(basic.history['val_loss'], label=\"Basic LSTM val loss\")\nplt.plot(rd.history['val_loss'], label=\"rec_dr val loss\")\nplt.plot(lstm_pool.history['val_loss'], label=\"LSTM with pool val loss\")\nplt.plot(gru.history['val_loss'], label=\"GRU val loss\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(cnn.history['val_accuracy'], label=\"cnn val acc\")\nplt.plot(basic.history['val_accuracy'], label=\"Basic LSTM val acc\")\nplt.plot(rd.history['val_accuracy'], label=\"rec_dr val acc\")\nplt.plot(lstm_pool.history['val_accuracy'], label=\"LSTM with pool val acc\")\nplt.plot(gru.history['val_accuracy'], label=\"GRU val acc\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the model with `recurrent_dropout` has the highest accuracy after 10 epochs. Let's make some predictions using this model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\npred = rec_drop.predict(test_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_classes = pred.argmax(axis=-1)\ny_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_classes, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\nAs mentioned in introduction, this project consists of 4 parts:\n1. Import and Text preprocessing\n2. Text to Numbers Conversion\n3. Model Training\n4. Model Validation\n\nThe first 2 steps were necessary to make it even possible to pass the data to the input Layer of the models. I can say those steps were completed successfully. I also added some fancy Wordclouds to find the most common words for each type of tweet sentiment. \n\nHowever training the model didn't go as expected. Each type of model I trained shows overfitting. I tried many techniques to deal with this problem, but none of them actually helped. The list techniques:\n* increasing & decreasing # of hidden neurons\n* increasing & decreasing size of embeddings vector\n* adding dropout and/or recurrent dropout\n* adding kernel and recurrent regularizers\n* increasing & decreasing learning rate hyperparameter\n* adding masking\n* using `GlobalMaxPooling`\n* using `GRU` and `CNN` architectures\n\nNot all of those attempts are shown here, because I'd have to add another 20 code cells showing similar results as other models. I've spent hours trying to tweak the model but failed. Great resource I used was this thread on SO:\nhttps://stackoverflow.com/questions/48714407/rnn-regularization-which-component-to-regularize. I learned a lot from it. Unfortunately it seems that nothing can reduce overfitting in this example.\n\nValidating the test set shows, that many wrong predictions are made between neutral and negative tweets. Maybe they are too similar or it is too difficult to separate noise. However 75% accuracy on validation data is still better than random guessing. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}