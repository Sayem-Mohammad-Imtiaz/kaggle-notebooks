{"cells":[{"metadata":{},"cell_type":"markdown","source":"****CONTENT****\n\n* [Decision Tree for Classification](#1)\n* [Decision Tree for Regression](#2)\n* [Generalization Error](#3)\n    * [Diagnose Bias and Variance Problems](#4)\n        * [Cross Validation (CV)](#5)\n            * [K-Fold CV](#6)\n* [Ensemble Learning](#7)\n    * [Voting Classifier](#8)\n    * [Bagging (Bootstrap Aggregation) - Classifier and Regressor](#9)\n        * [Out Of Bag (OOB) Evaluation](#10)\n    * [Random Forests - Classifier and Regressor](#11)\n    * [Boosting](#12)\n        * [Adaboost (Adaptive Boosting) - Classifier and Regressor](#13)\n        * [Gradient Boosting - Classifier and Regressor](#14)\n        * [Stochastic Gradient Boosting (SGB) - Classifier and Regressor](#15)\n* [Tuning a CART's Hyperparameters](#16)\n    * [Grid Search Cross Validation for DecisionTreeClassifier](#17)\n    * [Grid Search Cross Validation for RandomForestRegressor](#18)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n**Decision Tree for Classification**"},{"metadata":{},"cell_type":"markdown","source":"Labeled bir datasetteki samplelara if-else questions sorarak label'larını çıkarmaya yarar.\n\nBunu, feature'ları x-y koordinatı gibi düşün, bölgelere ayırır ve bu bölgelerdeki label'lar da zaten bellidir.\n\nBu decision regionlar işte decision tree if-else sorularına göre belirlenir. \n\nlinear modelin aksine, featurelar ve labellar arasındaki nonlinear ilişkiyle alakalı çıkarım yapabilir treeler\n\nScaling yapılmasına gerek yoktur. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.species = [2 if i == 'Iris-setosa' else 1 if i == 'Iris-versicolor' else 0 for i in df.species]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('species', axis=1).values  \ny = df['species'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n#Import accuracy_score\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 80% train, 20% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1) \n#stratify=y: train and test sets have the same proportion of class labels as the unsplit dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate DecisionTree\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n#random_state=1 for reproducability\n#max_depth=2 2 seviye iniyor tree\n\n#criterion='entropy' parametresi ile, decision region belirlemek için split yaparken hangi metodu kullanacağımızı seçiyoruz. default'u 'gini' dir.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit dt to the training set\ndt.fit(X_train, y_train)\n\n#Predict test set labels\ny_pred =dt.predict(X_test)\n\n#Evaluate test-set accuracy\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n**Decision Tree for Regression**"},{"metadata":{},"cell_type":"markdown","source":"Regression'da target variable continuous. Bunu biliyoruz.\n\nClassification'da yaptığımız gibi, x1 ve x2 iki feature olsun, ve bu değerlere göre grafiğe döktükten sonra \nsplit ede ede (if-else'e göre) decision region'lar bulup, bu bölgelerde yer alan data point'lerin \ntarget(train data set olduğu için biliyoruz bu değerleri) değerlerinin ortalamasını alıyoruz ve bu değerler bizim\nmodelimizin sonuçlarını veriyor.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/autompg-dataset/auto-mpg.csv')\ndf.drop(['car name', 'cylinders', 'model year'], axis=1, inplace=True)\ndf.replace('?','0', inplace=True)\ndf.horsepower = df.horsepower.astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df.mpg, df.displacement) \n\n#aşağıdaki gibi nonlinear bir grafiği linear modeller ile çözümleyemeyiz.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('mpg', axis=1).values  \ny = df['mpg'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n#Import accuracy_score\nfrom sklearn.metrics import mean_squared_error as MSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 80% train, 20% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate DecisionTreeRegressor\ndt = DecisionTreeRegressor(max_depth=4,min_samples_leaf=0.1, random_state=3)\n\n#random_state=3 for reproducability\n#max_depth=4; 4 seviye iniyor tree\n#min_samples_leaf: leaf dediğimiz şey decision regionların herbiri. \n#--bu parametre de, herbir leaf'e train datanın minimum 0.1'i gelecek diyor. 0.1'in altına düştüğünde duruyor. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit dt to the training set\ndt.fit(X_train, y_train)\n\n#Predict test set labels\ny_pred =dt.predict(X_test)\n\n#Compute test-set MSE\nmse_dt = MSE(y_test, y_pred)\n\n#Compute test-set RMSE\nrmse_dt = mse_dt**(1/2)\n\n#Print rmse_dt\nprint(rmse_dt) #bu performans ölçütünü bir de linear regression için yapıp sonuçlar arası farkı görebiliriz.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n**Generalization Error**"},{"metadata":{},"cell_type":"markdown","source":"Supervised learning'de, features ve labels'ın bir fonksyonla birbirine bağlı olduğu varsayımını yaparız: y = f(x) and f is unknown \nAmaç, bu x ile en iyi fit eden f fonksiyonunu bulmaktır.\n\nBunu yaparken, Generalization Error'ı minimize edecek şekilde yaparız.\n\nGeneralization Error = Bias (accuracy ile ilgili) + Variance (precision ile ilgili) + Irreducible Error(constant)\n\nBias: modelin bulduğu f'in gerçek datadan ne kadar saptığı. high bias -->> underfitting demektir.\n       \n       model complexity arttıkça bias azalır.\n\nVariance: modelin bulduğu f farklı training datasetlerle ne kadar inconsistent. high variance -->> overfitting\n\n       model complexity arttıkça variance artar.\n\nyani, bias azalırken variance artar. \n\narada bir denge vardır, ve bu dengede GE minimumdur.\n\nbuna Bias-Variance Trade-Off denir. İkisinin de en az olmasını sağlayamayabiliriz ama optimize edebiliriz.\n\nAmaç, overfitting(f fits the noise in the training set)-underfitting(f is not flexible enough to approximate) sorunları ile karşılaşmayacağımız best complexity of the model'a ulaşmaktır.\n\nModel overfit ediyorsa training seti, unseen dataset'i predict etme gücü zayıf olur. \nAslında, training set için low error verecek şekilde bir model oluşturur overfit olursa, ama test set için uygulandığında zayıf sonuçlar verir.\n\nUnderfit ediyorsa model, train error ve test error birbirine yakın olur, ama ikisi de high olur. Underfit durumunda, model aslında datayı az train ettiği için yeterli doğrulukta bir f bulamamış olur.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n**Diagnose Bias and Variance Problems**"},{"metadata":{},"cell_type":"markdown","source":"Bir supervised ML model train edildiği zaman, GE'yi direkt olarak bilmemiz mümkün değildir. \n\nÇünkü, f is unknown\n       usually only have one dataset\n       noise is unpredictible\n\nBunun çözümü şudur:\n       \n       datayı önce train-test-split yaparız\n       training dataset üzerinde f'i fit ederiz\n       sonra test set üzerinde uygulama yaparak error evaluation yaparız. \n       test set üzerinde çıkan error GE'yi yakınsar deriz.\n       Ancak, test set üzerinde uygulamadan önce, modelin performansından emin olmalıyız.\n       Model performansından emin olmak için ise, Cross Validation (CV) tekniğini kullanmalıyız.\n       \n       \n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n**Cross Validation (CV)**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n**K-Fold CV**"},{"metadata":{},"cell_type":"markdown","source":"Training set üzerinde yapılan bu uygulama için, \n     \n   k=5 için örnekleme yaparsak,\n     \n   training data'yı 5'e bölerek(parçalara fold dedik), 5 kez datayı train ederek 5 farklı error değeri elde ederiz.\n   \n   Bu 5 farklı cv-error değerinin alınan ortalaması ile training set'in error'ından büyük olup olmadığı kıyaslanır.\n   \n   Eğer büyükse, model -->> high variance (yani overfit demek)\n   \n    bu sorunu aşmak için model complexity'yi düşürmek (decrease max_depth, increase min_samples_leaf...) ya da dataseti genişletmek gerekir. \n    \n   Eğer cv-error ve training set error birbinine yakın ancak istenenden çok büyük ise -->> high bias (yani underfit  demek)\n        \n    bu sorunu aşmak için model complexity'yi artırmak (increasing max_depth, decrease min_samples_leaf...) ya da farklı features'i datasete eklemek gerekir.\n    \n   "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n#Import accuracy_score\nfrom sklearn.metrics import mean_squared_error as MSE\n\n#Import cross validation score\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#karışıklık çıkmasın diye df'yi baştan alıyoruz\ndf = pd.read_csv('../input/autompg-dataset/auto-mpg.csv')\ndf.drop(['car name', 'cylinders', 'model year'], axis=1, inplace=True)\ndf.replace('?','0', inplace=True)\ndf.horsepower = df.horsepower.astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('mpg', axis=1).values  \ny = df['mpg'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate DecisionTreeRegressor\ndt = DecisionTreeRegressor(max_depth=4,min_samples_leaf=0.14, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate the list of MSE obtained by 10-fold CV\n#Set n_jobs to -1 in order to exploit all CPU cores in computation\n# neg_mean_squared_error negative mse yap diyoruz metod olarak. bunun sebebi, cross val ile mse hesabının direkt oalrak yapılamaması\nMSE_CV = - cross_val_score(dt, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit dt to the training set\ndt.fit(X_train, y_train)\n\n#Predict the labels of the training set\ny_predict_train = dt.predict(X_train)\n\n#Predict the labels of the test set\ny_predict_test = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CV MSE:\nprint(MSE_CV.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Set MSE:\nprint(MSE(y_train, y_predict_train))\nprint()\n\n#Test Set MSE:\nprint(MSE(y_test, y_predict_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Set Error: 13.65\n# Test Set Error    : 22.00\n# CV Error          : 16.72\n# TrainingSetError < CVError: high variance(overfit): model complexity'yi düşür (decrease max_depth, increase min_samples_leaf...)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n**Ensemble Learning**"},{"metadata":{},"cell_type":"markdown","source":"Ensemble Learning bir supervised learning technique'dir.\n\nCART(Classification and Regression Trees) avantajları:\n- Uygulaması ve yorumlaması basit,\n- features ve labels arasındaki nonlinear dependencies'i tanımlayabilme yeteneği konusundaki esneklik\n- standardize/normalize ihtiyacı yok (bir preprocessing işlemi olarak)\nCART(Classification and Regression Trees) dezavantajları:\n- classification örneğin sadece orthogonal decision boundaries üretir.\n- daha önemlisi, !! CARTs training setteki ufak değişikliklere karşı very sensitive'dir!! \n  bir data point'in training setten çıkarılması bile sonuçları drastically değiştirir.\n- CART also suffer from high variance when they are trained without constraints. So they may overfit \n\n\nThe solution that takes advantage of the flexibility of CARTs while reducing their tendency to memorize noise is ENSEMBLE LEARNING:\n\nAs a summary;\nfirst step; different models are trained on the same dataset. Each model makes its own predictions. \n\na meta-model then aggregates the predictions of individual models and outputs a final prediction.\n\nthe final prediction is more robust and less prone to errors than each individual model.\n\nBest results are obtained when the models are skillful but in different ways meaning that if some models make predictions that are way off,\nthe other models should compensate these errors. In such cases, meta-model's predictions are more robust.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n**Voting Classifier**"},{"metadata":{},"cell_type":"markdown","source":"Bir ensemble learning tekniği.\n\nBinary classification task!\n\nBu metamodel'in prediction'ı : hard voting!\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import functions to compute accuracy and split data\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n#import models, including VotingClassifier as meta-model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate individual classifiers\nlr = LogisticRegression(random_state=SEED)\nknn = KNN()\ndt = DecisionTreeClassifier(random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a list called classifier that contains tupples (classifier_name, classifier)\nclassifiers = [('Logistic Regression', lr),\n              ('K Nearest Neighbours', knn),\n              ('Classification Tree', dt)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can now write a for loop to iterate over the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train, y_train)\n    \n    #predict the labels of the test set\n    y_pred = clf.predict(X_test)\n    \n    #evaluate the accuracy of clf o the test set\n    print(clf_name, ':', accuracy_score(y_test, y_pred))\n    \n#en iyi sonucu lr verdi.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate a VotingClassifier \nvc = VotingClassifier(estimators=classifiers)\n\n#fit vc to the training set and predict test set labels\nvc.fit(X_train, y_train)\ny_pred = vc.predict(X_test)\n\n#evaluate the test-accuracy score of vc\nprint('Voting Classifier:', accuracy_score(y_test, y_pred))\n\n#bu sonuç modelleri ayrı ayrı çözdüğümüzde çıkan sonuçlardan daha fazla.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n**Bagging (Bootstrap Aggregation) - for Classification and Regression**"},{"metadata":{},"cell_type":"markdown","source":"Voting Classifier aynı train set'i farklı algoritmalarla fit ederek sonuca ulaşırken,\n\nBagging aynı algoritmayı data'nın subset'leri üzerinde train ederek sonuca ulaşır. \n\nBagging, ensamble yapacağımız modelin variance'ını düşürme gibi istenilen bir sonuç doğurur.\n\nsubset oluştururken data'dan replacement metodu ile(yani aynı data point birden fazla kez seçilebilir) seçilerek N adet subset oluşturulur, bu N adet subset de aynı algoritmaya sokularak train edilir. Herbir oluşturulan model prediction verir. \n\nMeta-model bu prediction'ları toplar ve final prediction yapar. \n\n\nClassification için;\nBaggingClassifier kullanırız, \npredictions'ı by majority voting ile yaparken,\n\nRegression için;\nBaggingRegressor kullanırız,\npredictions'ı by averaging ile yapar."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import functions to compute accuracy and split data\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n#import models, including BaggingClassifier as meta-model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16 ,random_state=SEED)\n\n#Instantiate BaggingClassifier\nbc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1) #300 tree var; n_jobs=-1 so that all CPU cores are used in calculation\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit bc to the training set\nbc.fit(X_train, y_train)\n\n#predict test labels\ny_pred = bc.predict(X_test)\n\n#evaluate test-set accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))\n\n#normalde dt'yi bagging yapmadan uyguladığımızda 0.88 gibi bişey çıkıyormuş.\n#bagging ile dt'nin performansını artırmış olduk.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n**Out Of Bag (OOB) Evaluation**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Bagging'de data point'ler birden fazla sample'da yer alabilirler. Diğer taraftan, bazı data point'ler ise hiçbir sample'da yer almayabilir. \nÖrneğin, datanın 63%'si training sample'larda yer alırken, hiçbir training sample'da yer almayan 37% lik kısma Out-Of-Bag (OOB) instances diyoruz.\n\nOOB instances hiçbir training sette yer almayan kısım olup, cross-validation'a gerek kalmadan ensemble'ın performansını ölçmek için kullanılır.\nBu tekniğe OOB Evaluation denmektedir.\n\nBagging'de oluşturulan N adet sample için, her biri için, bootstrap instances (training edilecekler) ve OOB instances (dışarıda kalanlar) vardır.\nEnsemble zaten Bagging ile çalışırken, dışarıda kalan N adet OOB instances ayrı ayrı evaluate edilir. Sonuç olarak N adet OOB value elde edilir ve \nortalaması alınarak OOB score elde edilir.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import functions to compute accuracy and split data\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n#import models, including BaggingClassifier as meta-model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16 ,random_state=SEED)\n\n#Instantiate BaggingClassifier\nbc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1) #300 tree var; n_jobs=-1 so that all CPU cores are used in calculation\n#ayrıca oob_score=True -> oob score hesaplayabilmek için","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not: OOB-score \nclassifiers için accuracy,\nregressors için r-squared score verir."},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit bc to the training set\nbc.fit(X_train, y_train)\n\n#predict test labels\ny_pred = bc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate test-set accuracy\ntest_accuracy = accuracy_score(y_test, y_pred)\n\n#evaluate OOB accuracy from bc\noob_accuracy = bc.oob_score_\n\nprint('Accuracy of Bagging Classifier: {:.3f}'.format(test_accuracy))\nprint('OOB Accuracy of Bagging Classifier: {:.3f}'.format(oob_accuracy))\n#oob accuracy ile cross validation yapmadan bagging ensemble modeli performans tahmini yapabiliriz.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n**Random Forests**"},{"metadata":{},"cell_type":"markdown","source":"Bir ensemble learning tekniği.\n\nBagging'de bir base estimator'ımız vardı. Bu base estimator ile subdatalar train edilerek model çözülüyordu.\n\nRandom forest base estimator olarak decisionTree'yi kullanan bir ensemble learning tekniğidir diyebiliriz.\nAncak random forest'ta sub-data (yani bootstrap sample) eşit büyüklükte olacak şekilde train edilir. Ancak bagging'deki gibi replacement yoktur.\nData eşit olarak paylaşılmış gibi düşün.\nRandom forest'ta herbir tree train edilirken belirli bir sayıda (d) feature can be sampled at each node without replacement.\nd default olarak feature sayısının kareköküdür. \nDiğerlerinde olduğu gibi sub-data ların train edilmesiyle oluşan predictionlar random forest meta-model tarafından toplanır ve final prediction yapılır.\n\nClassification'da Final prediction majority voting'e göre yapılır ve RandomForestClassifier kullanılır.\nRegression'da final prediction averaging ile hesaplanır ve RandomForestRegressor kullanılır.\n\nRandom Forests, individual tree'lere göre düşük bir variance'a ulaşır.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import functions to compute accuracy and split data\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\n\n#import model as meta-model\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate random forest regressor with 400 estimators\nrf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED) #400 regression trees","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit rf to the training set\nrf.fit(X_train, y_train)\n\n#predict test labels\ny_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate RMSE\nrmse_test = MSE(y_test, y_pred)**(1/2)\n\nprint('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n#single regression tree'den daha düşük bir değere ulaşmışız.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tanımda bahsetmiştik, feature importance ile ilgili çıkarımlar yapılabilir diye:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a pd.series of features importances\nimportances_rf = pd.Series(rf.feature_importances_, index= df.drop('diagnosis', axis=1).columns) #index aslında X, ama array olmayacağı için values'İ çıkardık.\n\n#sort importances_rf\nsorted_importances_rf = importances_rf.sort_values()\n\n#make horizontal plot\nsorted_importances_rf.plot(kind='barh', color='lightgreen')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n**Boosting**"},{"metadata":{},"cell_type":"markdown","source":"Boosting bir ensemble learning metodudur.\n\nIn which many predictors are trained and each predictor learns from the errors of its predecessor.\nYani, boosting is ensemble method combining several weak learners to form a strong learner.\n\nWeak learner; model doing slightly better than random guessing.\nFor example, max_depth=1 olan decision tree (buna decision stump denir) bir weak learner'dır.\n\nIn boosting, train an ensemble of predictors sequentially. and each predictor tries to correct its predecessor.\n\nMost popular boosting methods: \n1- AdaBoost\n2- Gradient Boosting\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n**Adaboost (Adaptive Boosting) - Classifier and Regressor**"},{"metadata":{},"cell_type":"markdown","source":"In adaboost, each predictor pays more attention to the instances wrongly predicted by its predecessor by changing the weights of training instances.\n\nEach predictor is assigned a coefficient alpha that weights its contribution to final prediction.\n\nAlpha depends on predictor's training error.\n\nİlk olarak predictor1 is trained on the initial dataset (X,y), ve training error for predictor1 is determined.\nThis error alpha1 i belirlemek için kullanılır (predictor1'in coefficient'i).\nAlpha1 daha sonra predictor2'nin training instances'ının weight'lerinin belirlenmesinde kullanılır. \nBu şekilde, N adet predictor (N subdata ile modelin train edilmesi), alphalar belirlenerek incorrectly predicted instances'e dikkat edilmesi işlemi yapar.\n\nBir parametremiz daha var, learning rate!\nBu parametre ile number of estimators arasında bir trade-off vardır.\n\nClassification için AdaBoostClassifier kullanılır, ensemble's prediction is obtained by weighted majority voting.\nRegression için AdaBoostRegressor kullanılır, ensemble's prediction is obtained by weighted average.\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import functions to compute accuracy and split data\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n#import model as meta-model\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=1, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate AdaBoostClassifier\nadb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit adb_clf to the training set\nadb_clf.fit(X_train, y_train)\n\n#predict test set probabilities of positive class\ny_pred_proba = adb_clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate test set roc auc score\nadb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba) #roc_auc_score için y_pred_proba gerekliydi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test set roc auc score of dt: {:.2f}'.format(adb_clf_roc_auc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n**Gradient Boosting**"},{"metadata":{},"cell_type":"markdown","source":"It has Proven track record of winning many machine learning competiitons.\n\nIn gradient Boosting,\nSequential correction of predecessor's errors.\nadaboost'un tersine, does not tweak(çimdik) the weights of training instances. \nBunun yerine, each predictor is trained using residual errors of its predecessor as labels.\n\ntree1 is trained using the features matrix X and dataset labels y.\nthe predictions y1 are used to determine the training set residual errors r1.\nr1, gerçek değerler ile y1 arasındaki farklardan oluşuyor.\nardından,\ntree2 is trained using the features matrix X and the residual errors r1 of tree1 as labels.\nbu şekilde, N tree train edilene kadar devam edilir.\n\nGradient Boosting'de kullanılan önemli parametre: Shrinkage\nShrinkage: prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate.\n\nYine, Bu parametre ile number of estimators arasında bir trade-off vardır.\n\nClassification için GradientBoostingClassifier kullanılır\nRegression için GradientBoostingRegressor kullanılır"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import functions to compute accuracy and split data\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\n\n#import model as meta-model\nfrom sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate GradientBoostingClassifier\ngbt = GradientBoostingClassifier(n_estimators=300, max_depth=1, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit gbt to the training set\ngbt.fit(X_train, y_train)\n\n#predict the test set labels\ny_pred = gbt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate test set\nrmse_test = MSE(y_test, y_pred)**(1/2)\n\nprint('Test set RMSE of rf: {:.2f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n**Stochastic Gradient Boosting (SGB)**"},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting involves exhaustive search procedure.\nEach tree in the ensemble is trained to find the best split-points and the best features.\nThis procedure may lead to CARTs that use the same split-points and possibly the same features.\n\nBunun üstesinden gelmek için, you can use Stochastic Gradient Boosting (SGB).\nIn Stochastic Gradient Boosting, each CART is trained on a random subset of the training data.\nThis subset is sampled without replacement. \nat the level of each node, features are sampled without replacement when choosing the best split-points.\nAs a result, this creates further diversity in the ensemble and the net effect is adding more variance to the ensemble of trees.\n\nnormalde X ve y ile training yapılırken, burada bunlardan bir sample alınarak train edilir. Predictions yapılır ve residual errors are computed.\nBu residual errors are multiplied by the learning rate, then fed to the next tree in ensemble.\nBu procedure tüm tree ler bitene kadar uygulanır.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\ndf.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import functions to compute accuracy and split data\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\n\n#import model as meta-model\nfrom sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate STOCHASTiC GradientBoostingClassifier\nsgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n#subsample=0.8 -> each tree to sample 80% of the data for training\n#max_features=0.2 -> each tree uses 20% of available features to perform the best-split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit sgbt to the training set\nsgbt.fit(X_train, y_train)\n\n#predict the test set labels\ny_pred = sgbt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate test set\nrmse_test = MSE(y_test, y_pred)**(1/2)\n\nprint('Test set RMSE of rf: {:.2f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n**Tuning a CART's Hyperparameters**"},{"metadata":{},"cell_type":"markdown","source":"Parameters: are learned from data throUgh training.\nexample: split-point of a node, split feature in a CART\n\nHyperparameters: are not learned from data. they should be set prior to training.\nexample: max_depth, min_sample_leaf, in a CART\n\nHyperparameter tuning consists of searching for the set of optimal hyperparameters for the learning algorithm.\n\nBirçok tuning metodu var ancak biz grid search görücez."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a> <br>\n**Grid Search Cross Validation for DecisionTreeClassifier**"},{"metadata":{},"cell_type":"markdown","source":"first, you manually set a grid of hyperparameter values.\n\nthen, you pick a metric for scoring model performance and you search exhaustively through the grid.\n\nFor each set of hyperparameters, you evaluate each model's CV score.\n\nThe optimal hyperparameters are those of the model achieving the best CV score.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\ndf.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate a DecisionClassifier dt\ndt = DecisionTreeClassifier(random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print out dt's hyperparameters:\nprint(dt.get_params())\n#biz sadece max_depth, max_features ve min_samples_leaf i optimize edelim.\n#max_feature: nr of features to consider when looking for the best split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import GridSearchCV \nfrom sklearn.model_selection import GridSearchCV\n\n#define the grid of hyperparameters \nparams_dt = {'max_depth': [3,4,5,6],\n             'min_samples_leaf': [0.04, 0.06, 0.08],\n             'max_features': [0.2, 0.4, 0.6, 0.8]}\n\n#Instantiate a 10-fold CV grid search object \ngrid_dt = GridSearchCV(estimator=dt, \n                       param_grid=params_dt,\n                       scoring='accuracy',\n                       cv=10,\n                       n_jobs=-1)\n\n#fit grid_dt to the training set\ngrid_dt.fit(X_train, y_train)\n\n#extract best hyperparameters from 'grid_dt'\nbest_hyperparams = grid_dt.best_params_\nprint('best hyperparameters:', best_hyperparams)\n\n#extract best CV score from grid_dt\nbest_CV_score = grid_dt.best_score_\nprint('best CV score:', best_CV_score)\n\n#extract best model from grid_dt\nbest_model = grid_dt.best_estimator_\nprint('best model:', best_model)\n\n#evaluate test set accuracy\ntest_acc = best_model.score(X_test, y_test)\nprint('test set accuracy of best model:', test_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n**Grid Search Cross Validation for RandomForestRegressor**"},{"metadata":{},"cell_type":"markdown","source":"CART hyperparameters'a ek olarak number of estimators, bootstrap(True or False) ekleyebiliriz RandomForest'ta.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\ndf.diagnosis = [1 if i == 'M' else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis', axis=1).values  \ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seed for reproducability\nSEED = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate RandomForestRegressor\nrf = RandomForestRegressor(random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspect rf's hyperparameters\nrf.get_params()\n#we will optimize n_estimators, max_depth, min_samples_leaf, max_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import GridSearchCV and metric MSE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\n\n#define the grid of hyperparameters \nparams_rf = { 'n_estimators': [300, 400, 500],\n              'max_depth': [4,6,8],\n              'min_samples_leaf': [0.1, 0.2],\n              'max_features': ['log2', 'sqrt']}\n\n#Instantiate a 3-fold CV grid search object \ngrid_rf = GridSearchCV(estimator=rf, \n                       param_grid=params_rf,\n                       scoring='neg_mean_squared_error', #negative mse\n                       cv=3,\n                       verbose=1, #verbose: gereksiz sözlerle dolu demek, verbosity'yi kontrol etmek içinmiş\n                       n_jobs=-1)\n\n#fit grid_dt to the training set\ngrid_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract best hyperparameters\nbest_hyperparams = grid_rf.best_params_\nprint('best parameters: \\n',best_hyperparams)\n\n#extract best model \nbest_model = grid_rf.best_estimator_\nprint('\\nbest model: \\n',best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict the test set labels\ny_pred = best_model.predict(X_test)\n\n#Evaluate the test set RSME\nrsme_test = MSE(y_test, y_pred)**(1/2)\nprint('RSME of test set:{:.2f}'.format(rsme_test) )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}