{"cells":[{"metadata":{},"cell_type":"markdown","source":"****CONTENT****\n\n[SUPERVISED LEARNING](#1)\n*  [CLASSIFICATIONS](#2) \n    * [K-Nearest Neighbors (KNN)](#3)\n    * [Logistic Regression](#4)\n    * [ROC Curve with Log Reg](#5)\n    * [AUC (Area Under the ROC Curve)](#6)\n    * [AUC using Cross Validation](#7)\n    * [Confusion Matrix](#8)\n*  [REGRESSIONS](#9)  \n    * [Lınear Regression](#10)\n    * [Cross Validation](#11)\n    * [Regularized Regression](#12)\n        * [Ridge Regression](#13)\n        * [Lasso Regression](#14)  \n* [Hyperparameter Tuning](#15)\n    * [Grid Search CV](#16)\n    * [Randomized Search CV](#17)\n* [Preprocessing Data](#18)\n    * [Dummy Variables](#19)\n    * [Missing Values](#20)\n        * [Imputer](#21)\n        * [Pipeline](#22)  \n    * [Normalizing (Scaling, or Centering)](#23)\n    \n        \n        \n    "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Supervised Learning**: uses labeled data\n\n**Unsupervised Learning**: uses unlabeled data. uncovering hidden patterns and structures from unlabeled data.\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n**SUPERVISED LEARNING**\n\nSupervised learning'de, predictor variables (features)(independent variable) ve target variable (dependent variable) (response variable) vardır. \n\nAmaç, target variable'ı predictor variable'ları kullanarak tahmin edecek modeli oluşturmaktır."},{"metadata":{},"cell_type":"markdown","source":"**Not:**\n\nEğer **target** variable evet-hayır, spam-notspam gibi **kategori** biçiminde ise **CLASSIFICATION**,\n\nEğer **target** variable **continuously varying variable** ise, örneğin maaş, ev fiyatı gibi, **REGRESSION** uygulanır."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n**CLASSIFICATIONS**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n**K-Nearest Neighbors (KNN)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nIrıs data seti yükledik:\n\"\"\"\niris = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nsepal_length, sepal_width, petal_length, petal_width are features (predictor variables)\n\nspecies (setosa, versicolor, virginica) are target variables\n\n\"\"\"\n\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nbunu belirtmek için target variable = y, predictor variables = X olarak ayırmamız gerekiyor.\n\n\"\"\"\n\nX = iris.drop('species', axis=1).values  \ny = iris['species'].values\n\n\"\"\"\n.values diyerek modelin istediği formata (array) soktuk. \n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nBasic Idea of KNN: k-closest data point'e bakarak unlabeled data point'in label'ını tahmin etme\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model importing\n\nfrom sklearn.neighbors import KNeighborsClassifier #classifier ı import ettik.\n\n#calling classifier with a variable\n\nknn = KNeighborsClassifier(n_neighbors=6) #classifier'ı knn variable'ına atadık. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train-test-split\n\n#modele yerleştirilen datanın bir kısmı ile train yapılırken diğer bir kısmı ile de bu train\n#test edilir. yani, örneğin, datanın %70'i ile optimum weight'ler belirlenir, kalan %30luk kısım ise bu modelin\n#doğruluğu test edilir.\n\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"\nX_train: train data\nX_test: test data\ny_train: training labels\ny_test: test labels\n\nX:feature data, \ny:target, \ntest_size:train-test oranı, \nrandom_state:aynı sonucu verir denediğimiz randomstate Id'si.\n\"\"\"\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21) # X ve y'yi kullanarak train-test yaptık.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#.fit() \n\nknn.fit(X_train, y_train) #train data ile fit(yani train) yaptık. bu şu demek, datanın %70lik kısmının feature ve target(elimizde bilgi mevcut)\n#valuelarını kullanarak modeli oluşturduk. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#.predict()\n\ny_pred = knn.predict(X_test) #X_test (yani kalan %30luk feature datası) nı kullanarak bu feature dataya karşılık gelen prediction'ları oluşturduk\n#buna da y_pred dedik. elimizde şu an y_pred ve y_test(gerçek değerler) mevcut. yani artık modelin performansını ölçebiliriz.b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#.score()\n\nknn.score(X_test, y_test) #score() ile modelin performansını ölçeriz. X_test'i verip, ondan elde edilen tahmin ile, y_test(gerçek değer)i kıyaslar.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nOverfitting - Underfitting: finding optimum k-value\n\nburada, manual olarak belirlenmesi gereken n_neighbors(k) parametresi mevcut. (hyperparameter)\nk değeri küçük olursa overfitting yapmış olabiliriz.\nbüyüdükçe ise underfiting yapmış olabiliriz. \noptimum k değerini bulmamız gerekir!\n\n\"\"\"\n\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 20)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n\n#grafiğe göre, testing acuracy 2-3-4-5 aynı sonucu veriyor, sonra düşüş var. n=5 ideal seçim diyebiliriz.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nİsmi regression olabilir, ama classification'da kullanılır.\n\nMekanizmayı özetlemek gerekirse, \nLog Reg bize bir p,probability verir. p>0.5 ise target=1, p<0.5 ise target=0 diye sonuçlanır.\nlog reg bize linear decision boundary sağlar. \n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n**ROC Curve with Log Reg**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nyukarıda bahsettiğimiz p-value default 0.5, ancak bu değeri değiştirirsek ne olur:\nmesela, p=0.0 olursa, herşeyi 1 tahmin eder, p=1.0 olursa da herşeyi 0 tahmin edecektir.\nbu p-value'ya threshold deriz, bu threshold'un değişiminin etkisini incelediğimiz curve'e ise\nROC (Receiver Operating Characteristic) Curve deriz.\nAncak, ROC curve binary classification için geçerlidir. bu yüzden heart disease dataseti kullanacağız:\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('target', axis=1).values  \ny = df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob = logreg.predict_proba(X_test)[:,1] #predict_proba 2 column döner, 1.si index olduğu için 2.columnı seçtik.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) #fpr:false pos.rate, tpr: true positive rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grafiği çizelim:\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label= 'Log Reg')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('LogReg ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n**AUC (Area Under the ROC Curve)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" Another metric for classification models.\n\nROC Curve'un altındaki alan (Area Under the ROC Curve - AUC) ne kadar fazla ise model o kadar iyi.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n**AUC using Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc') #scoring ile metodu belirledik.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cv_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n**Confusion Matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nAccuracy scores her zaman bize doğru bilgi vermez. çünkü;\nörneğin, spam - not spam örneğinde, %99 real email, %1 spam iken hepsine real derse %99 doğru sonucu vermiş olur,\naccuracy %99 olur. ama, bu bize modelin doğruluğu ile ilgili bilgiden ziyade, datanın içeriğinden kaynaklı bilgi vermiş olur.\nyani, data imbalanced'tır. \n\nConfusion Matrix:\n    Classification problemlerinde kullanılır. CM nedir:\n    eğer email spam'se ve spam olarak tahmin edildiyse True Positive\n    eğer email spam'se ve real olarak tahmin edildiyse False Negative\n    eğer email real'se ve real olarak tahmin edildiyse True Negative\n    eğer email real'se ve spam olarak tahmin edildiyse False Positive'dir.\n    \n    burada spam aradığımız için spam'e positive demiş olduk. bu bize bağlı.\n    \n    accuracy = (tp+tn) / (tp+tn+fp+fn)\n    precision = tp / (tp+fp)\n    recall (sensitivity) = tp / (tp+fn)\n    f1-score = 2 * precision * recall / (precision + recall)\n    \n    high preision: not many real emails predicted as spam\n    high recall: predicted most spam emails correctly.\n    \n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nyukarıda açıkladık zaten CM'i.\n\"\"\"\n#model import:\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#yukarıdaki KNN modelini kullanarak Confusion Matrix'i örneklendirelim:\nX = iris.drop('species', axis=1).values  \ny = iris['species'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=8) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred)) #-->>> ilk arguman true labellardan, ikinci argüman da predictionlardan oluştu.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n**REGRESSIONS**"},{"metadata":{},"cell_type":"markdown","source":"Not:\n\nEğer **target** variable **continuously varying variable** ise, örneğin maaş, ev fiyatı gibi, **REGRESSION** uygulanır."},{"metadata":{"trusted":true},"cell_type":"code","source":"boston = pd.read_csv('../input/bostoncsv/Boston.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.drop('Unnamed: 0', axis=1, inplace=True)\nboston.head()\n\n#target value medv'dir, diğerleri feature(predictor variable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X ve y yi belirleyelim:\n\nX = boston.drop('medv', axis=1).values \ny = boston['medv'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n**Lınear Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regression, y=a+bx. more dimensional olunca y = a+bx1+cx2... \n#amaç, bu line öyle bir line olsun ki, residuals'ı(error unction)(loss or cost function) \n#(data point'lerin line'dan uzaklıkları(nın kareleri)) toplamı minimum olsun.\n#y target, x feature, a ve b de bizim öğrenmek istediğimiz parametreler.\n#elde ettiğimiz, residualsın karelerini minimize eden functiona da OLS (ORDİNARY Least Squares) denir.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sadece 1 column'ı alarak başlayalım; (one dimensional linear regression)\nX_rooms = X[:,5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape'lere bakalım:\n\nX_rooms.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape'leri istenen formata getirelim:\n\ny = y.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#istenen shape:\n\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_rooms = X_rooms.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_rooms.shape\n\n#tamamdır!..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bir grafikle iki variable'ı görelim:\n\nplt.scatter(X_rooms, y)\nplt.ylabel('value of house')\nplt.xlabel('nr of rooms')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#artık regression'a geldik:\n\nfrom sklearn import linear_model #linear model'ı import ettik.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = linear_model.LinearRegression() #modeli reg variable'ına atadık.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(X_rooms, y) #X_rooms ve y'ye göre fit ettik.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting line'ı grafik olarak göstermek için,\nprediction_space = np.linspace(min(X_rooms), max(X_rooms)).reshape(-1,1)\nplt.scatter(X_rooms, y)\nplt.plot(prediction_space, reg.predict(prediction_space), color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ŞİMDİ, tüm datayı kullanarak more dimensional linear regression yapalım:\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_all = linear_model.LinearRegression()\n\nreg_all.fit(X_train, y_train)\n\ny_pred = reg_all.predict(X_test)\n\nreg_all.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n**Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nModel performansı datanın nasıl split edildiğiyle alakalı olabilir.\nBu yüzden, bulduğumuz model performans score aldatıcı olabilir.\nBu sorunu çözmek için cross validation metodu kullanılır.\n\nİşleyişi şu şekildedir:\ntest_size=0.2 ise, CV ile, farklı 0.2'lik gruplara ayırarak modeli test eder.\nönce ilk 0.2'yi test olarak alır, sonra 2. 0.2lik kısmı...\nbu şekilde 5 farklı train-test grubuna göre performans belirlenir.\nbu, 5-fold CV olarak addedilir. \n\nk-fold CV 'deki k değerinin optimumunu bulmamız gerekir. (k büyük olursa işlem süresi artar, \nk küçük olursa da geçerliliğini ölçemeyiz)\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import CV:\nfrom sklearn.model_selection import cross_val_score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#üstteki dataya göre 5-fold cv:\ncv_results = cross_val_score(reg_all, X, y, cv=5) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sonuçlar:\ncv_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5 farklı sonucun ortalaması:\nnp.mean(cv_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n**Regularized Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nLinear Regression ile loss function'ı minimize etmektir fitting line'ın amacı.\ny=a1x1+a2x2+... 'daki a1,a2,ai lerin seçimi yapılır buna göre. \nburadaki coefficient'ların fazla büyük seçimi overfitting getirir.\nmulti dimensional linear regression'ı düşünürsek bu durum sağlıklı sonuç almayı engeller.\nbunu aşmak için large coefficientları penalize eden bir yapı ile loss function kontrol edilebilir.\nİşte buna regularization deniyor.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nRegularized Regression Types:\n\n1. Ridge Regression \n\n(Loss function = OLS loss function + alfa* sum of squared values of each coefficient) \n\n(bu şekilde, Loss function coefficientlerin karelerinin toplamlarının alfa ile çarpımı kadar artmıştır. \namaç ikinci kısmın da min olmasını sağlamaktır.)\n\n(alpha, hyperparameter'dır.)\n\n2. Lasso Regression\n\n(Loss function = OLS loss function + alfa* sum of absolute values of each coefficient) !!!Tek fark bu!!!\n\n!! Lasso regression, important feature seçimi yapılmakta kullanılır. Çünkü, küçük coefficientleri 0'a küçültme eğilimindedir.\n0'a yuvarlanmamış feature'lar important olarak tanımlanabilir.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n**Ridge Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing\nfrom sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha=0.1, normalize=True) #modeli oluşturduk. normalize=True tüm variable'lar aynı scale'da olması için.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.fit(X_train, y_train) #train ettik","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_pred= ridge.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n**Lasso Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha=0.1, normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_pred = lasso.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#yukarıda bahsettiğimiz, önemsiz feature'ları 0 yapıp önemli olanları belirleme işlemi:\nlasso_coef = lasso.fit(X, y).coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coef","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n**Hyperparameter Tuning**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n**Grid Search CV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nModeldeki hyperparameterları belirterek grid search yaparak hyperparameterların optimum değerini bulmamızı sağlar.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = iris.drop('species', axis=1).values  \ny = iris['species'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_neighbors': np.arange(1,50)} #dict içinde '' içinde grid edilecek parametreyi yazıp, karşılığında da hangi değerlerin \n#deneneceğini yazdık.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cv = GridSearchCV(knn, param_grid, cv=5) #this return the grid search object. fit etmeliyiz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cv.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cv.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a> <br>\n**Randomized Search CV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nGridSearchCV'den farkı, çoklu hyperparameter belirlemede calculation yoğunluğu olduğu zaman kullanılmasıdır.\nTüm değerler denenmez, bir prob.dist kullanılarak hesaplama yapılır.\n\nBu uygulamayı yapmak için RandomForest kullanalım:\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit it to the data\ntree_cv.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n**Preprocessing Data**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"19\"></a> <br>\n**Dummy Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ncategorical data type için, \n\n\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_species = pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_species.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n3'ünden 2si değilse diğeridir. bu yüzden kullandığımız bir parametre var.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_species2 = pd.get_dummies(df, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_species2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"20\"></a> <br>\n**Missing Values**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"21\"></a> <br>\n**Imputer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nbazen, missing value olarak görülmez, 0, '', vs vs olur.\nbu durumda, hem nan value^ları smartly doldurmak için Imputer object kullanabiliriz.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/diabetescsv/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head() #mesela burada, insulin = 0, bu not possible","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Insulin.replace(0, np.nan, inplace=True) #TÜM 0 ları nan value yaptık.\ndf.SkinThickness.replace(0, np.nan, inplace=True)\ndf.BMI.replace(0, np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = df.dropna() # missing value olan tüm row'ları drop ettik.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.shape #ancak, datanın yarısını kaybettik. unacceptible","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n#nan value'ların yerine başka şeyler koyabiliriz stratejiye bağlı olarak:\nfrom sklearn.preprocessing import Imputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = Imputer(missing_values='NaN', strategy='mean', axis=0) #missing_values = 'Nan' olarak gösterimiş demek. axis=0 ile de sadece o column'a baktık.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Outcome', axis=1).values  \ny = df['Outcome'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = imp.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"22\"></a> <br>\n**Pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nImputer'ın yaptığı işi yapar, üstüne bir de model çalıştırır.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\nlogreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps = [('imputation', imp), \n         ('logistic_regression', logreg)] #imputer modeli ve uygulanacak reg modelini steps e yazdık","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"23\"></a> <br>\n**Normalizing (Scaling, or Centering)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nFarklı feature'lar için farklı range'de değerlerin olması, ML modellerinin çalışmasını etkileyecektir.\nEğer scaling(normalizing) yapmazsak bazı feature'lar modelde daha ağırlıklı değerlendirilecek, \nsonuçlar yanıltıcı olacaktır.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X =iris.drop('species', axis=1).values  \ny = iris['species'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled = scale(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(X), np.std(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(X_scaled), np.std(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" burada da, pipeline ile hem imputation, hem scaling, hem model çalışması,\n    ardından da gridsearchcv ile hyperparameter optimization bir arada.\n    \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\n# Setup the pipeline steps: steps\nsteps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n         ('scaler', StandardScaler()),\n         ('elasticnet', ElasticNet())]\n\n# Create the pipeline: pipeline \npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(pipeline, parameters)\n\n# Fit to the training set\ngm_cv.fit(X_train, y_train)\n\n# Compute and print the metrics\nprint(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}