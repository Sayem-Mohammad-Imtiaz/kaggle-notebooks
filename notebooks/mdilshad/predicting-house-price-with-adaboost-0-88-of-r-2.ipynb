{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pridicting House Price "},{"metadata":{},"cell_type":"markdown","source":"## Overview"},{"metadata":{},"cell_type":"markdown","source":" Welcome to my Kernel! In this kernel, I use various regression methods and try to predict the house prices by using them. As you can guess, there are various methods to suceed this and each method has pros and cons. I think **regression is one of the most important methods when used with Adaboost** because it gives us more insight about the data. When we ask why, it is easier to interpret the relation between the response and explanatory variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing tool and libraries"},{"metadata":{},"cell_type":"markdown","source":"Importing libraries and data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport sklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, Normalizer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Forcing pandas to display any number of elements\ndef set_pandas_options() -> None:\n    pd.options.display.max_columns = 1000\n    pd.options.display.max_rows = 1000\n    pd.options.display.max_colwidth = 199\n    pd.options.display.width = None\n    pd.options.display.precision = 8\n    pd.options.display.float_format = '{:,.3f}'.format\nset_pandas_options()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing data\ndf = pd.read_csv('/kaggle/input/housesalesprediction/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First Look of data"},{"metadata":{},"cell_type":"markdown","source":"let see how data is look."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is pretty clean there is no null value in data-set"},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"### Price"},{"metadata":{},"cell_type":"markdown","source":"Lets explore distribution of Target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribtion of price variable\nplt.figure(figsize=(15,5))\nsns.distplot(df['price'], bins=20, kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribtion is Unimodel right skewed and centered at around 500000.0 and seem there is outlier in right side of distribtion. So need some kind of transformation to make it normal."},{"metadata":{},"cell_type":"markdown","source":"### Categorial Variable"},{"metadata":{},"cell_type":"markdown","source":"Now we explore relation between target variable(price) and categorial variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# by looking the values count of these variable we see that these are categorial varable. \ncat_features = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cat_features:\n    plt.figure(figsize=(15,8))\n    sns.boxplot(y='price', x=feature, data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I found that **bathrooms**, **waterfront**, and **grade** are very good correlation with price. I also found that two of bedrooms *11* and *33* are very unusual that we might removed these point.I can see that bathrooms is floating number I can make these integer to reduce complexity.There are some outlier that furture away I also need to further look at these data point to confirm these are acutual outlier."},{"metadata":{},"cell_type":"markdown","source":"### Numerical Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feature = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15',\n              'yr_built', 'yr_renovated', 'lat', 'long', 'price']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(df[num_feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is very good correlation of **sqft_living**, **sqft_above**, **sqft_basement**,**sqft_living15**, **lat** these feature and **price**. There are some data point that stand way out from rest of data. I need to further look at these point to declare as outlier.\nI can represent as **yr_renovated** as category variable to reduce complexity."},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing and Removing Outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# making target variable first column\ndef target_to_start(df, target):\n    feature = list(df)\n    feature.insert(0, feature.pop(feature.index(target)))\n    df = df.loc[:, feature]\n    return df\n    \ndf = target_to_start(df, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing id column because it not relevant.\ndf.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# coverting columns into integer from float.\ndf.price = df.price.astype(int)\ndf.bathrooms = df.bathrooms.astype(int)\ndf.floors = df.floors.astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlier"},{"metadata":{},"cell_type":"markdown","source":"By exploring further i find some potential outlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"#I remove bedrooms above 11 because price is not as high as no.of bedroom e.g with 33 bedrooms price is less than 9 bed rooms\ndf = df[df['bedrooms']<11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(df['bathrooms'] !=4) & (df['price'] != 7062500)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(df['sqft_living']<13000) & (df['price']!=2280000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# i think yr_built is not so useful feature so i convert this into age_of_house.\ndf['age_of_house'] = df['date'].apply(lambda x: int(x[:4])) - df['yr_built']\ndf.drop('yr_built', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping data columns because we do not need anymore.\ndf.drop('date', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert yr_renovated into categorical variable.\ndf['renovated'] = df['yr_renovated'].apply(lambda x: 0 if x == 0 else 1)\ndf.drop('yr_renovated', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing log transformation of numrical variable to get normal distribation.\ndf['price_log'] = np.log(df['price'])\ndf['sqft_living_log'] = np.log(df['sqft_living'])\ndf['sqft_lot_log'] = np.log(df['sqft_lot'])\ndf['sqft_above_log'] = np.log(df['sqft_above'])\ndf['sqft_living15_log'] = np.log(df['sqft_living15'])\ndf['sqft_lot15_log'] = np.log(df['sqft_lot15'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(df['price_log'], bins=20, kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly all other feature get normalize."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle('clean_dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_pickle('clean_dataset')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All features\nfeature1 = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors','waterfront', 'view', 'condition',\n            'grade', 'sqft_above', 'sqft_basement', 'zipcode', 'lat', 'long', 'sqft_living15','sqft_lot15',\n            'age_of_house', 'renovated']\n\n# features that correlation is greater than \"0.2\"\nfeature2 = ['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'view',\n            'grade', 'sqft_above', 'sqft_basement', 'lat', 'sqft_living15']\n\n# numerical features with log_transform\nfeature3 = ['bedrooms', 'bathrooms', 'sqft_living_log', 'sqft_lot_log', 'floors','waterfront', 'view', 'condition',\n            'grade', 'sqft_above', 'sqft_basement', 'zipcode', 'lat', 'long', 'sqft_living15_log','sqft_lot15_log',\n            'age_of_house', 'renovated']\n\n# numerical features with log_transform where correlation is greater that \"0.2\"\nfeature4 = ['bedrooms', 'bathrooms', 'sqft_living_log', 'floors', 'view',\n            'grade', 'sqft_above', 'sqft_basement', 'lat', 'sqft_living15_log']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_of_each_feature(dataset, features):\n    # get correlations of each features in dataset\n    features.append('price_log')\n    corrmat = dataset[features].corr()\n    top_corr_features = corrmat.index\n    plt.figure(figsize=(20,20))\n    #plot heat map\n    sns.heatmap(dataset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n\ncorrelation_of_each_feature(df, feature1.copy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MAchine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Matrix \nevaluation_df = pd.DataFrame(columns=['Name of Model','Feature Set', 'Target', 'R^2 of Training', 'R^2 of Testing', 'Mean Squaued Error Training',\n                                      'Mean Squaued Error Testing'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to split data into training and testing set\ndef feature_target(features, target):\n    X = df[features]\n    y = df[target]\n    feature_train, feature_test, label_train, label_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    return feature_train, feature_test, label_train, label_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_xyz(model, feature_train, feature_test, label_train, label_test, model_name='Linear Regression', feature_set=1,\n              target='price'):\n    model.fit(feature_train, label_train)  \n    y_pred_train = model.predict(feature_train)\n    y_pred_test = model.predict(feature_test)\n    r2_train = r2_score(label_train, y_pred_train)\n    r2_test = r2_score(label_test, y_pred_test)\n    rmse_train = mean_squared_error(label_train, y_pred_train)\n    rmse_test = mean_squared_error(label_test, y_pred_test)\n    \n    r = evaluation_df.shape[0]\n    evaluation_df.loc[r] = [model_name, feature_set, target, r2_train, r2_test, rmse_train, rmse_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature1, 'price')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 1,\n          target='price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature2, 'price')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 2,\n          target='price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 3,\n          target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature4, 'price_log')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 4,\n          target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I seen that model is not overfitting so no need of Regulariztion. Upto now transformed feature are perform well then other."},{"metadata":{},"cell_type":"markdown","source":"Now I try polynomial features."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature1, 'price')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2',\n          feature_set= 1, target='price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature2, 'price')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2', \n          feature_set= 2, target='price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2', \n          feature_set= 3, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature4, 'price_log')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2', \n          feature_set= 4, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Woo thats great!!!!!!** "},{"metadata":{},"cell_type":"markdown","source":"Linear regression with polynomial degree 2 with feature set **3** is given great result training **r^2 = 0.83** and testing **r^2 = 0.82**. That good but i try to improve it. Lets go with further degree of polynomial degree. "},{"metadata":{},"cell_type":"markdown","source":"From now onward I only go with transformed features because its give higher r^2 then other features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 3',\n          feature_set= 3, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature4, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 3',\n          feature_set= 4, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df.iloc[6:,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With polynomial degree *3* Training R^2 is **0.88** higher then testing R^2 **0.79**.Infact it is less then ploynomial degree *2* testing r^2 **0.81**."},{"metadata":{},"cell_type":"markdown","source":"So it may be overfitting so we need to apply **Regularization**."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = Lasso(alpha=10)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Lasso Regression with degree 3',\n          feature_set= 3, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df.iloc[8:,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now with gridsearchcv I try to find best value for alpha."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = Lasso()\nsearch_grid={'alpha':[0.001,0.01,0.05,1,10,20]}\nsearch=GridSearchCV(estimator=lr, param_grid=search_grid, \n                    scoring='neg_mean_squared_error', n_jobs=1, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.fit(feature_train, label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = Lasso(alpha=20)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Lasso Regression with degree 3 with alpha=20',\n          feature_set= 3, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That Mean squared error of polynomial degree 3 **(0.056)** is greater then Mean squared error of polynomial degree 2 **(0.45)**. that not good way to go further. "},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree with AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr =  DecisionTreeRegressor()\nsearch_grid={'max_depth':[6,7,8,9,10,11,12,13,14,15]}\nsearch=GridSearchCV(estimator=lr, param_grid=search_grid, \n                    scoring='neg_mean_squared_error', n_jobs=1, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.fit(feature_train, label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr =  DecisionTreeRegressor(max_depth=9)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Decision Tree Regressor with alpha 9',\n          feature_set= 3, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its **MSE(0.051)** is little higher then MSE of linear regressinon with polynomial degree **2(0.47)**"},{"metadata":{},"cell_type":"markdown","source":"Now i apply Adaboost let see what happen!..."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nada = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9))\n\nsearch_grid={'n_estimators':[200,300,400,500],'learning_rate':[0.05, 0.1, 0.3, 1]}\n\nsearch=GridSearchCV(estimator=ada, param_grid=search_grid, \n                    scoring='neg_mean_squared_error', n_jobs=1, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.fit(feature_train, label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9), learning_rate=1, n_estimators=500)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Decision Tree Regressor with alpha 9',\n          feature_set= 3, target='price_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Boom** MSE get down to **0.032** from **0.047**."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.exp(0.032)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validatation"},{"metadata":{},"cell_type":"markdown","source":"Now we validate our model by cross-validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[feature3]\ny = df['price_log']\nlr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9), learning_rate=1, n_estimators=500)\nscores = cross_val_score(lr, X, y, scoring='neg_mean_squared_error', cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool MSE is **0.033**  so my final model is Adaboost with decisionTreeRegressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9), learning_rate=1, n_estimators=500)\nmodel = lr.fit(feature_train, label_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(feature_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df  = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df['actual'] = np.exp(label_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df['predetion'] = np.exp(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"When we look at the evaluation table, **2nd degree polynomial (all features, with price_log as target variable)** is doing good job forpredicting outcome. But **Adaboost with DecisionTree** is doing best.\nFuther improvements can also be made by using feature engineering.\nIf you like my notebook please do not forget to **Upvote**."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}