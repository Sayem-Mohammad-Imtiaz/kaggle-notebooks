{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Data visualization\nimport seaborn as sb \nfrom itertools import product\n%matplotlib inline\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we will import the data and load it to a dataframe\ndf = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see our top 5 rows\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary of our dataframe\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see that our dataset contains 215 rows with 15 features asssociated with it.","metadata":{}},{"cell_type":"code","source":"# Checking for Null values in our Dataset\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Salary contains 67 NaN values in the dataset. We will replace all those NaN values in the next step.","metadata":{}},{"cell_type":"code","source":"# Rows whose Salary value is not present\ndf[df['salary'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's replace those NaN value with 0\ndf['salary'] = df['salary'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking all the null values are removed\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Univariate Exploration:","metadata":{}},{"cell_type":"markdown","source":"#### Different Candidate count in Placement Drive:","metadata":{}},{"cell_type":"code","source":"# Let's check the different data of Gender column\ndf['gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploration of different Candidate\nplt.figure(figsize = (14.7, 8.27))\nviscolor = sb.color_palette('colorblind')[0]\nax = sb.countplot(data = df, x = 'gender', color = viscolor, order = df['gender'].value_counts().index)\nplt.xlabel('Gender')\nplt.ylabel('Count')\nfor i, v in df['gender'].value_counts().reset_index().iterrows():\n    ax.text(i, v.gender + 0.2 , v.gender, color='black')\nplt.title('Different Candidate in Placement Drive');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see here that in the record of the placement drive there is more number of Male candidates than that of female candidates.","metadata":{}},{"cell_type":"markdown","source":"#### Education Specialization:","metadata":{}},{"cell_type":"code","source":"# Value of count of different Specialization\ndf['hsc_s'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of higher secondary specialization\ncls_name = ['Commerce', 'Science', 'Arts']\nfig, ax = plt.subplots(figsize = (14.7, 8.27))\nwedges, text, autotext = ax.pie(df['hsc_s'].value_counts(), labels = cls_name, autopct = '%1.2f%%')\nax.legend(wedges, cls_name, loc = \"center left\", bbox_to_anchor =(1, 0, 0.5, 1))\nax.set_title(\"Proportion of Different Specialization in Higher Secondary\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that in placement drive more candidate are from Commerce and Science background. Less candidates are from Art's background means only 5.12%.","metadata":{}},{"cell_type":"code","source":"# Value count of Degree specialization\ndf['degree_t'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Value count of Postgrad specialization\ndf['specialisation'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of Degree Specialization\nfig, ax = plt.subplots(figsize = (14.7, 8.27))\nwedges, text, autotext = ax.pie(df['degree_t'].value_counts(),\n                                labels = df['degree_t'].value_counts().index, \n                                autopct = '%1.2f%%')\nax.legend(wedges, df['degree_t'].value_counts().index,\n          loc = \"center left\", bbox_to_anchor =(1, 0, 0.5, 1))\nax.set_title(\"Proportion of Different Specialization in Degree\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see that more Candidate have Comm&Mgmt degree followed by Sci&Tech.","metadata":{}},{"cell_type":"code","source":"# Visualization of Postgrad Specialization\nfig, ax = plt.subplots(figsize = (14.7, 8.27))\nwedges, text, autotext = ax.pie(df['specialisation'].value_counts(),\n                                labels = df['specialisation'].value_counts().index, \n                                autopct = '%1.2f%%')\nax.legend(wedges, df['specialisation'].value_counts().index,\n          loc = \"center left\", bbox_to_anchor =(1, 0, 0.5, 1))\nax.set_title(\"Proportion of Different Specialization in Post Graduation\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see that 55.81% belongs to Mkt&Fin postgraduation program. Other belongs to Mkt&HR background.","metadata":{}},{"cell_type":"markdown","source":"#### Creating Category of Mark Secured in Different Educational Phase:\n- Here we will create 3 category:\n    - 85% + \n    - 60% - 85%\n    - < 60% ","metadata":{}},{"cell_type":"code","source":"# Defining a function which will be used to determine the above category.\n# We will store that in a new category\ndef checkCateg(perct):\n    if(perct >= 85):\n        return '85% +'\n    elif(perct < 85 and perct >= 60):\n        return '60% - 85%'\n    else:\n        return '< 60%'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding new columns in our dataframe with this category\ndf['ssc_p_catg'] = df['ssc_p'].apply(checkCateg)\ndf['hsc_p_catg'] = df['hsc_p'].apply(checkCateg)\ndf['mba_p_catg'] = df['mba_p'].apply(checkCateg)\ndf['degree_p_catg'] = df['degree_p'].apply(checkCateg)\ndf['etest_p_catg'] = df['etest_p'].apply(checkCateg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's visualize what is proportion of different score in HSC and SSC\ncateg_cls = ['60% - 85%', '< 60%', '85% +']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize =(14.70, 8.27))\nwedges, text, autotext = ax1.pie(df['hsc_p_catg'].value_counts(),\n                                 labels = df['hsc_p_catg'].value_counts().index,\n                                 autopct = '%1.2f%%')\nax1.set_title(\"Score of Different Candidate in HSC\");\nplt.tight_layout()\nwedges, text, autotext = ax2.pie(df['ssc_p_catg'].value_counts(),\n                                 labels = df['ssc_p_catg'].value_counts().index,\n                                 autopct = '%1.2f%%')\nax2.set_title(\"Score of Different Candidate in SSC\");\nax2.legend(wedges, categ_cls,\n           loc = \"upper right\", bbox_to_anchor =(1, 0, 0.5, 1));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Insights:\n- We can see that in the placement drive more number of candidate belong to 60%-85% cateogry in HSC and SSC. Only 4 to 6% candidate belongs to 85% category in HSC and SSC.\n- We will further visualize the other score and gain some insight.","metadata":{}},{"cell_type":"code","source":"# Let's visualize what is proportion of different score in Degree and MBA\ncateg_cls = ['60% - 85%', '< 60%', '85% +']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize =(14.70, 8.27))\nwedges, text, autotext = ax1.pie(df['degree_p_catg'].value_counts(),\n                                 labels = df['degree_p_catg'].value_counts().index,\n                                 autopct = '%1.2f%%')\nax1.set_title(\"Score of Different Candidate in Degree\");\nplt.tight_layout()\nwedges, text, autotext = ax2.pie(df['mba_p_catg'].value_counts(),\n                                 labels = df['mba_p_catg'].value_counts().index,\n                                 autopct = '%1.2f%%')\nax2.set_title(\"Score of Different Candidate in MBA\");\nhandles, labels = ax1.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper right');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see the similar trend as previously. We can see more number of candidate belong to 60%-85% category.\n- In Degree only 0.93% candidate belong to \"85% +\" category. In this placement drive No candidate have \"85% +\" in MBA.","metadata":{}},{"cell_type":"code","source":"# Finally we will visualize Etest\nfig, ax = plt.subplots(figsize =(14.70, 8.27))\nwedges, text, autotext = ax.pie(df['etest_p_catg'].value_counts(),\n                                labels = df['etest_p_catg'].value_counts().index,\n                                autopct = '%1.2f%%')\nax.set_title(\"Score of Different Candidate ETest\")\nhandles, labels = ax.get_legend_handles_labels()\nfig.legend(handles, labels,loc = \"center\", bbox_to_anchor =(0, 0, 0.5, 1));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In ETest we can observe the same trend as above. But we can see 22.33% candidate belong to 85% + category.","metadata":{}},{"cell_type":"markdown","source":"#### Visualization of Work Experience of Candidates in placement drive:","metadata":{}},{"cell_type":"code","source":"# Visualize of Work Experience\nplt.figure(figsize = (14.7, 8.27))\nax = sb.countplot(data = df, x = 'workex', color = viscolor, order = df['workex'].value_counts().index)\nplt.xlabel('Work Experience')\nplt.ylabel('Count')\nfor i, v in df['workex'].value_counts().reset_index().iterrows():\n    ax.text(i, v.workex + 0.2 , v.workex, color='black')\nplt.title('Work Experience of Candidates in Placement Drive');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that more candidate has no work experience. ","metadata":{}},{"cell_type":"markdown","source":"#### Candidate Placement Status:","metadata":{}},{"cell_type":"code","source":"# Visualizing candidate placement status\nplt.figure(figsize = (14.7, 8.27))\nax = sb.countplot(data = df, x = 'status', color = viscolor,\n            order = df['status'].value_counts().index)\nplt.xlabel('Placement Status')\nplt.ylabel('Count')\nfor i, v in df['status'].value_counts().reset_index().iterrows():\n    ax.text(i, v.status + 0.2 , v.status, color='black')\nplt.title('Candidate Placement Status');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see that more of the candidate are placed.\n\nIn the next exploration we will see the relation between two variable and gain some insights.","metadata":{}},{"cell_type":"markdown","source":"### Bivariate Exploration:","metadata":{}},{"cell_type":"markdown","source":"#### Placment Status of Different Candidate:","metadata":{}},{"cell_type":"code","source":"# defining a function which will show the frequency of the bar in the countplot\ndef withhue(ax, feature, nooffeature, huecategories):\n    a = [p.get_height() for p in ax.patches]\n    patch = [p for p in ax.patches]\n    for i in range(nooffeature):\n        total = df[feature].value_counts().values[i]\n        for j in range(huecategories):\n            percentage = '{:.1f}%'.format(100 * a[(j*nooffeature + i)]/total)\n            x = patch[(j*nooffeature + i)].get_x() + patch[(j*nooffeature + i)].get_width() / 2 - 0.15\n            y = patch[(j*nooffeature + i)].get_y() + patch[(j*nooffeature + i)].get_height()\n            ax.annotate(percentage, (x, y), size = 12, ha='center', va='bottom')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of placement status of different candidate based on gender\nplt.figure(figsize = (14.7, 8.27))\nax = sb.countplot(data = df, x = 'gender', hue = 'status')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.title('Placement Status of Different Candidate');\nwithhue(ax, 'gender', 2, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see that more number of Male candidates are placed than that of the female candidates. Also, more number of Male candidates are not placed as compared to female candidate.","metadata":{}},{"cell_type":"markdown","source":"#### Work Experience of Different Candidate:","metadata":{}},{"cell_type":"code","source":"# Visualization of Work experience of different candidate\nplt.figure(figsize = (14.7, 8.27))\nax = sb.countplot(data = df, x = 'gender', hue = 'workex')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.title('Work Experience of Different Candidate');\nwithhue(ax, 'gender', 2, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Proportion of No workexperience is more for both male and female candidate.","metadata":{}},{"cell_type":"markdown","source":"#### MBA Specialisation of Different Candidate:","metadata":{}},{"cell_type":"code","source":"# Visualisation of MBA specialisation of different candidate:\nplt.figure(figsize = (14.7, 8.27))\nax = sb.countplot(data = df, x = 'gender', hue = 'specialisation')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.title('MBA Specialisation of different Candidate');\nwithhue(ax, 'gender', 2, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Male candidate have more number of Mkt&Fn specialisation where as Female candidates have more number of Mkt&HR specialisation.","metadata":{}},{"cell_type":"markdown","source":"#### Salary of Different Candidate with WorkExperience:","metadata":{}},{"cell_type":"code","source":"# Visualisation of Salary of Different Candidate with WorkExperience\nplt.figure(figsize = (14.7, 8.27))\nsb.violinplot(data = df, x = 'workex', y = 'salary', color = viscolor, inner = 'quartile')\nplt.xlabel('Work Experience')\nplt.ylabel('Salary')\nplt.title('Salary of Candidate by Work Experience');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Candidate with work experience is getting more salary than that of the candidate having no experience.","metadata":{}},{"cell_type":"markdown","source":"#### HSC Percentage of different HSC Board:","metadata":{}},{"cell_type":"code","source":"# Visualization of HSC percentage distribution of different HSC Board\nplt.figure(figsize = (14.7, 8.27))\nsb.boxplot(data = df, x = 'hsc_b', y = 'hsc_p', color = viscolor)\nplt.xlabel('HSC Board')\nplt.ylabel('HSC Percetage')\nplt.title('HSC Percentage of Different HSC Board');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Central board candidate has better median score than that of Other boards. But we can see that Other board candiate has higher score range than that of the central board.","metadata":{}},{"cell_type":"markdown","source":"### Multivariate Exploration:","metadata":{}},{"cell_type":"markdown","source":"#### Placement status of different candidate by MBA and Degree:","metadata":{}},{"cell_type":"code","source":"g = sb.catplot(kind = 'count', data = df, x = 'gender',\n               hue = 'specialisation', col = 'status', row = 'degree_t');\ng.set_axis_labels('Gender', 'Count');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Candidates having Comm&Mgmt degree and Mkt&Fin specialisation are placed in more number than that of Mkt&HR specialisation.","metadata":{}},{"cell_type":"markdown","source":"#### Placement Status of Different Degree holders with Specialisation:","metadata":{}},{"cell_type":"code","source":"# Visualisation of Placement status of different degree holders\ng = sb.catplot(kind = 'violin', data = df, x = 'degree_t', y = 'degree_p' , col = 'status',\n              inner = 'quartile', color = viscolor, row = 'specialisation');\ng.set_axis_labels('Degree', 'Percentage');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Having higher percentage gives more chance of placement.","metadata":{}},{"cell_type":"markdown","source":"#### Salary of Different Candidates with Degree and Specialisation:","metadata":{}},{"cell_type":"code","source":"g = sb.catplot(kind = 'swarm', data = df.query('status == \"Placed\"'), x = 'degree_t', y = 'salary',\n              color = viscolor, row = 'specialisation', col = 'workex');\ng.set_axis_labels('Degree', 'Salary');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see that salary of different degree holders having Mkt&Fin specialisation is getting more salary than that of the Mkt&HR.","metadata":{}},{"cell_type":"markdown","source":"#### Job offers by Educational Qualification:","metadata":{}},{"cell_type":"code","source":"g = sb.catplot(kind = 'count', data = df, x = 'hsc_s',\n               hue = 'status', col = 'degree_t', row = 'specialisation');\ng.set_axis_labels('Qualification', 'Count');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see every sort of degree holders has received jobs. But Those who have Commerce background and has a degree in Comm&Mgmt and MBA Specialisation in Mkt&Fn, Mkt&HR are more likely to be placed followed by Science background with Sci&Tech degree with Mkt&Fn, Mkt&HR specialisation.","metadata":{}},{"cell_type":"markdown","source":"#### Insights:\n- By analyzing the data we saw that more number of Commerece background candidate are placed followed by Science background candidates.\n- Mkt&Fn MBA specialization candidates are placed in more numbers followed by Mkt&HR specialisation.\n- Work experience is your bonus point. Candidate having work experience are more likely to get a good salary package.\n- Scoring good percentage/score throughout the career may help candidate secure a job.","metadata":{}},{"cell_type":"markdown","source":"### Classification of Placement Status & Prediction of Salary using Different Attribute:\nAs we have gained some insights about the attributes helping a candidate secure a job and a good package. So, first using these attribute we will classify if a candidate is going to secure a job or not; then we are going to follow Regression mechanism to predict the package that a candidate might get.","metadata":{}},{"cell_type":"markdown","source":"#### Label Encoding & Feature Scaling:\n- There are different features that needed to be scaled before used in a model. So do different categorical variables which need to be encoded.\n###### Reference:\n- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html","metadata":{}},{"cell_type":"code","source":"# Label encoding\n# Importing the library\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation','status']] = df[['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation','status']].apply(le.fit_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Encoded values\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Scaling\n# Importing Library\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf[['ssc_p','hsc_p','degree_p','etest_p','mba_p','salary']] = scaler.fit_transform(df[['ssc_p','hsc_p','degree_p','etest_p','mba_p','salary']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking Scaled features\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separating Features for classification\nX = df.iloc[:,1:13]\ny = df.iloc[:, 13]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Training and Test data for classification\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing Classifier Libraries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_l, precision_l, accuracy_l = [], [], []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a Function to get the accuracy of different classifiers\ndef get_accuracy(y_test, y_pred):\n    matrix = confusion_matrix(y_test,y_pred)\n    TP = matrix[1][1]\n    TN = matrix[0][0]\n    FP = matrix[0][1]\n    FN = matrix[1][0]\n    # calculate the Recall\n    recall = TP / (TP + FN)    \n    # calculate the Precision\n    precision = TP / (TP + FP)\n    recall_l.append(recall)\n    precision_l.append(precision)\n    accuracy_l.append(accuracy_score(y_test, y_pred))\n    return recall, precision, accuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree Classifier\ndf_clf = DecisionTreeClassifier(random_state = 0)\ndf_clf.fit(X_train, y_train)\ny_pred = df_clf.predict(X_test)\nrecall, precision, accuracy = get_accuracy(y_test, y_pred)\nprint('================================')\nprint('Classifier: Decision Tree')\nprint('Recall    : ', recall)\nprint('Precision : ', precision)\nprint('Accuracy  : ', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classifier\nrf_clf = RandomForestClassifier(n_estimators = 50, random_state = 0, criterion = 'entropy')\nrf_clf.fit(X_train, y_train)\ny_pred = rf_clf.predict(X_test)\nrecall, precision, accuracy = get_accuracy(y_test, y_pred)\nprint('================================')\nprint('Classifier: Random Forest')\nprint('Recall    : ', recall)\nprint('Precision : ', precision)\nprint('Accuracy  : ', accuracy) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nlr = LogisticRegression(solver = 'lbfgs')\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nrecall, precision, accuracy = get_accuracy(y_test, y_pred)\nprint('================================')\nprint('Classifier: Logistic Regression')\nprint('Recall    : ', recall)\nprint('Precision : ', precision)\nprint('Accuracy  : ', accuracy) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Recall, Precision, Accuracy of classifier\nx = ['DecisionTree', 'RandomForest', 'LogisticRegression']\nx_axis = np.arange(len(x))\nplt.figure(figsize = [14.70, 8.27])\nplt.bar(x_axis - 0.2, recall_l, 0.15, label = 'Recall')\nplt.bar(x_axis, precision_l, 0.15, label = 'Precision')\nplt.bar(x_axis + 0.2, accuracy_l, 0.15, label = 'Accuracy')\nplt.xticks(x_axis, x)\nplt.legend()\nplt.title('Recall, Precision, Accuracy of Classifiers');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing the data for our Regression model\nX = df.iloc[:,1:14]\ny = df.iloc[:, 14]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Training and Test data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing Library\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Root Mean Squared Error List for Regressor\nrmse = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Regression\nregressor_rf = RandomForestRegressor(n_estimators = 20, random_state = 0)\nregressor_rf.fit(X_train, y_train)\ny_test = regressor_rf.predict(X_test)\nprint('==============================')\nprint('Regression Model      : RandomForest')\nprint('RMSE                  : ', mean_squared_error(y_test, y_pred, squared = False))\nrmse.append(mean_squared_error(y_test, y_pred, squared = False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree Regressor\nregressor_dt = DecisionTreeRegressor(random_state = 0)\nregressor_dt.fit(X_train, y_train)\ny_test = regressor_dt.predict(X_test)\nprint('==============================')\nprint('Regression Model      : DecisionTree')\nprint('RMSE                  : ', mean_squared_error(y_test, y_pred, squared = False))\nrmse.append(mean_squared_error(y_test, y_pred, squared = False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Regression\nregressor_lr = LinearRegression()\nregressor_lr.fit(X_train, y_train)\ny_test = regressor_lr.predict(X_test)\nprint('==============================')\nprint('Regression Model      : Linear')\nprint('RMSE                  : ', mean_squared_error(y_test, y_pred, squared = False))\nrmse.append(mean_squared_error(y_test, y_pred, squared = False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting RMSE\nx = ['RandomForest', 'DecisionTree', 'LinearRegression']\nx_axis = np.arange(len(x))\nplt.figure(figsize = (14.7, 8.27))\nplt.bar(x_axis, rmse, 0.15, label = 'RMSE')\nplt.xticks(x_axis, x)\nplt.title('RMSE for Regressor');","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}