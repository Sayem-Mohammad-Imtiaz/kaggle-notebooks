{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nimport category_encoders as ce\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/indian-liver-patient-records/indian_liver_patient.csv')\npd.set_option('display.max_columns', 90)\npd.set_option('display.max_rows', 90)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===================================________Data Exploration________==================================================\n\n\ndef data_exploration(data):\n    \"\"\"\n    Understanding data to make better feature engineering\n    :param data: Data to be explored\n    :return: None\n    \"\"\"\n    # ============______Basic FAMILIARIZATION________==================\n    print('______________DATA HEAD__________ \\n', data.head())\n    print('______________DATA DESCRIBE______ \\n', data.describe())\n    print('______________DATA INFO__________ \\n', data.info())\n\n    # ===========_______DATA FREQUENT TERM___________===================\n    print('_____________Total unique values in data_______ \\n', data.nunique())\n    print('___________________ DATA UNIQUE VALUES_____________ \\n')\n    print('\\n', [pd.value_counts(data[cols]) for cols in data.columns], '\\n')\n\n    # ===========_______DATA CORRELATION_____________====================\n    corr_mat_graph(data, 'EDA MATRIX')\n\n    # =================____________DISTRIBUTION VISUALIZATION_________=================\n    dist_plot(data)\n\n    # ======================___________ Outliers__________________======================\n    box_plot(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# ================================___________GRAPHS FUNCTIONS____________==============================================\n\n\ndef corr_mat_graph(data, title):\n    \"\"\"\n    function to plot correlation matrix for better understanding of data\n    :param data: correlation matrix\n    :param title: Title of the graph\n    :return: None\n    \"\"\"\n    print('\\n \\n ____________________CORRELATION MATRIX_______________ \\n \\n')\n    corr_matrix = data.corr()\n    corr_matrix_salePrice = corr_matrix['Dataset'].sort_values(ascending=False)\n    print('________CORRELATION MATRIX BY DATA SET________ \\n', corr_matrix_salePrice)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(corr_matrix, square=False, linewidths=0.5, ax=ax, vmax=0.8, vmin=0.42, annot=True)\n    ax.title.set_text(title)\n\n\ndef dist_plot(data):\n    \"\"\"\n    Function to plot subplots of distribution for numerical data\n    :param data: data which needs to be plotted\n    :return: None\n    \"\"\"\n    print('\\n \\n ________________________DISTRIBUTION PLOT___________________ \\n \\n')\n    # Plotting numerical graph\n    data = data.select_dtypes(exclude='object')\n    data_filed = data.dropna(axis=0)\n\n    for cols in data.columns:\n        fig, ax = plt.subplots()\n        sns.distplot(data_filed[cols])\n        ax.title.set_text(cols)\n\n\ndef box_plot(data):\n    \"\"\"\n    To find oultliers in the data\n    :param data: data to be plot\n    :return:\n    \"\"\"\n    print('\\n \\n ________________________BOX PLOT___________________ \\n \\n')\n    data = data.select_dtypes(exclude='object')\n    for cols in data.columns:\n        fig, ax = plt.subplots()\n        sns.boxplot(data[cols])\n        ax.title.set_text(cols)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================_____________________FEATURE ENGINEERING_________________===============================\n\n\ndef feature_engineering(data):\n    \"\"\"\n    To clean and add features in dataset\n    :param data: Dataset to be cleaned\n    :return: cleaned dataset\n    \"\"\"\n    print('\\n \\n ________________FEATURE ENGINEERING_________________ \\n \\n')\n    # =====================__________________OUTLIERS________________==========================\n    # We need to deal with outliers\n    # To many outliers in Total_Bilirubin should drop whole column after calculating indirect Bilirubin\n    # Direct_Bilirubin have many outliers after outliers 85, eliminating such outliers\n    data = data.drop(data[data.Direct_Bilirubin > data.Direct_Bilirubin.quantile(0.85)].index)\n\n    # Alkaline Phosphate have many outliers after outliers 85, eliminating such outliers\n    data = data.drop(data[data.Alkaline_Phosphotase > data.Alkaline_Phosphotase.quantile(0.82)].index)\n\n    # Alamine Aminotransferase has heavy outliers after 93% quantile, eliminating such outliers\n    data = data.drop(data[data.Alamine_Aminotransferase > data.Alamine_Aminotransferase.quantile(0.93)].index)\n\n    # Alamine Aminotransferase has heavy outliers after 93% quantile, eliminating such outliers\n    data = data.drop(data[data.Aspartate_Aminotransferase > data.Aspartate_Aminotransferase.quantile(0.92)].index)\n\n    # All the major outliers are taken care of but Total and Direct Bilirubin is still heavily right skewed.\n    # Further removal of data will decrease data size\n\n    # =============================____________________IMPUTING MISSING VALUES_________________=================\n    # Since all features are numerical except Gender we need to drop rows where Gender.\n    # Fill NA of numerical data with median as dataset have way too much outliers\n    data['Gender'].dropna(axis=0, inplace=True)\n    data.fillna(data.median(), inplace=True)\n\n    # ===========================_____________________ADDING NEW FEATURES_______________________================\n    # Indirect Bilirubin is calculated not tested\n    data['Indirect_Bilirubin'] = data['Total_Bilirubin'] - data['Direct_Bilirubin']\n\n    # Normal and high Bilirubin level in Total Bilirubin can be grouped together\n    data['TotalBilirubinGroup'] = data['Total_Bilirubin'].apply(lambda x: 'Normal' if x <= 1.2 else 'High')\n\n    # Normal and high Bilirubin level in Direct Bilirubin can be grouped together\n    data['DirectBilirubinGroup'] = data['Direct_Bilirubin'].apply(lambda x: 'Normal' if x <= 0.3 else 'High')\n\n    # Low, normal and high Bilirubin level in Indirect Bilirubin can be grouped together\n    data['IndirectBilirubinGroup'] = data['Indirect_Bilirubin'].apply(lambda x: 'Low' if x < 0.3\n    else ('Normal' if 0.3 <= x <= 1.0 else 'High'))\n\n    # Alkaline phosphotase levels in high and low bins\n    data['Alkaline_PhosphotaseGroup'] = data['Alkaline_Phosphotase'].apply(\n        lambda x: 'Low' if x < 20.0 else ('Normal' if 20.0 <= x <= 140.0 else 'High'))\n\n    # Alamine Aminotransferase levels in high and low bins\n    data['Alamine_AminotransferaseGroup'] = data['Alamine_Aminotransferase'].apply(lambda x: 'Low' if x < 20.0\n    else ('Normal' if 20.0 <= x <= 60.0 else 'High'))\n\n    # Aspartate Aminotransferase (Male) levels\n    data.loc[(data['Gender'] == 'Male'), 'AspartateLevel'] = data['Aspartate_Aminotransferase'].apply(\n        lambda x: 'Low' if x < 6\n        else ('Normal' if 6 <= x <= 34 else 'High'))\n    # Aspartate Aminotransferase (FEMALE)\n    data.loc[(data['Gender'] == 'Female'), 'AspartateLevel'] = data['Aspartate_Aminotransferase'].apply(\n        lambda x: 'Low' if x < 8 else ('Normal' if 8 <= x <= 40 else 'High'))\n\n    # Total protein levels\n    data['Total_Protiens_Level'] = data['Total_Protiens'].apply(lambda x: 'Low' if x < 6.0\n    else ('Normal' if 6.0 <= x <= 8.3 else 'High'))\n\n    # Albumin levels\n    data['Albumin_Level'] = data['Albumin'].apply(lambda x: 'Low' if x < 3.4\n    else ('Normal' if 3.4 <= x <= 5.4 else 'High'))\n\n    # ===================___________________REDUCING SKEWNESS BY LOG____________====================\n    numeric_cols = data.select_dtypes(exclude='object').columns\n    for cols in numeric_cols:\n        if cols not in ['Dataset']:\n            data[cols] = np.log1p(data[cols])\n\n    # ==================___________________VISUALIZING TRANSFORMED DATA____________==================\n    dist_plot(data)\n    corr_mat_graph(data, 'Feature Engineering')\n    return data\n\n\n# Calling data exploration and feature\ndata_exploration(df_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# ===================================_________________SPLITTING DATA______________=======================\nprint('___________________SPLITTING DATA________________')\n\nx_train, x_test = train_test_split(df_train, random_state=42, test_size=0.25)\n\nx_train = feature_engineering(x_train)\nx_test = feature_engineering(x_test)\ny_train = x_train['Dataset']\nx_train.drop('Dataset', axis=1, inplace=True)\ny_test = x_test['Dataset']\nx_test.drop('Dataset', axis=1, inplace=True)\n\nprint('train data size', x_train.shape, y_train.shape)\nprint('Test data size', x_test.shape, y_test.shape)\n# =========================__________________SCALING DATA____________====================\nsc = StandardScaler()\nenc = ce.OrdinalEncoder()\npipe = Pipeline(steps=[('enc', enc), ('sc', sc)])\nX_train = pipe.fit_transform(x_train)\nX_test = pipe.transform(x_test)\n\n# ===========================________________Model____________________==============================\nxgboost = xgb.XGBClassifier(n_jobs=-1)\n\ngrid_param = {'n_estimators': [500, 1000, 1500, 2000],\n              'max_depth': [9, 10, 11],\n              'learning_rate': [0.1, 0.07, 0.03, 0.01],\n             'subsample': [0.5, 1.0],\n             'booster' : ['dart', 'gbtree']}\n\n# GridSearchCv and Cross Validation\ngrid = GridSearchCV(xgboost, grid_param, cv=2, scoring='roc_auc')\ngrid.fit(X_train, y_train)\nprint('Best Params', grid.best_params_)\nmodel = grid.best_estimator_\n\n# Predicting\npredict = model.predict(X_test)\npredictions = cross_val_predict(model, X_test, y_test, cv=2)\nprint(confusion_matrix(y_test, predictions))\nscore = np.mean(cross_val_score(model, X_test, y_test, cv=2, scoring='roc_auc'))\nprint(np.around(score, decimals=4))\n\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}