{"cells":[{"metadata":{"id":"daOedV1A-JLP"},"cell_type":"markdown","source":"[](http://)# My objectives are to show different approaches for recommendation system on the huge Netflix dataset:\n\n# 0. Netflix dataset statistics\n# 1. Model-based CF - matrix factorization methods\n# 2. Model-based CF - clustering models methods\n# 3. Memory-based CF - statistic correlation coefficient methods\n> # 4. Future - I will try to compare all the different method, ATM i still did not find any way to do it effectively\n"},{"metadata":{"id":"HpqsSwsG_D-f"},"cell_type":"markdown","source":"\n---\n\n"},{"metadata":{"id":"hGZjt42G_oqk"},"cell_type":"markdown","source":"# First i will analyze the dataset\nData loading\n\nEach data file (there are 4 of them) contains below columns:\n\nMovie ID (as first line of each new movie record / file)\n\nCustomer ID\n\nRating (1 to 5)\n\nDate they gave the ratings\n\nThere is another file contains the mapping of Movie ID to the movie background like name, year of release, etc\n\nLet's import the library we needed before we get started:"},{"metadata":{"id":"YRcOHwt4ccCs","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport re\nfrom scipy.sparse import csr_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom surprise import Dataset, SVD,  SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering, accuracy \nfrom surprise.reader import Reader\nfrom surprise.model_selection.validation import cross_validate as cross_validate\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next let's load first data file and get a feeling of how huge the dataset is:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skip date\ndf1 = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header = None, names = ['CustomerID','Rating', 'Date'], usecols = [0,1, 2])\ndf1['Rating'] = df1['Rating'].astype(float)\n\ndf = df1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Let's also load the 3 remaining dataset as well<br>\n****it's on a seperate commented block because it is too heavy to load all datases on every test run - though it needs to be uncommented when the best accuracies are needed****:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df2 = pd.read_csv('../input/netflix-prize-data/combined_data_2.txt', header = None, names = ['CustomerID', 'Rating',  'Date'], usecols = [0,1, 2])\n# df3 = pd.read_csv('../input/netflix-prize-data/combined_data_3.txt', header = None, names = ['CustomerID', 'Rating',  'Date'], usecols = [0,1, 2])\n# df4 = pd.read_csv('../input/netflix-prize-data/combined_data_4.txt', header = None, names = ['CustomerID', 'Rating',  'Date'], usecols = [0,1, 2])\n\n# df2['Rating'] = df2['Rating'].astype(float)\n# df3['Rating'] = df3['Rating'].astype(float)\n# df4['Rating'] = df4['Rating'].astype(float)\n\n# print('Dataset 2 shape: {}'.format(df2.shape))\n# print('Dataset 3 shape: {}'.format(df3.shape))\n# print('Dataset 4 shape: {}'.format(df4.shape))\n\n# df = df1.append(df2)\n# df = df.append(df3)\n# df = df.append(df4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)lets peek at the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.index = np.arange(0,len(df))\nprint('Full dataset shape: {}'.format(df.shape))\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we load the movie mapping file:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_movie_titles = pd.read_csv('../input/netflix-prize-data/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\ndf_titles = df_movie_titles.set_index('Movie_Id', inplace = False)\nprint (df_movie_titles.head(10))\nprint(list(df_movie_titles.columns))\nprint (df_titles.head(10))\nprint(list(df_titles.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's give a first look on how the data spread:"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = df.groupby('Rating')['Rating'].agg(['count'])\n\n# get movie count\nmovie_count = df.isnull().sum()[1]\n\n# get customer count\ncust_count = df['CustomerID'].nunique() - movie_count\n\n# get rating count\nrating_count = df['CustomerID'].count() - movie_count\n\nax = p.plot(kind = 'barh', legend = False, figsize = (15,10))\nplt.title('Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, cust_count, rating_count), fontsize=20)\nplt.axis('off')\n\nfor i in range(1,6):\n    ax.text(p.iloc[i-1][0]/4, i-1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i-1][0]*100 / p.sum()[0]), color = 'white', weight = 'bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets watch some customers in a customer/rating graph:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_boxplot_of_categories(data_frame, categorical_column, numerical_column, limit):\n    import seaborn as sns\n    from collections import Counter\n    keys = []\n    for i in dict(Counter(df[categorical_column].values).most_common(limit)):\n        keys.append(i)\n    print(keys)\n    df_new = df[df[categorical_column].isin(keys)]\n    sns.set()\n    sns.boxplot(x = df_new[categorical_column], y =      df_new[numerical_column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_boxplot_of_categories(df_titles, 'CustomerID', 'Rating', 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets watch the distribution over movies release dates:  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, plot, iplot\n\ndata = df_movie_titles['Year'].value_counts().sort_index()\n\n# Create trace\ntrace = go.Scatter(x = data.index,\n                   y = data.values,\n                   marker = dict(color = '#db0000'))\n# Create layout\nlayout = dict(title = '{} Movies Grouped By Year Of Release'.format(df_movie_titles.shape[0]),\n              xaxis = dict(title = 'Release Year'),\n              yaxis = dict(title = 'Movies'))\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How are the ratings distributed?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get data\ndata = df['Rating'].value_counts().sort_index(ascending=False)\n\n# Create trace\ntrace = go.Bar(x = data.index,\n               text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n               textposition = 'auto',\n               textfont = dict(color = '#000000'),\n               y = data.values,\n               marker = dict(color = '#db0000'))\n# Create layout\nlayout = dict(title = 'Distribution Of {} Netflix-Ratings'.format(df.shape[0]),\n              xaxis = dict(title = 'Rating'),\n              yaxis = dict(title = 'Count'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that the rating tends to be relatively positive (>3). This may be due to the fact that unhappy customers tend to just leave instead of making efforts to rate. so low rating movies mean they are generally really bad.."},{"metadata":{},"cell_type":"markdown","source":"When Have The Movies Been Rated?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df['Date'].value_counts()\ndata.index = pd.to_datetime(data.index)\ndata.sort_index(inplace=True)\n\n# Create trace\ntrace = go.Scatter(x = data.index,\n                   y = data.values,\n                   marker = dict(color = '#db0000'))\n# Create layout\nlayout = dict(title = '{} Movie-Ratings Grouped By Day'.format(df.shape[0]),\n              xaxis = dict(title = 'Date'),\n              yaxis = dict(title = 'Ratings'))\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With beginning of november 2005 a strange decline in ratings can be observed. Furthermore two unnormal peaks are in january and april 2005."},{"metadata":{},"cell_type":"markdown","source":"Now lets add the movies column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nan = pd.DataFrame(pd.isnull(df.Rating))\ndf_nan = df_nan[df_nan['Rating'] == True]\ndf_nan = df_nan.reset_index()\n\nmovie_np = []\nmovie_id = 1\n\nfor i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n\n    temp = np.full((1,i-j-1), movie_id)\n    movie_np = np.append(movie_np, temp)\n    movie_id += 1\n\nlast_record = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\nmovie_np = np.append(movie_np, last_record)\n\ndf = df[pd.notnull(df['Rating'])]\n\ndf['Movie_Id'] = movie_np.astype(int)\ndf['CustomerID'] = df['CustomerID'].astype(int)\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How Are The Number Of Ratings Distributed For The Movies And The Users?"},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Ratings Per Movie #####\n# Get data\ndata = df.groupby('Movie_Id')['Rating'].count().clip(upper=9999)\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 10000,\n                                  size = 100),\n                     marker = dict(color = '#db0000'))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Ratings Per Movie (Clipped at 9999)',\n                   xaxis = dict(title = 'Ratings Per Movie'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n\n\n\n##### Ratings Per User #####\n# Get data\ndata = df.groupby('CustomerID')['Rating'].count().clip(upper=199)\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 200,\n                                  size = 2),\n                     marker = dict(color = '#db0000'))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Ratings Per User (Clipped at 199)',\n                   xaxis = dict(title = 'Ratings Per User'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most customers tend to rate less than 20 movies"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"Now, to work!"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Data slicing\nThe data set now is super huge and i cant work with it in the current form, so i will reduce the data volumn by improving the data quality below:\n\nRemove movie with too less reviews (they are relatively not popular)\nRemove customer who give too less reviews (they are relatively less active)\nHaving above benchmark will have significant improvement on efficiency, since those unpopular movies and non-active customers still occupy same volumn as those popular movies and active customers in the view of matrix (NaN still occupy space). This should help improve the statistical signifiance too."},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_percentile = 0.7\n#Movies rate count percentile\n#I will leave only movies on the (1-movies_percentile) percentile with respect to movies rating count\n\ncustomers_percentile = 0.7\n#Customers rate count percentile\n#I will leave only customers on the (1-customers_percentile) percentile with respect to customers rating count\n\n\ndf_movie_summary = df.groupby('Movie_Id')['Rating'].agg(['count'])\ndf_movie_summary.index = df_movie_summary.index.map(int)\nmovie_benchmark = round(df_movie_summary['count'].quantile(movies_percentile),0)\ndrop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\n\ndf_cust_summary = df.groupby('CustomerID')['Rating'].agg(['count'])\ndf_cust_summary.index = df_cust_summary.index.map(int)\ncust_benchmark = round(df_cust_summary['count'].quantile(customers_percentile),0)\ndrop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\n\nprint('Movies minimum rating count: {}'.format(movie_benchmark))\nprint('Customers minimum rating count: {}'.format(cust_benchmark))\n\nprint('Original Shape: {}'.format(df.shape))\ndf = df[~df['Movie_Id'].isin(drop_movie_list)]\ndf = df[~df['CustomerID'].isin(drop_cust_list)]\n\nprint('After Trim Shape: {}'.format(df.shape))\n\nprint('unique movies left:')\nprint(df['Movie_Id'].unique().size)\nprint('unique customers left:')\nprint(df['CustomerID'].unique().size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now i will pivot the dataset and convert it into a matrix M, \nwhere Mi,j is the rating the ith customer gave to the jth movie\n\nI will also replace all NaN values with zeros - and should keep in mind that there is no zero rating - the rating ranges from 1 to 5,\nso the value '0' will state that this movie was not being reviewed and not that it's given rating is zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p = pd.pivot_table(df,values='Rating',index='CustomerID',columns='Movie_Id')\ndf_p = df_p.fillna(0)\nprint(df_p.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# A  recommendation system is a subclass of information filtering system that seeks to predict the \"rating\" a user would give to an item"},{"metadata":{},"cell_type":"markdown","source":"# Task1 - Model-based CF - matrix factorization methods"},{"metadata":{},"cell_type":"markdown","source":"1. [Surprise](http://surpriselib.com/) is a Python scikit building and analyzing recommender systems that deal with explicit rating data.<br>\nI will use this library for the perpose of trying to recommend Netflix movies to Netflix users"},{"metadata":{},"cell_type":"markdown","source":"# I will try the following models:\n\n> SVD, SVDpp, NMF, NormalPredictor CoClustering"},{"metadata":{},"cell_type":"markdown","source":"Matrix Factorization-based algorithms\n> SVD<br>\n> SVD algorithm is equivalent to Probabilistic Matrix Factorization<br>\n> SVDpp<br>\n> The SVDpp algorithm is an extension of SVD that takes into account implicit ratings.<br>\n> NMF<br>\n> NMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization. It is very similar with SVD.<br>\n> Slope One<br>\n> SlopeOne is a straightforward implementation of the SlopeOne algorithm.<br>\n> Co-clustering<br>\n> Coclustering is a collaborative filtering algorithm based on co-clustering.<br>"},{"metadata":{},"cell_type":"markdown","source":"Step1 - evaluate:\n\nPerform 3-folds cross validation in order to determine the best predictor.<br>\nFor the accuracy metric i use “rmse” - root squared error."},{"metadata":{},"cell_type":"markdown","source":"**All the algorithms below excpects a dataset with the following scheme 'CustomerID', 'Movie_Id', 'Rating': <br>\nThe return is a function F: CustomerID -> Rating **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_validate_cf_algorithms(rows):\n\n    reader = Reader()\n\n    data = Dataset.load_from_df(df[['CustomerID', 'Movie_Id', 'Rating']][:rows], reader)\n\n    benchmark = []\n    # Iterate over all algorithms\n    for algorithm in [SVD(), SVDpp(), NMF(), NormalPredictor(),  CoClustering()]:\n        # Perform cross validation\n        results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n\n        # Get results & append algorithm name\n        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n        tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n        benchmark.append(tmp)\n\n    print(pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get just top 100K rows for faster run time\ncross_validate_cf_algorithms(100000)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVD performed best"},{"metadata":{},"cell_type":"markdown","source":"# Step2 - train the best model - SVD:"},{"metadata":{},"cell_type":"markdown","source":"Return the top-N recommendation for each user from a set of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom collections import defaultdict\ndef get_top_n(predictions, n=5):\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A simple function to retrieve the movie name from the movie ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_m = df.set_index('Movie_Id')\nnames_movie_mapping = df_titles.join(df_m)\nprint(names_movie_mapping)\n\ndef get_movie_name(id):\n    \n    return names_movie_mapping.loc[names_movie_mapping.index == id, 'Name'].unique()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_movie_name(id=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train SVD model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = Reader()\ndata = Dataset.load_from_df(df[['CustomerID', 'Movie_Id', 'Rating']][:100000], reader)\ntrainset = data.build_full_trainset()\n\nalgo = SVD()\npredictions = algo.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Print the recommanded movie for each customer on created test set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntestset = trainset.build_anti_testset()\npredictions = algo.test(testset)\n\ntop_n = get_top_n(predictions, n=10)\n\n# Print the recommended movies for each user\nrecommanded_movies = {}\nfor uid, user_ratings in top_n.items():\n\n    recommanded_movies[uid] = [get_movie_name(movie_id) for (movie_id, _) in user_ratings]\n    print(uid, recommanded_movies[uid])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we have a recommendation function"},{"metadata":{},"cell_type":"markdown","source":"The function's returns the recommanded movie for the customer:<br>\nThe prediction rule is to take each movie that the customer loved (rating = 5) , than for each movie predict using SVD and finaly take most frequent movie - **only because i want to output just a single value**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommand_SVD(CustomerID):\n\n    res = []\n\n    for x in df[(df['CustomerID'] == CustomerID) & (df['Rating'] == 5)]['Movie_Id']:\n        \n        p = algo.predict(CustomerID, x)[1]      \n        res.append(p) \n\n        \n    return get_movie_name(np.bincount(res).argmax())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's predict which movies a specific user would love to watch:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The recommanded movie for customer 1333 using clustering CF is: \", recommand_SVD(1333))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Task2 - Model-based CF- clustering models"},{"metadata":{"id":"vz47ZM-Wcd-d"},"cell_type":"markdown","source":"# Clusterize Netflix movies\n\nI will try to clusterize the movies based on the rating recieved from the users.\n\n\nThe steps:\n1. prepare the dataset to sklearn\n\n2.  Compare few clustering algorithms\n\n    *   DBSCAN\n    *   K-means\n    *   XXX\n\n3.   Create a function F that will map the movies to their corresponding clusters (F: movieID -> movieClusterId)\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"import the required SKlearn libraries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn_pandas import DataFrameMapper, cross_val_score\nimport numpy as np\nimport sklearn.preprocessing, sklearn.decomposition, sklearn.linear_model, sklearn.pipeline, sklearn.metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA,SparsePCA, TruncatedSVD, NMF\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.cm as cm\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.impute import SimpleImputer\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing the dataset - i would like each movie to be represented by it's column, that is, all rating recieved by all users,\nThe problem is the sparsness, that is, the zero ratings,\nSo, my solution for this will be to replace all zeroes rating with the mean of all non zeroes ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_p.as_matrix(columns=df_p.columns[:]).transpose()\n\nimp_mean = SimpleImputer(missing_values=0, strategy='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets reduce the dimensions first- for that i will use PCA - lets find the best size for the new dimension - n_component parameter in sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA().fit(X)\n\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Movies Dataset Explained Variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we can reduce the size to 600 (reducing it by more than 1 / 2) with a minimal loss of 0.1 of the variance\nSo now i will use PCA with n_component = 600 to reduce X to 600 dimensions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=600)\nX_reduced = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"Now i will try K-means on the reduced data with K in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]<br>\nThan, for each result of k ill plot the **silhouette score** and than i will pick the best value for K"},{"metadata":{"trusted":true},"cell_type":"code","source":"range_n_clusters = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\ndata = X_reduced\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(data)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(data, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(data, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(data[:, 0], data[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as we can see, the best K will be 35 with a Silhouette value of 0.86,<br>\nLets train the model again with 35 clusters:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"    clusterer = KMeans(n_clusters=35, random_state=10)\n    cluster_labels = clusterer.fit_predict(X_reduced)\n\n    silhouette = silhouette_score(X_reduced, cluster_labels)\n    print(\"For n_clusters =\", 35,\n          \"The silhouette_score is :\", silhouette)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"Next i will try DBScan algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"db = DBSCAN().fit(X_reduced)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nif n_clusters_ > 1:\n    print(\"Silhouette Coefficient: %0.3f\"\n          % metrics.silhouette_score(X, labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Infortunatly DBscan did not manage to find clusters in our data.."},{"metadata":{},"cell_type":"markdown","source":"Now i will try some more clustering algorithms:<br>\n'MiniBatchKMeans', 'AffinityPropagation', 'SpectralClustering', 'Ward', 'AgglomerativeClustering', 'OPTICS', 'Birch', 'GaussianMixture'<br>\ni will also plot the resulting clusters and print the scores (only if more than 10 clusters found)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\n\nnp.random.seed(0)\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = len(X_reduced[0])\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(9 * 2 + 3, 12.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\n\ndefault_base = {'quantile': .3,\n                'eps': .3,\n                'damping': .9,\n                'preference': -200,\n                'n_neighbors': 10,\n                'n_clusters': 35,\n                'min_samples': 20,\n                'xi': 0.05,\n                'min_cluster_size': 0.01}\n\ndatasets = [\n    (X_reduced, {'damping': .77, 'preference': -240,\n                     'quantile': .2, 'n_clusters': 35,\n                     'min_samples': 20, 'xi': 0.25})]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params['n_neighbors'], include_self=False)\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params['n_clusters'], linkage='ward',\n        connectivity=connectivity)\n    spectral = cluster.SpectralClustering(\n        n_clusters=params['n_clusters'], eigen_solver='arpack',\n        affinity=\"nearest_neighbors\")\n    dbscan = cluster.DBSCAN(eps=params['eps'])\n    optics = cluster.OPTICS(min_samples=params['min_samples'],\n                            xi=params['xi'],\n                            min_cluster_size=params['min_cluster_size'])\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params['damping'], preference=params['preference'])\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\", affinity=\"cityblock\",\n        n_clusters=params['n_clusters'], connectivity=connectivity)\n    birch = cluster.Birch(n_clusters=params['n_clusters'])\n    gmm = mixture.GaussianMixture(\n        n_components=params['n_clusters'], covariance_type='full')\n\n    clustering_algorithms = (\n        ('MiniBatchKMeans', two_means),\n        ('AffinityPropagation', affinity_propagation),\n        ('MeanShift', ms),\n        ('SpectralClustering', spectral),\n        ('Ward', ward),\n        ('AgglomerativeClustering', average_linkage),\n        ('DBSCAN', dbscan),\n        ('OPTICS', optics),\n        ('Birch', birch),\n        ('GaussianMixture', gmm)\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \" +\n                \"connectivity matrix is [0-9]{1,2}\" +\n                \" > 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning)\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\" +\n                \" may not work as expected.\",\n                category=UserWarning)\n            algorithm.fit(X)\n        \n        t1 = time.time()\n        if hasattr(algorithm, 'labels_'):\n            y_pred = algorithm.labels_.astype(np.int)\n            \n            if (len(np.unique(algorithm.labels_)) > 1):\n                print(name + \" silhouette_avg: \", silhouette_score(X, algorithm.labels_))\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                             '#f781bf', '#a65628', '#984ea3',\n                                             '#999999', '#e41a1c', '#dede00']),\n                                      int(max(y_pred) + 1))))\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment='right')\n        plot_num += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see 'Ward' performed best with a score of 0.61, it is a good result but still less than KMeans."},{"metadata":{},"cell_type":"markdown","source":"# So the best clustering algorithm for our task is KMeans with K=35"},{"metadata":{},"cell_type":"markdown","source":"# The recommandation function:<br>\nReturns the recommanded movie for the customer:<br>\nThe prediction rule is to take each movie that the customer loved (rating = 5)<br>\nTake it's corresponding row from the dataset - (all customers are the features in every row)<br>\nReduce the dimension with PCA<br>\nPredict using KMeans , K=35<br>\nTake a random movie from this cluster<br>\nFinaly take most frequent movie:"},{"metadata":{},"cell_type":"markdown","source":"Below is a simple function that returns a random movie that is inside a given cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_movie(cluster):\n     return random.choice(np.squeeze(np.argwhere(cluster_labels==cluster)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_movie(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommand_KMeans(CustomerID):\n    \n    res = []\n\n    for movieID in df[(df['CustomerID'] == CustomerID) & (df['Rating'] == 5)]['Movie_Id']:\n        \n        x = np.squeeze(df_p.as_matrix(columns=[movieID])).reshape(1, -1) \n         \n        transformed = pca.transform(x)\n        p = np.squeeze(clusterer.predict(transformed))\n        res.append(get_movie(p))   \n        \n        \n    return get_movie_name(np.bincount(res).argmax())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The recommanded movie(ID) for customer 1333 using clustering CF is: \", recommand_KMeans(1333))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Task3 - Memory-based CF"},{"metadata":{},"cell_type":"markdown","source":"# I will try the following correlation coefficients:\n\n> Pearson, Kendall, Spearman "},{"metadata":{},"cell_type":"markdown","source":"The idea here is to measure the linear correlation between rating of all pairs of movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_similar_movies(method_name, movie_title, n_movies, min_count=0):\n\n    i = int(df_titles.index[df_titles['Name'] == movie_title][0])\n    target = df_p[i]\n    similar_to_target = df_p.corrwith(target, method=method_name)\n    corr_target = pd.DataFrame(similar_to_target, columns = [method_name])\n    corr_target.dropna(inplace = True)\n    corr_target = corr_target.sort_values(method_name, ascending = False)\n    corr_target.index = corr_target.index.map(int)\n    corr_target = corr_target.join(df_titles).join(df_movie_summary)[[method_name, 'Name', 'count']]\n    return [name for name in corr_target[corr_target['count']>min_count][:n_movies]['Name']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The recommandation function:<br>\nReturns the recommanded movie for the customer:<br>\nThe prediction rule is to take all the movies that the customer loved (rating = 5), than take the corresponding movieID column from the dataset - find the highest correlated column of another movie and return it, do it for each of the movies that the customer loved."},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_movies - is the maximum number of movies to return\n# as_names - the names of the movies / only the id \n\ndef recommand_corr(method_name, CustomerID):\n\n    res = []\n\n    for x in df[(df['CustomerID'] == CustomerID) & (df['Rating'] == 5)]['Movie_Id']:\n        p = get_similar_movies(method_name, get_movie_name(x), 1)[0]\n        res.append(p)    \n\n        \n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor method in ['pearson', 'kendall', 'spearman']:\n    print(method, recommand_corr(method, 1333))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# My future work..\n\nMy next step will be to try and comapre all ther different approaches."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"**Thanks alot for the greate class, (Yoram you were greate!)**"}],"metadata":{"colab":{"name":"NetflixDataAnalyze.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}