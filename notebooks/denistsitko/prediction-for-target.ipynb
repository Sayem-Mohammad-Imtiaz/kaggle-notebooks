{"cells":[{"metadata":{},"cell_type":"markdown","source":"This report was generated using [FastBenchmark](http://fastbenchmark.me/) project.  \n# Prediction for target using aug_train.csv data set.  \nHere we will try to predict **target** using **aug_train.csv** data set.  \n## Reading the data and splitting it into train and test sets\nFirst we need to import all necessary libraries, read the data from csv and make \ntraining-test split. Standard split rate is 80 to 20, but you can change it in code \n(by changing 0.2 to any other value in _make train and test_ string)  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The code was generated by FastBenchmark telegram bot\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport xgboost as xgb\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_squared_log_error\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\n\nwarnings.simplefilter(action='ignore')\npd.options.mode.chained_assignment = None\n\n# which correlation is good enough?\ncorrelation_level = 0.7\n\n# how much points to draw to visualize correlations\nsequence_length = 100\n\n# list of errors\nerrors = []\n# list of columns to delete\nto_drop = []\n\n# coefficient to make plots for categories (if more categories - will not plot it)\ncat_coef = 10\n\n# reading the data\ndata = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv', sep = ',')\n\n\n# cheking if target column is in columns\ntarget_col_name = 'target'\n\n\n\nif target_col_name not in data.columns:\n    print('No ' + str(target_col_name) + ' in test set')\n    errors.append('No ' + str(target_col_name) + ' in test set')\n    to_drop.append(target_col_name)\n    quit()\n\n# coefficient to check if categories are rare enough\ncat_check = 0.01*len(data[target_col_name])\n\n# make train and test\ntrain, test = train_test_split(data, test_size=0.2)\n\n# remove rows were target is Na\ntrain = train[train[target_col_name].notna()]\nif target_col_name in test.columns:\n    test = test[test[target_col_name].notna()]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So data is loaded and now we have training and test sets.  \nWe will train our model on training set and will try to predict **target** values in test\n data set using other variables from test set.  \nBut first we need to look at the data and make some data preparation so we could use it to\ntrain xgboost model.   \n\nHere is the pair plot for given data (by diagonal we can see variable values distribution).  \nIt is very useful for better understanding dependencies in data:   \n"},{"metadata":{},"cell_type":"markdown","source":"Let's look at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train, hue=target_col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NaN functions\nHere are the functions for NaN values.  \nBy default we will replace NaNs in numeric columns by np.nanmean and for categories with -1.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def nan_numeric(data):\n    result = np.nanmean(data)\n    return result\n\n\ndef nan_categorical(data):\n    result = -1\n    return result\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target variable target (categorical)\nHere we will deal with **target** variable which is our target variable \n(the variable which values, we want to predict).  \nFirst we will look at its values distribution.\nAfter we will transform the data so we could be able to use it in xgboost ML model.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TARGET COLUMN (categorical) - target\n\ncol_name = train.columns[13]\ntrain[col_name] = train[col_name].astype(str)\nif col_name in test.columns:\n    test[col_name] = test[col_name].astype(str)\ncategoryType = 'category_bool'\ntry:\n    sns.displot(x=train[col_name])\nexcept:\n    sns.distplot(train[col_name])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if categoryType == 'category' or categoryType == 'category_bool':\n    # replacing category names with numbers\n    encoder = LabelEncoder()\n    encoder.fit(train[col_name])\n    target = encoder.transform(train[col_name])\n    target_real_values = {'Category name': encoder.classes_, 'Category value': range(len(encoder.classes_))}\n    if col_name in test.columns:\n        test_target = encoder.transform(test[col_name])\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    target = pd.to_numeric(target, errors='coerce')\n    target[pd.isna(target)] = np.nanmean(target)\n    if col_name in test.columns:\n        test_target = pd.to_numeric(test_target, errors='coerce')\n        test_target[pd.isna(test_target)] = np.nanmean(test_target)\n\nelse:\n    print('This version can predict only numeric and categorical values values')\n    quit()\n\n\n\nif col_name in train.columns:\n    to_drop.append(col_name)\nelse:\n    print('No ' + str(col_name) + ' in data set')\n    errors.append('No ' + str(col_name) + ' in data set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## enrollee_id variable (numeric)\nId column. Will be dropped\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUMERIC  COLUMN - enrollee_id\n\ncol_name = train.columns[0]\nto_drop.append(col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## city variable (categorical)\nHere we will deal with **city** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - city\n\ncol_name = train.columns[1]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## city_development_index variable (numeric)\nHere we will deal with **city_development_index** variable which is a numeric variable.  \nWe will look at at it's statistics, distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUMERIC  COLUMN - city_development_index\n\ncol_name = train.columns[2]\ntry:\n    sns.displot(x=train[col_name].dropna())\nexcept:\n    sns.distplot(train[col_name].dropna())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**city_development_index** variable statistics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train[col_name].describe())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will prepare **city_development_index** variable to use it in xgboost ML model."},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n\n    # dealing with NaNs in column\n    train[col_name] = pd.to_numeric(train[col_name], errors='coerce')\n    test[col_name] = pd.to_numeric(test[col_name], errors='coerce')\n    train[col_name][pd.isna(train[col_name])] = nan_numeric(train[col_name])\n    test[col_name][pd.isna(test[col_name])] = nan_numeric(train[col_name])\n\n# if there is no such column in test set - add information to errors and delete column in training set\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It could also be useful to add log transformed numeric variable to data set,so let's do so and check its distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    train[col_name + '_log'] = np.log(train[col_name] + 1 - min(0, min(train[col_name])))\n    test[col_name + '_log'] = np.log(test[col_name] + 1 - min(0, min(test[col_name])))\n    try:\n        sns.displot(x=train[col_name + '_log'])\n    except:\n        sns.distplot(train[col_name + '_log'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## gender variable (categorical)\nHere we will deal with **gender** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - gender\n\ncol_name = train.columns[3]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## relevent_experience variable (categorical)\nHere we will deal with **relevent_experience** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - relevent_experience\n\ncol_name = train.columns[4]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## enrolled_university variable (categorical)\nHere we will deal with **enrolled_university** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - enrolled_university\n\ncol_name = train.columns[5]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## education_level variable (categorical)\nHere we will deal with **education_level** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - education_level\n\ncol_name = train.columns[6]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## major_discipline variable (categorical)\nHere we will deal with **major_discipline** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - major_discipline\n\ncol_name = train.columns[7]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## experience variable (categorical)\nHere we will deal with **experience** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - experience\n\ncol_name = train.columns[8]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## company_size variable (categorical)\nHere we will deal with **company_size** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - company_size\n\ncol_name = train.columns[9]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## company_type variable (categorical)\nHere we will deal with **company_type** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - company_type\n\ncol_name = train.columns[10]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## last_new_job variable (categorical)\nHere we will deal with **last_new_job** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATEGORY COLUMN - last_new_job\n\ncol_name = train.columns[11]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## training_hours variable (numeric)\nHere we will deal with **training_hours** variable which is a numeric variable.  \nWe will look at at it's statistics, distribution and connection with target variable **target**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUMERIC  COLUMN - training_hours\n\ncol_name = train.columns[12]\ntry:\n    sns.displot(x=train[col_name].dropna())\nexcept:\n    sns.distplot(train[col_name].dropna())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**training_hours** variable statistics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train[col_name].describe())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will prepare **training_hours** variable to use it in xgboost ML model."},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n\n    # dealing with NaNs in column\n    train[col_name] = pd.to_numeric(train[col_name], errors='coerce')\n    test[col_name] = pd.to_numeric(test[col_name], errors='coerce')\n    train[col_name][pd.isna(train[col_name])] = nan_numeric(train[col_name])\n    test[col_name][pd.isna(test[col_name])] = nan_numeric(train[col_name])\n\n# if there is no such column in test set - add information to errors and delete column in training set\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It could also be useful to add log transformed numeric variable to data set,so let's do so and check its distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"if col_name in test.columns:\n    train[col_name + '_log'] = np.log(train[col_name] + 1 - min(0, min(train[col_name])))\n    test[col_name + '_log'] = np.log(test[col_name] + 1 - min(0, min(test[col_name])))\n    try:\n        sns.displot(x=train[col_name + '_log'])\n    except:\n        sns.distplot(train[col_name + '_log'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove all useless columns\nHere we will remove all useless columns\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean data\nfor name in to_drop:\n    if name in train.columns:\n        del train[name]\n    if name in test.columns:\n        del test[name]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction for target\nHere we will train xgboost model to predict our target variable **target**.  \nActually, all what we do above was just a sort of preparation. \nAnd now we will use all we done above to make better prediction for target variable.  \nHere you can use different models, make ensembles, and try to improve the solution.  \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAKE MODEL\n\n# subsample for feature engineering\nreal_train = train\nreal_target = target\nreal_test = test\ntrain, test, target, test_target = train_test_split(train, target, test_size=0.2)\n\n# more rounds -> better prediction, but longer training\nnum_boost_rounds = 1000\neta = 10/num_boost_rounds\n\n# parameters for xgboost\nxgb_params = {\n    'eta': eta,\n    'subsample': 0.80,\n    'objective': 'binary:logistic',\n    'eval_metric': 'error'\n}\n\n# transforming the data for xgboost\ndtrain = xgb.DMatrix(train, target)\ndtest = xgb.DMatrix(test)\ndreal_test = xgb.DMatrix(real_test)\n# training the model\nmodel = xgb.train(dict(xgb_params), dtrain, num_boost_round=num_boost_rounds)\n\n# predict\npreds = model.predict(dtest)\npreds = pd.to_numeric(preds)\npreds[pd.isna(preds)] = np.nanmean(preds)\n\nreal_preds = model.predict(dreal_test)\nreal_preds = pd.to_numeric(real_preds)\nreal_preds[pd.isna(real_preds)] = np.nanmean(real_preds)\n\npreds_categorical = []\ntest_target_categorical = []\n\nfor i in range(len(preds)):\n    if round(preds[i]) in target_real_values['Category value']:\n        pred_name_index = target_real_values['Category value'].index(round(preds[i]))\n    else:\n        pred_name_index = target_real_values['Category value'].index(min(target_real_values['Category value']))\n    test_target_name_index = target_real_values['Category value'].index(test_target[i])\n    preds_categorical.append(target_real_values['Category name'][pred_name_index])\n    test_target_categorical.append(target_real_values['Category name'][test_target_name_index])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature importance for **target** prediction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing the data and printing the statistics\nxgb_fea_imp = pd.DataFrame(list(model.get_fscore().items()),\ncolumns=['feature', 'importance']).sort_values('importance', ascending=False)\nxgb_fea_imp.style.hide_index()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification report and plot for prediction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for_plot = pd.DataFrame({'Id': range(len(preds_categorical)), 'Predictions': preds_categorical, 'Real values': test_target_categorical})\nsns.scatterplot(data=for_plot, x='Id', y='Predictions', hue='Real values')\npd.DataFrame(classification_report(test_target_categorical, preds_categorical, output_dict=True)).T\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also look at roc curve score for non-rounded dataand plot the non-rounded prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"for_plot = pd.DataFrame({'Id': range(len(preds)), 'Prediction': preds, 'Real values': np.round(test_target)})\nsns.scatterplot(data=for_plot, x='Id', y='Prediction', hue='Real values')\nprint('roc auc score is ' + str(roc_auc_score(np.round(test_target), preds)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Errors statistics\nHere we will look at the largest mistakes and compare it to our training set.  \nThat could help us with feature generation.\nFirst we will look at largest absolute errors.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# how much top errors to work with\ntop_errors = 5\n\n# how much best variables to use\ntop_features = 4\n\n# how much rows from training set to look at\ntop_rows = 5\n\n# how much could be the distance btw values in % to take them as similar (to assume that two values are in same claster)\ndist = 5\n\npreds_df = pd.DataFrame({'prediction': preds})\npreds_df.index = test.index\ntest_target_df = pd.DataFrame({target_col_name: test_target})\ntest_target_df.index = test.index\ntarget_df = pd.DataFrame({target_col_name: target})\ntarget_df.index = train.index\ntest_with_target = test.join(test_target_df).join(preds_df)\ntrain_with_target = train.join(target_df)\ntest_with_target = test.join(test_target_df).join(preds_df)\ntrain_with_target = train.join(target_df)\ntest_with_target['abs_err'] = abs(test_with_target[target_col_name] - test_with_target['prediction'])\ntest_top_errors = test_with_target.sort_values(by='abs_err', ascending = False).head(top_errors)\nmean_top_err = test_top_errors.describe().iloc[1:2, :]\nshow_order = [target_col_name, 'prediction', 'abs_err'] + xgb_fea_imp['feature'].tolist()\ntest_top_errors[show_order]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will try to find most similar rows to this largest error rows in our training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = xgb_fea_imp['feature'][0:top_features].tolist()\nto_compare = train_with_target\nfor feature in features:\n    condition1 = train_with_target[feature] >= (mean_top_err[feature]*(1 - (dist/100))).values[0]\n    condition2 = train_with_target[feature] < (mean_top_err[feature]*(1 + (dist/100))).values[0]\n    condition = condition1 & condition2\n    if to_compare[condition].shape[0] >= top_rows:\n        to_compare = to_compare[condition]\n    else:\n        break\nshow_order = [target_col_name] + xgb_fea_imp['feature'].tolist()\nto_compare[show_order].head(top_rows)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}