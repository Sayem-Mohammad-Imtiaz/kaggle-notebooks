{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import necessary libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,StackingClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,f1_score\n\nimport os\nimport time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_df  = pd.read_csv(\"/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the first few entries\nheart_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of the dataset is : {}\".format(heart_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that there is no null value present in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique values in each feature\nheart_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there is any duplicate\nheart_df[heart_df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the duplicate row\nheart_df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns with discrete numeric values\nnum_cat_cols = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']\n                \nfor col in num_cat_cols:\n    print(\"column : {}\".format(col))            \n    print(heart_df[col].value_counts(normalize=True))\n    print(\"\\n\")            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature matrix\nX = heart_df.drop(\"output\",axis=1)\n\n# target vector\ny = heart_df['output']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of the training set : {}\".format(X_train.shape))\nprint(\"The shape of the test set : {}\".format(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of classes in train and test target vectors\nprint(\"The training set :\")\nprint(y_train.value_counts(normalize=True))\nprint(\"\\n The test set :\")\nprint(y_test.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_pipeline(model):\n    \n    \"\"\"\n    A function to build pipeline with the following steps:\n           \n           1. 'scaler' : Standardize features by removing the mean and scaling to unit variance.\n           2. 'clf' : fit a classifier to the dataset.\n           \n    Parameter:\n    ------------\n    model : a classifier object\n    \n    Returns:\n    ----------\n    pipeline : a pipeline object\n    \n    \"\"\"\n    \n    pipeline = Pipeline([\n                       ('scaler', StandardScaler()),\n                       ('clf', model) \n                       ])\n    \n    return pipeline\n\n\ndef scan_model(pipeline,param_dict,X_train,y_train):\n    \n    \"\"\"\n    A function to find the optimal set of parameters for a classifier using GridSearchCV.\n    \n    Parameters:\n    ------------\n    pipeline : a pipeline object\n    \n    param_dict : dict\n          a dictionary with the names of hyperparameters as keys and the corresponding list of values to be scanned as values\n          \n    X_train : pandas dataframe\n          the feature matrix\n          \n    y_train : pandas series\n          the target vector\n    \n    Returns:\n    ----------\n    grid_cv : a GridSearchCV object fitted on the training data\n    \n    \n    \"\"\"\n    \n    grid_cv = GridSearchCV(pipeline,param_grid=param_dict,scoring='accuracy',cv=5,verbose=1)\n    grid_cv.fit(X_train,y_train)\n\n    return grid_cv\n\n\ndef eval_model(model,X_test,y_test):\n    \n    \"\"\"\n    A function to evaluate the performance of a classifier on unseen data.\n           \n    Parameter:\n    ------------\n    model : the classifier fitted on the training data\n    \n    X_test : pandas dataframe\n         the feature matrix\n         \n    y_test : pandas series\n         the target vector\n    \n    Returns:\n    ----------\n    acc_test : float\n         the accuracy score on the test set\n         \n    f1_test : float\n         the f1 score on the test set\n         \n    tn,fp,fn,tp : integer\n         true negative,false positive,false negative and true positive respectively (for the test set)\n    \n    \"\"\"\n    \n    pred_test = model.predict(X_test)\n    tn, fp, fn, tp = confusion_matrix(y_test,pred_test).ravel()\n    acc_test = accuracy_score(y_test,pred_test)\n    prec_test = precision_score(y_test,pred_test)\n    recall_test = recall_score(y_test,pred_test)\n    f1_test = f1_score(y_test,pred_test)\n    \n    return acc_test,f1_test,tn,fp,fn,tp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(),\n        LGBMClassifier(), GradientBoostingClassifier(), AdaBoostClassifier(), CatBoostClassifier(),\n        XGBClassifier()]\n\nlr_dict = {\n           'clf__C' : np.arange(0.1,6,0.5),\n           'clf__solver': ['liblinear','lbfgs']}\n\ndt_dict = {\n           'clf__criterion' : ['gini', 'entropy'],\n           'clf__max_features' : ['sqrt','log2'],\n           'clf__max_depth' : np.arange(10,60,10)}\n\nrf_dict = {\n           'clf__n_estimators': np.arange(20,100,20),\n           'clf__criterion' : ['gini', 'entropy'],\n           'clf__max_features' : ['sqrt','log2'],\n           'clf__max_depth' : np.arange(10,60,10)}\n\nknn_dict = {\n            'clf__n_neighbors': np.arange(5,30,5),\n            'clf__weights' : ['uniform','distance'],\n            'clf__algorithm' : ['auto','ball_tree','kd_tree','brute'],\n            'clf__metric' : ['minkowski','euclidean']}\n\nlgbm_dict = {\n             'clf__boosting_type': ['gbdt','dart','goss','rf'],\n             'clf__n_estimators': np.arange(50,150,10),\n             'clf__learning_rate': np.arange(0.1,5,0.5),\n             'clf__max_depth': np.arange(2,10,1)}\n\ngb_dict = {\n           'clf__loss' : ['deviance','exponential'],\n           'clf__learning_rate': np.arange(0.1,5,0.5),\n           'clf__n_estimators': np.arange(20,100,20),\n           'clf__criterion' : ['friedman_mse','mse','mae'],\n           'clf__max_features' : ['sqrt','log2']}\n\nada_dict = {\n           'clf__learning_rate': np.arange(0.1,5,0.5),\n           'clf__n_estimators': np.arange(20,100,20),\n           'clf__algorithm' : ['SAMME','SAMME.R']}\n\ncatb_dict = {\n           'clf__learning_rate': np.arange(0.1,5,0.5),\n           'clf__n_estimators': np.arange(50,100,10),    \n           'clf__auto_class_weights': [None,'balanced','SqrtBalanced']}\n\nxgb_dict = {\n           'clf__learning_rate': np.arange(0.1,5,0.5),\n           'clf__n_estimators': np.arange(20,100,20),\n           'clf__booster':['gbtree','gblinear','dart'],\n           'clf__gamma': np.arange(0.1,5,0.5)}\n\nparam_list = [lr_dict, dt_dict, rf_dict, knn_dict, lgbm_dict, gb_dict, ada_dict, catb_dict, xgb_dict]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list to hold the information about classifiers after grid search\nscan_list = []\n\n# list to hold the values of TN,FP,FN and TP for each classifier\npred_list = []\n\nfor model, param_dict in zip(model_list,param_list):\n    print(\"building model : {}\\n\".format(str(model).split(\"(\")[0]))\n    \n    # build the model\n    pipeline = build_pipeline(model)\n    start_time = time.time()      \n    \n    # perform grid search and fit the best model to the training set\n    grid_cv = scan_model(pipeline,param_dict,X_train,y_train)\n    print(\"\\n model fitted!\")\n    elapsed_time = time.time() - start_time        \n    \n    # evaluate the model\n    acc_test,f1_test,tn,fp,fn,tp = eval_model(grid_cv,X_test,y_test)   \n      \n    scan_list.append({\n              'model': str(model).split(\"(\")[0],\n              'best_score': grid_cv.best_score_,\n              'test_acc': acc_test,\n              'test_f1': f1_test,\n              'best_params':grid_cv.best_params_,\n              'time':elapsed_time\n    })      \n          \n          \n    pred_list.append({\n              'model': str(model).split(\"(\")[0],\n              'true_neg':tn,\n              'false_pos':fp,\n              'false_neg':fn,\n              'true_pos':tp\n    })      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scan_df = pd.DataFrame(scan_list)\nscan_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame(pred_list)\npred_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the top 3 models\n\nprint(\"Logistic Regression\")\nprint(scan_df.iloc[0][4])\n\nprint(\"\\n LGBMClassifier\")\nprint(scan_df.iloc[4][4])\n\nprint(\"\\n XGBClassifier\")\nprint(scan_df.iloc[8][4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best performing model is XGBClassifier with accuracy score 0.836066 and f1 score 0.857143. It also gives the lowest values of FN and FP (3 and 7 respectively). Note that, FN (individuals predicted having less chance of heart attack but are actually prone to heart diseases) has much more severe effect as compared to FP (individuals predicted having high chance of heart attack but are actually not prone to heart diseases). So, XGBClassifier is performing quite well."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}