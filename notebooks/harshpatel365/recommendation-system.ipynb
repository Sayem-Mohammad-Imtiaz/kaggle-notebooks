{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":" Myself [Harsh Patel](https://www.linkedin.com/in/harshp1035/) creating an ML based Recommendation Engine in collaboration with [Mr. Rocky Jagtiani](https://www.linkedin.com/today/author/rocky-jagtiani-3b390649/)\n \n> This is a simple Data Science project on Movies Recommendation System which recommends you the movie based on the Review of previous movie.\n\n> Dataset: tmdb_5000_credits.csv,tmdb_5000_movies.csv from kaggle itself\n\n> Tech Stack used: pandas, Scikit-learn,Python\n\n> Recommended links : \n\n> https://datascience.suvenconsultants.com  ( For DS / AI / ML )\n\n> https://monster.suvenconsultants.com  ( For Web development )"},{"metadata":{},"cell_type":"markdown","source":"Recommender systems are among the most popular applications of data science today. They are used to predict the \"rating\" or \"preference\" that a user would give to an item. Almost every major tech company has applied them in some form. Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow.\n\nRecommender systems have also been developed to explore research articles and experts, collaborators, and financial services. "},{"metadata":{},"cell_type":"markdown","source":"Recommender systems can be classified into Two types:\n\n> **Content-based recommenders**: suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person likes a particular item, he or she will also like an item that is similar to it. And to recommend that, it will make use of the user's past item metadata. A good example could be YouTube, where based on your history, it suggests you new videos that you could potentially watch.\n\n> **Collaborative filtering engines**: these systems are widely used, and they try to predict the rating or preference that a user would give an item-based on past ratings and preferences of other users. Collaborative filters do not require item metadata like its content-based counterparts."},{"metadata":{},"cell_type":"markdown","source":"Here we are going to implement **Content Based Filtering**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import Pandas\nimport pandas as pd\n\n# Loading Data sets\nfull_url='/kaggle/input/tmdb-movie-metadata/tmdb_5000_credits.csv'\n\nfull_url1='/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv'\n\ncredits = pd.read_csv(full_url)\nmovies=pd.read_csv(full_url1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing 1st 5 elements of credits dataset\ncredits.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing 1st 5 elements of movies dataset\nmovies.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the shapes of both the datasets\nprint(\"Credits:\",credits.shape)\nprint(\"Movies:\",movies.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column of credits data set\ncredits_renamed=credits.rename(index=str,columns={'movie_id':'id'})\ncredits_renamed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging both data sets\nmerge=movies.merge(credits_renamed,on='id')\nmerge.head()\n#to understand merge ,  merge work as joins","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping unnecessary columns \ncleaned=merge.drop(columns=['homepage','title_x','title_y','status','production_countries'])\ncleaned.head()\n## one aspect of cleaning is dropping unnecessary columns which won't help the task at hand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['overview'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['overview'].isnull().sum() \n#no nan or null values for overview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace NaN with an empty string\ncleaned['overview'] = cleaned['overview'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,3),min_df=3,analyzer='word')\n#refer : http://www.tfidf.com/\n\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(cleaned['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear_kernal is little bit more faster tha cosine_similarity but this dataset is not so big. So we can use cosine_similarity\n#from sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\n#cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n \n# Compute the cosine similarity matrix\ncosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cosine_sim.shape)\nprint(cosine_sim[0])\nprint(cosine_sim[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to define a function that takes in a movie title as an input and outputs a list of 10 most similar movies. Firstly, for this we need a reverse mapping fo movies titles and DataFrame incides. In outher words, we need a mechanism to identify the index of movie in out metadata DataFrame, given its title."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(cleaned.index, index=cleaned['original_title']).drop_duplicates()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_recommendations(title, cosine_sim=cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    print(sim_scores[0:5])\n    \n    print(\"--------------------------\")\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n    print(sim_scores[0:5])\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return cleaned['original_title'].iloc[movie_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the recommendation\nget_recommendations('Avatar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Enhancements Possible"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## From your new features, cast, crew, and keywords, \n## you need to extract the three most important actors, \n## the director and the keywords associated with that movie.\n\n## But first things first, your data is present in the form of \"stringified\" lists. \n## You need to convert them into a way that is usable for you.\n\n# Parse the stringified features into their corresponding python objects\nfrom ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres']\n\nfor feature in features:\n    cleaned[feature] = cleaned[feature].apply(literal_eval)\n    \n## about literal_eval()    \n## https://stackoverflow.com/questions/15197673/using-pythons-eval-vs-ast-literal-eval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets see the data stored for the 0th movie.  \ncleaned['crew'].values[0]\n\n## Notice : its an list of dict objects.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## user defn function , which finds the name of the director from \n## list od crew members\n\ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_list(x):\n    \n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n    \n    #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[ : 3 ]\n        return names\n\n    #Return empty list in case of missing/malformed data\n    return []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define new director, cast, genres and keywords features that are in a suitable form.\ncleaned['director'] = cleaned['crew'].apply(get_director)\n\nfeatures = ['cast', 'keywords', 'genres']\n\nfor feature in features:\n    cleaned[feature] = cleaned[feature].apply(get_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the new features of the first 3 films\ncleaned[['original_title', 'cast', 'director', 'keywords', 'genres']].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Akshay Kumar will be converted to akshaykumar and Akshay khanna -> akshaykhanna.\n#so that the vectorizer will consider akshaykumar and akhsaykhanna as different people.\n##replacing the blank space to no space askhay Kumar as akshaykumar to reduce the duplication\n\n# Function to convert all strings to lower case and strip names of spaces\ndef clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    cleaned[feature] = cleaned[feature].apply(clean_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#it will create a metadata(in string), combination of keyword,cast, director and genres. to feed it to vectorizer.\ndef create_metadata(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new metadata feature\ncleaned['metadata'] = cleaned.apply(create_metadata, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned[['metadata']].head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\n\ncount_matrix = count.fit_transform(cleaned['metadata'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset index of your main DataFrame and construct reverse mapping as before\n\n## cleaned = cleaned.reset_index()\nindices = pd.Series(cleaned.index, index = cleaned['original_title'])\nindices[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations(\"The Dark Knight Rises\", cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would like to humbly and sincerely thank my mentor [Rocky Jagtiani](https://www.linkedin.com/today/author/rocky-jagtiani-3b390649/). He is more of a friend to me then mentor. The Machine Learning course taught by him and various projects we did and are still doing is the best way to learn and skill in Data Science field. See https://datascience.suvenconsultants.com once for more."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}