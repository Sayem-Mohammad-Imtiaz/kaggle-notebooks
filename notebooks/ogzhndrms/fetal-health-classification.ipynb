{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import required base libraries.\n\nimport sys\nimport random\nimport time\nimport pickle\n\nimport IPython\n\nimport pandas as pd\nimport matplotlib\nimport numpy as np\nimport scipy as sp\nimport sklearn as sk\nimport plotly\n\nfrom IPython import display\nfrom pathlib import Path\n\nmain_path = Path(\"..\")\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"Matplotlib version: {matplotlib.__version__}\")\nprint(f\"Numpy version: {np.__version__}\")\nprint(f\"Scipy version: {sp.__version__}\")\nprint(f\"IPython version: {IPython.__version__}\")\nprint(f\"Sklearn version: {sk.__version__}\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Now import classification models and initialize visualisation tools.\n\n# Algorithms.\nfrom sklearn import (\n    tree, linear_model, neighbors, naive_bayes, ensemble,\n    discriminant_analysis, gaussian_process, neural_network,\n    multiclass,\n)\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Preprocessing tools.\nfrom sklearn.preprocessing import (\n    OneHotEncoder, OrdinalEncoder, LabelEncoder, MinMaxScaler,\n)\nfrom sklearn.metrics import (\n    classification_report, f1_score, precision_score,recall_score,\n)\n\n# Statistics\nfrom scipy.stats import boxcox, zscore\n\n# Collinearity analysis.\nfrom statsmodels.stats.outliers_influence import  variance_inflation_factor\n\n# Model selection tools.\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Copy models.\nfrom copy import deepcopy\nfrom sklearn import base\n\n# Visualization tools.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import express as px\n\nrnd_state=42\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\n    \"/kaggle/input/fetal-health-classification/fetal_health.csv\",\n    encoding=\"utf-8\"\n)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see is it balanced or not.\ndata.fetal_health.value_counts()\n\n# 2.0 and 3.0 are minority.\n# If i say 1 for all, approximately 77% of my predictions will be true.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()\n\n# No null values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop duplicates.\ndata.drop_duplicates(inplace=True, ignore_index=True)\n\ndata.info()\n\n# 13 rows deleted.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename columns.\n\ndata.rename(\n    columns={\n        \"baseline value\": \"baseline_value\",\n        \"fetal_health\": \"class\"\n    },\n    inplace=True\n)\n\ncolumns = list(data.columns)\ncolumns.remove(\"class\")\n\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, we know data contains full of numeric values.\nThere is no categorical column to convert it to numeric values.\n\n__Btw 1 -> Normal, 2 -> Suspect, 3 -> Pathological. There is no big or small relation between them.\nSo I used dummy encoding.__"},{"metadata":{},"cell_type":"markdown","source":"##Â See distributions for each feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"baseline_value\", color=\"class\")\n\n# Normal distribution type data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"accelerations\", color=\"class\")\n\n# Right skewed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"fetal_movement\", color=\"class\")\n\n# Right skewed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"uterine_contractions\", color=\"class\")\n\n# Normal like.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"light_decelerations\", color=\"class\")\n\n# Right skewed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"prolongued_decelerations\", color=\"class\")\n\n# Maybe we can categorize it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"abnormal_short_term_variability\", color=\"class\")\n\n# bi-model, also uniform like.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"mean_value_of_short_term_variability\", color=\"class\")\n\n# Right skewed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"percentage_of_time_with_abnormal_long_term_variability\", color=\"class\")\n\n# Right skewed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"mean_value_of_long_term_variability\", color=\"class\")\n\n# Right skewed but no transformation required.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_width\", color=\"class\")\n\n# Bimodel and it seems mixed..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_min\", color=\"class\")\n\n# Uniform.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_max\", color=\"class\")\n\n# Normal.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_number_of_peaks\", color=\"class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_number_of_zeroes\", color=\"class\")\n\n# Right skewed but i wont transform it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_mode\", color=\"class\")\n\n# Left skewed normal.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_mean\", color=\"class\")\n\n# Left skewed normal. I wont transform it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_median\", color=\"class\")\n\n# Left skewed normal. Mode, mean and median seems like same","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_variance\", color=\"class\")\n\n# Highly right skewed. This shows less variance in histograms.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x=\"histogram_tendency\", color=\"class\")\n\n# Highly right skewed. This shows less variance in histograms.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discriminant Analysis detects collinearity between independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transferred code for Variance Inflation Factor (VIF)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ReduceVIF(BaseEstimator, TransformerMixin):\n    def __init__(self, thresh=5.0, impute=False, impute_strategy='median'):\n        # From looking at documentation, values between 5 and 10 are \"okay\".\n        # Above 10 is too high and so should be removed.\n        self.thresh = thresh\n        \n        # The statsmodel function will fail with NaN values, as such we have to impute them.\n        # By default we impute using the median value.\n        # This imputation could be taken out and added as part of an sklearn Pipeline.\n        if impute:\n            self.imputer = Imputer(strategy=impute_strategy)\n\n    def fit(self, X, y=None):\n        print('ReduceVIF fit')\n        if hasattr(self, 'imputer'):\n            self.imputer.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        print('ReduceVIF transform')\n        columns = X.columns.tolist()\n        if hasattr(self, 'imputer'):\n            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n        return ReduceVIF.calculate_vif(X, self.thresh)\n\n    @staticmethod\n    def calculate_vif(X, thresh=5.0):\n        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n        dropped=True\n        while dropped:\n            variables = X.columns\n            dropped = False\n            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n            \n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n                dropped=True\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = ReduceVIF()\n\ncorrelated_features_dropped_df = transformer.fit_transform(data[columns], data[\"class\"])\n\ncorrelated_features_dropped_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect important features.\nfeature_selector = XGBClassifier()\nfeature_selector.fit(data[columns], data[\"class\"])\n\n# Create a dataframe for visualization and selection.\nfeature_selection_df = pd.DataFrame(columns=[\"Feature_Name\", \"Importance\", \"Cumulative_Importance\", \"Is_Correlated\"])\nfeature_selection_df[\"Feature_Name\"] = list(columns)\nfeature_selection_df[\"Importance\"] = feature_selector.feature_importances_\nfeature_selection_df.sort_values(inplace=True, ascending=False, by=\"Importance\")\nfeature_selection_df[\"Cumulative_Importance\"] = feature_selection_df.Importance.cumsum()\nfeature_selection_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_selection_df.loc[:, \"Is_Correlated\"] = feature_selection_df[feature_selection_df.isin(list(correlated_features_dropped_df.columns))].Feature_Name.isna()\n\nfeature_selection_df.reset_index(inplace=True, drop=True)\nfeature_selection_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* __mean_value_of_short_term_variability__ has .25 importance value. It can' t be discarded.  \n* __histogram_mean__ can' t be discarded.  \n* __abnormal_short_term_variabilty__ has 5.66 vif.\n* __histogram_max__, __histogram_mode__, __baseline_value__ has high vif value.\n* __histogram_width__ has __inf__ vif value.\n* __histogram_min__ and __histogram_median__ has high vif value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, columns selected.\n\ncolumns = list(feature_selection_df.iloc[[0, 1, 2, 3, 4, 5, 6, 10, 11, 14, 15, 17]].Feature_Name)\ncolumns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test models and get the first one."},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_algorithms = [\n    \n    # Ensemble Methods.\n    ensemble.AdaBoostClassifier(random_state=rnd_state),\n    ensemble.BaggingClassifier(random_state=rnd_state),\n    ensemble.ExtraTreesClassifier(random_state=rnd_state),\n    ensemble.GradientBoostingClassifier(random_state=rnd_state),\n    ensemble.RandomForestClassifier(random_state=rnd_state),\n    XGBClassifier(random_state=rnd_state),\n    LGBMClassifier(random_state=rnd_state),\n    \n    # Gaussian Processes.\n    gaussian_process.GaussianProcessClassifier(random_state=rnd_state),\n    \n    # Generalized Linear Methods.\n    linear_model.LogisticRegressionCV(random_state=rnd_state),\n    \n    # Naive Bayes.\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    # Nearest Neighbor.\n    neighbors.KNeighborsClassifier(),\n    \n    # Trees.\n    tree.DecisionTreeClassifier(random_state=rnd_state, max_depth=16, min_samples_leaf=1, min_samples_split=.2),\n    tree.ExtraTreeClassifier(random_state=rnd_state),\n    \n    # Discriminant Analysis.\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    # Neural Networks.\n    neural_network.MLPClassifier(max_iter=300, random_state=rnd_state),\n    \n    # Stacked Methods.\n    ensemble.StackingClassifier(\n        estimators=[\n            (\"adaboost\", ensemble.AdaBoostClassifier(random_state=rnd_state)),\n            (\"gradient\", ensemble.GradientBoostingClassifier(random_state=rnd_state)),\n            (\"knn\", neighbors.KNeighborsClassifier(n_jobs=-1))\n        ],\n        final_estimator=linear_model.LogisticRegression(random_state=rnd_state)\n    )\n]\n\ncv_split = model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=rnd_state)\n\nml_columns = [\n    \"Algorithm_Name\",\n    \"Algorithm_Parameters\",\n    \"Train_Balanced_Accuracy_Mean\",\n    \"Test_Balanced_Accuracy_Mean\",\n    \"Train_F1_Weighted_Mean\",\n    \"Test_F1_Weighted_Mean\",\n    \"Train_AUC_ROC_OVR_Weighted_Mean\",\n    \"Test_AUC_ROC_OVR_Weighted_Mean\",\n    \"Train_Balanced_Accuracy\",\n    \"Test_Balanced_Accuracy\",\n    \"Test_Accuracy_3*STD\",\n    \"Time (Mean)\",\n]\n\nml_compare = pd.DataFrame(columns=ml_columns)\n\ndata_used = data[columns]\nlabels = data[\"class\"]\n\nrow_index = 0\nfor alg in ml_algorithms:\n    ml_name = alg.__class__.__name__\n    ml_compare.loc[row_index, \"Algorithm_Name\"] = ml_name\n    ml_compare.loc[row_index, \"Algorithm_Parameters\"] = str(alg.get_params())\n    \n    # Cross validation.\n    cv_results = model_selection.cross_validate(\n        alg,\n        data_used,\n        labels,\n        cv=cv_split,\n        return_train_score=True,\n        scoring=\n            [\n                \"balanced_accuracy\",\n                \"f1_weighted\",\n                \"roc_auc_ovr_weighted\",\n            ],\n    )\n    \n    ml_compare.loc[row_index, \"Time (Mean)\"] = cv_results[\"fit_time\"].mean()\n    ml_compare.loc[row_index, \"Train_Balanced_Accuracy_Mean\"] = cv_results[\"train_balanced_accuracy\"].mean()\n    ml_compare.loc[row_index, \"Test_Balanced_Accuracy_Mean\"] = cv_results[\"test_balanced_accuracy\"].mean()\n    ml_compare.loc[row_index, \"Train_F1_Weighted_Mean\"] = cv_results[\"train_f1_weighted\"].mean()\n    ml_compare.loc[row_index, \"Test_F1_Weighted_Mean\"] = cv_results[\"test_f1_weighted\"].mean()\n    ml_compare.loc[row_index, \"Train_AUC_ROC_OVR_Weighted_Mean\"] = cv_results[\"train_roc_auc_ovr_weighted\"].mean()\n    ml_compare.loc[row_index, \"Test_AUC_ROC_OVR_Weighted_Mean\"] = cv_results[\"test_roc_auc_ovr_weighted\"].mean()\n    \n    ml_compare.loc[row_index, \"Test_Balanced_Accuracy\"] = str(cv_results[\"test_balanced_accuracy\"])\n    ml_compare.loc[row_index, \"Train_Balanced_Accuracy\"] = str(cv_results[\"train_balanced_accuracy\"])\n\n    # Worst case scenario.\n    ml_compare.loc[row_index, \"Test_Accuracy_3*STD\"] = cv_results[\"test_balanced_accuracy\"].std() * 3\n\n    row_index += 1\n\nml_compare.sort_values(by=[\"Test_AUC_ROC_OVR_Weighted_Mean\", \"Test_F1_Weighted_Mean\", \"Test_Balanced_Accuracy_Mean\"], ascending=False, inplace=True)\nml_compare\n\n# NOTE: Average precision can be added for evaluation.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now select the most performant algorithm, then evaluate it.\nml_compare.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a train test split.\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    data[columns],\n    data[\"class\"],\n    test_size=.3,\n    train_size=.7,\n    random_state=rnd_state,\n    shuffle=True,\n    stratify=data[\"class\"].values\n)\n\n# Create classifier with default parameters.\nclassifier = multiclass.OneVsRestClassifier(LGBMClassifier(class_weight=\"balanced\", random_state=rnd_state))\nclassifier.fit(X_train, y_train)\n\npredictions = classifier.predict(X_test)\n\nprecision = precision_score(y_test, predictions, average=\"weighted\")\nrecall = recall_score(y_test, predictions, average=\"weighted\")\nauc_roc = ml_compare.iloc[0][\"Test_AUC_ROC_OVR_Weighted_Mean\"]\nf1_weighted = ml_compare.iloc[0][\"Test_F1_Weighted_Mean\"]\n\nprint(f\"AUC ROC: {auc_roc}, F1 - Score: {f1_weighted}, Precision Score: {precision}, Recall Score: {recall}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print classification report.\nprint(classification_report(y_test, predictions, target_names=[\"Normal\", \"Suspect\", \"Pathological\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}