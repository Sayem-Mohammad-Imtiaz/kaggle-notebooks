{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1bff6dbc-abb1-44a4-86c4-3c70e9b18f5a"},"source":"# EVALUATING CLASSIFIERS FOR SMS SPAM DETECTION\n### Daniel Loden, May 2017\n\n# Overview\n\n\n----------\nThis document presents an evaluation of models for classifying SMS spam and ham messages based on message text. The following classifiers were assessed:\n\n - Naive Bayes;\n - random forest;\n - logistic regression (L1 and L2); and\n - support vector machine.\n\nMultiple classifiers were trained and compared using 10-fold cross-validation.  Of these, the support vector machine performed best, based on Cohen's kappa.  When applied to test data, the SVM performed fairly well, with kappa of 0.90 and F1-scores of 0.99 and 0.92 for the 'ham' (legitimate) and 'spam' (not legitimate) classes, respectively.\n\n# Data Processing\n\n\n----------\nA dataset containing 5,572 labelled records was loaded and split into training and testing sets, based on a 70:30 ratio."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f580356-03ef-e1a5-c63b-b659a69d0623"},"outputs":[],"source":"# Load packages\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import cohen_kappa_score, make_scorer, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# Read data\ndata = pd.read_csv('../input/spam.csv',\n                   encoding = 'ISO-8859-1')\ndata = data.ix[:, [1, 0]]\ndata.rename(columns={'v2':'text', 'v1':'ham_spam'}, inplace=True)\n\n# Code spam flag\nle = LabelEncoder()\ny = le.fit_transform(data['ham_spam'])\n\n# Create text variable\nX = data['text']\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=2008)"},{"cell_type":"markdown","metadata":{"_cell_guid":"81af6911-d6ae-5ae2-738f-49ec954180e3"},"source":"# Comparison of Classifiers\n\n----------\n\n## Assessment Approach\nMultiple classifiers were assessed to determine which one should be applied to the test dataset.  \n\nDue to the imbalance in classes (see below), Cohen's kappa was used as the performance metric, to account for chance agreement between actual and predicted values.  10-fold cross-validation was used to estimate performance on new data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08b1e9e8-5419-1dee-f7d3-ae09a2caf3ca"},"outputs":[],"source":"print('Proportion of spam messages in the training data:', round(np.mean(y_train), 2))\nkappa = make_scorer(cohen_kappa_score)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d70d4eaa-5347-63c5-099f-d0b5deb494af"},"source":"## Document-Term Matrix Specifications\nThe below specifications were used to create document-term matrices."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0078eb66-ad7c-7a36-2645-5e4678070c1e"},"outputs":[],"source":"count_vec = CountVectorizer(analyzer='word',\n                            stop_words='english',\n                            max_features=500)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6772905a-90ca-c2c2-8051-f0b5fb7f07e0"},"source":"## Naive Bayes\nNaive Bayes was tried first, as has been commonly used for spam detection.  This classifier performed fairly well."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a89f161-9e35-fec7-59a0-b029740c7633"},"outputs":[],"source":"nb = MultinomialNB()\n\nnb_clf = Pipeline([('Count vectorizer', count_vec),\n                   ('Naive Bayes', nb)])\n\nprint('Kappa (10-fold CV): ', \n      round(np.mean(cross_val_score(nb_clf, X_train, y_train, scoring=kappa, cv=10)), 3))"},{"cell_type":"markdown","metadata":{"_cell_guid":"2a785e74-07d1-099c-c84c-a6f0c41b7199"},"source":"## Random Forest\nRandom Forest did not perform as well as Naive Bayes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7020b5f-7762-7d69-0a3d-e57d030195cc"},"outputs":[],"source":"rf = RandomForestClassifier()\n\nrf_clf = Pipeline([('Count vectorizer', count_vec),\n                   ('Random Forest', rf)])\n\nprint('Kappa (10-fold CV): ', \n      round(np.mean(cross_val_score(rf_clf, X_train, y_train, scoring=kappa, cv=10)), 3))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f562298-44b8-8cbd-0817-2581784952be"},"source":"## Logistic Regression\n\n### L1 Regularisation\nLogistic regression with L1 regularisation did not perform as well as Naive Bayes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90cde27e-179e-4027-88c6-7a04a070aea5"},"outputs":[],"source":"lr = LogisticRegression(penalty='l1')\n\nlr_l1_clf = Pipeline([('Count vectorizer', count_vec),\n                      ('Logistic regression', lr)])\n\nprint('Kappa (10-fold CV): ', \n      round(np.mean(cross_val_score(lr_l1_clf, X_train, y_train, scoring=kappa, cv=10)), 3))"},{"cell_type":"markdown","metadata":{"_cell_guid":"92109bd1-35a6-9c9f-7f6c-1d8fbb30a5f9"},"source":"### L2 Regularisation\nHowever, logistic regression with L2 regularisation *did* perform better than Naive Bayes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d30ea0ec-e426-f069-80d5-5220d1dda99a"},"outputs":[],"source":"lr = LogisticRegression(penalty='l2')\n\nlr_l2_clf = Pipeline([('Count vectorizer', count_vec),\n                      ('Logistic regression', lr)])\n\nprint('Kappa (10-fold CV): ', \n      round(np.mean(cross_val_score(lr_l2_clf, X_train, y_train, scoring=kappa, cv=10)), 3))"},{"cell_type":"markdown","metadata":{"_cell_guid":"63169aff-4ada-28bd-b92e-74af3fdcfee0"},"source":"## Support Vector Machine\nThe linear SVM performed better than all other classifiers and was chosen as the final model to be tested."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6999326-842f-9b34-118c-c785f236012c"},"outputs":[],"source":"svm = SVC(kernel='linear')\n\nsvm_clf = Pipeline([('Count vectorizer', count_vec),\n                    ('SVM', svm)])\n\nprint('Kappa (10-fold CV): ', \n      round(np.mean(cross_val_score(svm_clf, X_train, y_train, scoring=kappa, cv=10)), 3))"},{"cell_type":"markdown","metadata":{"_cell_guid":"2efb1b5e-ed1b-6ed2-4b15-a52b989545eb"},"source":"# Support Vector Machine Performance on the Test Set\n\n----------\n\nAs expected, the SVM performed well on the test data, with strong F1-scores and Cohen's kappa.  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2ac78e1-2b5d-d18d-2ce4-be30b12fc841"},"outputs":[],"source":"svm_clf.fit(X_train, y_train)\ny_test_pred = svm_clf.predict(X_test)\nprint('Classification report: ')\nprint(\"----------------------\")\nprint(\"\")\nprint(classification_report(y_test, y_test_pred))\nprint(\"\")\nprint(\"Cohen's kappa:\")\nprint(\"--------------\")\nkappa = cohen_kappa_score(y_test, y_test_pred)\nprint(round(kappa, 2))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}