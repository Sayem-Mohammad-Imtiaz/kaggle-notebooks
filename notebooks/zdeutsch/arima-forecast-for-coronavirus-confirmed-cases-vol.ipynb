{"cells":[{"metadata":{"id":"xak6uX23mHzo","colab_type":"text"},"cell_type":"markdown","source":"# ARIMA forecast for Coronavirus confirmed cases volume"},{"metadata":{"id":"vs8jX80LhCu4","colab_type":"text"},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"id":"iuvVnU3NkZVf","colab_type":"text"},"cell_type":"markdown","source":"I will analyze and forecast coronavirus confirmed cases volume all over the world. Epidemy started in Wuhan in December 2019. On 2/11/2020, the virus is officially named COVID-19 by the World Health Organization.\nData comes from: https://github.com/CSSEGISandData/COVID-19.\n\nFirst we need to transform our dataset into series object containing date and comfirmed cases volumes."},{"metadata":{"id":"5xYIsxi5oyrn","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"sjyK_HICo5-Q","colab_type":"code","outputId":"cc713bc0-6b46-4e16-a4c3-7b217e589030","colab":{"base_uri":"https://localhost:8080/","height":215},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/novel-corona-virus-2019-dataset/time_series_covid_19_confirmed.csv')\ndf = df.fillna('unknow')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"2HetMQ01qykR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":422},"outputId":"49f32c9a-0781-415d-eb7b-2e7a7daa16d3","trusted":true},"cell_type":"code","source":"df = df.append(df.sum(numeric_only=True), ignore_index=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"AxGC0j_Bo-qk","colab_type":"code","outputId":"bbd8e92b-7d65-494f-d796-b04acad59816","colab":{"base_uri":"https://localhost:8080/","height":655},"trusted":true},"cell_type":"code","source":"df = df.iloc[df.shape[0] - 1][4:df.shape[1]]\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"NjtgMUmTlgi3","colab_type":"text"},"cell_type":"markdown","source":"As our series object is ready, we will split it into two, one for model development (dataset.csv) and the other for validation - last week(validation.csv)\n"},{"metadata":{"id":"Isa_URz5sfVl","colab_type":"code","outputId":"7628a85f-f039-43da-ad03-13fda2fffd8c","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"split_point = len(df) - 4\ndataset, validation = df[0:split_point], df[split_point:]\nprint('Dataset %d, Validation %d' % (len(dataset), len(validation)))\ndataset.to_csv('dataset.csv', header = False)\nvalidation.to_csv('validation.csv', header = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"yf40b0mjts19","colab_type":"text"},"cell_type":"markdown","source":"## Persistence"},{"metadata":{"id":"v0ca8yhvyFKU","colab_type":"text"},"cell_type":"markdown","source":"The first step before getting bogged in modeling is to establish a baseline of performance. This will provide a performance measure by which all more elaborate predictive models can be compared. Persistence is where the observation from the previous step is used as the prediction for the observation at the next time step"},{"metadata":{"id":"7W7zHjGNtwpM","colab_type":"code","outputId":"03e55645-4578-4b8c-dd5e-485c14c235de","colab":{"base_uri":"https://localhost:8080/","height":286},"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n# load data\nseries = pd.read_csv('dataset.csv', header=None, index_col=0, parse_dates=True, squeeze=True)\n# prepare data\nX = series.values\nX = X.astype('float32')\ntrain_size = int(len(X) * 0.50)\ntrain, test = X[0:train_size], X[train_size:]\n# walk-forward validation\nhistory = [x for x in train]\npredictions = list()\nfor i in range(len(test)):\n\t# predict\n\tyhat = history[-1]\n\tpredictions.append(yhat)\n\t# observation\n\tobs = test[i]\n\thistory.append(obs)\n\tprint('>Predicted=%.3f, Expected=%3.f' % (yhat, obs))\n# report performance\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"id":"QE2gigTSzeTs","colab_type":"text"},"cell_type":"markdown","source":"The example ends by printing the RMSE for the model. We can see that the persistence model achieved 4762. This means that on average, th model was wrong by about 4762 confirmed coronavirus cases for each prediction made."},{"metadata":{"id":"fe3epfTWuFz-","colab_type":"text"},"cell_type":"markdown","source":"## Summary Statistics"},{"metadata":{"id":"sjstRklhuJPo","colab_type":"code","outputId":"26ca3afe-d534-4488-cc0b-742efc16283a","colab":{"base_uri":"https://localhost:8080/","height":168},"trusted":true},"cell_type":"code","source":"series.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"1F54KcMH0Tgq","colab_type":"text"},"cell_type":"markdown","source":"Running the example provides a number of statistics to review:\n\n*   The mean is about 26213, which we might consider our level in this series\n*   The standard deviation (average spread from the mean) is relatively large at 23240 cases.\n*   The percentiles along ith the standard deviation suggest a large spread to the data\n\n\n\n"},{"metadata":{"id":"flVrCt-Quevg","colab_type":"text"},"cell_type":"markdown","source":"## Line and density plots"},{"metadata":{"id":"gvT7K4xMukpM","colab_type":"code","outputId":"fb0e2200-64cf-4da5-9fe0-f5040266c8a2","colab":{"base_uri":"https://localhost:8080/","height":331},"trusted":true},"cell_type":"code","source":"series.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"QTBFzkZD1uXF","colab_type":"text"},"cell_type":"markdown","source":"*   There is increasing trend over time which means that the dataset is non-stationary\n\n"},{"metadata":{"id":"hzb8bEefu5AY","colab_type":"code","outputId":"74435edf-946c-474c-b15f-690b71a024c4","colab":{"base_uri":"https://localhost:8080/","height":265},"trusted":true},"cell_type":"code","source":"plt.figure(1)\nplt.subplot(211)\nseries.hist()\nplt.subplot(212)\nseries.plot(kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"KGheCH3f2F0e","colab_type":"text"},"cell_type":"markdown","source":"\n\n*   The distribution is not Gaussian\n*   The distribution is left shifted and may be exponential or double Gaussian\n\n"},{"metadata":{"id":"0ZkoxNBnweUj","colab_type":"text"},"cell_type":"markdown","source":"## Manually cofigured ARIMA"},{"metadata":{"id":"Q_FiFu0o55Bs","colab_type":"text"},"cell_type":"markdown","source":"ARIMA(p, d, q) requires 3 parameters and is traditionally configured manually. We will try to *guess* probable values, starting from *d*.\n\nThe time series is non-stationary. We can make it stationary by differencing the series."},{"metadata":{"id":"8ixlE5n8wgl6","colab_type":"code","outputId":"ffe0d877-2db9-4c10-c998-1ef151290cd9","colab":{"base_uri":"https://localhost:8080/","height":118},"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\n# create a differenced time series\ndef difference(dataset):\n\tdiff = list()\n\tfor i in range(1, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - 1]\n\t\tdiff.append(value)\n\treturn pd.Series(diff)\n\n# difference data\nstationary = difference(X)\nstationary.index = series.index[1:]\n# check if stationary\nresult = adfuller(stationary)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))\n# save\nstationary.to_csv('stationary.csv', header = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"VtCnRgJS6vQE","colab_type":"text"},"cell_type":"markdown","source":"Running an example outputs the result of statistical significance test of whether the 1-lag differenced series is stationary. Specifically, the augmented Dickey-Fuller test. The results show that te test static value -3.897 is smaller than the critical value at 5% of -2.992. This suggests we can reject null hypothesis and conclude that 1-lag differenced series is stationary. Then at least one level of differencing is required.  *d* >= 1\n\nThe next step is to select the lag values for the Autoregression (AR) and Moving Average (MA) parameters, *p* and *q* respectively. We can do this by reviewing Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)."},{"metadata":{"id":"tssQxkoLxMP-","colab_type":"code","outputId":"412643e4-0918-4aab-c746-18f8f16b5384","colab":{"base_uri":"https://localhost:8080/","height":281},"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nplt.figure()\nplt.subplot(211)\nplot_acf(series, lags=24, ax=plt.gca())\nplt.subplot(212)\nplot_pacf(series, lags=24, ax=plt.gca())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1two-m0v87vB","colab_type":"text"},"cell_type":"markdown","source":"\n\n*   The ACF shows significant lag for 1-2 months\n*   The PACF does not show a significant lag\n\nGood starting point will be p = 1 and q = 0\n\n\n"},{"metadata":{"id":"jSNzWPjJyqW3","colab_type":"text"},"cell_type":"markdown","source":"# Grid search for ARIMA"},{"metadata":{"id":"g9j-hBoD9i3I","colab_type":"text"},"cell_type":"markdown","source":"We will use a grid search to explore all combinations of the ARIMA parameters and find the best one."},{"metadata":{"id":"DWSLF8Jgysc1","colab_type":"code","outputId":"c2c6578c-23c9-4c43-deef-8e9ce674db49","colab":{"base_uri":"https://localhost:8080/","height":202},"trusted":true},"cell_type":"code","source":"import warnings\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\ndef evaluate_arima_model(X, arima_order):\n\t# prepare training dataset\n\tX = X.astype('float32')\n\ttrain_size = int(len(X) * 0.50)\n\ttrain, test = X[0:train_size], X[train_size:]\n\thistory = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(len(test)):\n\t\tmodel = ARIMA(history, order=arima_order)\n\t\tmodel_fit = model.fit(disp=0)\n\t\tyhat = model_fit.forecast()[0]\n\t\tpredictions.append(yhat)\n\t\thistory.append(test[t])\n\t# calculate out of sample error\n\trmse = sqrt(mean_squared_error(test, predictions))\n\treturn rmse\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n\tdataset = dataset.astype('float32')\n\tbest_score, best_cfg = float(\"inf\"), None\n\tfor p in p_values:\n\t\tfor d in d_values:\n\t\t\tfor q in q_values:\n\t\t\t\torder = (p,d,q)\n\t\t\t\ttry:\n\t\t\t\t\trmse = evaluate_arima_model(dataset, order)\n\t\t\t\t\tif rmse < best_score:\n\t\t\t\t\t\tbest_score, best_cfg = rmse, order\n\t\t\t\t\tprint('ARIMA%s RMSE=%.3f' % (order,rmse))\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\tprint('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n\n# evaluate parameters\np_values = range(0,13)\nd_values = range(0, 4)\nq_values = range(0, 13)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(X, p_values, d_values, q_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_cfg = (0, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"mAxF59C20Qs5","colab_type":"text"},"cell_type":"markdown","source":"# Review Residual Errors"},{"metadata":{"id":"v8aOb4uG92Ey","colab_type":"text"},"cell_type":"markdown","source":"A good final check is to review residual forecast errors. Ideally, the distribution should be Gaussian with a zero mean."},{"metadata":{"id":"WB2R7qNez-h0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"d4be7ce8-3bc7-4d70-a0e5-07fd1bd30e53","trusted":true},"cell_type":"code","source":"train_size = int(len(X) * 0.50)\ntrain, test = X[0:train_size], X[train_size:]\n# walk-forward validation\nhistory = [x for x in train]\npredictions = list()\nfor i in range(len(test)):\n\t# predict\n\tmodel = ARIMA(history, order=best_cfg)\n\tmodel_fit = model.fit(disp=0)\n\tyhat = model_fit.forecast()[0]\n\tpredictions.append(yhat)\n\t# observation\n\tobs = test[i]\n\thistory.append(obs)\n# errors\nresiduals = [test[i]-predictions[i] for i in range(len(test))]\nresiduals = pd.DataFrame(residuals)\nplt.figure()\nplt.subplot(211)\nresiduals.hist(ax=plt.gca())\nplt.subplot(212)\nresiduals.plot(kind='kde', ax=plt.gca())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"t_ZOSya6_SYu","colab_type":"text"},"cell_type":"markdown","source":"## Validate Model"},{"metadata":{"id":"s0ZTtqbE_YBJ","colab_type":"text"},"cell_type":"markdown","source":"Now we can load the model and use it in a rolling - forecast manner, updating the transform and model for each time step. "},{"metadata":{"id":"mTvC9fkJ4lmV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":534},"outputId":"9bb9eedf-c222-4d3a-f73f-ad7fdc39ab40","trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom scipy.stats import boxcox\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom math import exp\nfrom math import log\nimport numpy\n\n# load and prepare datasets\ndataset = pd.read_csv('dataset.csv', header=None, index_col=0, parse_dates=True, squeeze=True)\nX = dataset.values.astype('float32')\nhistory = [x for x in X]\nvalidation = pd.read_csv('validation.csv', header=None, index_col=0, parse_dates=True, squeeze=True)\ny = validation.values.astype('float32')\n# run model\nmodel = ARIMA(history, order=best_cfg)\nmodel_fit = model.fit(disp=0)\n# make first prediction\npredictions = list()\nyhat = model_fit.forecast()[0][0]\npredictions.append(yhat)\nhistory.append(y[0])\nprint('>Predicted=%.3f, Expected=%3.f' % (yhat, y[0]))\n# rolling forecasts\nfor i in range(1, len(y)):\n  # predict\n  model = ARIMA(history, order=best_cfg)\n  model_fit = model.fit(disp=0)\n  yhat = model_fit.forecast()[0][0]\n  predictions.append(yhat)\n  # observation\n  obs = y[i]\n  history.append(obs)\n  print('>Predicted=%i, Expected=%i' % (yhat, obs))\n# report performance\nrmse = sqrt(mean_squared_error(y, predictions))\nprint('RMSE: %i' % rmse)\n#predict next day\n# predict\nmodel = ARIMA(history, order=(0,1,0))\nmodel_fit = model.fit(disp=0)\nyhat = model_fit.forecast()[0][0]\npredictions.append(yhat)\nprint('>Predicted next day volume=%i' % (yhat))\nplt.plot(y)\nplt.plot(predictions, color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"_Y1g6Hch_2s_","colab_type":"text"},"cell_type":"markdown","source":"Predicted number of confirmed coronavirus cases for the next day not available in dataset (currently 21.02) is 78807. A plot of the predictions compared to validation dataset is also provided. The forecast has the characteristics of a presistence forecast. This suggests that although this time serie has obvious tred , it is still reasonably difficult problem. "}],"metadata":{"colab":{"name":"Coronavirus.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}