{"cells":[{"metadata":{"_uuid":"e40b0df4c619a775e865cd302bc78173ff03e97d"},"cell_type":"markdown","source":"# Predicting Health Insurance Costs"},{"metadata":{"_uuid":"9818a01e351cfc3090ec362041135f8f8932e3b1"},"cell_type":"markdown","source":"## Introduction\n\nIn this project, we are assigned data about health insurance contractors, and we aim to construct a model that could predict a given contractor's insurance charges.\n\n## Exploratory Analysis\n\nTo start, we will perform an exploratory analysis on the data. This will help us understand the data, and decide what needs to be done to the data to preprocess it, and what techniques are necessary to build the best model. First, lets' take a look at the data.\n"},{"metadata":{"trusted":true,"_uuid":"f41c829282befe56a1832ca9dabb525cef6ede9c"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import data CSV into dataframe\ndata_raw = pd.read_csv(\"../input/insurance.csv\")\n\n#show sample of data\ndisplay(data_raw.head(n=7))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21bdce78e3ed65556b735ee4bc3a5d117870e662"},"cell_type":"markdown","source":"#### The data consists of 7 columns/variables:\n\n- **Age:** The age of the contractor in years expressed as a numerical value.\n- **Sex:** The contractor's sex, taking string values as either \"male\" or \"female\".\n- **BMI:** The contactor's Body Mass Index, which represents the ratio of the body's mass to its height, expressed in numerical values.\n- **Children:** Number of dependents of contractors, expressed as a numerical value.\n- **Smoker:** Denotes whether the contractor smokes tobacco or not. Expressed in string values of \"yes\" and \"no\".\n- **Region:** The region within the US where the contractor is. It is expressed in string values of \"northeast\", \"northwest\", \"southeast\", and \"southwest\".\n- **Charges:** The monetary amount that was billed by the health insurance company, expressed in a numerical value.\n\nTo make the data more manageable, we will change all binary string values (**sex** and **smoker** features) to binary numerical values (0 and 1). **Regions** is a non-binary categorical feature. We can perform one-hot encoding on it, but we wil first analyze the data to determine if this is necessary."},{"metadata":{"trusted":true,"_uuid":"9471cc63252e72935e5a6752dc10d3d3c4c839eb"},"cell_type":"code","source":"#smoker = 1; non-smoker = 0\ndata = data_raw.replace(['yes','no'], [1,0])\n#female = 1; male = 0\ndata = data.replace(['female','male'], [1,0])\n\ndisplay(data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97a188eeedb577585b8aa6d2559359337832db29"},"cell_type":"markdown","source":"After the changes, we will create graphical representations of the data to better understand it."},{"metadata":{"trusted":true,"_uuid":"f0182a68d2d4e9597875ac15b680d75c528d2166"},"cell_type":"code","source":"#prepare subplots, with 2 columns and 2 rows\nfig1, ((ax11,ax12),(ax13,ax14)) = plt.subplots(2,2)\n#set full plot size\nfig1.set_size_inches(15,12)\n\n#Create a pie chart of the region distribution\nax11.pie(data.groupby(\"region\").size().values, labels=data.groupby('region').size().keys(), autopct='%1.1f%%')\nax11.set_title(\"Region Distribution\", fontsize=20)\nax11.axis('equal')\n#Create a pie chart of the sex distribution\nax13.pie(data.groupby(\"sex\").size().values, labels=data.groupby('sex').size().keys(), autopct='%1.1f%%', startangle=90)\nax13.set_title(\"Sex Distribution\", fontsize=20)\nax13.axis('equal')\n#Create a pie chart of the smoker distribution\nax12.pie(data.groupby(\"smoker\").size().values, labels=data.groupby('smoker').size().keys(), autopct=\"%1.1f%%\")\nax12.set_title(\"Smoker Distribution\", fontsize=20)\nax12.axis('equal')\n#Create a histogram of the children/dependnet distribution\nax14.hist('children', data=data,edgecolor =' 0.2', bins = 5)\nax14.set_title(\"Dependent Distribution\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc878274b113929ced2527b0a8a5a7df080b051f"},"cell_type":"markdown","source":"> The contractors are divided between the 4 regions nearly equally, and are also split between the two sexes equally, as shown in the two pie charts on the left. The majority of contractors are non-smokers, and it is evident that contractors without children dominate, while the frequency decreases as the number of children goes up."},{"metadata":{"trusted":true,"_uuid":"e3b2328363a97034ffc0fe8f7f88cfba84b11d6b"},"cell_type":"code","source":"#Prepare subplots with 1 row, 2 columns\nfig21, (ax21,ax22) = plt.subplots(1,2)\nfig21.set_size_inches(15,6)\n\n#Create a density curve of the BMI distribution\nsns.kdeplot(data['bmi'], ax=ax21, shade=True, legend=False)\nax21.set_xlabel(\"BMI\", fontsize=14)\nax21.set_ylabel(\"Frequency\", fontsize=14)\nax21.set_title(\"BMI Distribution\", fontsize=20)\n\n#Create a histogram of the age distribution\nax22.hist('age', data=data, bins=10, edgecolor='0.2')\nax22.set_xlabel(\"Age\", fontsize=14)\nax22.set_ylabel(\"Frequency\", fontsize=14)\nax22.set_title(\"Age Distribution\", fontsize=20)\n\n#Create a separate subplot for the charges distribution\n#This is because this is a more important graph, and is better to take up two columns\nfig22, ax23 = plt.subplots()\nfig22.set_size_inches(15,6)\n\n#Create density plot of charges distribution\nsns.kdeplot(data['charges'], ax=ax23, shade=True, legend=False)\nax23.set_xlabel(\"Charges\", fontsize=14)\nax23.set_ylabel(\"Frequency\", fontsize=14)\nax23.set_title(\"Charges Distribution\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"659fabd6cd9c14af4cdfc1fb39ac43fa63d65f82"},"cell_type":"markdown","source":"> - The BMI distribution is bell-shaped and symmetrical\n- The Age distribution is mostly uniform, except for ages below 25 where the histogram peaks.\n- The charges distribution is skewed to the left.\n\nLeft skewness is typical of monetary distributions. A logarithmic transformation is often useful for such distributions, and particularly monetary distributions, given that money values tend to be thought of as multiplicative more=so than additive. For example, a $100 difference between $50,100 and $50,000 might seem insignificant, whereas the same difference between $150 and $250 is not.\n\nThe feature of interest is the **charges** feature. Let's examine the effect of categorical features on the distribution of billed charges. The categorical features are: Region, Sex, abd Smoker. We will also treat the \"children\" feature as categorical and examine it in the same manner, because it only has 6 values."},{"metadata":{"trusted":true,"_uuid":"985f74a3e8a145739c8f3b2717d6c9854fbe8b07"},"cell_type":"code","source":"#Prepare subplots of 4 rows and 1 column\nfig3, (ax31,ax32,ax33,ax34) = plt.subplots(4,1)\nfig3.set_size_inches(14,28)\n\n#Add 4 density curves to subplot ax31 for the charges distribution, each for one of the 4 regions\nsns.kdeplot(data.loc[data[\"region\"] == 'southeast'][\"charges\"], ax=ax31, shade=True, label=\"southeast\")\nsns.kdeplot(data.loc[data[\"region\"] == 'southwest'][\"charges\"], ax=ax31, shade=True, label=\"southwest\")\nsns.kdeplot(data.loc[data[\"region\"] == 'northwest'][\"charges\"], ax=ax31, shade=True, label=\"northwest\")\nsns.kdeplot(data.loc[data[\"region\"] == 'northeast'][\"charges\"], ax=ax31, shade=True, label=\"northeast\")\nax31.set_ylabel(\"Frequency\", fontsize=15)\nax31.set_xlabel('Charges', fontsize=15)\nax31.set_title(\"Effect of Regions on Cost\", fontsize=20)\n\n#Add 2 density curves to subplot ax32 for the charges distribution, each for one of the 2 sexes\n#Remember: female = 1, male = 0\nsns.kdeplot(data.loc[data[\"sex\"] == 0][\"charges\"], ax=ax32, shade=True, label=\"male\")\nsns.kdeplot(data.loc[data[\"sex\"] == 1][\"charges\"], ax=ax32, shade=True, label=\"female\")\nax32.set_ylabel(\"Frequency\", fontsize=15)\nax32.set_xlabel('Charges', fontsize=15)\nax32.set_title(\"Effect of Sex on Cost\", fontsize=20)\n\n#Add 6 density curves to subplot ax33 for the charges distribution, each for one caregory of the number of dependents\nsns.kdeplot(data.loc[data[\"children\"] == 0][\"charges\"], ax=ax33, shade=True, label=\"0 children\")\nsns.kdeplot(data.loc[data[\"children\"] == 1][\"charges\"], ax=ax33, shade=True, label=\"1 children\")\nsns.kdeplot(data.loc[data[\"children\"] == 2][\"charges\"], ax=ax33, shade=True, label=\"2 children\")\nsns.kdeplot(data.loc[data[\"children\"] == 3][\"charges\"], ax=ax33, shade=True, label=\"3 children\")\nsns.kdeplot(data.loc[data[\"children\"] == 4][\"charges\"], ax=ax33, shade=True, label=\"4 children\")\nsns.kdeplot(data.loc[data[\"children\"] == 5][\"charges\"], ax=ax33, shade=True, label=\"0 children\")\nax33.set_ylabel(\"Frequency\", fontsize=15)\nax33.set_xlabel('Charges', fontsize=15)\nax33.set_title(\"Effect of Number of Dependents on Cost\", fontsize=20)\n\n#Add 2 density curves to subplot ax34 for the charges distribution, one for smokers and one for non-smokers\nsns.kdeplot(data.loc[data[\"smoker\"] == 1][\"charges\"], ax=ax34, shade=True, label='smoker')\nsns.kdeplot(data.loc[data[\"smoker\"] == 0][\"charges\"], ax=ax34, shade=True, label='non-smoker')\nax34.set_ylabel(\"Frequency\", fontsize=15)\nax34.set_xlabel('Charges', fontsize=15)\nax34.set_title(\"Effect of Smoking on Cost\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9837257c971f0a96787cb8ce0e3de4d28982f1da"},"cell_type":"markdown","source":"> - The difference in region introduces no change to the cost distribution\n- Sex also has no effect on cost\n- All values of **children** do not effect the cost distribution, except for children = 5. But it is a small difference.\n- **Smoking** makes a significant difference in cost, and hence is an important feature\n\n> In Conclusion, region, sex, and children do not effect cost significantly, whereas smoking influences it very significantly.\nNext, we will analyze continuous features' effects on cost using scatter plots."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"18a29ab14181933e525c7f8b5577188dd89efb8d"},"cell_type":"code","source":"#Prepare subplots with 1 row and 2 columns\nfig4, (ax41,ax42) = plt.subplots(1,2)\nfig4.set_size_inches(15,6)\n\n#create a scatterplot with best linear fit for Age vs. Charges\nsns.regplot(\"age\", \"charges\", data, ax=ax41)\nax41.set_title(\"Effect of Age on Cost\", fontsize=20)\nax41.set_xlabel(\"Age\", fontsize=15)\nax41.set_ylabel(\"Charges\", fontsize=15)\n\n#create a scatterplot with best linear fit for BMI vs. Charges\nsns.regplot(\"bmi\", \"charges\", data, ax=ax42)\nax42.set_title(\"Effect of BMI on Cost\", fontsize=20)\nax42.set_xlabel(\"BMI\", fontsize=15)\nax42.set_ylabel(\"Charges\", fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73072070a7ed8d1ebf3e9946d672f4102e783e96"},"cell_type":"markdown","source":"> Looking at the **Age** scatter plot, we can see that there are three distinct clusters. Most of the data resides in the bottom cluster, which more packed (smaller thickness) than the other two clusters. Each of the clusters exhibit a linear form, however, a simple least-squares regression between **Age** and **Charges** would not be good, because, as we can see, the best line fit has much error associated with it. a More Complicated model would be needed.\nWe can attempt to break up the data by the important feature, **Smoking**, and see if this would yield any of the clusters we see.\n\n> **BMI** correlates with charges, but we could also attempt to break it up by by smoking to see if we get any clearer correlation."},{"metadata":{"trusted":true,"_uuid":"93a90e506f58402499622843512bd7074c9dab17"},"cell_type":"code","source":"#Prepare subplots with 1 row and 2 columns\nfig5, (ax51,ax52) = plt.subplots(1,2)\nfig5.set_size_inches(15,6)\n\n#create 2x Age vs Charges scatter plots on the same axes; one for smokers and one for non-smokers\nsns.regplot(\"age\", \"charges\", data.loc[data[\"smoker\"] == 1], ax=ax51)\nsns.regplot(\"age\", \"charges\", data.loc[data[\"smoker\"] == 0], ax=ax51)\nax51.set_title(\"Effect of Age on Cost\", fontsize=20)\nax51.set_xlabel(\"Age\", fontsize=15)\nax51.set_ylabel(\"Charges\", fontsize=15)\nax51.legend((\"smoker\", \"non-smoker\"))\n\n#create 2x BMI vs Charges scatter plots on the same axes; one for smokers and one for non-smokers\nsns.regplot(\"bmi\", \"charges\", data.loc[data[\"smoker\"] == 1], ax=ax52)\nsns.regplot(\"bmi\", \"charges\", data.loc[data[\"smoker\"] == 0], ax=ax52)\nax52.set_title(\"Effect of BMI on Cost\", fontsize=20)\nax52.set_xlabel(\"BMI\", fontsize=15)\nax52.set_ylabel(\"Charges\", fontsize=15)\nax52.legend((\"smoker\", \"non-smoker\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54efc9eefc5476acca6469a4ff5148aaba833b27"},"cell_type":"markdown","source":"> When separating Age vs. Charges scatter plot by **smoking**, we see that all smoker contractors fell into the upper two clusters that were previously observed. The non-smoking section of the data included all of the bottom cluster, plus some of the middle cluster.\n\n> In the BMI scatter plot, we can observe that for non-smokers, BMI has nearly no effect on cost. For smokers, however, there a strong trend. This means that it was the smoker data that was driving the correlation we witnessed above between all contractors' BMI and insurance charges.\n\nBased on these observations, a regression tree would be the ideal model for this problem."},{"metadata":{"trusted":true,"_uuid":"7d8191f1e95f6154000b1c07dfe0908ac320b0dd"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import r2_score\n\n#Create a dataframe of the input data X\n#Create a dataframe of the output data Y\nX = data.drop(['charges','sex','region'], axis=1)\nY = data[\"charges\"]\n\n#split data into training and testing sets; 20% testing size.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\n#Call training model\nregress=DecisionTreeRegressor()\n\n#Parameters for Grid Search\nparams_tree = {\"max_depth\":np.arange(3,6), \"min_samples_split\":np.arange(2,8), \"max_leaf_nodes\":np.arange(2,20)}\n\n#Call and fit grid search\ngrid_tree=GridSearchCV(regress, params_tree)\ngrid_tree.fit(X_train,Y_train)\n\n#Obtain optimized parameters\ngrid_tree.best_estimator_\npredictions = grid_tree.predict(X_test)\nr2_score(predictions, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e778d516072b258d3f9dd28e45de461d029c7a60"},"cell_type":"code","source":"#Now, call training model with optimized parameters\nclf_tree = DecisionTreeRegressor(max_depth=3,max_leaf_nodes=8,splitter=\"best\", random_state=1)\nclf_tree.fit(X_train,Y_train)\npredictions_tree = clf_tree.predict(X_test)\nprint(r2_score(predictions_tree, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b588b8eb0d37d06738bbb0238ce2035a4a9b14"},"cell_type":"code","source":"#Show which features are most significant\nfeats = {}\nfor feature, importance in zip(X_train.columns, clf_tree.feature_importances_):\n    feats[feature] = importance #add the name/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance',ascending=False).plot(kind='bar', rot=45)\nprint(importances.sort_values(by='Gini-importance',ascending=False))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bc11b8e97aebfb6b7158d9fca3d20aa9f80f86e"},"cell_type":"markdown","source":"To better understand this learning algorithm, we can visualize the decision tree in the graph below:\n\n** Edit: ** I could not get the package pydotplus to work on kaggle."},{"metadata":{"trusted":true,"_uuid":"c3247673b0a1b3398ce5f500cecd4a0e8dc23259"},"cell_type":"code","source":"import pydotplus\nfrom sklearn import tree\nimport collections\nfrom PIL import Image\n\ndot_data = tree.export_graphviz(clf_tree, feature_names=X_train.columns, out_file=None,filled=True, rounded=True)\n\n\ngraph = pydotplus.graph_from_dot_data(dot_data)\ncolors = ( 'lightblue','orange')\nedges = collections.defaultdict(list)\nfor edge in graph.get_edge_list():\n    edges[edge.get_source()].append(int(edge.get_destination()))\n\nfor edge in edges:\n    edges[edge].sort()    \n    for i in range(2):\n        dest = graph.get_node(str(edges[edge][i]))[0]\n        \n        dest.set_fillcolor(colors[i])\ngraph.write_png('tree.png')        \n\nfrom IPython.display import Image\nImage(\"tree.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3de21451628cb352b06e4f19ca13ed732ae423a"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n#Train and fit Linear Regression\nlinreg = LinearRegression()\nlinreg.fit(X_train,Y_train)\npredictions_linear = linreg.predict(X_test)\nr2_score(predictions_linear, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"529e7dc5cc539d2ec3e61747330e775df5d6b746"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\n#Train and fit AdaBoost Regressor\nadaboost = AdaBoostRegressor(n_estimators=5, learning_rate=2)\nadaboost.fit(X_train, Y_train)\npredictions_boost = adaboost.predict(X_test)\nr2_score(predictions_boost, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7dcb4448afd5ba9670298eca1026245ff68e3ac"},"cell_type":"code","source":"#Show which features are most significant\nfeats = {}\nfor feature, importance in zip(X_train.columns, adaboost.feature_importances_):\n    feats[feature] = importance #add the name/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance',ascending=False).plot(kind='bar', rot=45)\nprint(importances.sort_values(by='Gini-importance',ascending=False))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"241698552857586dcbe31b4a43a348aa299272d4"},"cell_type":"markdown","source":"# Conclusion:\n\nIn conclusion, we have seen that features like \"region\" and \"sex\" do not influence insurance charges. This was further verified by showing the feauture impotrances of the decision tree regressor and the AdaBoost regressor. As it was predicted during the initial analysis, smoking formed the largest influence on costs, followed by BMI and age, respectively. The number of dependents was shown to have little albeit presence influence (less than 2%).\n\nThe accuracy for Linear Regression was low, 65%. Whereas both AdaBoost and DecisionTree had accuracy scores near 80%. Although the scores are similar, DecisionTree would likely be preferable when working with large data, because it would require less running time and processing."},{"metadata":{"trusted":false,"_uuid":"a8d716983bda48fb5810e1750f0c960cffcca977"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"}},"nbformat":4,"nbformat_minor":1}