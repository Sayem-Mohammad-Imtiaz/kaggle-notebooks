{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Config\nseed = 42\n# sử dụng 90% cho trainning và 10% cho testing\ntraining_split_ratio = 0.9\nnum_epochs = 5\n\n# If the following values are False, the models will be downloaded and not computed\ncompute_histograms = True\ntrain_whole_images = True\ntrain_patches = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --quiet --upgrade pip\n!pip install --quiet highresnet==0.10.2\n!pip install --quiet unet==0.7.3\n!pip install --quiet torchio==0.18.11\n!apt-get -qq install tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torch==1.7.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nimport enum\nimport random; random.seed(seed)\nimport warnings\nimport tempfile\nimport subprocess\nimport multiprocessing\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid, save_image\ntorch.manual_seed(seed)\n\nimport torchio as tio\nfrom torchio import AFFINE, DATA\n\nimport numpy as np\nimport nibabel as nib\nfrom unet import UNet\nfrom scipy import stats\nimport SimpleITK as sitk\nimport matplotlib.pyplot as plt\n\nfrom IPython import display\nfrom tqdm.notebook import tqdm\n\nprint('TorchIO version:', tio.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_dataset = \"../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\nroot_dir = Path(path_dataset)\nflair_path = sorted(list(root_dir.glob('*/*_flair.nii')))\nt1_path = sorted(list(root_dir.glob('*/*_t1.nii')))\nt1ce_path = sorted(list(root_dir.glob('*/*_t1ce.nii')))\nt2_path = sorted(list(root_dir.glob('*/*_t2.nii')))\n\nseg_path = sorted(list(root_dir.glob('*/*_seg.nii')))\n\nimage_paths = []\nlabel_paths = []\n\ndef convert_path_str(arr):\n  for i in arr:\n    if \"355\" in str(i):\n      continue\n    flair_ = str(i).replace(\"flair\", \"flair\")\n    t1_ = str(i).replace(\"flair\", \"t1\")\n    t1ce_ = str(i).replace(\"flair\", \"t1ce\")\n    t2_ = str(i).replace(\"flair\", \"t2\")\n    image_paths.append({\"flair\": flair_, \"t1\": t1_, \"t1ce\": t1ce_, \"t2\": t2_})\n  \nconvert_path_str(flair_path)\n\nfor i in seg_path:\n  label_paths.append(str(i))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subjects = []\nfor (image_path, label_path) in zip(image_paths, label_paths):\n    subject = tio.Subject(\n        flair=tio.ScalarImage(image_path[\"flair\"]),\n        t1=tio.ScalarImage(image_path[\"t1\"]),\n        t1ce=tio.ScalarImage(image_path[\"t1ce\"]),\n        t2=tio.ScalarImage(image_path[\"t2\"]), \n        brain=tio.LabelMap(label_path),\n    )\n    subjects.append(subject)\ndataset = tio.SubjectsDataset(subjects)\nprint('Dataset size:', len(dataset), 'subjects')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_transform = tio.Compose([\n    tio.ToCanonical(),\n    tio.RandomMotion(p=0.2),\n    tio.RandomBiasField(p=0.3),\n    tio.RandomNoise(p=0.5),\n    tio.RandomFlip(axes=(0,)),\n    tio.OneOf({\n        tio.RandomAffine(): 0.8,\n        tio.RandomElasticDeformation(): 0.2,\n    }),\n])\n\nvalidation_transform = tio.Compose([\n    tio.ToCanonical(),\n])\n\nnum_subjects = len(dataset)\nnum_training_subjects = int(training_split_ratio * num_subjects)\n\ntraining_subjects = subjects[:num_training_subjects]\nvalidation_subjects = subjects[num_training_subjects:]\n\ntraining_set = tio.SubjectsDataset(\n    training_subjects, transform=training_transform)\n\nvalidation_set = tio.SubjectsDataset(\n    validation_subjects, transform=validation_transform)\n\nprint('Training set:', len(training_set), 'subjects')\nprint('Validation set:', len(validation_set), 'subjects')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_transform = tio.Compose([\n    tio.ToCanonical(),\n    tio.RandomMotion(p=0.2),\n    tio.RandomBiasField(p=0.3),\n    tio.RandomNoise(p=0.5),\n    tio.RandomFlip(axes=(0,)),\n    tio.OneOf({\n        tio.RandomAffine(): 0.8,\n        tio.RandomElasticDeformation(): 0.2,\n    }),\n])\n\nvalidation_transform = tio.Compose([\n    tio.ToCanonical(),\n])\n\nnum_subjects = len(dataset)\nnum_training_subjects = int(training_split_ratio * num_subjects)\n\ntraining_subjects = subjects[:num_training_subjects]\nvalidation_subjects = subjects[num_training_subjects:]\n\ntraining_set = tio.SubjectsDataset(\n    training_subjects, transform=training_transform)\n\nvalidation_set = tio.SubjectsDataset(\n    validation_subjects, transform=validation_transform)\n\nprint('Training set:', len(training_set), 'subjects')\nprint('Validation set:', len(validation_set), 'subjects')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport torch\nimport torch\nimport torch.nn as nn\nimport os\nimport nibabel as nib\nfrom albumentations import Compose, HorizontalFlip\nfrom skimage.util import montage\nimport matplotlib.pyplot as plt\n\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport time\nimport torch.nn.functional as F\n\nfrom IPython.display import clear_output\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(Conv3D -> BN -> ReLU) * 2\"\"\"\n    def __init__(self, in_channels, out_channels, num_groups=8):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            #nn.BatchNorm3d(out_channels),\n            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            #nn.BatchNorm3d(out_channels),\n            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n            nn.ReLU(inplace=True)\n          )\n\n    def forward(self,x):\n        return self.double_conv(x)\n\n    \nclass Down(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.MaxPool3d(2, 2),\n            DoubleConv(in_channels, out_channels)\n        )\n    def forward(self, x):\n        return self.encoder(x)\n\n    \nclass Up(nn.Module):\n\n    def __init__(self, in_channels, out_channels, trilinear=True):\n        super().__init__()\n        \n        if trilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose3d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n            \n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        diffZ = x2.size()[2] - x1.size()[2]\n        diffY = x2.size()[3] - x1.size()[3]\n        diffX = x2.size()[4] - x1.size()[4]\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2, diffZ // 2, diffZ - diffZ // 2])\n\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n    \nclass Out(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet3d(nn.Module):\n    def __init__(self, in_channels, n_classes, n_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.n_classes = n_classes\n        self.n_channels = n_channels\n\n        self.conv = DoubleConv(in_channels, n_channels)\n        self.enc1 = Down(n_channels, 2 * n_channels)\n        self.enc2 = Down(2 * n_channels, 4 * n_channels)\n        self.enc3 = Down(4 * n_channels, 8 * n_channels)\n        self.enc4 = Down(8 * n_channels, 8 * n_channels)\n\n        self.dec1 = Up(16 * n_channels, 4 * n_channels)\n        self.dec2 = Up(8 * n_channels, 2 * n_channels)\n        self.dec3 = Up(4 * n_channels, n_channels)\n        self.dec4 = Up(2 * n_channels, n_channels)\n        self.out = Out(n_channels, n_classes)\n\n    def forward(self, x):\n        x1 = self.conv(x)\n        x2 = self.enc1(x1)\n        x3 = self.enc2(x2)\n        x4 = self.enc3(x3)\n        x5 = self.enc4(x4)\n\n        mask = self.dec1(x5, x4)\n        mask = self.dec2(mask, x3)\n        mask = self.dec3(mask, x2)\n        mask = self.dec4(mask, x1)\n        mask = self.out(mask)\n\n        return mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n# Số lượng kênh đầu vào\nAXIS_CONCAT = 1\nSPATIAL_DIMENSIONS = 2, 3, 4\n\n# định nghĩa tùy chọn tranning hoặc validation\nclass Action(enum.Enum):\n    TRAIN = 'Training'\n    VALIDATE = 'Validation'\n\n# chuẩn bị dữ liệu cho mỗi batch\ndef prepare_batch(batch, device):\n    # Ghép các hình ảnh đầu vào lại với nhau\n    input_flair = batch['flair'][DATA].to(device)\n    input_t1 = batch['t1'][DATA].to(device)\n    input_t1ce = batch['t1ce'][DATA].to(device)\n    input_t2 = batch['t2'][DATA].to(device)\n\n    # Chuyển đầu vào về đúng kích thước mô hình\n    inputs = torch.cat(\n        (input_flair, input_t1, input_t1ce, input_t2), \n        dim=AXIS_CONCAT)\n    inputs = torch.movedim(inputs, (0, 1, 2, 3, 4), (0, 1, 4, 3, 2))\n    \n    # Tạo ra các hình ảnh mask cho từng trường hợp: WT, TC, ET\n    mask_tumors = batch['brain'][DATA].to(device)\n\n    mask_WT = mask_tumors.clone()\n    mask_WT[mask_WT == 1] = 1\n    mask_WT[mask_WT == 2] = 1\n    mask_WT[mask_WT == 4] = 1\n\n    mask_TC = mask_tumors.clone()\n    mask_TC[mask_TC == 1] = 1\n    mask_TC[mask_TC == 2] = 0\n    mask_TC[mask_TC == 4] = 1\n\n    mask_ET = mask_tumors.clone()\n    mask_ET[mask_ET == 1] = 0\n    mask_ET[mask_ET == 2] = 0\n    mask_ET[mask_ET == 4] = 1\n\n    # Chuyển đầu ra về đúng kích thước mô hình\n    masks = torch.cat(\n        (mask_WT, mask_TC, mask_ET),\n        dim=AXIS_CONCAT)\n    targets = torch.movedim(masks, (0, 1, 2, 3, 4), (0, 1, 4, 3, 2))\n\n    return inputs, targets\n\n# def get_dice_score(output, target, epsilon=1e-9):\n#   # Tính dice score\n#   p0 = output\n#   g0 = target\n#   p1 = 1 - p0\n#   g1 = 1 - g0\n#   tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n#   fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n#   fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n#   num = 2 * tp\n#   denom = 2 * tp + fp + fn + epsilon\n#   dice_score = num / denom\n\n#   return dice_score\n\n# def get_dice_loss(output, target):\n#   # Tính loss với dice score  \n#   return 1 - get_dice_score(output, target)\n\n############### FocalTversky Loss ###################\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nALPHA = 0.7\nGAMMA = 2\nBETA = 0.3\n\ndef get_focalTverskyLoss(inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):     \n    #flatten label and prediction tensors\n    inputs = inputs.reshape(-1)\n    targets = targets.reshape(-1)\n\n    #True Positives, False Positives & False Negatives\n    TP = (inputs * targets).sum()    \n    FP = ((1-targets) * inputs).sum()\n    FN = (targets * (1-inputs)).sum()\n\n    Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n    FocalTversky = (1 - Tversky)**gamma\n\n    return FocalTversky\n\ndef forward(model, inputs):\n  # Thực hiện truyền tiến trong model\n  with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    inputs = inputs.float()\n    inputs = inputs.to(device)\n    logits = model(inputs)\n    logits = logits.float()\n  \n  return logits\n\ndef get_model_and_optimizer(device):\n    model = UNet3d(in_channels=4, n_classes=3, n_channels=24).to(device)\n    optimizer = torch.optim.AdamW(model.parameters())\n    return model, optimizer\n\ndef run_epoch(epoch_idx, action, loader, model, optimizer):\n    is_training = action == Action.TRAIN\n    epoch_losses = []\n    model.train(is_training)\n    for batch_idx, batch in enumerate(tqdm(loader)):\n        inputs, targets = prepare_batch(batch, device)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(is_training):\n            logits = forward(model, inputs)\n#             probabilities = F.softmax(logits, dim=AXIS_CONCAT)\n            probabilities = torch.sigmoid(logits)\n#             batch_losses = get_dice_loss(probabilities, targets)\n            batch_losses = get_focalTverskyLoss(probabilities, targets)\n            batch_loss = batch_losses.mean()\n            if is_training:\n                batch_loss.backward()\n                optimizer.step()\n            epoch_losses.append(batch_loss.item())\n    epoch_losses = np.array(epoch_losses)\n    print(f'{action.value} mean loss: {epoch_losses.mean():0.3f}')\n\ndef train(num_epochs, training_loader, validation_loader, model, optimizer, weights_stem, start):\n    run_epoch(0, Action.VALIDATE, validation_loader, model, optimizer)\n    for epoch_idx in range(1, num_epochs + 1):\n        print('Starting epoch', epoch_idx)\n        run_epoch(epoch_idx, Action.TRAIN, training_loader, model, optimizer)\n        run_epoch(epoch_idx, Action.VALIDATE, validation_loader, model, optimizer)\n        torch.save(model.state_dict(), f'./{weights_stem}_whole_epoch_{epoch_idx+start}.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_batch_size = 1\nvalidation_batch_size = 1 * training_batch_size\n\ntraining_loader = torch.utils.data.DataLoader(\n    training_set,\n    batch_size=training_batch_size,\n    shuffle=True,\n    num_workers=multiprocessing.cpu_count(),\n)\n\nvalidation_loader = torch.utils.data.DataLoader(\n    validation_set,\n    batch_size=validation_batch_size,\n    num_workers=multiprocessing.cpu_count(),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model, optimizer = get_model_and_optimizer(device)\ntrain_whole_images = True\nload_pretrain = True\nif train_whole_images:\n    weights_stem = 'whole_images'\n    if load_pretrain:\n        weights_path = '../input/weight-unet3d/whole_images_whole_epoch_8.pth'\n        start = 9\n        model.load_state_dict(torch.load(weights_path))\n    train(num_epochs, training_loader, validation_loader, model, optimizer, weights_stem, start)\nelse:\n    weights_path = '/content/drive/MyDrive/BigData/Project/Computer Vision Project/EDA/weights/patches_epoch_2.pth'\n    model.load_state_dict(torch.load(weights_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import torch.nn.functional as F\n# import torch.nn as nn\n\n# ALPHA = 0.8\n# GAMMA = 2\n\n# def get_focalTverskyLoss(inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):     \n#     #flatten label and prediction tensors\n#     inputs = inputs.view(-1)\n#     targets = targets.view(-1)\n\n#     #True Positives, False Positives & False Negatives\n#     TP = (inputs * targets).sum()    \n#     FP = ((1-targets) * inputs).sum()\n#     FN = (targets * (1-inputs)).sum()\n\n#     Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n#     FocalTversky = (1 - Tversky)**gamma\n\n#     return FocalTversky","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_focalLoss(inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n#     #flatten label and prediction tensors\n#     inputs = inputs.view(-1)\n#     targets = targets.view(-1)\n\n#     #first compute binary cross-entropy \n#     BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n#     BCE_EXP = torch.exp(-BCE)\n#     focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n\n#     return focal_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_dice_score(output, target, epsilon=1e-9):\n#   # Tính dice score\n#   p0 = output\n#   g0 = target\n#   p1 = 1 - p0\n#   g1 = 1 - g0\n#   tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n#   fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n#   fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n#   num = 2 * tp\n#   denom = 2 * tp + fp + fn + epsilon\n#   dice_score = num / denom\n\n#   return dice_score\n\n# def get_dice_loss(output, target):\n#   # Tính loss với dice score  \n#   return 1 - get_dice_score(output, target)\n\n# def get_BCEDiceLoss(output, target):\n#     dice_loss = get_dice_loss(output, target)\n#     bce_loss = F.binary_cross_entropy(output, target, reduction='mean')\n    \n#     return bce_loss + dice_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}