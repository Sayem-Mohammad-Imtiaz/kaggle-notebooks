{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-19T13:36:09.009617Z","iopub.execute_input":"2021-08-19T13:36:09.010044Z","iopub.status.idle":"2021-08-19T13:36:09.035483Z","shell.execute_reply.started":"2021-08-19T13:36:09.009953Z","shell.execute_reply":"2021-08-19T13:36:09.034329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:09.039941Z","iopub.execute_input":"2021-08-19T13:36:09.040282Z","iopub.status.idle":"2021-08-19T13:36:10.272869Z","shell.execute_reply.started":"2021-08-19T13:36:09.040224Z","shell.execute_reply":"2021-08-19T13:36:10.271943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load The Dataset\n\nfrom loading the dataset we know that the dataset has 12 features. They are chemical parameters in wine. In this project I try to classify quality of wine. From basic analysis we know there is not any missing value in the dataset. Almost of all feature has float data type except **quality** parameter. Then, I continue to EDA. ","metadata":{}},{"cell_type":"code","source":"path ='../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv'\nrawData = pd.read_csv(path)\nrawData","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:10.276572Z","iopub.execute_input":"2021-08-19T13:36:10.276891Z","iopub.status.idle":"2021-08-19T13:36:10.341768Z","shell.execute_reply.started":"2021-08-19T13:36:10.276861Z","shell.execute_reply":"2021-08-19T13:36:10.340724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rawData.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:10.343714Z","iopub.execute_input":"2021-08-19T13:36:10.344041Z","iopub.status.idle":"2021-08-19T13:36:10.400021Z","shell.execute_reply.started":"2021-08-19T13:36:10.344011Z","shell.execute_reply":"2021-08-19T13:36:10.399009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rawData.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:10.401326Z","iopub.execute_input":"2021-08-19T13:36:10.401652Z","iopub.status.idle":"2021-08-19T13:36:10.41164Z","shell.execute_reply.started":"2021-08-19T13:36:10.401623Z","shell.execute_reply":"2021-08-19T13:36:10.410407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rawData.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:10.413075Z","iopub.execute_input":"2021-08-19T13:36:10.413441Z","iopub.status.idle":"2021-08-19T13:36:10.437798Z","shell.execute_reply.started":"2021-08-19T13:36:10.413409Z","shell.execute_reply":"2021-08-19T13:36:10.436512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndata = rawData.copy()\ndata1= data.drop('quality', axis=1)\nscaler = MinMaxScaler()\ndataScaled = pd.DataFrame(scaler.fit_transform(data), columns=[rawData.columns])\ndataST = dataScaled.drop('quality', axis=1)\ndataST","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:30:18.979395Z","iopub.execute_input":"2021-08-19T14:30:18.979735Z","iopub.status.idle":"2021-08-19T14:30:19.020301Z","shell.execute_reply.started":"2021-08-19T14:30:18.979705Z","shell.execute_reply":"2021-08-19T14:30:19.019267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataST\ny = rawData['quality']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nxTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:10.490962Z","iopub.execute_input":"2021-08-19T13:36:10.491368Z","iopub.status.idle":"2021-08-19T13:36:10.500281Z","shell.execute_reply.started":"2021-08-19T13:36:10.491337Z","shell.execute_reply":"2021-08-19T13:36:10.499301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nThe data exploratory analysis it self I broke down into two parts there are univariate analysis and bivariate analysis.","metadata":{}},{"cell_type":"markdown","source":"##  Univariate Analysis\n\nIn the univariate analysis I try to understand the distribution each feature visually and statitstic test. The data of each feature I visualize with histogram to see the pattern of the data. The data looks have normal distribution in a glance but I need validate this visual result with statistic test. I am using saphiro wilk test to know the distribution of each feature. From saphiro test we know that all feature doesn't have normal distribution.\n\n","metadata":{}},{"cell_type":"code","source":"def histPlot(col):\n    sns.histplot(x=col, data=rawData)\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:10.502607Z","iopub.execute_input":"2021-08-19T13:36:10.503069Z","iopub.status.idle":"2021-08-19T13:36:10.511803Z","shell.execute_reply.started":"2021-08-19T13:36:10.503036Z","shell.execute_reply":"2021-08-19T13:36:10.510846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in rawData.columns:\n    histPlot(col)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:10.513405Z","iopub.execute_input":"2021-08-19T13:36:10.51384Z","iopub.status.idle":"2021-08-19T13:36:13.855728Z","shell.execute_reply.started":"2021-08-19T13:36:10.513808Z","shell.execute_reply":"2021-08-19T13:36:13.85443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls = []\na = 0\ncolName = [j for j in rawData.columns]\nwhile a < 4:\n    for i in range(5):\n        ls.append((a, i))\n    a += 1\nfig, ax = plt.subplots(3, 5, figsize=(25, 15))\nfor k in range(12):\n    sns.boxplot(ax=ax[ls[k][0], ls[k][1]], y=colName[k], data=rawData)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:13.857458Z","iopub.execute_input":"2021-08-19T13:36:13.857928Z","iopub.status.idle":"2021-08-19T13:36:15.988553Z","shell.execute_reply.started":"2021-08-19T13:36:13.857841Z","shell.execute_reply":"2021-08-19T13:36:15.987492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xquality=rawData['quality'].value_counts()\nsns.barplot(y=xquality, x=xquality.index)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:15.992019Z","iopub.execute_input":"2021-08-19T13:36:15.992341Z","iopub.status.idle":"2021-08-19T13:36:16.175796Z","shell.execute_reply.started":"2021-08-19T13:36:15.992311Z","shell.execute_reply":"2021-08-19T13:36:16.174771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Saphiro Wilk-Test\n\nfrom scipy import stats\n\nfor i in dataST.columns:\n    stat, p = stats.shapiro(dataST[i])\n    if p > 0.05:\n        print('{} feature has normal distribution (p ={})'.format(i, p))\n    else:\n        print('{} feature has not normal distribution (p = {})'.format(i, p))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:36:15.938595Z","iopub.execute_input":"2021-08-19T14:36:15.938939Z","iopub.status.idle":"2021-08-19T14:36:15.954401Z","shell.execute_reply.started":"2021-08-19T14:36:15.938909Z","shell.execute_reply":"2021-08-19T14:36:15.953262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bivariate Analysis\n\nIn bivariate analysis I try to know the pattern and correlation between predictor and target variable (**quality**). From boxplot we know that some predictor like **alcohol, sulphates, and citric acid** has positive correlation with target variable. From literaure we know that **sulphate and alcohol** apppears in fermentation process. So, it's make sense if the **sulphate and alcohol** has positive correlation wiht quality of wine. The sulphate and alcohol value indicate age of wine.\n\nHeatmap shows us the correlation between each feature. We can know some feature has strong correlation with other feature but it's not strong enough. ","metadata":{}},{"cell_type":"code","source":"ls1 = []\nb = 0\ncolName1 = [j for j in dataST.columns]\nwhile b < 4:\n    for i in range(5):\n        ls1.append((b, i))\n    b += 1\nfig, ax1 = plt.subplots(3, 5, figsize=(25, 15))\nfor k in range(11):\n    sns.boxplot(ax=ax1[ls1[k][0], ls1[k][1]], y=dataST[colName1[k]], x=y)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:16.176992Z","iopub.execute_input":"2021-08-19T13:36:16.177279Z","iopub.status.idle":"2021-08-19T13:36:19.115049Z","shell.execute_reply.started":"2021-08-19T13:36:16.177252Z","shell.execute_reply":"2021-08-19T13:36:19.113842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\ncorr = dataST.corr()\nsns.heatmap(corr, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:19.11639Z","iopub.execute_input":"2021-08-19T13:36:19.116694Z","iopub.status.idle":"2021-08-19T13:36:20.182705Z","shell.execute_reply.started":"2021-08-19T13:36:19.116666Z","shell.execute_reply":"2021-08-19T13:36:20.18133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\n\nI am using **Feature Importance** parameter from extra trees model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(x_train, y_train)\nfeatImportance=pd.Series(model.feature_importances_, index=x_train.columns)\nfeatImportance.nlargest(5).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:20.18431Z","iopub.execute_input":"2021-08-19T13:36:20.184718Z","iopub.status.idle":"2021-08-19T13:36:20.888141Z","shell.execute_reply.started":"2021-08-19T13:36:20.184678Z","shell.execute_reply":"2021-08-19T13:36:20.886872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y.apply(lambda value : 1 if value >= 7 else 0)\ny.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:20.889473Z","iopub.execute_input":"2021-08-19T13:36:20.889781Z","iopub.status.idle":"2021-08-19T13:36:20.90101Z","shell.execute_reply.started":"2021-08-19T13:36:20.889749Z","shell.execute_reply":"2021-08-19T13:36:20.899906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataST[['alcohol', 'total sulfur dioxide', 'sulphates', 'volatile acidity', 'density']]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:41:47.609966Z","iopub.execute_input":"2021-08-19T13:41:47.610375Z","iopub.status.idle":"2021-08-19T13:41:47.619253Z","shell.execute_reply.started":"2021-08-19T13:41:47.61034Z","shell.execute_reply":"2021-08-19T13:41:47.618495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Oversampling\n\nBecause of dataset that we use unbalance, we need to balance it. It's to avoid bias in predicting label. I am using oversampling with SMOTE to balance the data. It's generate syntetic label of the data and we can avoid the bias.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\noversample = SMOTE()\nx_train, y_train = oversample.fit_resample(x_train, y_train)\ny_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:20.921154Z","iopub.execute_input":"2021-08-19T13:36:20.921599Z","iopub.status.idle":"2021-08-19T13:36:21.102734Z","shell.execute_reply.started":"2021-08-19T13:36:20.921567Z","shell.execute_reply":"2021-08-19T13:36:21.101594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Building Model\n\nIn this process I build three different model. There are **KNN, Logistic Regression and SVM**. From model building result, we can conclude that **KNN** best performance among the other models. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV , cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn_model = knn.fit(x_train, y_train)\ny_pred = knn_model.predict(x_test)\nacc = accuracy_score(y_test, y_pred)\nprint('accuracy score of KNN :{}'.format(acc))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:21.103794Z","iopub.execute_input":"2021-08-19T13:36:21.104106Z","iopub.status.idle":"2021-08-19T13:36:21.135899Z","shell.execute_reply.started":"2021-08-19T13:36:21.104078Z","shell.execute_reply":"2021-08-19T13:36:21.134961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_params = {'n_neighbors':[2, 3, 5, 7, 9]}\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:21.137033Z","iopub.execute_input":"2021-08-19T13:36:21.137347Z","iopub.status.idle":"2021-08-19T13:36:22.014293Z","shell.execute_reply.started":"2021-08-19T13:36:21.137319Z","shell.execute_reply":"2021-08-19T13:36:22.013503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best Parameters:', knn_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:22.015365Z","iopub.execute_input":"2021-08-19T13:36:22.015762Z","iopub.status.idle":"2021-08-19T13:36:22.021058Z","shell.execute_reply.started":"2021-08-19T13:36:22.015732Z","shell.execute_reply":"2021-08-19T13:36:22.019881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=2)\nopt_knn = knn.fit(x_train, y_train)\ny_predopt = opt_knn.predict(x_test)\naccuracy_score(y_test, y_predopt)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:22.022403Z","iopub.execute_input":"2021-08-19T13:36:22.02272Z","iopub.status.idle":"2021-08-19T13:36:22.063922Z","shell.execute_reply.started":"2021-08-19T13:36:22.022689Z","shell.execute_reply":"2021-08-19T13:36:22.062785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_predopt))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:36:22.065138Z","iopub.execute_input":"2021-08-19T13:36:22.065502Z","iopub.status.idle":"2021-08-19T13:36:22.077125Z","shell.execute_reply.started":"2021-08-19T13:36:22.065469Z","shell.execute_reply":"2021-08-19T13:36:22.075849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodelLog = LogisticRegression()\nmodelLog.fit(x_train, y_train)\npredLog = modelLog.predict(x_test)\naccuracylog = accuracy_score(y_test, predLog)\naccuracylog","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:46.298715Z","iopub.execute_input":"2021-08-19T13:43:46.299234Z","iopub.status.idle":"2021-08-19T13:43:46.324542Z","shell.execute_reply.started":"2021-08-19T13:43:46.299193Z","shell.execute_reply":"2021-08-19T13:43:46.323655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, predLog))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:44:48.228908Z","iopub.execute_input":"2021-08-19T13:44:48.229277Z","iopub.status.idle":"2021-08-19T13:44:48.240011Z","shell.execute_reply.started":"2021-08-19T13:44:48.229221Z","shell.execute_reply":"2021-08-19T13:44:48.238818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nmodelsvm = SVC(kernel='poly')\nmodelsvm.fit(x_train, y_train)\npredsvm = modelsvm.predict(x_test)\naccsvm = accuracy_score(y_test, predsvm)\naccsvm","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:58:53.985812Z","iopub.execute_input":"2021-08-19T13:58:53.986361Z","iopub.status.idle":"2021-08-19T13:58:54.083127Z","shell.execute_reply.started":"2021-08-19T13:58:53.986326Z","shell.execute_reply":"2021-08-19T13:58:54.082052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, predsvm))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:58:56.957985Z","iopub.execute_input":"2021-08-19T13:58:56.958358Z","iopub.status.idle":"2021-08-19T13:58:56.9692Z","shell.execute_reply.started":"2021-08-19T13:58:56.958327Z","shell.execute_reply":"2021-08-19T13:58:56.968171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}