{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Basic NLP Classify\n**In this kernel, I will try to classify \"Sentiments\" with different classifier models.**\n* [Import Data](#1)\n* [Sentiment Value Count](#2)\n* [Word Value Count](#3)\n* [Data Preparation](#4)\n* [NLP Preparation](#5)\n* [Building Models](#6)\n* [Model Training and Prediction](#7)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# **Import Data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/stockmarket-sentiment-dataset/stock_data.csv')\nprint(data.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As everyone can see we have 2 columns in this dataset:**\n1.  \"Text\"      : Text with special characters and numbers\n2. \" Sentiment\" : Positive and Negative ones\n\nLet's see value counts and most used words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport plotly.graph_objects as go\nfrom plotly.offline import iplot\nimport plotly.express as px\n\nfig = px.bar(x=data.Sentiment.unique(),y=[data.Sentiment.value_counts()],color=[\"1\",\"-1\"],text=data.Sentiment.value_counts())\nfig.update_traces(hovertemplate=\"Sentiment:'%{x}' Counted: %{y}\")\nfig.update_layout(title={\"text\":\"Sentiment Counts\"},xaxis={\"title\":\"Sentiment\"},yaxis={\"title\":\"Count\"})\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wordList = list()\nfor i in range(len(data)):\n    temp = data.Text[i].split()\n    for k in temp:\n        wordList.append(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nwordCounter = Counter(wordList)\ncountedWordDict = dict(wordCounter)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\nsortedWordDict[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num = 100\nlist1 = list()\nlist2 = list()\nfor i in range(num):\n    list1.append(wordCounter.most_common(num)[i][0])\n    list2.append(wordCounter.most_common(num)[i][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = px.bar(x=list1,y=list2,color=list2,hover_name=list1,hover_data={'Word':list1,\"Count\":list2})\nfig2.update_traces(hovertemplate=\"Word:'%{x}' Counted: %{y}\")\nfig2.update_layout(title={\"text\":\"Word Counts\"},xaxis={\"title\":\"Words\"},yaxis={\"title\":\"Count\"})\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\n\nwordList2 = \" \".join(wordList)\nstopwordCloud = set(stopwords.words(\"english\"))\nwordcloud = WordCloud(stopwords=stopwordCloud,max_words=2000,background_color=\"white\",min_font_size=3).generate_from_frequencies(countedWordDict)\nplt.figure(figsize=[13,10])\nplt.axis(\"off\")\nplt.title(\"Most used words\",fontsize=20)\nplt.imshow(wordcloud)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# Data Preparation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First of all, We need to change negative ones to zeros for our NN\nprint(\"***********Before************\")\nprint(data.Sentiment.head(10))\ndata.Sentiment = data.Sentiment.replace(-1,0)\nprint(\"***********After*************\")\nprint(data.Sentiment.head(10))\nfig = px.bar(x=data.Sentiment.unique(),y=[data.Sentiment.value_counts()],color=[\"1\",\"0\"],text=data.Sentiment.value_counts())\nfig.update_traces(hovertemplate=\"Sentiment:'%{x}' Counted: %{y}\")\nfig.update_layout(title={\"text\":\"Sentiment Counts\"},xaxis={\"title\":\"Sentiment\"},yaxis={\"title\":\"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Secondly, It's not very important but I wanna use same sizes of values due to overfitting\ndata2 = data.sort_values(by=\"Sentiment\")\ndata2 = data2.reset_index().iloc[0:,1:3]\nprint(\"2105:\",data2[\"Sentiment\"][2105])\nprint(\"2106:\",data2[\"Sentiment\"][2106])\ndata3 = data2.iloc[0:2106*2]\nprint(\"New value counts\")\nprint(data3.Sentiment.value_counts())\ndata = data3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# NLP Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize, WordNetLemmatizer\n\nps = PorterStemmer()\nlemma = WordNetLemmatizer()\nstopwordSet = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So let's print one by one to see what is going on\nprint(\"1)\",data['Text'][0])\ntext = re.sub('[^a-zA-Z]',\" \",data['Text'][0]) # clearing special characters and numbers\nprint(\"2)\",text)\ntext = text.lower()                            # lower\nprint(\"3)\",text)\ntext = word_tokenize(text,language='english')  # split\nprint(\"4)\",text)\ntext1 = [word for word in text if not word in stopwordSet] #clearing stopwords like \"to\", \"it\", \"over\"\ntext2 = [lemma.lemmatize(word) for word in text]           #same thing\ntext = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet] # I prefer using both but as you can see they are same\nprint(\"5.1)\",text1)\nprint(\"5.2)\",text2)\nprint(\"5)\",text)\ntext = \" \".join(text)                          # list -> string\nprint(\"6)\",text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textList = list()\nfor i in range(len(data)):\n    text = re.sub('[^a-zA-Z]',\" \",data['Text'][i])\n    text = text.lower()\n    text = word_tokenize(text,language='english')\n    text = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet]\n    text = \" \".join(text)\n    textList.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ncv = CountVectorizer(max_features=5001)  # you can change max_features to see different results\nx = cv.fit_transform(textList).toarray() # strings to 1 and 0\n#cvs = x.sum(axis=0)\n#print(cvs)          # to see word sum column by column\n\ny = data[\"Sentiment\"]\n\npca = PCA(n_components=256) # you can change n_components to see different results\nx = pca.fit_transform(x)    # fits 5001 columns to 256 with minimal loss\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=21) # splitting x and y for train/test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n# Building Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nmodelList = []\nmodelList.append((\"LogisticReg\",LogisticRegression()))\nmodelList.append((\"GaussianNB\",GaussianNB()))\nmodelList.append((\"BernoulliNB\",BernoulliNB()))\nmodelList.append((\"DecisionTree\",DecisionTreeClassifier()))\nmodelList.append((\"RandomForest\",RandomForestClassifier()))\nmodelList.append((\"KNeighbors\",KNeighborsClassifier(n_neighbors=5)))\nmodelList.append((\"SVC\",SVC()))\nmodelList.append((\"XGB\",XGBClassifier()))\n\ndef train_predict(x_train,x_test,y_train,y_test):\n    for name, classifier in modelList:\n        classifier.fit(x_train,y_train)\n        y_pred = classifier.predict(x_test)\n        print(\"{} Accuracy: {}\".format(name,accuracy_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.utils import plot_model\n\ndef build_model():\n    model = Sequential()\n    \n    model.add(Dense(units=16,activation=\"relu\",init=\"uniform\",input_dim=x.shape[1]))\n    model.add(Dense(units=16,activation=\"relu\",init=\"uniform\"))\n    model.add(Dense(units=1,activation=\"sigmoid\",init=\"uniform\"))\n    \n    optimizer = Adam(lr=0.0001,beta_1=0.9,beta_2=0.999)\n    #optimizer = RMSprop(lr=0.0001,rho=0.9)\n    \n    model.compile(optimizer=optimizer,metrics=[\"accuracy\"],loss=\"binary_crossentropy\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nplot_model(model,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Model Training and Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train,y_train,epochs=15,verbose=1)\ny_pred3 = model.predict_classes(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predict(x_train,x_test,y_train,y_test)\nprint(\"ANN Accuracy: \",accuracy_score(y_test,y_pred3.ravel()))\nprint(\"ANN Confusion Matrix\")\nprint(confusion_matrix(y_test,y_pred3.ravel()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thanks for reading, I'm open to your advices.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}