{"cells":[{"metadata":{"_uuid":"8c610f07-c2bf-4e5f-8bea-74816e6e0040","_cell_guid":"13744573-a887-4a66-99c1-d54677463360","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# In[32]:\n\n\n#importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# In[33]:\n\n\n# read Data \ndf= pd.read_csv(\"data.csv\")\n\n\n# In[34]:\n\n\n# Exploratrory Data analysis\n\ndf.info() # no null values\ndf.shape # 33 features present\n\n\n# In[38]:\n\n\ndf = df.drop(labels = ['id', 'Unnamed: 32'], axis = 1) #drop unwanted columns id , Unnamed: 32\ndf.describe()\n\ndf.head()\n\n\n# In[9]:\n\n\n# check for duplicates\n\nduplicate = df.duplicated()\nprint (duplicate.sum())\n\ndf[duplicate]\n\n# no duplicates found\n\n\n# In[45]:\n\n\n# check and Treat outliers if present for columns\n\ncols = list(df.columns)\n\ncols = cols.remove('diagnosis')\n\nplt.figure(figsize = (32,32))\n\ndf.boxplot(column = cols)\n\n\n# In[50]:\n\n\n# columns perimeter_mean, area_mean, perimeter_se, area_se, perimeter_worst, area_worst has outliers.\n\n# create a function to find the lower range and upper range from IQR.\n\ndef get_IQRRange(col):\n    \n    sorted(col) # sort column values in ascending order\n    Q1, Q3 = col.quantile([0.25, 0.75]) # quarter 1 and quarter 3 values\n    IQR = Q3 - Q1\n    upper_range = Q3 + (1.5 * IQR)\n    lower_range = Q1 - (1.5 * IQR)\n    \n    return lower_range, upper_range\n\n\n# In[52]:\n\n\n# remove outliers and replace with upper and lower ranges for the 6 columns.\n\n#Finding lower and upper range from IQR\n\nlow_periMean, up_periMean = get_IQRRange(df['perimeter_mean'])\nlow_areaMean, up_areaMean = get_IQRRange(df['area_mean'])\nlow_periSE, up_periSE = get_IQRRange(df['perimeter_se'])\nlow_areaSE, up_areaSE = get_IQRRange(df['area_se'])\nlow_periWors, up_periWors = get_IQRRange(df['perimeter_worst'])\nlow_areaWors, up_areaWors = get_IQRRange(df['area_worst'])\n\n\n# In[54]:\n\n\n# replacing the outliers with lower and upper range for the columns\n\ndf['perimeter_mean'] = np.where (df['perimeter_mean'] > up_periMean, up_periMean, df['perimeter_mean'])\ndf['perimeter_mean'] = np.where (df['perimeter_mean'] < low_periMean, low_periMean, df['perimeter_mean'])\ndf['area_mean'] = np.where (df['area_mean'] > up_areaMean, up_areaMean, df['area_mean'])\ndf['area_mean']= np.where (df['area_mean'] < low_areaMean, low_areaMean, df['area_mean'])\ndf['perimeter_se'] = np.where (df['perimeter_se'] > up_periSE, up_periSE, df['perimeter_se'])\ndf['perimeter_se'] = np.where (df['perimeter_se'] < low_periSE, low_periSE, df['perimeter_se'])\ndf['area_se'] = np.where (df['area_se'] > up_areaSE, up_areaSE, df['area_se'])\ndf['area_se'] = np.where (df['area_se'] < low_areaSE, low_areaSE, df['area_se'])\ndf['perimeter_worst'] =  np.where (df['perimeter_worst'] > up_periWors, up_periWors, df['perimeter_worst'])\ndf['perimeter_worst'] = np.where (df['perimeter_worst'] < low_periWors, low_periWors, df['perimeter_worst'])\ndf['area_worst'] = np.where (df['area_worst'] > up_areaWors, up_areaWors, df['area_worst'])\ndf['area_worst'] = np.where (df['area_worst'] < low_areaWors, low_areaWors, df['area_worst'])\n\n\n# In[56]:\n\n\n# box plot after treating outliers:\n\nplt.figure(figsize = (32,32))\n\ndf.boxplot(column = cols)\n\n\n# In[76]:\n\n\n# split predictor and responser variables.\n\ny = df.iloc[:,0]\nx = df.iloc[:,1:]\n\n\n# In[78]:\n\n\n# split test and train records\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 100)\n\n\n# In[80]:\n\n\n# scale values using standard scaler.\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nsc.fit_transform(x_train)\n\nsc.fit(x_test)\n\n\n# ### Model Building Using Logistic Regression\n\n# In[87]:\n\n\n# building a logistic regression model and calculating accuracy scores.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_class = LogisticRegression()\n\nlog_class.fit(x_train, y_train)\n\ny_pred = log_class.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nprint(confusion_matrix (y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\n\n# ## Model Building using Naive Bayes Classfier\n\n# In[88]:\n\n\n# building a Gaussian NB classifier model and calculating accuracy scores.\n\nfrom sklearn.naive_bayes import GaussianNB\n\nNB_class = GaussianNB()\n\nNB_class.fit(x_train, y_train)\n\ny_pred = NB_class.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nprint(confusion_matrix (y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\n\n# ## Model Building using K-NN\n\n# In[89]:\n\n\n# building a KNN classifier model and calculating accuracy scores.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nKNN_class = KNeighborsClassifier(n_neighbors= 5, p=2,metric='minkowski')\n\nKNN_class.fit(x_train, y_train)\n\ny_pred = KNN_class.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nprint(confusion_matrix (y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\n\n# ## Model Building using Decision Trees\n\n# In[91]:\n\n\n# building a Decision Tree classifier model and calculating accuracy scores.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nDT_class = DecisionTreeClassifier(criterion = 'entropy')\n\nDT_class.fit(x_train, y_train)\n\ny_pred = DT_class.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nprint(confusion_matrix (y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\n\n# ## Model Building using Random Forests bagging technique.\n\n# In[98]:\n\n\n# building a Decision Tree classifier model and calculating accuracy scores.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF_class = RandomForestClassifier(n_estimators = 40, criterion = 'entropy')\n\nRF_class.fit(x_train, y_train)\n\ny_pred = RF_class.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nprint(confusion_matrix (y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\n\n# ### We can conclude that random Forest with default parameters explains the dataset with 95 % accuracy and with minimum number of False Positives 2 compared to others.\n\n# In[ ]:","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}