{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Credit Card Default "},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the csv file and putting it into 'df' object.\ndf = pd.read_csv('../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing test_train_split from sklearn library\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Dependent and Independent Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting feature variable to X\nX = df.drop('default.payment.next.month',axis=1)\n\n# Putting response variable to y\ny = df['default.payment.next.month']\n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building A Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\npredictions = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the report of our default model\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can infer we get a recall of 0.36 , goal is to maximize the recall score in order to identify maximum defaulters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing confusion matrix\nprint(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Printing Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Renaming the Columns of the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming Columns into more understandable/user-friendly terms\n\n# SEX changed to GENDER\n# PAY_0 changed to PAY_1\n# default.payment.next.month is too long and changed to something simplier, DEFAULT\ndf.rename(columns={'SEX':'GENDER',\n                   'PAY_0':'PAY_1',\n                   'default.payment.next.month':'DEFAULT',} , inplace=True)\n\ndf.drop('ID', axis=1, inplace=True) # Drop column ID\n\ndf.info()  # we see that we have 30,000 observations and no null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We inspect the data as a whole|\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## Creating New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_column = df[\"PAY_1\"] + df[\"PAY_2\"]+ df[\"PAY_3\"]+ df[\"PAY_4\"]+ df[\"PAY_5\"]+ df[\"PAY_6\"]\ndf['pay_sum'] = sum_column\nbill_sum = df[\"BILL_AMT1\"]+df[\"BILL_AMT2\"]+df[\"BILL_AMT3\"]+df[\"BILL_AMT4\"]+df[\"BILL_AMT5\"]+df[\"BILL_AMT6\"]\ndf[\"bill_sum\"]=bill_sum\npay_amt = df['PAY_1']+df['PAY_2']+df['PAY_3']+df['PAY_4']+df['PAY_5']+df['PAY_6']\ndf[\"pay_amt_sum\"]=pay_amt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Education Column Values: ', df['EDUCATION'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" fig, ax = plt.subplots()\nsns.countplot(data=df,x='EDUCATION', order = df['EDUCATION'].value_counts().index, color='salmon')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df['EDUCATION'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There exists values 0, 5 and 6 in this column.\n# Since these are unknown (undefined), they can be grouped into the category 4: \"Others\"\n\ndf['EDUCATION'] = df['EDUCATION'].apply(lambda edu_value: edu_value \n                                        if ((edu_value > 0 and edu_value < 4)) \n                                        else 4) # Changes every value of x not within (and inclusive of) 1 ~ 3 to 4  \n\n# Corrected changes\ndf['EDUCATION'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countplt\nfig, ax = plt.subplots()\nsns.countplot(data=df,x='EDUCATION', order = df['EDUCATION'].value_counts().index, color='salmon');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Marriage Column Values: \", df['MARRIAGE'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MARRIAGE'] = df['MARRIAGE'].apply(lambda marriage_value: marriage_value\n                                     if (marriage_value > 0 and marriage_value < 3)\n                                     else 3) # changes every value of x not within (and inclusive of) 1 and 2 to 3\n\n# Corrected changes\ndf['MARRIAGE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AGE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating a Function to Distribute the Age\ndef func(x):\n    if(x >=20 and x<30 ):\n        return 1\n    elif(x>=30 and x<40):\n        return 2\n    elif(x>=40 and x<50):\n        return 3\n    elif(x>=50 and x<60):\n        return 4\n    elif(x>=60 and x<=80):\n        return 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the function\ndf['AGE'] = df['AGE'].apply(func)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots()\nsns.countplot(data=df,x='AGE', order = df['AGE'].value_counts().index, color='salmon');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new dataframe with just the categorical explanatory variables\ndf_categorical = df[['GENDER', 'EDUCATION', 'MARRIAGE','AGE','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'\n                     ,'DEFAULT']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 3, figsize=(19,14), facecolor='white')\nf.suptitle(\"FREQUENCY OF CATEGORICAL VARIABLES (BY TARGET)\",size=20)\n\n# Creating plots of each categorical variable to target \nax1 = sns.countplot(x='GENDER', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,0])\nax2 = sns.countplot(x='EDUCATION', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,1])\nax3 = sns.countplot(x='MARRIAGE', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,2])\nax4 = sns.countplot(x='PAY_1', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,0])\nax5 = sns.countplot(x='PAY_2', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,1])\nax6 = sns.countplot(x='PAY_3', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,2])\nax7 = sns.countplot(x='PAY_4', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,0])\nax8 = sns.countplot(x='PAY_5', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,1])\nax9 = sns.countplot(x='PAY_6', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,2])\nax10 = sns.countplot(x='AGE', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,2])\n# Setting legends to upper right\nax1.legend(loc=\"upper right\")\nax2.legend(loc=\"upper right\")\nax3.legend(loc=\"upper right\")\nax4.legend(loc=\"upper right\")\nax5.legend(loc=\"upper right\")\nax6.legend(loc=\"upper right\")\nax7.legend(loc=\"upper right\")\nax8.legend(loc=\"upper right\")\nax9.legend(loc=\"upper right\")\nax10.legend(loc=\"upper right\")\n# Changing ylabels to horizontal and changing their positions\nax1.set_ylabel('COUNTS', rotation=0, labelpad=40)  # Labelpad adjusts distance of the title from the graph\nax1.yaxis.set_label_coords(-0.1,1.02)              # (x, y)\nax2.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax2.yaxis.set_label_coords(-0.1,1.02)\nax3.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax3.yaxis.set_label_coords(-0.1,1.02)\nax4.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax4.yaxis.set_label_coords(-0.1,1.02)\nax5.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax5.yaxis.set_label_coords(-0.1,1.02)\nax6.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax6.yaxis.set_label_coords(-0.1,1.02)\nax7.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax7.yaxis.set_label_coords(-0.1,1.02)\nax8.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax8.yaxis.set_label_coords(-0.1,1.02)\nax9.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax9.yaxis.set_label_coords(-0.1,1.02)\nax10.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax10.yaxis.set_label_coords(-0.1,1.02)\n\n# Shifting the Super Title higher\nf.tight_layout()  # Prevents graphs from overlapping with each other\nf.subplots_adjust(top=0.9);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate binary values using get_dummies\nage = pd.get_dummies(df['AGE'], prefix='AGE' )\nmr = pd.get_dummies(df['MARRIAGE'], prefix='MARRIAGE' )\ned = pd.get_dummies(df['EDUCATION'],prefix='EDUCATION')\n# merge with main df bridge_df on key values\ndf = df.join(age)\ndf = df.join(mr)\ndf= df.join(ed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['AGE','MARRIAGE','EDUCATION'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['DEFAULT'].value_counts(),'\\n')\nprint(len(df['DEFAULT']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can infer it as Highly Imbalanced Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency of the defaults\ndefault = df['DEFAULT'].sum() # adds up all the default cases in the df\nno_default = len(df['DEFAULT']) - default  # entire dataset - default cases\n\n# Percentage of the defaults\ndefault_perc = round(default/len(df['DEFAULT']) * 100, 1)\nno_default_perc = round(no_default/len(df['DEFAULT']) * 100, 1)\n\n# Plotting Target\nfig, ax = plt.subplots(figsize=(10,7))  # Sets size of graph\nsns.set_context('notebook', font_scale=1.2)  # Affects things like size of label, lines and other elements of the plot.\n\nsns.countplot('DEFAULT',data=df, palette=\"Reds\")   \nplt.annotate('Non-default: {}'.format(no_default), \n             xy=(-0.25, 3000), # xy = (x dist from 0, y dist from 0)\n            size=15.5)\n\nplt.annotate('Default: {}'.format(default), \n             xy=(0.8, 3000), # xy = (x dist from 0, y dist from 0)\n            size=15)\nplt.annotate('{}%'.format(no_default_perc), xy=(-0.1, 8000),size=15)\nplt.annotate('{}%'.format(default_perc), xy=(0.9, 8000),size=15)\nplt.title('CREDIT CARD COUNT', size=18)\nplt.xlabel(\"Default\",size=15)\nplt.ylabel('Count', rotation=0, \n           labelpad=40, # Adjusts distance of the title from the graph\n           size=15)\nax.yaxis.set_label_coords(-0.1,.9)\n\nplt.box(False)        # Removes the bounding area\nplt.savefig('target_skew.png', transparent = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 22 % of Defaulters "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freq distribution of all data\nfig, ax = plt.subplots(figsize=(15,15))\npd.DataFrame.hist(df,ax=ax)\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Can we infer more? what about the columns for lIMIT_BALANCE?\nx1 = list(df[df['DEFAULT'] == 1]['LIMIT_BAL'])\nx2 = list(df[df['DEFAULT'] == 0]['LIMIT_BAL'])\n\nfig2, ax_lim_bal = plt.subplots(figsize=(12,4))\nsns.set_context('notebook', font_scale=1.2)\n#sns.set_color_codes(\"pastel\")\nplt.hist([x1, x2], bins = 40, density=False, color=['firebrick', 'salmon'])\nplt.xlim([0,600000])\nplt.legend(['Yes', 'No'], title = 'Default', loc='upper right', facecolor='white')\nplt.xlabel('Limit Balance (NT dollar)')\nplt.ylabel('Frequency', rotation=0,labelpad=40)\nplt.title('LIMIT BALANCE HISTOGRAM BY TYPE OF CREDIT CARD', SIZE=15)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that we have our features, let's plot them on a correlation matrix to remove anything that might \n# cause multi-colinearity within our model\n\nsns.set(style=\"white\")\n# Creating the data\ndata = df.corr()\n\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(data, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(60,50))\n\n\n# Create a custom color palette\ncmap = \\\nsns.diverging_palette(133, 10,\n                      as_cmap=True)  \n# as_cmap returns a matplotlib colormap object rather than a list of colors\n# Green = Good (low correlation), Red = Bad (high correlation) between the independent variables\n\n# Plot the heatmap\ng = sns.heatmap(data=data, annot=True, cmap=cmap, ax=ax, \n                mask=mask, # Splits heatmap into a triangle\n                annot_kws={\"size\":20},  #Annotation size\n               cbar_kws={\"shrink\": 0.8} # Color bar size\n               );\n\n\n# Prevent Heatmap Cut-Off Issue\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\n# Changes size of the values on the label\nax.tick_params(labelsize=25) \n\nax.set_yticklabels(g.get_yticklabels(), rotation=0);\nax.set_xticklabels(g.get_xticklabels(), rotation=80);\n\nplt.savefig('correlation_heatmap.png', transparent = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_default_corrs = data.filter(like='DEFAULT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_default_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_default_corrs.plot(kind='bar',figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PAY_1,PAY_SUM,PAY_AMT_SUM and PAY_2 highly correlate with the Defaulters\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n# Data splitting for 80% Train/Val and 20% Test \nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.2, random_state=69) # 20% holdout \nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.25, random_state=69) # Train/Val\n\n# Initializing the scaler  (Just scale every single time lol)\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nstd.fit(X_train_val.values)\n\n## Scale the Predictors on the train/val dataset\nX_train_val_scaled = std.transform(X_train_val.values) \n\n## This line instantiates the model. \nrf = RandomForestClassifier() \n\n## Fit the model on your training data.\nrf.fit(X_train_val_scaled, y_train_val) \n\n# Obtain the feature importance\nfeature_importance = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                   columns=['Variable_Importance']).sort_values('Variable_Importance',ascending=True)\n\n# Set seaborn contexts \nsns.set(style=\"whitegrid\")\n\nfeature_importance.plot.barh(figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Required Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\n\n# Classifier Metrics \nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\nfrom sklearn.metrics import auc, roc_curve, roc_auc_score, precision_recall_curve\nfrom sklearn.metrics import fbeta_score, cohen_kappa_score\n\n# Pre-processing packages\nfrom sklearn.preprocessing import StandardScaler\n\n\n# CV, Gridsearch, train_test_split, model selection packages\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Base Line Model (No Scaling)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Baseline model performance evaluation\n\n# to give model baseline report with cross-validation in dataframe \ndef baseline_report_cv_(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    # Splitting the data into 80% training/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n#     # Splitting the training data into 60% training data and 20% validation data.\n#     X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n     \n    # Creating a shuffled kfold of 5\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    accuracy     = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='accuracy'))\n    precision    = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='precision'))\n    recall       = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='recall'))\n    f1score      = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='f1'))\n    rocauc       = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='roc_auc'))\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale = df_models.drop('index', axis=1)\ndf_models_no_scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GuassianNB with Recall of 0.86"},{"metadata":{},"cell_type":"markdown","source":"## Building Base Line Model(Scaled)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scaled Dataset Model performance evaluation\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale = df_models.drop('index', axis=1)\ndf_models_scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Poor Perfomance as compared to scale"},{"metadata":{},"cell_type":"markdown","source":"## SMOTE "},{"metadata":{"trusted":true},"cell_type":"code","source":"## SMOTE Dataset model performance evaluation\nfrom imblearn import under_sampling, over_sampling\nfrom imblearn.over_sampling import SMOTE\n\n# Smote with no scaling with cross-validation in dataframe \ndef baseline_report_cv_smote(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model, SMOTE's the data\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    from imblearn.over_sampling import SMOTE # Allows for smoting if you forget to initialize it before running func\n    \n    # Splitting the data into 80% training/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n    # Splitting the training data into 60% training data and 20% validation data.\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n    \n    \n    #this helps with the way kf will generate indices below\n    X_train_val, y_train_val = np.array(X_train_val), np.array(y_train_val)\n    \n    \n    # Creating a shuffled kfold of 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    clf_model_acc_scores_cv = []\n    clf_model_precision_scores_cv = []\n    clf_model_recall_scores_cv = []\n    clf_model_f1_scores_cv = []\n    clf_model_rocauc_scores_cv = []\n    \n    # Manual Cross-Validation\n    for train_ind, val_ind in kf.split(X_train_val, y_train_val):\n\n        # Assigning train and validation values for an individual fold\n        X_train, y_train = X_train_val[train_ind], y_train_val[train_ind]\n        X_val, y_val = X_train_val[val_ind], y_train_val[val_ind] \n\n        # Creating the SMOTE data\n        X_smoted, y_smoted = SMOTE(random_state=69).fit_sample(X_train, y_train)\n        \n        # Initializing model\n        clf_model = model.fit(X_smoted, y_smoted) # Train model on SMOTE'd data\n        y_pred = clf_model.predict(X_val)  # Y pred after testing on validation data split\n        \n        # Save scores of model\n        clf_model_acc_score = accuracy_score(y_val, y_pred)\n        clf_model_precision_score = precision_score(y_val, y_pred)\n        clf_model_recall_score = recall_score(y_val, y_pred)\n        clf_model_f1_score = f1_score(y_val, y_pred)   \n        clf_model_rocauc_score = roc_auc_score(y_val, y_pred)\n        \n        # Append scores of model their scoring lists\n        clf_model_acc_scores_cv.append(clf_model_acc_score)\n        clf_model_precision_scores_cv.append(clf_model_precision_score)\n        clf_model_recall_scores_cv.append(clf_model_recall_score)\n        clf_model_f1_scores_cv.append(clf_model_f1_score)\n        clf_model_rocauc_scores_cv.append(clf_model_rocauc_score)\n        \n\n    \n    accuracy     = np.mean(clf_model_acc_scores_cv)\n    precision    = np.mean(clf_model_precision_scores_cv)\n    recall       = np.mean(clf_model_recall_scores_cv)\n    f1score      = np.mean(clf_model_f1_scores_cv)\n    rocauc       = np.mean(clf_model_rocauc_scores_cv)\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_smote(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_smote(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_smote(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_smote(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_smote(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_smote(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale_cv_smote = df_models.drop('index', axis=1)\ndf_models_no_scale_cv_smote","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GaussianNB with recall of 0.93 !!!"},{"metadata":{},"cell_type":"markdown","source":"## Oversampling the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Oversample Dataset model performance evaluation\n\ndef baseline_report_cv_oversampling(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model, oversamples the data\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    # Allows for oversampling if you forget to initialize it before running func\n    from imblearn.over_sampling import RandomOverSampler\n    \n    # Splitting the data into 80% training/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n    # Splitting the training data into 60% training data and 20% validation data.\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n    \n    \n    #this helps with the way kf will generate indices below\n    X_train_val, y_train_val = np.array(X_train_val), np.array(y_train_val)\n    \n    \n    # Creating a shuffled kfold of 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    clf_model_acc_scores_cv = []\n    clf_model_precision_scores_cv = []\n    clf_model_recall_scores_cv = []\n    clf_model_f1_scores_cv = []\n    clf_model_rocauc_scores_cv = []\n    \n    # Manual Cross-Validation\n    for train_ind, val_ind in kf.split(X_train_val, y_train_val):\n\n        # Assigning train and validation values for an individual fold\n        X_train, y_train = X_train_val[train_ind], y_train_val[train_ind]\n        X_val, y_val = X_train_val[val_ind], y_train_val[val_ind] \n\n        # Creating the OverSampled data\n        X_resampled, y_resampled = RandomOverSampler(random_state=69).fit_sample(X_train, y_train)\n        \n        # Initializing model\n        clf_model = model.fit(X_resampled, y_resampled) # Train model on SMOTE'd data\n        y_pred = clf_model.predict(X_val)  # Y pred after testing on validation data split\n        \n        # Save scores of model\n        clf_model_acc_score = accuracy_score(y_val, y_pred)\n        clf_model_precision_score = precision_score(y_val, y_pred)\n        clf_model_recall_score = recall_score(y_val, y_pred)\n        clf_model_f1_score = f1_score(y_val, y_pred)   \n        clf_model_rocauc_score = roc_auc_score(y_val, y_pred)\n        \n        # Append scores of model their scoring lists\n        clf_model_acc_scores_cv.append(clf_model_acc_score)\n        clf_model_precision_scores_cv.append(clf_model_precision_score)\n        clf_model_recall_scores_cv.append(clf_model_recall_score)\n        clf_model_f1_scores_cv.append(clf_model_f1_score)\n        clf_model_rocauc_scores_cv.append(clf_model_rocauc_score)\n        \n   \n    accuracy     = np.mean(clf_model_acc_scores_cv)\n    precision    = np.mean(clf_model_precision_scores_cv)\n    recall       = np.mean(clf_model_recall_scores_cv)\n    f1score      = np.mean(clf_model_f1_scores_cv)\n    rocauc       = np.mean(clf_model_rocauc_scores_cv)\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_oversampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_oversampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_oversampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_oversampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_oversampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_oversampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale_oversampled = df_models.drop('index', axis=1)\ndf_models_no_scale_oversampled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recall of 0.93 with f1 0.38"},{"metadata":{},"cell_type":"markdown","source":"## Undersampling the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Undersample Dataset model performance evaluation\n\n\ndef baseline_report_cv_undersampling(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model, undersamples the data\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    # Allows for undersampling if you forget to initialize it before running func\n    from imblearn.under_sampling import RandomUnderSampler\n    \n    # Splitting the data into 80% training/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n    # Splitting the training data into 60% training data and 20% validation data.\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n    \n    \n    #this helps with the way kf will generate indices below\n    X_train_val, y_train_val = np.array(X_train_val), np.array(y_train_val)\n    \n    \n    # Creating a shuffled kfold of 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    clf_model_acc_scores_cv = []\n    clf_model_precision_scores_cv = []\n    clf_model_recall_scores_cv = []\n    clf_model_f1_scores_cv = []\n    clf_model_rocauc_scores_cv = []\n    \n    # Manual Cross-Validation\n    for train_ind, val_ind in kf.split(X_train_val, y_train_val):\n\n        # Assigning train and validation values for an individual fold\n        X_train, y_train = X_train_val[train_ind], y_train_val[train_ind]\n        X_val, y_val = X_train_val[val_ind], y_train_val[val_ind] \n\n        # Creating the UnderSampled data\n        X_resampled, y_resampled = RandomUnderSampler(random_state=69).fit_sample(X_train, y_train)\n        \n        # Initializing model\n        clf_model = model.fit(X_resampled, y_resampled) # Train model on SMOTE'd data\n        y_pred = clf_model.predict(X_val)  # Y pred after testing on validation data split\n        \n        # Save scores of model\n        clf_model_acc_score = accuracy_score(y_val, y_pred)\n        clf_model_precision_score = precision_score(y_val, y_pred)\n        clf_model_recall_score = recall_score(y_val, y_pred)\n        clf_model_f1_score = f1_score(y_val, y_pred)   \n        clf_model_rocauc_score = roc_auc_score(y_val, y_pred)\n        \n        # Append scores of model their scoring lists\n        clf_model_acc_scores_cv.append(clf_model_acc_score)\n        clf_model_precision_scores_cv.append(clf_model_precision_score)\n        clf_model_recall_scores_cv.append(clf_model_recall_score)\n        clf_model_f1_scores_cv.append(clf_model_f1_score)\n        clf_model_rocauc_scores_cv.append(clf_model_rocauc_score)\n        \n\n    \n    accuracy     = np.mean(clf_model_acc_scores_cv)\n    precision    = np.mean(clf_model_precision_scores_cv)\n    recall       = np.mean(clf_model_recall_scores_cv)\n    f1score      = np.mean(clf_model_f1_scores_cv)\n    rocauc       = np.mean(clf_model_rocauc_scores_cv)\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_undersampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_undersampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_undersampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_undersampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_undersampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_undersampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale_undersample = df_models.drop('index', axis=1)\ndf_models_no_scale_undersample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recall of 0.934 f1 of 0.381"},{"metadata":{},"cell_type":"markdown","source":"## SMOTE(SCALED)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## SMOTE Datset with Scaling for Model Performance Evaluation\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# Creating n_splits for the function since it already has kfold creation in them\nn_splits = 5\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_smote(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_smote(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_smote(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_smote(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_smote(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_smote(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale_cv_smote = df_models.drop('index', axis=1)\ndf_models_scale_cv_smote","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recall of 0.82 with improved f1 of 0.41"},{"metadata":{},"cell_type":"markdown","source":"## Oversampled(Scaled)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Oversampling & Scaled Dataset on Model Performance Evaluation\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# Creating n_splits for the function since it already has kfold creation in them\nn_splits = 5\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_oversampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_oversampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_oversampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_oversampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_oversampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_oversampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale_oversampled = df_models.drop('index', axis=1)\ndf_models_scale_oversampled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recall of 0.78 with improved f1score of0.43!!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Undersampling & Scaled Dataset for model performance evaluation\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# Creating n_splits for the function since it already has kfold creation in them\nn_splits = 5\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_undersampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_undersampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_undersampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_undersampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_undersampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_undersampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale_undersample = df_models.drop('index', axis=1)\ndf_models_scale_undersample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recall of 0.74 with F1 score of 0.44 !!! Much better model"},{"metadata":{},"cell_type":"markdown","source":"## Let's Check all the Models and Select the Best!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the scores of the models\ndf_models_no_scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recall is good !!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models_scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pretty bad model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models_no_scale_cv_smote","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Good Model with Recall of 0.9323"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models_no_scale_oversampled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recall is same 0.93 with further improvement in f1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models_no_scale_undersample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pretty Same \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models_scale_cv_smote","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is the Best model with Recall of 0.82 and F1 score of 0.41!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_models_scale_oversampled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pretty optimized model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models_scale_undersample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Model for harmonic mean i.e f1score with recall of 0.74"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}