{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center><u> CarPrice Data Set</u></center>"},{"metadata":{},"cell_type":"raw","source":"CRIME DATA SET NOTE BOOK STRUCTURE\n\n1.\tImporting Libraries\n2.\tData Loading\n3.\tData Description\n\n4.\tBivariate Analysis (for Feature Selection)\n    4.1.\tDistribution of Each Numerical Attribute against price\n\n5.\tFeature Selection\n    5.1.\tPearson Correlation\n\n6.\tNormalization of Numerical Features\n\n7.\tFeature Encoding\n    7.1.\tManual Encoding\n    7.2.\tOne Hot Encoding\n\n8.\tMachine Learning\n    8.1.\tTransforming Features and Target Variables into Arrays\n    8.2.\tTrain Test Split\n    8.3.\tLinear Regression Model\n        8.3.1.\tFitting Linear Regression Model\n        8.3.2.\tLinear Regression Model Evaluation\n        8.3.2.1.\tVisualization of Actual vs Predicted\n    \n    8.4.\tRandom Forest Regressor\n        8.4.1.\tFitting Random Forest Regressor\n        8.4.2.\tRandom Forest Regressor Evaluation\n    \n    8.5.\tRidge Regression\n        8.5.1.\tApplying Ridge Regression\n        8.5.2.\tRidge Regressor Evaluation\n    \n    8.6.\tLasso Regression\n        8.6.1.\tApplying Lasso Regression\n        8.6.2.\tLasso Regression Evaluation\n    \n    8.7.\tElastic Net Regression\n        8.7.1.\tApplying Elastic Net Regression\n        8.7.2.\tElastic Net Regressor Evaluation\n    \n    8.8.\tRandom Forest Classifier\n        8.8.1.\tBuilding Classifier for Random Forest\n        8.8.2.\tTrain Test Split\n        8.8.3.\tFitting Random Forest Classifier\n        8.8.4.\tRandom Forest Classifier Evaluation\n    \n    8.9. Model Metrics Comparision\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport plotly.figure_factory as ff\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score,recall_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice = pd.read_csv('../input/car-price-prediction/CarPrice_Assignment.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Description"},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\033[1mRows :\\033[0m' , carprice.shape[0])\nprint('\\033[1m\\nColumns :\\033[0m', carprice.shape[1])\nprint('\\033[1m\\nFeatures :\\033[0m', carprice.columns.tolist())\nprint('\\033[1m\\nNull Values :\\033[0m',carprice.isnull().sum().values.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Bivariate Analysis (for Feature Selection)"},{"metadata":{},"cell_type":"markdown","source":"### 4.1. Distribution of Each Numerical Attribute against Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nfor i in carprice.columns:\n    \n    if carprice[i].dtype=='O':\n            sns.boxplot(x=carprice[i], y=carprice['price'],data=carprice)\n            plt.show()\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above box plots we can see that there is nout much difference in price when the door numbers are two or four. Therefore assuming a very low correlation we will drop the door number column. For the car name column we cant clearly see the labels but the plot is some what depicting a high correlation with price. \n\nTo visualize this better lets split the column and also rename the values in it."},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice['CarName']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the values we can see spelling mistakes in the company names as well as car model names. We will therefore split the column into company name and car model name."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = carprice['CarName'].str.split(\" \", expand=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice['Company'] = x[0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice['Company'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice['Company'] = carprice['Company'].replace({'toyouta': 'Toyota','vw':'Volkswagen','vokswagen':'Volkswagen',\n                                                      'maxda':'Mazda','porcshce':'Porsche'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice['Company'] = carprice['Company'].str.title()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(carprice['Company'], y=carprice['price'], data=carprice)\nplt.xticks(rotation=60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From th above visulizations we can clearly see how widely the prices vary from company to company. We therefore can use this feature to train our model and predict price based on company names rather than using car name (models)."},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =carprice.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"### 5.1. Pearson Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = carprice.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.heatmap(abs(correlation), annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation.price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation matrix we found out that carheight, stroke, compressionratio and peakrpm have no noticable affect on the price of the cars therefore, we will drop these columns. car_ID column is also irrelevant for the prediction of car price. From the box plot visualizations above we saw carNames and doornumber attribute can also be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.drop(columns =['car_ID','carheight', 'stroke', 'compressionratio','peakrpm', 'CarName', 'doornumber'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labeel_for_DS = carprice['price'].copy() #Will use it in the Random Classifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Normalization of Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import zscore\nnumeric_cols = carprice.select_dtypes(include=[np.number]).columns\ncarprice[numeric_cols] = carprice[numeric_cols].apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Feature Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in carprice.columns:\n    if carprice[i].dtype =='O':\n        print(i+' : ',carprice[i].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.1. Manual Encoding"},{"metadata":{},"cell_type":"markdown","source":"For categorical features that have two unique values, we will  manually encode them."},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.fueltype = carprice.fueltype.map({'gas': 1,'diesel':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.aspiration = carprice.aspiration.map({'std':1, 'turbo':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.enginelocation = carprice.enginelocation.map({'front':1,'rear':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.2. One Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"For Categorical variables having more than two unique values we will hot encode those features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for i in carprice.columns:\n    if carprice[i].dtype == 'O' and carprice[i].nunique() >2:\n        #print(i, i+'_Dummies')\n        carprice.Dummies =pd.get_dummies(carprice[i])\n        carprice = pd.concat([carprice, carprice.Dummies], axis=1)\n        carprice.drop(columns=[i], inplace=True)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carprice.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=carprice.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Variable\ny = carprice['price']\n\n#Features\nx= carprice.drop(columns=['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1 Transforming Features and Target Variables into Arrays"},{"metadata":{},"cell_type":"markdown","source":"We need to convert both x and y into arrays before applying the Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First converting the Features into Dictionary\nx = x.to_dict(orient='records')\n\n#Importing vectorizer to convert Dictionary to array\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\nx = vec.fit_transform(x).toarray()\n\n#converting our target variable into array\ny = np.asarray(y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Before splitting the data its worth mentioning here that we havent removed the outliers. I think the outliers here may represent a real picture for eg prices of some cars may in real world be too high. Therefore I am of the opinion that removing outliers in the data set at hand would be not a wise thing to do.</b>"},{"metadata":{},"cell_type":"markdown","source":"### 8.2 Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"xtrain shape : \", xtrain.shape,\" :: xtest shape  : \", xtest.shape,\" :: ytrain shape : \", ytrain.shape,\" :: ytest shape  : \", ytest.shape) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.3 Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.3.1 Fitting Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Training\nregressor.fit(xtrain, ytrain)\n\n#Model Prediction\ny_pred_linear = regressor.predict(xtest) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.3.2 Linear Regression Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.scatter(ytest, y_pred_linear, c = 'blue') \nplt.xlabel(\"Expected\") \nplt.ylabel(\"Predicted value\") \nplt.title(\"True value vs predicted value : Linear Regression\") \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\033[1mMean Squared Error is:\\033[0m', metrics.mean_squared_error(ytest, y_pred_linear))  \nprint('\\033[1mMean Absolute Error is:\\033[0m', metrics.mean_absolute_error(ytest, y_pred_linear))  \nprint('\\033[1mRoot Mean Squared Error is:\\033[0m', np.sqrt(metrics.mean_squared_error(ytest, y_pred_linear)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.3.2.1 Visualization of Actual vs Predicted Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'Actual': ytest.flatten(), 'Predicted': y_pred_linear.flatten()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.head(25)\ndf.plot(kind='bar',figsize=(10,5))\nplt.grid(which='major', linestyle=':', linewidth='0.99', color='black')\nplt.xlabel(\"No. of Records\")\nplt.ylabel(\"Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.4 Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.4.1 Fitting Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Training\nrfr = RandomForestRegressor(n_estimators =1000, random_state=42)\nrfr.fit(xtrain,ytrain);\n\n#Model Prediction\ny_pred_rfr = rfr.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.4.2 Random Forest Regressor Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\033[1mMean Absolute Error:\\033[0m', metrics.mean_absolute_error(ytest,y_pred_rfr))\nprint('\\033[1mMean Squared Error:\\033[0m', metrics.mean_squared_error(ytest, y_pred_rfr))\nprint('\\033[1mRoot Mean Square Error:\\033[0m', np.sqrt(metrics.mean_squared_error(ytest, y_pred_rfr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.5 Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.5.1 Applying Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nparameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\nridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(xtrain,ytrain);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\nprint(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Prediction\ny_pred_ridge = ridge_regressor.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.5.2 Ridge Regressor Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\033[1mUSING ALPHA =1\")\nprint(\"\\nMean Sqaured Error for Ridge Regression is : \\033[0m\", metrics.mean_squared_error(ytest, y_pred_ridge))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Actual vs Predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(y_pred_ridge)\nplt.plot(ytest)\nplt.legend([\"Predicted\",\"Actual\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.6 Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.6.1 Applying Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso()\nparamaters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\nlasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=3)\nlasso_regressor.fit(xtrain, ytrain);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\nprint(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction using Lasso\ny_pred_lasso = lasso_regressor.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.6.2 Lasso Regressor Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\033[1mUSING ALPHA =0.001\")\nprint(\"\\nMean Sqaured Error for Lasso is : \\033[0m\", metrics.mean_squared_error(ytest, y_pred_lasso))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Actual vs Predicted Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(y_pred_lasso)\nplt.plot(ytest)\nplt.legend([\"Predicted\",\"Actual\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.7. Elastic Net Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.7.1. Applying Elastic Net Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic = ElasticNet()\nparamaters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\nelastic_regressor = GridSearchCV(elastic, parameters, scoring='neg_mean_squared_error',cv=5)\nelastic_regressor.fit(xtrain, ytrain);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The best fit alpha value is found out to be :\" ,elastic_regressor.best_params_)\nprint(\"\\nUsing \",elastic_regressor.best_params_, \" the negative mean squared error is: \", elastic_regressor.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_elastic = elastic_regressor.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.7.2. ElasticNet Regressor Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\033[1mUSING ALPHA =0.001\")\nprint(\"\\nMean Sqaured Error for Elastic Net Regression is : \\033[0m\", metrics.mean_squared_error(ytest, y_pred_elastic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Actual vs Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(y_pred_elastic)\nplt.plot(ytest)\nplt.legend([\"Predicted\",\"Actual\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.8 Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"### 8.8.1 Building Classifer for Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorizing Target variable as High for income >11000 and Low for  <11000\ny2 =np.where(labeel_for_DS>11000, 'High',\"Low\") \n\n#Manual Encoding of Target Varibale\ny2 = np.where(y2=='High',1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  8.8.2 Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain2,xtest2,ytrain2,ytest2 = train_test_split(x,y2,test_size=0.3, random_state=42 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(xtrain2.shape,xtest2.shape,ytrain2.shape,ytest2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.8.3 Applying Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel = rfc.fit(xtrain2, ytrain2)\ny_pred_rfc = rfc.predict(xtest2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.8.4 Random Forest Classifier Evaluation:"},{"metadata":{},"cell_type":"markdown","source":"#### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest2, y_pred_rfc))\nprint (\"\\n \\033[1m Accuracy : \\033[0m\\n\",metrics.accuracy_score(ytest2, y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\ncf_matrix = confusion_matrix(y_pred_rfc, ytest2)\nx_y_labels = ['High','Low']\nsns.heatmap(cf_matrix.T, square=True, annot=True, xticklabels=x_y_labels, yticklabels=x_y_labels)\nplt.xlabel('Predicted label')\nplt.ylabel('Actual label');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.9. Model Metrics Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    #accuracy     = accuracy_score(testing_y,predictions)\n    mean_sq_err  = metrics.mean_squared_error(testing_y,predictions)\n    mean_abs_err = metrics.mean_absolute_error(testing_y,predictions)\n    Rmean_sq_err = np.sqrt(metrics.mean_squared_error(testing_y,predictions) )\n        \n    df = pd.DataFrame({\"Model\"                  : [name],\n                       \"Mean Square Error\"      : [mean_sq_err],\n                       \"Mean Absolute Error\"    : [mean_abs_err],\n                       \"Root Mean Square Error\" : [Rmean_sq_err],\n                       \n                      })\n    return df\n\nmodel1 = model_report(regressor,xtrain,xtest,ytrain,ytest,\n                      \"Linear Regression\")\n\nmodel2 = model_report(ridge_regressor,xtrain,xtest,ytrain,ytest,\n                      \"Ridge Regression\")\n\nmodel3 = model_report(lasso_regressor,xtrain,xtest,ytrain,ytest,\n                      \"Lasso Regression\")\n\nmodel4 = model_report(elastic_regressor,xtrain,xtest,ytrain,ytest,\n                      \"Elastic Net Regression\")\n\nmodel5 = model_report(rfr,xtrain,xtest,ytrain,ytest,\n                      \"Random Forest Regressor\")\n\nmodel_performances = pd.concat([model1,model2,model3,model4,model5],axis = 0).reset_index()\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <center>---**---End---**---</center>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}