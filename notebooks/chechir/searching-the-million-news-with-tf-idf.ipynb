{"cells":[{"metadata":{"_uuid":"b0d52396e8cd746cb9efc75b93d5a6e19c270048"},"cell_type":"markdown","source":"# Search Engine\n\nThis is a small experiment that performs a search in a Pandas dataframe. The idea is that the results are returned in a reasonable time and also that they are relevant.\n\n## The Solution\n\n###Summary\nThe program creates a TF-IDF representation for every row in the data frame,, scoring every word (or n-gram) in the text. This produces a sparse matrix with around 1 million rows and n columns (n = number of n-grams obtained in the entire df). Then the program produces the equivalent TF-IDF matrix for a given query and calculates the cosine similarity between both matrices. This cosine similarity is used as the relevance score for each row in our df. Finally, the program returns the top 10 most relevant news headlines in the df for the query.\n\n### Implementation\nThe implemented code is basically a class called 'SearchEngine'. This class performs mainly two tasks: \n1. Prepare the data set and build a 'model' based on the entire df (method: fit). \n2. Given a query, return the most relevant products in the df that match this query (method: get_results)\n\n### Method *fit*\nThis method will perform the following actions: \n\n1. Prepare the data: It concatenates the 'name' and 'brand' columns into one column, converts the text to lower case for this new column, removes stop words and optionally stems the words using the Porter stemer.\n\n2. Calculate a TF-IDF matrix: It will calculate the TF-IDF matrix for the entire catalog, using different n-grams. By default it uses 1 to 3 n-grams. More information on TF-IDF (Term frequency â€“ Inverse document frequency) can be found in wikipedia: https://en.wikipedia.org/wiki/Tf%E2%80%93idf. More information about n-grams could be found in here: https://en.wikipedia.org/wiki/N-gram\nFor this task the program uses the sklearn package.\n\n3. Save the objects: It saves the sparse matrix for the catalog and the sklearn models as attributes of the class\n\n\n### Method *get_results*\nThis method will receive as a parameter a single query and it will perform the search in the catalog returning up to 10 results ordered by the ranking score. The ranking score will be obtained in the following way:\n\n1. Preprocess the query using the same transformations used in the 'fit' method for the entire df (convert the text to lower case, remove stop words, stem words if it applies)\n\n2. Obtain a TF-IDF representation of our query. This will be a sparse matrix of only 1 row\n\n3. Calculate the cosine similarity between the matrix representation of our query and our entire catalog matrix. This vector will be our ranking score\n\n4. Sort the catalog by the ranking score obtained in the previous step (in descending order) and return the top 10 most relevant products for our queries \n\n## Code"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport copy\nimport pandas as pd\nimport numpy as np\nimport sys\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom stemming.porter2 import stem\n\ndata_path = '../input/abcnews-date-text.csv'\n        \nclass SearchEngine():  \n    replace_words = {'&': '_and_', 'unknown':' '}    \n\n    def __init__(self, text_column='name', id_column='id'):\n        self.text_column = text_column\n        self.id_column = id_column\n        pass\n    \n    def fit(self, df, ngram_range=(1,3), perform_stem=True):\n        self.df = df\n        self.perform_stem = perform_stem\n        doc_df = self.preprocess(df)\n        stopWords = stopwords.words('english')    \n        self.vectoriser = CountVectorizer(stop_words = stopWords, ngram_range=ngram_range)\n        train_vectorised = self.vectoriser.fit_transform(doc_df)\n        self.transformer = TfidfTransformer()\n        self.transformer.fit(train_vectorised)\n        self.fitted_tfidf = self.transformer.transform(train_vectorised)\n\n    def preprocess(self, df):\n        result = df[self.text_column]\n        result = np.core.defchararray.lower(result.values.astype(str))\n        for word in self.replace_words:\n            result = np.core.defchararray.replace(result, word, self.replace_words[word])\n        if self.perform_stem:\n            result = self.stem_array(result)\n        return result\n\n    def preprocess_query(self, query):\n        result = query.lower()\n        for word in self.replace_words:\n            result = result.replace(word, self.replace_words[word])\n        if self.perform_stem:\n            result = self.stem_document(result)\n        return result\n\n    def stem_array(self, v):\n        result = np.array([self.stem_document(document) for document in v])\n        return result\n    \n    def stem_document(self, text):\n        result = [stem(word) for word in text.split(\" \")]\n        result = ' '.join(result)\n        return result\n    \n    def get_results(self, query, max_rows=10):\n        score = self.get_score(query)\n        results_df = copy.deepcopy(self.df)\n        results_df['ranking_score'] = score\n        results_df = results_df.loc[score>0]\n        results_df = results_df.iloc[np.argsort(-results_df['ranking_score'].values)]\n        results_df = results_df.head(max_rows)\n        self.print_results(results_df, query)\n        return results_df        \n        \n    def get_score(self, query):\n        query_vectorised = self.vectoriser.transform([query])    \n        query_tfidf = self.transformer.transform(query_vectorised)\n        cosine_similarities = linear_kernel(self.fitted_tfidf, query_tfidf).flatten()\n        return cosine_similarities\n    \n    def print_results(self, df, query):\n        print(\"---------\")\n        print('results for \"{}\"'.format(query))\n        for i, row in df.iterrows():\n            print('{}, {}, {}'.format(\n                    row['ranking_score'],\n                    row[self.id_column],\n                    row[self.text_column]))\n    \ndef load_data():\n    df = pd.read_csv(data_path)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Examples\nHere you can play with different queries, using more or less n-grams, and using stemming (that will increase the time to process)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a2cd17064287918f242beb1b56df8a3ed065afee"},"cell_type":"code","source":"queries = [\n    'global warming',\n    'how can I win kaggle competitions from my cell phone',\n    'what is the meaning of life',\n    'donald trump riding an skate board',\n    'some people like weird things, like pizza with pineapple',\n    'I dont like cricket, I love it'\n    ]\n\ndf = load_data()\nmodel = SearchEngine(text_column='headline_text',  id_column='publish_date')\nmodel.fit(df, perform_stem=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f3bca3548e1cf47f1f0f0de128fddb5184281044"},"cell_type":"code","source":"# Getting results \nfor query in queries:\n    model.get_results(query)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2977abce379c3ea43a8d8c13ad6347196379331"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}