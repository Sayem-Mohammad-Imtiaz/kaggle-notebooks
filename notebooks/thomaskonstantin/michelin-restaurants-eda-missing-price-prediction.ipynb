{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Introduction</h1>","metadata":{}},{"cell_type":"markdown","source":"\n<img src=\"https://images.unsplash.com/photo-1457520986485-d19e5f3a046d?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=1352&q=80\" alt=\"Girl in a jacket\" width=\"1000\" height=\"600\">\n\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; line-height: 115%; color: black; font-family: \"Times New Roman\", Times, serif;'>Greetings my fellow Kagglers!</span><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br style=\"text-align:start;\">In the following Kernel we will perform a short EDA section, as well as text vectorization ,dimensionality reduction and blending model results.</span></span></p>\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"line-height: 115%; color: black;\"><br>&nbsp;We will construct a Random Forest model to predict the missing values in the &lsquo;price&rsquo; feature (both to provide feature Kagglers with a reasonable solution for the missing value and as a price category prediction model in general).</span></span></span></p>\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"line-height: 115%; color: black;\">&nbsp;</span></span></span></p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>&nbsp;</span></p>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import r2_score,mean_squared_error,jaccard_score,f1_score,confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport string\nimport plotly.graph_objs as go\ndef RMSE(Y,Y_HAT):\n    return np.sqrt(mean_squared_error(Y_HAT,Y))\n\n\nplt.rc('figure',figsize=(20,11))\n\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Loading And Assessment</h1>\n","metadata":{}},{"cell_type":"code","source":"one_star = pd.read_csv('/kaggle/input/michelin-restaurants/one-star-michelin-restaurants.csv')\ntwo_star = pd.read_csv('/kaggle/input/michelin-restaurants/two-stars-michelin-restaurants.csv')\nthree_star = pd.read_csv('/kaggle/input/michelin-restaurants/three-stars-michelin-restaurants.csv')\none_star['Michelin Stars'] = 1\ntwo_star['Michelin Stars'] = 2\nthree_star['Michelin Stars'] = 3\nr_data = pd.concat([one_star,two_star,three_star],axis=0).drop(columns=['url','zipCode'])\nr_data = r_data[~r_data.city.isna()]\nr_data","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Missing Value Counts In Our Dataset',fontsize=20)\nsns.heatmap(pd.DataFrame(r_data.isna().sum()).T,annot=True,cmap='Reds',fmt='d')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; line-height: 115%; color: black; font-family: \"Times New Roman\", Times, serif;'>So in total we have 176 missing price values out of a total of 693 samples,<br> as explained in the introduction these are our target values for prediction.<br> Also it is important to note that some features were omitted, features such as &lsquo;url&rsquo; and &rsquo;zipCode&rsquo; because of their low relevance as predictor in my opinion.</span></p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h1>\n","metadata":{}},{"cell_type":"code","source":"plt.title('Top 10 Cities With The Most Michelin Restaurants',fontsize=20)\nsns.barplot(x=r_data.city.value_counts()[:10],y=r_data.city.value_counts()[:10].index,palette='twilight')\nplt.xlabel('Number Of Restaurants',fontsize=18)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; line-height: 115%; color: black; font-family: \"Times New Roman\", Times, serif;'>In the above plot we observe the top 10 cities in our dataset merged by the amount of Michelin restaurant located in them.</span></p>\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; line-height: 115%; color: black; font-family: \"Times New Roman\", Times, serif;'>And we can see that major size cities such as New York, Hong Kong And San Francisco are leading with the highest count of Michelin restaurant, it is quite expected when taking into mind known facts such as that all those cities belong to first world, developed countries. </span></p>","metadata":{}},{"cell_type":"code","source":"plt.title('Top 10 Cuisines With The Most Michelin Restaurants',fontsize=20)\nsns.barplot(x=r_data.cuisine.value_counts()[:10],y=r_data.cuisine.value_counts()[:10].index,palette='mako')\nplt.xlabel('Number Of Restaurants',fontsize=18)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; line-height: 115%; color: black; font-family: \"Times New Roman\", Times, serif;'>Looking at the above plot we can observe that the cuisines that are the most popular in Michelin Restaurants are surprisingly different from what one may expect, we see that world famous cuisines such as the Italian, French and British are lower in our list.<br>&nbsp;It may be due to the nature of the data collected but it is interesting none the less.</span></p>\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; line-height: 115%; color: black; font-family: \"Times New Roman\", Times, serif;'>&nbsp;</span></p>","metadata":{}},{"cell_type":"code","source":"ex.pie(r_data,names='Michelin Stars',title='Proportion Of Resturants According To Michelin Starts')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; line-height: 115%; color: black; font-family: \"Times New Roman\", Times, serif;'>As one would expect most of the samples (almost 80%) in our dataset represent 1 start Michelin Restaurants and a very small amount represent 3 star Michelin Restaurants of course due to the nature of receiving the stars which is more difficult from star to star.</span></p>","metadata":{}},{"cell_type":"code","source":"ex.pie(r_data,names='region',title='Proportion Of Different Regions In Our Dataset')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex.pie(r_data,names='year',title='Proportion Of Different Years in Our Dataset')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = ' '.join(re.findall(r'\\w+',' '.join(r_data.name.str.lower())))\nnames = ' '.join(name for name in names.split(' ') if len(name) > 1)\nwc = WordCloud(width=800,height=400,stopwords=STOPWORDS,colormap='coolwarm').generate(names)\nplt.title('Most Used Words In Restaurants Names',fontsize=20)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can see that there are definitely some words which repeat across the name of different<br>Michelin Restaurants even though most of those restaurants incorporate unique words even if using one of the more popular words that can be seen in the word cloud above.</span></p>","metadata":{}},{"cell_type":"code","source":"r_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(\n    rows=2, cols=2,\n    column_widths=[0.6, 0.4],\n    row_heights=[0.4, 0.6],\n    specs=[[{\"type\": \"scattergeo\", \"rowspan\": 2}, {\"type\": \"scatterpolar\", \"rowspan\": 2}],\n           [            None                    , None]])\n\nfig.add_trace(\n    go.Scatterpolar(r=r_data['Michelin Stars'].value_counts().values,theta=['1 Star','2 Stars','3 Stars'],name='Number Of Michelin Stars',fill='toself'),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scattergeo(lat=r_data[\"latitude\"],\n                  lon=r_data[\"longitude\"],\n                  mode=\"markers\",\n                  text=r_data['Michelin Stars'],\n                  hoverinfo=\"text\",\n                  showlegend=False,\n                  marker=dict(color=\"crimson\", size=4, opacity=0.8)),\n    row=1, col=1\n)\n\nfig.update_geos(\n    projection_type=\"natural earth\",\n    landcolor=\"white\",\n    oceancolor=\"MidnightBlue\",\n    showocean=True,\n    lakecolor=\"LightBlue\",\n    projection_rotation_lon=-30,\n    projection_rotation_lat=15\n\n)\n\nfig.update_xaxes(tickangle=45)\n\nfig.update_layout(title='Distribution Of Samples Around The World And The Amount Of Stars Given')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(\n    go.Scattergeo(lat=r_data[\"latitude\"],\n                  lon=r_data[\"longitude\"],\n                  mode=\"markers\",\n                  name='Michelin Stars By Size',\n                  marker_size =r_data['Michelin Stars']*5,\n                  text=r_data['Michelin Stars'],\n                  hoverinfo=\"text\",\n                  showlegend=True,\n                  marker=dict(color=\"crimson\", size=4, opacity=0.8)),\n    \n)\n\nfig.update_geos(\n    projection_type=\"natural earth\",\n    landcolor=\"white\",\n    oceancolor=\"MidnightBlue\",\n    showocean=True,\n    lakecolor=\"LightBlue\",\n    projection_rotation_lon=-30,\n    projection_rotation_lat=15\n\n)\n\nfig.update_layout(title='Michelin Star Resturants Around The World')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing And Text Feature Vectorization</h3>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>In the following step we will use the &lsquo;name&rsquo; feature from our dataset in order to make a make a rough prediction of the price category.<br>&nbsp;Those prediction will be used as a feature in our final prediction model (Similar idea to what we do while performing model blending)</span></p>\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>To perform the following maneuver we will first vectorize the name feature use the CountVectorizer,<br> from which we will receive a sparse matrix of words and there counts, we will than apply an SVD to that sparse matrix in order to reduce the dimensionality of the matrix to a dimension with a reasonable amount of explained variance.</span></p>","metadata":{}},{"cell_type":"code","source":"N_COMPONENTS = 300\n\nCV = CountVectorizer()\nname_c_matrix = CV.fit_transform(r_data.name)\nSVD_T = TruncatedSVD(n_components= N_COMPONENTS)\nt_name_matirx = SVD_T.fit_transform(name_c_matrix)\n\ndesc_ex_var = np.cumsum(SVD_T.explained_variance_ratio_)\n\ntr1 = go.Scatter(x=np.arange(0,len(desc_ex_var)),y=desc_ex_var,name='Cumulative EV')\ntr2 = go.Scatter(x=np.arange(0,len(desc_ex_var)),y=SVD_T.explained_variance_ratio_,name='Individual Component Variance')\nfig = go.Figure(data=[tr1,tr2],\n          layout=dict(title='Ocean Proximity Explained Variance Ratio Using {} Components'.format(N_COMPONENTS),xaxis_title='# Componenets',yaxis_title='Total Variance Explained'))\n\nfig.show()\n\nname_cv_df = pd.DataFrame(t_name_matirx)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Using 300 components (instead of 931) we still are able to explain more than 70% of the variance which is more than enough for our rough prediction.</span></p>\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>It is important to note that we are producing this rough prediction in order not to waste the text feature we have especially after we saw that there are recurrent words (in other words we don&rsquo;t have uniformly distributed word count across our samples) &nbsp;</span></p>","metadata":{}},{"cell_type":"code","source":"r_data.price = r_data.price.apply(lambda x: len(x) if type(x) is str  else np.nan)\ncity_encoding_dict = dict(r_data.city.value_counts())\nregion_encoding_dict = dict(r_data.region.value_counts())\ncuisine_encoding_dict = dict(r_data.cuisine.value_counts())\nr_data.city = r_data.city.apply(lambda x: city_encoding_dict[x])\nr_data.region = r_data.region.apply(lambda x: region_encoding_dict[x])\nr_data.cuisine = r_data.cuisine.apply(lambda x: cuisine_encoding_dict[x])","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Because of the cardinality of the categorical features (cuisine, city, and region) one-hot encoding is not an option, so the method of encoding I selected was Count encoding where each category was replaced with the amount of times it appears in the dataset.</span></p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Random Forest Model Prediction And Evaluation</h1>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>For the Random Forest model prediction we will use a rough estimate that we will predict as planned above using the text decomposed matrix together with the following features : &apos;<u>year</u>&apos;, &apos;<u>city</u>&apos;, &apos;<u>region</u>&apos; ,&apos;<u>cuisine</u>&apos; ,&apos;<u>Michelin Stars</u>&apos;, &apos;<u>price</u>&apos;.<br><br></span></p>\n<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The predictions will be evaluated using the f1-score and the Jaccard index.</span></p>","metadata":{}},{"cell_type":"code","source":"X_train = r_data[~r_data.price.isna()][['year','city','region','cuisine','Michelin Stars','price']]\nY_train = X_train.pop('price')\nX_name_train = name_cv_df.iloc[X_train.index,:]\nX_test  = r_data[r_data.price.isna()][['year','city','region','cuisine','Michelin Stars','price']]\nY_test  = X_test.pop('price')\nX_name_test = name_cv_df.iloc[X_test.index,:]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(X_name_train,Y_train)\nname_based_preiction = knn_model.predict(X_name_train)\ntest_name_based_preiction = knn_model.predict(X_name_test)\n\ntext_jacc = jaccard_score(Y_train,name_based_preiction,average='macro')\ntext_f1 = f1_score(Y_train,name_based_preiction,average='macro')\nprint('F1 Score {:.2f} | Jaccard Score {:.2f}  Of Text Based Prediction'.format(text_f1,text_jacc))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Train Data Prediction Using Text Data Confision Matrix',fontsize=20)\nsns.heatmap(confusion_matrix(Y_train,name_based_preiction),annot=True,fmt='d',cmap='PRGn')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Above are the results of the &ldquo;rough&rdquo; prediction we made using the decomposed text matrix we constructed earlier.<br> We see that we have both horrible F1-Scores and Jaccard Scores but it will do as a rough estimate which will hopefully help our random forest model create stronger basis.</span></p>","metadata":{}},{"cell_type":"code","source":"X_train['Text_Based_Prediction'] = name_based_preiction\nX_test['Text_Based_Prediction'] = test_name_based_preiction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>We add both to the train and the test(our evaluation set of originally missing values) the rough estimation done using the above section.</span></p>","metadata":{}},{"cell_type":"code","source":"dt_pipe = Pipeline(steps=[\n    ('scale',StandardScaler()),\n    ('model',RandomForestClassifier(random_state=42))\n])\ndt_pipe.fit(X_train,Y_train)\ndt_pred = dt_pipe.predict(X_train)\n\ntext_jacc = jaccard_score(Y_train,dt_pred,average='macro')\ntext_f1 = f1_score(Y_train,dt_pred,average='macro')\nprint('F1 Score {:.2f} | Jaccard Score {:.2f}  Of Overall Prediction On Train Data'.format(text_f1,text_jacc))","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Train Data Prediction Confision Matrix',fontsize=20)\nsns.heatmap(confusion_matrix(Y_train,dt_pred),annot=True,fmt='d',cmap='coolwarm')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"margin: 0cm 0cm 10pt; line-height: 115%; font-size: 15px; font-family: Calibri, sans-serif; text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>So we trained our Random Forest model using the selected above features together with the rough estimate and we see that we get very comforting metrics , F1-Scroe of <u><strong>0.82 </strong></u>and Jaccard Score of <strong><u>0.7</u></strong><br>(also can be evaluated by looking at the confusion matrix).<br> Overall I would estimate that the model does fairly well even though we did not validate it on train and test partitions.</span></p>","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'Prediction':dt_pred,'Actual':Y_train})\noutput.to_csv('view_prediction.csv',index=False)\n\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Actual\"])),\n        y=output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"],\n        mode=\"markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_prediction_df  = r_data[r_data.price.isna()]\nmissing_prediction_df['price'] = dt_pipe.predict(X_test)\nmissing_prediction_df.to_csv('Missing_Price_Values_Predictions.csv',index=False)\nmissing_prediction_df.sample(5)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}