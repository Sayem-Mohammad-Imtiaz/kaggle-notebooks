{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Introduction</h1>\n\n<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">Hello, my fellow Kagglers; in the following Kernel, we will be exploring Airbnb listings from the USA; the main goal of this Kernel will be exploring the data and predicting the price of a listing given a new sample.\nOur methodology in this Kernel will follow a standard analysis and prediction methodology, where we will first assess the data for any missing values followed by outlier imputation.\nThe next stage will be the EDA. We will conduct two types of analysis: a distribution analysis on our numeric features and a location-based analysis, hopefully seeing patterns in different USA locations, which will support our model later on.\nAfter we are done with the EDA, we will construct our model; the model will consist of two parts; the first will be a sequential neural network with an embedding layer which will break down the name of the listing and create predictions based on those embeddings, the prediction from the sequential model will be fed into a random forest model together with the remaining numeric features from our dataset depending on what we find during the EDA, the idea is to create an ensembled model pipeline which will take full advantage of the listing name which is usually a key feature when looking at the price (many key features are described as words in the name and do not exist as individual features in our data.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities</h3>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nimport string\nimport re\nimport nltk\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport os\nimport collections\ndef RMSE(Y,YHAT):\n    return np.sqrt(mean_squared_error(Y,YHAT))\n\nstopwords=list(STOPWORDS)\n\nplt.rc('figure',figsize=(20,11))\n\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Importation And Missing Value Assessment</h3>\n","metadata":{}},{"cell_type":"code","source":"a_data = pd.read_csv('/kaggle/input/us-airbnb-open-data/AB_US_2020.csv',usecols=['id','name','latitude','longitude','room_type','price','minimum_nights','number_of_reviews','last_review','reviews_per_month','calculated_host_listings_count',\n                                                                                'availability_365','city'])\na_data.head(3)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_features = a_data.select_dtypes(include=['int64','float64']).columns\nnominal_features = a_data.select_dtypes(include=['object'])\nnumeric_features=numeric_features.delete(0)\n\na_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = a_data.isna().sum()\n\nmissing /= a_data.shape[0]\nmissing *=100\nmissing = missing.to_frame().rename(columns={0:'Precent Of Missing Values'})\nmissing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.heatmap(a_data.isna().T)\nax.set_title('Missing Values Proportion',fontsize=19,fontweight='bold')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_data = a_data.dropna()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h1>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Numeric Features Distribution Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=4)\naux = 0\nfig.set_figheight(17)\nfig.set_figwidth(25)\nfor row in axes:\n    for col in row:\n        a_data[numeric_features[aux]].plot(kind='kde',ax=col)\n        col.set_title(numeric_features[aux] +' Distribution',fontsize=16,fontweight='bold')\n        aux+=1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Outlier Removal</h3>\n","metadata":{}},{"cell_type":"code","source":"# Removing Outliers\nlower_bound = .25\nupper_bound = .75\niqr = a_data[a_data['price'].between(a_data['price'].quantile(lower_bound), a_data['price'].quantile(upper_bound), inclusive=True)]\niqr = iqr[iqr['number_of_reviews'] > 0]\niqr = iqr[iqr['calculated_host_listings_count'] < 10]\niqr = iqr[iqr['number_of_reviews'] < 200]\niqr = iqr[iqr['minimum_nights'] < 10]\niqr = iqr[iqr['reviews_per_month'] < 5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Processed Distributions After Ourlier Removal</h3>\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=4)\naux = 0\nfig.set_figheight(17)\nfig.set_figwidth(25)\nfor row in axes:\n    for col in row:\n        iqr[numeric_features[aux]].plot(kind='kde',ax=col)\n        if numeric_features[aux] not in ['latitude','longitude']:\n            col.set_xlim(0,iqr[numeric_features[aux]].max()+iqr[numeric_features[aux]].max()*0.25)\n        col.set_title(numeric_features[aux] +' Distribution',fontsize=16,fontweight='bold')\n        aux+=1\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\n    So we see that after we removed most of the outliers in our data, we are left we more meaningful distributions from which we can extract some insight.\nWe can see that the number_of_reveiws feature, as well as the reveiws_pre_month feature, follow an exponential distribution in contrast to all other numeric features in the dataset, which seem to follow a multimodal distribution, which makes sense because there are many underlying groups in our dataset (different location for example) \n</p>","metadata":{}},{"cell_type":"code","source":"pr_data =iqr.copy()\ndesc = pr_data.describe()\ndesc.loc['skew'] = desc.skew()\ndesc.loc['skew'] = desc.kurt()\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\ndesc.drop(columns='id').style.highlight_max(axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(\n    rows=2, cols=2,\n    column_widths=[0.6, 0.4],\n    row_heights=[0.6, 0.5],\n    specs=[[{\"type\": \"scattergeo\", \"rowspan\": 2}, {\"type\": \"bar\"}],\n           [            None                    , {\"type\": \"bar\"}]])\n\nfig.add_trace(\n    go.Bar(y=pr_data.room_type.value_counts(),x=pr_data.room_type.value_counts().index,name='Room Type'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Bar(y=pr_data.minimum_nights.value_counts(),x=pr_data.minimum_nights.value_counts().index,name='Minimum Nights'),\n    row=2, col=2\n)\n\nfig.add_trace(\n    #go.Scatter3d(x=pr_data.longitude, y=pr_data.latitude,z=pr_data.price,color=pr_data.room_type,mode='markers'),\n    go.Scattergeo(lat=pr_data[\"latitude\"],\n                  lon=pr_data[\"longitude\"],\n                  mode=\"markers\",\n                  text=pr_data.price,\n                  hoverinfo=\"text\",\n                  showlegend=True,\n                  name='Price'\n                  ,\n                  marker=dict(color=\"crimson\", size=4, opacity=0.8)),\n    row=1, col=1\n)\n\nfig.update_geos(\n    projection_type=\"orthographic\",\n    landcolor=\"white\",\n    oceancolor=\"MidnightBlue\",\n    showocean=True,\n    lakecolor=\"LightBlue\",\n    projection_rotation_lon=-92,\n    projection_rotation_lat=15\n\n)\n\nfig.update_xaxes(tickangle=45)\n\nfig.update_layout(title='Distribution Of Different Room Types')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"2.4\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">City And State Based Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(y=pr_data['city'],order=pr_data['city'].value_counts().index,palette='rocket')\nax.set_yticklabels(ax.get_yticklabels(),fontsize=11,fontweight='bold')\nax.set_title('Distribution Of Different Cities In Our Data',fontsize=16,fontweight='bold')\nax.set_xlabel('Count',fontsize=14,fontweight='bold')\n#ax.set_title()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"states_dic = {'Asheville':'NC','Austin':'TX','Boston':'MA','Broward County':'FL','Cambridge':'MA','Chicago':'IL','Clark County':'NV','Columbus':'OH','Denver':'CO','Hawaii':'HI','Jersey City':'NJ',\n             'Los Angeles':'SC','Nashville':'TN','New Orleans':'MS','New York City':'NY','Oakland':'CA','Pacific Grove':'CA','Portland':'OR','Rhode Island':'RI','Salem':'MA','San Clara Country':'CA',\n             'Santa Cruz County':'CA','San Diego':'CA','San Francisco':'CA','San Mateo County':'CA','Seattle':'WA','Twin Cities MSA':'MN','Washington D.C.':'DC'}\n\npr_data['state'] = pr_data['city'].apply(lambda x : states_dic[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = pr_data['state'].value_counts()\nfig = ex.choropleth(locations=counts.index,color=counts.values, locationmode=\"USA-states\", scope=\"usa\",title='Number Of Listings By State ', color_continuous_scale=ex.colors.diverging.Portland)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nMost of the listings in our dataset are cities in the New York state\n</p>","metadata":{}},{"cell_type":"code","source":"g_dat = pr_data.groupby(by='state').mean()\n#g_dat = g_dat.reset_index()\nfig = ex.choropleth(g_dat,locations=g_dat.index,color='price', locationmode=\"USA-states\", scope=\"usa\",title='Average Listing Price At Each State', color_continuous_scale=ex.colors.diverging.Portland)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nWe can see that there are a couple of states where the average price is significantly lower / higher than the others, Hawaii stands out as the state with the highest average price and Oregon has the lowset average price.\n</p>","metadata":{}},{"cell_type":"code","source":"fig = ex.choropleth(g_dat,locations=g_dat.index,color='number_of_reviews', locationmode=\"USA-states\", scope=\"usa\",title='Average Listing Number Of Reviews At Each State', color_continuous_scale=ex.colors.diverging.Portland)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nThe number of reviews at each state also shows us that some states have on average significantly more/fewer reviews than others; we can see that on average, North Carolina and Oregon have the largest amount of reviews in comparison to Florida and New York, which tend to have fewer reviews.\n</p>","metadata":{}},{"cell_type":"code","source":"fig = ex.choropleth(g_dat,locations=g_dat.index,color='availability_365', locationmode=\"USA-states\", scope=\"usa\",title='Average Listing Availability At Each State', color_continuous_scale=ex.colors.diverging.Portland)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nWhen we look at the availability feature, which tells us how many days a year each listing is available, we see that only a few states differ significantly like Colorado and Hawaii for example where listings in Colorado are available 87 days a year on average, unlike Hawaii listings which on average are available  213 days a year.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.5\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Text Based Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"names = ' '.join(pr_data['name'].str.lower().values)\nplt.imshow(WordCloud(width=800,height=600,min_font_size=10,stopwords=stopwords).generate(names))\nplt.title('Most Common Words In The Name Feature',fontsize=18,fontweight='bold')\nplt.axis('off')\n\n\nnames = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \"\", names)\nnames = re.findall(r'\\w+', names)\nnames = ' '.join([tok for tok in names if tok not in stopwords and len(tok) >2])\n\nnames_freq_dic = nltk.FreqDist(names.split(' '))\ndict(names_freq_dic)\n\nplt.show()\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nFrom the word cloud above, we can confirm our assumption that words in the name of the listing describe the product itself, words that can point to elements and attributes that donate to the price formulation.\nThe sequential neural network model we will construct will try to predict the price of a listing solely on these words.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Model Selection And Evaluation</h1>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Vocabulary Extraction And Preprocessing</h3>\n","metadata":{}},{"cell_type":"code","source":"vocab = collections.Counter(' '.join(pr_data['name']).split(' '))\n\n\nMAX_LENGTH = max(pr_data['name'].apply(lambda x: len(x)))\nVOCAB_SIZE = len(vocab.keys())\nVECTOR_SPACE = 100\n\nencoded_docs = [tf.keras.preprocessing.text.one_hot(d,VOCAB_SIZE) for d in pr_data.name]\n\npadded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding='post')\n\npadded_docs_eval = padded_docs[0:1000]\npadded_docs = padded_docs[1000:]\nY = pr_data.price[1000:]\nY_eval = pr_data.price[:1000]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nWe have created constant variables for our Embedding layer, representing the vocabulary size of the 'name' feature, the target vector space we want to map our word embeddings to, and the length of the input.\nWe also preprocessed all our words by converting them into a one-hot sparse matrix and padding each vector to the same length.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Sequantial Model Assembling</h3>\n","metadata":{}},{"cell_type":"code","source":"FCNN_MODEL = Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,VECTOR_SPACE,input_length=MAX_LENGTH),\n    tf.keras.layers.Flatten(),\n    Dense(activation='relu',units=5),\n    Dense(activation='relu',units=1)\n    \n])\n\nFCNN_MODEL.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(FCNN_MODEL,show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Training</h3>\n","metadata":{}},{"cell_type":"code","source":"history = FCNN_MODEL.fit(padded_docs, Y,validation_data=(padded_docs_eval,Y_eval),epochs=2,batch_size=150)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = FCNN_MODEL.predict(padded_docs)\npredictions = predictions.reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation</h3>\n","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({\"Actual\":Y,'Prediction':predictions})\n\nplt.title('Residual Analysis for Heteroskedasticity Evaluation Of ANN Model',fontsize=18,fontweight='bold')\nax = sns.residplot(x=results['Actual'],y=results['Prediction'])\n\ntextstr = f'RMSE: {np.round(RMSE(predictions,Y),3)}'\n\nprops = dict(boxstyle='round', facecolor='tab:red', alpha=0.5)\nax.text(0.83, 0.95, textstr, transform=ax.transAxes, fontsize=24,\n        verticalalignment='top', bbox=props)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.5\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Creating New DataFrame For Ensembled Learning</h3>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nUsing the predicted price from our sequential embeddings model, we will combine the results with the corresponding USA state of each listing (encoded into one hot features) and use this data to train a new random forest model.\n</p>\n","metadata":{}},{"cell_type":"code","source":"t_df = pd.DataFrame({\"Actual Price\":Y.values,'Prediction':predictions})\nt_df['number_of_reviews'] = pr_data.loc[Y.index,'number_of_reviews'].values\nt_df = pd.concat([t_df,pd.get_dummies(pr_data.loc[Y.index,'state']).drop(columns= ['WA']).reset_index().drop(columns='index')],axis=1)\nt_df = pd.concat([t_df,pd.get_dummies(pr_data.loc[Y.index,'room_type']).drop(columns=['Shared room']).reset_index().drop(columns='index')],axis=1)\nt_df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cols =t_df.iloc[:,1:]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.6\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Fitting Ensembled Random Forest Model</h3>\n","metadata":{}},{"cell_type":"code","source":"RF_pipeline = Pipeline(steps=[('model',RandomForestRegressor(random_state=42))])\n\nRF_pipeline.fit(X_cols,t_df.iloc[:,0])\n\nRF_predictions = RF_pipeline.predict(X_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Results</h3>\n","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({'id':pr_data.loc[Y.index,'id'],\"Actual\":Y.values,'Prediction':RF_predictions})\nresults.head(10)\n\nplt.title('Residual Analysis for Heteroskedasticity Evaluation',fontsize=18,fontweight='bold')\nax = sns.residplot(x=results['Actual'],y=results['Prediction'])\n\ntextstr = f'RMSE: {np.round(RMSE(RF_predictions,Y),3)}'\n\nprops = dict(boxstyle='round', facecolor='tab:red', alpha=0.5)\nax.text(0.83, 0.95, textstr, transform=ax.transAxes, fontsize=24,\n        verticalalignment='top', bbox=props)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman;font-size:150%;text-align:center\">\nOverall the residual between the actual values and the predicted values varies around -20/20 USD, which is a fairly small interval for error.\nWe can also see our model has a small degree of heteroskedasticity in the minimum and maximum prices. Still, the degree is low, so we can assume the model did not overfit to a certain part of our data.\n</p>\n","metadata":{}},{"cell_type":"code","source":"results.to_csv('Price_Predictions.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}